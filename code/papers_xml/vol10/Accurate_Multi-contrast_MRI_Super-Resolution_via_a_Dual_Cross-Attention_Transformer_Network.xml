<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network</title>
				<funder ref="#_FwMaHpS #_7F9ygXv">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_gFTu2TK">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_MXn94pd">
					<orgName type="full">Shenzhen Higher Education Stable Support Program</orgName>
				</funder>
				<funder ref="#_ddR9d5z">
					<orgName type="full">Natural Science Foundation of Top Talent of Shenzhen Technology University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shoujin</forename><surname>Huang</surname></persName>
							<idno type="ORCID">0000-0001-6094-129X</idno>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Technology University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingyu</forename><surname>Li</surname></persName>
							<idno type="ORCID">0000-0002-7483-8872</idno>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Technology University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lifeng</forename><surname>Mei</surname></persName>
							<idno type="ORCID">0000-0002-8222-2461</idno>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Technology University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tan</forename><surname>Zhang</surname></persName>
							<idno type="ORCID">0000-0002-3187-1453</idno>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Technology University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziran</forename><surname>Chen</surname></persName>
							<idno type="ORCID">0000-0003-1113-7697</idno>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Technology University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Dong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Samii Medical Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linzheng</forename><surname>Dong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shaojun</forename><surname>Liu</surname></persName>
							<idno type="ORCID">0000-0002-9201-9301</idno>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Technology University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shenzhen Samii Medical Center</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Mengye</forename><surname>Lyu</surname></persName>
							<email>lvmengye@sztu.edu.cn</email>
							<idno type="ORCID">0000-0001-5548-8136</idno>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Technology University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="313" to="322"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">7BFA1DB399B00526CADE77CC9F797FB9</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_30</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Magnetic resonance imaging</term>
					<term>Super-resolution</term>
					<term>Multi-contrast</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Magnetic Resonance Imaging (MRI) is a critical imaging tool in clinical diagnosis, but obtaining high-resolution MRI images can be challenging due to hardware and scan time limitations. Recent studies have shown that using reference images from multi-contrast MRI data could improve super-resolution quality. However, the commonly employed strategies, e.g., channel concatenation or hard-attention based texture transfer, may not be optimal given the visual differences between multi-contrast MRI images. To address these limitations, we propose a new Dual Cross-Attention Multi-contrast Super Resolution (DCAMSR) framework. This approach introduces a dual cross-attention transformer architecture, where the features of the reference image and the upsampled input image are extracted and promoted with both spatial and channel attention in multiple resolutions. Unlike existing hard-attention based methods where only the most correlated features are sought via the highly down-sampled reference images, the proposed architecture is more powerful to capture and fuse the shareable information between the multi-contrast images. Extensive experiments are conducted on fastMRI knee data at high field and more challenging brain data at low field, demonstrating that DCAMSR can substantially outperform the stateof-the-art single-image and multi-contrast MRI super-resolution methods, and even remains robust in a self-referenced manner. The code for DCAMSR is avaliable at https://github.com/Solor-pikachu/DCAMSR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Magnetic Resonance Imaging (MRI) has revolutionized medical diagnosis by providing a non-invasive imaging tool with multiple contrast options <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. However, generating high-resolution MRI images can pose difficulties due to hardware limitations and lengthy scanning times <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. To tackle this challenge, super-resolution techniques have been developed to improve the spatial resolution of MRI images <ref type="bibr" target="#b4">[5]</ref>. However, while several neural network-based super-resolution methods (e.g., EDSR <ref type="bibr" target="#b5">[6]</ref>, SwinIR <ref type="bibr" target="#b6">[7]</ref>, and ELAN <ref type="bibr" target="#b7">[8]</ref>) have emerged from the computer vision field, they primarily utilize single-contrast data, ignoring the valuable complementary multi-contrast information that is easily accessible in MRI.</p><p>Recent studies have shown that multi-contrast data routinely acquired in MRI examinations can be used to develop more powerful super-resolution methods tailored for MRI by using fully sampled images of one contrast as a reference (Ref) to guide the recovery of high-resolution (HR) images of another contrast from low-resolution (LR) inputs <ref type="bibr" target="#b8">[9]</ref>. In this direction, MINet <ref type="bibr" target="#b9">[10]</ref> and SANet <ref type="bibr" target="#b10">[11]</ref> have been proposed and demonstrated superior performance over previous single-image super-resolution approaches. However, these methods rely on relatively simple techniques, such as channel concatenation or spatial addition between LR and Ref images, or using channel concatenation followed by self-attention to identify similar textures between LR and Ref images. These approaches may overlook the complex relationship between LR and Ref images and lead to inaccurate super-resolution.</p><p>Recent advances in super-resolution techniques have led to the development of hard-attention-based texture transfer methods (such as TTSR <ref type="bibr" target="#b11">[12]</ref>, MASA <ref type="bibr" target="#b12">[13]</ref>, and McMRSR <ref type="bibr" target="#b13">[14]</ref>) using the texture transformer architecture <ref type="bibr" target="#b11">[12]</ref>. However, these methods may still underuse the rich information in multi-contrast MRI data. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>(a), these methods focus on spatial attention and only seek the most relevant patch for each query. They also repetitively use low-resolution attention maps from down-sampled Ref images (Ref ↓↑ ), which may not be sufficient to capture the complex relationship between LR and Ref images, potentially resulting in suboptimal feature transfer. These limitations can be especially problematic for noisy low-field MRI data, where down-sampling the Ref images (as the key in the transformer) can cause additional image blurring and information loss.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b), our proposed approach is inspired by the transformerbased cross-attention approach <ref type="bibr" target="#b14">[15]</ref>, which provides a spatial cross-attention mechanism using full-powered transformer architecture without Ref image downsampling, as well as the UNETR++ architecture <ref type="bibr" target="#b15">[16]</ref>, which incorporates channel attention particularly suitable for multi-contrast MRI images that are anatomically aligned. Building upon these developments, the proposed Dual Cross-Attention Multi-contrast Super Resolution (DCAMSR) method can flexibly search the reference images for shareable information with multi-scale attention maps and well capture the information both locally and globally via spatial and channel attention. Our contributions are summarized as follows: 1) We present a novel MRI super-resolution framework different from existing hardattention-based methods, leading to efficient learning of shareable multi-contrast information for more accurate MRI super-resolution. 2) We introduce a dual cross-attention transformer to jointly explore spatial and channel information, substantially improving the feature extraction and fusion processes. 3) Our proposed method robustly outperforms the current state-of-the-art single-image as well as multi-contrast MRI super-resolution methods, as demonstrated by extensive experiments on the high-field fastMRI <ref type="bibr" target="#b16">[17]</ref> and more challenging low-field M4Raw <ref type="bibr" target="#b17">[18]</ref> MRI datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Overall Architecture. Our goal is to develop a neural network that can restore an HR image from an LR image and a Ref image. Our approach consists of several modules, including an encoder, a dual cross-attention transformer (DCAT) and a decoder, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Firstly, the LR is interpolated to match the resolution of HR. Secondly, we use the encoder to extract multi-scale features from both the up-sampled LR and Ref, resulting in features F LR and F Ref . Thirdly, the DCAT, which contains of dual cross-attention (DCA), Layer Normalization (LN) and feed-forward network (FFN), is used to search for texture features from F LR and F Ref . Fourthly, the texture features are aggregated with F LR through the Fusion module at each scale. Finally, a simple convolution is employed to generate SR from the fused feature.</p><p>Encoder. To extract features from the up-sampled LR, we employ an encoder consisting of four stages. The first stage uses the combination of a depth-wise convolution and a residual block. In stages 2-4, we utilize a down-sampling layer and a residual block to extract multi-scale features. In this way, the multiscale features for the LR ↑ are extracted as  The core of DCAT is dual cross-attention mechanism, which is diagrammed in Fig. <ref type="figure" target="#fig_3">3</ref>. Firstly, we project F LR and F Ref to q, k and v. For the two crossattention branches, the linear layer weights for q and k are shared, while those for v are different:</p><formula xml:id="formula_0">F H×W LR , F H 2 × W 2 LR , F H 4 × W 4 LR and F H 8 × W 8 LR ,</formula><formula xml:id="formula_1">q share = W q share (F LR ), k share = W k share (F Ref ),<label>(1)</label></formula><formula xml:id="formula_2">v spatial = W v spatial (F Ref ), v channel = W v channel (F Ref ),<label>(2)</label></formula><p>where q share ,k share ,v spatial and v channel are the parameter weights for shared queries, shared keys, spatial value layer, and channel value layer, respectively. In spatial cross-attention, we further project k share and v spatial to k project and v project through linear layers, to reduce the computational complexity. The spatial and channel attentions are calculated as: Finally, X spatial and X channel are reduced to half channel via 1 × 1 convolutions, and then concatenate to obtain the final feature:</p><formula xml:id="formula_3">X spatial = sof tmax( q share • k T share √ d ) • v project , (<label>3</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">X channel = sof tmax( q T share • k share √ d ) • v T channel . (<label>4</label></formula><formula xml:id="formula_6">)</formula><formula xml:id="formula_7">X = Concat(Conv(X spatial ), Conv(X channel )). (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>For the whole DCAT, the normalized features LN (F LR ) and LN (F Ref ) are fed to the DCA and added back to F LR . The obtained feature is then processed by the FFN in a residual manner to generate the texture feature. Specifically, the DCAT is summarized as:</p><formula xml:id="formula_9">X = F LR + DCA(LN (F LR ), LN(F Ref )),<label>(6)</label></formula><formula xml:id="formula_10">T exture = X + F F N(LN (X)).<label>(7)</label></formula><p>Feeding the multi-scale features of LR ↑ and Ref to DCAT, we can generate the texture features in multi-scales, denoted as T exture H×W , T exture</p><formula xml:id="formula_11">H 2 × W 2 , and T exture H 4 × W 4 .</formula><p>Decoder. In the decoder, we start from the feature F is then up-sampled and feed to Fusion along with T exture</p><formula xml:id="formula_12">H 2 × W 2 , generating F used H 2 × W 2 . Similarly, F used H 2 × W 2</formula><p>is up-sampled and feed to Fusion along with T exture H×W , generating F used H×W . Finally, F used H×W is processed with a 1 × 1 convolution to generate SR.</p><p>In the Fusion module, following <ref type="bibr" target="#b12">[13]</ref>, the texture feature T exture and input feature F LR are first fed to Spatial Adaptation Module (SAM), a learnable structure ensuring the distributions of T exture consistent with F LR , as shown in Fig. <ref type="figure" target="#fig_1">2(d</ref>). The corrected texture feature is then concatenated with the input feature F LR and further incorporated via a convolution and a residual block, as shown in Fig. <ref type="figure" target="#fig_1">2(c)</ref>.</p><p>Loss Function. For simplicity and without loss of generality, L 1 loss between the restored SR and ground-truth is employed as the overall reconstruction loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets and Baselines. We evaluated our approach on two datasets: 1) fastMRI, one of the largest open-access MRI datasets. Following the settings of SANet <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, 227 and 24 pairs of PD and FS-PDWI volumes are selected for training and validation, respectively. 2) M4Raw, a publicly available dataset including multi-channel k-space and single-repetition images from 183 participants, where each individual haves multiple volumes for T1-weighted, T2weighted and FLAIR contrasts <ref type="bibr" target="#b17">[18]</ref>. 128 individuals/6912 slices are selected for training and 30 individuals/1620 slices are reserved for validation. Specifically, T1-weighted images are used as reference images to guide T2-weighted images. To generate the LR images, we first converted the original image to k-space and cropped the central low-frequency region. For down-sampling factors of 2× and 4×, we kept the central 25% and 6.25% values in k-space, respectively, and then transformed them back into the image domain using an inverse Fourier transform. The proposed method is compared with SwinIR <ref type="bibr" target="#b6">[7]</ref>, ELAN <ref type="bibr" target="#b7">[8]</ref>, SANet (the journal version of MINet) <ref type="bibr" target="#b10">[11]</ref>, TTSR <ref type="bibr" target="#b11">[12]</ref>, and MASA <ref type="bibr" target="#b12">[13]</ref>.</p><p>Implementation Details. All the experiments were conducted using Adam optimizer for 50 epochs with a batch size of 4 on 8 Nvidia P40 GPUs. The initial learning rate for SANet was set to 4 × 10 -5 according to <ref type="bibr" target="#b10">[11]</ref>, and 2 × 10 -4 for the other methods. The learning rate was decayed by a factor of 0.1 for the last 10 epochs. The performance was evaluated for enlargement factors of 2× and 4× in terms of PNSR and SSIM.</p><p>Quantitative Results. The quantitative results are summarized in Table <ref type="table" target="#tab_0">1</ref>. The proposed method achieves the best performance across all datasets for both single image super-resolution (SISR) and multi-contrast super-resultion (MCSR). Specifically, our LR-guided DCAMSR version surpasses state-of-theart methods such as ELAN and SwinIR in SISR, and even outperforms SANet (a MCSR method). Among the MCSR methods, neither SANet, TTSR or MASA achieves better results than the proposed method. In particular, the PSNR for MASA is even 0.18 dB lower than our SISR version of DCAMSR at 4× enlargement on M4Raw dataset. We attribute this performance margin to the difficulty of texture transformers in extracting similar texture features between Ref and Ref ↓↑ . Despite the increased difficulty of super-resolution at 4× enlargement, our model still outperforms other methods, demonstrating the powerful texture transfer ability of the proposed DCA mechanism.</p><p>Qualitative Evaluation. Visual comparison is shown in Fig. <ref type="figure" target="#fig_5">4</ref>, where the upsampled LR, the ground-truth HR, the restored SR and the error map for each method are visualized for 4× enlargement on both datasets. The error map depicts the degree of restoration error, where the more prominent texture indicating the poorer restoration quality. As can be seen, the proposed method produces the least errors compared with other methods.</p><p>Ablation Study. We conducted ablation experiments on the M4Raw dataset and the results are shown in Table <ref type="table" target="#tab_1">2</ref>. Three variations are tested: w/o reference,  where LR ↑ is used as the reference instead of Ref ; w/o multi-scale attention, where only the lowest-scale attention is employed and interpolated to other scales; and w/o channel attention, where only spatial attention is calculated. The improvement from w/o reference to DCAMSR demonstrates the effectiveness of MCSR compared with SISR. The performance degradation of w/o multiscale attention demonstrates that the lowest-scale attention is not robust. The improvement from w/o channel attention to DCAMSR shows the effectiveness of the channel attention. Moreover, our encoder and decoder have comparable parameter size to MASA but we achieved higher scores, as shown in Table <ref type="table" target="#tab_0">1</ref>, demonstrating that the spatial search ability of DCAMSR is superior to the original texture transformer.</p><p>Discussion. Our reported results on M4Raw contain instances of slight interscan motion <ref type="bibr" target="#b17">[18]</ref>, demonstrating certain resilience of our approach to image misalignment, but more robust solutions deserve further studies. Future work may also extend our approach to 3D data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we propose a Dual Cross-Attention Multi-contrast Super Resolution (DCAMSR) framework for improving the spatial resolution of MRI images. As demonstrated by extensive experiments, the proposed method outperforms existing state-of-the-art techniques under various conditions, proving a powerful and flexible solution that can benefit a wide range of medical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Illustration of Texture Transformer. (b) Illustration of the proposed Dual Cross-Attention Transformer.</figDesc><graphic coords="3,82,47,134,36,287,77,167,74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Network architecture of the proposed Dual Cross-attention Multi-contrast Super Resolution (DCAMSR). (b) Details of Dual Cross-Attention Transformer (DCAT). (c) Details of Fusion block. (d) Details of Spatial Adaptation Module (SAM).</figDesc><graphic coords="4,44,79,54,35,334,51,198,37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>respectively. Similarly, the multi-scale features for Ref are extracted via the same encoder in stages 1-3 and denoted as F H×W Ref , Dual Cross-Attention Transformer (DCAT). The DCAT consists of a DCA module, 2 LNs, and a FFN comprising several 1×1 convolutions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Details of Dual Cross-Attention (DCA).</figDesc><graphic coords="5,69,96,54,14,312,64,197,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>H 8 × W 8 LRH 4 × W 4 LR,</head><label>8844</label><figDesc>and process it with a convolution and a residual block. Then it is up-sampled and concatenated with F and then feed to a convolution to further incorporate the both information. Next, the incorporated feature is fed to the Fusion module along with T extureH 4 × W 4 ,to produce the fused feature at H 4 × W 4 scale, denoted as F used H 4 × W 4 . F used H 4 × W 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visual comparison of reconstruction results and error maps for 4× enlargement on both datasets. The upper two rows are fastMRI and the lower two rows are M4Raw.</figDesc><graphic coords="8,44,79,53,75,334,48,196,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results on two datasets with different enlargement scales, in terms of PSNR and SSIM. SISR means single image super resolution, MCSR means multi-contrast super resolution. The best results are marked in for multi-contrast super resolution, and in blue for single image super resolution. Note that TTSR and MASA are not applicable to 2× enlargement based on their official implementation.</figDesc><table><row><cell>Dataset</cell><cell cols="2">fastMRI</cell><cell></cell><cell>M4Raw</cell><cell></cell></row><row><cell>Scale</cell><cell>2×</cell><cell></cell><cell>4×</cell><cell>2×</cell><cell></cell><cell>4×</cell></row><row><cell>Metrics</cell><cell cols="6">PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM</cell></row><row><cell>SISR ELAN</cell><cell cols="6">32.00 0.715 30.45 0.619 31.71 0.770 28.70 0.680</cell></row><row><cell>SwinIR</cell><cell cols="6">32.04 0.717 30.58 0.624 32.08 0.775 29.42 0.701</cell></row><row><cell cols="5">DCAMSR 32.07 0.717 30.71 0.627 32.19</cell><cell cols="2">0.777 29.74 0.709</cell></row><row><cell>MCSR SANet</cell><cell cols="6">32.00 0.716 30.40 0.622 32.06 0.775 29.48 0.704</cell></row><row><cell>TTSR</cell><cell>NA</cell><cell>NA</cell><cell cols="2">30.67 0.628 NA</cell><cell>NA</cell><cell>29.84 0.712</cell></row><row><cell>MASA</cell><cell>NA</cell><cell>NA</cell><cell cols="2">30.78 0.628 NA</cell><cell>NA</cell><cell>29.52 0.704</cell></row><row><cell cols="7">DCAMSR 32.20 0.721 30.97 0.637 32.31 0.779 30.48 0.728</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on the M4Raw dataset with 4× enlargement.</figDesc><table><row><cell>Variant</cell><cell>Modules</cell><cell>Metrics</cell></row><row><cell></cell><cell cols="3">reference multi-scale attention channel attention PSNR↑ SSIM↑ NMSE↓</cell></row><row><cell>w/o reference</cell><cell></cell><cell>29.74</cell><cell>0.709 0.035</cell></row><row><cell>w/o multi-scale attention</cell><cell></cell><cell>30.40</cell><cell>0.725 0.031</cell></row><row><cell>w/o channel attention</cell><cell></cell><cell>29.79</cell><cell>0.712 0.035</cell></row><row><cell>DCAMSR</cell><cell></cell><cell>30.48</cell><cell>0.728 0.029</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62101348</rs>, the <rs type="funder">Shenzhen Higher Education Stable Support Program</rs> under Grant <rs type="grantNumber">20220716111838002</rs>, and the <rs type="funder">Natural Science Foundation of Top Talent of Shenzhen Technology University</rs> under Grants <rs type="grantNumber">20200208</rs>, <rs type="grantNumber">GDRC202117</rs>, and <rs type="grantNumber">GDRC202134</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gFTu2TK">
					<idno type="grant-number">62101348</idno>
				</org>
				<org type="funding" xml:id="_MXn94pd">
					<idno type="grant-number">20220716111838002</idno>
				</org>
				<org type="funding" xml:id="_ddR9d5z">
					<idno type="grant-number">20200208</idno>
				</org>
				<org type="funding" xml:id="_FwMaHpS">
					<idno type="grant-number">GDRC202117</idno>
				</org>
				<org type="funding" xml:id="_7F9ygXv">
					<idno type="grant-number">GDRC202134</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 30.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Super-resolution methods in MRI: can they improve the trade-off between resolution, signal-to-noise ratio, and acquisition time?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Plenge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1983" to="1993" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Super-resolution in magnetic resonance imaging: a review</title>
		<author>
			<persName><forename type="first">E</forename><surname>Van Reeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Poh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concepts Magn. Reson. Part A</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="306" to="325" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Brain MRI super-resolution using coupled-projection residual network</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">456</biblScope>
			<biblScope unit="page" from="190" to="199" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High-resolution pelvic MRI reconstruction using a generative adversarial network with attention and cyclic loss</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="105951" to="105964" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Brain MRI super resolution using 3D deep densely connected neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Christodoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="739" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SwinIR: image restoration using Swin transformer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient long-range attention network for image super-resolution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2022, Part XVII</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13677</biblScope>
			<biblScope unit="page" from="649" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-contrast super-resolution MRI through a progressive network</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2738" to="2749" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-contrast MRI super-resolution via a multi-stage integration network</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI, Part VI</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13677</biblScope>
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploring separable attention for multi-contrast MR image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01664</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5791" to="5800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Masa-SR: matching acceleration and spatial adaptation for reference-based image super-resolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6368" to="6377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transformer-empowered multi-scale contextual matching and aggregation for multi-contrast mri super-resolution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20636" to="20645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Jaegle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.14795</idno>
		<title level="m">Perceiver IO: a general architecture for structured inputs &amp; outputs</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Unetr++: delving into efficient and accurate 3d medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.04497</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08839</idno>
		<title level="m">fastMRI: an open dataset and benchmarks for accelerated MRI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">M4raw: a multi-contrast, multi-repetition, multi-channel MRI kspace dataset for low-field MRI research</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">264</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
