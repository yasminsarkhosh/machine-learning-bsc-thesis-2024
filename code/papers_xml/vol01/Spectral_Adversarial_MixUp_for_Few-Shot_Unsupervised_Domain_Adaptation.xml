<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation</title>
				<funder ref="#_ScN4W22">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_rZYp6NR">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiajin</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanqing</forename><surname>Chao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amit</forename><surname>Dhurandhar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Tajer</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical, Computer, and Systems Engineering</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yangyang</forename><surname>Xu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Mathematical Sciences</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pingkun</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="728" to="738"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F549CE2BE69EEFA8F751B64897F829B0</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_69</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Few-shot UDA</term>
					<term>Data Augmentation</term>
					<term>Spectral Sensitivity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain shift is a common problem in clinical applications, where the training images (source domain) and the test images (target domain) are under different distributions. Unsupervised Domain Adaptation (UDA) techniques have been proposed to adapt models trained in the source domain to the target domain. However, those methods require a large number of images from the target domain for model training.</p><p>In this paper, we propose a novel method for Few-Shot Unsupervised Domain Adaptation (FSUDA), where only a limited number of unlabeled target domain samples are available for training. To accomplish this challenging task, first, a spectral sensitivity map is introduced to characterize the generalization weaknesses of models in the frequency domain. We then developed a Sensitivity-guided Spectral Adversarial MixUp (SAMix) method to generate target-style images to effectively suppresses the model sensitivity, which leads to improved model generalizability in the target domain. We demonstrated the proposed method and rigorously evaluated its performance on multiple tasks using several public datasets. The source code is available at https://github.com/ RPIDIAL/SAMix.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A common challenge for deploying deep learning to clinical problems is the discrepancy between data distributions across different clinical sites <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. This discrepancy, which results from vendor or protocol differences, can cause a significant performance drop when models are deployed to a new site <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>. To solve this problem, many Unsupervised Domain Adaptation (UDA) methods <ref type="bibr" target="#b5">[6]</ref> have been developed for adapting a model to a new site with only unlabeled data (target domain) by transferring the knowledge learned from the original dataset (source domain). However, most UDA methods require sufficient target samples, which are scarce in medical imaging due to the limited accessibility to patient data. This motivates a new problem of Few-Shot Unsupervised Domain Adaptation (FSUDA), where only a few unlabeled target samples are available for training.</p><p>Few approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> have been proposed to tackle the problem of FSUDA. Luo et. al <ref type="bibr" target="#b10">[11]</ref> introduced Adversarial Style Mining (ASM), which uses a pretrained style-transfer module to generate augmented images via an adversarial process. However, this module requires extra style images <ref type="bibr" target="#b8">[9]</ref> for pre-training. Such images are scarce in clinical settings, and style differences across sites are subtle. This hampers the applicability of ASM to medical image analysis. SM-PPM <ref type="bibr" target="#b21">[22]</ref> trains a style-mixing model for semantic segmentation by augmenting source domain features to a fictitious domain through random interpolation with target domain features. However, SM-PPM is specifically designed for segmentation tasks and cannot be easily adapted to other tasks. Also, with limited target domain samples in FSUDA, the random feature interpolation is ineffective in improving the model's generalizability.</p><p>In a different direction, numerous UDA methods have shown high performance in various tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. However, their direct application to FSUDA can result in severe overfitting due to the limited target domain samples <ref type="bibr" target="#b21">[22]</ref>. Previous studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> have demonstrated that transferring the amplitude spectrum of target domain images to a source domain can effectively convey image style information and diversify training dataset. To tackle the overfitting issue of existing UDA methods, we propose a novel approach called Sensitivityguided Spectral Adversarial MixUp (SAMix) to augment training samples. This approach uses an adversarial mixing scheme and a spectral sensitivity map that reveals model generalizability weaknesses to generate hard-to-learn images with limited target samples efficiently. SAMix focuses on two key aspects. 1) Model generalizability weaknesses: Spectral sensitivity analysis methods have been applied in different works <ref type="bibr" target="#b25">[26]</ref> to quantify the model's spectral weaknesses to image amplitude corruptions. Zhang et al. <ref type="bibr" target="#b26">[27]</ref> demonstrated that using a spectral sensitivity map to weigh the amplitude perturbation is an effective data augmentation. However, existing sensitivity maps only use single-domain labeled data and cannot leverage target domain information. To this end, we introduce a Domain-Distance-modulated Spectral Sensitivity (DoDiSS) map to analyze the model's weaknesses in the target domain and guide our spectral augmentation. 2) Sample hardness: Existing studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref> have shown that mining hard-to-learn samples in model training can enhance the efficiency of data augmentation and improve model generalization performances. Therefore, to maximize the use of the limited target domain data, we incorporate an adversarial approach into the spectral mixing process to generate the most challenging data augmentations. This paper has three major contributions. 1) We propose SAMix, a novel approach for augmenting target-style samples by using an adversarial spectral mixing scheme. SAMix enables high-performance UDA methods to adapt easily to FSUDA problems. 2) We introduce DoDiSS to characterize a model's generalizability weaknesses in the target domain. 3) We conduct thorough empirical analyses to demonstrate the effectiveness and efficiency of SAMix as a plug-in module for various UDA methods across different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We denote the labeled source domain as X S = {(x s n , y s n )} N n=1 and the unlabeled K-shot target domain as <ref type="figure" target="#fig_0">1</ref> depicts the framework of our method as a plug-in module for boosting a UDA method in the FSUDA scenario. It contains two components. First, a Domain-Distancemodulated Spectral Sensitivity (DoDiSS) map is calculated to characterize a source model's weaknesses in generalizing to the target domain. Then, this sensitivity map is used for Sensitivity-guided Spectral Adversarial MixUp (SAMix) to generate target-style images for UDA models. The details of the components are presented in the following sections. </p><formula xml:id="formula_0">X T = {x t k } K k=1 , x s n , x t k ∈ R h×w . Figure</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Domain-Distance-Modulated Spectral Sensitivity (DoDiSS)</head><p>The prior research <ref type="bibr" target="#b26">[27]</ref> found that a spectral sensitivity map obtained using Fourier-based measurement of model sensitivity can effectively portray the generalizability of that model. However, the spectral sensitivity map is limited to single-domain scenarios and cannot integrate target domain information to assess model weaknesses under specific domain shifts. Thus, we introduce DoDiSS, extending the previous method by incorporating domain distance to tackle domain adaptation problems. Fig. <ref type="figure" target="#fig_0">1</ref> (a) depicts the DoDiSS pipeline. It begins by computing a domain distance map for identifying the amplitude distribution difference between the source and target domains in each frequency. Subsequently, this difference map is used for weighting amplitude perturbations when calculating the DoDiSS map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Distance Measurement.</head><p>To overcome the limitations of lacking target domain images, we first augment the few-shot images from the target domain with random combinations of various geometric transformations, including random cropping, rotation, flipping, and JigSaw <ref type="bibr" target="#b12">[13]</ref>. These transformations keep the image intensities unchanged, preserving the target domain style information. The Fast Fourier Transform (FFT) is then applied to all the source images and the augmented target domain images to obtain their amplitude spectrum, denoted as A S and ÂT , respectively. We calculate the probabilistic distributions p S i,j and p T i,j of A S and ÂT at the (i, j) th frequency entry, respectively. The domain distance map at (i, j) is defined as</p><formula xml:id="formula_1">D W (i, j) = W 1 (p S i,j , p T i,j )</formula><p>, where W 1 is the 1-Wasserstein distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DoDiSS Computation.</head><p>With the measured domain difference, we can now compute the DoDiSS map of a model. As shown in Fig. <ref type="figure" target="#fig_0">1 (a)</ref>, a Fourier basis is defined as a Hermitian matrix H i,j ∈ R h×w with only two non-zero elements at (i, j) and (-i, -j). A Fourier basis image U i,j can be obtained by 2 -normalized Inverse Fast Fourier Transform (IFFT) of A i,j , i.e., U i,j =</p><formula xml:id="formula_2">IFFT (A i,j ) ||IF F T (A i,j )||2 .</formula><p>To analyze the model's generalization weakness with respect to the frequency (i, j), we generate perturbed source domain images by adding the Fourier basis noise N i,j = r • D W (i, j) • U i,j to the original source domain image x s as x s + N i,j . D W (i, j) controls the 2 -norm of N i,j and r is randomly sampled to be either -1 or 1. The N i,j only introduces perturbations at the frequency components (i, j) to the original images. The D W (i, j) guarantees that images are perturbed across all frequency components following the real domain shift. For RGB images, we add N i,j to each channel independently following <ref type="bibr" target="#b26">[27]</ref>. The sensitivity at frequency (i, j) of a model F trained on the source domain is defined as the prediction error rate over the whole dataset X S as in <ref type="bibr" target="#b0">(1)</ref>, where Acc denotes the prediction accuracy</p><formula xml:id="formula_3">M S (i, j) = 1 - Acc (x s ,y s )∈X S (F (x s + r • D W (i, j) • U i,j ), y s ). (<label>1</label></formula><formula xml:id="formula_4">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sensitivity-Guided Spectral Adversarial Mixup (SAMix)</head><p>Using the DoDiSS map M S and an adversarially learned parameter λ * as a weighting factor, SAMix mixes the amplitude spectrum of each source image with the spectrum of a target image. DoDiSS indicates the spectral regions where the model is sensitive to the domain difference. The parameter λ * mines the heard-tolearn samples to efficiently enrich the target domain samples by maximizing the task loss. Further, by retaining the phase of the source image, SAMix preserves the semantic meaning of the original source image in the generated target-style sample. Specifically, as shown in Fig. <ref type="figure" target="#fig_0">1 (b)</ref>, given a source image x s and a target image x t , we compute their amplitude and phase spectrum, denoted as (A s , Φ s ) and (A t , Φ t ), respectively. SAMix mixes the amplitude spectrum by</p><formula xml:id="formula_5">A st λ * = λ * • M S • A t + (1 -λ * ) • (1 -M S ) • A s . (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>The target-style image is reconstructed by x st λ * = IFFT (A st λ * , Φ s ). The adversarially learned parameter λ * is optimized by maximizing the task loss L T using the projected gradient descent with T iterations and step size of δ:</p><formula xml:id="formula_7">λ * = arg max λ L T (F (x st λ ; θ), y), s.t. λ ∈ [0, 1].<label>(3)</label></formula><p>In the training phase, as shown in Fig. <ref type="figure" target="#fig_0">1</ref> (b), the SAMix module generates a batch of augmented images, which are combined with few-shot target domain images to train the UDA model. The overall training objective is to minimize</p><formula xml:id="formula_8">L tot (θ) = L T (F (x s ; θ), y) + μ • JS(F (x s ; θ), F (x st λ * ; θ)) + L UDA , (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>where L t is the supervised task loss in the source domain; JS is the Jensen-Shannon divergence <ref type="bibr" target="#b26">[27]</ref>, which regularizes the model predictions consistency between the source images x s and their augmented versions x st λ * ; L UDA is the training loss in the original UDA method, and μ is a weighting parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We evaluated SAMix on two medical image datasets. Fundus <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref> is an optic disc and cup segmentation task. Following <ref type="bibr" target="#b20">[21]</ref>, we consider images collected from different scanners as distinct domains. The source domain contains 400 images of the REFUGE <ref type="bibr" target="#b13">[14]</ref> training set. We took 400 images from the REFUGE validation set and 159 images of RIM-One <ref type="bibr" target="#b4">[5]</ref> to form the target domain 1 &amp; 2. We center crop and resize the disc region to 256 × 256 as network input. Camelyon <ref type="bibr" target="#b0">[1]</ref> is a tumor tissue binary classification task across 5 hospitals. We use the training set of Camelyon as the source domain (302, 436 images from hospitals 1 -3) and consider the validation set (34, 904 images from hospital 4) and test set (85, 054 images from the hospital 5) as the target domains 1 and 2, respectively. All the images are resized into 256 × 256 as network input. For all experiments, the source domain images are split into training and validation in the ratio of 4 : 1. We randomly selected K-shot target domain images for training, while the remaining target domain images were reserved for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>SAMix is evaluated as a plug-in module for four UDA models: AdaptSeg <ref type="bibr" target="#b16">[17]</ref> and Advent <ref type="bibr" target="#b17">[18]</ref> for Fundus, and SRDC <ref type="bibr" target="#b15">[16]</ref> and DALN <ref type="bibr" target="#b3">[4]</ref> for Camelyon. For a fair comparison, we adopted the same network architecture for all the methods on each task. For Fundus, we use a DeepLabV2-Res101 <ref type="bibr" target="#b2">[3]</ref> as the backbone with SGD optimizer for 80 epochs. The task loss L t is the Dice loss. The initial learning rate is 0.001, which decays by 0.1 for every 20 epochs. The batch size is 16. For Camelyon, we use a ResNet-50 <ref type="bibr" target="#b7">[8]</ref> with SGD optimizer for 20 epochs. L t is the binary cross-entropy loss. The initial learning rate is 0.0001, which decays by 0.1 every 5 epochs. The batch size is 128. We use the fixed weighting factor μ = 0.01, iterations T = 10, and step size δ = 0.1 in all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method Effectiveness</head><p>We demonstrate the effectiveness of SAMix by comparing it with two sets of baselines. First, we compare the performance of UDA models with and without SAMix. Second, we compare SAMix against other FSUDA methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Fundus. Table <ref type="table" target="#tab_0">1</ref> shows the 10-run average Dice coefficient (DSC) and Average Surface Distance (ASD) of all the methods trained with the source domain and 1-shot target domain image. The results are evaluated in the two target domains. Compared to the model trained solely on the source domain (Source only), the performance gain achieved by UDA methods (AdaptSeg and Advent) is limited. However, incorporating SAMix as a plug-in for UDA methods (Adapt-Seg+SAMix and Advent+SAMix) enhances the original UDA performance significantly (p &lt; 0.05). Moreover, SAMix+Advent surpasses the two FSUDA methods (ASM and SM-PPM) significantly. This improvement is primarily due to spectrally augmented target-style samples by SAMix.</p><p>To assess the functionality of the target-aware spectral sensitivity map in measuring the model's generalization performance on the target domain, we computed the DoDiSS maps of the four models (AdaptSeg, ASM, SM-PPM, and AdaptSeg+SAMix). The results are presented in Fig. <ref type="figure" target="#fig_1">2(a)</ref>. The DoDiSS map of AdaptSeg+SAMix demonstrates a clear suppression of sensitivity, leading to improved model performance. To better visualize the results, the model generalizability (average DSC) versus the averaged 1 -norm of the DoDiSS map is presented in Fig. <ref type="figure" target="#fig_1">2 (b)</ref>. The figure shows a clear trend of improved model performance as the averaged DoDiSS decreases. To assess the effectiveness of SAMix-augmented target-style images in bridging the gap of domain shift, the feature distributions of Fundus images before and after adaptation are visualized in Fig. <ref type="figure" target="#fig_1">2 (c</ref>) by t-SNE <ref type="bibr" target="#b11">[12]</ref>. Figure <ref type="figure" target="#fig_1">2</ref>(c1) shows the domain shift between the source and target domain features. The augmented samples from SAMix build the connection between the two domains with only a single example image from the target domain. Please note that, except the 1-shot sample, all the other target domain samples are used here for visualization only but never seen during training/validation. Incorporating these augmented samples in AdaptSeg merges the source and target distributions as in Fig. <ref type="figure" target="#fig_1">2</ref> (c2).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Efficiency</head><p>As the availability of target domain images is limited, data efficiency plays a crucial role in determining the data augmentation performance. Therefore, we evaluated the model's performance with varying numbers of target domain images in the training process.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>To assess the efficacy of the components in SAMix, we conducted an ablation study with AdaptSeg+SAMix and DALN+SAMix (Full model) on Fundus and Camelyon datasets. This was done by 1) replacing our proposed DoDiSS map with the original one in <ref type="bibr" target="#b26">[27]</ref> (Original map); 2) replacing the SAMix module with the random spectral swapping (FDA, β = 0.01, 0.09) in <ref type="bibr" target="#b24">[25]</ref>; 3) removing the three major components (No L UDA , No SAMix, No JS) in a leave-one-out manner. Figure <ref type="figure" target="#fig_5">5</ref> suggests that, compared with the Full model, the model performance degrades when the proposed components are either removed or replaced by previous methods, which indicates the efficacy of the SAMix components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>This paper introduces a novel approach, Sensitivity-guided Spectral Adversarial MixUp (SAMix), which utilizes an adversarial mixing scheme and a spectral sensitivity map to generate target-style samples effectively. The proposed method facilitates the adaptation of existing UDA methods in the few-shot scenario. Thorough empirical analyses demonstrate the effectiveness and efficiency of SAMix as a plug-in module for various UDA methods across multiple tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the proposed framework. (a) DoDiSS map characterizes a model's generalizability weaknesses. (b) SAMix enables UDA methods to solve FSUDA.</figDesc><graphic coords="3,41,79,285,59,339,46,162,46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Method effectiveness analysis. (a) The DoDiSS maps visualization; (b) Scattering plot of model generalizability v.s. sensitivity; (c) Feature space visualization.</figDesc><graphic coords="7,43,29,60,23,337,87,74,47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Data efficiency of FSUDA methods on (a) Fundus and (b) Camelyon.</figDesc><graphic coords="8,75,03,54,65,321,01,78,79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>(a) and (b) illustrate the domain adaptation results on Fundus and Camelyon (both in target domain 1), respectively. Our method consistently outperforms other baselines with just a 1-shot target image for training. Furthermore, we qualitatively showcase the data efficiency of SAMix.Figure 4 (a) displays the generated image of SAMix given the target domain image. While maintaining the retinal structure of the source image, the augmented images exhibit a more similar style to the target image, indicating SAMix can effectively transfer the target domain style. Figure 4 (b) shows an example case of the segmented results. Compared with other baselines, the SAMix segmentation presents much less prediction error, especially in the cup region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) SAMix generated samples. (b) Case study of the Fundus segmentation.</figDesc><graphic coords="8,75,30,485,45,283,30,53,17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Ablation study. (a) Average DSC on Fundus. (b) AUC on Camelyon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>10-run average DSC (%) and ASD of models on REFUGE. The best performance is in bold and the second best is indicated with underline. 66.54 * 63.85 * 14.37 * 11.69 * 13.03 * 55.77 * 58.62 * 57.20 * 20.95 * 17.63 * 19.30 * AdaptSeg 61.45 * 66.61 * 64.03 * 13.79 * 11.47 * 12.64 * 56.67 * 60.50 * 58.59 * 20.44 * 17.97 * 19.21 * Advent 62.03 * 66.82 * 64.43 * 12.82 * 11.54 * 12.18 * 56.43 * 60.56 * 58.50 * 20.31 * 17.86 * 19.09 * ASM 69.18 * 71.91 * 70.05 * 8.92 * 8.35 * 8.64 * 57.79 * 61.86 * 59.83 * 19.26 * 16.94 * 18.10 * SM-PPM 74.55 * 77.62 * 76.09 * 6.09 * 5.66 * 5.88 * 59.62 * 64.17 * 61.90 * 14.52 * 12.22 * 13.37 *</figDesc><table><row><cell>Method</cell><cell cols="5">Source Domain → Target Domain 1</cell><cell></cell><cell cols="5">Source Domain → Target Domain 2</cell></row><row><cell></cell><cell cols="2">DSC (↑)</cell><cell></cell><cell cols="2">ASD (↓)</cell><cell></cell><cell cols="2">DSC (↑)</cell><cell></cell><cell cols="2">ASD (↓)</cell></row><row><cell></cell><cell>cup</cell><cell>disc</cell><cell>avg</cell><cell>cup</cell><cell>disc</cell><cell>avg</cell><cell>cup</cell><cell>disc</cell><cell>avg</cell><cell>cup</cell><cell>disc</cell><cell>avg</cell></row><row><cell cols="5">Source Only 61.16  AdaptSeg+SAMix 76.56 80.57 78.57 4.97</cell><cell>4.12</cell><cell>4.55</cell><cell cols="6">61.75 66.20 63.98 12.75 11.09 11.92</cell></row><row><cell>Advent+SAMix</cell><cell cols="6">76.32 80.64 78.48 4.90 3.98 4.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>*  </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>44 62.02 66.35 64.19 11.97 10.85 11.41</head><label></label><figDesc>* p &lt; 0.05 in the one-tailed paired t-test with Advent+SAMix.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>10-run average Acc (%) and AUC (%) of models on Camelyon. The best performance is in bold and the second best is indicated with underline.</figDesc><table><row><cell>Method</cell><cell cols="4">Source Domain → Target Domain 1 Source Domain → Target Domain 2</cell></row><row><cell></cell><cell cols="2">Acc (↑) AUC (↑)</cell><cell cols="2">Acc (↑) AUC (↑)</cell></row><row><cell>Source Only</cell><cell cols="2">75.42  *  71.67  *</cell><cell cols="2">65.55  *  60.18  *</cell></row><row><cell>DALN</cell><cell cols="2">78.63  *  74.74  *</cell><cell cols="2">62.57  *  56.44  *</cell></row><row><cell>ASM</cell><cell cols="2">83.66  *  80.43</cell><cell cols="2">77.75  *  73.47  *</cell></row><row><cell cols="2">SRDC+SAMix 84.28</cell><cell>80.05</cell><cell>78.64</cell><cell>74.62</cell></row><row><cell cols="2">DALN+SAMix 86</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>.41 82.58 80.84 75.90</head><label></label><figDesc>* p &lt; 0.05 in the one-tailed paired t-test with DALN+SAMix. The evaluation results of the 10-run average accuracy (Acc) and Area Under the receiver operating Curve (AUC) of all methods trained with 1shot target domain image are presented in Table2. The clustering-based SRDC is not included in the table, as the model crashed in this few-shot scenario. Also, the SM-PPM is not included because it is specifically designed for segmentation tasks. The results suggest that combining SAMix with UDA not only enhances the original UDA performance but also significantly outperforms other FSUDA methods.</figDesc><table /><note><p>Camelyon.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This research was partially supported by the <rs type="funder">National Science Foundation (NSF)</rs> under the <rs type="grantName">CAREER award OAC 2046708</rs>, the <rs type="funder">National Institutes of Health (NIH)</rs> under award <rs type="grantNumber">R21EB028001</rs>, and the <rs type="institution">Rensselaer-IBM AI Research Collaboration of the IBM AI Horizons Network</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ScN4W22">
					<orgName type="grant-name">CAREER award OAC 2046708</orgName>
				</org>
				<org type="funding" xml:id="_rZYp6NR">
					<idno type="grant-number">R21EB028001</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Synergistic image and feature adaptation: Towards cross-modality domain adaptation for medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="865" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reusing the task-specific classifier as a discriminator: discriminator-free adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7181" to="7190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rimone: an open retinal image database for optic nerve evaluation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fumero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alayón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sigut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gonzalez-Hernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 24th International Symposium on Computer-Based Medical Systems (CBMS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation for medical image analysis: a survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1173" to="1185" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image phase or amplitude? Rapid scene categorization is an amplitude-based process</title>
		<author>
			<persName><forename type="first">N</forename><surname>Guyader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chauvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peyrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hérault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marendaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">C.R. Biol</title>
		<imprint>
			<biblScope unit="volume">327</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="313" to="318" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feddg: federated domain generalization on medical image segmentation via episodic learning in continuous frequency space</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1013" to="1023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial style mining for one-shot unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20612" to="20623" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46466-45" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9910</biblScope>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Refuge challenge: a unified framework for evaluating automated methods for glaucoma assessment from fundus photographs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Orlando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">101570</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation via structurally regularized deep clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8725" to="8735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Advent: adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">AugMax: adversarial composition of random augmentations for robust training</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="237" to="250" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains: a survey on domain generalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dofe: domain-oriented feature embedding for generalizable fundus image segmentation on unseen datasets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Style mixing and patchwise prototypical matching for one-shot unsupervised domain adaptive semantic segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2740" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for medical image segmentation by disentanglement learning and self-training</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2022.3192303</idno>
		<ptr target="https://doi.org/10.1109/TMI.2022.3192303" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Fourier-based framework for domain generalization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14383" to="14392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FDA: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Fourier perspective on model robustness in computer vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gontijo Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">When neural networks fail to generalize? a model sensitivity perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Task-oriented low-dose CT image denoising</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-1" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page">43</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward adversarial robustness in unlabeled target domains</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2023.3242141</idno>
		<ptr target="https://doi.org/10.1109/TIP.2023.3242141" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1272" to="1284" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
