<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometry-Invariant Abnormality Detection</title>
				<funder ref="#_WMz52zA">
					<orgName type="full">Wellcome</orgName>
				</funder>
				<funder ref="#_xbErzfN">
					<orgName type="full">Wellcome/ EPSRC Centre for Medical Engineering</orgName>
				</funder>
				<funder>
					<orgName type="full">GE Healthcare</orgName>
				</funder>
				<funder ref="#_JXBYezq">
					<orgName type="full">King&apos;s College London</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Ashay</forename><surname>Patel</surname></persName>
							<email>ashay.patel@kcl.ac.uk</email>
							<idno type="ORCID">0000-0003-4212-2578</idno>
							<affiliation key="aff0">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<postCode>WC2R 2LS</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Petru-Daniel</forename><surname>Tudosiu</surname></persName>
							<idno type="ORCID">0000-0001-6435-5079</idno>
							<affiliation key="aff0">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<postCode>WC2R 2LS</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Walter</forename><forename type="middle">Hugo Lopez</forename><surname>Pinaya</surname></persName>
							<idno type="ORCID">0000-0003-3739-1087</idno>
						</author>
						<author>
							<persName><forename type="first">Olusola</forename><surname>Adeleke</surname></persName>
							<idno type="ORCID">0000-0002-4388-8754</idno>
							<affiliation key="aff0">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<postCode>WC2R 2LS</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gary</forename><surname>Cook</surname></persName>
							<idno type="ORCID">0000-0002-8732-8134</idno>
							<affiliation key="aff0">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<postCode>WC2R 2LS</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vicky</forename><surname>Goh</surname></persName>
							<idno type="ORCID">0000-0002-2321-8091</idno>
							<affiliation key="aff0">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<postCode>WC2R 2LS</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastien</forename><surname>Ourselin</surname></persName>
							<idno type="ORCID">0000-0002-5694-5340</idno>
							<affiliation key="aff0">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<postCode>WC2R 2LS</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">Jorge</forename><surname>Cardoso</surname></persName>
							<idno type="ORCID">0000-0003-1284-2558</idno>
							<affiliation key="aff0">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<postCode>WC2R 2LS</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Geometry-Invariant Abnormality Detection</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="300" to="309"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">CAA5C7FF0586CFDDE8CF1C6C7F37DD0F</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_29</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cancer is a highly heterogeneous condition best visualised in positron emission tomography. Due to this heterogeneity, a generalpurpose cancer detection model can be built using unsupervised learning anomaly detection models. While prior work in this field has showcased the efficacy of abnormality detection methods (e.g. Transformer-based), these have shown significant vulnerabilities to differences in data geometry. Changes in image resolution or observed field of view can result in inaccurate predictions, even with significant data pre-processing and augmentation. We propose a new spatial conditioning mechanism that enables models to adapt and learn from varying data geometries, and apply it to a state-of-the-art Vector-Quantized Variational Autoencoder + Transformer abnormality detection model. We showcase that this spatial conditioning mechanism statistically-significantly improves model performance on whole-body data compared to the same model without conditioning, while allowing the model to perform inference at varying data geometries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The use of machine learning for anomaly detection in medical imaging analysis has gained a great deal of traction over previous years. Most recent approaches have focused on improvements in performance rather than flexibility, thus limiting approaches to specific input types -little research has been carried out to generate models unhindered by variations in data geometries. Often, research assumes certain similarities in data acquisition parameters, from image dimensions to voxel dimensions and fields-of-view (FOV). These restrictions are then carried forward during inference <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25]</ref>. This strong assumption can often be complex to maintain in the real-world and although image pre-processing steps can mitigate some of this complexity, test error often largely increases as new data variations arise. This can include variances in scanner quality and resolution, in addition to the FOV selected during patient scans. Usually training data, especially when acquired from differing sources, undergoes significant preprocessing such that data showcases the same FOV and has the same input dimensions, e.g. by registering data to a population atlas. Whilst making the model design simpler, these pre-processing approaches can result in poor generalisation in addition to adding significant pre-processing times <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref>. Given this, the task of generating an anomaly detection model that works on inputs with a varying resolution, dimension and FOV is a topic of importance and the main focus of this research.</p><p>Unsupervised methods have become an increasingly prominent field for automatic anomaly detection by eliminating the necessity of acquiring accurately labelled data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref> therefore relaxing the stringent data requirements of medical imaging. This approach consists of training generative models on healthy data, and defining anomalies as deviations from the defined model of normality during inference. Until recently, the variational autoencoder (VAE) and its variants held the state-of-the-art for the unsupervised approach. However, novel unsupervised anomaly detectors based on autoregressive Transformers coupled with Vector-Quantized Variational Autoencoders (VQ-VAE) have overcome issues associated with autoencoder-only methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. In <ref type="bibr" target="#b21">[22]</ref>, the authors explore the advantage of tractably maximizing the likelihood of the normal data to model the long-range dependencies of the training data. The work in <ref type="bibr" target="#b20">[21]</ref> takes this method a step further through multiple samplings from the Transformer to generate a non-parametric Kernel Density Estimation (KDE) anomaly map.</p><p>Even though these methods are state-of-the-art, they have stringent data requirements, such as having a consistent geometry of the input data, e.g., in a whole-body imaging scenario, it is not possible to crop a region of interest and feed it to the algorithm, as this cropped region will be wrongly detected as an anomaly. This would happen even in the case that a scan's original FOV was restricted <ref type="bibr" target="#b16">[17]</ref>.</p><p>As such, we propose a geometric-invariant approach to anomaly detection, and apply it to cancer detection in whole-body PET via an unsupervised anomaly detection method with minimal spatial labelling. Through adapting the VQ-VAE Transformer approach in <ref type="bibr" target="#b20">[21]</ref>, we showcase that we can train our model on data with varying fields of view, orientations and resolutions by adding spatial conditioning in both the VQ-VAE and Transformer. Furthermore, we show that the performance of our model with spatial conditioning is at least equivalent to, and sometimes better, than a model trained on whole-body data in all testing scenarios, with the added flexibility of a "one model fits all data" approach. We greatly reduce the pre-processing requirements for generating a model (as visualised in Fig. <ref type="figure" target="#fig_0">1</ref>), demonstrating the potential use cases of our model in more flexible environments with no compromises on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>The main building blocks behind the proposed method are introduced below. Specifically, a VQ-VAE plus a Transformer are jointly used to learn the probability density function of 3D PET images as explored in prior research <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vector-Quantized Variational Autoencoder</head><p>The VQ-VAE model provides a data-efficient encoding mechanism-enabling 3D inputs at their original resolution-while generating a discrete latent representation that can trivially be learned by a Transformer network <ref type="bibr" target="#b19">[20]</ref>. The VQ-VAE is composed of an encoder that maps an image X ∈ R H×W ×D onto a compressed latent representation Z ∈ R h×w×d×nz where n z is the latent embedding vector dimension. Z is then passed through a quantization block where each feature column vector is mapped to its nearest codebook vector. Each spatial code Z ijl ∈ R nz is then replaced by its nearest codebook element e k ∈ R nz , k ∈ 1, ..., K where K denotes the codebook vocabulary size, thus obtaining Z q . Given Z q , the VQ-VAE decoder then reconstructs the observations X ∈ R H×W ×D . The architecture used for the VQ-VAE model used an encoder consisting of three downsampling layers that contain a convolution with stride 2 and kernel size 4 followed by a ReLU activation and 3 residual blocks. Each residual block consists of a kernel of size 3, followed by a ReLU activation, a convolution of kernel size 1 and another ReLU activation. Similar to the encoder, the decoder has 3 layers of 3 residual blocks, each followed by a transposed convolutional layer with stride 2 and kernel size 4. Finally, before the last transposed convolutional layer, a Dropout layer with a probability of 0.05 is added. The VQ-VAE codebook used had 256 atomic elements (vocabulary size), each of length 128. The CT VQ-VAE was identical in hyperparameters except each codebook vector has length 64. See Appendix A for implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer</head><p>After training a VQ-VAE model, the next stage is to learn the probability density function of the discrete latent representations. Using the VQ-VAE, we can obtain a discrete representation of the latent space by replacing the codebook elements in Z q with their respective indices in the codebook yielding Z iq . To model the imaging data, we require the discretized latent space Z iq to take the form of a 1D sequence s, which we achieve via a raster scan of the latent. The Transformer is then trained to maximize the log-likelihoods of the latent tokens sequence in an autoregressive manner. By doing this, the Transformer can learn the codebook distribution for position i within s with respect to previous codes p(s i ) = p(s i |s &lt;i ). As with <ref type="bibr" target="#b20">[21]</ref>, we additionally use CT data to condition the Transformer via cross-attention using a separate VQ-VAE to encode the CT. This transforms the problem to learning the codebook distribution at position i as p(s i ) = p(s i |s &lt;i , c) where c is the entire CT latent sequence. The performer used in this work corresponds to a decoder Transformer architecture with 14 layers, each with 8 heads, and an embedding dimension of 256. Similarly the embedding dimension for the CT data and the spatial conditioning data had an embedding dimension of 256. See Appendix B for implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Anomaly Detection via Kernel Density Estimation Maps</head><p>Building on <ref type="bibr" target="#b20">[21]</ref>, given a sample for inference, a tokenized representation Z iq is extracted from the VQ-VAE. Then, the representation is flattened into s where the trained Transformer model obtains the likelihoods for each token. These inferred likelihoods represent the probability of each token appearing at a certain position in the sequence -p(s i ) = p(s i |s &lt;i , c). This can then be used to single out tokens with low probability, i.e. anomalous tokens. We then resample anomalous tokens p(s i ) &lt; t where t is the resampling threshold chosen empirically using the validation set performance. Anomalous tokens are then replaced with higher likelihood (normal) tokens by resampling from the Transformer. We can then reshape the "healed" sequence back into its 3D quantized representation to feed into the VQ-VAE to generate a healed reconstruction X r without anomalies.</p><p>In this work, abnormalities are defined as deviations between the distribution of "healed" reconstructions and the observed data, measured using a Kernel Density Estimation (KDE) approach. We generate multiple healed latent sequences by sampling multiple times for each position i with a likelihood p(s i ) &lt; t. In each resampling, the Transformer outputs the likelihood for every possible token at position i. Based on these probabilities, we can create a multinomial distribution showcasing the probability of each token. We can then randomly sample multiple tokens. Each of these healed latent spaces is then decoded via the VQ-VAE multiple times with dropout. This generates multiple healed representations of the original image. A voxel-wise KDE anomaly map is generated by fitting a KDE independently at each voxel position to estimate the probability density function f across reconstructions. This is then scored at the original intensity of that voxel in the scan. Our KDE implementation used 60 samples for each anomalous token in s, followed by five decodings with dropout, yielding 300 "healed" reconstructions that are then used to calculate the KDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">VQ-VAE Spatial Conditioning</head><p>To date, there has been little research on generating autoencoder models capable of using images of varying sizes and resolutions (i.e. the input tensor shape to a autoencoder is assumed to be fixed). Although fully convolutional models can ingest images of varying dimensions, we have found that using training data with varying resolutions resulted in poor auto-encoder reconstructions. In this work, we take inspiration from CoordConv <ref type="bibr" target="#b18">[19]</ref> as a mechanism to account for some level of spatial awareness, an approach which has been applied to various tasks in medical imaging scenarios with ranging levels of success <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>A CoordConv layer is a concatenation of channels to the input image referencing a predefined coordinate system. After concatenation, the input is simply fed through a standard convolutional layer. For a 3D scan, we would have 3 coordinates, ijk, where the i coordinate channel is an h × w × d rank-1 matrix with its first row filled with 0's, its second row with 1's, and so on. This would be the same for the j coordinate channel, except the columns would be filled with constant values, not the rows, and likewise for the k coordinate channel in a depth-wise fashion. These channels are then normalised between [0, 1].</p><p>The advantage of the CoordConv implementation is the constant scale of 0-1 across the channels regardless of image resolution. For example, two wholebody images with large differences in voxel-size will have CoordConv channels from 0-1 along each axis, thus conveying the notion of spatial resolution to the network. We found when training the VQ-VAE model on data with varying resolutions and dimensions that reconstructions showcased unwanted and significant artifacts, while by adding the CoordConv channels this issue was not present (See Appendix C for examples). Furthermore, when dealing with images of a ranging FOV, we adapted the [0, 1] channel values to convey the image's FOV. For example, suppose a whole body image (neck to upper leg) represented our range [0, 1] where 0 is the upper leg, and 1 is the neck. In that case, we can contract this range to represent the area displayed in the image (Fig. <ref type="figure" target="#fig_1">2</ref>). In doing so, we convey information about the FOV to the VQ-VAE through CoordConv layers. Note that while the proposed model assumes only translation and scale changes between samples, it can be trivially extended to a full affine mapping of the coordinate system (including rotations/shearing between samples).</p><p>We used random crops during training to simulate varying FOVs of wholebody data. The random crop parameters are then used to define the coordinate system. For the implementation of the CoordConv layer, these channels are added once to the original input image and at the beginning of the VQ-VAE decoder, concatenated to the latent space, using the same value ranges but at a lower resolution given the reduced spatial dimension of the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformer Spatial Conditioning</head><p>Numerous approaches have used Transformers in the visual domain <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Given that Transformers work natively on 1D sequences, the spatial information in images is often lost. While various works have aimed to convey the spatial information of the original image when projected onto a 1D sequence <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b27">28]</ref>, we require our spatial positioning to encode both where in the image ordering a token belongs, and where the token belongs in the context of the whole body. As the images have different FOVs and the image resolution, this results in To do this, we use the same CoordConv principle applied to the input fed to the VQ-VAE. In order to map image coordinates to the token latent representation, we apply average pooling to each CoordConv channel separately, with kernel size and stride equal to the downsampling used in the VQ-VAE (8 used in this research). This gives us three channels i, j, k in the range of [0, 1], the same dimension as our latent space, but at lower spatial resolution to the original input. We then bin each value in each channel and combine the three values using base notation. For example, we use 20 bins (equal bins of 0.05), to which the final quantized spatial value for a given token is given as</p><formula xml:id="formula_0">sp ijk = b i + b j × B + b k * B 2</formula><p>where sp is the quantized spatial value allocated to a given token at position ijk in the latent space, and b represents the binned value along a given channel for that token, and B is a pre-defined bin size. The choice of B = 20 bins was empirically chosen to closely resemble the average latent dimension of images.</p><p>During training, whole-body images and random crops are used. The spatial conditioning tokens are then generated and fed through an embedding layer of equal dimension to the CT embedding. The two embedded sequences (CT and spatial) are then added together and fed to the Transformer via cross-attention. For reference, this mechanism can be visualised in Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data</head><p>For this work we leveraged whole-body PET/CT data from different sources to explore the efficacy of our approach for varying image geometries. 211 scans from NSCLC Radiogenomics <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref> combined with 83 scans from a proprietary dataset constitute our lower resolution dataset with voxel dimensions of 3.6 × 3.6×3 mm. From this, we split the data to give 210 training samples, 34 validation and 50 testing. Our higher resolution dataset uses AutoPET <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref> (1014 scans) with voxel dimensions of 2.036 × 2.036 × 3 mm. From this, 850 scans are used for training, 64 for validation and 100 for testing.</p><p>All baseline models work in a single space with constant dimensions, obtained by registering the AutoPET images to the space of the NSCLC dataset.</p><p>For evaluation, we use four testing sets: a lower resolution set derived from both the NSCLC and the private dataset; a higher resolution set from AutoPET; a testing set with random crops of the same NSCLC/private testing dataset and finally a testing set that has been rotated through 90 • using the high resolution testing data. As the cropped and rotated dataset cannot be fed into the baseline models, we pad the images to the common image sizing before inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The proposed model was trained on the data described in Sect. 3.3, with random crops applied while training. Model and anomaly detection hyperparameter tuning was done on our validation samples using the best DICE scores. We then test our model and baselines on 4 hold-out test sets: a low-resolution whole-body set, a low-resolution cropped set, a high-resolution rotated set and a high-resolution test set of PET images with varying cancers. The visual results shown in Fig. <ref type="figure" target="#fig_3">4</ref> show outputs rotated back to the original orientation. We measure our models'  performance using the DICE score, obtained by thresholding the residual/density score maps. In addition, we calculate the area under the precision-recall curve (AUPRC) as a suitable measure for segmentation performance under class imbalance. We additionally showcase the performance of the classic VQ-VAE + Transformer approach trained on whole-body data only (without the proposed spatial conditioning), as well as the proposed CoordConv model trained with varying image geometries but without the transformer spatial conditioning to explicitly showcase the added contribution of both spatial conditionings. The full results are presented in Table <ref type="table" target="#tab_0">1</ref> with visual examples shown in Fig. <ref type="figure" target="#fig_3">4</ref>. We can observe that the addition of spatial conditioning improves performance even against the same model without conditioning trained on whole-body data (Mann Whitney U test, P &lt; 0.01 on high resolution and P &lt; 0.001 on cropped data for DICE and AUPRC). For cropped data, models trained on whole-body data fail around cropping borders, as showcased in Fig. <ref type="figure" target="#fig_3">4</ref>. This is not the case for the models trained on varying geometries. Note that the VQ-VAE + Transformer trained on varying geometries still shows adequate performance, highlighting the resilience of the Transformer network to varying sequence lengths without any form of spatial conditioning. However, by adding the transformer spatial conditioning, we see improvements across all test sets (most significantly on cropped data and the rotated data P &lt; 0.001) for both evaluation metrics. For the rotated data, we see little performance degradation in the conditioned model thanks to the spatial conditioning. The same model without conditioning showed much lower performance with higher false positives likely due to the model's inability to comprehend the anatomical structures present due to the rotated orientation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Detection and segmentation of anomalous regions, particularly for cancer patients, is essential for staging, treatment and intervention planning. Generally, the variation scanners and acquisition protocols can cause failures in models trained on data from single sources. In this study, we proposed a system for anomaly detection that is robust to variances in geometry. Not only does the proposed model showcase strong and statistically-significant performance improvements on varying image resolutions and FOV, but also on whole-body data. Through this, we demonstrate that one can improve the adaptability and flexibility to varying data geometries while also improving performance. Such flexibility also increases the pool of potential training data, as they dont require the same FOV. We hope this work serves as a foundation for further exploration into geometry-invariant deep-learning methods for medical-imaging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Flowchart showcasing traditional data pipelines for developing machine learning models in medical imaging (top) vs. the reduced pipeline for our approach (bottom)</figDesc><graphic coords="3,71,79,54,17,280,45,75,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. CoordConv example showing whole-body image with values from 0 to 1 vs. a cropped image with values from 0.2 to 0.7 to reflect the field of view</figDesc><graphic coords="6,85,47,54,08,281,83,74,62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Pipeline for Transformer training. PET and CT are encoded to generate a discrete latent space. CoordConv layers are used to generate the spatial conditionings that are added to the CT conditioning and fed to the Transformer via cross-attention</figDesc><graphic coords="7,70,29,53,87,283,51,142,84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Columns display (1st) the input image; (2nd) the gold standard segmentation; (3rd) residual for the VAE, (4th) AE Spatial, (5th) a KDE anomaly map for VQ-VAE Transformer trained on the whole body, (6th) trained with varied geometries, (7th) with spatial conditioning. Results are provided for a random subject in each test set.</figDesc><graphic coords="8,71,46,53,99,309,64,175,72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Anomaly detection results with best achievable DICE-score ( DICE ) and AUPRC on test sets. Bold values indicate best performing model with underlined values showcasing statistically significant results to the next best alternative P &lt; 0.05 ± 0.18 0.42 ± 0.19 0.31 ± 0.15 0.20 ± 0.11 0.26 ± 0.15 0.40 ± 0.21 0.31 ± 0.18 0.19 ± 0.09 VQ-VAE + Transformer [21] 0.57 ± 0.07 0.65 ± 0.10 0.59 ± 0.10 0.31 ± 0.16 0.55 ± 0.09 0.64 ± 0.11 0.57 ± 0.10 0.29 ± 0.13</figDesc><table><row><cell>Model</cell><cell>DICE</cell><cell></cell><cell></cell><cell></cell><cell>AUPRC</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Whole Body</cell><cell>Low Res</cell><cell>High Res</cell><cell>Cropped</cell><cell>Rotated</cell><cell>Low Res</cell><cell>High Res</cell><cell>Cropped</cell><cell>Rotated</cell></row><row><cell>AE Dense [4]</cell><cell cols="8">0.22 ± 0.15 0.25 ± 0.17 0.30 ± 0.19 0.25 ± 0.19 0.18 ± 0.12 0.26 ± 0.16 0.23 ± 0.14 0.23 ± 0.13</cell></row><row><cell>AE Spatial [4]</cell><cell cols="8">0.32 ± 0.13 0.48 ± 0.21 0.34 ± 0.16 0.14 ± 0.08 0.26 ± 0.12 0.45 ± 0.20 0.33 ± 0.14 0.10 ± 0.07</cell></row><row><cell>AE SSIM [6]</cell><cell cols="8">0.28 ± 0.16 0.30 ± 0.19 0.27 ± 0.17 0.18 ± 0.07 0.20 ± 0.15 0.26 ± 0.18 0.21 ± 0.12 0.15 ± 0.09</cell></row><row><cell>VAE [4]</cell><cell cols="8">0.35 ± 0.19 0.48 ± 0.22 0.34 ± 0.21 0.19 ± 0.08 0.33 ± 0.18 0.45 ± 0.20 0.35 ± 0.17 0.18 ± 0.09</cell></row><row><cell cols="2">F-Anogan [23] 0.30 Geometry-Invariant (proposed)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>VQ-VAE CoordConv</p>0.57 ± 0.09 0.65 ± 0.08 0.63 ± 0.12 0.32 ± 0.17 0.55 ± 0.09 0.64 ± 0.09 0.61 ± 0.13 0.30 ± 0.15 Full CoordConv 0.58 ± 0.08 0.68 ± 0.10 0.67 ± 0.10 0.65 ± 0.12 0.56 ± 0.09 0.66 ± 0.11 0.64 ± 0.11 0.62 ± 0.12</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This research was supported by <rs type="funder">Wellcome/ EPSRC Centre for Medical Engineering</rs> (<rs type="grantNumber">WT203148/Z/16/Z</rs>), <rs type="funder">Wellcome</rs> <rs type="grantName">Flagship Programme</rs> (<rs type="grantNumber">WT213038/Z/18/Z</rs>), <rs type="institution">The London AI Centre for Value-based Heathcare</rs> and <rs type="funder">GE Healthcare</rs>. The models were trained on the NVIDIA Cambridge-1. Private dataset was obtained through <rs type="funder">King's College London</rs> (<rs type="grantNumber">14/LO/0220</rs> ethics application number).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xbErzfN">
					<idno type="grant-number">WT203148/Z/16/Z</idno>
				</org>
				<org type="funding" xml:id="_WMz52zA">
					<idno type="grant-number">WT213038/Z/18/Z</idno>
					<orgName type="grant-name">Flagship Programme</orgName>
				</org>
				<org type="funding" xml:id="_JXBYezq">
					<idno type="grant-number">14/LO/0220</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_29.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Part affinity fields and CoordConv for detecting landmarks of lumbar vertebrae and sacrum in X-ray images</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.3390/s22228628</idno>
		<ptr target="https://doi.org/10.3390/s22228628" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">8628</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bakr</surname></persName>
		</author>
		<title level="m">Data for NSCLC radiogenomics collection</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A radiogenomic dataset of non-small cell lung cancer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakr</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2018.202</idno>
		<ptr target="https://doi.org/10.1038/sdata.2018.202" />
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180202</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autoencoders for unsupervised anomaly segmentation in brain MR images: a comparative study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Denner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101952</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2006/file/b1" />
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
	<note>b0432ceafb0ce714426e9114852ac7-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">MVTec AD -a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00982</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00982" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="9584" to="9592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heewoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Generative pretraining from pixels</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Cancer Imaging Archive (TCIA): maintaining and operating a public information repository</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10278-013-9622-7</idno>
		<ptr target="https://doi.org/10.1007/s10278-013-9622-7" />
	</analytic>
	<monogr>
		<title level="j">J. Digit. Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1045" to="1057" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Artificial intelligence with deep learning in nuclear medicine and radiology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Decuyper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Holen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vandenberghe</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40658-021-00426-y</idno>
		<ptr target="https://doi.org/10.1186/s40658-021-00426-y" />
	</analytic>
	<monogr>
		<title level="j">EJNMMI Phys</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Jukebox: a generative model for music</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Challenges for machine learning in clinical translation of big data imaging studies</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Dinsdale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bluemke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Namburete</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2022.09.012</idno>
		<ptr target="https://doi.org/10.1016/j.neuron.2022.09.012" />
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="3866" to="3881" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An image is worth 16 × 16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A whole-body FDG-PET/CT Dataset with manually annotated Tumor Lesions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gatidis</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-022-01718-3</idno>
		<idno>41597- 022-01718-3</idno>
		<ptr target="https://doi.org/10.1038/s" />
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">601</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Non-small cell lung cancer: identifying prognostic imaging biomarkers by leveraging public gene expression microarray data-methods and preliminary results</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gevaert</surname></persName>
		</author>
		<idno type="DOI">10.1148/radiol.12111607</idno>
		<ptr target="https://doi.org/10.1148/radiol.12111607" />
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page" from="387" to="396" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Transformer-based out-of-distribution detection for clinically safe segmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CoordConv-Unet: investigating CoordConv for organ segmentation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Jurdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Abdallah</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.irbm.2021.03.002</idno>
		<ptr target="https://doi.org/10.1016/j.irbm.2021.03.002" />
	</analytic>
	<monogr>
		<title level="j">IRBM</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="415" to="423" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">An intriguing failing of convolutional neural networks and the Coord-Conv solution</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Neural discrete representation learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross attention transformers for multi-modal unsupervised wholebody pet anomaly detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-18576-2_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-18576-2_2" />
	</analytic>
	<monogr>
		<title level="m">DGM4MICCAI 2022</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Oksuz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13609</biblScope>
			<biblScope unit="page" from="14" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised brain anomaly detection and segmentation with transformers</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H L</forename><surname>Pinaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-59050-9_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-59050-9_12" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10265</biblScope>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Morphology-preserving autoregressive 3D generative modelling of the brain</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Tudosiu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16980-9_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16980-9_7" />
	</analytic>
	<monogr>
		<title level="m">SASHIMI 2022</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Svoboda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolterink</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Escobar</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13570</biblScope>
			<biblScope unit="page" from="66" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A theory of the learnable</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1134" to="1142" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Machine learning for medical imaging: methodological failures and recommendations for the future</title>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheplygina</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-022-00592-y</idno>
		<ptr target="https://doi.org/10.1038/s41746-022-00592-y" />
	</analytic>
	<monogr>
		<title level="j">NPJ Digit. Med</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Rethinking and improving relative position encoding for vision transformer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
