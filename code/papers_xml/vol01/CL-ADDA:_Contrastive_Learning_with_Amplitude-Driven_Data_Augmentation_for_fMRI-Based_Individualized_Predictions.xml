<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiangcong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Information Technology</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Beijing Key Laboratory of Traffic Data Analysis and Mining</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Information Technology</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Guan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Information Technology</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Information Technology</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Lixia</forename><surname>Tian</surname></persName>
							<email>lxtian@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Information Technology</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<postCode>100044</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="384" to="393"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">8BEDAD386B257BE8F8A999BFAF6AF943</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_37</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>SimSiam</term>
					<term>Individualized Prediction</term>
					<term>fMRI</term>
					<term>Functional Connectivity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Effective representations of human brain function are essential for fMRI-based predictions of individual traits and classifications of neuropsychiatric disorders. Contrastive learning techniques can be favorable choices for representations of human brain function, if it were not for their requirement of large batch sizes. In this study, we proposed a novel method, namely, contrastive learning with amplitude-driven data augmentation (CL-ADDA), for effective representations of human brain function and ultimately fMRI-based individualized predictions. Sim-Siam, which sets no requirement on large batches, was used in this study to obtain discriminative representations among subjects to facilitate later predictions of individuals' traits. The fMRI data in this study was augmented based on recent neuroscience findings that fMRI frames with high-and low-amplitude are of quite different functional significance. Accordingly, we generated a positive pair by concatenating the fMRI frames with high-amplitude into one augmented sample and the frames with low-amplitude into another sample. The two augmented samples were used as inputs for CL-ADDA, and individualized predictions were made in an end-to-end way. The performance of the proposed CL-ADDA was evaluated with individualized age and IQ predictions based on a public dataset (Cam-CAN). The experimental results demonstrate that the proposed CL-ADDA can substantially improve the prediction performance as compared to the existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In combination with machine learning techniques, functional magnetic resonance imaging (fMRI) has recently been widely used in predictions of individual traits (e.g., age and intelligence quotient (IQ)) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref> and classifications of neuropsychiatric disorders (e.g., Alzheimer's disease). Representations of human brain function are essential for such predictions, as effective representations can provide information discriminative among individuals and facilitate the final predictions.</p><p>Contrastive learning techniques can be favorable choices for representations of human brain function <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, if the available samples size is large enough for large-batch training (this is not the fact for most fMRI datasets). Contrastive learning can learn effective representations through minimizing/maximizing the distance between similar/dissimilar samples in the representation space <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. SimSiam <ref type="bibr" target="#b4">[5]</ref> is a contrastive learning framework that maximizes the similarity between two augmentations of one image. With the involvement of a Siamese structure, SimSiam does not rely on large-batch training. Accordingly, SimSiam can be a favorable choice for fMRI-based representations of human brain function.</p><p>Generation of similar/dissimilar samples is critical for contrastive learning <ref type="bibr" target="#b3">[4]</ref>. Among the few studies on fMRI-based individualized predictions in which contrastive learning is involved <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref>, fMRI data has often been augmented using classic methods in the region of computer vision, such as random erasing, random cropping and adversarial generation. In the pioneering study <ref type="bibr" target="#b20">[21]</ref>, two highly similar augmented samples were generated for each subject by excerpting two non-overlapping fMRI temporal segments. The highly similar pairs containing redundancy information can lead to poor performance of contrastive learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24]</ref>. Recent neuroscience findings provide an intuitive idea regarding data augmentation for RS-fMRI data. Specifically, fMRI frames with high-and low-amplitude were reported to be of quite different functional significance <ref type="bibr" target="#b22">[23]</ref>. Accordingly, a "discrepant-enough" positive pair can be generated by acquiring one sample based on fMRI frames with high-amplitude and the other based on frames with low-amplitude.</p><p>In this study, we proposed a framework named contrastive learning with amplitudedriven data augmentation (CL-ADDA) for effective representations of human brain function and ultimately fMRI-based individualized predictions. Two augmented samples of CL-ADDA were generated through excerpting fMRI frames with relatively high amplitude and those with relatively low amplitude. With two augmented samples of the same subject used as inputs, a SimSiam-based contrastive learning framework was used to learn effective representations of human brain function. For the consideration that label information can guide SimSiam to learn more prediction-relevant representation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25]</ref>, individualized predictions were performed in an end-to-end way through concatenated fully connected layers.</p><p>Our major contributions are as follows:</p><p>-SimSiam was utilized to learn representations of human brain function.</p><p>-A neuroscience-oriented amplitude-driven data augmentation method was introduced to generate positive pairs. -Predictions were made in an end-to-end way to improve the generalizability of the predictive models. -CL-ADDA outperformed a variety of state-of-the-art methods for fMRI-based individualized predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall Workflow of CL-ADDA</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the workflow of the proposed CL-ADDA. fMRI data of one subject is first excerpted into two segments, one composed of frames with high-amplitude and the other composed of frames with low-amplitude. Two functional connectivity (FC) maps (FC high and FC low ) can then be obtained based on the augmented samples, and the two FC maps are used as inputs for the contrastive learning module. The SimSiam structure is used to perform contrastive learning in this study, and the classic convolution in SimSiam is replaced by row and column convolutions to adapt to FC maps. SimSiam learns representations of brain function (r high and r low ) based on the two FC maps, using encoders (F) with shared parameters. Individualized predictions can be made through a predictor ( ) (three fully connected layers in this study) based on the learned representations. The whole model is trained though simultaneously minimizing the distance between the two representations (r high and r low ) and the difference between the predicted and actual label (individual traits in this study). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Amplitude-Driven Data Augmentation</head><p>Figure <ref type="figure" target="#fig_2">2</ref> provides an illustration of the amplitude-driven data augmentation method. In this study, we generated two augmented samples for each subject based on the amplitude of fMRI frames following <ref type="bibr" target="#b22">[23]</ref>. Specifically, we first defined N ROIs and extracted the mean time series of each ROI. We then z-scored each time series and obtained frame-wise co-fluctuation of each ROI pair as follows:</p><formula xml:id="formula_0">e jk = e 1 jk , e 2 jk , ..., e T jk , e t jk = x t j • x t k (1)</formula><p>where e t jk is the co-fluctuation of ROIs-j and k at time t; x t j x t k is the z-scored fMRI signal amplitude of ROI-j (-k) at time t; e jk is obtained the co-fluctuation time series, and T is the length of fMRI time series. Accordingly, a C  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contrastive Learning on Functional Connectivity Maps</head><p>We constructed the contrastive learning model based mainly on SimSiam. Setting no requirement on large batches, SimSiam can be a favorable choice for fMRI-based representation learning <ref type="bibr" target="#b4">[5]</ref>. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the SimSiam structure in this study consists of two branches of the same encoders (F) with shared weights for encoding the two input FC maps in parallel, followed by the same nonlinear projectors (G) with share weights for further processing the brain function representations from the encoders, and one predictor (H) on one branch for transforming the output of the branch to match it to the output of the other branch. A stop-gradient operation is applied on the branch without predictor to avoid model collapsing <ref type="bibr" target="#b4">[5]</ref>.</p><p>For the consideration that spatial locality does not exist among adjacent elements on FC maps <ref type="bibr" target="#b13">[14]</ref>, row and column convolutions were used to construct the backbone of the encoder (F) and extract effective information from the FC maps. Specifically, each encoder (F) in this study consists of one row convolution layer (with C r @1 × N row convolution filter, C r is the channel number, N is the ROI number) and one column convolution layer (with C c @N × 1 column convolution filters, C c is the channel number, outputs C c @1 × 1 representation). Information throughout the brain is expected to be integrated with the use of row and column convolutions. The row and column convolution layers in CL-ADDA are each followed by a batch normalization layer and a Leaky_ReLU layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Individualized Prediction and Loss Function</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, individualized prediction in this study was implemented using two parallel predictors ( ) with shared weights. Each predictor (( )) consists of three fully connected layers, which transform the C c outputs from the corresponding encoder (F) (learned representations of brain function, r high and r low ∈ R C c in Fig. <ref type="figure" target="#fig_0">1</ref>) into the predicted individual trait for the branch. For model training, the final prediction is made based on a weighted sum of the predictions from the two branches as follows:</p><formula xml:id="formula_1">ŷ = α • F ξ high + (1 -α) • (F(ξ low )) (2)</formula><p>where ξ ∈ R N×N denotes a FC map, F denotes the encoder, and α is a hyper-parameter.</p><p>In the model testing stage, prediction can directly be made as ŷ = (F(ξ )), where ξ can be a FC map calculated based on the whole fMRI scan (rather than augmented data).</p><p>The whole loss function for CL-ADDA includes two parts: contrastive loss and prediction loss. Contrastive learning minimizes the negative cosine similarity between the two branches:</p><formula xml:id="formula_2">D(p high , z low ) = - p high ||p high || 2 • z low ||z low || 2 D(p low , z high ) = -p low ||p low || 2 • z high ||z high || 2 (3)</formula><p>where</p><formula xml:id="formula_3">p low = H (G(F(ξ low ))), p high = H (G(F(ξ high ))), z low = G(F(ξ low )), z high = G(F(ξ high ))</formula><p>, G denotes the projector in Fig. <ref type="figure" target="#fig_0">1</ref>, and H denotes the predictor in the contrastive learning (Fig. <ref type="figure" target="#fig_0">1</ref>); ||.|| 2 is L2-norm.</p><p>Following <ref type="bibr" target="#b24">[25]</ref>, we defined contrastive loss as:</p><formula xml:id="formula_4">Loss cl = 1 2 D(p low , stopgrad (z high )) + 1 2 D(p high , stopgrad (z low ))<label>(4)</label></formula><p>L1 loss was used as the prediction loss:</p><formula xml:id="formula_5">Loss pred = ||y -y || 1<label>(5)</label></formula><p>where y and ŷ are the actual and predicted labels, respectively. The total loss was defined as follows:</p><formula xml:id="formula_6">Loss = λ • Loss pred + (1 -λ) • Loss cl (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where λ is hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>The resting-state fMRI data included in the dataset collected and released by the Cambridge Centre for Ageing and Neuroscience (Cam-CAN) <ref type="bibr" target="#b18">[19]</ref> was used in this study.</p><p>The public dataset contains multi-modal data from a large cohort of adult lifespan population-based samples. After removing the subjects with excessive head motions (translation/rotation more than 2.0 mm/2.0°in/around any of the x, y, or z directions) throughout the scan, 600 subjects remained (18-87, 53.900 ± 18.549 years), and IQ scores of 568 of them were available <ref type="bibr">32</ref>.032 ± 6.762). We preprocessed the data to remove the spatial and temporal artifacts and register the images to standard space (MNI) using FSL. We defined 200 ROIs based on independent component analysis and extracted the ROI time series through regressing the spatial maps of the independent components released by the Human Connectome Project (HCP) <ref type="bibr" target="#b19">[20]</ref> against the preprocessed fMRI data using the dual_regress command included in FSL. Later analyses were all based on the extracted ROI time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Performance of CL-ADDA</head><p>Age and IQ predictions were taken as test cases to evaluate the performance of the proposed method, based on the Cam-CAN dataset. Amplitude-driven data augmentation was performed on the 200 ROI time series of each subject, and two FC maps were obtained based on two augmented samples for each subject. The two augmented FC maps were used as inputs for SimSiam for representation learning and later individualized predictions. We performed 1000 epochs of model training, with the batch size set to 128.</p><p>For the consideration that high-amplitude FC maps (FC high ) may carry more detailed information about individuals' brain function, we empirically weight the predictions based on FC high more, by setting the α in Eq. ( <ref type="formula">2</ref>) to 0.8 The hyper-parameter λ in Eq. ( <ref type="formula" target="#formula_6">6</ref>) was set to 0.5. Adam optimizer with a learning rate of 0.001 was used. Ten-fold cross-validation was used to evaluate the performance of CL-ADDA, and Pearson's correlation coefficient (r) and mean absolute error (MAE) between the predicted and actual labels were used to quantitatively measure this performance. The results show that CL-ADDA performed well on both age and IQ predictions, as indicated by an r-value (MAE) of 0.886 (6.992 years) for age prediction, and 0.620 (4.531) for IQ prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison Experiments</head><p>We compared the performance of our proposed method with six deep learning methods for fMRI-based individualized predictions, namely, spatial-temporal graph convolutional network (ST-GCN) <ref type="bibr" target="#b21">[22]</ref>, RNN based on gated recurrent units (GRU) <ref type="bibr" target="#b5">[6]</ref>, pooling regularized graph neural network (PR-GNN) <ref type="bibr" target="#b15">[16]</ref>, brain graph neural network (BrainGNN) <ref type="bibr" target="#b16">[17]</ref>, simple fully convolutional network (SFCN) <ref type="bibr" target="#b17">[18]</ref>, and BrainNetCNN <ref type="bibr" target="#b13">[14]</ref>. Each of the six methods has been reported to perform well on fMRI-based individualized predictions, or even provide state-of-the-art results. Each method was implemented based on its online code, with the hyper-parameters set according to its original paper.</p><p>Table <ref type="table" target="#tab_0">1</ref> is a list of prediction accuracies based on the seven methods (including CL-ADDA). According to Table <ref type="table" target="#tab_0">1</ref>, CL-ADDA outperformed the methods for comparison by large margins. For instance, compared with the second best (ST-GCN), CL-ADDA demonstrated an r-value increase of 0.085 for IQ prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Experiments</head><p>To evaluate the effectiveness of the proposed amplitude-driven data augmentation strategy, we performed age and IQ predictions with the data augmented using classic methods, and the strategy of excerpting non-overlapping segments as proposed in <ref type="bibr" target="#b20">[21]</ref>. For classic augmentation, two 175 × 175 augmented FC maps were generated by applying random cropping and Gaussian blurring on the 200 × 200 FC matrix calculated based on the whole fMRI scan <ref type="bibr" target="#b4">[5]</ref>. For the non-overlapping segment excerpting strategy, we generated two 200 × 200 augmented FC matrices based on the two non-overlapping fMRI data segments excerpted from the same scan <ref type="bibr" target="#b20">[21]</ref>. According to Table <ref type="table" target="#tab_1">2</ref>, the proposed amplitude-driven data augmentation is obviously superior to the other two data augmentation strategies. We further evaluated the effectiveness of contrastive learning, as well as the endto-end individualized prediction strategy. Specifically, (1) we performed individualized age and IQ predictions based on a network composed of one encoder (F) and one predictor (F) to imitate a network with contrastive learning removed. (2) We pre-trained CL-ADDA and then predicted age and IQ using the representations based on the pretrained CL-ADDA to imitate abandoning the end-to-end individualized prediction strategy. According to Table <ref type="table" target="#tab_2">3</ref>, both contrastive learning and the end-to-end individualized prediction strategy were critical for CL-ADDA. The results indicate that supervised contrastive learning can be a favorable choice for neuroimage-based individualized predictions and neuropsychiatric disease classifications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we proposed CL-ADDA for effective representation learning and ultimately precise fMRI-based individualized predictions. Originating from a recent neuroscientific finding, the proposed amplitude-driven data augmentation method provides the contrastive learning module discrepant-enough positive pairs for effective representation learning. SimSiam-based contrastive learning enables effective representation learning on fMRI dataset including limited samples. We evaluated the performance of CL-ADDA with age and IQ predictions based on a public dataset, and the experiments demonstrate that CL-ADDA achieved state-of-the-art predictions for both age and IQ.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Workflow of CL-ADDA. The high-and low-amplitude time series are generated from the same subject. TS -time series; FC -functional connectivity; amp. -amplitude.</figDesc><graphic coords="3,54,81,275,60,314,83,111,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2 N × T co-fluctuation matrix E = e 12 ; e 13 ; ...e 1N ; e 23 ; e 24 ; ...e 2N ; ...e (N -1)N can be obtained. We computed the root sum square (RSS) of the co-fluctuation matrix E at each time point and finally generated one high-amplitude sample by excerpting the top 50% of frames with high co-fluctuation RSS, and one low-amplitude sample by concatenating the remaining frames (with low co-fluctuation RSS).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the amplitude-driven data augmentation method. TS -time series; amp.amplitude; RSS -root sum square.</figDesc><graphic coords="4,115,98,136,13,220,36,134,74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Age and IQ prediction accuracies based on different deep learning methods.</figDesc><table><row><cell cols="2">Age Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Method ST-GCN GRU PR-GNN BrainGNN SFCN BrainNetCNN CL-ADDA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(ours)</cell></row><row><cell>r</cell><cell>0.848</cell><cell>0.789 0.808</cell><cell>0.811</cell><cell cols="2">0.738 0.711</cell><cell>0.886</cell></row><row><cell>MAE</cell><cell>7.702</cell><cell>9.733 9.347</cell><cell>9.008</cell><cell cols="2">9.815 11.566</cell><cell>6.992</cell></row><row><cell>(years)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">IQ Prediction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Method ST-GCN GRU PR-GNN BrainGNN SFCN BrainNetCNN CL-ADDA</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(ours)</cell></row><row><cell>r</cell><cell>0.535</cell><cell>0.458 0.488</cell><cell>0.515</cell><cell cols="2">0.402 0.412</cell><cell>0.620</cell></row><row><cell>MAE</cell><cell>4.602</cell><cell>5.572 5.263</cell><cell>4.614</cell><cell>5.53</cell><cell>6.935</cell><cell>4.531</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Age and IQ prediction accuracies based on different data augmentation strategies.</figDesc><table><row><cell>Age Prediction</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Classic Augmentation</cell><cell>Non-Overlapping</cell><cell>Amplitude-Driven (ours)</cell></row><row><cell></cell><cell></cell><cell>Segments</cell><cell></cell></row><row><cell>r</cell><cell>0.349</cell><cell>0.831</cell><cell>0.886</cell></row><row><cell>MAE (years)</cell><cell>14.849</cell><cell>9.167</cell><cell>6.992</cell></row><row><cell>IQ Prediction</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Classic Augmentation</cell><cell>Non-overlapping</cell><cell>Amplitude-Driven (ours)</cell></row><row><cell></cell><cell></cell><cell>Segments</cell><cell></cell></row><row><cell>r</cell><cell>0.288</cell><cell>0.589</cell><cell>0.620</cell></row><row><cell>MAE</cell><cell>5.333</cell><cell>5.034</cell><cell>4.531</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Age and IQ prediction accuracies based on CL-ADDA with different modules being removed.</figDesc><table><row><cell>Age Prediction</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>with Individualized</cell><cell>with Contrastive Learning</cell><cell>CL-ADDA (ours)</cell></row><row><cell></cell><cell>Prediction Removed</cell><cell>Removed</cell><cell></cell></row><row><cell>r</cell><cell>0.782</cell><cell>0.873</cell><cell>0.886</cell></row><row><cell>MAE (years)</cell><cell>9.467</cell><cell>7.377</cell><cell>6.992</cell></row><row><cell>IQ Prediction</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>with Individualized</cell><cell>with Contrastive Learning</cell><cell>CL-ADDA</cell></row><row><cell></cell><cell>Prediction Removed</cell><cell>Removed</cell><cell></cell></row><row><cell>r</cell><cell>0.573</cell><cell>0.589</cell><cell>0.620</cell></row><row><cell>MAE</cell><cell>4.779</cell><cell>5.042</cell><cell>4.531</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. We thank investigators from <rs type="affiliation">Cambridge Centre for Ageing and Neuroscience</rs> for sharing the public dataset.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Possible principles underlying the transformation of sensory messages</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Barlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sens. Commun</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="217" to="233" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9912" to="9924" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contrastive learning with continuous proxy meta-data for 3D MRI classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dufumier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grigis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wessa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brambilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Polosan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Piguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-3_6" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Cotin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Essert</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolution for resting-state fMRI analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gadgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pfefferbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Pohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Zuluaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Racoceanu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59728-3_52</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59728-3_52" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2020</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12267</biblScope>
			<biblScope unit="page" from="528" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting cortical signatures of consciousness using dynamic functional connectivity graph-convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grigis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tasserie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ambroise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Frouin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2020" to="2005" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richemond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep neural networks and kernel regression achieve comparable accuracies for functional connectivity prediction of behavior and demographics</title>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page">116276</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Behavior score-embedded brain encoder network for improved classification of Alzheimer disease using resting state fMRI</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lefort-Besnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the IEEE Engineering in Medicine &amp; Biology Society</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5486" to="5489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BrainNetCNN: convolutional neural networks for brain networks; towards predicting neurodevelopment</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page" from="1038" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multi-task contrastive learning for automatic CT and X-ray diagnosis of COVID-19. Pattern Recogn</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page">107848</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pooling regularized graph neural network for fmri biomarker analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ventola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Zuluaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Racoceanu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59728-3_61</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59728-3_61" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12267</biblScope>
			<biblScope unit="page" from="625" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Braingnn: interpretable brain graph neural network for FMRI analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dvornek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">102233</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate brain age prediction with lightweight deep neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">101871</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Cambridge Centre for Ageing and Neuroscience (Cam-CAN) data repository: Structural and functional MRI, MEG, and cognitive data from a cross-sectional adult lifespan sample</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cusack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Shafto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="262" to="269" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The human connectome project: a data acquisition perspective</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Van Essen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ugurbil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Auerbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Behrens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2222" to="2231" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrastive functional connectivity graph learning for population-based fMRI classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rekik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-6_21" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High-amplitude cofluctuations in cortical activity drive functional connectivity</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zamani Esfahlani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Faskowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Byrge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="28393" to="28401" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12310" to="12320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised feature selection via spectral analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM international conference on data mining</title>
		<meeting>the SIAM international conference on data mining</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="641" to="646" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
