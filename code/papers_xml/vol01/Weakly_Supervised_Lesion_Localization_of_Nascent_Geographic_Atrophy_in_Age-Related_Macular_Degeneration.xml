<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration</title>
				<funder>
					<orgName type="full">Victorian Government. Ellex R&amp;D Pty Ltd (Adelaide, Australia)</orgName>
				</funder>
				<funder ref="#_hrV9jFk #_qZ24mpw">
					<orgName type="full">National Health and Medical Research Council of Australia</orgName>
				</funder>
				<funder ref="#_z7V2hkJ">
					<orgName type="full">BUPA Health Foundation (Australia)</orgName>
				</funder>
				<funder>
					<orgName type="full">Macular Disease Foundation Australia</orgName>
				</funder>
				<funder>
					<orgName type="full">Genentech, Inc.</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Heming</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South San Francisco</orgName>
								<address>
									<settlement>Genentech</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adam</forename><surname>Pely</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South San Francisco</orgName>
								<address>
									<settlement>Genentech</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhichao</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Eye Research Australia</orgName>
								<orgName type="institution">Royal Victorian Eye and Ear Hospital</orgName>
								<address>
									<settlement>East Melbourne</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">VIC</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Surgery</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<settlement>Ophthalmology, Melbourne</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South San Francisco</orgName>
								<address>
									<settlement>Genentech</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robyn</forename><forename type="middle">H</forename><surname>Guymer</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Eye Research Australia</orgName>
								<orgName type="institution">Royal Victorian Eye and Ear Hospital</orgName>
								<address>
									<settlement>East Melbourne</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">VIC</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Surgery</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<settlement>Ophthalmology, Melbourne</settlement>
									<region>VIC</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South San Francisco</orgName>
								<address>
									<settlement>Genentech</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohsen</forename><surname>Hejrati</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South San Francisco</orgName>
								<address>
									<settlement>Genentech</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Miao</forename><surname>Zhang</surname></persName>
							<email>zhang.miao@gene.com</email>
							<affiliation key="aff0">
								<orgName type="institution">South San Francisco</orgName>
								<address>
									<settlement>Genentech</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">employees of Genentech, Inc. and shareholders in F. Hoffmann La Roche</orgName>
								<address>
									<region>Ltd</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="477" to="485"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">1E3B9C466EA51B59A3231DA9C2405881</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_46</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>OCT</term>
					<term>weakly supervised learning</term>
					<term>object detection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The optical coherence tomography (OCT) signs of nascent geographic atrophy (nGA) are highly associated with GA onset. Automatically localizing nGA lesions can assist patient screening and endpoint evaluation in clinical trials. This task can be achieved with supervised object detection models, but they require laborious bounding box annotations. This study thus evaluated whether a weakly supervised method could localize nGA lesions based on the saliency map generated from a deep learning nGA classification model. This multi-instance deep learning model is based on 2D ResNet with late fusion and was trained to classify nGA on OCT volumes. The proposed method was cross-validated using a dataset consisting of 1884 volumes from 280 eyes of 140 subjects, which had volumewise nGA labels and expert-graded slice-wise lesion bounding box annotations. The area under Precision-Recall curve (AUPRC) or correctly localized lesions was 0.72(±0.08), compared to 0.77(±0.07) from a fully supervised method with YOLO V3. No statistically significant difference is observed between the weakly supervised and fully supervised methods (Wilcoxon signed-rank test, p = 1.0).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nascent geographic atrophy (nGA), originally described by Wu et al. <ref type="bibr" target="#b0">[1]</ref>, describes features of photoreceptor degeneration seen on optical coherence tomography (OCT) imaging that are strongly associated with the development of geographic atrophy (GA), a late stage complication of age-related macular degeneration (AMD). A recent study reported that the development of nGA in individuals with intermediate AMD was associated with a 78-fold increased rate of GA development <ref type="bibr" target="#b1">[2]</ref>. Thus, nGA could potentially act as an earlier biomarker of AMD progression, or potentially as an earlier endpoint in intervention studies aiming to slow GA development <ref type="bibr" target="#b2">[3]</ref>. Thus being able to easily identify eyes with nGA and localize nGA lesions is important in clinical trials and research.</p><p>However, identifying and grading the location of nGA lesions in OCT volume scans can be a laborious task, and would be an operationally expensive undertaking in clinical trials. Automation of this task would be invaluable when seeking to quantify the number of nGA lesions present, or when seeking to identify a smaller subset of B-scans for manual expert review (an "AI-assisted" approach). While the localization of nGA could be tackled by supervised object detection models -as demonstrated in other types of lesions <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> -it takes domain experts a large amount of time to provide sufficient number of lesion level annotations (e.g. with a bounding box). On the other hand, weakly supervised methods require only coarse annotations, and they have been popular in computer vision tasks where dense annotations are difficult to obtain <ref type="bibr" target="#b6">[7]</ref>.</p><p>In this work, we sought to develop a deep learning-based method to automate the localization of nGA lesions on OCT imaging, trained only on the information about the presence or absence of nGA at the volume level. A weakly supervised algorithm was developed that utilizes the saliency maps from Gradient Class Activation Maps (GradCAM) technique <ref type="bibr" target="#b7">[8]</ref>. While existing literature has demonstrated the ability of the GradCAM in post-hoc model interpretation <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>, it is unknown whether the saliency map can help further localize the class-related lesions or abnormalities that are often sparse anatomically. We thus explored the possibility of GradCAM in identifying the location of nGA-related abnormalities after training a model for classifying nGA in a 3D volume scan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset</head><p>This study included participants in the sham treatment arm of the Laser Intervention in the Early Stages of AMD study (LEAD, clinicaltrials.gov identifier, NCT01790802). The LEAD study was conducted according to the International Conference on Harmonization Guidelines for Good Clinical Practice and the tenets of the Declaration of Helsinki. Institutional review board approval was obtained at all sites and all participants provided written informed consent.</p><p>The participants of LEAD study were required to have bilateral large drusen and a best-corrected visual acuity of 20/40 or better in both eyes at baseline <ref type="bibr" target="#b12">[13]</ref>. Participants were evaluated at the baseline and every 6-month follow up visits for up to 36-months. At each visit, OCT imaging was performed following pupillary dilation, by obtaining a 3D volume scan consisting of 49 B-scans (i.e. 2D slices along X-Z direction) covering a 20°× 20°× 1.9 mm region of the macula, with 1024 × 49 × 496 voxels anisotropically sampled along X, Y and Z d directions respectively.</p><p>Multimodal imaging was used to assess the development of late AMD as an endpoint in the LEAD study, which included nGA detected on OCT imaging. In order to evaluate the association between nGA and the subsequent development of GA as detected on color fundus photographs (CFP; the historical gold standard for atrophic AMD) in a previous study, OCT imaging and CFP were independently re-graded for the presence of nGA and GA respectively <ref type="bibr" target="#b13">[14]</ref>. In this sub-study, we included individuals who did not have nGA at baseline based on the above independent re-grading of OCT imaging, and who had at least one follow-up visit.</p><p>A total of 1,884 OCT volumes from 280 eyes of 140 individuals were included in this analysis (1,910 volumes were collected, but 26 volumes were excluded from the study due to the development of neovascular AMD in the eye). In this study, the development of nGA was assessed by manual grading of all 49 B-scans of each OCT volume scans, and nGA was defined by the subsidence of the outer plexiform layer and inner nuclear layer, and/or the presence of a hyporeflective wedge-shaped band within Henle's fiber layer, as per the original definition <ref type="bibr" target="#b0">[1]</ref>. All OCT volume scans were initially assessed by a senior grader, and all visits of any eye deemed to have questionable or definite nGA were then reviewed by two further experienced graders <ref type="bibr" target="#b1">[2]</ref>.</p><p>Overall, nGA was graded as being absent and present in 1,766 and 118 OCT volume scans respectively. In the context of this study, note that nGA also includes lesions that could also meet the criteria for having complete retinal pigment epithelium and outer retinal atrophy (cRORA), if the lesion also had choroidal signal hypertransmission and retinal pigment epithelium (RPE) attenuation or disruption of ≥250 µm <ref type="bibr" target="#b14">[15]</ref>. Graders also concurrently graded the location of nGA lesions by identifying the B-scans with nGA lesions and by drawing bounding boxes on the B-scans horizontally covering the subsidence, vertically from the inner limiting membrane (ILM) to Bruch's membrane. For the weakly supervised model, the bounding boxes were used only in evaluating the weakly supervised lesion localization, not in model training. The bounding boxes were then used to train a fully supervised object detector to compare the results of the weakly supervised and fully supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning Architecture</head><p>A late-fusion model with a 2D ResNet backbone was developed to classify 3D OCT volumes, considering their anisotropic nature. As shown in Fig. <ref type="figure" target="#fig_0">1a,</ref><ref type="figure">B</ref>-scans from a 3D OCT volume were fed into a B-scan detector, and the outputs, which are vectors of classification logits for each B-scan, were averaged to generate prediction scores for the volume. Thinking of the B-scans as instances and the OCT volumes as bags, this framework can be categorized as simplified multi-instance learning <ref type="bibr" target="#b15">[16]</ref> in which the network was trained on weakly labeled data, using labels on bags (OCT volumes) only. During the training process, given an OCT volume annotated as nGA, the network was forced to identify as many B-scans with nGA lesion to improve the final prediction of nGA, thus the trained model allows prediction of nGA labels on OCT volumes as well as on individual B-scans.</p><p>The details of the B-scan classifier are shown in Fig. <ref type="figure" target="#fig_0">1b</ref>. An individual B-scan of size 1024 × 496 from the volume is downsampled to 512 × 496 and passed through the ResNet-18 backbone, which outputs activation maps of 512 × 16 × 16. A maxpooling layer and an average pooling layer are concatenated to generate a feature vector of 1024. Then a fully connected layer was applied to generate the classification logit for the B-scan.</p><p>The classification model was evaluated on its own both in terms of volume-wise and slice-wise performance in classifying nGA. After it was confirmed that the classification model worked well, the ability of the model to localize the lesions within individual OCT slices was evaluated.</p><p>Given an OCT volume and a trained model, saliency maps were generated with the GradCAM technique <ref type="bibr" target="#b7">[8]</ref> to visualize regions making larger contributions to the final classification. In Fig. <ref type="figure" target="#fig_1">2a</ref>, GradCAM output was overlaid as the yellow channel on the input images for easy visualization of the saliency as well as the original grayscale image. The saliency map from a legitimate model should highlight nGA lesions, thus GradCAM output can help localize nGA lesions. The objective was to localize nGA lesions in the 3D OCT volume, i.e. to identify which B-scans have nGA lesions and to generate a bounding box surrounding the lesions in those B-scans with confidence scores. As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, the automated image processing pipeline was built upon adaptive thresholding and connected component analysis <ref type="bibr" target="#b16">[17]</ref>. For each B-scan with positive logit, one or multiple bounding boxes covering potential lesions were detected. The confidence score for each bounding box was estimated from the individual classification logit of the B-scan classifier as Eq. ( <ref type="formula" target="#formula_0">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S l n h</head><formula xml:id="formula_0">h (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where S is the sigmoid function, l is the individual B-scan classification logit, and n is the number of B-scans in a volume, h is the mean saliency in the detected region and Σh is the total mean saliency of all detected regions within the B-scan. A higher confidence score implies a higher possibility that the detected region covers nGA lesions. Since only class labels of 3D OCT volume are required for training, the proposed lesion localization algorithm was weakly supervised. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Training, Tuning, and Validation Test</head><p>Considering the relatively small number of participants in the dataset, a five-fold crossvalidation was applied to evaluate the proposed method's performance. We followed the nomenclature for data splitting as recommended previously <ref type="bibr" target="#b17">[18]</ref>. For each fold, the validation test set of OCT volumes were obtained from roughly 20% of the participants stratified on whether the individual developed nGA. The OCT volumes from the remaining 80% individuals were further split into training (64%) and tuning sets (16%), with volumes from one individual only existing in one of the sets. With the proposed data split strategy, the corresponding validation test set was not used in the training or hyperparameter tuning process.</p><p>Pre-processing was performed on B-scans for standardization. The B-scans were first resized to 512 × 496, followed by rescaling the intensity range to [0, 1]. Data augmentation, including rotation of small angles, horizontal flips, add on of Gaussian noises, and Gaussian blur were randomly applied to improve the model's invariance to those transformations.</p><p>A Resnet-18 backbone pre-trained on the ImageNet dataset was used. During the model training, the Adam optimizer was used to minimize focal loss. The L2 weight decay regularization was used to improve the model's generalization.</p><p>As a benchmark for the weakly supervised lesion localization, a fully supervised YOLOv3 object detector <ref type="bibr" target="#b18">[19]</ref> with a Resnet-18 backbone was trained using the bounding box information for each B-scan.</p><p>A successful lesion localization was recorded only if the bounding box output overlapped with the bounding boxes annotated by clinicians with an intersection over union (IoU) value of at least 0.05. The area under the Precision-Recall curve (AUPRC) was calculated to evaluate the model performance. In patient screening, a high recall is preferred over precision. Considering the difference of the two methods, different strategies were used to determine the confidence threshold in calculating the precision and recall values in the validation test dataset. For the weakly supervised method, the threshold for confidence score that would achieve a recall value of 0.98 for nGA volume classification in the training and tuning sets is used. For the supervised method, the confidence threshold which would achieve a recall value of 0.9 for bounding box detection in the turning set is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Performance and Saliency Map Analysis of the nGA Classification Model on OCT Volumes</head><p>The deep learning based nGA classification model achieved an AUPRC of 0.83(±0.09) in classifying 3D OCT volumes. Based on the trained 3D OCT volume classification model and input OCT volumes, we generated the corresponding saliency map using GradCAM technique. Examples of GradCAM output are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Values in the GradCAM output indicate the importance of the corresponding pixel in the input Bscan to the model's prediction. A higher (brighter) value means the corresponding pixel contributes more to the model's prediction that the input OCT volume is positive. A thresholding was applied to the pixel values to determine the region where the bounding box delineating nGA should be drawn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance of the Weakly Supervised Localization of nGA Lesions</head><p>The weakly supervised algorithm achieved a similar level of performance for localizing nGA when compared to the YOLOv3 based fully supervised method, without utilizing bounding box annotations and these findings are illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. The YOLOv3 based method achieved an AUPRC of 0.77(±0.07) compared to 0.72(±0.08) for the weakly supervised model; no statistically significant difference was observed between the two methods (Wilcoxon signed-rank test for AUPRC, p = 1.0). In the patient screening setting described previously, the YOLOv3 based method achieved a precision and recall of 0.53(±0.16) and 0.88(±0.06), compared to 0.39(±0.13) and 0.88(±0.02) for the weakly supervised method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Discussion</head><p>This study demonstrates that the performance for localizing nGA lesions by only using OCT volume-wise classification labels with the GradCAM technique was on par with a fully supervised approach using B-scan level annotations with the YOLOv3 detector. These findings therefore underscore the potential of a weakly supervised approach for enabling the development of a robust model for lesion localization without the need for laborious, lesion-level annotations on OCT B-scans.</p><p>One limitation of the GradCAM-based lesion localization is its relatively large bounding box size, often exceeding the annotated region. This is expected, considering the low spatial resolution of GradCAM saliency map, but also potentially because this approach identified contextual features that are distinguishing of nGA lesions that were not annotated by the graders. In addition, the weakly supervised model uses adaptive threshold of the saliency to determine the bounding box size, which was not optimized to match the ground truth grading. This limitation with the larger bounding box size could impact the quantification of the number of nGA lesions present, but it would unlikely have a substantial impact on the task of identifying a subset of OCT B-scans requiring manual review in an AI-assisted evaluation.</p><p>In conclusion, this study demonstrates that a weakly supervised method, requiring only volume-wise tags, can achieve a similar level of performance for localizing lesions compared to a fully supervised method using slice-wise bounding box labels. A weakly supervised approach could thus minimize the labeling burden when seeking to develop a lesion localization model, and could even leverage existing volume-wise labels for its development.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Deep learning architecture of the nGA classification model. (a) Model network for 3D OCT volume. (b) The B-scan classifier. ReLU = rectified linear unit. FC = fully connected.</figDesc><graphic coords="4,41,79,226,13,340,51,168,97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Illustration of locating nGA lesions with B-scan logit and GradCAM. B-scans with positive logits were selected, their GradCAM outputs (in the viridis colormap) were thresholded adaptively, then a bounding box was generated by the connected component analysis. A confidence score of the bounding box was estimated based on its average saliency and the corresponding Bscan logit. (b) Examples of a B-scan overlaid with GradCAM output (left) and bounding boxes with confidence scores (right), the yellow bounding box with confidence score below threshold was removed in following processing. (Color figure online)</figDesc><graphic coords="5,56,97,56,39,339,01,272,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a)-(f) Example results from the weakly and fully supervised detection methods alongside the ground truth. The ground truth is in green, the YOLOv3 detector output in red, and the GradCAM based detector output is in yellow. (a-c) A true positive case for both methods. (d) A true positive case for the fully supervised method, but false negative for the weakly supervised method. (e) A true positive case for the weakly supervised method, but false negative for the fully supervised method. (f) A case where there are two ground truth lesions, but both methods detected only the left one. (g) The per fold precision-recall curve for the GradCAM based detector along with the AUPRC values. (h) The per fold precision-recall curve for YOLOv3 detector along with the AUPRC values. (i) A comparison between the GradCAM based detector and YOLOv3 on the metrics of AUPRC, as well as recall and precision in the patient screening setting described in Methods. (Color figure online)</figDesc><graphic coords="7,58,98,57,35,334,39,327,04" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We would like to thank all of the study participants and their families, and</p></div>
			</div>
			<div type="funding">
<div><p>Supported by the <rs type="funder">National Health and Medical Research Council of Australia</rs> (project grant no.: <rs type="grantNumber">APP1027624</rs> [R.H.G.], and fellowship grant nos.: <rs type="grantNumber">GNT1103013</rs> [R.H.G.], #<rs type="grantNumber">2008382</rs> [Z.W.]; the <rs type="funder">BUPA Health Foundation (Australia)</rs> (R.H.G.) and the <rs type="funder">Macular Disease Foundation Australia</rs> (Z.W. and R.H.G.). The Centre for Eye Research Australia receives operational infrastructure support from the <rs type="funder">Victorian Government. Ellex R&amp;D Pty Ltd (Adelaide, Australia)</rs> provided partial funding of the central coordinating center and the in-kind provision the Macular Integrity Assessment microperimeters for the duration of the LEAD study. The web-based Research Electronic Data Capture application and open-source platform OpenClinica allowed secure electronic data capture. The LEAD study was sponsored by the <rs type="institution">Centre for Eye Research Australia, East Melbourne, Australia</rs>, an independent medical research institute and a not-for-profit company. This sub-study was supported by <rs type="funder">Genentech, Inc.</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hrV9jFk">
					<idno type="grant-number">APP1027624</idno>
				</org>
				<org type="funding" xml:id="_qZ24mpw">
					<idno type="grant-number">GNT1103013</idno>
				</org>
				<org type="funding" xml:id="_z7V2hkJ">
					<idno type="grant-number">2008382</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optical coherence tomography-defined changes preceding the development of drusen-associated atrophy in age-related macular degeneration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2415" to="2422" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Prospective longitudinal evaluation of nascent geographic atrophy in age-related macular degeneration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmol. Retina</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="568" to="575" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Can the onset of atrophic age-related macular degeneration be an acceptable endpoint for preventative trials?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Guymer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ophthalmologica. J. Int. d&apos;ophtalmologie. Int. J. Ophthalmol. Zeitschrift fur Augenheilkunde</title>
		<imprint>
			<biblScope unit="volume">243</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="399" to="403" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fullyautomated atrophy segmentation in dry age-related macular degeneration in optical coherence tomography</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Derradji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mosinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Apostolopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ciller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>De Zanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">21893</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated identification of incomplete and complete retinal epithelial pigment and outer retinal atrophy using machine learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Corradetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Investig. Ophthalmol. Vis. Sci</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">3860</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated identification of incomplete and complete retinal epithelial pigment and outer retinal atrophy using machine learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmol. Retina</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="118" to="126" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised lesion localization for age-related macular degeneration detection using optical coherence tomography images</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">215076</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Grad-CAM: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving interpretability in machine diagnosis: detection of geographic atrophy in OCT scans</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmol. Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">100038</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optical coherence tomography-based deep-learning model for detecting central serous chorioretinopathy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">18852</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explainable deep learning for biomarker classification of OCT images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Furst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raicu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 20th International Conference on Bioinformatics and Bioengineering (BIBE)</title>
		<meeting><address><addrLine>Cincinnati, OH</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="204" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Development and validation of a deep learning system to screen vision-threatening conditions in high myopia using optical coherence tomography images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="633" to="639" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Subthreshold nanosecond laser intervention in age-related macular degeneration: the lead randomized controlled clinical trial</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Guymer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="829" to="838" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting progression of age-related macular degeneration using OCT and fundus photography</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bogunović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Guymer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmol. Retina</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="118" to="125" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Incomplete retinal pigment epithelial and outer retinal atrophy in agerelated macular degeneration: classification of atrophy meeting report 4</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Guymer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="394" to="409" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multiple instance learning: a survey of problem characteristics and applications. Pattern Recog</title>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Carbonneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheplygina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gagnon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="329" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A threshold selection method from gray-level histograms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="62" to="66" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Digit. Health</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="271" to="e297" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">YOLOv3: An Incremental Improvement</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
