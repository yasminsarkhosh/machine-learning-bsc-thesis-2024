<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiale</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Medical Robotics</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<addrLine>No. 800, Dongchuan Road</addrLine>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><forename type="middle">F</forename><surname>Heimann</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Orthopaedic Surgery</orgName>
								<orgName type="institution">HFR Cantonal Hospital</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Fribourg</orgName>
								<address>
									<settlement>Fribourg</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Moritz</forename><surname>Tannast</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Orthopaedic Surgery</orgName>
								<orgName type="institution">HFR Cantonal Hospital</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Fribourg</orgName>
								<address>
									<settlement>Fribourg</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Guoyan</forename><surname>Zheng</surname></persName>
							<email>guoyan.zheng@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Medical Robotics</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<addrLine>No. 800, Dongchuan Road</addrLine>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="497" to="507"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">9815C68803BA8F74BBF5E1F0B355807E</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_48</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised image super-resolution</term>
					<term>Cross-modality image translation</term>
					<term>CT-Guided</term>
					<term>Magnetic resonance imaging</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning-based algorithms for single MR image (MRI) super-resolution have shown great potential in enhancing the resolution of low-quality images. However, many of these methods rely on supervised training with paired low-resolution (LR) and high-resolution (HR) MR images, which can be difficult to obtain in clinical settings. This is because acquiring HR MR images in clinical settings requires a significant amount of time. In contrast, HR CT images are acquired in clinical routine. In this paper, we propose a CT-guided, unsupervised MRI super-resolution reconstruction method based on joint cross-modality image translation and super-resolution reconstruction, eliminating the requirement of high-resolution MRI for training. The proposed approach is validated on two datasets respectively acquired from two different clinical sites. Well-established metrics including Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Metrics (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) are used to assess the performance of the proposed method. Our method achieved an average PSNR of 32.23, an average SSIM of 0.90 and an average LPIPS of 0.14 when evaluated on data of the first site. An average PSNR of 30.58, an average SSIM of 0.88, and an average LPIPS of 0.10 were achieved by our method when evaluated on data of the second site.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>High-resolution magnetic resonance (MR) images (MRI) provide a wealth of structural details, which facilitate early and precise diagnosis <ref type="bibr" target="#b0">[1]</ref>. However, images obtained in clinical practice are anisotropic due to the limitation of scan</p><p>This study was partially supported by the National Natural Science Foundation of China via project U20A20199.</p><p>time and signal-noise ratio <ref type="bibr" target="#b1">[2]</ref>. In order to speed up clinical scanning procedures, only a limited number of two-dimensional (2D) slices are acquired, despite the fact that the interested anatomical structures are in three-dimensional (3D). The acquired medical images have low inter-plane resolution, i.e., large spacing between slices. Such anisotropic images will lead to misdiagnosis and can greatly impact the performance of various clinical tasks, including computer-aided diagnosis and computer-assisted interventions. Therefore, we investigate the problem of reducing the slice spacing <ref type="bibr" target="#b2">[3]</ref> via super-resolution (SR) reconstruction. Specifically, we refer to the image with large slice spacing as a low-resolution (LR) image and the image with small slice spacing as a high-resolution (HR) image. Our goal is to reconstruct the HR image from the LR input, which is an ill-posed inverse problem and presents significant challenges.</p><p>Deep learning-based algorithms for single MR image super-resolution show great potential in restoration of HR images from LR inputs <ref type="bibr" target="#b3">[4]</ref>. Pham et al. <ref type="bibr" target="#b4">[5]</ref> proposed the SRCNN method, which applied convolutional neural networks (CNN) to image super-resolution of MRI and achieved a better performance than the conventional methods, such as B-spline interpolation and low-rank total variation (LRTV) <ref type="bibr" target="#b5">[6]</ref> method. Chaudhariet al. <ref type="bibr" target="#b6">[7]</ref> proposed a 3D residual network, which learned the residual-based transformations between paired LR and HR images for the SR reconstruction of MRI. Chen et al. <ref type="bibr" target="#b7">[8]</ref> proposed a densely connected super-resolution network (DCSRN), which reused the block features through the dense connection in the SR reconstruction of MRI. Chen et al. <ref type="bibr" target="#b8">[9]</ref> extended this work by using generative adversarial network (GAN) <ref type="bibr" target="#b10">[10]</ref> in SR reconstruction of MRI in order to improve the realism of the recovered images. Feng et al. <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref> proposed a multi-contrast MRI SR method, which aimed to learn clearer anatomical structure and edge information with the help of auxiliary contrast MRI. Despite significant progress, however, there are still spaces for further improvement. Most networks require a large amount of paired LR and HR MR images for training, which are unrealistic in clinical practice. To address the challenge of organizing paired images, methods based on unpaired images have been proposed <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref>. However, HR MR images are still difficult to obtain, as acquiring HR MR images in clinical settings requires a significant amount of time. In contrast, CT images are acquired in clinical routine. Therefore, it is of great significance to use HR CT images as a guidance to synthesize HR MR images from LR MR images.</p><p>To this end, we propose a CT-guided, unsupervised MRI super-resolution reconstruction method based on joint cross-modality image translation (CIT) and super-resolution reconstruction, eliminating the requirement of HR MR images for training. Specifically, our network design features a super-resolution Network (SRNet) and a cross-modality image translation network (CITNet) based on disentanged representation learning. After pretraining, the SRNet can generate pseudo HR MR images from LR MR images. The generated pseudo HR MR images are then taken together with the HR CT images as the input to the CITNet, which can generate quality-improved pseudo HR MR images by combining disentangled content code of the input CT data with the attribute  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Super-Resolution Network (SRNet)</head><p>We choose to use the residual dense network (RDN) as the SRNet. The RDN utilizes cascaded residual dense blocks (RDBs), a powerful convolutional block that leverages residual and dense connections to fully aggregate hierarchical features.</p><p>For further details on the structure of the RDN, please refer to the original paper <ref type="bibr" target="#b15">[15]</ref>. Mathematically, we denote the SRNet as F s (•; Θ s ) with trainable parameters Θ s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-Modality Image Translation Network (CITNet)</head><p>The CITNet is inspired by MUNIT <ref type="bibr" target="#b16">[16]</ref>. As depicted in Fig. <ref type="figure" target="#fig_0">1</ref></p><formula xml:id="formula_0">-(A.2), it com- prises two content encoders E C X , E C Y , two attribute encoders E A X , E A Y , and two generators {G X , G Y }.</formula><p>The encoder in each domain disentangles an input image separately into a domain-invariant content space C and a domain-specific attribute space A. And the generator networks combine a content code with an attribute code to generate translated images in the target domain. For instance, when translating CT image y H ∈ Y to MR image x H ∈ X , we first randomly sample from the prior distribution p(A x ) ∼ N (0, I) to obtain an MRI attribute code A x , which is empirically set as a 8-bit vector. We then combine A x with the disentangled content code of the CT image</p><formula xml:id="formula_1">C y = E C Y (y H ) to generate the translated MRI image x H ∈ X through the generator G X . Similarly, we can get the the translated CT image ỹ H ∈ Y through the generator G Y (C x , A y ), where C x = E C X (F s (x L ; Θ s ))</formula><p>and A y is also sampled from the prior distribution p(A y ) ∼ N (0, I).</p><p>Disentangled Representation Learning. Cross-modality image translation is based on disentangled representation learning, trained with self-and crosscycle reconstruction losses. As shown in Fig. <ref type="figure" target="#fig_0">1-(C.1, C</ref>.2), the self-reconstruction loss L self is utilized to regularize the training when the content and attribute code originate from the same domain, whereas the cross-cycle consistency loss L cycle is used when the content and attribute code come from different domains. The self-reconstruction and cross-cycle reconstruction losses are defined as follows:</p><formula xml:id="formula_2">L self = G X E C X (x H ), E A X (x H ) -xH 1 + G Y E C Y (y H ), E A Y (y H ) -y H 1 (1) L cycle = G X (E C Y (ỹ H ), E A X (x H )) -xH 1 + G Y (E C X (x H ), E A Y (y H )) -y H 1 (2)</formula><p>where</p><formula xml:id="formula_3">xH = F s (x L ; Θ s ), x H = G X (E C Y (y H ), A x ), ỹ H = G Y (E C X (x H ), A y ).</formula><p>Specially, in the cross-cycle translation processes, we employe a latent reconstruction loss to maintain the invertible mapping between the image and the latent space. In details, we have:</p><formula xml:id="formula_4">L latent = Ĉx -C x 1 + Ĉy -C y 1 + Âx -A x 1 + Ây -A y 1<label>(3)</label></formula><p>We further use pretrained vgg16 network, denoted as φ(•), to extract highlevel features for computing the perceptual loss <ref type="bibr" target="#b17">[17]</ref>:</p><formula xml:id="formula_5">L percep = 1 CHW φ(ỹ H ) -φ(x H ) 2 2 + 1 CHW φ(x H ) -φ(y H ) 2 2<label>(4)</label></formula><p>where C, H, W indicate the channel number and the image size, respectively. Adversarial Learning. As shown in Fig. <ref type="figure" target="#fig_0">1-(A.</ref>2), we use GAN <ref type="bibr" target="#b10">[10]</ref> to learn the translation between MR and CT image domains better. A GAN typically contains a generation network and a discrimination network. We use the discriminator D X to judge whether the image is from MR image domain, and the discriminator D Y to judge whether the image is from CT image domain. The auto-encoders try to generate the image of the target domain to fool the discriminators so that the distribution of the translated images can match that of the target images. The minmax game is trained by:</p><formula xml:id="formula_6">L X adv = E xH ∼PX (xH ) [log D X (x H )] + E yH ∼PY (yH ) log(1 -D X (x H ))<label>(5)</label></formula><formula xml:id="formula_7">L Y adv = E yH ∼PY (yH ) [log D Y (y H )] + E xH ∼PX (xH ) log(1 -D Y (ỹ H ))<label>(6)</label></formula><p>Joint Optimization. The SRNet and the CITNet are jointly optimized by minimizing following loss function:</p><formula xml:id="formula_8">L disentangle = L X adv + L Y adv + λ 1 (L self + L cycle ) + λ 2 L latent + λ 3 L percep (7)</formula><p>where λ 1 , λ 2 , and λ 3 are parameters controlling the relative weights of different losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Strategy</head><p>Empirically, we found that training the network shown in Fig. <ref type="figure" target="#fig_0">1-(A</ref>) end to end did not converge. We thus design the following three-stage training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage 1. Let's denote the downsampling function as D(•).</head><p>In this stage, we pretrain the SRNet using the HR CT images, as shown in Fig. <ref type="figure" target="#fig_0">1-(</ref> further pretrain the SRNet with pseudo MR images, as shown in Fig. <ref type="figure" target="#fig_0">1-(B.</ref>2), for another T iterations. At each iteration, we first sample a batch of LR MR images x L and input them into the SRNet to get the pseudo HR MR images xH = F s (x L ; Θ s ). We then downsample xH to get corresponding pseudo LR MR images xL = D(x H ). The SRNet is trained with the paired pseudo LR-HR MR images by minimizing L1 loss F s (x L ; Θ s ) -F s (D(F s (x L ; Θ s )); Θ s ) 1 . The idea behind such a pretraining stategy is that since both CT and MR images share the common structural information, the model pretrained with CT images in stage 1 facilitates the super-resolution reconstruction of pseudo HR MR images in stage 2. On the other hand, the training done in stage 2 can help the SRNet to learn MRI-specific domain information.</p><p>Stage 3. The MR images generated by the model pretrained at the first two stages can be further improved. In stage 3, we conduct joint optimization of the SRNet and the CITNet as shown in Fig. <ref type="figure" target="#fig_0">1-(C</ref>), for another 8 × T iterations. At each iteration, we first train</p><formula xml:id="formula_9">D X , D Y by maximizing L X adv + L Y adv . We then train E C X , E C Y , E A X , E A Y , G X , G Y</formula><p>and the SRNet by minimizing L disentangle as defined in Eq. <ref type="bibr" target="#b6">(7)</ref>.</p><p>The training procedure of our method is illustrated by Algorithm 1. Implementation Details. To train the proposed network, each training sample is unpaired LR MRI and HR CT images. All images are normalized to the range between -1.0 and 1.0. Optimization is performed using Adam with a batch size of 1. The initial learning rate is set to 0.0001 and decreased by a factor of 5 every 2 epochs. We empirically set λ 1 = 10, λ 2 = λ 3 = 1 and T = 100, 000.</p><p>Table <ref type="table">1</ref>. The mean and the standard deviation when the proposed method was compared with the state-of-the-art (SOTA) unsupervised <ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref> and supervised <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b21">21]</ref> methods on both datasets. Paired T-Tests of all evaluation metrics achieved by ours and other methods are all smaller than 0.0001.    Experimental Results. We compare our method with the conventional algorithm bicubic interpolation, and the state-of-the-art (SOTA) unsupervised SR methods including TSCN <ref type="bibr" target="#b18">[18]</ref>, ZSSR <ref type="bibr" target="#b19">[19]</ref>, SMORE <ref type="bibr" target="#b20">[20]</ref> as well as the SOTA supervised methods including RDN <ref type="bibr" target="#b15">[15]</ref> and ReconResNet <ref type="bibr" target="#b21">[21]</ref>. Well-established metrics including Peak Signal-to-Noise Ratio (PSNR) <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>, Structural Similarity Index Metrics (SSIM) <ref type="bibr" target="#b24">[24]</ref>, and Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b25">[25]</ref> are used to assess the performance of different methods.</p><p>Table <ref type="table">1</ref> shows the mean and the standard deviation of the evaluation results of each method on both datasets. Figure <ref type="figure" target="#fig_2">2</ref> and Fig. <ref type="figure" target="#fig_3">3</ref> respectively show the superresolution results on data from Site1 and Site2, when the scale factor is set as K = 4, as well as the corresponding LR and ground truth (GT) images. Both qualitative and quantitative results demonstrated that our method achieved better results than other SOTA unsupervised SR methods. It achieved comparable performance when compared with the supervised SR methods.</p><p>Our method is trained in two pretrain stages and one joint optimization stage. We thus conduct ablation study on dataset from Site1 to analyze the quality of the generated pseudo HR MR images at each stage. As shown in Table <ref type="table" target="#tab_1">2</ref>, quantitatively, the quality of the generated pseudo HR MR images is become better and better, demonstrating the effectiveness of the training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed a CT-guided, unsupervised MRI super-resolution reconstruction method based on joint cross-modality image translation and super-resolution reconstruction, eliminating the requirement of HR MRI for training. We conducted experiments on two datasets respectively acquired from two different clinical centers to validate the effectiveness of the proposed method. Quantitatively and qualitatively, the proposed method achieved superior performance over the SOTA unsupervised SR methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A schematic illustration of our CT-guided, unsupervised MRI super-resolution reconstruction method. (A) Network architecture, including a SRNet and a CITNet; (B) Pretraining the SRNet; and (C) Joint optimization of the CITNet and the SRNet. Different colors represent different domains, i.e., orange represents the MR domain, green represents the CT domain, and white shows the shared content space. (Color figure online)</figDesc><graphic coords="3,55,98,54,41,340,24,201,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 MethodologyFigure 1</head><label>21</label><figDesc>Figure1presents a schematic illustration of our CT-guided, unsupervised MRI super-resolution reconstruction method. It features two networks: the SRNet and the CITNet (Fig.1-(A)). Figure1-(B)shows how to pretrain the SRNet while Fig.1-(C) presents how to conduct joint optimization. Below we first present the design of the SRNet and the CITNet, followed by a description of the traing strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual comparison of different methods when evaluated on dataset from Site1.</figDesc><graphic coords="7,55,98,321,68,340,18,119,77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual comparison of different methods when evaluated on dataset from Site2.</figDesc><graphic coords="7,55,98,477,98,340,18,97,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of cross-modality image translation between MRI and CT using data from Site2.</figDesc><graphic coords="8,41,79,53,78,340,33,130,93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>B.1), for T iterations. At each iteration, we sample a batch of HR CT images. We then downsample the sampled HR CT images y H to get the paired LR CT images y L = D(y H ). The SRNet is trained with the paired LR-HR CT images by minimizing L1 loss y H -F s (D(y H ); Θ s ) 1 . In this stage, we are aiming to train the SRNet to learn the upsampling kernels.Stage 2. As the SRNet is only pretrained with CT images in stage 1, we need to generalize the learned upsampling kernels to the MR image domain. We thus</figDesc><table><row><cell>Algorithm 1. Training procedure</cell></row><row><cell>(Stage1) Pretrain SRNet with CT based self-supervision:</cell></row><row><cell>GET HR CT images yH</cell></row><row><cell>FOR t = 1 to T</cell></row><row><cell>Train SRNet by minimizing yH -Fs(D(yH ); Θs) 1</cell></row><row><cell>END FOR</cell></row><row><cell>(Stage2) Pretrain SRNet with pseudo MR based self-supervision:</cell></row><row><cell>GET LR MR images xL</cell></row><row><cell>FOR t = T to 2T</cell></row><row><cell>Train SRNet by minimizing Fs(xL; Θs) -Fs(D(Fs(xL; Θs)); Θs) 1</cell></row><row><cell>END FOR</cell></row><row><cell>(Stage3) Joint optimization of CITNet and SRNet:</cell></row><row><cell>GET unpaired LR MR images xL and HR CT images yH</cell></row><row><cell>FOR t = 2T to 10T</cell></row><row><cell>Train DX , DY by maximizing L X adv + L Y adv Train E C X , E C Y , E A X , E A</cell></row></table><note><p>Y , GX , GY and SRNet by minimizing L disentangle END FOR</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of ablation study on dataset from Site1.</figDesc><table><row><cell>Dataset</cell><cell>Site1</cell><cell></cell><cell></cell><cell>Site2</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>PSNR↑</cell><cell>SSIM↑</cell><cell>LPIPS↓</cell><cell>PSNR↑</cell><cell>SSIM↑</cell><cell>LPIPS↓</cell></row><row><cell>Bicubic</cell><cell cols="6">30.67±1.96 0.86±0.03 0.32±0.05 28.69±1.92 0.83±0.03 0.26±0.02</cell></row><row><cell>TSCN [18]</cell><cell cols="6">29.00±2.03 0.83±0.05 0.24±0.03 28.00±1.71 0.85±0.02 0.14±0.01</cell></row><row><cell>ZSSR [19]</cell><cell cols="6">30.95±2.12 0.88±0.03 0.16±0.02 28.94±1.69 0.83±0.03 0.16±0.01</cell></row><row><cell>SMORE [20]</cell><cell cols="6">31.78±1.98 0.89±0.03 0.21±0.03 29.93±2.00 0.86±0.03 0.14±0.02</cell></row><row><cell>Ours</cell><cell cols="6">32.23±1.98 0.90±0.02 0.14±0.02 30.58±1.97 0.88±0.02 0.10±0.01</cell></row><row><cell>Supervised [15]</cell><cell cols="6">32.99±2.07 0.91±0.02 0.10±0.02 31.66±1.72 0.90±0.02 0.08±0.01</cell></row><row><cell cols="7">ReconResNet [21] 32.93±3.12 0.88±0.05 0.09±0.02 29.97±1.50 0.84±0.03 0.07±0.01</cell></row><row><cell cols="6">Stage 1 Stage 2 Stage 3 PSNR↑ √ --31.01±2.18 0.87±0.03 0.20±0.03 SSIM↑ LPIPS↓ √ √ -31.62±2.08 0.89±0.03 0.16±0.02 √ √ √ 32.23±1.98 0.90±0.02 0.14±0.02</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A new sparse representation framework for reconstruction of an isotropic high spatial resolution MR volume from orthogonal anisotropic resolution scans</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1182" to="1193" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Super-resolution methods in MRI: can they improve the trade-off between resolution, signal-to-noise ratio, and acquisition time?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Plenge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1983" to="1993" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Reducing magnetic resonance image spacing by learning without ground-truth. Pattern Recogn</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">108103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Brain MRI super-resolution using deep 3D convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ducournau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fablet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="197" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LRTV: MR image superresolution with low-rank and total variation regularizations</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2459" to="2466" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Super-resolution musculoskeletal MRI using deep learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Chaudhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2139" to="2154" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Brain MRI super resolution using 3d deep densely connected neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Christodoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="739" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient and accurate MRI super-resolution using a generative adversarial network and 3D multilevel densely connected network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Christodoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11070</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00928-1_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00928-1_11" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exploring separable attention for multi-contrast MR image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01664</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-contrast MRI super-resolution via a multi-stage integration network</title>
		<author>
			<persName><forename type="first">C.-M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-1_14" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CT super-resolution GAN constrained by the identical, residual, and cycle learning ensemble (GAN-circle)</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="188" to="203" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">UASSR: unsupervised arbitrary scale super-resolution reconstruction of single anisotropic 3D images via disentangled representation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_43" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="453" to="462" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised imageto-image translation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46475-6_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46475-6_43" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Two-stage self-supervised cycleconsistency network for reconstruction of thin-slice MR images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-1_1" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">zero-shot super-resolution using deep internal learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3118" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Smore: a self-supervised anti-aliasing and super-resolution algorithm for MRI using deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Dewey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Calabresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Reich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="805" to="817" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ReconResNet: regularised residual learning for MR image reconstruction of undersampled cartesian and radial data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page">105321</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An arbitrary scale super-resolution approach for 3-dimensional magnetic resonance image using implicit neural representation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14476</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Super-resolution reconstruction of single anisotropic 3D MR images using residual convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">392</biblScope>
			<biblScope unit="page" from="209" to="220" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
