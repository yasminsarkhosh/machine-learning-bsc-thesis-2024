<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning</title>
				<funder ref="#_nWGRA6z">
					<orgName type="full">Ministry of Food and Drug Safety</orgName>
				</funder>
				<funder ref="#_jYCEKtQ #_QmfCQKU">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Trade, Industry and Energy</orgName>
				</funder>
				<funder>
					<orgName type="full">Korea Medical Device Development Fund</orgName>
				</funder>
				<funder>
					<orgName type="full">Korea government</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Health &amp; Welfare</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kyungsu</forename><surname>Lee</surname></persName>
							<email>ks_lee@dgist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Daegu Gyeongbuk Institute of Science and Technology</orgName>
								<address>
									<postCode>42988</postCode>
									<settlement>Daegu</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haeyun</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Production Engineering Research Team</orgName>
								<orgName type="institution">Samsung SDI</orgName>
								<address>
									<postCode>17084</postCode>
									<settlement>Yongin</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Georges</forename><surname>El Fakhri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jonghye</forename><surname>Woo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Gordon Center for Medical Imaging</orgName>
								<orgName type="department" key="dep2">Department of Radiology</orgName>
								<orgName type="institution" key="instit1">Massachusetts General Hospital</orgName>
								<orgName type="institution" key="instit2">Harvard Medical School</orgName>
								<address>
									<postCode>02114</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jae</forename><forename type="middle">Youn</forename><surname>Hwang</surname></persName>
							<email>jyhwang@dgist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">Daegu Gyeongbuk Institute of Science and Technology</orgName>
								<address>
									<postCode>42988</postCode>
									<settlement>Daegu</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="539" to="550"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">2553AD9F085AD9BD5E5E74B150E48B1E</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_52</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised Domain Adaptation</term>
					<term>Test-Time Tuning</term>
					<term>Breast Cancer</term>
					<term>Segmentation</term>
					<term>Ultrasound Imaging</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation (UDA) has become increasingly popular in imaging-based diagnosis due to the challenge of labeling a large number of datasets in target domains. Without labeled data, well-trained deep learning models in a source domain may not perform well when applied to a target domain. UDA allows for the use of large-scale datasets from various domains for model deployment, but it can face difficulties in performing adaptive feature extraction when dealing with unlabeled data in an unseen target domain. To address this, we propose an advanced test-time fine-tuning UDA framework designed to better utilize the latent features of datasets in the unseen target domain by fine-tuning the model itself during diagnosis. Our proposed framework is based on an auto-encoder-based network architecture that finetunes the model itself. This allows our framework to learn knowledge specific to the unseen target domain during the fine-tuning phase. In order to further optimize our framework for the unseen target domain, we introduce a re-initialization module that injects randomness into network parameters. This helps the framework to converge to a local minimum that is better-suited for the target domain, allowing for improved performance in domain adaptation tasks. To evaluate our framework, we carried out experiments on UDA segmentation tasks using breast cancer datasets acquired from multiple domains. Our experimental results demonstrated that our framework achieved state-of-the-art performance, outperforming other competing UDA models, in segmenting breast cancer on ultrasound images from an unseen domain, which supports its clinical potential for improving breast cancer diagnosis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, deep learning (DL) methods have demonstrated remarkable performance in detecting and localizing tumors on ultrasound images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">27]</ref>. Compared with conventional image processing methods, DL methods provide an accurate feature extraction capability on ultrasound images, despite their low resolution and noise disturbance, leading to superior segmentation accuracy <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref>. However, there are some limitations in developing a DL model in a source domain and deploying it in an unseen target domain. The primary limitation is that DL models require a large number of training samples to achieve accurate predictions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">24]</ref>. Yet, acquiring large training datasets and their corresponding labels, especially from a cohort of patients, can be costly or even infeasible, which poses a significant challenge in developing a DL model with high performance <ref type="bibr" target="#b6">[7]</ref>. Second, even when large-scale datasets are available through collaborative research from multiple sites, DL models trained on such datasets may yield sub-optimal solutions due to domain gaps caused by differences in images acquired from different sites <ref type="bibr" target="#b20">[20]</ref>. Third, due to the small number of datasets from each domain, the images for each individual domain may not capture representative features, limiting the ability of DL models to generalize across domains <ref type="bibr" target="#b2">[3]</ref>.</p><p>Domain adaptation (DA) has been extensively studied to alleviate the aforementioned limitations, the goal of which is to reduce the domain gap caused by the diversity of datasets from different domains <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b33">33]</ref>. Example solutions include transfer learning-and style transfer-based methods. Nonetheless, unlike natural images, generating labels can be a challenging task, making it difficult to apply general DA methods; thus bridging domain gaps by DA methods remains limited <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b33">33]</ref>. This is due to sensitive privacy issues in patients' data, particularly in collaborative research, which restricts access to labels from different domains. As a result, conventional DA methods cannot be easily applied <ref type="bibr" target="#b9">[10]</ref>. More recently, unsupervised domain adaptation (UDA) has been introduced to address this issue <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">33]</ref>, aiming to generate semi-predictions (pseudo-labels) in target domains first, followed by producing accurate predictions using the pseudo-labels. One critical limitation of pseudo-label-based UDA is the possibility of error accumulation due to mispredicted pseudo-labels. This can lead to significant degradation of the performance of DL models, as errors can compound and become more pronounced over time <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">25]</ref>.</p><p>To alleviate the problem of pseudo-label-based UDA, in this work, we propose an advanced UDA framework based on self-supervised DA with a test-time finetuning network. Test-time adaptation methods have been developed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">23]</ref> to improve the learning of knowledge in target domains. The distinctive feature of our test-time self-supervised DA is that it enables the DL network (i) to learn knowledge about the features of target domains by fine-tuning the network itself during the test-time phase, rather than generating pseudo-labels and then (ii) to provide precise predictions on images in target domains, by using the fine-tuned network. Specifically, we adopt self-supervised learning and verify the model via thorough mathematical analysis. Our framework was tested on the task of breast cancer segmentation in ultrasound images, but it could also be applied to other lesion segmentation tasks.</p><p>To summarize, our contributions are three-fold:</p><p>• We design a self-supervised DA framework that includes a parameter search method and provide a mathematical justification for it. With our framework, we are able to identify the best-performing parameters that result in improved performance in DA tasks. • Our framework is effective at preserving privacy, since it carries out DA using only pre-trained network parameters, without transferring any patient data. • We applied our framework to the task of segmenting breast cancer from ultrasound imaging data, demonstrating its superior performance over competing UDA methods.</p><p>Our results indicate that our framework is effective in improving the accuracy of breast cancer segmentation from ultrasound images, which could have potential implications for improving the diagnosis and treatment of breast cancer. Sample batches of (t, ?) ∼ T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><formula xml:id="formula_0">13: t = H • (D S seg ⊕ D FT ) • E (t)) 14:</formula><p>return ŷ 15: End Output: Predictions ( ŷ) on T Fig. <ref type="figure" target="#fig_0">1</ref>. Architecture of our TTFT network (Left) and its pipeline (Right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Test-Time Fine-Tuning (TTFT) Network and Its Pipeline</head><p>Network Architecture. Our proposed TTFT network is based on selfsupervised DA <ref type="bibr" target="#b31">[31]</ref>, which is a part of UDA and can be seen as multi-task learning, involving both the main and pretext tasks, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. In the main task, an encoder (E), a decoder for segmentation (D seg ), and a segmentation header (H) are included. The main task is the segmentation task,</p><formula xml:id="formula_1">(H•D seg •E)(x).</formula><p>In predicting segmentation labels in the target domain (T ), D FT is also involved in the main task, and the final prediction after the fine-tuning is </p><formula xml:id="formula_2">provided by H • (D seg ⊕ D FT ) • E (x),</formula><formula xml:id="formula_3">Θ m S , Θ p S = argmin θ m S ,θ p S s L BCE (H • Dseg • E)(s), s + L GAN (Dgen • E)(s), s ,<label>(1)</label></formula><p>where L BCE and L GAN represent the loss functions for binary cross-entropy and generative adversarial network <ref type="bibr" target="#b5">[6]</ref>, respectively. Θ m S includes E S , D S seg , and H S , while Θ p S includes E S , D S gen , and C S . Additionally, D S seg = D S gen .</p><p>Fine-Tuning in Target Domain. Since the pre-trained model is likely to produce imprecise predictions in T , the model should learn domain knowledge about T . To this end, in the pretext task, for self-supervised learning, the model is fine-tuned in T to generate synthetic images identical to the input images as below:</p><formula xml:id="formula_4">Θ p T = argmin θ p T t L GAN (D S gen • E S )(t), t ⇒ Θ p T ⊇ E S ∪ D S→T gen ,<label>(2)</label></formula><p>where only D gen is fine-tuned to achieve memory efficiency and to decrease the fine-tuning time, and D S gen is fine-tuned as D S→T gen . Then, D S→T gen is transferred to D FT , and knowledge distillation via self-supervised learning is realized. Hence, the precise predictions in T could be provided by</p><formula xml:id="formula_5">H • (D S seg ⊕ D T FT ) • E (x).</formula><p>Benefits of Our Dual-Pipeline. Due to the symmetric property of mutual information in information entropy (H), we have</p><formula xml:id="formula_6">I(X; Y ) = H(X) + H(Y ) - H(X, Y ).</formula><p>As a result, the predictions made by the fine-tuned network in the target domain (T ) lead to reduced entropy, as shown below:</p><formula xml:id="formula_7">H (H • (D S seg ⊕ D T FT ) • E)(t), t ≤ H (H • D S seg • E)(t), t + H (H • D T FT • E)(t), t .</formula><p>(</p><p>Since D S seg is fully optimized for S in a supervised manner, it guarantees a baseline segmentation performance. Furthermore, since D T FT is fine-tuned in T </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Parameter Fluctuation: Parameter Randomization Method</head><p>Since the loss function and its values can vary based on the distribution of inputs, and different domains can have different distributions, the local minimum identified in the source domain (S) cannot be considered as the same local minimum in T , as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. The y-axis of Fig. <ref type="figure" target="#fig_1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>indicates 1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|X |</head><p>x L(M (x; θ), x), and the local minimum is different in S and T as Θ S in Fig. <ref type="figure" target="#fig_1">2a</ref> and<ref type="figure">Θ</ref> T in Fig. <ref type="figure" target="#fig_1">2c</ref>, respectively. A longer fine-tuning time is required to re-position Θ S to Θ T as in Fig. <ref type="figure" target="#fig_1">2c</ref> than to re-position θ T to Θ T . Therefore, efficient fine-tuning is necessary to re-position the local minimum in Fig. <ref type="figure" target="#fig_1">2b</ref> and this process is known as parameter fluctuation. Note that the parameter fluctuation is followed by the fine-tuning step. Suppose C i be the i th convolution operator in D seg with weight w i , then</p><formula xml:id="formula_9">C i (x) = w i • x. Since D S</formula><p>seg provides the baseline segmentation performance, D T F T should provide similar feature maps to achieve the baseline performance. To this end, the mid-feature maps generated should be similar, i.e.,</p><formula xml:id="formula_10">∀ i C i (F i ) ≈ C i (F i ),</formula><p>where C i represents the convolution in D T F T , F i represents i th feature map, and</p><formula xml:id="formula_11">F 0 = E(x). Suppose ∀ i |C i (F i ) -C i (F i )| &lt; i 1, such that ∀ i F i ≈ F i by mathematical induction. Therefore, the sum of errors ( |C i (F i ) -C i (F i )|) is approximated by |w i F 0 -w i F 0 | iff ∀ i F i ≈ F i ,</formula><p>which can be expressed as:</p><formula xml:id="formula_12">|wiF0 -w i F0| &lt; 1 ⇐ |wiF0 -w i F0| ≈ 0 ⇔ |wi -w i | = 0.<label>(4)</label></formula><p>Here, we denote w i -w i = f i as the fluctuation vector in the vector space, and the condition f i = 0 indicates that the sum of the fluctuation vectors should be zero under the condition of |f i | &lt; r 1. Hence, we achieve the condition for the parameter fluctuation that the centers of parameters of Θ S and θ T should be the same in the vector space, and the length of the fluctuation vector should be less than a certain small threshold (0 &lt; r 1). Therefore, the parameter fluctuation aims to add random vectors of which length is less than 0 &lt; r 1 on the parameters of Θ S , and the sum of vectors should be zero. To summarize, the parameter fluctuation aims to add randomness on Θ S as follows:</p><formula xml:id="formula_13">θT = {wi + fi| wi ∈ ΘS , fi = 0, 0 &lt; |fi| &lt; r 1}.</formula><p>(5)</p><p>3 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Set-Ups</head><p>To evaluate the segmentation performance of our TTFT framework, we used three different ultrasound databases: BUS <ref type="bibr" target="#b32">[32]</ref>, BUSI <ref type="bibr" target="#b0">[1]</ref>, and BUV <ref type="bibr" target="#b17">[18]</ref>, which are considered to be different domains. All three databases contain ultrasound imaging data and segmentation masks for breast cancer, with the masks labeled as 0 (background) and 1 (lesion) using a one-hot encoding. The BUS database consists of 163 images along with corresponding labels. The BUSI database contains 780 images, with 133 images belonging to the NORMAL class and having labels containing only 0 values. The BUV database originally consists of ultrasound videos, providing a total of 21,702 frames. While the database also provides labels for the detection task, we processed these labels as segmentation masks using a region growing method <ref type="bibr" target="#b14">[15]</ref>. We employed different deep-learning models for evaluation. Specifically, U-Net <ref type="bibr" target="#b22">[22]</ref> and FusionNet <ref type="bibr" target="#b21">[21]</ref> were employed as our baseline models, since U-Net is a widely used basic model for segmentation, and FusionNet contains advanced residual modules, compared with U-Net. Ours I and Ours II were based on U-Net and FusionNet as the baseline network, respectively. Additionally, MIB-Net <ref type="bibr" target="#b28">[28]</ref>, which is a state-of-the-art model for breast cancer segmentation using ultrasound images, was employed for comparison. Furthermore, CBST <ref type="bibr" target="#b33">[33]</ref> and CT-Net <ref type="bibr" target="#b15">[16]</ref> were employed as the comparison models for UDA methods. As the evaluation metrics, dice coefficient (D. Coef), PRAUC, which is an area under a precision-recall curve, and cohen kappa (κ) were employed <ref type="bibr" target="#b30">[30]</ref>. Our experimental set-ups included: (i) individual databases were used to assess the baseline segmentation performance (Appendix); (ii) the domain adaptive segmentation performance was assessed using the three databases, where two databases were regarded as the source domain, and the remaining database was regarded as the target domain; and (iii) the ablation study was carried out to evaluate the proposed network architecture along with the randomized re-initialization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison Analysis</head><p>Since all compared DL models show similar D. Coef, only UDA performance is comparable as a control in our experiments. In this experiment, two databases were used for training, and the remaining database was used for testing. For instance, BUS in Fig. <ref type="figure" target="#fig_2">3</ref> illustrates the BU S database was used for testing, and  the other two databases of BUSI and BUV were used for training. Figs. <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref> show quantitative results, and Fig. <ref type="figure" target="#fig_4">5</ref> shows the sample segmentation results. Unlike the experiment using the individual database, U-Net, FusionNet, and MIB-Net showed significantly inferior scores due to domain gaps. In contrast, UDA methods of CBST and CT-Net showed superior scores, compared with others, and the scores were not strongly reduced, compared with the experiment with the single database. Note that, our TTFT framework achieved the best performance compared with other DL models. Additionally, Ours II, based on FusionNet, showed the best scores, potentially due to the advanced residual connection module. Furthermore, as illustrated in Fig <ref type="figure" target="#fig_3">4</ref>, our framework provides superior precision scores in a long range of (0, 0.7), indicating that our frameworks estimated unnecessary mispredictions but precise predictions on cancer.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>In this work, we proposed a DL-based segmentation framework for multi-domain breast cancer segmentation on ultrasound images. Due to the low resolution of ultrasound images, manual segmentation of breast cancer is challenging even for expert clinicians, resulting in a sparse number of labeled data. To address this issue, we introduced a novel self-supervised DA network for breast cancer segmentation in ultrasound images. In particular, we proposed a test-time finetuning network to learn domain-specific knowledge via knowledge distillation by self-supervised learning. Since UDA is susceptible to error accumulation due to imprecise pseudo-labels, which can lead to degraded performance, we employed a self-supervised learning-based pretext task. Specifically, we utilized an autoencoder-based network architecture to generate synthetic images that matched the input images. Moreover, we introduced a randomized re-initialization module that injects randomness into network parameters to reposition the network from the local minimum in the source domain to a local minimum that is better suited for the target domain. This approach enabled our framework to efficiently fine-tune the network in the target domain and achieve better segmentation performance. Experimental results, carried out with three ultrasound databases from different domains, demonstrated the superior segmentation performance of our framework over other competing methods. Additionally, our framework is well-suited to a scenario in which access to source domain data is limited, due to data privacy protocols. It is worth noting that we used vanilla U-Net <ref type="bibr" target="#b22">[22]</ref> and FusionNet <ref type="bibr" target="#b21">[21]</ref> as baseline models to evaluate the basic performance of our TTFT framework. However, the use of more advanced baseline models could lead to even better segmentation performance, which is a subject for our future work. Moreover, our proposed framework is not limited to breast cancer segmentation on ultrasound images acquired from different domains. It can also be applied to other disease groups or imaging modalities such as MRI or CT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Test-Time Fine-Tuning Scheme Input: E, H, C, and Dgen = Dseg 1: def Training_on_Source: 2: Sample batches of (s, s) ∼ S 3: Update E and Dseg via L BCE ((H • Dseg • E)(s), s)) Update E and Dgen via L GAN ((Dgen • E)(s), s) 4: return E S and D S seg = D S gen 5: End 6: def Fine_Tuning_on_Target: Sample batches of (t, ?) ∼ T 7: Update D S gen via L GAN (D S gen (E(t)), t), then D S→T gen 8: Share parameters from D S→T gen to D FT 9: return D FT = D S→T seg 10: End 11: def Prediction_on_Target: 12:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the local minimum of the source (a) and target (b) domains and parameter fluctuation (c)</figDesc><graphic coords="5,74,97,53,96,302,08,79,75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison analysis of our framework and comparison models: performance comparison table (Left) and Box-and-Whisker plot (Right).</figDesc><graphic coords="7,60,48,54,14,331,72,197,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Precision-Recall curves by ours and comparison models on each database. Area under the precision-recall curve (PR-AUC) values were reported.</figDesc><graphic coords="7,57,48,298,73,337,36,88,06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Segmentation results by ours and comparison models on each database.</figDesc><graphic coords="8,42,81,53,72,338,11,80,98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Illustration of feature maps: style loss comparison (Left) and a T-SNE plot of generated images by different decoders (Right)</figDesc><graphic coords="8,61,29,170,75,301,24,110,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where ⊕ is the concatenation operation. In the pretext task, E, a decoder for a generator, D gen , and a discriminator, C, are involved. The pretext task aims to generate synthetic images, (D gen • E)(t). Note that D gen and D seg share the same parameters to enable knowledge transfer. However, since the headers of image reconstruction and generating segmentation mask are different (different output), a new header incorporating D F T and D seg is devised and leverages the outputs of two decoders. Besides, D gen = D F T is fine-tuned during the fine-tuning step, and the D F T learns the knowledge of the input domain via image reconstruction. Two distinct knowledge (information) from D seg and D F T enable the network to utilize target domain knowledge and predict precise predictions.Pre-training in Source Domain. The model M is first trained in S in a supervised manner with (s, s) ∼ S in both main and pretext tasks as below:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Dice coefficients by different versions of our TTFT framework. Random Init is DFT is randomly initialized, and Offset indicates DFT is initialized with the value of Dseg added by the offset value.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was partially supported by the <rs type="funder">Korea Medical Device Development Fund</rs> grant funded by the <rs type="funder">Korea government</rs> (the <rs type="institution">Ministry of Science and ICT</rs>, the <rs type="funder">Ministry of Trade, Industry and Energy</rs>, the <rs type="funder">Ministry of Health &amp; Welfare</rs>, the <rs type="funder">Ministry of Food and Drug Safety</rs>) (Project Number: <rs type="grantNumber">1711174564</rs>, <rs type="grantNumber">RS-2022-00141185</rs>). Also, this work was partially supported by the <rs type="programName">Technology Innovation Program</rs>(<rs type="grantNumber">20014214</rs>) funded By the <rs type="institution">Ministry of Trade, Industry &amp; Energy(MOTIE, Korea</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_nWGRA6z">
					<idno type="grant-number">1711174564</idno>
				</org>
				<org type="funding" xml:id="_jYCEKtQ">
					<idno type="grant-number">RS-2022-00141185</idno>
					<orgName type="program" subtype="full">Technology Innovation Program</orgName>
				</org>
				<org type="funding" xml:id="_QmfCQKU">
					<idno type="grant-number">20014214</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_52.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>In order to assess the effectiveness of each of the proposed modules, including the parameter fluctuation and fine-tuning methods, the ablation study was carried out. Since our framework contains three types of decoders, including D S seg , D fl seg , and D S→T seg for the fine-tuning, we mainly targeted those decoders in our ablation study. Table <ref type="table">1</ref> illustrates the quantitative results by different types of decoders. The higher D. coef value (+3.4%) of Pre-train + PF than that of Pre-train + Random Init and Pre-train + Offset confirms the effectiveness of the parameter fluctuation in the UDA performance. Additionally, the higher score (+11%) of Fine-tuning than Pre-train shows an outstanding UDA performance of the fine-tuning pipeline. Furthermore, the simultaneous utilization of the dual pipeline with D S seg and D S→T seg is justified by the scores of Pre-train + Fine-tuning. Using dual-pipeline and parameter fluctuation yielded the best performance. However, the utilization of ensemble pipelines of multiple fine-tuning modules was inefficient, since negligible performance improvements (+0.002) were observed, despite the heavy memory utilization.</p><p>Furthermore, Fig. <ref type="figure">6</ref> shows the effectiveness of the parameter fluctuation and fine-tuning methods. We first compared the similarity of feature-maps by decoders, including D S seg , D fl seg , and D S→T seg , with D S seg and D T seg , which was fully optimized decoder in T . Here, a style loss <ref type="bibr" target="#b8">[9]</ref> was employed to measure the similarity of feature maps. <ref type="bibr">Our</ref>   in S and T are plotted with T-SNE, where the short distance represents the similar features <ref type="bibr" target="#b19">[19]</ref>. The generated images became similar to T in order of D S seg , D fl seg , and D S→T seg , which confirmed the effectiveness of the fine-tuning method in terms of knowledge distillation. Additionally, the parameters were successfully re-positioned from the local minimum in S by parameter fluctuation, which was confirmed by the distances from S to D S gen and D fl gen .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dataset of breast ultrasound images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Al-Dhabyani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fahmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Brief</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">104863</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic semantic segmentation of breast tumors in ultrasound images based on combining fuzzy logic and deep learning-a feasibility study</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Badawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E N A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Hefnawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Zidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Gadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>El-Banby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">251899</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent space regularization for unsupervised domain adaptation in semantic segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Barbato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Toldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zanuttigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2835" to="2845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Source-free domain adaptation for image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bateson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lombaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102617</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep neural networks with intersection over union loss for binary image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Van Beers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lindström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Okafor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Wiering</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="438" to="445" />
		</imprint>
		<respStmt>
			<orgName>ICPRAM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain adaptation for medical image analysis: a survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1173" to="1185" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46475-6_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46475-6_43" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Secure, privacypreserving and federated machine learning in medical imaging</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Kaissis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Makowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rückert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Braren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="305" to="311" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Test-time adaptable neural networks for robust medical image segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Karani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">101907</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An introduction to domain adaptation and transfer learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Kouw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11806</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalize then adapt: source-free domain adaptive semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7046" to="7056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Channel attention module with multi-scale grid average pooling for breast cancer segmentation in an ultrasound image. Ferroelectrics, and Frequency Control</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Ultrasonics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wide-field 3D ultrasound imaging platform with a semi-automatic 3D segmentation algorithm for quantitative analysis of rotator cuff tears</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="65472" to="65487" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation by content transfer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8306" to="8315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploring uncertainty in pseudo-label guided unsupervised domain adaptation. Pattern Recogn</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">106996</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new dataset and a baseline model for breast lesion detection in ultrasound videos</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="614" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_59</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_59" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reducing domain gap by reducing style bias</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8690" to="8699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">FusionNet: a deep fully residual convolutional neural network for image segmentation in connectomics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Jeong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05360</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Uncertainty-guided source-free domain adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19806-9_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19806-9_31" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022: 17th European Conference</title>
		<meeting><address><addrLine>Tel Aviv, Israel; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">October 23-27, 2022. 2022</date>
			<biblScope unit="page" from="537" to="555" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXV</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An overview of gradient descent optimization algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04747</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation through self-supervision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11825</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation in semantic segmentation: a review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Toldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maracani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zanuttigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technologies</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention-enriched deep learning model for breast tumor segmentation in ultrasound images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vakanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Freer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound Med. Biol</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2819" to="2833" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Information bottleneck-based interpretable multitask network for breast cancer classification and segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="page">102687</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Continual test-time domain adaptation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7201" to="7211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Breast lesion detection using an anchor-free network from ultrasound images with segmentation-based enhancement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-supervised domain adaptation for computer vision tasks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="156694" to="156706" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automated breast ultrasound lesions detection using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1218" to="1226" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
