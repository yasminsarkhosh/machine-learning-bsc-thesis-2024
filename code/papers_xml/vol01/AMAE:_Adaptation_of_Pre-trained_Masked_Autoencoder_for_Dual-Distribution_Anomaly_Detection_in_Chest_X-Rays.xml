<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Behzad</forename><surname>Bozorgtabar</surname></persName>
							<email>behzad.bozorgtabar@epfl.ch</email>
							<idno type="ORCID">0000-0002-5759-4896</idno>
							<affiliation key="aff0">
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Lausanne University Hospital (CHUV)</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dwarikanath</forename><surname>Mahapatra</surname></persName>
							<email>dwarikanath.mahapatra@inceptioniai.org</email>
							<idno type="ORCID">0000-0001-9749-7858</idno>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of AI (IIAI)</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">United Arab Emirates</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean-Philippe</forename><surname>Thiran</surname></persName>
							<email>jean-philippe.thiran@epfl.ch</email>
							<idno type="ORCID">0000-0003-2938-9657</idno>
							<affiliation key="aff0">
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Lausanne University Hospital (CHUV)</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="195" to="205"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">108E70BE176EDA842F247560B8325D0C</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_19</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Anomaly detection</term>
					<term>Chest X-ray</term>
					<term>Masked autoencoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised anomaly detection in medical images such as chest radiographs is stepping into the spotlight as it mitigates the scarcity of the labor-intensive and costly expert annotation of anomaly data. However, nearly all existing methods are formulated as a one-class classification trained only on representations from the normal class and discard a potentially significant portion of the unlabeled data. This paper focuses on a more practical setting, dual distribution anomaly detection for chest X-rays, using the entire training data, including both normal and unlabeled images. Inspired by a modern self-supervised vision transformer model trained using partial image inputs to reconstruct missing image regions-we propose AMAE, a two-stage algorithm for adaptation of the pre-trained masked autoencoder (MAE). Starting from MAE initialization, AMAE first creates synthetic anomalies from only normal training images and trains a lightweight classifier on frozen transformer features. Subsequently, we propose an adaptation strategy to leverage unlabeled images containing anomalies. The adaptation scheme is accomplished by assigning pseudo-labels to unlabeled images and using two separate MAE based modules to model the normative and anomalous distributions of pseudo-labeled images. The effectiveness of the proposed adaptation strategy is evaluated with different anomaly ratios in an unlabeled training set. AMAE leads to consistent performance gains over competing self-supervised and dual distribution anomaly detection methods, setting the new state-of-the-art on three public chest X-ray benchmarks -RSNA, NIH-CXR, and VinDr-CXR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To reduce radiologists' reading burden and make the diagnostic process more manageable, especially when the number of experts is scanty, computer-aided diagnosis (CAD) systems, particularly deep learning-based anomaly detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22]</ref>, have witnessed the flourish due to their capability to detect rare anomalies for different imaging modalities including chest X-ray (CXR). Nonetheless, unsupervised anomaly detection methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref> are strongly preferred due to the difficulties of highly class-imbalanced learning and the tedious annotation of anomaly data for developing such systems. Most current anomaly detection methods are formulated as a one-class classification (OCC) problem <ref type="bibr" target="#b17">[18]</ref>, where the goal is to model the distribution of normal images used for training and thus detect abnormal cases that deviate from normal class at test time. On this basis, image reconstruction based, e.g., autoencoder <ref type="bibr" target="#b8">[9]</ref> or generative models <ref type="bibr" target="#b19">[20]</ref>, self-supervised learning (SSL) based, e.g., contrastive learning <ref type="bibr" target="#b25">[26]</ref>, and embedding-similarity-based methods <ref type="bibr" target="#b6">[7]</ref> have been proposed for anomaly detection. Some recent self-supervised methods proposed synthetic anomalies using cut-and-paste data augmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref> to approximate real sub-image anomalies. Nonetheless, their performances lag due to the lack of real anomaly data. More importantly, these methods have often ignored readily available unlabeled images. More recently, similar to our method, DDAD <ref type="bibr" target="#b2">[3]</ref> leverages readily available unlabeled images for anomaly detection, but it requires training an ensemble of several reconstruction-based networks. Self-supervised model adaptation on unlabeled data has been widely investigated using convolutional neural networks (CNNs) in many vision tasks via self-training <ref type="bibr" target="#b16">[17]</ref>, contrastive learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>, and anatomical visual words <ref type="bibr" target="#b9">[10]</ref>. Nonetheless, the adaptation of vision transformer (ViT) <ref type="bibr" target="#b7">[8]</ref> architectures largely remains unexplored, particularly for anomaly detection. Recently, masked autoencoder (MAE) <ref type="bibr" target="#b10">[11]</ref> based models demonstrated great scalability and substantially improved several selfsupervised learning benchmarks <ref type="bibr" target="#b26">[27]</ref>.</p><p>In this paper, inspired by the success of the MAE approach, we propose a two-stage algorithm for "Adaptation of pre-trained Masked AutoEncoder" (AMAE) to leverage simultaneously normal and unlabeled images for anomaly detection in chest X-rays. As for Stage 1 of our method, (i) AMAE creates synthetic anomalies from only normal training images, and the usefulness of pretrained MAE <ref type="bibr" target="#b10">[11]</ref> is evaluated by training a lightweight classifier using a proxy task to detect synthetic anomalies. (ii) For the Stage 2, AMAE customizes the recipe of MAE adaptation based on an unlabeled training set. In particular, we propose an adaptation strategy based on reconstructing the masked-out input images. The rationale behind the proposed adaptation strategy is to assign pseudo-labels to unlabeled images and train two separate modules to measure the distribution discrepancy between normal and pseudo-labeled abnormal images. (iii) We conduct extensive experiments across three chest X-ray datasets and verify the effectiveness of our adaptation strategy in apprehending anomalous features from unlabeled images. In addition, we evaluate the model with different anomaly ratios (ARs) in an unlabeled training set and show consistent performance improvement with increasing AR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Notation. We first formally define the problem setting for the proposed dualdistribution anomaly detection. Contrary to previous unsupervised anomaly detection methods, AMAE fully uses unlabeled images, yielding a training data T train = T n ∪ T u consisting of both normal T n and unlabeled T u training sets. We denote the normal training set as T n = {x ni } N i=1 , with N normal images, and the unlabeled training set as T u = {x ui } M i=1 , with M unlabeled images to be composed of both normal and abnormal images. At test time, given a test set T test = {(x ti , y i )} S i=1 with S normal or abnormal images, where y i ∈ {0, 1} is the corresponding label to x ti (0 for normal (negative) and 1 for abnormal (positive) image), the trained anomaly detection model should identify whether the test image is abnormal or not.</p><p>Architecture. Our architecture is &lt;-shaped: the ViT-small (ViT-S/16) <ref type="bibr" target="#b7">[8]</ref> encoder f followed by a ViT head g and lightweight (3-layer) multilayer perception (MLP) based projection head h, simultaneously. Starting from the pretrained MAE on 0.3M unlabeled chest X-rays and officially released checkpoints, we use exactly the same ViT encoder f and decoder g as MAE <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Stage 1-Proxy Task to Detect Synthetic Anomalies</head><p>AMAE starts the first training stage using only normal training images by defining a proxy task to detect synthetic anomalies shorn of real known abnormal images. For this purpose, we utilize the state-of-the-art (SOTA) anatomy-aware cut-and-paste augmentation, AnatPaste <ref type="bibr" target="#b18">[19]</ref>, to create synthetic anomalies from only a set of normal training images T n . AnatPaste integrates an anatomical mask x mask created from unsupervised lung region segmentation, which guides generating anomalous images via cutting a patch from a normal chest radiograph x n and randomly pasting it at another image location x paste as:</p><formula xml:id="formula_0">Aug (x n ) = x n * (1 -x mask ) + x paste * x mask (1)</formula><p>where Aug (•) is the AnatPaste augmentation (see <ref type="bibr" target="#b18">[19]</ref> for more details). Given a normal training set T n , for each normal image x n ∼ T n , we create a synthetic anomaly, denoted as Aug (x i ). In preparation for input to the frozen ViT encoder, f 0 (obtained by MAE pre-training), each input image with the h × w spatial resolution is split into T = (h/p) × (w/p) patches of size p × p. Then, for every input patch, a token is created by linear projection with an added positional embedding. The sequence of tokens is then fed to the frozen ViT encoder f 0 consisting stack of transformer blocks, yielding the embeddings of tokens z 1  i , z 2 i ∈ R T ×d corresponding to i th normal and synthetic anomaly images. The returned embeddings z 1  i , z 2 i ∈ R T ×d are pooled via average pooling to form d-dimensional embeddings, which are fed to an MLP anomaly classifier projection head h (see Fig. <ref type="figure" target="#fig_0">1</ref> for schematic overview). Subsequently, we only train an anomaly classifier projection head h on top of the frozen embeddings to detect synthetic anomalies using the cross-entropy loss l ce as follows:</p><formula xml:id="formula_1">h 0 = arg min h E xn∼Tn [l ce (h • f 0 (x n ) , 0) + l ce (h • f 0 (Aug (x n )) , 1)] (2)</formula><p>We set the label for the normal image to 0 and 1 otherwise (synthetic anomaly). The above gradient-based optimization produces a trained classifier projection head h 0 . Thus, the whole architecture can be trained with much fewer parameters while making only the classifier projection head specialized at recognizing anomalies without influencing the ViT encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stage 2-MAE Inter-Discrepancy Adaptation</head><p>The proposed MAE adaptation scheme is inspired by <ref type="bibr" target="#b2">[3]</ref> to model the dual distribution of training data. Unlike <ref type="bibr" target="#b2">[3]</ref>, which treats all unlabeled images similarly, we propose assigning pseudo-labels to unlabeled images and formulating anomaly detection by measuring the distribution discrepancy between normal and pseudo-labeled abnormal images from unlabeled sets. We use a pre-trained anomaly classifier (Stage 1) to assign pseudo labels to unlabeled images. To begin with, for each unlabeled image x u ∼ T u , we consider the anomaly detection model's confidence from Stage 1 (h 0 • f 0 (x u )). Those images on which the model is highly confident (normal or abnormal) are treated as reliable images used for adaptation. For this purpose, we collect all output probabilities and opt for a threshold per class t c corresponding to each class's top K-th percentile (K = 50) of all given confidence values. Those unlabeled images deemed reliable yield two subsets: a subset of pseudo-labeled normal images T un and a subset of pseudo-labeled abnormal images T ua . We then utilize two MAE-based modules, Module A and Module B (see Fig. <ref type="figure" target="#fig_1">2</ref>, Top), using the same MAE architecture and pixel-wise mean squared error (MSE) optimization in <ref type="bibr" target="#b10">[11]</ref>. Within each module, the input patches for each image are masked out using a set of L random masks m (j) ∈ {0, 1}</p><formula xml:id="formula_2">T L j=1</formula><p>in which a different small subset of the patches (ratio of 25%) is retained each time to be fed to the ViT encoder f . The lightweight ViT decoder g receives unmasked patches' embeddings and adds learnable masked tokens to replace the masked-out patches. Subsequently, the full set of embeddings of visible patches and masked tokens with added positional embeddings to all tokens is processed by the ViT decoder g to reconstruct the missing patches of each image in pixels. This yields the reconstructed image x(j) = g • f m (j) (x) , which is then compared against the input image x to optimize both ViT encoder </p><formula xml:id="formula_3">E x∼Ttrain ⎡ ⎣ 1 L L j=1 l mse m (j) x(j) , m (j) (x) ⎤ ⎦<label>(3)</label></formula><p>All pixels in the t th patch of both input image and reconstructed images are multiplied by m (j) t ∈ {0, 1}. The above self-supervised loss term averages L pixel-wise mean squared errors for each image. Module A is trained on a combination of the normal training set T n and pseudo-labeled normal images from the unlabeled set T un . In contrast, Module B is trained using only pseudo-labeled abnormal images from an unlabeled set T ua . Optimization for Eq. 3 always starts from pre-trained f 0 and g 0 , and we reset the MAE weights to f 0 and g 0 before training each module. A high discrepancy between the reconstruction outputs of the two modules can indicate potential abnormal regions. Similar to the training stage, we apply L random masks to the test image x t ∼ T test to obtain L reconstructions (see Fig. <ref type="figure" target="#fig_1">2</ref>, Bottom). Thus, the anomaly score based on the inter-discrepancy of the two MAE modules is computed as follows:</p><formula xml:id="formula_4">A p inter = |μ p A -μp B | (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where p is the index of pixels, μA and μB are the mean maps of L reconstructed images from Module A and Module B, respectively. The pixel-level anomaly scores for each image are averaged, yielding the image-level anomaly score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets. We evaluated our method on three public CXR datasets: 1) the RSNA Pneumonia Detection Challenge dataset 1 , 2) the VinBigData Chest X-ray Abnormalities Detection Challenge dataset (VinDrCXR) 2 <ref type="bibr" target="#b14">[15]</ref>, and a subset of 3) the curated NIH dataset (NIH-CXR) 3 <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>, by including only posteroanterior view images of both male and female patients aged over 18. We show a summary of each dataset's repartitions in Table <ref type="table" target="#tab_0">1</ref>. Except for NIH, where we use only the normal set T n (OCC setting), for the other two datasets, we utilize both T n and T u and exact repartition files from <ref type="bibr" target="#b2">[3]</ref> for model training.</p><p>1 https://www.kaggle.com/c/rsna-pneumonia-detection-challenge. Implementation Details. We adopt AdamW <ref type="bibr" target="#b12">[13]</ref> optimizer and set the learning rate (lr) and batch size to 2.5e-4 and 16, where we linearly warm up the lr for the first 20 epochs and decay it following a cosine schedule thereafter till 150 epochs. We follow the exact recipe as <ref type="bibr" target="#b26">[27]</ref> for other hyperparameters (see Supplementary Material). The number of generated masks L per image is set to 2 and 4 for the adaptation and test stages (see ablation in Supplementary Material). We use PyTorch 1.9 <ref type="bibr" target="#b15">[16]</ref> and train each model on a single GeForce RTX 2080 Ti GPU. We use the area under the ROC curve (AUC) and average precision (AP) for the evaluation metrics.</p><p>Comparison with SOTA Methods. Table <ref type="table" target="#tab_1">2</ref> compares AMAE with a comprehensive collection of SOTA methods, including self-supervised synthetic anomaly and reconstruction (Rec.) based methods using their official codes and under two experimental protocols. We use the Y protocol to indicate if access to the unlabeled images is possible in which an AR of 60% of T u is assumed in the experiments; otherwise, we use N. Under protocol N (OCC setting), except for VinDr-CXR, AMAE-Stage 1 achieves SOTA results on two CXR benchmarks, demonstrating the effectiveness of pre-trained ViT using MAE and synthetic anomalies. In particular, AMAE-Stage 1 surpasses the best-performing synthetic anomaly-based method, AnatPaste <ref type="bibr" target="#b18">[19]</ref>, with the same synthesis approach as ours but using ResNet18 as a feature extractor. ), e.g., improved AUC from 89% to 92% on AR=80%, suggesting high-quality pseudo-labeled images. We also analyze the discriminative capability of our adaptation with and without pseudo labeling by levering all unlabeled images in Module B. We utilize the χ 2-distance between the histograms of anomaly scores (AS) of normal and abnormal images in the RSNA test set (Fig. <ref type="figure" target="#fig_2">3</ref> (b)), showing a more substantial discriminative capability of incorporating pseudo labeling (improved χ 2-distance from 38.53 to 58.37). Finally, the ViT encoder obtained by MAE pre-training (Stage 1) surpasses DenseNet-121 (DN121), either pre-trained by MAE <ref type="bibr" target="#b26">[27]</ref> or an advanced contrastive learning method (MoCo v2 <ref type="bibr" target="#b4">[5]</ref>) on the RSNA dataset (Fig. <ref type="figure" target="#fig_2">3 (c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We present AMAE, an adaptation strategy of the pre-trained MAE for dual distribution anomaly detection in CXRs, which makes our method capable of more effectively apprehending anomalous features from unlabeled images. Experiments on the three CXR benchmarks demonstrate that AMAE is generalizable to different model architectures, achieving SOTA performance. As for the limitation, an adequate number of normal training images is still required, and we will extend our pseudo-labeling scheme in our future work for robust anomaly detection bypassing any training annotations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic overview of AMAE training (Stage 1). Top. Illustration of Anatpaste augmentation [19] generated from normal training images. Bottom. Starting from MAE initialization, only the MLP-based projection head (Proj.) is trained to classify synthetic anomalies.</figDesc><graphic coords="3,59,46,54,32,317,11,167,86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Schematic overview of AMAE training (Stage 2) and test stage. Top. Our adaptation strategy first assigns pseudo labels to unlabeled images using a pretrained classifier from Stage 1 and uses two separate MAE modules to model the normative and anomalous distributions of pseudo-labeled images. Bottom. The discrepancy between the average of multiple reconstructions from two modules is used at test time to compute the anomaly score.</figDesc><graphic coords="5,75,96,59,12,297,73,237,58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Ablations on the RSNA dataset. (a) Performance comparison with a varying AR of Tu. (b) The χ 2-distance of AS histograms with and without pseudo labeling. (c) Ablation for different backbones and pre-training schemes. Test AUC performances are presented as mean ± 1.96std averaged over four replicates. Samples are statistically tested for H0: means are similar, using a bilateral Welch t-test. ns non-significant, ** pvalue ∈ [0.05, 0.01], *** pvalue ∈ [0.01, 0.001].</figDesc><graphic coords="8,243,39,54,23,136,36,143,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of dataset repartitions. Unlabeled image set Tu is constructed from the images presented in parentheses without using their annotations.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Normal training set Tn Repartition</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Unlabeled training set Tu</cell><cell>Test set Ttest</cell></row><row><cell>RSNA</cell><cell>3851</cell><cell cols="2">4000 (4000 normal + 5012 abnormal) 1000 normal + 1000 abnormal images</cell></row><row><cell cols="2">VinDr-CXR 4000</cell><cell cols="2">4000 (5606 normal + 3394 abnormal) 1000 normal + 1000 abnormal images</cell></row><row><cell>NIH-CXR</cell><cell>3614</cell><cell>0</cell><cell>543 normal + 262 abnormal images</cell></row><row><cell cols="2">f and decoder g:</cell><cell></cell><cell></cell></row><row><cell>f</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* , g * = arg min f,g</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison with SOTA methods (Avg. over four replicate). The two best results for each protocol are highlighted in bold and underlined. Note that "IN" refers to "ImageNet-Pretrained," and "e2e" refers to end-to-end training. The experimental results of competing methods † are taken from<ref type="bibr" target="#b3">[4]</ref>.</figDesc><table><row><cell>Method</cell><cell>Protocol</cell><cell>Taxonomy</cell><cell cols="6">CXR Datasest &amp; Metrics RSNA VinDr-CXR NIH-CXR AUC % AP % AUC % AP % AUC % AP %</cell></row><row><cell>AE  †</cell><cell cols="2">N Rec.</cell><cell>66.9</cell><cell>66.1</cell><cell>55.9</cell><cell>60.3</cell><cell>70.0</cell><cell>65.4</cell></row><row><cell>AE-U  † [14]</cell><cell cols="2">N Rec.</cell><cell>86.7</cell><cell>84.7</cell><cell>73.8</cell><cell>72.8</cell><cell>91.0</cell><cell>83.2</cell></row><row><cell>f-AnoGAN  † [20]</cell><cell cols="2">N Rec.</cell><cell>79.8</cell><cell>75.6</cell><cell>76.3</cell><cell cols="2">74.8 84.1</cell><cell>79.2</cell></row><row><cell>IGD  † [6]</cell><cell cols="2">N Rec.</cell><cell>81.2</cell><cell>78.0</cell><cell>59.2</cell><cell>58.7</cell><cell>85.2</cell><cell>80.3</cell></row><row><cell>DRAEM  † [28]</cell><cell cols="3">N Rec.+SSL 62.3</cell><cell>61.6</cell><cell>63.0</cell><cell>68.3</cell><cell>65.5</cell><cell>62.4</cell></row><row><cell>CutPaste e2e † [21]</cell><cell cols="2">N SSL</cell><cell>55.0</cell><cell>58.0</cell><cell>54.6</cell><cell>55.5</cell><cell>58.0</cell><cell>-</cell></row><row><cell>AnatPaste [19]</cell><cell cols="2">N SSL</cell><cell>83.1</cell><cell>83.7</cell><cell>66.0</cell><cell>66.2</cell><cell>94.0</cell><cell>93.5</cell></row><row><cell>FPI  † [23]</cell><cell cols="2">N SSL</cell><cell>47.6</cell><cell>55.7</cell><cell>48.2</cell><cell>49.9</cell><cell>70.5</cell><cell>-</cell></row><row><cell>PII  † [24]</cell><cell cols="2">N SSL</cell><cell>82.9</cell><cell>83.6</cell><cell>65.9</cell><cell>65.8</cell><cell>92.2</cell><cell>-</cell></row><row><cell>NSA  † [21]</cell><cell cols="2">N SSL</cell><cell>82.2</cell><cell>82.6</cell><cell>64.4</cell><cell>65.8</cell><cell>94.1</cell><cell>-</cell></row><row><cell>DDAD-AE [3]</cell><cell cols="2">N Rec.</cell><cell>69.4</cell><cell>-</cell><cell>6 0 . 1</cell><cell>-</cell><cell>73.0</cell><cell>71.5</cell></row><row><cell>AMAE IN -Stage 1</cell><cell cols="2">N SSL</cell><cell>83.3</cell><cell>83.8</cell><cell>65.9</cell><cell>66.0</cell><cell>94.1</cell><cell>93.7</cell></row><row><cell>AMAE -Stage 1</cell><cell cols="2">N SSL</cell><cell>86.8</cell><cell cols="2">84.9 74.2</cell><cell>72.9</cell><cell>95.0</cell><cell>94.9</cell></row><row><cell>CutPaste e2e † [21]</cell><cell cols="2">Y SSL</cell><cell>59.8</cell><cell>61.7</cell><cell>59.2</cell><cell>60.0</cell><cell>-</cell></row><row><cell>AnatPaste [19]</cell><cell cols="2">Y SSL</cell><cell>84.4</cell><cell>85.5</cell><cell>67.1</cell><cell>67.5</cell><cell>-</cell></row><row><cell>FPI  † [23]</cell><cell cols="2">Y SSL</cell><cell>46.6</cell><cell>53.8</cell><cell>47.4</cell><cell>49.4</cell><cell>-</cell></row><row><cell>PII  † [24]</cell><cell cols="2">Y SSL</cell><cell>84.3</cell><cell>85.4</cell><cell>66.8</cell><cell>67.2</cell><cell>-</cell></row><row><cell>NSA  † [21]</cell><cell cols="2">Y SSL</cell><cell>84.2</cell><cell>84.3</cell><cell>64.4</cell><cell>64.8</cell><cell>-</cell></row><row><cell>DDAD-AE [3] (Ainer)</cell><cell cols="2">Y Rec.</cell><cell>81.5</cell><cell>81.0</cell><cell>71.0</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">AMAE -Stage 2 (Ainter ) Y Rec.</cell><cell>91.4</cell><cell cols="2">91.7 86.1</cell><cell cols="2">84.5 -</cell></row></table><note><p><p><p><p>2 </p>https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection.</p>3 </p>https://nihcc.app.box.com/v/ChestXray-NIHCC/file/371647823217.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Furthermore, outperforming MAE pre-training on ImageNet (e.g., improved AUC from 65.9% to 74.2% on the VinDr-CXR) indicates the importance of in-domain adaptation. Under the Y protocol, our adaptation strategy, AMAE-Stage 2 (A inter ), outperforms the current SSL methods by a larger margin, underlining the importance of modeling the dual distribution and leveraging unlabeled images more effectively. Furthermore, we consider an additional baseline for model adaptation based on patch reconstruction in<ref type="bibr" target="#b10">[11]</ref> on pooled normal and unlabeled images, denoted as (AMAE -Stage 2 (Mask Rec. )). AMAE -Stage 2 (A inter ) rises with a steeper slope than AMAE -Stage 2 (Mask Rec.</figDesc><table><row><cell>Ablation Studies. To understand the effectiveness of AMAE in capturing</cell></row><row><cell>abnormal features from unlabeled images, we conduct ablation experiments on</cell></row><row><cell>the RSNA with the AR of T u varying from 0 to 100% (Fig. 3 (a)). Concern-</cell></row><row><cell>ing reconstruction-based methods, the baseline aggregating multiple reconstruc-</cell></row><row><cell>tions via MAE achieves consistent improvement compared with the AE baseline</cell></row><row><cell>(+14.6% AUC), implying better capturing of fine-grained texture information.</cell></row><row><cell>With an increasing AR of T u , our MAE adaptation strategy (AMAE -Stage</cell></row><row><cell>2 (A</cell></row></table><note><p><p><p>inter )) performs favorably against the SOTA method (DDAD</p><ref type="bibr" target="#b2">[3]</ref></p>), and AMAE without adaptation (Stage 1).</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_19.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention-conditioned augmentations for selfsupervised anomaly detection and localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bozorgtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahapatra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="14720" to="14728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SALAD: self-supervised aggregation learning for anomaly detection on X-Rays</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bozorgtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Thiran</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-8_46" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="468" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dual-distribution discrepancy for anomaly detection in chest x-rays</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_56" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="page" from="584" to="593" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dual-distribution discrepancy with self-supervised refinement for anomaly detection in medical images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">102794</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep one-class classification via interpolated gaussian descriptor</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PaDiM: a patch distribution modeling framework for anomaly detection and localization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Audigier</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-68799-1_35</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-68799-1_35" />
	</analytic>
	<monogr>
		<title level="m">ICPR 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12664</biblScope>
			<biblScope unit="page" from="475" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transferable visual words: exploiting the semantics of anatomical patterns for self-supervised learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R H</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2857" to="2868" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cutpaste: Self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9664" to="9674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Abnormality detection in chest X-ray images using uncertainty prediction autoencoders</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_51</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-2_51" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="529" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">VinDr-CXR: an open dataset of chest x-rays with radiologist&apos;s annotations</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">429</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pytorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kartik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8558" to="8567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Anatomy-aware self-supervised learning for anomaly detection in chest radiographs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.04282</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">f-AnoGan: fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Natural synthetic anomalies for self-supervised anomaly detection and localization</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19821-2_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19821-2_27" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022: 17th European Conference</title>
		<meeting><address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">October 23-27, 2022. 2022</date>
			<biblScope unit="page" from="474" to="489" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXXI</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-taught semi-supervised anomaly detection on upper limb x-rays</title>
		<author>
			<persName><forename type="first">A</forename><surname>Spahr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bozorgtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1632" to="1636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04197</idno>
		<title level="m">Detecting outliers with foreign patch interpolation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting outliers with poisson image interpolation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_56" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="581" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automated abnormality classification of chest radiographs using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Med</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Constrained contrastive distribution learning for unsupervised anomaly detection and localisation in medical images</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Tain</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_13" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="128" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Delving into masked autoencoders for multilabel thorax disease classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3588" to="3600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Draem-a discriminatively trained reconstruction embedding for surface anomaly detection</title>
		<author>
			<persName><forename type="first">V</forename><surname>Zavrtanik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Skočaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8330" to="8339" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
