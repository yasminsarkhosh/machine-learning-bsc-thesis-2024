<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI</title>
				<funder ref="#_pyCduBs #_zxnjb4b">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Rajen Kilachand Fund for Integrated Life Science and Engineering</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Boran</forename><surname>Hao</surname></persName>
							<idno type="ORCID">0000-0001-7922-0513</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoyao</forename><surname>Shen</surname></persName>
							<idno type="ORCID">0000-0002-5479-8332</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechanical Engineering and the Photonics Center</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruidi</forename><surname>Chen</surname></persName>
							<idno type="ORCID">0000-0002-1508-1742</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chad</forename><forename type="middle">W</forename><surname>Farris</surname></persName>
							<idno type="ORCID">0000-0002-1133-3834</idno>
							<affiliation key="aff2">
								<orgName type="department">Boston Medical Center</orgName>
								<orgName type="institution">Boston University Chobanian &amp; Avedisian School of Medicine</orgName>
								<address>
									<postCode>02118</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephan</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
							<idno type="ORCID">0000-0002-5367-7459</idno>
							<affiliation key="aff2">
								<orgName type="department">Boston Medical Center</orgName>
								<orgName type="institution">Boston University Chobanian &amp; Avedisian School of Medicine</orgName>
								<address>
									<postCode>02118</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
							<idno type="ORCID">0000-0002-4413-5084</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechanical Engineering and the Photonics Center</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioannis</forename><forename type="middle">Ch. </forename><surname>Paschalidis</surname></persName>
							<idno type="ORCID">0000-0002-3343-2913</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="768" to="777"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">7BF2CF694B2E5274260DCA2D7D0A45AD</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_74</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Stroke diagnosis</term>
					<term>MRI acceleration</term>
					<term>Distributionally robust optimization</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Magnetic Resonance Imaging (MRI) acceleration techniques using k-space sub-sampling (KS) can greatly improve the efficiency of MRI-based stroke diagnosis. Although Deep Neural Networks (DNN) have shown great potential on stroke lesion recognition tasks when the MR images are reconstructed from the full k-space, they are vulnerable to the lower quality MR images generated by KS. In this paper, we propose a Distributionally Robust Learning (DRL) approach to improve the performance of stroke recognition DNN models when the MR images are reconstructed from the sub-sampled k-space. For Convolutional Neural Network (CNN) and Vision Transformer (ViT)-based models, our methods improve the stroke classification AUROC and AUPRC by up to 11.91% and 9.32% on the KS-perturbed brain MR images, respectively, compared against Empirical Risk Minimization (ERM) and other baseline defensive methods. We further show that DRL models can successfully recognize the stroke cases from highly perturbed MR images where clinicians may fail, which provides a solution for improved diagnosis in an accelerated MRI setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Magnetic Resonance Imaging (MRI) has been extensively applied to clinical diagnosis <ref type="bibr" target="#b15">[16]</ref>. Compared with Computed Tomography (CT), a brain MRI is more sensitive for multiple stroke types <ref type="bibr" target="#b2">[3]</ref>, therefore considered as the gold standard for stroke diagnosis. Nevertheless, the long acquisition time for a brain MRI (20 to 30 min) imposes challenges, especially in cases of acute stroke where rapid diagnosis is essential and patient movement during this distressing period of time commonly limits evaluation. As a result, MRI acceleration techniques have been developed to achieve more rapid diagnosis, increasing resource availability while reducing costs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>. A k-space sub-sampling (KS) approach serves as a simple MRI acceleration solution <ref type="bibr" target="#b19">[20]</ref>, compared with other hardware-based acceleration methods. However, the signal loss by KS leads to blurry reconstructed MR images that are less than ideal for a reliable clinical diagnosis.</p><p>Artificial Intelligence (AI) plays an increasingly important role in MRI-based diagnosis, for both MR image reconstruction and clinical decision making. Deep Neural Networks (DNN) were trained to reconstruct the MR images from the sub-sampled k-space <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref>, which provides a better reconstruction than the Inverse Fast Fourier Transform (IFFT). Nevertheless, detailed information in the brain may still be lost in the reconstructed MR images due to the signal sparsity in the k-space. On the other hand, traditional Convolutional Neural Network (CNN) <ref type="bibr" target="#b14">[15]</ref> and the latest Vision Transformer (ViT)-based <ref type="bibr" target="#b5">[6]</ref> predictive models have shown impressive prediction accuracy on stroke diagnosis tasks, such as slice classification and lesion segmentation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>. However, these DNNs trained on clean images through Empirical Risk Minimization (ERM) are vulnerable to perturbations in the input images <ref type="bibr" target="#b1">[2]</ref>. Whatever the reconstruction method used, even the slightest perturbation in accelerated MR images can lead to a wrong stroke prediction from the AI models. Therefore, building robust DNN models to handle the perturbed MR image input is important for MRI acceleration.</p><p>In this paper, we introduce a Distributionally Robust Learning (DRL)-based approach <ref type="bibr" target="#b3">[4]</ref> into the deep MR image classifier training, in order to improve the model robustness to the image perturbation resulting from the signal sparsity in accelerated MRI. Compared with ERM, DRL is an optimization method minimizing the worst-case loss over an ambiguity set, therefore, can tolerate outliers in the data <ref type="bibr" target="#b4">[5]</ref>. We implemented DRL to different linear layers in deep CNN/ViT classifiers, and applied a randomized training approach to improve the training efficiency. Our results show that on a real-world dataset, DRL can significantly improve the stroke classification performance of ERM and other baseline defensive training methods, when the signal sparsity and noise in accelerated MRI are generated by the Cartesian Undersampling (CU) method <ref type="bibr" target="#b19">[20]</ref> and White Gaussian Noise (WGN). We further show that in highly perturbed MR images where the ERM model and even clinicians cannot give a reliable diagnosis, our DRL model can still correctly recognize stroke, which establishes that our method can assist accelerated MRI diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distributionally Robust Learning</head><p>We will use the DRL framework under a multi-class classification setting developed by <ref type="bibr" target="#b3">[4]</ref> in a DNN-based stroke diagnosis application. We provide a brief overview of the DRL model. Assume that there are K classes, and our goal is to classify an example with an input feature x ∈ R d to one of the K classes with a one-hot class label y ∈ {0, 1} K . Logistic regression solves this problem by minimizing the following expected true risk</p><formula xml:id="formula_0">inf B E P * h B (x, y)],<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">B [w 1 • • • w K ]</formula><p>is the coefficient matrix, P * is the true distribution of the data (x, y), h B (x, y) log 1 e B x -y B x is the loss function to be minimized, and E P * denotes the expectation under the distribution P * . Since P * is usually unknown, Problem (1) cannot be solved directly. The ERM approach tackles this by replacing the expected true risk by a sample averaged risk. Given N realizations of (x, y), ERM minimizes the following empirical risk</p><formula xml:id="formula_2">inf B 1 N N i=1 h B (x i , y i ).<label>(2)</label></formula><p>ERM can produce unreliable solutions when the samples are contaminated by noise or drawn from an outlying distribution. To obtain robust estimators that can hedge against noise in the training data and generalize well out-of-sample, <ref type="bibr" target="#b3">[4]</ref> proposed the DRL framework under the Wasserstein metric. Specifically, it minimizes the worst-case expected loss over a set of probability distributions</p><formula xml:id="formula_3">inf B sup Q∈Ω E Q h B (x, y) , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where Ω contains a set of probability distributions that are close to the empirical distribution PN measured by the Wasserstein metric, Ω {Q ∈ P(Z) : W 1 (Q, PN ) ≤ }, where Z is the set of possible values for (x, y), P(Z) is the space of all probability distributions supported on Z, is a pre-specified radius of the ambiguity set Ω, PN is the empirical distribution that assigns an equal probability 1/N to each observed sample (x i , y i ), and W 1 (Q, PN ) is the order-1 Wasserstein distance between Q and PN defined as</p><formula xml:id="formula_5">W 1 (Q, PN ) min Π∈P(Z×Z) Z×Z l(z 1 , z 2 ) Π dz 1 , dz 2 ,</formula><p>where Π is the joint distribution of z 1 (x 1 , y 1 ) and z 2 (x 2 , y 2 ) with marginals Q and PN , respectively, and l is a distance metric on the data space.</p><p>An equivalent reformulation (4) of (3) was developed by <ref type="bibr" target="#b3">[4]</ref> when</p><formula xml:id="formula_6">l(z 1 , z 2 ) W 1/2 (x 1 -x 2 ) 2 + M y 1 -y 2 2</formula><p>, where W is a positive semidefinite weight matrix to account for any transformation on the input feature x and can be estimated from data using metric learning (see Sec. 2.2) and with M → ∞:</p><formula xml:id="formula_7">inf B 1 N N i=1 h B (x i , y i ) + 2 1/2 W -1/2 B 2 , (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where</p><formula xml:id="formula_9">W -1/2 B 2 is the induced 2 -norm of W -1/2 B.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DRL for Deep Stroke Diagnosis Networks</head><p>We apply DRL to ViT and CNN-based MR image stroke classification models, in order to enhance their robustness against image perturbations in accelerated MRI. We apply the DRL reformulation (4) to the last layer B, and certain intermediate linear layers in a deep MR image classifier. For a ViT model (cf. Fig. <ref type="figure" target="#fig_0">1</ref>), we apply DRL to the patch projection layer P and the final linear classification layer B, in order to predict if an MR image slice is normal (label 0) or depicts a stroke lesion (label 1). To speed up the training process, during each epoch, we randomly pick one layer L to train while keeping all other layers frozen. A validation set V is used to tune the hyperparameters (e.g., regularization coefficients). To account for the non-linear transformation of the raw image resulted from all layers before L, we solve the following Linear Matrix Inequality (LMI) problem <ref type="bibr" target="#b3">[4]</ref> to estimate a weight matrix W:</p><formula xml:id="formula_10">min W 0 xi∈D T t=1 W 1/2 (φ (t) L (x i ) -φ (t) L (x i )) 2 2 s.t. 1 |S| (i,j)∈S T t=1 W 1/2 (φ (t) L (x i ) -φ (t) L (x j )) 2 2 ≥ c, 1 |S| (i,j)∈S T t=1 W 1/2 (φ (t) L (x i ) -φ (t) L (x j )) 2 2 ≥ c, (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>where xi is the perturbed version of an MR image slice x i , D the training set, S {(i, j)|x i , x j ∈ D, y i = y j }, |S| denotes the cardinality of the set S, φ L is the input to L and φ (t) L is the t-th hidden state (i.e., the vector representation for each instance in the sequence, output by and fed into different layers in ViT) in the sequence φ L of length T , and c is a fixed parameter. T = 1 if L refers to the B layer. The intuition of ( <ref type="formula" target="#formula_10">5</ref>) is that in the transformed feature space, distance between the clean and perturbed version of a slice will be minimized, while slices from different classes (normal and stroke) are sufficiently far away. For a CNN model, we only applied DRL to the final linear layer B.</p><p>We chose two approaches to generate the perturbation in accelerated MRI. Cartesian Undersampling (CU) perturbation <ref type="bibr" target="#b19">[20]</ref> keeps only the central and a few randomly-sampled parts of the k-space; the corresponding reconstructed MR image only keeps the main structural information in the brain and introduces misalignments. A smaller central fraction f used in k-space indicates a larger perturbation. Noise might be introduced during the signal transmission, so we add White Gaussian Noise (WGN) as another type of perturbation, where the standard deviation σ is regarded as the perturbation intensity. To show the strength of DRL, in addition to ERM, we also apply Brute-force Adversarial Training (BAT) <ref type="bibr" target="#b1">[2]</ref> and Projected Gradient Descent (PGD) <ref type="bibr" target="#b11">[12]</ref> as baseline methods. Among all of the current defensive training methods which improve the model robustness against perturbations, BAT and PGD are representative. BAT adds noisy samples into the training set, therefore is simple and widely used. PGD is known to be robust to a wide range of image perturbations, and is considered a state-of-the-art method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Materials and Settings</head><p>Our dataset included MRI brain scans from 226 patients performed at an urban tertiary referral academic medical center that is a comprehensive stroke center. Clinical scans of adult patients aged 18-89 years with recent (acute or subacute) strokes were identified between 1/1/2013 and 1/1/2021 for inclusion in this study via a search of the Philips Performance Bridge. Scans meeting this criteria were downloaded and simultaneously anonymized to preserve patient anonymity and prevent disclosure of protected health information as part of this IRB exempt study. No patient demographic information was retained for the scans, as it was considered to represent an unnecessary risk for accidental release of protected health information. The diffusion weighted images with a gradient of B=1000 were utilized for the analysis (see the Supplement<ref type="foot" target="#foot_0">1</ref> for information about the MRI scanner and parameter settings). Each MR image contains multiple slices, and every slice was annotated as normal or stroke by a board-certified neuroradiologist with a subspecialty certification. Annotation of the strokes was performed on the diffusion weighted images using ITK-SNAP (ver. 3.80) <ref type="bibr" target="#b18">[19]</ref>, and all included MRI examinations were reviewed by the neuroradiologist during the annotation process to ensure that the images were of diagnostic quality without significant motion degradation or other artifacts. To avoid the dependency among the slices from the same subject, we applied a 2-d acquisition during the MR imaging, and implemented a slice-level MR image preprocessing. While the whole dataset includes 4,883 (74.7%) normal slices and 1,650 (25.3%) stroke slices, we further randomly split them into training/validation/test sets using the ratio 80%/10%/10%. For the training set, we implemented data augmentation strategies by rotating or flipping each slice. Finally, the training/validation/test set contains 31,356/653/654 slices, correspondingly. We implemented DRL to both CNN and ViT models. For the CNN model, we used a ResNet-18 <ref type="bibr" target="#b8">[9]</ref> architecture, while for the ViT model, we first pre-trained a 4-layer ViT using a self-supervised pre-training method called Masked Autoencoder (MAE) <ref type="bibr" target="#b7">[8]</ref>, using the T1/T2-weighted brain MR images in the IXI dataset <ref type="bibr" target="#b0">[1]</ref>. MAE pre-training first randomly masks 75% of the image patches in an MRI slice input, and then uses a ViT encoder-decoder architecture to reconstruct the masked MRI patches, in order to learn the dependency among different locations in the brain. After 400 pre-training epochs, an overall satisfying reconstruction result can be observed in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>To evaluate the binary classification performance of different models, we use the Area Under the Receiver Operating Characteristic (AUROC) curve as our main metric. As our dataset is unbalanced, we also considered the Area Under Precision-Recall Curve (AUPRC). We ran the experiments 3 times using different random seeds. The training of our DNNs were implemented on 3 NVIDIA RTX A6000 (48GB VRAM) GPUs, and each DRL training epoch can be completed within 0.03 GPU hours. We used a learning rate of 1 × 10 -5 and batchsize of 128 for DRL training, while no weight decay was applied. To solve the LMI problem in (5), we used SDPT3 v4.0 <ref type="bibr" target="#b16">[17]</ref> as the solver. We set the CU perturbation with the acceleration factor of 4, 6, 8, 12 with the central fraction of 8%, 6%, 4% and 2% in k-space respectively, and the remaining parts were chosen randomly in the peripheral region accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>We show the stroke classification AUROC in Fig. <ref type="figure" target="#fig_2">3</ref>. When the k-space subsampling fraction decreased and the signal became sparser, the performance of both ViT and CNN models trained under ERM dropped significantly, from around 95% to below 80%. DRL significantly improved the AUROC of the ERMbased ViT model from 74.5% to 83.1% when the MR images were under extreme CU perturbation, while only slightly influenced model performance on the clean test set. For WGN, the largest improvement brought to ERM-based ViT model was 16.9%. Although we only applied DRL to the last layer of the CNN model, the improvement against ERM was still remarkable, up to 11.9%/4.9% for CU/WGN. With BAT and PGD adversarial training, the corresponding ViT or CNN models were also improved, though when DRL was combined with BAT and PGD, the model robustness can be further enhanced. Table <ref type="table" target="#tab_0">1</ref> shows the maximum AUROC and AUPRC improvement that DRL can bring to different baseline methods. For ViT and CNN models, the AUROC improvement w.r.t BAT/PGD defensive methods is up to 23.9%/12.2%, respectively. Note that the perturbed MRI samples used to implement BAT were the same as those used by DRL, which shows that DRL is a more effective way to exploit the information in adversarial samples, compared to simply adding the blurry images into the training set. For CU perturbation, our best combined model using DRL improved the AUROC/AUPRC of the ERM model by up to 15%/12.5%, while this improvement under WGN perturbation was up to 18.8%/36.2%. Under CU perturbation, we further show that our DRL model can recognize stroke while clinicians may fail to. In Fig. <ref type="figure" target="#fig_3">4</ref>, the stroke MRI slices from the test sets are under different levels of CU perturbations. For both ERM-and DRLbased ViT models, we maximized the F1 score of the stroke class on the training set to calculate the optimal decision threshold for stroke prediction, in order to balance the precision and recall. When the k-space signal becomes more sparse, the reconstructed MRI slices get more blurry and the lesion areas become less recognizable, even for human eyes. As a result, the ERM model fails to detect stroke under high perturbation levels. Nevertheless, the DRL model can tolerate more intense CU perturbation and recognize stroke slices that may even be misclassified by clinicians, which reveals its value in improving the diagnosis in  an accelerated MRI mode. We verified the effectiveness of our approach on the actual clinical scans acquired for clinical care and not just for research purposes, suggesting that the methods and findings in the current study should be generalizable to routine clinical practice conditions and potentially other types of clinical image-based diagnosis (e.g., brain tumor) as well. In addition, our DRL framework does not necessarily need to be used in isolation, rather it can also be combined with other performance boosting methods in accelerated MRI to further improve them, just like for BAT and PGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this study, we implemented a DRL-based robust learning approach to improve the robustness of deep image classifiers, in order to achieve more accurate stroke classification from brain MR images reconstructed from a sub-sampled k-space.</p><p>Our work can potentially be applied to accelerate and improve time-critical stroke diagnosis. Future work can apply DRL to more MRI diagnosis tasks (e.g., lesion area segmentation), justifying its effectiveness on more types of subsampling methods in MRI acceleration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Adding DRL into different layers in ViT.</figDesc><graphic coords="5,59,31,54,23,305,32,130,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. MAE pre-trained models reconstruct the masked MR images. This example uses a T2weighted image.</figDesc><graphic coords="6,193,14,57,68,205,00,66,97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. AUROC of different methods using ViT and CNN models.</figDesc><graphic coords="7,42,81,149,54,338,08,259,06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Stroke slices where the ERM model and clinicians may fail when the CU perturbation is large, while the DRL model succeeds. The red boxes indicate the stroke lesion areas in the clean images. (Color figure online)</figDesc><graphic coords="8,56,46,211,97,339,43,205,90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>AUROC and AUPRC improvement of DRL over three baseline methods using ViT/CNN models under CU and WGN perturbations on the test set.PGD+DRL AUROC 3.83% 0.01% 12.19% 0.05% 6.00% 0.14% 3.45% 0.28% AUPRC 2.88% 0.02% 18.68% 0.06% 3.85% 0.21% 4.13% 0.77%</figDesc><table><row><cell>ViT</cell><cell></cell><cell>CNN</cell><cell></cell></row><row><cell>CU</cell><cell>WGN</cell><cell>CU</cell><cell>WGN</cell></row><row><cell cols="4">Mean Std. Mean Std. Mean Std. Mean Std.</cell></row><row><cell cols="4">ERM+DRL AUROC 8.65% 0.00% 16.87% 0.04% 11.91% 0.18% 4.92% 0.24%</cell></row><row><cell cols="4">AUPRC 8.31% 0.02% 29.77% 0.10% 8.65% 0.15% 6.81% 0.79%</cell></row><row><cell cols="4">BAT+DRL AUROC 4.29% 0.00% 23.90% 0.05% 11.26% 0.07% 1.42% 0.04%</cell></row><row><cell cols="4">AUPRC 4.37% 0.01% 39.75% 0.12% 9.32% 0.11% 3.59% 0.22%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Supplement and source code are available at https://github.com/noc-lab/drl_mri.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported by the <rs type="funder">Rajen Kilachand Fund for Integrated Life Science and Engineering</rs> and by the <rs type="funder">NSF</rs> under grant <rs type="grantNumber">CCF-2200052</rs> and <rs type="grantNumber">IIS-1914792</rs>. <rs type="person">Ruidi Chen</rs> contributed to this work while at <rs type="institution">Boston University</rs>, before moving to her current position at Amazon SCOT.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pyCduBs">
					<idno type="grant-number">CCF-2200052</idno>
				</org>
				<org type="funding" xml:id="_zxnjb4b">
					<idno type="grant-number">IIS-1914792</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_74.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dataset</surname></persName>
		</author>
		<ptr target="https://brain-development.org/ixi-dataset/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Threat of adversarial attacks on deep learning in computer vision: a survey</title>
		<author>
			<persName><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="14410" to="14430" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aspiration after stroke: lesion analysis by brain mri</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Alberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Horner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Brazer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dysphagia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="170" to="173" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributionally robust multiclass classification and applications in deep image classifiers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Paschalidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A robust learning approach for regression models based on distributionally robust optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Paschalidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="517" to="564" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SthardNet: swin transformer with HarDNet for MRI segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">468</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning for undersampled MRI reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phy. Med. Bio</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">135007</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for automatically segmenting acute ischemic stroke lesion in multimodality MRI</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6545" to="6558" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Machine learning in magnetic resonance imaging: image reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Montalt-Tordera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Muthurangu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Steeden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica Medica</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic contrast-enhanced myocardial perfusion MRI accelerated with k-t sense</title>
		<author>
			<persName><forename type="first">S</forename><surname>Plein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ryf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radjenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boesiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kozerke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic Resonance in Medicine. Official J. Int. Soc. Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="777" to="785" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><surname>Frangi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015: 18th International Conference</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename></persName>
		</editor>
		<meeting><address><addrLine>Munich, Germany; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">October 5-9, 2015. 2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Assessment of cartilage-dedicated sequences at ultra-high-field MRI: comparison of imaging performance and diagnostic confidence between 3.0 and 7.0 t with respect to osteoarthritis-induced changes at the knee joint</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Skeletal Radiol</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SDPT3 -a MATLAB software package for semidefinite programming, version 1.3. Optim</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Tütüncü</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods Softw</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="545" to="581" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey of GPU-based acceleration techniques in MRI reconstructions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Imaging Med. Surg</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">196</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">User-guided 3d active contour segmentation of anatomical structures: significantly improved efficiency and reliability</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1116" to="1128" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08839</idno>
		<title level="m">FastMRI: an open dataset and benchmarks for accelerated MRI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
