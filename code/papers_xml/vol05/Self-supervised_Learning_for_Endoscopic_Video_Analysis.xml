<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Learning for Endoscopic Video Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Roy</forename><surname>Hirsch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Verily AI</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
								<address>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Regev</forename><surname>Cohen</surname></persName>
							<email>regevcohen@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Verily AI</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amir</forename><surname>Livne</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Verily AI</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ron</forename><surname>Shapiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Verily AI</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tomer</forename><surname>Golany</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Verily AI</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roman</forename><surname>Goldenberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Verily AI</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Freedman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Verily AI</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Verily AI</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-supervised Learning for Endoscopic Video Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="569" to="578"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">E077A2D25F1F7AA6FB7EA683EC5D75AA</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_55</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Artificial intelligence</term>
					<term>Self-Supervised Learning</term>
					<term>Endoscopy Video Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning (SSL) has led to important breakthroughs in computer vision by allowing learning from large amounts of unlabeled data. As such, it might have a pivotal role to play in biomedicine where annotating data requires a highly specialized expertise. Yet, there are many healthcare domains for which SSL has not been extensively explored. One such domain is endoscopy, minimally invasive procedures which are commonly used to detect and treat infections, chronic inflammatory diseases or cancer. In this work, we study the use of a leading SSL framework, namely Masked Siamese Networks (MSNs), for endoscopic video analysis such as colonoscopy and laparoscopy. To fully exploit the power of SSL, we create sizable unlabeled endoscopic video datasets for training MSNs. These strong image representations serve as a foundation for secondary training with limited annotated datasets, resulting in state-of-the-art performance in endoscopic benchmarks like surgical phase recognition during laparoscopy and colonoscopic polyp characterization. Additionally, we achieve a 50% reduction in annotated data size without sacrificing performance. Thus, our work provides evidence that SSL can dramatically reduce the need of annotated data in endoscopy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Endoscopic operations are minimally invasive medical procedures which allow physicians to examine inner body organs and cavities. During an endoscopy, a thin, flexible tube with a tiny camera is inserted into the body through a small orifice or incision. It is used to diagnose and treat a variety of conditions, including ulcers, polyps, tumors, and inflammation. Over 250 million endoscopic procedures are performed each year globally and 80 million in the United States, signifying the crucial role of endoscopy in clinical research and care.</p><p>A cardinal challenge in performing endoscopy is the limited field of view which hinders navigation and proper visual assessment, potentially leading to high detection miss-rate, incorrect diagnosis or insufficient treatment. These limitations have fostered the development of computer-aided systems based on artificial intelligence (AI), resulting in unprecedented performance over a broad range of clinical applications <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. Yet the success of such AI systems heavily relies on acquiring annotated data which requires experts of specific knowledge, leading to an expensive, prolonged process. In the last few years, Self-Supervised Learning (SSL <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>) has been shown to be a revolutionary strategy for unsupervised representation learning, eliminating the need to manually annotate vast quantities of data. Training large models on sizable unlabeled data via SSL leads to powerful representations which are effective for downstream tasks with few labels. However, research in endoscopic video analysis has only scratched the surface of SSL which remains largely unexplored.</p><p>This study introduces Masked Siamese Networks (MSNs <ref type="bibr" target="#b1">[2]</ref>), a prominent SSL framework, into endoscopic video analysis where we focus on laparoscopy and colonoscopy. We first experiment solely on public datasets, Cholec80 <ref type="bibr" target="#b31">[32]</ref> and PolypsSet <ref type="bibr" target="#b32">[33]</ref>, demonstrating performance on-par with the top results reported in the literature. Yet, the power of SSL lies in large data regimes. Therefore, to exploit MSNs to their full extent, we collect and build two sizable unlabeled datasets for laparoscopy and colonoscopy with 7, 700 videos (&gt;23M frames) and 14, 000 videos (&gt;2M frames) respectively. Through extensive experiments, we find that scaling the data size necessitates scaling the model architecture, leading to state-of-the-art performance in surgical phase recognition of laparoscopic procedures, as well as in polyp characterization of colonoscopic videos. Furthermore, the proposed approach exhibits robust generalization, yielding better performance with only 50% of the annotated data, compared with standard supervised learning using the complete labeled dataset. This shows the potential to reduce significantly the need for expensive annotated medical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>There exist a wide variety of endoscopic applications. Here, we focus on colonoscopy and laparoscopy, which combined covers over 70% of all endoscopic procedures. Specifically, our study addresses two important common tasks, described below.</p><p>Cholecystectomy Phase Recognition. Cholecystectomy is the surgical removal of the gallbladder using small incisions and specialized instruments. It is a common procedure performed to treat gallstones, inflammation, or other conditions affecting the gallbladder. Phase recognition in surgical videos is an important task that aims to improve surgical workflow and efficiency. Apart from measuring quality and monitoring adverse event, this task also serves in facilitating education, statistical analysis, and evaluating surgical performance. Furthermore, the ability to recognize phases allows real-time monitoring and decision-making assistance during surgery, thus improving patient safety and outcomes. AI solutions have shown remarkable performance in recognizing surgical phases of cholecystectomy procedures <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b31">32]</ref>; however, they typically require large labelled training datasets. As an alternative, SSL methods have been developed <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>, however, these are early-days methods that based on heuristic, often require external information and leads to sub-optimal performance. A recent work <ref type="bibr" target="#b26">[27]</ref> presented an extensive analysis of modern SSL techniques for surgical computer vision, yet on relatively small laparoscopic datasets.</p><p>Optical Polyp Characterization. Colorectal cancer (CRC) remains a critical health concern and significant financial burden worldwide. Optical colonoscopy is the standard of care screening procedure for preventing CRC through the identification and removal of polyps <ref type="bibr" target="#b2">[3]</ref>. According to colonoscopy guidelines, all identified polyps must be removed and histologically evaluated regardless of their malignant nature. Optical biopsy enables practitioners to remove pre-cancerous adenoma polyps or leave distal hyperplastic polyps in situ without the need for pathology examination, by visually predicting histology. However, this technique is highly dependent on operator expertise <ref type="bibr" target="#b13">[14]</ref>. This limitation has motivated the development of AI systems for automatic optical biopsy, allowing non-experts to also effectively perform optical biopsy during polyp management. In recent years, various AI systems have been developed to this end <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19]</ref>. However, training such automatic optical biopsy systems relies on a large body of annotated data, while SSL has not been investigated in this context, to the best of our knowledge.</p><p>3 Self-supervised Learning for Endoscopy SSL approaches have produced impressive results recently <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, relying on two key factors: (i) effective algorithms for unsupervised learning and (ii) training on large-scale datasets. Here, we first describe Masked Siamese Networks <ref type="bibr" target="#b1">[2]</ref>, our chosen SSL framework. Additionally, we present our large-scale data collection (see Fig. <ref type="figure" target="#fig_1">2</ref>). Through extensive experiments in Sect. 4, we show that training MSNs on these substantial datasets unlocks their potential, yielding effective representations that transfer well to public laparoscopy and colonoscopy datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Masked Siamese Networks</head><p>SSL has become an active research area, giving rise to efficient learning methods such as SimCLR <ref type="bibr" target="#b6">[7]</ref>, SwAV <ref type="bibr" target="#b4">[5]</ref> and DINO <ref type="bibr" target="#b5">[6]</ref>. Recently, Masked Siamese Networks <ref type="bibr" target="#b1">[2]</ref> have set a new state-of-the-art among SSL methods on the ImageNet benchmark <ref type="bibr" target="#b28">[29]</ref>, with a particular focus on the low data regime. This is of great interest for us since our downstream datasets are typically of small size <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. We briefly describe MSNs below and refer the reader to <ref type="bibr" target="#b1">[2]</ref> for further details.</p><p>During pretraining, on each image x i ∈ R n of a mini-batch of B ≥ 1 samples (e.g. laparoscopic images) we apply two sets of random augmentations to generate anchor and target views, denoted by x a i and x t i respectively. We convert each view into a sequence of non-overlapping patches and perform an additional masking ("random" or "focal" styles) step on the anchor view by randomly discarding some of its patches. The resultant anchor and target sequences are used as inputs to their respective image encoders f θ a and f θ t . Both encoders share the same Vision Transformer (ViT <ref type="bibr" target="#b15">[16]</ref>) architecture where the parameters θ t of the target encoder are updated via an exponential moving average of the anchor encoder parameters θ a . The outputs of the networks are the representation vectors z a i ∈ R d and z t i ∈ R d , corresponding to the [CLS] tokens of the networks. The similarity between each view and a series of K &gt; 1 learnable prototypes is then computed, and the results undergo a softmax operation to yield the following probabilities p a i = sof tmax</p><formula xml:id="formula_0">Qz a i τ a and p t i = sof tmax Qz t i τ t</formula><p>where 0 &lt; τ t &lt; τ a &lt; 1 are temperatures and Q ∈ R K×d is a matrix whose rows are the prototypes. The probabilities are promoted to be the same by minimizing the cross-entropy loss H(p t i , p a i ), as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. In practice, a sequence of M ≥ 1 anchor views are generated, leading to multiple probabilities {p a i,m } M m=1 . Furthermore, to prevent representation collapse and encourage the model to fully exploit the prototypes, a mean entropy maximization (me-max) regularizer <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref> is added, aiming to maximize the entropy H(p a ) of the average prediction across all the anchor views pa</p><formula xml:id="formula_1">1 MB B i=1 M m=1 p a i,m</formula><p>. Thus, the overall training objective to be minimized for both θ a and Q is where λ &gt; 0 is an hyperparameter and the gradients are computed only with respect to the anchor predictions p a i,m (not the target predictions p t i ). Applying MSNs on the large datasets described below, generates representations that serve as a strong basis for various downstream tasks, as shown in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Private Datasets</head><p>Laparoscopy. We compiled a dataset of laparoscopic procedures videos exclusively performed on patients aged 18 years or older. The dataset consists of 7,877 videos recorded at eight different medical centers in Israel. The dataset predominantly consists of the following procedures: cholecystectomy (35%), appendectomy (20%), herniorrhaphy (12%), colectomy (6%), and bariatric surgery (5%). The remaining 21% of the dataset encompasses various standard laparoscopic operations. The recorded procedures have an average duration of 47 min, with a median duration of 40 min. Each video recording was sampled at a rate of 1 frame per second (FPS), resulting in an extensive dataset containing 23.3 million images. Further details are given in the supplementary materials.</p><p>Colonoscopy. We have curated a dataset comprising 13,979 colonoscopy videos of patients aged 18 years or older. These videos were recorded during standard colonoscopy procedures performed at six different medical centers between the years 2019 and 2022. The average duration of the recorded procedures is 15 min, with a median duration of 13 min. To identify and extract polyps from the videos, we employed a pretrained polyp detection model <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Using this model, we obtained bounding boxes around the detected polyps. To ensure high-quality data, we filtered out detections with confidence scores below 0.5. For each frame, we cropped the bounding boxes to generate individual images of the polyps. This process resulted in a comprehensive collection of 2.2 million polyp images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we empirically demonstrate the power of SSL in the context of endoscopy. Our experimental protocol is the following: (i) first, we perform SSL pretraining with MSNs over our unlabeled private dataset to learn informative and generic representations, (ii) second we probe these representations by utilizing them for different public downstream tasks. Specifically, we use the following two benchmarks. (a) Cholec80 <ref type="bibr" target="#b31">[32]</ref>: 80 videos of cholecystectomy procedures resulting in nearly 200k frames at 1 FPS. Senior surgeons annotated each frame to one out of seven phases. (b) PolypsSet <ref type="bibr" target="#b32">[33]</ref>: A unified dataset of 155 colonoscopy videos (37,899 frames) with labeled polyp classes (hyperplastic or adenoma) and bounding boxes. We use the provided detections to perform binary classification. Downstream Task Evaluation Protocols. (a) Linear evaluation: A standard protocol consisting in learning a linear classifier on top of frozen SSL features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>. (b) Temporal evaluation: A natural extension of the linear protocol where we learn a temporal model on top of the frame-level frozen features. We specifically use Multi-Stage Temporal Convolution Networks (MS-TCN) as used in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>. This incorporates the temporal context which is crucial for video tasks such as phases recognition. (c) Fine-tuning: An end-to-end training of a classification head on top of the (unfrozen) pretrained backbone. We perform an extensive hyperparameter grid search for all downstream experiments and report the test results for the models that exceed the best validation results. We report the Macro F1 (F-F1) as our primary metric. For phase recognition we also report the per-video F1 (V-F1), computed by averaging the F1 scores across all videos <ref type="bibr" target="#b26">[27]</ref>.</p><p>Implementation Details. For SSL we re-implemented MSNs in JAX using Scenic library <ref type="bibr" target="#b14">[15]</ref>. As our image encoders we train Vision Transformer (ViT <ref type="bibr" target="#b15">[16]</ref>) of different sizes, abbreviated as ViT-S/B/L, using 16 TPUs. Downstream experiments are implemented in TensorFlow where training is performed on 4 Nvidia Tesla V100 GPUs. See the supplementary for further implementation details. <ref type="foot" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results and Discussion</head><p>Scaling Laws of SSL. We explore large scale SSL pretraining for endoscopy videos. Table <ref type="table" target="#tab_0">1</ref> compares the results of pretraining with different datasets (public and private) and model sizes. We pretrain the models with MSN and then report their downstream performances. We present results for the cholecystectomy phase recognition task based on fine-tuned models and for the optical polyp characterization task based on linear evaluation, due to the small size of the public dataset. As baselines, we report fully-supervised ResNet50 results, trained on public datasets. We find that replacing ResNet50 with ViT-S, despite comparable number of parameters, yields sub-optimal performance. SSL pretraining on public datasets (without labels) provides comparable or better results than fully supervised baselines. The performance in per-frame phase recognition is comparable with the baseline. Phase recognition per-video results improve by 1.3 points when using the MSN pretraining, while polyp characterization improve by 2.2 points. Importantly, we see that the performance gap becomes prominent when using the large scale private datasets for SSL pretraining. Here, per-frame and per-video phase recognition performances improve by 6.7% and 8.2%, respectively. When using the private colonoscopy dataset the Macro F1 improves by 11.5% compared to the fully supervised baseline. Notice that the performance improves with scaling both model and private data sizes, demonstrating that both factors are crucial to achieve optimal performance. Low-Shot Regime. Next, we examine the benefits of using MSNs to improve downstream performance in a low-shot regime with few annotated samples.  Note that MSNs have originally been found to produce excellent features for low data regime <ref type="bibr" target="#b1">[2]</ref>. We train a linear classifier on top of the extracted features and report the test classification results. Figure <ref type="figure" target="#fig_2">3</ref> shows the low-shot performance for the two endoscopic tasks. We report results using a fraction k = {12%, 25%, 50%, 75%, 100%} of the annotated public videos. We also report results for fully-supervised baselines trained on the same fraction of annotated samples. Each experiment is repeated three times with a random sample of train videos, and we report the mean and standard deviation (shaded area).</p><p>As seen, SSL-based models provide enhanced robustness to limited annotations. When examining the cholecystectomy phase recognition task, it is evident that we can achieve comparable frame-level performance by using only 12% of the annotated videos. Using 25% of the annotated videos yields comparable results to the fully supervised temporal models. Optical polyp characterization results show a similar trend, but with a greater degree of variability. Using small portions of PolypSet (12% and 25%) hindered the training process and increased sensitivity to the selected portions. However, when using more than 50% of PolypSet, the training process stabilized, yielding results comparable to the fully supervised baseline. This feature is crucial for medical applications, given the time and cost involved in expert-led annotation processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Table <ref type="table" target="#tab_1">2</ref> details different design choices regarding our SSL pretraining. Ablations are done on ViT-S trained over the public Cholec80. We report results on the validation set after linear evaluation. In Table <ref type="table" target="#tab_1">2a</ref>), we see that the method is robust to the number of prototypes, though over-clustering <ref type="bibr" target="#b3">[4]</ref> with 1k prototypes is optimal. In Table <ref type="table" target="#tab_1">2b</ref>) and Table <ref type="table" target="#tab_1">2c</ref>), we explore the effect of random and focal masking. We see that 50% random masking (i.e. we keep 98 tokens out of 196 for the global view) and using 4 local views gives the best of performance. In Table <ref type="table" target="#tab_1">2d</ref>) we study the effect of data augmentation. SSL augmentation pipelines have been developed on ImageNet-1k <ref type="bibr" target="#b6">[7]</ref>, hence, it is important to re-evaluate these choices for medical images. Surprisingly, we see that augmentations primarily found to work well on ImageNet-1k are also effective on laparoscopic videos (e.g. color jiterring and horizontal flips). In Table2e), we look at the effect of the training length when starting from scratch or from a good SSL pretrained checkpoint on ImageNet-1k. We observe that excellent performance is achieved with only 10 epochs of finetuning on medical data when starting from a strong DINO checkpoint <ref type="bibr" target="#b5">[6]</ref>. Table <ref type="table" target="#tab_1">2g</ref>) shows that ImageNet-1k DINO is a solid starting point compared to other alternatives <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>. Finally, Table2f) confirms the necessity of regularizing with Sinkhorn-Knopp and me-max to avoid representation collapse by encouraging the use of all prototypes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This study showcases the use of Masked Siamese Networks to learn informative representations from large, unlabeled endoscopic datasets. The learnt representations lead to state-of-the-art results in identifying surgical phases of laparoscopic procedures and in optical characterization of colorectal polyps. Moreover, this methodology displays strong generalization, achieving comparable performance with just 50% of labeled data compared to standard supervised training on the complete labeled datasets. This dramatically reduces the need for annotated medical data, thereby facilitating the development of AI methods for healthcare.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic of Masked Siamese Networks.</figDesc><graphic coords="5,56,97,117,38,326,17,86,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Data Samples. Top: Laparoscopy. Bottom: Colonoscopy.</figDesc><graphic coords="5,55,98,279,44,340,30,102,19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Low-shot evaluation comparing MSN to fully supervised baselines.</figDesc><graphic coords="7,55,98,262,79,340,15,81,61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparing the downstream F1 performances of: (i) Models trained on the private (Pri) and public (Pub) datasets using SSL. (ii) Fully supervised baselines pretrained on ImageNet-1K (IN1K). Best results are highlighted.</figDesc><table><row><cell>Method</cell><cell>Arch</cell><cell cols="2">Pretrain Cholec80 frame</cell><cell cols="3">Cholec80 temporal PolypsSet F-F1 V-F1</cell></row><row><cell cols="2">Fully Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FS [27]</cell><cell cols="2">RN50 IN1K</cell><cell>71.5</cell><cell>-</cell><cell>80.3</cell><cell>72.1</cell></row><row><cell>TeCNO</cell><cell cols="2">RN50 IN1K</cell><cell>-</cell><cell>83.3</cell><cell>-</cell><cell>-</cell></row><row><cell>OperA</cell><cell cols="2">RN50 IN1K</cell><cell>-</cell><cell>84.4</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Self Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DINO</cell><cell cols="2">ViT-S IN1K</cell><cell>64.9</cell><cell>77.4</cell><cell>72.4</cell><cell>61.0</cell></row><row><cell cols="3">DINO [27] RN50 Pub</cell><cell>71.1</cell><cell>-</cell><cell>81.6</cell><cell>72.4</cell></row><row><cell>MSN</cell><cell cols="2">ViT-S Pub</cell><cell>65.0</cell><cell>83.4</cell><cell>80.9</cell><cell>70.6</cell></row><row><cell>MSN</cell><cell cols="2">ViT-B Pub</cell><cell>71.2</cell><cell>82.6</cell><cell>82.9</cell><cell>74.6</cell></row><row><cell>MSN</cell><cell cols="2">ViT-L Pub</cell><cell>65.6</cell><cell cols="2">84.0 82.0</cell><cell>73.6</cell></row><row><cell>MSN</cell><cell cols="2">ViT-S Pri</cell><cell>70.7</cell><cell>87.0</cell><cell>84.3</cell><cell>78.5</cell></row><row><cell>MSN</cell><cell cols="2">ViT-B Pri</cell><cell>73.5</cell><cell>87.3</cell><cell>85.8</cell><cell>78.2</cell></row><row><cell>MSN</cell><cell cols="2">ViT-L Pri</cell><cell>76.3</cell><cell>89.6</cell><cell>86.9</cell><cell>80.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of different design choices (default setting is highlighted).</figDesc><table><row><cell cols="4">a) Number of prototypes</cell><cell cols="3">d) Data augmentation</cell><cell></cell><cell>f) Avoiding collapse.</cell></row><row><cell cols="2">K 10 1 10 2</cell><cell cols="2">10 3 10 4</cell><cell>color jit</cell><cell cols="2">flip (hor) blur</cell><cell>val</cell><cell>SK+me-max SK ∅</cell></row><row><cell cols="2">val 65.4 67.8</cell><cell cols="2">69.8 69.1</cell><cell></cell><cell></cell><cell></cell><cell>69.8</cell><cell>69.8</cell><cell>67.7 34.0</cell></row><row><cell cols="4">b) Effect of random masking</cell><cell></cell><cell></cell><cell></cell><cell>69.8</cell><cell>g) ImNet-1k initialization</cell></row><row><cell>% 0</cell><cell>50</cell><cell>70</cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell>68.6</cell><cell>weights. (ViT-B/16) val</cell></row><row><cell cols="4">val 69.1 69.8 68.4 68.2</cell><cell></cell><cell></cell><cell></cell><cell>67.4</cell><cell>MAE [20]</cell><cell>53.5</cell></row><row><cell cols="6">c) Local crops (focal masking) e) Training length</cell><cell></cell><cell></cell><cell>Supervised [31]</cell><cell>63.1</cell></row><row><cell># 0</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>epochs</cell><cell>10</cell><cell cols="3">100 200 500 MoCo-v3 [9]</cell><cell>63.3</cell></row><row><cell cols="2">val 67.7 69.1</cell><cell cols="2">69.8 68.1</cell><cell>scratch</cell><cell>33.8</cell><cell cols="3">63.3 65.5 66.5 iBOT [34]</cell><cell>65.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SSL init 68.2</cell><cell cols="3">69.3 69.8 68.4 DINO [6]</cell><cell>65.9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For reproducibility purposes, code and model checkpoints are available at https:// github.com/RoyHirsch/endossl.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_55.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Current and future implications of artificial intelligence in colonoscopy</title>
		<author>
			<persName><forename type="first">G</forename><surname>Antonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rizkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Iacopini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Gastroenterol</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="114" to="122" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Masked siamese networks for label-efficient learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Assran</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19821-2_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19821-2_26" />
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Will computer-aided detection and diagnosis revolutionize colonoscopy?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shahidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Rex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterology</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1460" to="1464" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01264-9_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01264-9_9" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="139" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">It has potential: gradient-driven denoisers for convergent solutions to inverse problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18152" to="18164" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Regularization by denoising via fixed-point projection (RED-PRO)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imag. Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1374" to="1406" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-supervised surgical tool segmentation using kinematic information</title>
		<author>
			<persName><forename type="first">C</forename><surname>Da Costa Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8720" to="8726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TeCNO: surgical phase recognition with multi-stage temporal convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-0_33" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="343" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Asge technology committee systematic review and metaanalysis assessing the asge pivi thresholds for adopting real-time endoscopic assessment of the histology of diminutive colorectal polyps</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K A</forename><surname>Dayyeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointest. Endosc</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="502" to="e503" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Scenic: a jax library for computer vision research and beyond</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="21393" to="21398" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An image is worth 16 × 16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">AI for phase recognition in complex laparoscopic cholecystectomy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Golany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surgical Endoscopy</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bounded future MS-TCN++ for surgical gesture recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldbraikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Avisdris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Pugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laufer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25066-8_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25066-8_22" />
	</analytic>
	<monogr>
		<title level="m">Proceedings, Part III</title>
		<meeting>Part III</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">October 23-27, 2022. 2023</date>
			<biblScope unit="page" from="406" to="421" />
		</imprint>
	</monogr>
	<note>ECCV 2022 Workshops</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Performance of artificial intelligence in colonoscopy for adenoma and polyp detection: a systematic review and meta-analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointest. Endosc</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="85" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Intrator</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Aizenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Livne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goldenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.08591</idno>
		<title level="m">Self-supervised polyp re-identification in colonoscopy</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A convex relaxation for weakly supervised classifiers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6413</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Estimating withdrawal time in colonoscopies</title>
		<author>
			<persName><forename type="first">L</forename><surname>Katzir</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25066-8_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25066-8_28" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="495" to="512" />
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conformal prediction masks: visualizing uncertainty in medical imaging</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kutiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2023 Workshop on Trustworthy Machine Learning for Healthcare</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detection of elusive polyps using a large-scale artificial intelligence system (with videos)</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Livovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointest. Endosc</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1099" to="1109" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Polyp-YOLOv5-Tiny: a lightweight model for real-time polyp detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1106" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dissecting self-supervised learning methods for surgical computer vision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">102844</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploiting the potential of unlabeled endoscopic video data with self-supervised learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="925" to="933" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A kinematic bottleneck approach for pose regression of flexible surgical instruments directly from images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sestini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Momi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ferrigno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2938" to="2945" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.07118</idno>
		<title level="m">DeIT III: Revenge of the ViT</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Endonet: a deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Replication data for: colonoscopy polyp detection and classification: dataset creation and comparative evaluations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.7910/DVN/FCBUOR</idno>
		<ptr target="https://doi.org/10.7910/DVN/FCBUOR" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Harvard Dataverse</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07832</idno>
		<title level="m">ibot: image bert pre-training with online tokenizer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
