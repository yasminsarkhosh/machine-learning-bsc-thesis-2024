<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self- and Semi-supervised Learning for Gastroscopic Lesion Detection</title>
				<funder ref="#_rgvczCf">
					<orgName type="full">Chinese Key-Area Research and Development Program of Guangdong Province</orgName>
				</funder>
				<funder ref="#_Z65xvrr">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_9gpKt5p">
					<orgName type="full">Shenzhen Sustainable Development Project</orgName>
				</funder>
				<funder>
					<orgName type="full">Guangdong Provincial Key Laboratory of Big Data Computing</orgName>
				</funder>
				<funder>
					<orgName type="full">Chinese University of Hong Kong, Shenzhen</orgName>
				</funder>
				<funder ref="#_Vx9QcBh">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_vHrFSsf #_8ycPQjZ">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xuanye</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shenzhen Research Institute of Big Data</orgName>
								<orgName type="institution" key="instit2">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaige</forename><surname>Yin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Gastroenterology</orgName>
								<orgName type="department" key="dep2">Hebei Clinical Research Center for Digestive Diseases</orgName>
								<orgName type="laboratory">Hebei Key Laboratory of Gastroenterology</orgName>
								<orgName type="institution">Hebei Institute of Gastroenterology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The Second Hospital of Hebei Medical University</orgName>
								<address>
									<settlement>Shijiazhuang</settlement>
									<region>Hebei</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siqi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shenzhen Research Institute of Big Data</orgName>
								<orgName type="institution" key="instit2">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhijie</forename><surname>Feng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Gastroenterology</orgName>
								<orgName type="department" key="dep2">Hebei Clinical Research Center for Digestive Diseases</orgName>
								<orgName type="laboratory">Hebei Key Laboratory of Gastroenterology</orgName>
								<orgName type="institution">Hebei Institute of Gastroenterology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The Second Hospital of Hebei Medical University</orgName>
								<address>
									<settlement>Shijiazhuang</settlement>
									<region>Hebei</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">FNii</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">School of Science and Engineering</orgName>
								<orgName type="institution">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<email>liguanbin@mail.sysu.edu.cn</email>
							<affiliation key="aff5">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Research Institute of Sun Yat-sen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
							<email>wanxiang@sribd.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shenzhen Research Institute of Big Data</orgName>
								<orgName type="institution" key="instit2">CUHK-Shenzhen</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self- and Semi-supervised Learning for Gastroscopic Lesion Detection</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="83" to="93"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">FBAAE184E3FD77D61C0A450EFFD9DC1E</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gastroscopic Lesion Detection</term>
					<term>Self-Supervised Backbone Pre-training</term>
					<term>Semi-supervised Detector Training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gastroscopic Lesion Detection (GLD) plays a key role in computer-assisted diagnostic procedures. However, this task is not well studied in the literature due to the lack of labeled data and the applicable methods. Generic detectors perform below expectations on GLD tasks for 2 reasons: 1) The scale of labeled data of GLD datasets is far smaller than that of natural-image object detection datasets. 2) Gastroscopic images exhibit distinct differences from natural images, which are usually of high similarity in global but high diversity in local. Such characteristic of gastroscopic images also degrades the performance of using generic self-supervised or semi-supervised methods to solve the labeled data shortage problem using massive unlabeled data. In this paper, we propose Self-and Semi-Supervised Learning (SSL) for GLD tailored for using massive unlabeled gastroscopic images to enhance GLD tasks performance, which consists of a Hybrid Self-Supervised Learning (HSL) method for backbone pre-training and a Prototype-based Pseudo-label Generation (PPG) method for semi-supervised detector training. The HSL combines patch reconstruction with dense contrastive learning to boost their advantages in feature learning from massive unlabeled data. The PGG generates pseudo-labels for unlabeled data based on similarity to the prototype feature vector to discover potential lesions and avoid introducing much noise. Moreover, we contribute the first Large-scale GLD Datasets (LGLDD), which contains 10,083 gastroscopic images with 12,292 well-annotated boxes for four-category lesions. Experiments on LGLDD demonstrate that SSL can bring significant improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gastroscopic Lesion Detection (GLD) plays a key role in computer-assisted diagnostic procedures. Although deep neural network-based object detectors achieve tremendous success within the domain of natural images, directly training generic object detectors on GLD datasets performs below expectations for two reasons: 1) The scale of labeled data in GLD datasets is limited in comparison to natural images due to the annotation costs. Though gastroscopic images are abundant, those containing lesions are rare, which necessitates extensive image review for lesion annotation. 2) The characteristic of gastroscopic images exhibits distinct differences from the natural images <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b21">21]</ref> and is often of high similarity in global but high diversity in local. Specifically, each type of lesion may have diverse appearances though gastroscopic images look quite similar. Some appearances of lesions are quite rare and can only be observed in a few patients. Generic self-supervised backbone pre-training or semi-supervised detector training methods can solve the first challenge for natural images but its effectiveness is undermined for gastroscopic images due to the second challenge.</p><p>Self-Supervised Backbone Pre-training methods enhance object detection performance by learning high-quality feature representations from massive unlabelled data for the backbone. The mainstream self-supervised backbone pretraining methods adopt self-supervised contrast learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> or masked Fig. <ref type="figure">1</ref>. Pipeline of Self-and Semi-Supervised Learning (SSL) for GLD. SSL consists of a Hybrid Self-Supervised Learning (HSL) method and a Prototype-based Pseudo-label Generation (PPG) method. HSL combines patch reconstruction with dense contrastive learning. PPG generates pseudo-labels for potential lesions based on the similarity to the prototype feature vectors.</p><p>image modeling <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>. Self-supervised contrastive learning methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref> can learn discriminative global feature representations, and <ref type="bibr" target="#b9">[10]</ref> can further learn discriminative local feature representations by extending contrastive learning to dense paradigm. However, these methods usually cannot grasp enough local detailed information. On the other hand, masked image modeling is expert in extracting local detailed information but is weak in preserving the discriminability of feature representation. Therefore, both types of methods have their own weakness for GLD tasks.</p><p>Semi-Supervised object detection methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref> first use detectors trained with labeled data to generate pseudo-labels for unlabeled data and then enhance object detection performance by regarding these unlabeled data with pseudo-labels as labeled data to train the detector. Current pseudolabel generation methods rely on the objectiveness score threshold to generate pseudo-labels, which makes them perform below expectations on GLD, because the characteristic of gastroscopic lesions makes it difficult to set a suitable threshold to discover potential lesions meanwhile avoiding introducing much noise.</p><p>The motivation of this paper is to explore how to enhance GLD performance using massive unlabeled gastroscopic images to overcome the labeled data shortage problem. The main challenge for this goal is the characteristic of gastroscopic lesions. Intuitively, such a challenge requires local feature representations to contain enough detailed information, meanwhile preserving discriminability. Enlightened by this, we propose the Self-and Semi-Supervised Learning (SSL) framework tailored to address challenges in daily clinical practice and use massive unlabeled data to enhance GLD performance. SSL overcomes the challenges of GLD by leveraging a large volume of unlabeled gastroscopic images using self-supervised learning for improved feature representations and semi-supervised learning to discover and utilize potential lesions to enhance performance. Specifically, it consists of a Hybrid Self-Supervised Learning (HSL) method for self-supervised backbone pre-training and a Prototype-based Pseudo-label Generation (PPG) method for semi-supervised detector training. The HSL combines the dense contrastive learning <ref type="bibr" target="#b9">[10]</ref> with the patch reconstruction to inherit the advantages of discriminative feature learning and grasp the detailed information that is important for GLD tasks. The PPG generates pseudo-labels based on the similarity to the prototype feature vectors (formulated from the feature vectors in its Memory Module) to discover potential lesions from unlabeled data, and avoid introducing much noise at the same time. Moreover, we propose the first Large-scale GLD Datasets (LGLDD), which contains 10,083 gastroscopic images with 12,292 well-annotated lesion bounding boxes of four categories of lesions (polyp, ulcer, cancer, and sub-mucosal tumor). We evaluate SSL with multiple detectors on LGLDD and SSL brings significant improvement compared with baseline methods (CenterNet <ref type="bibr" target="#b5">[6]</ref>: +2.7AP, Faster RCNN <ref type="bibr" target="#b12">[13]</ref>: +2.0AP). In summary, our contributions include:</p><p>-A Self-and Semi-supervise Learning (SSL) framework to leverage massive unlabeled data to enhance GLD performance. -A Large-scale Gastroscopic Lesion Detection datasets (LGLDD) -Experiments on LGLDD demonstrate that SSL can bring significant enhancement compared with baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we introduce the main ideas of the proposed SSL for GLD. The proposed approach includes 2 main components and is illustrated in Fig. <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hybrid Self-supervised Learning</head><p>The motivation of Hybrid Self-Supervised Learning (HSL) is to learn the local feature representations of high discriminability meanwhile contain detailed information for the backbone from massive unlabeled gastroscopic images. Among existing backbone pre-training methods, dense contrastive learning can preserve local discriminability and masked image modeling can grasp local detailed information. Therefore, to leverage the advantages of both types of methods, we propose Hybrid Self-Supervised Learning (HSL), which combines patch reconstruction with dense contrastive learning to achieve the goal.</p><p>Structure. HSL heritages the structure of the DenseCL <ref type="bibr" target="#b9">[10]</ref> but adds an extra reconstruction projection head to reconstruct patches. Specifically, HSL consists of a backbone network and 3 parallel sub-heads. The global projection head and the dense projection head heritages from the DenseCL <ref type="bibr" target="#b9">[10]</ref>, and the proposed reconstruction projection head is inspired by the Masked Image Modeling.</p><p>Enlightened by the SimMIM <ref type="bibr" target="#b14">[15]</ref>, we adopt a lightweight design for the reconstruction projection head, which only contains 2 convolution layers.</p><p>Learning Pipeline. Like other self-supervised contrastive learning methods, HSL randomly generates 2 different "views" of the input image, uses the backbone to extract the dense feature maps F 1 , F 2 ∈ R H×W ×C , and then feeds them to the following projection heads. The global projection head of HSL uses F 1 , F 2 to obtain the global feature vector f g1 , f g2 like MoCo <ref type="bibr" target="#b8">[9]</ref>. The dense projection head and the reconstruction projection head crop the dense feature maps F 1 , F 2 into S×S patches and obtain the local feature vector sets F 1 and F 2 of each view</p><formula xml:id="formula_0">(F = {f 1 , f 2 , ..., f S 2 }).</formula><p>The dense projection head use F 1 and F 2 to obtain local feature vector sets F l1 and F l2 (F l = {f l1 , f l2 , ..., f lS 2 }) like DenseCL <ref type="bibr" target="#b9">[10]</ref>. The reconstruction projection head uses each feature vector in F 1 , F 2 to reconstruct corresponding patches and obtains the patch set P 1 , P 2 (P = {p i1 , p i2 , ..., p iS 2 }.</p><p>Training Objective. The HSL formulates the two contrastive learning as dictionary look-up tasks like DenseCL <ref type="bibr" target="#b9">[10]</ref> while the reconstruction learning as a regression task. The global contrastive learning uses the global feature vector f g of an image as query q and feature vectors from the alternate view of the query image and the other images within the batch as keys K = {k 1 , k 2 , ..., }. For each query q, the only positive key k + is the different views of the same images and the others are all negative keys (k -) like MoCo <ref type="bibr" target="#b8">[9]</ref>. We adopt the InfoNCE loss function for it:</p><formula xml:id="formula_1">L G = -log exp(q • k + /τ ) exp(q • k + ) + k-exp(q • k -/τ )</formula><p>The dense contrastive learning uses the local feature vector in F li as query r and keys T l = {t 1 , t 2 , ..., }. The negative keys t -here are the feature vectors of different images while the positive key t + is the correspondence feature vector of r in another view of the images. Specifically, we adopt the correspondence methods in DenseCL <ref type="bibr" target="#b9">[10]</ref> to obtain the positive key t + , which first conducts the matching process based on vector-wise cosine similarity between r and feature vectors in T and then selects the t j of highest similarity as the t + . The loss function is also the InfoNCE loss but in a dense paradigm:</p><formula xml:id="formula_2">L L = 1 S 2 -log exp(r s • t s + /τ ) exp(r s • t s + ) + t s - exp(r s • t s -/τ )</formula><p>The reconstruction task uses the feature vector in F to reconstruct each patch and obtain P i . The ground truth is the corresponding patches</p><formula xml:id="formula_3">V i = {v i1 , v i2 , ..., v iS 2 }</formula><p>of the input view. We adopt the MSE loss function for it:</p><formula xml:id="formula_4">L R = 1 2S 2 E(v i -p i ) 2</formula><p>The overall loss function is the weighted sum of these losses:</p><formula xml:id="formula_5">L H = L G + λ D L D + λ R L R</formula><p>where λ D and λ R are the weights of L D and L R and are set to 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prototype-Based Pseudo-label Generation Method</head><p>We propose the Prototype-based Pseudo-label Generation method (PPG) to discover potential lesions from unlabeled gastroscopic data meanwhile avoid introducing much noise to further enhance GLD performance. Specifically, PPG adopts a Memory Module to remember feature vectors of the representative lesions as memory and generates prototype feature vectors for each class based on the memories stored. To preserve the representativeness of the memory and further the prototype feature vectors, PPG designs a novel Memory Update Strategy. In semi-supervised learning, PPG generates pseudo-labels for unlabeled data relying on the similarity to the prototype feature vectors, which achieves a better balance between lesion discovery and noise avoidance.</p><p>Memory Module. Memory Module stores a set of lesion feature vectors as memory. For a C-class GLD task, the Memory Module stores C × N feature vectors as memory. Specifically, for each lesion, we denote the feature vector used to classify the lesion in the detector as f c . PPG stores N feature vectors for each class c to formulate the class memory m c = {f c1 , f c2 , ..., f cN }, and the memory M of PPG can be expressed as M = {m 1 , m 2 , ..., m C }. Then, PPG obtains the prototype feature vector p c by calculating the center of each class memory m c , and the prototype feature vector set can be expressed as P t = {p 1 , p 2 , ..., p C }. Moreover, the prototype feature vectors further serve as supervision for detector training under a contrastive clustering formulation and adopt a contrastive loss:</p><formula xml:id="formula_6">L cc = f c , p c + C j =c max(0, 1 -f c , p j )</formula><p>If the detector training loss is L Det , the overall loss L can be expressed as:</p><formula xml:id="formula_7">L = L Det + λ cc L cc</formula><p>where the λ cc is the weight of the contrastive learning loss and is set to 0.5.</p><p>Memory Update Strategy. Memory Update Strategy directly influences the representativeness of the class memory m c and further the prototype feature vector p c . Therefore, PPG adopts a novel Memory Update Strategy, which follows the idea that "The Memory Module should preserve the more representative feature vector among similar feature vectors". The pipeline of the strategy is as follows: 1) Acquisition the lesion feature vector f c . 2) Identification of the most similar f s to f c from corresponding class memory m c based on similarity:</p><formula xml:id="formula_8">f s = max j sim(f cj , f c )</formula><p>3) Updating the memory by selecting more unique features f s of F = {f s , f c } compared to the class prototype feature vector p c based upon similarity:</p><formula xml:id="formula_9">f s = argmin f ∈F sim(f , p c )</formula><p>The similarity function sim(u, v) can be expressed as sim(u, v) = u T v/ u v .</p><p>To initialize the memories, we empirically select 50 lesions randomly for each class. To maintain stability, we start updating the memory and calculating its loss after fixed epochs, and only the positive sample feature vector can be selected to update the memory.</p><p>Pseudo-label Generation. PPG proposes to generate pseudo-labels based on the similarity between the prototype feature vectors and the feature vector of potential lesions. To be specific, PPG first detects a large number of potential lesions with a low objectiveness score threshold τ u and then matches them with all the prototype feature vectors P to find the most similar one:</p><formula xml:id="formula_10">c = argmax pc∈P sim(p c , f u )</formula><p>PPG assigns the pseudo-label c for similarity value sim(p c , f u ) greater than the similarity threshold τ s otherwise omits it. We set τ u = 0.5 and τ s = 0.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>We contribute the first Large-scale Gstroscopic Lesion Detection Datasets (LGLDD) in the literature. Collection : LGMDD collects about 1M+ gastroscopic images from 2 hospitals of about 500 patients and their diagnosis reports. After consulting some senior doctors and surveying gastroscopic diagnosis papers <ref type="bibr" target="#b0">[1]</ref>, we select to annotate 4-category lesions: polyp(pol), ulcer(ulc), cancer(can) and sub-mucosal tumor(smt). We invite 10 senior doctors to annotate them from the unlabeled endoscopic images. To preserve the annotation quality, doctors can refer to the diagnosis reports, and each lesion is annotated by a doctor and checked by another. Finally, they annotates 12,292 lesion boxes in 10,083 images after going through about 120,000 images. The polyp, ulcer, cancer, and sub-mucosal tumor numbers are 7,779, 2,171, 1,164and 1,178, respectively. The train/val split of LGMDD is 8,076/2,007. The other data serves as unlabeled data.</p><p>Evaluation Metrics : We use standard object detection metrics to evaluate the GLD performance, which computes the average precision (AP) under multiple intersection-of-union (IoU) thresholds and then evaluate the performance using the mean of APs (mAP) and the AP of some specific IoU threshold. For mAP, we follow the popular object detection datasets COCO <ref type="bibr" target="#b10">[11]</ref> and calculate the mean of 11 APs of IoU from 0.5 to 0.95 with stepsize 0.05 (mAP @[.5:.05:.95]).</p><p>We also report AP under some specific IoU threshold (AP 50 for .5, AP 75 for .75) and AP of different scale lesions (AP S , AP M , AP L ) like COCO <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Please kindly refer to the Supplemental Materials for implementation details and training setups. The Objectiveness score threshold controls the quality of pseudo-labels. a) A low threshold generates noisy pseudo-labels, leading to reduced performance (-0.6/-0.2 AP at thresholds 0.5/0.6). b) A high threshold produces high-quality pseudo-labels but may miss potential lesions, resulting in only slight performance improvement (+0.3 AP at threshold 0.7). c) PPG approach uses a low threshold (0.5) to identify potential lesions, which are then filtered using prototype feature vectors, resulting in the most significant performance enhancement (+0.9 AP).</p><p>3) Memory Update Strategy influences the representativeness of memory and the prototype feature vectors. We compare our Memory Update Strategy with a queue-like ('Q-like') memory update strategy (first in &amp; first out). Experiment results (Table <ref type="table" target="#tab_1">2</ref>.c) show our Memory Update Strategy performs better. 4) Endo21: To further evaluate the effectiveness of SSL, we conduct experiments on Endo21 <ref type="bibr" target="#b1">[2]</ref> Sub-task 2 (Endo21 challenge consists of 4 sub-tasks and only the Sub-task 2 train/test split is available according to the <ref type="bibr" target="#b1">[2]</ref>). Experimental results in Table <ref type="table" target="#tab_1">2</ref>.d show that SSL can bring significant improvements to publicly available datasets. Moreover, SSL overperforms current SOTA (YOLO v5 <ref type="bibr" target="#b1">[2]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose Self-and Semi-Supervised Learning (SSL) for GLD tailored for using massive unlabeled gastroscopic to enhance GLD performance.</p><p>The key novelties of the proposed method include a Hybrid Contrastive Learning method for backbone pre-training and a Prototype-based Pseudo-Label Generation method for semi-supervised learning. Moreover, we contribute the first Large-scale GLD Datasets (LGLDD). Experiments on LGLDD prove that SSL can bring significant improvements to GLD performance. Since annotation cost always limits of datasets scale of such tasks, we hope SSL and LGLDD could fully realize its potential, as well as kindle further research in this direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative Results of SSL on LGLDD. SSL can actually enhance the GLD performance for some challenging cases.</figDesc><graphic coords="7,57,96,122,00,336,40,103,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,64,80,363,86,294,28,184,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative Results of SSL on LGLDD. Both components of SSL (HSL &amp; PPG) can bring significant performance enhancement for GLD tasks.</figDesc><table><row><cell>Detector</cell><cell cols="3">Pre-training PPG AP AP50 AP75 APS APM APL pol stm uls</cell><cell>can</cell></row><row><cell>CenterNet</cell><cell cols="2">Supervised x</cell><cell cols="2">29.3 57.2 25.4 22.0 31.6 31.3 41.6 36.0 27.3 12.1</cell></row><row><cell cols="3">Faster RCNN Supervised x</cell><cell cols="2">34.1 70.6 28.1 28.2 29.2 35.6 44.0 44.4 24.0 24.2</cell></row><row><cell>CenterNet</cell><cell>DenseCL</cell><cell>x</cell><cell cols="2">31.9 60.7 29.5 22.6 32.1 34.7 43.2 43.1 28.2 13.2</cell></row><row><cell cols="2">Faster RCNN DenseCL</cell><cell>x</cell><cell cols="2">35.3 71.9 29.4 29.9 31.8 37.1 44.9 46.7 25.4 24.3</cell></row><row><cell>CenterNet</cell><cell>HSL</cell><cell>x</cell><cell cols="2">33.7 64.2 30.6 23.1 33.7 35.9 42.5 45.3 28.8 18.0</cell></row><row><cell cols="2">Faster RCNN HSL</cell><cell>x</cell><cell cols="2">36.4 74.0 31.4 27.9 31.8 38.3 43.7 48.0 26.1 27.6</cell></row><row><cell>CenterNet</cell><cell>HSL</cell><cell></cell><cell cols="2">34.6 65.6 31.6 21.7 32.9 37.3 43.1 46.3 29.3 19.6</cell></row><row><cell cols="2">Faster RCNN HSL</cell><cell></cell><cell cols="2">37.3 74.8 33.2 28.8 33.5 39.4 44.9 51.0 26.1 27.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Parameters Analysis Experiment Results. (a) Reconstruction loss weight λR. (b) Objectiveness Score Threshold τu. (c) Memory update strategies. (d) Extension experiment on Endo21. Reconstruction Loss Weight λ R is designed to balance the losses of contrastive learning and the reconstruction, which is to balance the discriminability and the detailed information volume of local feature representations. As illustrated in Table2.a, only suitable λ R can fully boost the detection performance.</figDesc><table><row><cell>(a)</cell><cell></cell><cell>(b)</cell><cell>(c)</cell><cell></cell><cell>(d)</cell></row><row><cell>λR AP AP50 AP75</cell><cell>τu</cell><cell>AP AP50 AP75</cell><cell>AP AP50 AP75</cell><cell></cell><cell>AP AP50 AP75</cell></row><row><cell>0.5 35.8 73.1 30.7</cell><cell cols="2">w/o 36.4 74.0 31.4</cell><cell>Q-like 37.0 74.2 31.4</cell><cell>YOLO v5</cell><cell>60.5 81.0 66.4</cell></row><row><cell>1 36.4 74.0 31.4</cell><cell cols="2">0.7 36.7 74.0 32.1</cell><cell>P P G 37.3 74.8 33.2</cell><cell cols="2">Faster RCNN 57.8 79.1 68.1</cell></row><row><cell>2 36.3 73.4 31.8</cell><cell cols="2">0.6 36.2 73.7 31.8</cell><cell></cell><cell cols="2">+DenseCL 59.0 80.9 66.0</cell></row><row><cell>5 35.5 71.6 29.5</cell><cell cols="2">0.5 35.8 72.4 30.9</cell><cell></cell><cell>+HSL</cell><cell>61.4 83.0 67.3</cell></row><row><cell></cell><cell cols="2">P P G 37.3 74.8 33.2</cell><cell></cell><cell>+PPG</cell><cell>61.9 83.0 69.2</cell></row><row><cell cols="6">Main Results. Table 1 shows the quantitative results of SSL on LGLDD. As</cell></row><row><cell cols="6">is illustrated, when compared with the DenseCL [10] baseline, SSL can enhance</cell></row><row><cell cols="6">2.0AP and 2.7AP for Faster RCNN and CenterNet respectively. When compared</cell></row><row><cell cols="6">with the supervised pre-training (ImageNet [5] weights) baseline, SSL can boost</cell></row><row><cell cols="6">more AP enhancement (CenterNet: +5.3AP, FasterRCNN: +3.2AP). Qualitative</cell></row><row><cell cols="6">Results are shown in Fig. 2. It can be noticed, SSL can actually enhance the GLD</cell></row><row><cell cols="6">performance for both types of detectors, especially for some challenging cases.</cell></row><row><cell cols="6">Ablation Studies. We further analyze each component of SSL (HSL &amp; PPG).</cell></row><row><cell cols="6">HSL can bring 1.8 AP and 1.1 AP enhancement for CenterNet and FasterRCNN</cell></row><row><cell cols="6">respectively compared with DenseCL. PPG can bring extra 0.9AP and 0.9AP</cell></row><row><cell cols="5">enhancement for CenterNet and FasterRCNN respectively.</cell></row><row><cell cols="6">Parameter Analysis. We conduct extra experiments based on Faster RCNN</cell></row><row><cell cols="6">to further analyze the effect of different parameter settings on LGLDD.</cell></row><row><cell>1)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>2) Objectiveness score threshold τ u : We compare PPG with objectiveness score-based pseudo-label generation methods with different τ u (Table 2.b).</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work is supported by <rs type="funder">Chinese Key-Area Research and Development Program of Guangdong Province</rs> (<rs type="grantNumber">2020B0101350001</rs>), the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">NO. 61976250</rs>), the <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> (NO. <rs type="grantNumber">2020B1515020048</rs>), the <rs type="funder">Shenzhen Science and Technology Program</rs> (<rs type="grantNumber">JCYJ20220818103001002</rs>, <rs type="grantNumber">JCYJ20220530141211024</rs>), the <rs type="funder">Shenzhen Sustainable Development Project</rs> (<rs type="grantNumber">KCXFZ20201221173008022</rs>), the <rs type="funder">Guangdong Provincial Key Laboratory of Big Data Computing</rs>, and the <rs type="funder">Chinese University of Hong Kong, Shenzhen</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rgvczCf">
					<idno type="grant-number">2020B0101350001</idno>
				</org>
				<org type="funding" xml:id="_Vx9QcBh">
					<idno type="grant-number">NO. 61976250</idno>
				</org>
				<org type="funding" xml:id="_Z65xvrr">
					<idno type="grant-number">2020B1515020048</idno>
				</org>
				<org type="funding" xml:id="_vHrFSsf">
					<idno type="grant-number">JCYJ20220818103001002</idno>
				</org>
				<org type="funding" xml:id="_8ycPQjZ">
					<idno type="grant-number">JCYJ20220530141211024</idno>
				</org>
				<org type="funding" xml:id="_9gpKt5p">
					<idno type="grant-number">KCXFZ20201221173008022</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03376</idno>
		<title level="m">Endoscopy disease detection challenge 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Assessing generalisability of deep learning-based polyp detection and segmentation methods through a computer vision challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.12031</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring simple Siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CenterNet: keypoint triplets for object detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16000" to="16009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dense semantic contrast for self-supervised visual representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1368" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-10602-1_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-10602-1_48" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unbiased teacher for semi-supervised object detection</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09480</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A simple semi-supervised learning framework for object detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04757</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SimMIM: a simple framework for masked image modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9653" to="9663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end semi-supervised object detection with soft teacher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3060" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised video salient object detection using pseudo-labels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7284" to="7293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lesion-aware dynamic kernel for polyp segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="99" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_10" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive context selection for polyp segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-2_25" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised correction learning for semisupervised biomedical image segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-3_13" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="134" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-level contrastive learning and consistency constraint for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI52829.2022.9761710</idno>
		<ptr target="https://doi.org/10.1109/ISBI52829.2022.9761710" />
	</analytic>
	<monogr>
		<title level="m">IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised spatial temporal attention network for video polyp segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_44</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_44" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="456" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dense teacher: dense pseudo-labels for semi-supervised object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20077-9_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20077-9_3" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022: 17th European Conference</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Tel Aviv, Israel; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">October 23-27, 2022. 2022</date>
			<biblScope unit="page" from="35" to="50" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IX</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
