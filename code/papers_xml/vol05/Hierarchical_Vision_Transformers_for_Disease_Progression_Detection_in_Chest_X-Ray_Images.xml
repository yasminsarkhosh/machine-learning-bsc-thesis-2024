<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Amarachi</forename><forename type="middle">B</forename><surname>Mbakwe</surname></persName>
							<email>bmamarachi@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lyuyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">McMaster University</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mehdi</forename><surname>Moradi</surname></persName>
							<email>moradm4@mcmaster.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">McMaster University</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ismini</forename><surname>Lourentzou</surname></persName>
							<email>ilourentzou@vt.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A4CFB085FF72C8ECEDC38C801CC97D1C</idno>
					<idno type="DOI">10.1007/978-3-031-43904-966.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vision Transformers</term>
					<term>Disease Progression</term>
					<term>Chest X-Ray Comparison Relations</term>
					<term>Longitudinal CXR Relationships</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Chest radiography is a commonly used diagnostic imaging exam for monitoring disease progression and treatment effectiveness. While machine learning has made significant strides in tasks such as image segmentation, disease diagnosis, and automatic report generation, more intricate tasks such as disease progression monitoring remain fairly underexplored. This task presents a formidable challenge because of the complex and intricate nature of disease appearances on chest X-ray images, which makes distinguishing significant changes from irrelevant variations between images challenging. Motivated by these challenges, this work proposes CheXRelFormer, an end-to-end siamese Transformer disease progression model that takes a pair of images as input and detects whether the patient's condition has improved, worsened, or remained unchanged. The model comprises two hierarchical Transformer encoders, a difference module that compares feature differences across images, and a final classification layer that predicts the change in the patient's condition. Experimental results demonstrate that CheXRelFormer outperforms previous counterparts. Code is available at https://github.com/PLAN-Lab/CheXRelFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chest X-rays (CXRs) are frequently used for disease detection and disease progression monitoring. However, interpreting CXRs can be challenging and timeconsuming, particularly in regions with a shortage of radiologists. This can lead to delayed or inaccurate diagnoses and management, potentially harming patients. Automating the CXR interpretation process can lead to faster and more accurate diagnoses. Advances in Artificial Intelligence (AI), particularly in the field of computer vision for medical imaging, have significantly alleviated the challenges faced in radiology. The availability of large labeled collections of CXRs has been instrumental in driving progress in this area <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. Both CXR disease detection and automatic report generation have witnessed substantial improvements <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>. Remarkably, AI-based methods for finding detection are now approaching the performance level of experienced radiologists <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>. Moreover, just as vision transformers have revolutionized various areas of computer vision <ref type="bibr" target="#b11">[12]</ref>, they have also become an integral part of automatic CXR analysis <ref type="bibr" target="#b17">[18]</ref>.</p><p>Although significant strides have been made in AI-assisted medical image segmentation and disease detection, tasks requiring intricate reasoning have received less attention. One such complex task is monitoring disease progression in a sequence of images, which is particularly critical in assessing patients with pneumonia and other CXR findings. For example, temporal lung changes serve as vital indicators of patient outcomes and are routinely mentioned in radiology reports for determining the course of treatment <ref type="bibr" target="#b19">[20]</ref>. Prior work has investigated tracking the progression of COVID-19 pulmonary diseases and predicting outcomes <ref type="bibr" target="#b12">[13]</ref>. Recently, CheXRelNet was proposed, which utilizes graph attention networks to capture anatomical correlations and detect changes in CXRs using both local and global anatomical information <ref type="bibr" target="#b10">[11]</ref>. Change detection between longitudinal patient visits has also been studied in modalities beyond CXR, such as osteoarthritis in knee radiographs and retinopathy in retinal photographs <ref type="bibr" target="#b13">[14]</ref>. Nonetheless, prior works have faced limitations in effectively attending to finegrained relevant changes while disregarding irrelevant variations. Additionally, it is important to capture long-range spatial and temporal information to identify pertinent changes in medical images effectively.</p><p>Inspired by the success of Transformer models in remote sensing change detection tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b27">28]</ref>, we introduce CheXRelFormer, an end-to-end siamese disease progression model. CheXRelFormer takes a pair of CXR images as input, extracts visual features with a hierarchical vision Transformer module, and subsequently computes multi-level feature differences with a difference module. A self-attention mechanism allows the model to identify the most informative regions of the input images. By attending to fine-grained relevant changes, our model can accurately detect whether the patient's condition has improved, worsened, or remained unchanged. We evaluate the performance of our model on a large dataset of paired CXR images with corresponding disease progression labels. Experimental results demonstrate that our model outperforms existing state-of-the-art methods in detecting disease progression in CXR images. Our model has the potential to improve the efficiency and accuracy of CXR interpretation and thereby lead to more personalized treatment plans for patients. The contributions of our work can be summarized as follows:</p><p>(1) We propose CheXRelFormer, an end-to-end siamese disease progression model that can accurately detect changes in CXR image pairs by attend- ing to informative regions and identifying fine-grained relevant visual differences.</p><p>(2) CheXRelFormer leverages hierarchical vision Transformers and a difference module to compute multi-level feature differences across CXR images, allowing the model to capture long-range spatial and temporal information. (3) We experimentally demonstrate that CheXRelFormer outperforms existing state-of-the-art baselines in detecting disease progression in CXR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Let C = {(X, X ) i } N i=1 be a set of CXR image pairs, where X, X ∈ R H×W ×C , and H, W , and C are the height, width, and number of channels, respectively. Each image pair (X, X ) i is associated with a set of labels Y i = {y i,m } M m=1 , where y i,m ∈ {0, 1, 2} indicates whether the pathology m appearing in the image pair has improved, worsened, or remained the same. The goal is to design a model that accurately predicts the disease progression labels for an unseen image pair (X, X ) and a wide range of pathologies.</p><p>To this end, we use a hierarchical Transformer <ref type="bibr" target="#b11">[12]</ref> encoder to process each image pair. Specifically, let X, X ∈ R H×W ×C be the input image pair. The encoder consists of L identical Transformer layers, each with a multi-head selfattention block followed by a position-wise feedforward network. The multi-head self-attention block contains a series of self-attention heads and is defined as</p><formula xml:id="formula_0">MultiHead(Q, K, V) = Concat(head 1 , head 2 , . . . , head J )W O , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where Q, K, V ∈ R N ×C are the queries, keys, and values, respectively; W O is a learned weight matrix, J is the number of heads, and head j is the j-th attention head, computed as</p><formula xml:id="formula_2">head j = Attention(Q j , K j , V j ) = softmax Q j K T j √ d k V j . (2)</formula><p>Here, d k is the dimensionality of the key and query vectors in each head, and the softmax function is applied along the rows of the matrix. The queries, keys, and values Q j , K j , and V j are obtained via a set of linear projection matrices as</p><formula xml:id="formula_3">Q j = XW Q j , K j = XW K j , V j = XW V j ,<label>(3)</label></formula><p>where W Q , W K , W V are learned weight tensors that project the input embeddings onto a lower-dimensional space. Similarly, the query, key, and value matrices for the second image in the pair are computed as</p><formula xml:id="formula_4">Q j = X W Q j , K j = X W K j , V j = X W V j , (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where the weight tensors W Q , W K , W V are shared across the two images in the pair. The output of each multi-head self-attention block for each image pair, denoted by (F, F ), is then fed into a position-wise feedforward network which consists of two linear transformations and a depth-wise convolution <ref type="bibr" target="#b3">[4]</ref> that captures local spatial information:</p><formula xml:id="formula_6">F c = f 1 (Conv2D(f 2 (F), W d ))<label>(5)</label></formula><formula xml:id="formula_7">F c = f 1 (Conv2D(f 2 (F ), W d )).<label>(6)</label></formula><p>Here, W d is the shared depth-wise convolution weight matrix and each feedforward layer f 1 , f 2 consists of a linear transformation followed by a non-linear activation. The difference module then processes the visual features from each Transformer layer to compute multi-level feature differences as follows:</p><formula xml:id="formula_8">C l = φ Conv2D ([F l , F l ] , W l ) . (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>Here, l = 1, . . . , L denotes the l-th Transformer layer, with initial inputs (F 1 , F 1 ) = (F c , F c ). Furthermore, φ is a non-linear activation, W l is a learned weight parameter that essentially represents a multi-scale trainable distance metric, and [•, •] is the concatenation operation. By computing differences between features at different scales, the proposed model can capture local and global structures that are relevant to the disease progression task. Each multi-scale feature difference map is then passed through a feed-forward layer that maps the input features to a common feature space</p><formula xml:id="formula_10">C l out = f θ l (C l ), ∀l ∈ [1, L], (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where θ l represents the set of learnable parameters for the l-th feed-forward network. The concatenated feature tensor combines information from multiple scales and is denoted as</p><formula xml:id="formula_12">C out = C 1 out , . . . , C l out , . . . , C L out ,<label>(9)</label></formula><p>where [•] denotes concatenation along the channel dimension. A feed-forward network with a global average pooling step, denoted by g, creates a fused feature representation with fixed dimensionality, which is finally passed through the final classification layer, denoted by h, to obtain the label predictions</p><formula xml:id="formula_13">ŷ = h g(C out ) . (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>The network is trained end-to-end with a multi-label cross-entropy classification loss</p><formula xml:id="formula_15">L = 1 N 1 M N i=1 M m=1 y i,m log(σ(ŷ i,m )) + (1 -y i,m ) log(1 -σ(ŷ i,m )),<label>(11)</label></formula><p>where σ represents the sigmoid function and ŷi,m , y i,m are the model prediction and the ground truth for example (X, X ) i . An overview of the model architecture is represented in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>CheXRelFormer is implemented in Pytorch <ref type="bibr" target="#b18">[19]</ref>. The encoder comprises four Transformer blocks with embedding dimensions 32, 64, 128, and 256, respectively. The number of heads on each multi-head attention block is 2, 2, 4, and 8. We train the encoder with a stochastic depth decay rule <ref type="bibr" target="#b5">[6]</ref>, with depths 3, 3, 6, and 18, for each Transformer block. To decrease the spatial dimension and reduce complexity, we perform spatial-reduction operations <ref type="bibr" target="#b21">[22]</ref>. The positionwise feedforward network uses Gaussian Error Linear Unit (GELU) <ref type="bibr" target="#b4">[5]</ref> activation functions. The multi-level image features extracted from the Transformer encoder are passed to the difference module. The difference module is composed of 2D convolutions, ReLU activations <ref type="bibr" target="#b1">[2]</ref>, and Batch Normalization <ref type="bibr" target="#b6">[7]</ref>. The feedforward layers consist of 64 neurons. The outputs from the difference module are upsampled, concatenated, and passed through a linear fusion layer followed by global average pooling. The model is trained using AdamW optimizer <ref type="bibr" target="#b16">[17]</ref> with a learning rate of 6 × 10 -5 and 16 batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset</head><p>We make use of the Chest ImaGenome dataset <ref type="bibr" target="#b24">[25]</ref>, which comprises 242, 072 frontal MIMIC-CXRs <ref type="bibr" target="#b8">[9]</ref> that were locally labeled using a combination of rulebased natural language processing (NLP) and CXR atlas-based bounding box detection techniques <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref> to generate the annotations. Chest ImaGenome is represented as an anatomy-centered scene graph with 1, 256 combinations of relation annotations between 29 CXR anatomical locations and their attributes. Each image is structured as one scene graph, resulting in approximately 670, 000 localized comparison relations between the anatomical locations across sequential exams. In this work, we focus on the localized comparison relations data within Chest ImaGenome that pertains to cross-image relations for nine diseases of interest. Each comparison relation in the Chest ImaGenome dataset includes the DICOM identifiers of the two CXRs being compared, the comparison label, and the disease label name. The comparison is labeled as "no change", "improved" or "worsened", which indicates whether the patient's condition w.r.t. the disease has remained stable, improved, or worsened, respectively. The dataset contains 122, 444 unique comparisons. We use 35, 908 CXR pairs in total that pertain to the nine diseases of interest. The distribution of the data is improved <ref type="bibr" target="#b11">(12,</ref><ref type="bibr">396)</ref>, worsened <ref type="bibr" target="#b11">(12,</ref><ref type="bibr">287)</ref> and no change <ref type="bibr" target="#b10">(11,</ref><ref type="bibr">205)</ref>. Table <ref type="table" target="#tab_0">1</ref> presents high-level dataset statistics and training/validation/test splits employed in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>To assess the performance of the proposed CheXRelFormer model, we conduct a comparative analysis with several baselines.</p><p>Local: This model employs a previously proposed siamese network <ref type="bibr" target="#b10">[11]</ref> that only focuses on specific regions of the image, without considering inter-region dependencies or global information. The Local model is essentially a siamese network with a pretrained ResNet101 autoencoder trained on cropped Regionsof-Interest (RoIs), which are available in the Chest ImaGenome dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global:</head><p>The Global model is a siamese network similar to the Local model but encodes global image-level information.</p><p>CheXRelNet: CheXRelNet combines global image-level information with local intra-image and inter-image information <ref type="bibr" target="#b10">[11]</ref>. This model consists of a 2-layer graph neural network with a ResNet101 autoencoder for feature extraction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental Results</head><p>Table <ref type="table" target="#tab_1">2</ref> lists the CXR change detection accuracy of all models across the nine diseases. We also report the mean weighted overall accuracy. CheXRelFormer outperforms baselines with a mean accuracy of 0.493±0.0012 in this three-way classification task. The closest baseline is CheXRelNet with an accuracy of 0.468 ± 0.0041. Additionally, we perform a one-tailed t-test between CheXRelFormer and CheXRelNet, with p = 0.00027 indicating that CheXRelFormer significantly outperforms CheXRelNet in seven of the nine diseases (pleural effusion, atelectasis, pulmonary edema/hazy opacity, heart failure, pneumonia, and consolidation).</p><p>Most importantly, we observe up to 12% performance gains for pathology labels with limited amounts of data, such as atelectasis and consolidation. These findings suggest that CheXRelFormer has the potential to be a valuable tool for detecting changes in CXR images associated with various common diseases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablations on CheXRelFormer Architecture Components</head><p>We perform an ablation study to understand the impact of four factors, the difference module, the use of global vs. localized visual information, and the impact of multi-level features. Specifically, in  An interesting observation is that CheXRelFormer Local underperforms as the focus on specific anatomies limits the visual information available to the model. Given the highly fine-grained nature of this task, this result suggests that the relationship between an area and its surroundings is critical to a radiologist's perception of change, and the local Transformer cannot provide the necessary second-order information to the model. Therefore, our results show that global image-level information is crucial for accurately predicting disease change.</p><p>In addition, the absolute difference model, CheXRelFormer AbsDiff, failed to perform the disease change classification task, indicating the importance of the proposed difference module. By incorporating the difference module and computing multi-level feature differences at multiple resolutions, CheXRelFormer learns to focus on the changes between two CXRs and to ignore irrelevant information. Our results demonstrate that multi-level feature differences are critical for improving performance in predicting disease change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Qualitative Analysis</head><p>In Fig. <ref type="figure">2</ref>, we visualize the model predictions from CheXRelFormer using attention rollout <ref type="bibr" target="#b0">[1]</ref>. The produced attention maps clearly show the model's focus regions, which confirm that the model concentrated on the correct region in each image pair. CheXRelFormer can better differentiate between important and extraneous visual signals, allowing it to more accurately predict the 'no change' label. The model's ability to learn the optimal distance metric for each scale allows differentiating between relevant and irrelevant differences. In addition, analyzing multiple scales of visual features enables capturing subtle changes in pairs of CXR images, resulting in a better predictive performance for the 'improved' label. In contrast, the CheXRelFormer AbsDiff model has difficulty in predicting both 'no change' and 'improved' labels (as shown in Fig. <ref type="figure">3</ref>) due to the fact that images are not co-registered and exhibit several differences in their spatial or spectral characteristics -even though there was no actual change in the observed pathology.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. CheXRelFormer overview. Given two CXR images (X, X ) a shared Transformer encoder extracts visual features at multiple resolutions. These feature maps are then fed into a difference module that computes visual differences across multiple scales. The difference module enhances the ability of the model to capture disease progression by effectively attending to relevant changes in the image pair. The decoder fuses information and performs the final disease progression classification task.</figDesc><graphic coords="3,56,46,73,58,246,79,57,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dataset Statistics: pathology ID and label, number of training, validation and test CXR image pairs, and total number of CXR pairs</figDesc><table><row><cell>ID Pathology Label</cell><cell cols="4">Train Val Test Total Pairs</cell></row><row><cell>LO Lung Opacity</cell><cell cols="2">5,516 912</cell><cell cols="2">1,579 8,007</cell></row><row><cell>PE Pleural Effusion</cell><cell cols="4">7,450 1,089 2,165 10,704</cell></row><row><cell>AT Atelectasis</cell><cell>77</cell><cell>11</cell><cell>26</cell><cell>114</cell></row><row><cell>EC Enlarged Cardiac Silhouette</cell><cell cols="2">5,251 715</cell><cell cols="2">1,555 7,521</cell></row><row><cell cols="3">HO Hazy Opacity/Pulmonary Edema 2,939 454</cell><cell>884</cell><cell>4,277</cell></row><row><cell>PX Pneumothorax</cell><cell>979</cell><cell>132</cell><cell>261</cell><cell>1,372</cell></row><row><cell>CO Consolidation</cell><cell>142</cell><cell>15</cell><cell>46</cell><cell>203</cell></row><row><cell>HF Heart Failure/Fluid Overload</cell><cell>791</cell><cell>113</cell><cell>223</cell><cell>1,127</cell></row><row><cell>PN Pneumonia</cell><cell cols="2">1,757 283</cell><cell>543</cell><cell>2,583</cell></row><row><cell>Total</cell><cell cols="4">24,902 3,724 7,282 35,908</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison against the baselines Method LO PE AT EC HO PX CO HF PN All</figDesc><table><row><cell>Local</cell><cell>0.41 0.37 0.41 0.29 0.37 0.37 0.49 0.29 0.42 0.43</cell></row><row><cell>Global</cell><cell>0.45 0.47 0.44 0.48 0.48 0.36 0.47 0.50 0.43 0.45</cell></row><row><cell>CheXRelNet</cell><cell>0.49 0.47 0.44 0.49 0.49 0.36 0.47 0.44 0.47 0.47</cell></row><row><cell cols="2">CheXRelFormer 0.48 0.51 0.54 0.40 0.58 0.35 0.59 0.53 0.51 0.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation analysis of CheXRelFormer variants.</figDesc><table><row><cell>Method</cell><cell>LO PE AT EC HO PX CO HF PN All</cell></row><row><cell cols="2">CheXRelFormer AbsDiff 0.35 0.37 0.27 0.29 0.35 0.37 0.24 0.34 0.38 0.34</cell></row><row><cell>CheXRelFormer Local</cell><cell>0.33 0.39 0.12 0.26 0.38 0.27 0.24 0.46 0.49 0.35</cell></row><row><cell>CheXRelFormer</cell><cell>0.48 0.51 0.54 0.40 0.58 0.35 0.59 0.53 0.51 0.49</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 ,</head><label>3</label><figDesc>we present a comparison of in CheXRelFormer against CheXRelFormer AbsDiff, where we replace the proposed difference module with an absolute difference component that subtracts two visual features from the last Transformer block. We also consider CheXRelFormer Local, a variant that is trained on cropped anatomical RoIs instead of the entire image. Additional model variants are presented in the supplementary material. Table 4 presents the number of trainable parameters (i.e., model capacity) and training time per epoch for each model variant.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Model Capacity Comparison</figDesc><table><row><cell>Measures</cell><cell cols="3">CheXRelFormer AbsDiff CheXRelFormer Local CheXRelFormer</cell></row><row><cell cols="2">Number of Parameters (M) 28.9</cell><cell>41.0</cell><cell>41.0</cell></row><row><cell cols="2">Time per epoch (minutes) 148.8</cell><cell>56.4</cell><cell>171</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior Current</head><note type="other">Ground</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Monitoring disease progression is a critical aspect of patient management. This task requires skilled clinicians to carefully reason and evaluate changes in a patient's condition. In this paper, we propose CheXRelFormer, a hierarchical Transformer with a multi-scale difference module, trained on global image pair information to detect disease changes. Our model is inspired by the way clinicians monitor changes between CXRs, and improves the state of the art in this challenging medical imaging task. Our ablation studies show that global attention and the proposed difference module are critical components, and both help detect fine-grained changes between images. While our work shows significant progress, given the fine-grained nature of visual features that characterize findings in CXRs, disease progression remains a challenging task. In future work, we intend to include multimodal contextual information beyond the images, such as patient history and reports, to enhance the results. CheXRelFormer offers a promising solution for monitoring disease progression, and future work can extend the proposed methodology to various medical imaging modalities.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4190" to="4197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep learning using rectified linear units</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Agarap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A transformer-based siamese network for change detection</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G C</forename><surname>Bandara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2022-2022 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="207" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46493-039" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016: 14th European Conference</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">October 11-14, 2016. 2016</date>
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chexpert: a large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Berkowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mimic-iii, a freely accessible critical care database</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CheXRelNet: an anatomy-aware model for tracking longitudinal relationships between chest X-rays</title>
		<author>
			<persName><forename type="first">G</forename><surname>Karwande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Mbakwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lourentzou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_55</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-655" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="page" from="581" to="591" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated assessment and tracking of Covid-19 pulmonary disease severity on chest radiographs using convolutional siamese neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology: Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Siamese neural networks for continuous disease severity evaluation and change detection in medical imaging</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive attention for automatic chest x-ray report generation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="269" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clinically accurate chest x-ray report generation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Machine Learning for Healthcare Conference. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Wiens</surname></persName>
		</editor>
		<meeting>the 4th Machine Learning for Healthcare Conference. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="249" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-task vision transformer using low-level chest x-ray feature corpus for Covid-19 diagnosis and severity quantification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">102299</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual Conference on Neural Information Processing Systems (NeurIPs)</title>
		<meeting>the 32nd Annual Conference on Neural Information Processing Systems (NeurIPs)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Chest x-ray findings and temporal lung changes in patients with Covid-19 pneumonia</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Rousan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elobeid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karrar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Khader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Pulm. Med</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated abnormality classification of chest radiographs using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Digital Med</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tienet: Text-image embedding network for common thorax disease classification and reporting in chest x-rays</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference On Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference On Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9049" to="9058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic bounding box annotation of chest x-ray data for localization of abnormalities</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<meeting>the 17th International Symposium on Biomedical Imaging (ISBI)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="799" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Chest imagenome dataset for clinical reasoning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ai accelerated human-in-the-loop structuring of radiology reports</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Americal Medical Informatics Association (AMIA) Annual Symposium</title>
		<meeting>the Americal Medical Informatics Association (AMIA) Annual Symposium</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comparison of chest radiograph interpretations by artificial intelligence algorithm vs radiology residents</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Netw. Open</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Remote sensing image change detection based on deep multi-scale multi-attention siamese transformer network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">842</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
