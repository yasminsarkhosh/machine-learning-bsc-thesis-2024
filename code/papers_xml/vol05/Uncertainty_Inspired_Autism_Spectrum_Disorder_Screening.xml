<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty Inspired Autism Spectrum Disorder Screening</title>
				<funder ref="#_CpYKC7s #_bXXJTaH">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_w5k8vTP">
					<orgName type="full">Beijing Natural Science Foundation</orgName>
				</funder>
				<funder ref="#_xsDkdgN">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Yaping Huang (B</roleName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Laboratory of Traffic Data Analysis and Mining</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaping</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiansong</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Laboratory of Traffic Data Analysis and Mining</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sihui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Laboratory of Traffic Data Analysis and Mining</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mei</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Laboratory of Traffic Data Analysis and Mining</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Key Laboratory of Traffic Data Analysis and Mining</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Uncertainty Inspired Autism Spectrum Disorder Screening</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="399" to="408"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">3E43019F46FC3F7DBBE9B483E3039E63</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_39</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>ASD Screening</term>
					<term>Eye-tracking</term>
					<term>Data Uncertainty</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>People with autism spectrum disorder (ASD) show distinguishing preferences for specific visual stimuli compared to typically developed (TD) individuals, opening the door for objective and quantitative screening by eye-tracking data analysis. However, existing eyetracking-based ASD screening approaches often assume that there are no individual differences and that all stimuli contribute equally to the prediction of an ASD. Consequently, a fixed number of images are usually selected by a pre-defined strategy for further training and testing, ignoring the distinct characteristics of various subjects viewing the same image. To address the aforementioned difficulties, we propose a novel Uncertainty-inspired ASD Screening Network (UASN) that dynamically modifies the contribution of each stimulus viewed by different subjects. Specifically, we estimate the uncertainty of each stimulus by considering the variation between the subject's fixation map and the ones of the two clinical groups (i.e., ASD and TD) and further utilize it for weighting the training loss. Besides, to reduce the diagnosis time, instead of the shuffle-appeared mode of image viewing, we propose an uncertaintybased personalized diagnosis method to dynamically rank the viewing images according to the preferences of different subjects, which can achieve high prediction accuracy with only a small set of images. Experiments demonstrate the superior performance of our proposed UASN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Autism Spectrum Disorder (ASD) has been a prevalent neurodevelopmental disorder worldwide that one in 44 kids aged 8 years in the United States suffers from it as reported in 2021 <ref type="bibr" target="#b14">[15]</ref> and there is still a steady and substantial growth in the population.</p><p>However, diagnosing ASD relies on subjective evaluations that are expensive and clinically demanding.</p><p>Seminal works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref> have pointed out that eye movement patterns of people with ASD play an irreplaceable vital role in identifying ASD.</p><p>Early efforts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> usually focus on low-level behavior features combined with machine learning algorithms to identify autism, while in recent years, the eye-tracking data driven method <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref> boosts the performance of ASD screening by utilizing deep neural networks (DNNs) which extract high-level semantic information of eye movement.</p><p>However, existing deep-learning-based methods usually define an imageranking strategy as a pre-processing step to select a certain number of images.</p><p>During training, each visual stimulus is treated equally, ignoring the distinct contributions of different stimuli. Besides, during the diagnosis procedure, a fixed number of images are shown to a subject which takes a relatively long time, thereby leading to poor cooperation of subjects, especially little kids.</p><p>To tackle the above issues, in this paper, we propose a novel Uncertaintyinspired ASD Screening Network, named UASN, to distinguish the importance of each visual stimulus for different individuals. Despite the success of uncertainty in computer vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, to our best knowledge, this is the first attempt to introduce uncertainty estimation into ASD screening. Our uncertainty-inspired UASN can enforce the model learning from more distinctive gaze patterns during training. Meanwhile, when the model is deployed in the real clinical scenario, we further design an efficient personalized diagnosis strategy, which can dramatically reduce the diagnosis time without a performance drop.</p><p>Specifically, the uncertainty in UASN works in two ways to ensure both higher accuracy and lower time consumption. On the one hand, given an input gaze pattern, we estimate the uncertainty by comparing the difference between the fixation map and the ones of ASD and TD groups. The uncertainty will be assigned a lower value for a larger disparity, suggesting the importance of the given gaze pattern for identifying a certain individual. Subsequently, guided by the estimated uncertainty, we design a truncated weighting loss to select the most distinctive gaze patterns and further dynamically adjust the contributions made by different stimuli, resulting in a more efficient classification. On the other hand, how to reduce the diagnosis time is also a key factor in real clinical applications, especially for preschool children. To achieve this goal, we propose a personalized diagnosis method that ranks the stimuli according to the estimated uncertainty. Instead of the random shuffle mode for image viewing, we recommend a top similar or dissimilar stimulus for the next viewing according to the decision of the previous gaze patterns. Following the proposed protocol, our method achieves state-of-the-art performance while spending much less diagnosis time.</p><p>In general, our contributions can be summarized as follows: 1) we propose the first usage of the Uncertainty-inspired ASD Screening Network, named UASN, for identifying ASD people; 2) we estimate the uncertainty of each gaze pattern and further design a truncated weighting loss, which can enforce the model to dynamically adjust the contributions of different gaze patterns during training; 3) we design a personalized online diagnosis protocol that can dramatically reduce the diagnosis time without losing accuracy; 4) we conduct comprehensive experiments on the Saliency4ASD benchmark and achieve state-of-the-art performance only using 1/5 visual stimuli compared with other leading approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Uncertainty Inspired ASD Screening</head><p>Our UASN is built upon traditional DNNs and consists of two novel stages: 1) uncertainty-guided training, and 2) uncertainty-guided personalized diagnosis.</p><p>During training, we estimate an uncertainty value for each gaze pattern and further apply it for weighting the training loss. Besides, for a more simplified diagnosis procedure, we design an uncertainty-based strategy that adaptively selects the most discriminative images based on the subject's gaze behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Uncertainty Guided Training</head><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates the detailed training process of UASN. Firstly, we extract the features of gaze patterns by taking the temporal eye tracking information as input and resulting in the classification prediction. Then, we design an uncertainty estimation module to compute the uncertainty values of each subject on all the visual stimuli. Moreover, we further apply the estimated uncertainty to weight the training samples by a truncated loss in a reasonable manner.</p><p>Gaze Pattern Feature Extraction. Formally, by collecting a group of ASD and TD subjects S = {s i } M i=1 's eye movement data on a set of images X = {x j } N j=1 , we get the corresponding scanpaths which comprise each fixation point's position and duration in the temporal order. The labels of the two clinical groups are denoted as Y = {y i } M i=1 ∈ {0, 1}. To generate the discriminative features of the given gaze pattern (i-th subject watching j-th image), we first feed the image x j into a ResNet-50 <ref type="bibr" target="#b8">[9]</ref> with the last max pooling layer removed to learn a 2048-dimension visual feature. Then the visual feature sequence taken from the scanpath of i-th subject is fed into a Long Short Term Memory (LSTM) network <ref type="bibr" target="#b7">[8]</ref>. A final hidden state is obtained at the end of the sequence which is then fed into a fully connected (FC) layer followed by a sigmoid function to get the prediction result for the i-th subject viewing j-th image, which is denoted as ŷj i . The network can be optimized by the binary cross-entropy (BCE) loss:</p><formula xml:id="formula_0">L bce = - 1 NM i j (y j i log ŷj i + (1 -y j i ) log(1 -ŷj i )),<label>(1)</label></formula><p>where ŷj i and y j i denote the predicted and ground truth labels of i-th subject respectively. If belonging to ASD, y j i = 1 for all images {x j } N j=1 , otherwise 0. Uncertainty Estimation. We believe that different images contribute unequally to a subject's final classification due to the subject's unique preferences for viewing images. As a result, we estimate an uncertainty value for each gaze pattern based on the variance between its fixation map and the ones of the two clinical groups (i.e., ASD and TD). For instance, an ASD's fixation map on a discriminative image should appear more similar to the ASD group's averaged one than the TD group's so the variance is supposed to be large.</p><p>We first generate the fixation map for each gaze pattern according to the fixation data. Then, given two groups' fixation maps on each image in the dataset, for each subject, we apply cosine similarity to compute an uncertainty measurement on each image. Let F j i denote the fixation map of the i-th subject's fixation map on the j-th image, and F j + , F j -denote the fixation maps of ASD and TD group for j-th image respectively. The uncertainty can be written as:</p><formula xml:id="formula_1">D j i = C(F j i , F j + ) -C(F j i , F j -) , (<label>2</label></formula><formula xml:id="formula_2">)</formula><formula xml:id="formula_3">µ j i = 1 -D j i , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where C is the cosine similarity function, D j i is the distinguishability of the j-th image when viewed by the i-th subject and µ j i denotes the uncertainty. Specifically, for images that do not contain certain subjects' eye-tracking data, we reasonably set the µ j i to be large because we assume that the absence of eye-tracking data is due to a lack of interest, and further signals ineffectiveness.</p><p>Truncated Weighting Loss. Upon obtaining the uncertainty value µ j i , we can utilize the uncertainty to re-weight the training loss by teaching the model which images to trust and which to discredit. We hope that the larger the µ is, the less the image contributes to the final classification, so the corresponding loss needs to be reduced correspondingly.</p><p>However, for some gaze patterns that are confusing and much more difficult to distinguish between ASD and healthy people, it is more suitable to discard those unreliable patterns. Considering this, we finally propose a truncated weighting BCE loss for training. Specifically, when the estimated uncertainty value is larger than a pre-defined threshold t, we set the corresponding loss to zero. In summary, The final loss function is denoted as:</p><formula xml:id="formula_5">L tr_bce = i j I [µ≤t] (1 -µ j i )L bce , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where I is the indicator function and t is the threshold. Only when the condition of [µ ≤ t] is met, is the value of the indicator function set to 1, and the L tr _bce remains. Specifically, for a more reasonable computation, we do not simply set the t to be a fixed value. Instead, we determine t with an adaptive technique.</p><p>For each subject, we sort the uncertainty values from low to high and choose the 1/3 of the images with the lowest uncertainty and retain the contributions they make to the prediction while discarding the remainder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Uncertainty Guided Personalized Diagnosis</head><p>On the basis of training our model in an uncertainty-guided way, we are encouraged to go deeper to simplify the diagnostic procedure. To this end, we incorporate personalized diagnosis into our work, taking into account the gaze behavior features of each subject. Figure <ref type="figure" target="#fig_1">2</ref> presents the workflow of our personalized diagnosis protocol after completing the training process in Fig. <ref type="figure" target="#fig_0">1</ref>. Specifically, by extracting features of images and forming a feature bank, we selectively choose the most suitable images to update the viewing list according to the subject's viewing pattern. Image Feature Extraction. First, we assume that the visual similarity between images brings the potential of personalized diagnosis that similar images may contribute similarly in distinguishing an individual. From this point, we extract the feature of all the 300 images in the dataset (the Image set in Fig. <ref type="figure" target="#fig_1">2</ref>) using a ResNet-18 <ref type="bibr" target="#b8">[9]</ref> followed by an FC to get a 128-dimension feature, thereby forming a feature bank getting prepared for the further dynamical ranking procedure.</p><p>Similarity-Based Image Ranking. Then, we build a viewing list simulating the diagnosis procedure where images are shown to a subject one by one and the list is updated in real-time. In the beginning, we select an image from the image set randomly to initialize the viewing list. When a trial is completed, the viewing list is subsequently updated. In each trial, we generate an average feature of all images in the viewing list. We then compute the cosine similarity between the feature of the current image and images in the feature bank to obtain a similarity list. We sort the list in similarity-ascending order.</p><p>Uncertainty-Based Viewing List Updating. To determine which images should be included in the viewing list, a method based on uncertainty is developed. First, we feed the image in the list and the corresponding eye-tracking data into the ASD screening network which is composed of the Uncertainty Estimation module and the Gaze Pattern Feature Extraction module in Fig. <ref type="figure" target="#fig_0">1</ref> to get the uncertainty and the prediction result. By pre-defining a threshold p, we separate the scenario into a positive case and a negative one. When the average uncertainty value is larger than p, we consider it negative so we select the top K dissimilar images to join the viewing list and vice versa. After T trials, we achieve a relatively confident prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Experimental Settings</head><p>Dataset. So far, only Saliency4ASD <ref type="bibr" target="#b3">[4]</ref> is publicly released for the evaluation of ASD screening. It consists of 300 images from a public dataset collected by Judd et al . <ref type="bibr" target="#b10">[11]</ref> and the eye movement data collected from 14 kids with ASD and 14 with TD. For each image, the scanpath of each subject is provided, allowing us to derive the single fixation map for additional uncertainty computation.</p><p>Evaluation Protocol. We employ the leave-one-subject-out cross-validation method for evaluating the model's performance. Specifically, for the Saliency4ASD dataset, we perform a 28-round validation with each round selecting only one subject for testing while the remaining 27 subjects work as the training data.</p><p>Metrics. We follow the previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref> to adopt the accuracy, sensitivity, specificity, and AUC to evaluate the performance of the prediction for each subject. Besides, we assess the performance of the prediction for each scanpath to generate a more strict measurement. We still adopt accuracy, sensitivity, and specificity, called Acc_I, Sen_I, and Spe_I. Accordingly, for the previously used three metrics, we denote them as Acc_S, Sen_S, and Spe_S.</p><p>Implementation Details. The experimental setting mostly follows the <ref type="bibr" target="#b1">[2]</ref> during training. Besides, we manually set the uncertainty values of those images without some certain subjects' eye movement data to be 1-10 -5 which is a relatively large margin that rarely contributes to the classification. For personalized diagnosis, we initialize the viewing list by randomly selecting an image. By considering the gaze pattern with an uncertainty level lower than p = 0.9 a positive case, we choose the top K most similar images to update the viewing list. We set K to be 1 and update the viewing list T = 20 trials in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with State-of-the-Art</head><p>We conduct extensive experiments to explore whether our proposed model outperforms the state-of-the-art. We compare our UASN with <ref type="bibr" target="#b1">[2]</ref> which selects a fixed 100 images subset out of a total of 300 images according to a Fisher-Scorebased image selection strategy for training and testing. We design the following three UASN variants: 1) UASN-noUnLoss only uses uncertainty to choose the top most discriminative images and removes the weighting loss procedure for training; 2) UASN-Fixed selects a 100-image subset with the lowest uncertainty for each subject during training and testing without dynamically adjusting the diagnosis process; 3) UASN-Dynamic uses 100 images with low uncertainty for training. In the diagnosis process, we adopt the proposed uncertainty-guided personalized diagnosis to recommend a small number of images, which significantly reduces the diagnosis time while maintaining high accuracy. Table <ref type="table" target="#tab_0">1</ref> shows the results. We can see that our model's three variants all outperform the baseline model by all evaluation metrics. The result demonstrates that introducing uncertainty during training can reach 100% accuracy. When removing the uncertainty weighting loss part, the UASN-noUnLoss model's performance drops slightly in scanpath level metrics than the best UASN-Dynamic, i.e., 3% (Acc_I), 3% (Sen_I) and 6% (Spe_I). Besides, the personalized diagnosis strategy achieves the same accuracy but largely reduces the number of images to 1/5 (20 images), which decreases the diagnosis time dramatically. In terms of scanpath level metrics, our UASN-Dynamic outperforms the previous leading model <ref type="bibr" target="#b1">[2]</ref> by 20% in Acc_I and Sen_I, and 19% in Spe_I. The results suggest introducing uncertainty both in the training and testing stage achieves the best performance with a quite small image subset at classifying the two clinical groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>Effect of the Uncertainty Estimation. To verify the effect of uncertainty estimation, we divide the 300 images into three non-overlapping subsets based on the ascending order of uncertainty level, denoted as top-100, middle-100, and bottom-100 subsets. Table <ref type="table" target="#tab_1">2</ref> shows the results. It is not surprising that UASN-Dynamic with top-100 achieves the best performance and the large performance drop on both UASN-Fixed and UASN-Dynamic approves our model's strong capability of selecting the most discriminative images for classifying ASD and TD. We further visualize the relation between Acc_I and uncertainty, as well as give some samples of ASD and TD's fixation maps with different uncertainty values to support the effectiveness of our method. Details are given in Fig. <ref type="figure">I</ref>  Effect of Different Inference Strategies. During the diagnosis process, we need to define the number of trials (T ) and the number of images (K) selected to append to the viewing list. The results are given in Table <ref type="table" target="#tab_2">3</ref>. We tried various permutations of T and K and finally found that appending one image each trial </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we present UASN, a novel ASD screening approach, inspired by uncertainty. The uncertainty benefits the ASD diagnosis in two ways: a weighted truncated training loss that enables the model to learn the most discriminative and effective features of gaze patterns and a personalized procedure that dynamically ranks the stimuli according to the subject's gaze behaviors. Comprehensive results show superior performance in classifying ASD people.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the uncertainty-guided training module of UASN. Images with corresponding gaze patterns are fed into the Gaze Pattern Feature Extraction module to get the prediction result. By applying the Uncertainty Estimation module, we compute an uncertainty for each gaze pattern and further use it to weight the training BCE loss to get a more reasonable as well as efficient result.</figDesc><graphic coords="4,59,79,53,75,304,12,151,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of our proposed uncertainty-guided personalized diagnosis module. We feed the gaze patterns in the current viewing list into the ASD screening network to get a prediction as well as the uncertainty value µ. The µ is further used to refresh the viewing list with images based on the similarity-based ranking result.</figDesc><graphic coords="5,81,96,374,03,278,92,112,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and Fig. II of the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison on both the subject and scanpath level with state-of-the-art<ref type="bibr" target="#b1">[2]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">Subject level metrics</cell><cell></cell><cell cols="3">Scanpath level metrics</cell></row><row><cell></cell><cell cols="7">Acc_S Sen_S Spe_S AUC Acc_I Sen_I Spe_I</cell></row><row><cell>[2]</cell><cell>0.93</cell><cell>0.93</cell><cell>0.93</cell><cell cols="2">0.98 0.59</cell><cell>0.58</cell><cell>0.59</cell></row><row><cell cols="2">UASN-noUnLoss 1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0.76</cell><cell>0.75</cell><cell>0.72</cell></row><row><cell>UASN-Fixed</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0.74</cell><cell>0.71</cell><cell>0.72</cell></row><row><cell cols="2">UASN-Dynamic 1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell cols="3">0.79 0.78 0.78</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of using images of different uncertainty levels. The "top 100", "middle 100" and "bottom 100" denote the three subsets of 100-image with the lowest, the middle, and the highest uncertainty level.</figDesc><table><row><cell>Method</cell><cell cols="8">Selected ImageSet Acc_S Sen_S Spe_S AUC Acc_I Sen_I Spe_I</cell></row><row><cell>UASN-Fixed</cell><cell>top-100</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0.74</cell><cell>0.71</cell><cell>0.72</cell></row><row><cell></cell><cell>middle-100</cell><cell>0.78</cell><cell>0.54</cell><cell>0.93</cell><cell cols="2">0.86 0.56</cell><cell>0.50</cell><cell>0.56</cell></row><row><cell></cell><cell>bottom-100</cell><cell>0.54</cell><cell>0.25</cell><cell>0.83</cell><cell cols="2">0.59 0.50</cell><cell>0.46</cell><cell>0.51</cell></row><row><cell cols="2">UASN-Dynamic top-100</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell cols="3">0.79 0.78 0.78</cell></row><row><cell></cell><cell>middle-100</cell><cell>0.64</cell><cell>0.64</cell><cell>0.64</cell><cell cols="2">0.68 0.52</cell><cell>0.50</cell><cell>0.58</cell></row><row><cell></cell><cell>bottom-100</cell><cell>0.38</cell><cell>0.33</cell><cell cols="3">0.742 0.39 0.47</cell><cell>0.45</cell><cell>0.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of how different inference strategies influence our model's performance. T denotes the number of inference trials during the diagnosis and K denotes the number of images inferred in each trial. and performing 20 trials for one subject obtained the best performance while cost the least diagnosing time.Effect of Different Similarity Measurements and Backbones. We conduct experiments on replacing the similarity measurement method and the model's backbone. Results can be referred to in the TableIand Table II of supplementary.Computational Cost. Another crucial metric for assessing our model's efficiency is the time interval between every two viewing list update rounds during the personalized diagnosis process. Upon conducting experiments, we get an average of 0.038 s for each interval, demonstrating that our dynamic update strategy causes no delay in the clinical diagnosis.</figDesc><table><row><cell cols="7">T K Acc_S Sen_S Spe_S AUC Acc_I Sen_I Spe_I</cell></row><row><cell>20 1 1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell cols="3">0.79 0.78 0.78</cell></row><row><cell>10 2 0.96</cell><cell>0.93</cell><cell>1</cell><cell cols="2">0.99 0.75</cell><cell>0.74</cell><cell>0.77</cell></row><row><cell>10 1 0.96</cell><cell>0.93</cell><cell>1</cell><cell>1</cell><cell>0.73</cell><cell>0.71</cell><cell>0.74</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">Beijing Natural Science Foundation</rs> (<rs type="grantNumber">M22022</rs>, <rs type="grantNumber">L211015</rs>) and the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">62271042</rs>, <rs type="grantNumber">61906013</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_w5k8vTP">
					<idno type="grant-number">M22022</idno>
				</org>
				<org type="funding" xml:id="_xsDkdgN">
					<idno type="grant-number">L211015</idno>
				</org>
				<org type="funding" xml:id="_CpYKC7s">
					<idno type="grant-number">62271042</idno>
				</org>
				<org type="funding" xml:id="_bXXJTaH">
					<idno type="grant-number">61906013</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_39.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data uncertainty learning in face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention-based autism spectrum disorder screening with privileged modality</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1181" to="1190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Control of goal-directed and stimulus-driven attention in the brain</title>
		<author>
			<persName><forename type="first">M</forename><surname>Corbetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Shulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="215" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A dataset of eye movements for the children with autism spectrum disorder</title>
		<author>
			<persName><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Multimedia Systems Conference</title>
		<meeting>the 10th ACM Multimedia Systems Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="255" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>El Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R G</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Natsoulis</surname></persName>
		</author>
		<title level="m">Robust classification with interval data</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A meta-analysis of gaze differences to social and nonsocial information between individuals with and without autism</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Frazier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Acad. Child Adolesc. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="546" to="555" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Do gaze cues in complex scenes capture and direct the attention of high functioning adolescents with ASD? Evidence from eye-tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Freeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ropar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Autism Dev. Disord</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="534" to="547" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning visual attention to identify people with autism spectrum disorder</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3267" to="3276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2009.5459462</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2009.5459462" />
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Two-year-olds with autism orient to non-social contingencies rather than biological motion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gorrindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">459</biblScope>
			<biblScope unit="issue">7244</biblScope>
			<biblScope unit="page" from="257" to="261" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Applying machine learning to identify autistic adults using imitation: an exploratory study</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gowen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">182652</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identifying children with autism spectrum disorder based on their face processing abnormality: a machine learning framework</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="898" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Prevalence and characteristics of autism spectrum disorder among children aged 8 years-autism and developmental disabilities monitoring network, 11 sites, United States</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Maenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MMWR Surveill. Summ</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018">2018. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Patterns of visual attention to faces and objects in autism spectrum disorder</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mcpartland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dawson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Autism Dev. Disord</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="148" to="157" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual scanning of faces in autism</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Pelphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Sasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Reznick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Piven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Autism Dev. Disord</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="249" to="261" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Eye tracking reveals abnormal visual preference for geometric images as an early biomarker of an autism spectrum disorder subtype associated with increased symptom severity</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marinero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hazin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mckenna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Psychiat</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="657" to="666" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Children with autism demonstrate circumscribed attention during passive viewing of complex social and nonsocial picture arrays</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Sasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Turner-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Holtzclaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Bodfish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism Res</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="42" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autism spectrum disorder screening: machine learning adaptation and DSM-5 fulfillment</title>
		<author>
			<persName><forename type="first">F</forename><surname>Thabtah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Medical and Health Informatics</title>
		<meeting>the 1st International Conference on Medical and Health Informatics</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Atypical visual saliency in autism spectrum disorder quantified through model-based eye tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="604" to="616" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data-uncertainty guided multi-phase learning for semi-supervised object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4568" to="4577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data uncertainty in face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1950" to="1961" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
