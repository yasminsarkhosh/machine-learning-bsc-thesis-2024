<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification</title>
				<funder ref="#_eqNsxzX #_crPAVae #_kjyzCS7 #_pM8zbYA">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yin</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qilong</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Anhui Chest Hospital</orgName>
								<address>
									<postCode>230039</postCode>
									<settlement>Hefei, Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuhong</forename><surname>Min</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Anhui Chest Hospital</orgName>
								<address>
									<postCode>230039</postCode>
									<settlement>Hefei, Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minghui</forename><surname>Wang</surname></persName>
							<email>mhwang@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ao</forename><surname>Li</surname></persName>
							<email>aoli@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="358" to="367"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">13C7BE949057CF4B1658CEA05AE6F6DF</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cross-view Alignment</term>
					<term>Representation Learning</term>
					<term>Multi-view</term>
					<term>Histologic Subtype Classification</term>
					<term>Non-small Cell Lung Cancer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurately classifying the histological subtype of non-small cell lung cancer (NSCLC) using computed tomography (CT) images is critical for clinicians in determining the best treatment options for patients. Although recent advances in multi-view approaches have shown promising results, discrepancies between CT images from different views introduce various representations in the feature space, hindering the effective integration of multiple views and thus impeding classification performance. To solve this problem, we propose a novel method called cross-aligned representation learning (CARL) to learn both view-invariant and view-specific representations for more accurate NSCLC histological subtype classification. Specifically, we introduce a cross-view representation alignment learning network which learns effective view-invariant representations in a common subspace to reduce multi-view discrepancies in a discriminability-enforcing way. Additionally, CARL learns view-specific representations as a complement to provide a holistic and disentangled perspective of the multi-view CT images. Experimental results demonstrate that CARL can effectively reduce the multi-view discrepancies and outperform other state-of-the-art NSCLC histological subtype classification methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lung cancer is currently the foremost cause of cancer-related mortalities globally, with non-small cell lung cancer (NSCLC) being responsible for 85% of reported cases <ref type="bibr" target="#b24">[25]</ref>. Within NSCLC, squamous cell carcinoma (SCC) and adenocarcinoma (ADC) are recognized as the two principal histological subtypes. Since SCC and ADC differ in the effectiveness of chemotherapy and the risk of complications, accurate identification of different subtypes is crucial for clinical treatment options <ref type="bibr" target="#b14">[15]</ref>. Although pathological diagnosis via lung biopsy can provide a reliable result of subtype identification, it is highly invasive with potential clinical implications <ref type="bibr" target="#b18">[19]</ref>. Therefore, non-invasive methods utilizing computed tomography (CT) images have garnered significant attention over the last decade <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Recently, several deep-learning methods have been put forward to differentiate between the NSCLC histological subtypes using CT images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22]</ref>. Chaunzwa et al. <ref type="bibr" target="#b3">[4]</ref> and Marentakis et al. <ref type="bibr" target="#b12">[13]</ref> both employ a convolutional neural network (CNN) model with axial view CT images to classify the tumor histology into SCC and ADC. Albeit the good performance, the above 2D CNN-based models only take CT images from a single view as the input, limiting their ability to describe rich spatial properties of CT volumes <ref type="bibr" target="#b19">[20]</ref>. Multi-view deep learning, a 2.5D method, represents a promising solution to this issue, as it focuses on obtaining a unified joint representation from different views of lung nodules to capture abundant spatial information <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>. For example, Wu et al. <ref type="bibr" target="#b21">[22]</ref> aggregate features from axial, coronal, and sagittal view CT images via a multi-view fusion model. Similarly, Li et al. <ref type="bibr" target="#b10">[11]</ref> also extract patches from three orthogonal views of a lung nodule and present a multi-view ResNet for feature fusion and classification. By integrating multi-view representations, these methods efficiently preserve the spatial information of CT volumes while significantly reducing the required computational resource compared to 3D CNNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Despite the promising results of previous multi-view methods, they still confront a severe challenge for accurate NSCLC histological subtype prediction. In fact, due to the limitation of scan time and hardware capacity in clinical practice, different views of CT volumes are anisotropic in terms of in-plane and inter-plane resolution <ref type="bibr" target="#b20">[21]</ref>. Additionally, images from certain views may inevitably contain some unique background information, e.g., the spine in the sagittal view <ref type="bibr" target="#b16">[17]</ref>. Such anisotropy and background dissimilarity both reveal the existence of significant variations between different views, which lead to markedly various representations in feature space. Consequently, the discrepancies of distinct views will hamper the fusion of multi-view information, limiting further improvements in the classification performance.</p><p>To overcome the challenge mentioned above, we propose a novel cross-aligned representation learning (CARL) method for the multi-view histologic subtype classification of NSCLC. CARL offers a holistic and disentangled perspective of multi-view CT images by generating both view-invariant and -specific representations. Specifically, CARL incorporates a cross-view representation alignment learning network which targets the reduction of multi-view discrepancies by obtaining discriminative view-invariant representations. A shared encoder with a novel discriminability-enforcing similarity constraint is utilized to map all representations learned from multi-view CT images to a common subspace, enabling cross-view representation alignment. Such aligned projections help to capture view-invariant features of cross-view CT images and meanwhile make full use of the discriminative information obtained from each view. Additionally, CARL learns view-specific representations as well which complement the view-invariant ones, providing a comprehensive picture of the CT volume data for histological subtype prediction. We validate our approach by using a publicly available NSCLC dataset from The Cancer Imaging Archive (TCIA). Detailed experimental results demonstrate the effectiveness of CARL in reducing multi-view discrepancies and improving NSCLC histological subtype classification performance. Our contributions can be summarized as follows:</p><p>-A novel cross-aligned representation learning method called CARL is proposed for NSCLC histological subtype classification. To reduce the discrepancies of multiview CT images, CARL incorporates a cross-view representation alignment learning network for discriminative view-invariant representations. -We employ a view-specific representation learning network to learn view-specific representations as a complement to the view-invariant representations. -We conduct experiments on a publicly available dataset and achieve superior performance compared to the most advanced methods currently available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture Overview</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the overall architecture of CARL. The cross-view representation alignment learning network includes a shared encoder which projects patches of axial, coronal, and sagittal views into a common subspace with a discriminability-enforcing similarity constraint to obtain discriminative view-invariant representations for multi-view discrepancy reduction. In addition, CARL introduces a view-specific representation learning network consisting of three unique encoders which focus on learning view-specific representations in respective private subspaces to yield complementary information to view-invariant representations. Finally, we introduce a histological subtype classification module to fuse the view-invariant and -specific representations and make accurate NSCLC histological subtype classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross-View Representation Alignment Learning</head><p>Since the discrepancies of different views may result in divergent statistical properties in feature space, e.g., huge distributional disparities, aligning representations of different views is essential for multi-view fusion. With the aim to reduce multi-view discrepancies, CARL introduces a cross-view representation alignment learning network for mapping the representations from distinct views into a common subspace, where view-invariant representations can be obtained by cross-view alignment. Specifically, inspired by <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref>, we exert a discriminability-enforcing similarity constraint to align all sub-view representations with those of the main view, significantly mitigating the distributional disparities of multi-view representations.</p><p>Technically speaking, given the axial view image I av , coronal view image I cv , and sagittal view image I sv , the cross-view representation alignment learning network tries to generate view-invariant representations h c v , v ∈ {av, cv, sv} via a shared encoder based on a residual neural network <ref type="bibr" target="#b9">[10]</ref>. This can be formulated as below:</p><formula xml:id="formula_0">h c υ = E c (I υ ), υ ∈ {aυ, cυ, sυ}<label>(1)</label></formula><p>where E c (•) indicates the shared encoder, and av, cv, sv represent axial, coronal, and sagittal views, respectively. In the common subspace, we hope that through optimizing the shared encoder E c (•), the view-invariant representations can be matched to some extent. However, the distributions of h c av , h c cv and h c sv are very complex due to the significant variations between different views, which puts a burden on obtaining well-aligned view-invariant representations with merely an encoder.</p><p>To address this issue, we design a discriminability-enforcing similarity loss L dsim to further enhance the alignment of cross-view representations in the common subspace. Importantly, considering that the axial view has a higher resolution than other views and are commonly used in clinical diagnosis, we choose axial view as the main view and force the sub-views (e.g., the coronal and sagittal views) to seek distributional similarity with the main view. Mathematically, we introduce a cross-view similarity loss L sim which calculates the central moment discrepancy (CMD) metric <ref type="bibr" target="#b23">[24]</ref> between all sub-views and the main view as shown below:</p><formula xml:id="formula_1">L sim = 1 N N i=1 CMD h sub i , h main (2)</formula><p>where CMD(•) denotes the distance metric which measures the distribution disparities between the representations of i-th sub-view h sub i and the main view h main . N is the number of sub-views. Despite the fact that minimizing the L sim can efficiently mitigate the issue of distributional disparities, it may not guarantee that the alignment network will learn informative and discriminative representations. Inspired by recent work on multimodal feature extraction <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, we impose a direct supervision by inputting h main into a classifier f (•) to obtain the prediction of histological subtype, and use a crossentropy loss to enforce the discriminability of the main-view representations. Finally, the discriminability-enforcing similarity loss L dsim is as follows:</p><formula xml:id="formula_2">L dsim = L sim + λ • L CE f h main , y<label>(3)</label></formula><p>where y denotes the ground-truth subtype labels, λ controls the weight of L CE . We observed that L CE is hundred times smaller than L sim , so this study uses an empirical value of λ = 110 to balance the magnitude of two terms. By minimizing L dsim , the cross-view representation alignment learning network pushes the representations of each sub-view to align with those of the main view in a discriminability-enforcing manner.</p><p>Notably, the benefits of such cross-alignment are twofold. Firstly, it greatly reduces the discrepancies between the sub-views and the main view, leading to consistent viewinvariant representations. Secondly, since the alignment between distinct views compels the representation distribution of the sub-views to match that of the discriminative main view, it can also enhance the discriminative power of the sub-view representations.</p><p>In other words, the cross-alignment procedure spontaneously promotes the transfer of discriminative information learned by the representations of the main view to those of the sub-views. As a result, the introduced cross-view representation alignment learning network is able to generate consistent and discriminative view-invariant representations cross all views to effectively narrow the multi-view discrepancies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">View-Specific Representation Learning</head><p>On the basis of learning view-invariant representations, CARL additionally learns view-specific representations in respective private subspaces, which provides supplementary information for the view-invariant representations and contribute to subtype classification as well. To be specific, a view-specific representation learning network containing three unique encoders is proposed to learn view-specific representations h p v , v ∈ {av, cv, sv}, enabling effective exploitation of the specific information from each view. We formulate the unique encoders as follows:</p><formula xml:id="formula_3">h p υ = E p υ (I υ ), υ ∈ {aυ, cυ, sυ}<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">E p v (•)</formula><p>is the encoder function dedicated to capture single-view characteristics. To induce the view-invariant and -specific representations to learn unique characteristics of each view, we draw inspiration from <ref type="bibr" target="#b13">[14]</ref> and adopt an orthogonality loss L orth with the squared Frobenius norm between the representations in the common and private subspaces of each view, which is denoted by</p><formula xml:id="formula_5">L orth = h c v h p v 2 F , v ∈ {av, cv, sv}.</formula><p>A reconstruction module is also employed to calculate a reconstruction loss L rec between original image I v and reconstructed image I r v using the L 1 -norm, which ensures the hidden representations to capture details of the respective view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Histologic Subtype Classification</head><p>After obtaining view-invariant and -specific representations from each view, we integrate them together to perform NSCLC subtype classification. Specifically, we apply a residual block <ref type="bibr" target="#b9">[10]</ref> to fuse view-invariant and -specific representations into a unified multi-view representation h. Then, h is sent to a multilayer perceptron neural network (MLP) to make the precise NSCLC subtype prediction. The NSCLC histological subtype classification loss L cls can be calculated by using cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Network Optimization</head><p>The optimization of CARL is achieved through a linear combination of several loss terms, including discriminability-enforcing similarity loss L dsim , orthogonality loss L orth , reconstruction loss L rec and the classification loss L cls . Accordingly, the total loss function can be formulated as a weighted sum of these separate loss terms:</p><formula xml:id="formula_6">L total = L cls + αL dsim + βL orth + γ L rec (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where α, β and γ denote the weights of L dsim , L rec and L orth . To normalize the scale of L dsim which is much larger than the other terms, we introduce a scaling factor S = 0.001, and perform a grid search for α, β and γ in the range of 0.1S-S, 0.1-1, and 0.1-1, respectively.</p><p>Throughout the experiments, we set the values of α, β and γ to 0.6S, 0.4 and 0.6, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>Our dataset NSCLC-TCIA for lung cancer histological subtype classification is sourced from two online resources of The Cancer Imaging Archive (TCIA) <ref type="bibr" target="#b4">[5]</ref>: NSCLC Radiomics <ref type="bibr" target="#b0">[1]</ref> and NSCLC Radiogenomics <ref type="bibr" target="#b1">[2]</ref>. Exclusion criteria involves patients diagnosed with large cell carcinoma or not otherwise specified, along with cases that have contouring inaccuracies or lacked tumor delineation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref>. Finally, a total of 325 available cases (146 ADC cases and 179 SCC cases) are used for our study. We evaluate the performance of NSCLC classification in five-fold cross validation on the NSCLC-TCIA dataset, and measure accuracy (Acc), sensitivity (Sen), specificity (Spe), and the area under the receiver operating characteristic (ROC) curve (AUC) as evaluation metrics. We also conduct analysis including standard deviations and 95% CI, and DeLong statistical test for further AUC comparison.</p><p>For preprocessing, given that the CT data from NSCLC-TCIA has an in-plane resolution of 1 mm × 1 mm and a slice thickness of 0.7-3.0 mm, we resample the CT images using trilinear interpolation to a common resolution of 1mm × 1mm × 1mm. Then one 128 × 128 pixel slice is cropped from each view as input based on the center of the tumor. Finally following <ref type="bibr" target="#b6">[7]</ref>, we clip the intensities of the input patches to the interval (-1000, 400 Hounsfield Unit) and normalize them to the range of [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>The implementation of CARL is carried out using PyTorch and run on a workstation equipped with Nvidia GeForce RTX 2080Ti GPUs and Intel Xeon CPU 4110 @ 2.10GHz. Adam optimizer is used with an initial learning rate of 0.00002, and the batch size is set to 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Comparison with Existing Methods. Several subtype classification methods have been employed for comparison including: two conventional methods, four single-view and 3D deep learning methods, and four representative multi-view methods. We use publicly available codes of these comparison methods and implement models for methods without code. The experimental results are reported in Table <ref type="table" target="#tab_0">1</ref>. The multi-view methods are generally superior to the single-view and 3D deep learning methods. It illustrates that the multi-view methods can exploit richer spatial properties of CT volumes than the single-view methods while greatly reducing the model parameters to avoid overfitting compared to the 3D methods. The floating point operations (FLOPs) comparison between CARL (0.9 GFLOPs) and the 3D method <ref type="bibr" target="#b8">[9]</ref> (48.4 GFLOPs) also proves the computational efficiency of our multi-view method. Among all multi-view methods, our proposed CARL achieves the best results, outperforming Wu et al. by 3.2%, 3.2%, 1.5% and 4.1% in terms of AUC, Acc, Sen and Spe, respectively. Not surprisingly, the ROC curve of CARL in Fig. <ref type="figure" target="#fig_1">2(a</ref>) is also closer to the upper-left corner, further indicating its superior performance. These results demonstrate that CARL can effectively narrow the discrepancies of different views by obtaining view-invariant representations in a discriminative way, thus leading to excellent classification accuracy compared to other methods.   Ablation Analysis. We evaluate the efficacy of different losses in our method. The results are reported in Table <ref type="table" target="#tab_1">2</ref>, where CARL-B0 refers to CARL only using the classification loss, +L * indicates the loss superimposed on CARL-B0 and L all denotes that we utilize all the losses in Eq. 5. We can observe that CARL-B2 performs better than CARL-B0 by employing the discriminability-enforcing similarity loss to align crossview representations. Besides, CARL-B3 and CARL-B4 show better performance than CARL-B0, illustrating view-specific representations as a complement which can also contribute to subtype classification. Though single loss already contributes to performance improvement, CARL-B5 to CARL-B7 demonstrate that the combinations of different losses can further enhance classification results. More importantly, CARL with all losses achieves the best performance among all methods, demonstrating that our proposed method effectively reduces multi-view discrepancies and significantly improves the performance of histological subtype classification by providing a holistic and disentangled perspective of the multi-view CT images. The ROC curve of CARL in Fig. <ref type="figure" target="#fig_1">2</ref>(b) is generally above its variants, which is also consistent with the quantitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In summary, we propose a novel multi-view method called cross-aligned representation learning (CARL) for accurately distinguishing between ADC and SCC using multi-view CT images of NSCLC patients. It is designed with a cross-view representation alignment learning network which effectively generates discriminative view-invariant representations in the common subspace to reduce the discrepancies among multi-view images. In addition, we leverage a view-specific representation learning network to acquire viewspecific representations as a necessary complement. The generated view-invariant and -specific representations together offer a holistic and disentangled perspective of the multi-view CT images for histological subtype classification of NSCLC. The experimental result on NSCLC-TCIA demonstrates that CARL reaches 0.817 AUC, 76.8% Acc, 73.2% Sen, and 79.7% Spe and surpasses other relative approaches, confirming the effectiveness of the proposed CARL method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the proposed CARL.</figDesc><graphic coords="3,45,81,207,17,332,68,228,88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. ROC plots of (a) compared methods and (b) ablation analysis on NSCLC-TCIA dataset.</figDesc><graphic coords="8,77,46,56,48,297,49,110,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on NSCLC-TCIA dataset when CARL was compared with other SOTA methods using five-fold cross validation. * indicates the p-value is less than 0.05 in DeLong test between the AUC of compared method and CARL.</figDesc><table><row><cell>Category</cell><cell>Methods</cell><cell>AUC</cell><cell>95% CI</cell><cell>Acc</cell><cell>Sen</cell><cell>Spe</cell></row><row><cell>Conventional</cell><cell>RF [3]</cell><cell cols="2">0.742 ± 0.061* 0.684-0.791</cell><cell cols="3">0.667 0.623 0.703</cell></row><row><cell></cell><cell>SVM [6]</cell><cell cols="2">0.756 ± 0.069* 0.714-0.817</cell><cell cols="3">0.699 0.670 0.725</cell></row><row><cell cols="2">Deep learning Chaunzwa</cell><cell cols="2">0.774 ± 0.051* 0.710-0.816</cell><cell cols="3">0.713 0.692 0.729</cell></row><row><cell></cell><cell>et al. [4]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Marentakis</cell><cell cols="2">0.770 ± 0.076* 0.707-0.813</cell><cell cols="3">0.715 0.663 0.757</cell></row><row><cell></cell><cell>et al. [13]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Yanagawa</cell><cell cols="2">0.777 ± 0.062* 0.718-0.822</cell><cell cols="3">0.719 0.650 0.776</cell></row><row><cell></cell><cell>et al. [23]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Guo et al. [9] 0.772 ± 0.072* 0.676-0.783</cell><cell cols="3">0.676 0.635 0.712</cell></row><row><cell>Multi-view</cell><cell>MVCNN</cell><cell cols="2">0.784 ± 0.052* 0.707-0.811</cell><cell cols="3">0.691 0.637 0.733</cell></row><row><cell></cell><cell>[18]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">GVCNN [8] 0.767 ± 0.054* 0.704-0.809</cell><cell cols="3">0.685 0.581 0.770</cell></row><row><cell></cell><cell>Wu et al.</cell><cell>0.785 ± 0.080</cell><cell>0.744-0.844</cell><cell cols="3">0.736 0.717 0.756</cell></row><row><cell></cell><cell>[22]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Li et al. [11] 0.782 ± 0.069* 0.719-0.822</cell><cell cols="3">0.729 0.705 0.748</cell></row><row><cell></cell><cell>CARL</cell><cell>0.817 ± 0.055</cell><cell cols="4">0.770-0.862 0.768 0.732 0.797</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of ablation analysis on the NSCLC-TCIA dataset.</figDesc><table><row><cell>Methods</cell><cell>AUC</cell><cell>Acc</cell><cell>Sen</cell><cell>Spe</cell></row><row><cell>CARL-B0 (with only L cla )</cell><cell>0.783</cell><cell>0.713</cell><cell>0.670</cell><cell>0.748</cell></row><row><cell>CARL-B1 (+L sim )</cell><cell>0.798</cell><cell>0.734</cell><cell>0.726</cell><cell>0.741</cell></row><row><cell>CARL-B2 (+L dsim )</cell><cell>0.811</cell><cell>0.738</cell><cell>0.705</cell><cell>0.764</cell></row><row><cell>CARL-B3 (+L orth )</cell><cell>0.793</cell><cell>0.722</cell><cell>0.705</cell><cell>0.736</cell></row><row><cell>CARL-B4 (+L rec )</cell><cell>0.790</cell><cell>0.701</cell><cell>0.697</cell><cell>0.705</cell></row><row><cell>CARL-B5 (+L dsim +L orth )</cell><cell>0.813</cell><cell>0.756</cell><cell>0.725</cell><cell>0.782</cell></row><row><cell>CARL-B6 (+L dsim +L rec )</cell><cell>0.814</cell><cell>0.747</cell><cell>0.732</cell><cell>0.758</cell></row><row><cell>CARL-B7 (+L orth +L rec )</cell><cell>0.810</cell><cell>0.752</cell><cell>0.712</cell><cell>0.786</cell></row><row><cell>CARL (L all )</cell><cell>0.817</cell><cell>0.768</cell><cell>0.732</cell><cell>0.797</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grants <rs type="grantNumber">61971393</rs>, <rs type="grantNumber">62272325</rs>, <rs type="grantNumber">61871361</rs> and <rs type="grantNumber">61571414</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eqNsxzX">
					<idno type="grant-number">61971393</idno>
				</org>
				<org type="funding" xml:id="_crPAVae">
					<idno type="grant-number">62272325</idno>
				</org>
				<org type="funding" xml:id="_kjyzCS7">
					<idno type="grant-number">61871361</idno>
				</org>
				<org type="funding" xml:id="_pM8zbYA">
					<idno type="grant-number">61571414</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Decoding tumour phenotype by noninvasive imaging using a quantitative Radiomics approach</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J W L</forename><surname>Aerts</surname></persName>
		</author>
		<idno type="DOI">10.1038/ncomms5006</idno>
		<ptr target="https://doi.org/10.1038/ncomms5006" />
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4006</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A radiogenomic dataset of non-small cell lung cancer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakr</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2018.202</idno>
		<ptr target="https://doi.org/10.1038/sdata.2018.202" />
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">180202</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1010933404324</idno>
		<ptr target="https://doi.org/10.1023/A:1010933404324" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning classification of lung cancer histology using CT images</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Chaunzwa</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-021-84630-x</idno>
		<ptr target="https://doi.org/10.1038/s41598-021-84630-x" />
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5471</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cancer imaging archive (TCIA): Maintaining and operating a public information repository</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10278-013-9622-7</idno>
		<ptr target="https://doi.org/10.1007/s10278-013-9622-7" />
	</analytic>
	<monogr>
		<title level="j">J. Digit Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1045" to="1057" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00994018</idno>
		<ptr target="https://doi.org/10.1007/BF00994018" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilevel contextual 3-D CNNs for false positive reduction in pulmonary nodule detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBME.2016.2613502</idno>
		<ptr target="https://doi.org/10.1109/TBME.2016.2613502" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1558" to="1567" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GVCNN: Group-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00035</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2018.00035" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histological subtypes classification of lung cancers on CT images using 3D deep learning and radiomics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.acra.2020.06.010</idno>
		<ptr target="https://doi.org/10.1016/j.acra.2020.06.010" />
	</analytic>
	<monogr>
		<title level="j">Acad. Radiol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="258" to="e266" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1512.03385</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1512.03385" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-view mammographic density classification by dilated and attention-guided residual learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCBB.2020.2970713</idno>
		<ptr target="https://doi.org/10.1109/TCBB.2020.2970713" />
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinf</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1003" to="1013" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive multimodal fusion with attention guided deep supervision net for grading hepatocellular carcinoma</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/JBHI.2022.3161466</idno>
		<ptr target="https://doi.org/10.1109/JBHI.2022.3161466" />
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4123" to="4131" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lung cancer histology classification from CT images based on radiomics and deep learning models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Marentakis</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11517-020-02302-w</idno>
		<ptr target="https://doi.org/10.1007/s11517-020-02302-w" />
	</analytic>
	<monogr>
		<title level="j">Med. Biol. Eng. Comput</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="226" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MSMFN: an ultrasound based multi-step modality fusion network for identifying the histologic subtypes of metastatic cervical lymphadenopathy</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2022.3222541</idno>
		<ptr target="https://doi.org/10.1109/TMI.2022.3222541" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comprehensive perspective for lung cancer characterisation based on AI solutions using CT images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pereira</surname></persName>
		</author>
		<idno type="DOI">10.3390/jcm10010118</idno>
		<ptr target="https://doi.org/10.3390/jcm10010118" />
	</analytic>
	<monogr>
		<title level="j">J. Clin. Med</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A lightweight multi-section CNN for lung nodule classification and malignancy estimation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sahu</surname></persName>
		</author>
		<idno type="DOI">10.1109/JBHI.2018.2879834</idno>
		<ptr target="https://doi.org/10.1109/JBHI.2018.2879834" />
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="960" to="968" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-invasive postural assessment of the spine in the sagittal plane: a systematic review</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Sedrez</surname></persName>
		</author>
		<idno type="DOI">10.6063/motricidade.6470</idno>
		<ptr target="https://doi.org/10.6063/motricidade.6470" />
	</analytic>
	<monogr>
		<title level="j">Motricidade</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="140" to="154" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.114</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.114" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identification of expression signatures for non-small-cell lung carcinoma subtype classification</title>
		<author>
			<persName><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz557</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btz557" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="339" to="346" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lung nodule diagnosis and cancer histology classification from computed tomography data by convolutional neural networks: a survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tomassini</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiomed.2022.105691</idno>
		<ptr target="https://doi.org/10.1016/j.compbiomed.2022.105691" />
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="page">105691</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">UASSR: unsupervised arbitrary scale super-resolution reconstruction of single anisotropic 3D images via disentangled representation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_43" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="453" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning-based multi-view fusion model for screening 2019 novel coronavirus pneumonia: a multicentre study</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ejrad.2020.109041</idno>
		<ptr target="https://doi.org/10.1016/j.ejrad.2020.109041" />
	</analytic>
	<monogr>
		<title level="j">Eur. J. Radiol</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page">109041</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diagnostic performance for pulmonary adenocarcinoma on CT: comparison of radiologists with and without three-dimensional convolutional neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yanagawa</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00330-020-07339-x</idno>
		<ptr target="https://doi.org/10.1007/s00330-020-07339-x" />
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1978">1978-1986 (2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Zellinger</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1702.08811" />
		<title level="m">Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Circular RNA circSATB2 promotes progression of non-small cell lung cancer cells</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12943-020-01221-6</idno>
		<ptr target="https://doi.org/10.1186/s12943-020-01221-6" />
	</analytic>
	<monogr>
		<title level="j">Mol. Cancer</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">101</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
