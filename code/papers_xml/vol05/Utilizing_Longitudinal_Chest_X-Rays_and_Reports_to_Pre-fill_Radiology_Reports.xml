<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports</title>
				<funder ref="#_EUbZygE">
					<orgName type="full">NSF CAREER</orgName>
				</funder>
				<funder ref="#_mjBbnUd">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder>
					<orgName type="full">Amazon Research Award (Peng)</orgName>
				</funder>
				<funder>
					<orgName type="full">Intramural Research Program of the National Library of Medicine and Clinical Center</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qingqing</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Center for Biotechnology Information</orgName>
								<orgName type="institution">National Library of Medicine</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Institutes of Health</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tejas</forename><forename type="middle">Sudharshan</forename><surname>Mathai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pritam</forename><surname>Mukherjee</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology and Imaging Sciences</orgName>
								<orgName type="laboratory">Imaging Biomarkers and Computer-Aided Diagnosis Laboratory</orgName>
								<orgName type="institution">National Institutes of Health Clinical Center</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Peng</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Population Health Sciences</orgName>
								<orgName type="department" key="dep2">Weill Cornell Medicine</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ronald</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology and Imaging Sciences</orgName>
								<orgName type="laboratory">Imaging Biomarkers and Computer-Aided Diagnosis Laboratory</orgName>
								<orgName type="institution">National Institutes of Health Clinical Center</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
							<email>zhiyong.lu@nih.gov</email>
							<affiliation key="aff0">
								<orgName type="department">National Center for Biotechnology Information</orgName>
								<orgName type="institution">National Library of Medicine</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Institutes of Health</orgName>
								<address>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="189" to="198"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B03808DEAFACB379E6A445EDCE80E3EE</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_19</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Chest X-Rays</term>
					<term>Radiology reports</term>
					<term>Longitudinal data</term>
					<term>Report Pre-Filling</term>
					<term>Report Generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the reduction in turn-around times in radiology reporting with the use of speech recognition software, persistent communication errors can significantly impact the interpretation of radiology reports. Pre-filling a radiology report holds promise in mitigating reporting errors, and despite multiple efforts in literature to generate comprehensive medical reports, there lacks approaches that exploit the longitudinal nature of patient visit records in the MIMIC-CXR dataset.</p><p>To address this gap, we propose to use longitudinal multi-modal data, i.e., previous patient visit CXR, current visit CXR, and the previous visit report, to pre-fill the "findings" section of the patient's current visit. We first gathered the longitudinal visit information for 26,625 patients from the MIMIC-CXR dataset, and created a new dataset called Longitudinal-MIMIC. With this new dataset, a transformer-based model was trained to capture the multi-modal longitudinal information from patient visit records (CXR images + reports) via a cross-attention-based multi-modal fusion module and a hierarchical memory-driven decoder. In contrast to previous works that only uses current visit data as input to train a model, our work exploits the longitudinal information available to pre-fill the "findings" section of radiology reports. Experiments show that our approach outperforms several recent approaches by ≥3% on F1 score, and ≥2% for BLEU-4, METEOR and ROUGE-L respectively. Code will be published at https://github.com/CelestialShine/Longitudinal-Chest-X-Ray.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In current radiology practice, a signed report is often the primary form of communication, to communicate results of a radiological imaging exam between radiologist. Speech recognition software (SRS), which converts dictated words or sentences into text in a report, is widely used by radiologists. Despite SRS reducing the turn-around times for radiology reports, correcting any transcription errors in the report has been assumed by the radiologists themselves. But, persistent report communication errors due to SRS can significantly impact report interpretation, and also have dire consequences for radiologists in terms of medical malpractice <ref type="bibr" target="#b0">[1]</ref>. These errors are most common for cross-sectional imaging exams (e.g., CT, MR) and chest radiography <ref type="bibr" target="#b1">[2]</ref>. Problems also arise when re-examining the results from external examinations and in interventional radiology procedural reports. Such errors are due to many factors, including SRS finding a nearest match for a dictated word, the lack of natural language processing (NLP) for real-time recognition and dictation conversion <ref type="bibr" target="#b1">[2]</ref>, and unnoticed typographical mistakes. To mitigate these errors, a promising alternative is to automate the pre-filling of a radiology report with salient information for a radiologist to review. This enables standardized reporting via structured reporting. A number of methods to generate radiology reports have been proposed previously, with significant focus on CXR images <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. Various attention mechanisms were proposed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref> to drive the encoder and the decoder to emphasize more informative words in the report, or visual regions in the CXR, and improve generation accuracy. Other approaches <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> effectively used Transformer-based models with memory matricies to store salient information for enhanced report generation quality. Despite these advances, there has been scarce research into harnessing the potential of longitudinal patient visits for improved patient care.</p><p>In practice, CXR images from multiple patient visits are usually examined simultaneously to find interval changes; e.g., a radiologist may compare a patient's current CXR to a previous CXR, and identify deterioration or improvement in the lungs for pneumonia. Reports from longitudinal visits contain valuable information regarding the patient's history, and harnessing the longitudinal multimodal data is vital for the automated pre-filling of a comprehensive "findings" section in the report.</p><p>In this work, we propose to use longitudinal multi-modal data, i.e., previous visit CXR, current visit CXR, and previous visit report, to pre-fill the "findings" section of the patient's current visit report. To do so, we first gathered the longitudinal visit information for 26,625 patients from the MIMIC-CXR dataset<ref type="foot" target="#foot_0">1</ref> and created a new dataset called Longitudinal-MIMIC. Using this new dataset, we trained a transformer-based model containing a cross-attention-based multimodal fusion module and a hierarchical memory-driven decoder to capture the features of longitudinal multi-modal data (CXR images + reports). In contrast to current approaches that only use the current visit data as input, our model exploits the longitudinal information available to pre-fill the "findings" section of reports with accurate content. Experiments conducted with the proposed dataset and model validate the utility of our proposed approach. Our main contribution in this work is training a transformer-based model that fully tackles the longitudinal multi-modal patient visit data to pre-fill the "findings" section of reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Dataset. The construction of the Longitudinal-MIMIC dataset involved several steps, starting with the MIMIC-CXR dataset, which is a large publicly available dataset of 377,110 chest X-ray images corresponding to 227,835 radiographic reports from 65,379 patients <ref type="bibr" target="#b12">[13]</ref>. The first step in creating the Longitudinal-MIMIC dataset was to pre-process MIMIC-CXR to ensure consistency with prior works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Specifically, patient visits where the report did not contain a "findings" section were excluded. For each patient visit, there was at least one chest X-ray image (frontal, lateral or other view) and a corresponding medical report. In our work, we only generated pre-filled reports with the "findings" section. Next, the pre-processed dataset was partitioned into training, validation, and test sets using the official split provided with the MIMIC-CXR dataset. Table <ref type="table" target="#tab_0">1</ref> shows that 26,625 patients in MIMIC-CXR had ≥ 2 visit records, providing a large cohort of patients with longitudinal study data that could be used for our goal of pre-filling radiology reports. For patients with ≥2 visits, consecutive pairs of visits were used to capture richer longitudinal information. The dataset was then arranged chronologically based on the "StudyTime" attribute present in the MIMIC-CXR dataset. "StudyTime" represents the exact time at which a particular chest X-ray image and its corresponding medical report were acquired.</p><p>Following this, patients with ≥2 visit records were selected, resulting in 26,625 patients in the final Longitudinal-MIMIC dataset with a total of 94,169 samples. Each sample used during model training consisted of the current visit CXR, current visit report, previous visit CXR, and the previous visit report. The final dataset was divided into training (26,156 patients and 92,374 samples), validation (203 patients and 737 samples), and test (266 patients and 2,058 samples) splits. We aimed to create the Longitudinal-MIMIC dataset to enable the development and evaluation of models leveraging multi-modal data (CXR + reports) from longitudinal patient visits.</p><p>Model Architecture. Figure <ref type="figure" target="#fig_0">1</ref> shows the pipeline to generate a pre-filled "findings" section in the current visit report R C , given the current visit CXR image I C , previous visit CXR image I P , and the previous visit report R P . Mathematically, we can write:</p><formula xml:id="formula_0">p(R C | I C , I P , R P ) = t=1 p (w t | w 1 , . . . , w t-1 , I C , I P , R P )</formula><p>, where w i is the i-th word in the current report. The Text Encoder encoded text information for language feature embedding using a previously published method <ref type="bibr" target="#b14">[15]</ref>. First, the radiology report R P was tokenized into a sequence of M tokens, and then transformed into vector representations V = [v 1 , . . . , v M ] using a lookup table <ref type="bibr" target="#b15">[16]</ref>. They were then fed to the text encoder, which had the same architecture as the image encoder, but with distinct network parameters. The final text feature embedding H RP was defined as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder. Our model uses an Image Encoder</head><formula xml:id="formula_1">H RP = θ E R (V ),</formula><p>where θ E R refers to the parameters of the report text encoder. Cross-Attention Fusion Module. A multi-modal fusion module integrated longitudinal representations of images and texts using a cross-attention mechanism <ref type="bibr" target="#b16">[17]</ref>, which was defined as: Sub-block-1 uses H IC and consists of a self-attention layer, an encoderdecoder attention layer, and feed-forward layers. It also employs residual connections and conditional layer normalization <ref type="bibr" target="#b7">[8]</ref>. The encoder-decoder attention layer performs multi-head attention over H IC . It also uses a memory matrix M to store output and important pattern information. The memory representations not only store the information of generated current reports over time in the decoder, but also the information across different encoders. Following <ref type="bibr" target="#b7">[8]</ref>, we adopted a matrix M to store the output over multiple generation steps and record important pattern information. Then we enhance M by aligning it with H IC to create an attention-aligned memory M IC matrix. Different from <ref type="bibr" target="#b7">[8]</ref>, we use M IC while transforming the normalized data instead of M . The decoding process of sub-block-1 D I is formalized as: H dec,b,I = D I (H O , H IC , M IC ), where b stands for the block index. The output of sub-block 1 is combined with H O through a fusion layer: H dec,b = (1β)H O + βH dec,b,I . β is a hyper-parameter to balance H O and H dec,b,I . In our experiment, we set it to 0.2.</p><formula xml:id="formula_2">H I * P = softmax q(H I P )k(H R P ) √ d k v H RP and H R * P = sof tmax q(H R P )k(H I P ) √ d k v H IP ,</formula><p>The input to sub-block-2 D L is H dec,b . This structure is similar to sub-block-1, but interacts with H L instead of H IC . The output of this block is H dec,b,L and combined with H dec,b,I by adding them together. After fusing these embeddings and doing traditional layer normalization for them, we use these embeddings as the output of a block. The output of the previous block is used as the input of the next block. After N blocks, the final hidden states are obtained and used with a Linear and Softmax layer to get the target report probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Baseline Comparisons. We compared our proposed method against prior image captioning and medical report generation works respectively. The same Longitudinal-MIMIC dataset was used to train all baseline models, such as AoANet <ref type="bibr" target="#b17">[18]</ref>, CNNTrans <ref type="bibr" target="#b15">[16]</ref>, Transformer <ref type="bibr" target="#b14">[15]</ref>, R2gen <ref type="bibr" target="#b7">[8]</ref>, and R2CMN <ref type="bibr" target="#b8">[9]</ref>. Implementation of these methods is detailed in the supplementary material.</p><p>Evaluation Metrics. Conventional natural language generation (NLG) metrics, such as BLEU <ref type="bibr" target="#b18">[19]</ref>, MET EOR <ref type="bibr" target="#b19">[20]</ref>, and Rouge L <ref type="bibr" target="#b20">[21]</ref> were used to evaluate the utility of our approach against other baseline methods. Similar to prior work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>, the CheXpert labeler <ref type="bibr" target="#b21">[22]</ref> classified the predicted report for the presence of 14 disease conditions<ref type="foot" target="#foot_1">2</ref> and compared them against the labels of the groundtruth report. Clinical Efficacy (CE) metrics, such as; accuracy, precision, recall, and F-1 score, were used to evaluate model performance. <ref type="table" target="#tab_2">2</ref> shows the summary of the NLG metrics and CE metrics for the 14 disease observations for our proposed approach when compared against prior baseline approaches. In particular, our model achieves the best performance over previous baselines across all NLG and CE metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results. Table</head><p>Generic image captioning approaches like AoANet resulted in unsatisfactory performance on the Longitudinal-MIMIC dataset as they failed to capture specific disease observations. Moreover, our approach outperforms previous report generation methods, R2Gen and R2CMN that also use memory-based models, due to the added longitudinal context arising from the use of longitudinal multimodal study data (CXR images + reports). In our results, the BLEU scores show a substantial improvement, particularly in BLEU-4, where we achieve a 1.4% increase compared to the previous method R2CMN. BLEU scores measure how many continuous sequences of words appear in predicted reports, while Rouge L evaluates the fluency and sufficiency of predicted reports. The highest Rouge L score demonstrates the ability of our approach to generate accurate reports, rather than meaningless word combinations. We also use METEOR for evaluation, taking into account the precision, recall, and alignment of words and phrases in generated reports and the ground truth. Our METEOR score shows a 1.1% improvement over the previous outstanding method, which further solidifies the effectiveness of our approach. Meanwhile, our model exhibits a significant Fig. <ref type="figure">2</ref>. Two examples of pre-filled "findings" sections of reports. Gray highlighted text indicates the same words or words with similar meaning that appear in the current reports and other reports. Purple highlighted text represents similar words in the current visit report generated by our approach, previous visit reports, and groundtruth current visit report. The red highlighted text indicates similar words that only exist in the report generated by our approach and the current ground truth report. R2Gen was the baseline method that generated the report. The "Labels" array shows the CheXpert classification of 14 disease observations (see text for details) as positive (1), negative (-1), uncertain (0) or unmentioned (×). (Color figure online) improvement in clinical efficacy metrics compared to other baselines. Notably, F1 is the most important metric, as it provides a balanced measure of both precision and recall. Our approach outperforms the best-performing method by 3.1% in terms of F1 score. These observations are particularly significant, as higher NLG scores do not necessarily correspond to higher clinical scores <ref type="bibr" target="#b7">[8]</ref>, confirming the effectiveness of our proposed method.</p><p>Effect of Model Components. We also studied the contribution of different model components and detail results in Table <ref type="table" target="#tab_2">2</ref>. The Baseline experiment refers to a basic Transformer model trained to generate a pre-filled report given a chest CXR image without any additional longitudinal information. The NLG and CE metrics are poor for the vanilla transformer compared to our proposed approach. We also analyze the contributions of the previous chest CXR image + image and previous visit report + report when added to the model separately. These two experiments included memory-enhanced conditional normalization. We observed that with each added feature enhanced the pre-filled report quality compared to the baseline, but the previous visit report had a higher impact than the previous CXR image. We hypothesize that the previous visit reports contain more text that can be directly transferred to the current visit reports.</p><p>In our simple fusion experiment, we removed the cross-attention module and concatenated the encoded embeddings of the previous CXR image and previous visit report as one longitudinal embedding, while retaining the rest of the model. We saw a performance drop compared to our approach on our dataset, and also noticed that the results were worse than using the images or reports alone. These experiments demonstrate the utility of the cross-attention module in our proposed work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>Case Study. We also ran a qualitative evaluation of our proposed approach on two cases as seen in Fig. <ref type="figure">2</ref>. In these cases, we compare our generated report with the report generated by the R2Gen. In the first case, certain highlighted words in purple, such as "status post", "aortic valve" and "cardiac silhouette in the predicted current visit report are also seen in the previous visit report. The CheXpert classified "Labels" also show the pre-filled "findings" generated is highly consistent with the ground truth report in contrast to the R2Gen model. For example, the "cardiac silhouette enlarged" was not generated by the R2Gen model, but our prediction contains them and is consistent with the word "cardiomegaly" in the ground truth report. In the second case, our generated report is also superior. Not only does our report generate more of the same content as the ground truth, but the positive diagnosis labels classified by CheXpert in our report are completely consistent with those in the ground truth. We also provide more cases in the supplementary material.</p><p>Error Analysis. To analyze errors from our model, we examine generated reports alongside ground truths and longitudinal information. It is found that the label accuracy of the observations in the generated reports is greatly affected by the previous information. For example, as time changes, for the same observation "pneumothorax", the label can change from "positive" to "negative". And such changing examples are more difficult to generate accurately. According to our statistics, on the one hand, when the label results of current and previous report are the same, 88.96% percent of the generated results match them. On the other hand, despite mentioning the same observations, when the labels of current and previous report are different, there is an 84.42% probability of generated results being incorrect. Thus how to track and generate the label accurately of these examples is a possible future work to improve the generated radiology reports. One possible way to address this issue is to use active learning <ref type="bibr" target="#b22">[23]</ref> or curriculum learning <ref type="bibr" target="#b23">[24]</ref> methods to differentiate different types of samples and better train the machine learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>In this paper, we propose to pre-fill the "findings" section of chest X-Ray radiology reports by considering the longitudinal multi-modal (CXR images + reports) information available in the MIMIC-CXR dataset. We gathered 26,625 patients with multiple visits to constitute the new Longitudinal-MIMIC dataset, and proposed a model to fuse encoded embeddings of multi-modal data along with a hierarchical memory-driven decoder. The model generated a pre-filled "findings" section of the report, and we evaluated the generated results against prior image captioning and medical report generation works. Our model yielded a ≥ 3% improvement in terms of the clinical efficacy F-1 score on the Longitudinal-MIMIC dataset. Moreover, experiments that evaluated the utility of different components of our model proved its effectiveness for the task of pre-filling the "findings" section of the report.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our proposed approach uses the CXR image and report from a previous patient visit and the current visit CXR image to pre-fill the "findings" section of the current visit report. The transformer-based model uses a cross-attention-based multi-modal fusion module and a hierarchical memory-driven decoder to generate the required text.</figDesc><graphic coords="2,44,79,303,59,334,54,103,87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and a Text Encoder to process the CXR images and text input separately. Both encoders were based on transformers. First, a pre-trained ResNet-101<ref type="bibr" target="#b13">[14]</ref> extracted image features F = [f 1 , . . . , f S ] from the CXR images, where S is the number of patch features. They were then passed to the Image Encoder, which consisted of a stack of blocks. The encoded output was a list of encoded hidden states H = [h 1 , . . . , h S ]. The CXR images from the previous and the current visits were encoded in the same manner, and denoted by H IP and H IC respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,63,48,54,08,325,99,187,63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>A breakdown of the MIMIC-CXR dataset to show the number of patients with a specific number of visit records.</figDesc><table><row><cell cols="2"># visit records 1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>&gt;5</cell></row><row><cell># patients</cell><cell cols="6">33,922 10,490 5,079 3,021 1,968 6,067</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>I * P and H R * P were concatenated to obtain the multi-modal longitudinal representations H L . Hierarchical Decoder with Memory. Our model's backbone decoder is a Transformer decoder with multiple blocks (The architecture of an example block is shown in the supplementary material). The first block takes partial output embedding H O as input during training and a pre-determined starting symbol during testing. Subsequent blocks use the output from the previous block as input. To incorporate the encoded H L and H IC , we use a hierarchical structure for each block that divides it into two sub-blocks: D I and D L .</figDesc><table /><note><p>where q(•), k(•), and v(•) are linear transformation layers applied to features of proposals. d k is the number of attention heads for normalization. Finally, H</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results of the NLG metrics (BLEU (BL), Meteor (M), Rouge RL) and clinical efficacy (CE) metrics (Accuracy, Precision, Recall and F-1 score) on the Longitudinal-MIMIC dataset. Best results are highlighted in bold.</figDesc><table><row><cell>Method</cell><cell>NLG metrics</cell><cell></cell><cell cols="2">CE metrics</cell><cell></cell></row><row><cell></cell><cell>BL-1 BL-2 BL-3 BL-4 M</cell><cell>RL</cell><cell>A</cell><cell>P</cell><cell>R</cell><cell>F-1</cell></row><row><cell>AoANet</cell><cell cols="6">0.272 0.168 0.112 0.080 0.115 0.249 0.798 0.437 0.249 0.317</cell></row><row><cell cols="7">CNN+Trans 0.299 0.186 0.124 0.088 0.120 0.263 0.799 0.445 0.258 0.326</cell></row><row><cell cols="7">Transformer 0.294 0.178 0.119 0.085 0.123 0.256 0.811 0.500 0.320 0.390</cell></row><row><cell>R2gen</cell><cell cols="6">0.302 0.183 0.122 0.087 0.124 0.259 0.812 0.500 0.305 0.379</cell></row><row><cell>R2CMN</cell><cell cols="6">0.305 0.184 0.122 0.085 0.126 0.265 0.817 0.521 0.396 0.449</cell></row><row><cell>Ours</cell><cell cols="6">0.343 0.210 0.140 0.099 0.137 0.271 0.823 0.538 0.434 0.480</cell></row><row><cell>Baseline</cell><cell cols="6">0.294 0.178 0.119 0.085 0.123 0.256 0.811 0.500 0.320 0.390</cell></row><row><cell>+ report</cell><cell cols="6">0.333 0.201 0.133 0.094 0.135 0.268 0.823 0.539 0.411 0.466</cell></row><row><cell>+ image</cell><cell cols="5">0.320 0.195 0.130 0.092 0.130 0.268 0.817 0.522 0.34</cell><cell>0.412</cell></row><row><cell cols="7">simple fusion 0.317 0.193 0.128 0.090 0.130 0.266 0.818 0.521 0.396 0.450</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://physionet.org/content/mimic-cxr-jpg/2.0.0/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>No Finding, Enlarged Cardiomediastinum, Cardiomegaly, Lung Lesion, Airspace Opacity, Edema, Consolidation, Pneumonia, Atelectasis, Pneumothorax, Pleural Effusion, Pleural Other, Fracture and Support Devices.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This research was supported by the <rs type="funder">Intramural Research Program of the National Library of Medicine and Clinical Center</rs> at the <rs type="funder">NIH</rs>. The authors thank to <rs type="person">Qingyu Chen</rs> and <rs type="person">Xiuying Chen</rs> for their time and effort in providing thoughtful comments and suggestions to revise this paper. This work was also supported by the <rs type="funder">National Institutes of Health</rs> under Award No. <rs type="grantNumber">4R00LM013001</rs> (Peng), <rs type="funder">NSF CAREER</rs> Award No. <rs type="grantNumber">2145640</rs> (Peng), and <rs type="funder">Amazon Research Award (Peng)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mjBbnUd">
					<idno type="grant-number">4R00LM013001</idno>
				</org>
				<org type="funding" xml:id="_EUbZygE">
					<idno type="grant-number">2145640</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 19.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Signing a colleague&apos;s radiology report</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Berlin</surname></persName>
		</author>
		<idno type="PMID">11133532</idno>
	</analytic>
	<monogr>
		<title level="j">Am. J. Roentgenol</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="30" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Syntactic and semantic errors in radiology reports associated with speech recognition software</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ringler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Goss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Bartholmai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Health Inform. J</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to read chest x-rays: Recurrent neural cascade model for automated image annotation</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2497" to="2506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the automatic generation of medical imaging reports</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2577" to="2586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hybrid retrieval-generation reinforced agent for medical image report generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tienet: text-image embedding network for common thorax disease classification and reporting in chest x-rays</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9049" to="9058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12274</idno>
		<title level="m">Show, describe and conclude: on exploiting the structure information of chest x-ray reports</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating radiology reports via memory-driven transformer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1439" to="1449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">STEM education in Singapore</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-16-1357-9_3</idno>
		<ptr target="https://doi.org/10.1007/978-981-16-1357-93" />
	</analytic>
	<monogr>
		<title level="m">Singapore Math and Science Education Innovation</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="43" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-modal prototype driven network for radiology report generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19833-5_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19833-533" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision. ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13695</biblScope>
			<biblScope unit="page" from="563" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring and distilling posterior and prior knowledge for radiology report generation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13753" to="13762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal recurrent model with attention for automated radiology report generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07042</idno>
		<title level="m">MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multi-modal understanding and generation for medical images and text via vision-language pre-training</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11333</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention bottlenecks for multimodal fusion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="14200" to="14213" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4634" to="4643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2002</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Meteor universal: language specific translation evasukhbaatar2015endluation for any target language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chexpert: a large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
		<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Active Learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-01560-1</idno>
		<idno>978-3-031-01560-1</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
