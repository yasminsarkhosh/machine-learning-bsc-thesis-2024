<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification</title>
				<funder ref="#_xuFzpwX">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Linhao</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Digital Medical Research Center</orgName>
								<orgName type="department" key="dep2">School of Basic Medical Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200032</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Key Lab of Medical Image Computing and Computer Assisted Intervention</orgName>
								<address>
									<postCode>200032</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingfan</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Digital Medical Research Center</orgName>
								<orgName type="department" key="dep2">School of Basic Medical Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200032</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Key Lab of Medical Image Computing and Computer Assisted Intervention</orgName>
								<address>
									<postCode>200032</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiwei</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Key Lab of Medical Image Computing and Computer Assisted Intervention</orgName>
								<address>
									<postCode>200032</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200433</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manning</forename><surname>Wang</surname></persName>
							<email>mnwang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Digital Medical Research Center</orgName>
								<orgName type="department" key="dep2">School of Basic Medical Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200032</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Key Lab of Medical Image Computing and Computer Assisted Intervention</orgName>
								<address>
									<postCode>200032</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhijian</forename><surname>Song</surname></persName>
							<email>zjsong@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Digital Medical Research Center</orgName>
								<orgName type="department" key="dep2">School of Basic Medical Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>200032</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Key Lab of Medical Image Computing and Computer Assisted Intervention</orgName>
								<address>
									<postCode>200032</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="3" to="13"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">2B55CD0B20C84F27A3E5331B8BDCD668</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Active learning</term>
					<term>Openset</term>
					<term>Pathology image classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Active learning (AL) is an effective approach to select the most informative samples to label so as to reduce the annotation cost. Existing AL methods typically work under the closed-set assumption, i.e., all classes existing in the unlabeled sample pool need to be classified by the target model. However, in some practical clinical tasks, the unlabeled pool may contain not only the target classes that need to be fine-grainedly classified, but also non-target classes that are irrelevant to the clinical tasks. Existing AL methods cannot work well in this scenario because they tend to select a large number of non-target samples. In this paper, we formulate this scenario as an open-set AL problem and propose an efficient framework, OpenAL, to address the challenge of querying samples from an unlabeled pool with both target class and non-target class samples. Experiments on fine-grained classification of pathology images show that OpenAL can significantly improve the query quality of target class samples and achieve higher performance than current state-of-the-art AL methods. Code is available at https:// github.com/miccaiif/OpenAL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning techniques have achieved unprecedented success in the field of medical image classification, but this is largely due to large amount of annotated data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b20">20]</ref>. However, obtaining large amounts of high-quality annotated data is usually expensive and time-consuming, especially in the field of pathology image processing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b18">18]</ref>. Therefore, a very important issue is how to obtain the highest model performance with a limited annotation budget. The unlabeled sample pool contains K target categories (red-boxed images) and L non-target categories (blue-boxed images). Existing AL methods cannot accurately distinguish whether the samples are from the target classes or not, thus querying a large number of non-target samples and wasting the annotation budget, while our method can accurately query samples from the target categories. (Color figure online)</p><p>Active learning (AL) is an effective approach to address this issue from a data selection perspective, which selects the most informative samples from an unlabeled sample pool for experts to label and improves the performance of the trained model with reduced labeling cost <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b19">19]</ref>. However, existing AL methods usually work under the closed-set assumption, i.e., all classes existing in the unlabeled sample pool need to be classified by the target model, which does not meet the needs of some real-world scenarios <ref type="bibr" target="#b10">[11]</ref>. Figure <ref type="figure" target="#fig_0">1</ref> shows an AL scenario for pathology image classification in an open world, which is very common in clinical practice. In this scenario, the Whole Slide Images (WSIs) are cut into many small patches that compose the unlabeled sample pool, where each patch may belong to tumor, lymph, normal tissue, fat, stroma, debris, background, and many other categories. However, it is not necessary to perform finegrained annotation and classification for all categories in clinical applications. For example, in the cell classification task, only patches of tumor, lymphatic and normal cells need to be labeled and classified by the target model. Since the nontarget patches are not necessary for training the classifier, labeling them would waste a large amount of budget. We call this scenario in which the unlabeled pool consists of both target class and non-target class samples open-set AL problem. Most existing AL algorithms can only work in the closed-set setting. Even worse, in the open-set setting, they even query more non-target samples because these samples tend to have greater uncertainty compared to the target class samples <ref type="bibr" target="#b10">[11]</ref>. Therefore, for real-world open-set pathology image classification scenarios, an AL method that can accurately query the most informative samples from the target classes is urgently needed.</p><p>Recently, Ning et al. <ref type="bibr" target="#b10">[11]</ref> proposed the first AL algorithm for open-set annotation in the field of natural images. They first trained a network to detect target class samples using a small number of initially labeled samples, and then modeled the maximum activation value (MAV) distribution of each sample using a Gaussian mixture model <ref type="bibr" target="#b15">[15]</ref> (GMM) to actively select the most deterministic target class samples for labeling. Although promising performance is achieved, their detection of target class samples is based on the activation layer values of the detection network which has limited accuracy and high uncertainty with small initial training samples.</p><p>In this paper, we propose a novel AL framework under an open-set scenario, and denote it as OpenAL, which cannot only query as many target class samples as possible but also query the most informative samples from the target classes. OpenAL adopts an iterative query paradigm and uses a two-stage sample selection strategy in each query. In the first stage, we do not rely on a detection network to select target class samples and instead, we propose a feature-based target sample selection strategy. Specifically, we first train a feature extractor using all samples in a self-supervised learning manner, and map all samples to the feature space. There are three types of samples in the feature space, the unlabeled samples, the target class samples labeled in previous iterations, and the non-target class samples queried in previous iterations but not being labeled. Then we select the unlabeled samples that are close to the target class samples and far from the non-target class samples to form a candidate set. In the second stage, we select the most informative samples from the candidate set by utilizing a model-based informative sample selection strategy. In this stage, we measure the uncertainty of all unlabeled samples in the candidate set using the classifier trained with the target class samples labeled in previous iterations, and select the samples with the highest model uncertainty as the final selected samples in this round of query. After the second stage, the queried samples are sent for annotation, which includes distinguishing target and non-target class samples and giving a fine-grained label to every target class sample. After that, we train the classifier again using all the fine-grained labeled target class samples.</p><p>We conducted two experiments with different matching ratios (ratio of the number of target class samples to the total number of samples) on a public 9-class colorectal cancer pathology image dataset. The experimental results demonstrate that OpenAL can significantly improve the query quality of target class samples and obtain higher performance with equivalent labeling cost compared with the current state-of-the-art AL methods. To the best of our knowledge, this is the first open-set AL work in the field of pathology image analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We consider the AL task for pathology image classification in an open-set scenario. The unlabeled sample pool P U consists of K classes of target samples and L classes of non-target samples (usually, K &lt; L). N iterative queries are performed to query a fixed number of samples in each iteration, and the objective is to select as many target class samples as possible from P U in each query, while selecting as many informative samples as possible in the target class samples. Each queried sample is given to experts for labeling, and the experts will give fine-grained category labels for target class samples, while only giving a "non-target class samples" label for non-target class samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Framework Overview</head><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates the workflow of the proposed method, OpenAL. OpenAL performs a total of N iterative queries, and each query is divided into two stages. In Stage 1, OpenAL uses a feature-based target sample selection (FTSS) strategy to query the target class samples from the unlabeled sample pool to form a candidate set. Specifically, we first train a feature extractor with all samples by self-supervised learning, and map all samples to the feature space. Then we model the distribution of all unlabeled samples, all labeled target class samples from previous iterations, and all non-target class samples queried in previous iterations in the feature space, and select the unlabeled samples that are close to the target class samples and far from the non-target class samples. In Stage 2, OpenAL adopts a model-based informative sample selection (MISS) strategy. Specifically, we measure the uncertainty of all unlabeled samples in the candidate set using the classifier trained in the last iteration, and select the samples with the highest model uncertainty as the final selected samples, which are sent to experts for annotation. After obtaining new labeled samples, we train the classifier using all fine-grained labeled target class samples with cross-entropy as the loss function. The FTSS strategy is described in Sect. 2.2, and the MISS strategy is described in Sect. 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature-Based Target Sample Selection</head><p>Self-supervised Feature Representation. First, we use all samples to train a feature extractor by self-supervised learning and map all samples to the latent feature space. Here, we adopt DINO <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> as the self-supervised network because of its outstanding performance.</p><p>Sample Scoring and Selection in the Feature Space. Then we define a scoring function on the base of the distribution of unlabeled samples, labeled target class samples and non-target class samples queried in previous iterations. Every unlabeled sample in the current iteration is given a score, and a smaller score indicates that the sample is more likely to come from the target classes. The scoring function is defined in Eq. 1.</p><formula xml:id="formula_0">s i = s ti -s wi (1)</formula><p>where s i denotes the score of the unlabeled sample x U i . s ti measures the distance between x U i and the distribution of features derived from all the labeled target class samples. The smaller s ti is, the closer x U i is to the known sample distribution of the target classes, and the more likely x U i is from a target class. Similarly, s wi measures the distance between x U i and the distribution of features derived from all the queried non-target class samples. The smaller s wi is, the closer x U i is from the known distribution of non-target class samples, and the less likely x U i is from the target class. After scoring all the unlabeled samples, we select the top ε% samples with the smallest scores to form the candidate set. In this paper, we empirically take twice the current iterative labeling budget (number of samples submitted to experts for labeling) as the sample number of the candidate set. Below, we give the definitions of s ti and s wi .</p><p>Distance-Based Feature Distribution Modeling. We propose a category and Mahalanobis distance-based feature distribution modeling approach for calculating s ti and s wi . The definitions of these two values are slightly different, and we first present the calculation of s ti , followed by that of s wi .</p><p>For all labeled target class samples from previous iterations, their fine-grained labels are known, so we represent these samples as different clusters in the feature space according to their true class labels, where a cluster is denoted as C L t (t = 1, . . . , K). Next, we calculate the score s ti for z U i using the Mahalanobis distance (MD) according to Eq. 2. MD is widely used to measure the distance between a point and a distribution because it takes into account the mean and variance of the distribution, which is very suitable for our scenario.</p><formula xml:id="formula_1">s ti = Nom min t D z U i , C L t = Nom min t z U i -μ t T Σ -1 t z U i -μ t (2) Nom(X) = X -X min X max -X min<label>(3)</label></formula><p>where D(•) denotes the MD function, μ t and Σ t are the mean and covariance of the samples in the target class t, and Nom(•) is the normalization function. It can be seen that s ti is essentially the minimum distance of the unlabeled sample x U i to each target class cluster. For all the queried non-target class samples from previous iterations, since they do not have fine-grained labels, we first use the K-means algorithm to cluster their features into w classes, where a cluster is denoted as C L w (w = 1, . . . , W ).</p><p>W is set to 9 in this paper. Next, we calculate the score s wi for z U i using the MD according to Eq. 4.</p><formula xml:id="formula_2">s wi = Nom min w D z U i , C L w = Nom min w z U i -μ w T Σ -1 t z U i -μ w (<label>4</label></formula><p>) where μ w and Σ w are the mean and covariance of the non-target class sample features in the wth cluster. It can be seen that s wi is essentially the minimum distance of z U i to each cluster of known non-target class samples. The within-cluster selection and dynamic cluster changes between rounds significantly enhance the diversity of the selected samples and reduce redundancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model-Based Informative Sample Selection</head><p>To select the most informative samples from the candidate set, we utilize the model-based informative sample selection strategy in Stage 2. We measure the uncertainty of all unlabeled samples in the candidate set using the classifier trained in the last iteration and select the samples with the highest model uncertainty as the final selected samples. The entropy of the model output is a simple and effective way to measure sample uncertainty <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Therefore, we calculate the entropy of the model for the samples in the candidate set and select 50% of them with the highest entropy as the final samples in the current iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset, Settings, Metrics and Competitors</head><p>To validate the effectiveness of OpenAL, we conducted two experiments with different matching ratios (the ratio of the number of samples in the target class to the total number of samples) on a 9-class public colorectal cancer pathology image classification dataset (NCT-CRC-HE-100K) <ref type="bibr" target="#b5">[6]</ref>. The dataset contains a total of 100,000 patches of pathology images with fine-grained labeling, with nine categories including Adipose (ADI 10%), background (BACK 11%), debris (DEB 11%), lymphocytes (LYM 12%), mucus (MUC 9%), smooth muscle (MUS 14%), normal colon mucosa (NORM 9%), cancer-associated stroma (STR 10%), and colorectal adenocarcinoma epithelium (TUM, 14%). To construct the openset datasets, we selected three classes, TUM, LYM and NORM, as the target classes and the remaining classes as the non-target classes. We selected these target classes to simulate a possible scenario for pathological cell classification in clinical practice. Technically, target classes can be randomly chosen. In the two experiments, we set the matching ratio to 33% (3 target classes, 6 non-target classes), and 42% (3 target classes, 4 non-target classes), respectively.</p><p>Metrics. Following <ref type="bibr" target="#b10">[11]</ref>, we use three metrics, precision, recall and accuracy to compare the performance of each AL method. We use precision and recall to measure the performance of different methods in target class sample selection.</p><p>As defined in Eq. 5, precision is the proportion of the target class samples among the total samples queried in each query and recall is the ratio of the number of the queried target class samples to the number of all the target class samples in the unlabeled sample pool.</p><formula xml:id="formula_3">precision m = k m k m + l m , recall m = m j=0 k m n target<label>(5)</label></formula><p>where k m denotes the number of target class samples queried in the mth query, l m denotes the number of non-target class samples queried in the mth query, and n target denotes the number of target class samples in the original unlabeled sample pool. Obviously, the higher the precision and recall are, the more target class samples are queried, and the more effective the trained target class classifier will be. We measure the final performance of each AL method using the accuracy of the final classifier on the test set of target class samples.</p><p>Competitors. We compare the proposed OpenAL to random sampling and five AL methods, LfOSA <ref type="bibr" target="#b10">[11]</ref>, Uncertainty <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, Certainty <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, Coreset <ref type="bibr" target="#b17">[17]</ref> and RA <ref type="bibr" target="#b20">[20]</ref>, of which only LfOSA <ref type="bibr" target="#b10">[11]</ref> is designed for open-set AL. For all AL methods, we randomly selected 1% of the samples to label and used them as the initial labeled set for model initialization. It is worth noting that the initial labeled samples contain target class samples as well as non-target class samples, but the non-target class samples are not fine-grained labeled. After each query round, we train a ResNet18 model of 100 epochs, using SGD as the optimizer with momentum of 0.9, weight decay of 5e-4, initial learning rate of 0.01, and batchsize of 128. The annotation budget for each query is 5% of all samples, and the length of the candidate set is twice the budget for each query. For each method, we ran four experiments and recorded the average results for four randomly selected seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Comparison</head><p>Figure <ref type="figure" target="#fig_2">3</ref> A and B show the precision, recall and model accuracy of all comparing methods at 33% and 42% matching ratios, respectively. It can be seen that OpenAL outperforms the other methods in almost all metrics and all query numbers regardless of the matching ratio. Particularly, OpenAL significantly outperforms LfOSA <ref type="bibr" target="#b10">[11]</ref>, which is specifically designed for open-set AL. The inferior performance of the AL methods based on the closed-set assumption is due to the fact that they are unable to accurately identify more target class samples, thus wasting a large amount of annotation budget. Although LfOSA <ref type="bibr" target="#b10">[11]</ref> utilizes a dedicated network for target class sample detection, the performance of the detection network is not stable when the number of training samples is small, thus limiting its performance. In contrast, our method uses a novel feature-based target sample selection strategy and achieves the best performance. Upon analysis, our OpenAL is capable of effectively maintaining the balance of sample numbers across different classes during active learning. We visualize the cumulative sampling ratios of OpenAL for the target classes in each round  on the original dataset with a 33% matching ratio, as shown in Fig. <ref type="figure" target="#fig_3">4A</ref>. Additionally, we visualize the cumulative sampling ratios of the LfOSA method on the same setting in Fig. <ref type="figure" target="#fig_3">4B</ref>. It can be observed that in the first 4 rounds, LYM samples are either not selected or selected very few times. This severe sample imbalance weakens the performance of LfOSA compared to random selection initially. Conversely, our method selects target class samples with a more bal-anced distribution. Furthermore, we constructed a more imbalanced setting for the target classes LYM (6000 samples), NORM (3000 samples), and TUM (9000 samples), yet the cumulative sampling ratios of our method for these three target classes remain fairly balanced, as shown in Fig. <ref type="figure" target="#fig_3">4C</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>To further validate the effectiveness of each component of OpenAL, we conducted an ablation test at a matching ratio of 33%. Figure <ref type="figure" target="#fig_2">3C</ref> shows the results, where w/o s w indicates that the distance score of non-target class samples is not used in the scoring of Feature-based Target Sample Selection (FTSS), w/o s t indicates that the distance score of target class samples is not used, w/o MISS means no Model-based Informative Sample Selection is used, i.e., the length of the candidate set is directly set to the annotation budget in each query, and only MISS means no FTSS strategy is used, but only uncertainty is used to select samples.</p><p>It can be seen that the distance modeling of both the target class samples and the non-target class samples is essential in the FTSS strategy, and missing either one results in a decrease in performance. Although the MISS strategy does not significantly facilitate the selection of target class samples, it can effectively help select the most informative samples among the samples in the candidate set, thus further improving the model performance with a limited labeling budget. In contrast, when the samples are selected based on uncertainty alone, the performance decreases significantly due to the inability to accurately select the target class samples. The above experiments demonstrate the effectiveness of each component of OpenAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we present a new open-set scenario of active learning for pathology image classification, which is more practical in real-world applications. We propose a novel AL framework for this open-set scenario, OpenAL, which addresses the challenge of accurately querying the most informative target class samples in an unlabeled sample pool containing a large number of non-target samples. OpenAL significantly outperforms state-of-the-art AL methods on real pathology image classification tasks. More importantly, in clinical applications, on one hand, OpenAL can be used to query informative target class samples for experts to label, thus enabling better training of target class classifiers under limited budgets. On the other hand, when applying the classifier for future testing, it is also possible to use the feature-based target sample selection strategy in the OpenAL framework to achieve an open-set classifier. Therefore, this framework can be applied to both datasets containing only target class samples and datasets also containing a large number of non-target class samples during testing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Description of the open-set AL scenario for pathology image classification. The unlabeled sample pool contains K target categories (red-boxed images) and L non-target categories (blue-boxed images). Existing AL methods cannot accurately distinguish whether the samples are from the target classes or not, thus querying a large number of non-target samples and wasting the annotation budget, while our method can accurately query samples from the target categories. (Color figure online)</figDesc><graphic coords="2,47,31,66,38,325,90,68,59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Workflow of OpenAL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A. Selection and model performance results under a 33% matching ratio. B. Selection and model performance results under a 42% matching ratio. C. Ablation Study of OpenAL under a 33% matching ratio.</figDesc><graphic coords="8,49,29,53,81,325,63,251,89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. A. Cumulative sampling ratios of our OpenAL for the target classes LYM, NORM, and TUM across QueryNums 1-7 on the original dataset (under 33% matching ratio). B. Cumulative sampling ratios of LfOSA on the original dataset (under 33% matching ratio). C. Cumulative sampling ratios of our OpenAL on a newly-constructed more imbalanced setting for the target classes LYM (6000 samples), NORM (3000 samples), and TUM (9000 samples).</figDesc><graphic coords="8,46,80,363,11,330,22,66,13" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was supported by <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">82072021</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xuFzpwX">
					<idno type="grant-number">82072021</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discrepancy-based active learning for weakly supervised bleeding segmentation in wireless capsule endoscopy images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q H</forename><surname>Meng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-13" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Consistency-based semi-supervised evidential active learning for diagnostic radiograph classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Balaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krishnaswamy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_64</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-664" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="675" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Self-supervised vision transformers learn visual concepts in histopathology</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.00585</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Not-so-supervised: a survey of semisupervised, multi-instance, and transfer learning in medical image analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cheplygina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="280" to="296" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-class texture analysis in colorectal cancer histology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A sequential algorithm for training text classifiers: corrigendum and additional data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="13" to="19" />
			<date type="published" when="1995">1995</date>
			<publisher>ACM</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Latent structured active learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient active learning for image classification and segmentation using a sample selection and conditional generative adversarial network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bozorgtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Thiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00934-2_65</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00934-265" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11071</biblScope>
			<biblScope unit="page" from="580" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Warm start active learning with proxy labels and selection via semi-supervised fine-tuning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-129" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="297" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active learning for open-set annotation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="41" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards label-efficient automatic diagnosis and analysis: a comprehensive survey of advanced deep learning-based weakly-supervised, semi-supervised and self-supervised techniques in histopathological image analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dgmil: Distribution guided multiple instance learning for whole slide image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-73" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bi-directional weakly supervised knowledge distillation for whole slide image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="15368" to="15381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gaussian mixture models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Encyclopedia Biometrics</title>
		<imprint>
			<biblScope unit="volume">741</biblScope>
			<biblScope unit="page" from="659" to="663" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiclass deep active learning for detecting red blood cell subtypes in brightfield microscopy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sadafi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_76</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-7" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page">76</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Active learning for convolutional neural networks: a core-set approach</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00489</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep neural network models for computational histopathology: a survey</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Srinidhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ciga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101813</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bayesian generative active deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6295" to="6304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Biomedical image segmentation via representative annotation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5901" to="5908" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
