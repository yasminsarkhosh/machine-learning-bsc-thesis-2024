<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nina</forename><surname>Weng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark</orgName>
								<address>
									<settlement>Kongens Lyngby</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martyna</forename><surname>Plomecka</surname></persName>
							<email>martyna.plomecka@uzh.ch</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manuel</forename><surname>Kaufmann</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ard</forename><surname>Kastrati</surname></persName>
							<email>akastrati@ethz.ch</email>
							<affiliation key="aff3">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roger</forename><surname>Wattenhofer</surname></persName>
							<email>wattenhofer@ethz.ch</email>
							<affiliation key="aff3">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Langer</surname></persName>
							<email>n.langer@psychologie.uzh.ch</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">16115EC228839D2C64BA8B2B9B760486</idno>
					<idno type="DOI">10.1007/978-3-031-43895-069.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>EEG</term>
					<term>Interpretable model</term>
					<term>Attention Mechanism</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Eye movements can reveal valuable insights into various aspects of human mental processes, physical well-being, and actions. Recently, several datasets have been made available that simultaneously record EEG activity and eye movements. This has triggered the development of various methods to predict gaze direction based on brain activity. However, most of these methods lack interpretability, which limits their technology acceptance. In this paper, we leverage a large data set of simultaneously measured Electroencephalography (EEG) and Eye tracking, proposing an interpretable model for gaze estimation from EEG data. More specifically, we present a novel attention-based deep learning framework for EEG signal analysis, which allows the network to focus on the most relevant information in the signal and discard problematic channels. Additionally, we provide a comprehensive evaluation of the presented framework, demonstrating its superiority over current methods in terms of accuracy and robustness. Finally, the study presents visualizations that explain the results of the analysis and highlights the potential of attention mechanism for improving the efficiency and effectiveness of EEG data analysis in a variety of applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gaze information is a widely used behavioral measure to study attentional focus <ref type="bibr" target="#b6">[7]</ref>, cognitive control <ref type="bibr" target="#b18">[19]</ref>, memory traces <ref type="bibr" target="#b22">[23]</ref> and decision making <ref type="bibr" target="#b27">[28]</ref>. The most commonly used gaze estimation technique in laboratory settings is the infrared eye tracker, which detects gaze position by emitting invisible near-infrared light and then capturing the reflection from the cornea <ref type="bibr" target="#b5">[6]</ref>. While infrared eye tracker still remains the most accurate and reliable solution for the gaze estimation, these systems have several limitations, including individual differences in the contrast of the pupil and iris and the need for time-consuming setup and calibration before each scanning session <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Recently, Electroencephalogram (EEG) has been explored as an alternative method to estimate eye movements by recording electrical activity from the brain non-invasively with high temporal resolution <ref type="bibr" target="#b15">[16]</ref>. The growing body of literature has shown that Deep Learning architectures could be significantly effective for many EEG-based tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>. Nevertheless, with the advantages that Deep Learning brings, new challenges arise. Most of these models applied to electroencephalography (EEG) data tend to lack interpretability, making it difficult to understand the underlying reasons for their predictions, which subsequently leads to a decrease in the acceptability of advanced technology in neuroscience <ref type="bibr" target="#b24">[25]</ref>. However, a potential solution already exists, in the form of the attention mechanism <ref type="bibr" target="#b28">[29]</ref>. The attention mechanism has the potential to provide a more transparent and understandable way of analyzing EEG data, enabling us to comprehend the relationships between different brain signals better and make more informed decisions based on the results. With the development and implementation of these techniques, we can look forward to a future where EEG data can be utilized more effectively and efficiently in various applications.</p><p>Attention mechanisms have recently emerged as a powerful tool for processing sequential data, including time-series data in various fields such as natural language processing, speech recognition, and computer vision <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29]</ref>. In the context of EEG signal analysis, attention mechanism has shown promising results in various applications, including sleep stage classification, seizure detection, and event-related potential analysis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17]</ref>. Since different electrodes record the brain activity from the different brain areas and functions, the information density from each electrode can vary for different tasks <ref type="bibr" target="#b14">[15]</ref>.</p><p>In this study, we introduce a new deep learning framework for analyzing EEG signals applying attention mechanisms. For the method evaluation, we used the EEGEyeNet dataset and benchmark <ref type="bibr" target="#b15">[16]</ref>, which includes concurrent EEG and infrared eye-tracking recordings, with eye tracking data serving as a ground truth. Our method incorporates attention modules to assign weights to individual electrodes based on their importance, allowing the network to prioritize relevant information in the signal. Specifically, we demonstrate the ability of our framework to accurately predict gaze position and saccade direction, achieving superior performance compared to previously benchmarked methods. Furthermore, we provide visualizations of model's interpretability through case studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation</head><p>In this study, our primary goal was to build a model sensitive to different electrodes. The motivation for this goal is two-fold. Firstly, with regards to interpreting the model, the electrodes can be considered the smallest entity as they record signals from specific regions of the brain. Therefore, the electrodebased explanation is a reasonable approach considering human understanding. Second, in the context of model learning, incorporating adaptive weighting of electrodes within a neural network can potentially enhance the accuracy and reliability of gaze estimation systems. This is because electrodes are functionally connected to cognitive behaviors. Specifically, in tasks such as gaze estimation, electrodes positioned near the eyes can capture electrical signals from the orbicularis oculi muscles <ref type="bibr" target="#b1">[2]</ref>, thereby making the pre-frontal brain areas more crucial for precise estimation <ref type="bibr" target="#b14">[15]</ref>. Additionally, the noise of EEG recordings could be induced by broken wire contacts, too much or dried gel, or loose electrodes <ref type="bibr" target="#b26">[27]</ref>, the influence of such electrodes should be reduced in the network under ideal circumstances. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, our model design focuses on enhancing an existing deep learning architecture with an electrode-sensitive component. This component first extracts electroderelated information, and then utilizes this information for two purposes:</p><p>(1) emphasizing the reliable electrodes and diminishing the influence of suspicious electrodes, while simultaneously (2) providing explanations for each prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention-CNN</head><p>Following the idea from the previous section, we propose the Attention-CNN model, where the attention blocks are used as the electrode-sensitive component. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the Attention-CNN model is structured by adding an attention block after each convolution block in every layer and an additional single attention block before the final prediction block (the blocks in blue). A convolution block contains a convolution layer, a batch-norm layer <ref type="bibr" target="#b13">[14]</ref>, a leaky ReLU <ref type="bibr" target="#b17">[18]</ref> and a max-pooling layer. In addition, the residual <ref type="bibr" target="#b9">[10]</ref> techniques are applied in the CNN framework. The convolution layer operates only in the time dimension. The attention blocks, acting as an electrode-sensitive component, can be carried out by Squeeze-and-Excitation Block (SE Block) <ref type="bibr" target="#b11">[12]</ref> and/or Self-Attention Block (SA Block) <ref type="bibr" target="#b28">[29]</ref>. In the attention blocks, the retrieved electrode importance is used to weigh the features in each layer. Additionally, the same weights can provide explanations for the predictions of the model. In the prediction block, the features are flattened and then fed into the fully connected layer to finally obtain the predictions. While the SA Block is only required once in the process, the SE Blocks are added in every residual block. In order to keep the same scale for the same sample, the parameters of the SE Blocks are shared for the whole process. All building blocks are trained end-to-end, including the weights for the electrode importance used in the attention blocks.</p><p>Squeeze and Excitation Block: the SE block involves two principle operations. The Squeeze operation compresses features u ∈ R T ×J into electrodewise vectors z ∈ R J by using global average pooling. Here, T denotes the feature size, and J is the number of electrodes. More precisely, the j-th element of z is calculated by</p><formula xml:id="formula_0">z j = F sq (u j ) = 1 T T i=1 u j (i).</formula><p>The Excitation operation first computes activation s by employing the gating mechanism with sigmoid activation:</p><formula xml:id="formula_1">s = F ex (z, W) = σ(W 2 δ(W 1 z))</formula><p>, where σ refers to the sigmoid function, δ represents the ReLU <ref type="bibr" target="#b19">[20]</ref> function, and W are learnable weights. The final output of SE block weigh each channel adaptively by re-scaling U with s: xj = F scale (u j , s j ) = s j • u j . In contrast to the original implementation <ref type="bibr" target="#b11">[12]</ref> which deals with 3-dimensional data, the input data in our setup has only 2 dimensions (electrodes and time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self Attention Block:</head><p>The self-attention mechanism <ref type="bibr" target="#b21">[22]</ref> was first used in the field of Natural language processing (NLP), aiming at catching the attention of/between different words in a sentence or paragraph. The attention is obtained by letting the input data interact with themselves and determining which features are more important. This was implemented by introducing the Query, Key, Value technique, which is defined as</p><formula xml:id="formula_2">Q = φ Q (U, W Q ), K = φ K (U, W K ), V = φ V (U, W V )</formula><p>, where U denotes the input of self-attention block and φ(•, •) represents linear transformation.</p><p>Then, Attention Weights are computed using Query and Key:</p><formula xml:id="formula_3">M att = sof tmax( Q • K T √ d k )</formula><p>where d k stands for the dimensions of the Key, and √ d k works as a scaling factor. The softmax function was applied to adjust the range of the value in attention weights (M att ) to [0, 1].</p><p>Unlike the transformer model, the attention weights are first compressed into a one-dimensional vector by a layer of global average pooling (ψ) and normalized by a sigmoid function. More precisely, we compute Z att = sigmoid(ψ(M att )). Finally, the output of SA Block X is computed by : X = κ(Z att , V ), where κ denotes the electrode-wise production.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Materials and Experimental Settings</head><p>EEGEyeNet Dataset: For our experiments, we utilized the EEGEyeNet dataset <ref type="bibr" target="#b15">[16]</ref>, which includes synchronized EEG and Eye-tracking data. The EEG signals were collected using a high-density, 128-channel EEG Geodesic Hydrocel system sampled at a frequency of 500 Hz. Eye-tracking data, including eye position and pupil size, were gathered using an infrared video-based eye tracker (Eye-Link 1000 Plus, SR Research), also operating at a sampling rate of 500 Hz. The recorded EEG and eye-tracking information was pre-processed, synchronized and segmented into 1-second clips based on eye movements. The infrared eye tracking recordings were used as ground truth. In this paper, the processed dataset we utilized contains two parts: the Position Task and Direction Task, which correspond to two types of eye movements: fixation, i.e., the maintaining of the gaze on a single location, and saccade, i.e. the rapid eye movements that shift the centre of gaze from one point to another. While Position Task estimates the absolute position from fixation, Direction Task estimates the relative changes during saccades, involving two sub-tasks, i.e., the prediction of amplitude and angle. The statistics and primary labels of these two parts are shown in Table <ref type="table" target="#tab_0">1</ref>. To ensure data integrity and prevent data leakage, the dataset was split into training, validation, and test sets across subjects, with 70 % of the subjects used for training, and 15% each for validation and testing. This procedure ensures that no data from the same subject appears in both the training and validation/testing phases, thereby avoiding potential subject-related patterns from being learned by the model during training and tested on in validation/testing. For more details of this dataset, please refer to <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details:</head><p>The experiments are implemented with PyTorch <ref type="bibr" target="#b20">[21]</ref>. When training the Attention-CNN model, the batch size is set to 32, the number of epochs is 50, and the learning rate is 1e -4 . There are 12 convolution blocks, and the residual operation repeats every three convolution blocks. The feature length of the hidden layer is set as 64, and the kernel size is 64. The number of convolutional layers, kernel size and hidden feature length, are selected based on validation performance. We conducted experiments with three configurations: the SE Block and the SA Block together, only one of the attention blocks, or no attention blocks at all. For the angle prediction in Direction Task, we use angle loss l angle = |(atan(sin(pt), cos(pt))|, where p denotes the predicted results, and t denotes the targets. For Position Task and Amplitude prediction in the Direction Task, the loss function is set to smooth-L1 <ref type="bibr" target="#b8">[9]</ref>.</p><p>Evaluation: For Position task, Euclidean distance is applied as the evaluation metric in both pixels and visual angles. Compared to pixel distance, visual angles depend on both object size on the screen and the viewing distance, thus enabling the comparison across varied settings. The performance of Direction Task is measured by the square root of the mean squared error (RMSE) for the angle (in radians) and the amplitude (in pixels) of saccades. In order to avoid the error caused by the repeatedness of angles in the plane (i.e. 2π and 0 rad represents the same direction), atan(sin(α), cos(α)) is applied, just like in angle loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance of the Attention-CNN</head><p>Table <ref type="table" target="#tab_1">2</ref> shows the quantitative performance of the Attention-CNN in this work. For the Position Task, CNN with SE block has an average performance with the RMSE of 109.58 pixels. Likewise, the CNN model with both SE block and the SA block has a similar performance (110.05 pixels). Similar to Position Task, in amplitude prediction of Direction Task, the attention blocks aid the prediction evidently, heightening the performance by 5 pixels. Here, the model with both attention blocks has a lower variance. For angle prediction, the CNN model with both SE block and SA block has the best performance among all with the RMSE of 0.1707 rad.</p><p>We can conclude that the CNN model with both attention blocks consistently outperforms the CNN model alone by 5 to 10 percent across all tasks, indicating that electrode-wise attention assists in the learning process of the models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Interpretability by Case Studies</head><p>To provide a more detailed analysis of the interpretability of our proposed Attention-CNN model, as well as to further investigate the underlying reasons for the observed accuracy improvement, we conducted a visual analysis of the model performance, with a particular focus on the role of the attention block.</p><p>Our analysis yielded two key findings, which are as follows: Firstly, the attention blocks were able to detect the electrical difference between the right and left pre-frontal area in case of longer saccades, i.e. rapid eye movements from one side of the screen to the other; see the saccades (d) and (e) in Fig. <ref type="figure" target="#fig_2">3</ref>. We present the sequence of saccades and observed the EEG signals as well as the electrode importance from proposed models in Fig. <ref type="figure" target="#fig_2">3</ref>. The attention block effectively captured this phenomenon by highlighting the electrodes surrounding the prominent signals (saccades (d) and (e) in Fig. <ref type="figure" target="#fig_2">3</ref>). Conversely, in cases where the saccade was of a shorter distance (other saccades in Fig. <ref type="figure" target="#fig_2">3</ref>), attention was more widely distributed across the scalp rather than being concentrated in specific regions. This is justifiable as the neural network aims to integrate a more comprehensive set of information from all EEG channels.</p><p>Additionally, the attention block effectively learned to circumvent the interference caused by noisy electrodes and redirected attention towards the frontal region. Figure <ref type="figure" target="#fig_3">4</ref> illustrates a scenario where problematic electrodes were situated around both ears, exhibiting abnormal amplitudes (±100 µV). Using Layer-wise Relevance Propagation <ref type="bibr" target="#b0">[1]</ref> to elucidate the CNN model's predictions, the result depicted in Fig. <ref type="figure" target="#fig_3">4b</ref> revealed that the most significant electrodes were located over the left ear, coinciding with the noisy electrodes. In contrast, as shown in Fig. <ref type="figure" target="#fig_3">4c</ref>, the Attention-CNN model effectively excluded the unreliable electrodes and allocated greater attention to the frontal region of the brain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Explainability Quantification</head><p>We further examine the validity in explainability of the proposed method by comparing the distribution of learned attention of noisy and non-noisy electrodes in the Direction Task. The attention block's effectiveness is demonstrated by its ability to assign lower weights to these noisy electrodes in contrast to the non-noisy ones. Within all samples in the Direction Task that feature at least one noisy electrode, only 19% of the non-noisy electrodes had normalized attention weights below 0.05. In contrast, 42% of the noisy electrodes exhibited this trait, implying the attention block's ability to reduce weights of abnormal electrodes. We direct readers to the Supplementary materials for a distribution plot showcasing the difference between noisy and non-noisy electrodes, along with additional details. It's important to note that quantifying explainability methods for signal-format data, such as EEG, presents a significant challenge and has limited existing research. Therefore, additional investigations in this field are anticipated in future studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we aimed to address the issue of the lack of interpretability in deep learning models for EEG-based tasks. Our approach was to leverage the fact that EEG signal noise or artifacts are often localized to specific electrodes. We accomplished this by incorporating attention modules as electrode-sensitive components within a neural network architecture. These attention blocks were used to emphasize the importance of specific electrodes, resulting in more accurate predictions and improved interpretability through the use of scaling.</p><p>Moreover, our proposed approach was less susceptible to noise. We conducted comprehensive experiments to evaluate the performance of our proposed Attention-CNN model. Our results demonstrate that this model can accurately classify EEG and eye-tracking data while also providing insights into the quality of the recorded EEG signals. This contribution is significant as it can lead to the development of new decoding techniques that are less sensitive to noise.</p><p>In summary, our study underscores the importance of incorporating attention mechanisms into deep learning models for analyzing EEG and eye-tracking data. This approach opens up new avenues for future research in this area and has the potential to provide valuable insights into the neural basis of cognitive processes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. We augment an electrode-sensitive component to a deep learning model, which works as follows: a) extract electrode-wise information from input data, b) control the predictions, and c) provide explanations.</figDesc><graphic coords="3,231,66,212,06,136,60,65,77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The Architecture of the Attention-CNN model. (color figure online)</figDesc><graphic coords="3,43,80,468,80,336,46,107,83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization of signal intensity across scalp and electrode importance from our models. Left: the track of a continuous sequence of saccades. Right: the corresponding brain activities (red: positive electrical signal, blue: negative electrical signal) and the important electrodes detected by the attention-based model (denoted as yellow nodes, the threshold is set as the mean value of all electrodes during the sequence). The model used here is the CNN with SA block. (Color figure online)</figDesc><graphic coords="7,60,30,53,84,304,00,143,26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. One example of test samples containing problematic electrodes is the Position Task. As shown in (a), the dark red areas around the ears represent intense electrical signals with abnormal amplitudes (&gt;100 V). In (b), the Layer-wise Relevance Propagation (LRP) results from the CNN model reveal that the electrodes around the left ear still play a crucial role in the prediction process. Conversely, the Attention-CNN model's results (c), indicate that it bypasses the ear area and allocates more emphasis to the pre-frontal region. As a result, the error in Euclidean Distance improved by 200.85 pixels for this specific sample (from 265.18 to 64.33). (Color figure online)</figDesc><graphic coords="8,58,98,100,91,334,42,89,23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dataset Description</figDesc><table><row><cell>Task</cell><cell cols="3">#Subjects #Samples Primary labels</cell></row><row><cell cols="2">Position 72</cell><cell>50264</cell><cell>subject id: the identical ID of the participant</cell></row><row><cell></cell><cell></cell><cell></cell><cell>pos: the fixation position in the form of (x, y)</cell></row><row><cell cols="2">Direction 72</cell><cell>41783</cell><cell>subject id: the identical ID of the participant</cell></row><row><cell></cell><cell></cell><cell></cell><cell>amplitude:the distance in pixels during the saccade</cell></row><row><cell></cell><cell></cell><cell></cell><cell>angle: the saccade direction in radians</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The performance of the Attention-CNN on Direction and Position Task.</figDesc><table><row><cell>Models</cell><cell>Angle/Amplitude</cell><cell></cell><cell>Abs. Position</cell></row><row><cell></cell><cell cols="2">Angle RMSE Amp. RMSE</cell><cell>Euclidean Distance (Visual Angle)</cell></row><row><cell>CNN</cell><cell>0.1947 ± 0.021</cell><cell>57.4486 ± 2.053</cell><cell>115.0143 ± 0.648 (2.39 ± 0.010)</cell></row><row><cell>CNN + SE</cell><cell>0.1754 ± 0.007</cell><cell>55.1656 ± 3.513</cell><cell>109.5816 ± 0.238 (2.27 ± 0.004)</cell></row><row><cell cols="2">CNN + SA 0.1786 ± 0.010</cell><cell cols="2">52.1583 ± 1.943 112.3823 ± 0.851 (2.33 ± 0.013)</cell></row></table><note><p>CNN + both 0.1707 ± 0.011 52.2782 ± 1.169 110.0523 ± 0.670 (2.28 ± 0.010)</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Eye movement analysis for activity recognition using electrooculography</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gellersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tröster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="741" to="753" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Best practices in eye tracking research</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Luke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Psychophysiol</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning for electroencephalogram (EEG) classification tasks: a review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Craik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Contreras-Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neural Eng</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">31001</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Eye tracking techniques. eye tracking methodology: Theory Pract</title>
		<author>
			<persName><forename type="first">A</forename><surname>Duchowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Duchowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond eye gaze: what else can eyetracking reveal about cognition and cognitive development?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guerra-Carrillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T M</forename><surname>Singley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Bunge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dev. Cogn. Neurosci</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="69" to="91" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic sleep staging algorithm based on time attention mechanism</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Hum. Neurosci</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">692054</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Eye tracker data quality: what it is and how to measure it</title>
		<author>
			<persName><forename type="first">K</forename><surname>Holmqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nyström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mulvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Eye Tracking Research and Applications</title>
		<meeting>the Symposium on Eye Tracking Research and Applications</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">EEG-based emotion recognition using convolutional recurrent neural network with multi-head self-attention</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">11255</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Electrode clustering and bandpass analysis of eeg data for gaze estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kastrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Plomecka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Küchler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Langer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wattenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.12710</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Eegeyenet: a simultaneous electroencephalography and eyetracking dataset and benchmark for eye movement prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kastrati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05100</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">EEG-transformer: Self-attention from transformer architecture for decoding eeg of imagined speech</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 10th International Winter Conference on Brain-Computer Interface (BCI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Look away: the anti-saccade task and the voluntary control of eye movement</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Everling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="218" to="228" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pytorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM Sigkdd International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM Sigkdd International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note>why should i trust you?</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eye movement monitoring of memory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Riggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcquiggan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JoVE (J. Visualized Exp.)</title>
		<imprint>
			<biblScope unit="issue">42</biblScope>
			<biblScope unit="page">2108</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Self-attention with relative position representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interpretable deep neural networks for single-trial EEG classification</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci. Methods</title>
		<imprint>
			<biblScope unit="volume">274</biblScope>
			<biblScope unit="page" from="141" to="145" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A novel deep learning approach for classification of EEG motor imagery signals</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">R</forename><surname>Tabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Halici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neural Eng</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16003</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fundamentals of EEG measurement. Measure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Teplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scie. Rev</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What eye tracking can reveal about dynamic decisionmaking</title>
		<author>
			<persName><forename type="first">F</forename><surname>Vachon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tremblay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Cogn. Eng. Neuroergonom</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="157" to="165" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
