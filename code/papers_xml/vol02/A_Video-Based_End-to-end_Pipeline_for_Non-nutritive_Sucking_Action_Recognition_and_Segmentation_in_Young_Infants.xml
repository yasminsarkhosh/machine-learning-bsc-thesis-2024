<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants</title>
				<funder>
					<orgName type="full">MathWorks</orgName>
				</funder>
				<funder ref="#_HhvPT4U">
					<orgName type="full">NSF-CAREER</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaotong</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Augmented Cognition Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Augmented Cognition Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Roux Institute</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>ME</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elaheh</forename><surname>Hatamimajoumerd</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Augmented Cognition Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kashish</forename><surname>Jain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Augmented Cognition Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuel</forename><surname>Zlota</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Augmented Cognition Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cholpady</forename><forename type="middle">Vikram</forename><surname>Kamath</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Augmented Cognition Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cassandra</forename><forename type="middle">B</forename><surname>Rowan</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Psychology Department</orgName>
								<orgName type="institution">University of Maine</orgName>
								<address>
									<settlement>Orono</settlement>
									<region>ME</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emma</forename><forename type="middle">C</forename><surname>Grace</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Psychology Department</orgName>
								<orgName type="institution">University of Maine</orgName>
								<address>
									<settlement>Orono</settlement>
									<region>ME</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Goodwin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Khoury College of Computer Sciences</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Bouvé College of Health Sciences</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marie</forename><forename type="middle">J</forename><surname>Hayes</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Psychology Department</orgName>
								<orgName type="institution">University of Maine</orgName>
								<address>
									<settlement>Orono</settlement>
									<region>ME</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rebecca</forename><forename type="middle">A</forename><surname>Schwartz-Mette</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Psychology Department</orgName>
								<orgName type="institution">University of Maine</orgName>
								<address>
									<settlement>Orono</settlement>
									<region>ME</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emily</forename><surname>Zimmerman</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Bouvé College of Health Sciences</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Sarah</forename><surname>Ostadabbas</surname></persName>
							<email>ostadabbas@ece.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Augmented Cognition Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="586" to="595"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">05FC280160A77CF132274D86E62DEDF3</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_55</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Non-nutritive sucking</term>
					<term>Action recognition</term>
					<term>Action segmentation</term>
					<term>Optical flow</term>
					<term>Temporal convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an end-to-end computer vision pipeline to detect non-nutritive sucking (NNS)-an infant sucking pattern with no nutrition delivered-as a potential biomarker for developmental delays, using off-the-shelf baby monitor video footage. One barrier to clinical (or algorithmic) assessment of NNS stems from its sparsity, requiring experts to wade through hours of footage to find minutes of the relevant activity. Our NNS activity segmentation algorithm tackles this problem by identifying periods of NNS with high certainty-up to 94.0% average precision and 84.9% average recall across 30 heterogeneous 60 s clips, drawn from our manually annotated NNS clinical in-crib dataset of 183 h of overnight baby monitor footage from 19 infants. Our method is based on an underlying NNS action recognition algorithm, which uses spatiotemporal deep learning networks and infant-specific pose estimation, achieving 94.9% accuracy in binary classification of 960 2.5 s balanced NNS vs. non-NNS clips. Tested on our second, independent, and public NNS in-the-wild dataset, NNS recognition classification reaches 92.3% accuracy, and NNS segmentation achieves 90.8% precision and 84.2% recall.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>. Top: Illustration of non-nutritive sucking (NNS) signal extracted from a pressure transducer pacifier device <ref type="bibr" target="#b15">[15]</ref>. Our computer vision-based NNS recognition and segmentation algorithms enable algorithmic identification of the relatively rare periods of high NNS activity from long videos, facilitating subsequent clinical expert evaluation. Bottom: Still frames from our NNS clinical in-crib dataset (left) and our public NNS in-the-wild dataset (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Non-nutritive sucking (NNS) is an infant oral sucking pattern characterized by the absence of nutrient delivery <ref type="bibr" target="#b11">[11]</ref>. NNS reflects neural and motor development in early life <ref type="bibr" target="#b16">[16]</ref> and may reduce the risk of SIDS <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b24">24]</ref>, the leading cause of death for US infants aged 1-12 months <ref type="bibr" target="#b1">[2]</ref>. However, studying the relationship between NNS patterns and breathing, feeding, and arousal during sleep has been challenging due to the difficulty of measuring the NNS signal.</p><p>NNS occurs in bursts of 6-12 sucks at 2 Hz per suck, with bursts happening a few times per minute during high activity periods <ref type="bibr" target="#b25">[25]</ref>. However, active periods are sporadic, representing only a few minutes per hour, creating a burden for researchers studying characteristics of NNS. Current transducer-based approaches (see Fig. <ref type="figure">1</ref>) are effective, but expensive, limited to research use, and may affect the sucking behavior <ref type="bibr" target="#b26">[26]</ref>. This motivates our development of an end-to-end computer vision system to recognize and segment NNS actions from lengthy videos, enabling applications in automatic screening and telehealth, with a focus on high precision to enable periods of sucking activity to be reliably extracted for analysis by human experts.</p><p>Our contributions address the fine-grained NNS action recognition problem of classifying 2.5 s video clips, and the NNS action segmentation problem of detecting NNS activity in minute-long video clips. The action recognition method uses convolutional long short-term memory networks for spatiotemporal learning. We address data scarcity and reliability issues in real-world baby monitor footage using tailored infant pose state estimation, focusing on the face and pacifier region, and enhancing it with dense optical flow. The action segmentation method aggregates local NNS recognition signals from sliding windows.</p><p>We present two new datasets in our work: the NNS clinical in-crib dataset, consisting of 183 h of nighttime in-crib baby monitor footage collected from 19 infants and annotated for NNS activity and pacifier use by our interdisciplinary team of behavioral psychology and machine learning researchers, and the NNS in-the-wild dataset, consisting of 10 naturalistic infant video clips annotated for NNS activity. Figure <ref type="figure">1</ref> displays sample frames from both datasets.</p><p>Our main contributions are (1) creation of the first infant video datasets manually annotated with NNS activity; (2) development of an NNS classification system using a convolutional long short-term memory network, aided by infant domain-specific face localization, video stabilization, and customized signal enhancement, and (3) successful NNS segmentation on longer clips by aggregating local NNS recognition results across sliding windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Current methods for measuring the NNS signal are limited to the pressured transducer approach <ref type="bibr" target="#b25">[25]</ref>, and a video-based approach that uses facial landmark detection to extract the jaw movement signal <ref type="bibr" target="#b9">[9]</ref>. The latter relies on a 3D morphable face model <ref type="bibr" target="#b10">[10]</ref> learned from adult face data, limiting its accuracy, given the domain gap between infant and adult faces <ref type="bibr" target="#b22">[22]</ref>; its output also does not directly address NNS classification or segmentation. Our approach offers an efficient, end-to-end solution for both tasks and is freely available.</p><p>Action recognition is the task of identifying the action label of a short video clip from a set of predetermined classes. In our case, we wish to classify short infant clips based on the presence or absence of NNS. As with many action recognition algorithms, our core model is based on extending 2D convolutional neural networks to the temporal dimension for spatiotemporal data processing.</p><p>In particular, we make use of sequential networks (such as long short-term memory (LSTM) networks) after frame-wise convolution to enhance mediumrange temporal dependencies <ref type="bibr" target="#b23">[23]</ref>.</p><p>Action segmentation is the task of identifying the periods of time during which specified events occur, often from longer untrimmed videos containing mixed activities. We follow an approach to segmentation common in limiteddata contexts, patching together signals from a local low-level layer-our NNS action recognition-to obtain a global segmentation result <ref type="bibr" target="#b4">[5]</ref>.  involves dividing the video into short sliding windows, applying our NNS action recognition module to classify NNS vs. non-NNS (or obtain confidence scores), and aggregating the output classes (or scores) to generate a segmentation result with predicted start and end timestamps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Infant NNS Action Recognition and Segmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NNS Action Recognition</head><p>The core of our model is the NNS action recognition system shown in Fig. <ref type="figure" target="#fig_1">2b</ref>. It consists of a frame-based preprocessing module and a spatiotemporal classifier. The preprocessing module utilizes a pre-trained model, while only the spatiotemporal classifier is trained with our data.</p><p>Preprocessing Module. Our frame-based preprocessing module applies the following transformations in sequence. All three steps are used to produce training data for the subsequent spatiotemporal classifier, but during inference, the data augmentation step is not applicable and is omitted.</p><p>Smooth Facial Crop. The RetinaFace face detector <ref type="bibr" target="#b3">[4]</ref> is applied to frames in each clip until a face bounding box is found and propagated to earlier and later frames using the minimum output sum of squared error (MOSSE) tracker <ref type="bibr" target="#b0">[1]</ref>. To smooth the facial bounding box sequence and address temporal discontinuity, saliency corners <ref type="bibr" target="#b19">[19]</ref> are detected from the initial frame and tracked to the next frame using the Lucas-Kanade optical flow algorithm <ref type="bibr" target="#b14">[14]</ref>. The trajectory is smoothed using a moving average filter and applied to each bounding box to stabilize the facial area. The raw input video is then cropped to this smoothed bounding box, resulting in a video featuring the face alone. Data Augmentation. When preprocessing videos to create training data for the spatiotemporal classifier, we apply random rotations, scaling, and flipping to the face-cropped video, to improve generalizability in our data-limited setting.</p><p>Optical Flow. After trimming and augmentation, we calculate the short-time dense optical flow <ref type="bibr" target="#b13">[13]</ref> between adjacent frames, and map the results into the hue, saturation, and value (HSV) color space by cascading the optical flow direction vector and magnitude of each pixel. This highlights the apparent motion between frames, magnifying subtle NNS movements (as illustrated in Supp. Fig. <ref type="figure" target="#fig_2">S3</ref>.) <ref type="foot" target="#foot_0">1</ref> .</p><p>Spatiotemporal-Based Action Classifier. Finally, the optical flow video is processed by a spatiotemporal model that outputs an action class label (NNS or non-NNS). Two-dimensional convolutional neural networks extract spatial representations from static images, which are then fed in sequence to a temporal convolution network for spatiotemporal processing. The final classification outcome is the output of the last temporal convolution network unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NNS Action Segmentation</head><p>To segment NNS actions in mixed videos with transitions between NNS and non-NNS activity, we applied NNS recognition in 2.5 s sliding windows and aggregated results to predict start and end timestamps. This window length provides fine-grained resolution for segmentation while being long enough (26 frames at a 10 Hz frame rate) for consistent human and machine detection of NNS behavior. To address concerns about the coarseness of this resolution, we tested the following window-aggregation configurations, the latter two of which have finer 0.5 s effective resolutions: Tiled: 2.5 s windows precisely tile the length of the video with no overlaps, and the classification outcome for each window is taken directly to be the segmentation outcome for that window. Sliding: 2.5 s windows are slid across with 0.5 s overlaps, and the classification outcome for each window is assigned to its (unique) middle-fifth 0.5 s segment as the segmentation outcome. Smoothed: 2.5 s windows are slid across with 0.5 s overlaps, the classification confidence score for each window is assigned to its middle-fifth 0.5 s segment, a 2.5 s moving average of these confidence scores are taken, then the averaged confidence scores are thresholded for the final segmentation outcome. Tens of thousands of timestamps for NNS and pacifier activity were placed, by two trained behavioral coders per video. For NNS, the definition of an event segment was taken to be an NNS burst: a sequence of sucks with &lt;1 s gaps between. We restrict our subsequent study to NNS during pacifier use, which was annotated more consistently. Cohen κ annotator agreement of NNS events during pacifier use (among 10 pacifier-using infants) averaged 0.83 in 10 s incidence windows, indicating strong agreement by behavioral coding standards, but we performed further manual selection to increase precision for machine learning use, as detailed below<ref type="foot" target="#foot_1">2</ref> . We also created a smaller but publicly available NNS in-the-wild dataset of 14 YouTube videos featuring infants in natural conditions, with lengths ranging from 1 to 30 min, and similar annotations.</p><p>From each of these two datasets, we extracted 2.5 s clips for the classification task and 60 s clips for the segmentation task. In the NNS clinical in-crib dataset, we restricted our attention to six infant videos containing enough NNS activity during pacifier use for meaningful clip extraction. From each of these, we randomly drew up to 80 2.5 s clips consisting entirely of NNS activity and 80 2.5 s clips containing non-NNS activity for classification, for a total of 960; and five 60 s clips featuring transitions between NNS and non-NNS activity for segmentation, for a total of 30; redrawing if available when annotations were not sufficiently accurate. In the NNS in-the-wild dataset, we restricted to five infants exhibiting sufficient NNS activity during pacifier use, from which we drew 38 2.5 s clips each of NNS and no NNS activity for classification, for a total of 76; and from two to 26 60 s clips of mixed activity from each infant for segmentation, for a total of 39; again redrawing in cases of poor annotations. The 2.5 s clips for classification are equally balanced for NNS and non-NNS activity to support machine learning training; the 60 s mixed clips intended for segmentation intentionally over-represent NNS compared to its natural incidence rate (see Supp. Table <ref type="table" target="#tab_1">S1</ref>), to enable meaningful statistical conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NNS Recognition Implementation and Results</head><p>For the spatiotemporal core of our NNS action recognition, we experimented with four configurations of 2D convolutional networks, a 1-layer CNN, ResNet18, ResNet50, and ResNet101 <ref type="bibr" target="#b8">[8]</ref> (all ResNet are pre-trained using ImageNet dataset and we finetune their last fully connected layer on our data); and three configurations of sequential networks, an LSTM, a bi-directional LSTM, and a transformer model <ref type="bibr" target="#b21">[21]</ref> <ref type="foot" target="#foot_2">3</ref> . The models were trained for 20 epochs under a learning rate of 0.0001, and the best model was chosen based on a held-out validation set.</p><p>We trained and tested this method with NNS clinical in-crib data from six infant subjects under a subject-wise leave-one-out cross-validation paradigm. Action recognition accuracies are reported on the top left of Table <ref type="table" target="#tab_1">1</ref>. The ResNet18-LSTM configuration performed best, achieving 94.9% average accuracy over six infants using optical flow input. The strong performance (≥85.2%) across all configurations indicates the viability of the overall method. We also evaluated a model trained on all six infants from the clinical in-crib dataset on the independent in-the-wild dataset. Results on the bottom left of Table <ref type="table" target="#tab_1">1</ref> again show strong cross-configuration performance (≥79.5%), with ResNet101-Transformer reaching 92.3%, demonstrating strong generalizability of the method. As expected, models trained on the clinical in-crib data test worse on the independent in-the-wild data. But interestingly, models with the smaller ResNet18 network suffered steep drop-offs in performance when tested on the in-the-wild data, while models based on the complex ResNet101 fared better under the domain shift. Beyond this, it is hard to identify clear trends between configurations or capacities and performance.</p><p>Optical Flow Ablation. Performance of all models with raw RGB input replacing optical flow frames can be found on the right side of Table <ref type="table" target="#tab_1">1</ref>. The results are weak and close to random guessing, demonstrating the critical role played by optical flow in detecting the subtle NNS signal. This can also be seen clearly in the sample optical flow frames visualized in Supp. Fig. <ref type="figure" target="#fig_2">S3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">NNS Segmentation Results</head><p>Adopting the best ResNet18-LSTM recognition model, we tested the three configurations of the derived segmentation method on the 60 s mixed activity clips, under the same leave-one-out cross-validation paradigm on the six infants. In addition to the default classifier threshold of 0.5 used by our recognition model, we tested a 0.9 threshold to coax higher precision, as motivated in Sect. 1. We use the standard evaluation metrics of average precision AP t and average recall AR t based on hits and misses defined by an intersection-over-union (IoU) with threshold t, across common thresholds t ∈ {0.1, 0.3, 0.5}<ref type="foot" target="#foot_3">4</ref> . Averages are taken with subjects given equal weight, and results tabulated in Table <ref type="table" target="#tab_2">2</ref>.</p><p>The metrics reveal strong performance from all methods and both confidence thresholds on both test sets. Generally, as expected, setting a higher confidence threshold or employing the more tempered tiled or smoothed aggregation methods favours precision, while lowering the confidence threshold or employing the more responsive sliding aggregation method favours recall. The results are excellent at the IoU threshold of 0.1 but degrade as the threshold is raised, suggesting that while these methods can readily perceive NNS behavior, they are still limited by the underlying ground truth annotator accuracy. The consistency of the performance of the model across both cross-validation testing in the clinical in-crib dataset and the independent testing on the NNS in-the-wild dataset suggests strong generalizability. Figure <ref type="figure" target="#fig_2">3</ref> visualizes predictions (and underlying confidence scores) of the sliding model configuration with a confidence threshold of 0.9, highlighting the excellent precision characteristics and illustrating the overall challenges of the detection problem.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present our novel computer vision method for the detection of non-nutritive sucking from videos, with a spatiotemporal action recognition model for classifying short video clips and a segmentation model for determining event timestamps in longer videos. Our work is grounded in our methodological collection and annotation of infant video data from varied settings. We use domain-specific techniques such as dense optical flow and infant state tracking to detect subtle sucking movements and ameliorate a relative scarcity of data. Future work could improve the robustness these methods in challenging examples of NNS activity, such as more ambiguous sucking or sucking while moving. This would require more precisely and reliably annotated data to train and evaluate, which in our experience could be difficult to obtain. An alternative approach would be to aim for more robust but less exacting, split-second results. Beyond improvements to the core NNS detection algorithms, algorithmic extraction of NNS signal characteristics, such as individual suck frequency, strength, duration, and temporal pattern, could further NNS research and one day aid in clinical care.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>Figure2illustrates our NNS action segmentation pipeline, which predicts NNS event timestamps in long-form videos of infants using pacifiers. The process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a): Proposed NNS segmentation pipeline: Aggregates local NNS action recognition results from sliding windows. (b): Proposed NNS action recognition pipeline: Utilizes dense optical flow on preprocessed frames, followed by a convolutional layer and a temporal layer to predict actions based on spatiotemporal information.</figDesc><graphic coords="4,74,97,54,23,302,17,152,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Segmentation predictions and ground truth for each 60 s mixed clip from the NNS clinical in-bed dataset, under the sliding window aggregation model configuration and with a confidence threshold of 0.9, boosting precision at the cost of recall.</figDesc><graphic coords="8,73,98,403,31,304,63,129,79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,76,47,54,14,299,95,172,57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Our primary dataset is the NNS clinical in-crib dataset, consisting of 183 h of baby monitor footage collected from 19 infants during overnight sleep sessions by our clinical neurodevelopment team, with Institutional Review Board (IRB #17-08-19) approval. Videos were shot in-crib with the baby monitors set up by caregivers, under low-light triggering the monochromatic infrared mode.</figDesc><table><row><cell>4 Experiments, Results, and Ablation Study</cell></row><row><cell>4.1 NNS Dataset Creation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Classification accuracy of our NNS action recognition model, under various convolutional and temporal configurations and two image modalities. We test on the NNS clinical in-crib data under subject-wise leave-one-out cross-validation, and on the NNS in-the-wild data directly, both with balanced classes. Strongest results in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Optical Flow</cell><cell></cell><cell></cell><cell>RGB</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="9">Convolutional 1-lr. CNN ResNet18 ResNet50 ResNet101 1-lr. CNN ResNet18 ResNet50 ResNet101</cell></row><row><cell>Dataset</cell><cell>Sequential</cell><cell cols="2"># Tr. Params. 333K</cell><cell>154K</cell><cell>614K</cell><cell>614K</cell><cell>333K</cell><cell>154K</cell><cell>614K</cell><cell>614K</cell></row><row><cell>Clinical</cell><cell cols="2">Transformer 393K</cell><cell>90.9</cell><cell>89.4</cell><cell>88.5</cell><cell>89.2</cell><cell>63.5</cell><cell>53.5</cell><cell>56.4</cell><cell>47.3</cell></row><row><cell></cell><cell>LSTM</cell><cell>418K</cell><cell>90.7</cell><cell>94.9</cell><cell>87.9</cell><cell>85.2</cell><cell>52.9</cell><cell>52.1</cell><cell>57.5</cell><cell>46.8</cell></row><row><cell></cell><cell>Bi-LSTM</cell><cell>535K</cell><cell>86.5</cell><cell>94.5</cell><cell>90.6</cell><cell>91.4</cell><cell>56.2</cell><cell>46.3</cell><cell>53.5</cell><cell>50.4</cell></row><row><cell cols="3">In-the-wild Transformer 393K</cell><cell>83.6</cell><cell>79.5</cell><cell>81.4</cell><cell>92.3</cell><cell>54.0</cell><cell>53.3</cell><cell>48.9</cell><cell>59.4</cell></row><row><cell></cell><cell>LSTM</cell><cell>418K</cell><cell>84.5</cell><cell>80.8</cell><cell>84.6</cell><cell>82.7</cell><cell>50.5</cell><cell>55.0</cell><cell>50.2</cell><cell>50.2</cell></row><row><cell></cell><cell>Bi-LSTM</cell><cell>535K</cell><cell>87.2</cell><cell>85.2</cell><cell>87.5</cell><cell>87.2</cell><cell>54.4</cell><cell>51.7</cell><cell>50.2</cell><cell>49.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Average precision APt and average recall ARt performance for various IoU thresholds t of our NNS segmentation model. We test three local classification aggregation methods and two different classifier confidence thresholds. Precision-recall pairs with the highest precision in each threshold configuration in bold.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Classifier Confidence Threshold = 0.9</cell><cell cols="3">Classifier Confidence Threshold = 0.5</cell></row><row><cell>Dataset</cell><cell>Method</cell><cell cols="6">AP0.1 AR0.1 AP0.3 AR0.3 AP0.5 AR0.5 AP0.1 AR0.1 AP0.3 AR0.3 AP0.5 AR0.5</cell></row><row><cell>Clinical</cell><cell>Tiled</cell><cell cols="4">94.0 84.9 80.6 74.9 56.4 53.0 86.9 91.0</cell><cell>74.0 72.9</cell><cell>42.6 44.8</cell></row><row><cell></cell><cell>Sliding</cell><cell>84.8 86.1</cell><cell>70.3 72.1</cell><cell>44.3 47.7</cell><cell>78.3 92.7</cell><cell>70.3 82.5</cell><cell>45.4 53.1</cell></row><row><cell></cell><cell cols="2">Smoothed 91.4 74.9</cell><cell>70.4 60.2</cell><cell>39.6 36.5</cell><cell cols="3">90.3 91.5 77.8 76.6 51.0 50.8</cell></row><row><cell cols="2">In-the-wild Tiled</cell><cell>90.7 81.5</cell><cell cols="2">76.3 68.3 56.7 49.7</cell><cell cols="3">90.8 84.2 80.5 74.4 67.9 63.5</cell></row><row><cell></cell><cell>Sliding</cell><cell>82.7 80.5</cell><cell>70.6 64.7</cell><cell cols="2">60.9 54.7 79.0 85.1</cell><cell>67.2 72.7</cell><cell>62.8 66.5</cell></row><row><cell></cell><cell cols="3">Smoothed 90.8 72.4 73.3 56.4</cell><cell>53.1 41.9</cell><cell>90.0 78.7</cell><cell>77.0 67.5</cell><cell>72.2 62.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Informal qualitative tests determined the superiority of dense optical flow over other implementations, such as Farneback<ref type="bibr" target="#b5">[6]</ref>, TV-L1<ref type="bibr" target="#b17">[17]</ref>, and RAFT<ref type="bibr" target="#b20">[20]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>See Supp. Fig.S1and Supp. Fig.S2for more on the creation of the NNS clinical in-crib dataset, and Supp. TableS1for full Cohen κ scores, biographical data, and NNS and pacifier event statistics.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Informal tests showed that the popular I3D<ref type="bibr" target="#b2">[3]</ref> and X3D<ref type="bibr" target="#b7">[7]</ref> models were not able to learn from the data, due possibly to the limited dataset size or subtleness of NNS movements. Formal quantitative tests will be included in forthcoming work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We follow definitions from<ref type="bibr" target="#b12">[12]</ref>, with tiebreaks decided by IoU instead of confidence.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>Our code and the manually annotated NNS in-the-wild dataset can be found at https://github.com/ostadabbas/NNS-Detection-and-Segmentation. Supported by <rs type="funder">MathWorks</rs> and <rs type="funder">NSF-CAREER</rs> Grant #<rs type="grantNumber">2143882</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HhvPT4U">
					<idno type="grant-number">2143882</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_55.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="2544" to="2550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Risk factors, protective factors, and current recommendations to reduce sudden infant death syndrome: a review</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Pediatr</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="180" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RetinaFace: single-shot multi-level face localisation in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.10352</idno>
		<title level="m">Temporal Action Segmentation: An Analysis of Modern Technique</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName><forename type="first">G</forename><surname>Farnebäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCIA 2003</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Bigun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Gustavsson</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">2749</biblScope>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-45103-X_50</idno>
		<ptr target="https://doi.org/10.1007/3-540-45103-X_50" />
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">X3D: expanding architectures for efficient video recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="200" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Infant contact-less nonnutritive sucking pattern quantification via facial gesture analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ostadabbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A multiresolution 3D morphable face model and fitting framework</title>
		<author>
			<persName><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Joint Conference on Computer Vision</title>
		<meeting>the 11th International Joint Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Imaging and Computer Graphics Theory and Applications. University of Surrey</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Development of Human Fetal Activity and its Relation to Postnatal Behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Humphrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Child Development and Behavior</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1970">1970</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The THUMOS challenge on action recognition for videos &quot;in the wild</title>
		<author>
			<persName><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Beyond pixels: exploring new representations and applications for motion analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">81</biblScope>
			<pubPlace>Vancouver</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Changes in non-nutritive suck between 3 and 12 months</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zimmerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Early Hum. Dev</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page">105141</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neonatal sucking behaviors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Medoff-Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image: J. Nurs. Sch</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="195" to="200" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Duality Based Algorithm for TV-L 1 -Optical-Flow Image Registration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beichel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-75759-7_62</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-75759-7_62" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2007</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Maeder</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer, Heidelberg</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4792</biblScope>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Infant pacifiers for reduction in risk of sudden infant death syndrome</title>
		<author>
			<persName><forename type="first">K</forename><surname>Psaila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pulbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Jeffery</surname></persName>
		</author>
		<idno>CD011147</idno>
	</analytic>
	<monogr>
		<title level="j">Cochrane Database of Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994">1994. 1994</date>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">RAFT: recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58536-5_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58536-5_24" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12347</biblScope>
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">InfAnFace: bridging the infant-adult domain gap in facial landmark estimation in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond short snippets: deep networks for video classification</title>
		<author>
			<persName><forename type="first">Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How might non nutritional sucking protect from sudden infant death syndrome</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zavala Abed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Abreu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Chediak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Hypotheses</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page">109868</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Changes in infant non-nutritive sucking throughout a suck sample at 3-months of age</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Carpenito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">235741</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Patterned auditory stimulation and suck dynamics in full-term infants</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Foran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Paediatr</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="727" to="732" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
