<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification</title>
				<funder ref="#_x2GSd7t">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_dQxbBGN">
					<orgName type="full">Open Fund Project of Guangdong Academy of Medical Sciences, China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zikang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Division of Life Sciences and Medicine</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei, Anhui</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Suzhou Institute for Advanced Research</orgName>
								<orgName type="institution" key="instit2">University of Science and Technology of China</orgName>
								<address>
									<postCode>215123</postCode>
									<settlement>Suzhou, Jiangsu</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Division of Life Sciences and Medicine</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei, Anhui</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Suzhou Institute for Advanced Research</orgName>
								<orgName type="institution" key="instit2">University of Science and Technology of China</orgName>
								<address>
									<postCode>215123</postCode>
									<settlement>Suzhou, Jiangsu</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quan</forename><surname>Quan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingsong</forename><surname>Yao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">Kevin</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Division of Life Sciences and Medicine</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei, Anhui</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Suzhou Institute for Advanced Research</orgName>
								<orgName type="institution" key="instit2">University of Science and Technology of China</orgName>
								<address>
									<postCode>215123</postCode>
									<settlement>Suzhou, Jiangsu</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FF39F267F668A3DA63B564DD59739ACA</idno>
					<idno type="DOI">10.1007/978-3-031-43895-029.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dermatological</term>
					<term>Fairness</term>
					<term>Batch Normalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning is becoming increasingly ubiquitous in medical research and applications while involving sensitive information and even critical diagnosis decisions. Researchers observe a significant performance disparity among subgroups with different demographic attributes, which is called model unfairness, and put lots of effort into carefully designing elegant architectures to address unfairness, which poses heavy training burden, brings poor generalization, and reveals the trade-off between model performance and fairness. To tackle these issues, we propose FairAdaBN by making batch normalization adaptive to sensitive attributes. This simple but effective design can be adapted to several classification backbones that are originally unaware of fairness. Additionally, we derive a novel loss function that restrains statistical parity between subgroups on mini-batches, encouraging the model to converge with considerable fairness. In order to evaluate the trade-off between model performance and fairness, we propose a new metric, named Fairness-Accuracy Trade-off Efficiency (FATE), to compute normalized fairness improvement over accuracy drop. Experiments on two dermatological datasets show that our proposed method outperforms other methods on fairness criteria and FATE. Our code is available at https://github.com/ XuZikang/FairAdaBN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The past years have witnessed a rapid growth of applying deep learning methods in medical imaging <ref type="bibr" target="#b30">[31]</ref>. As the performance improves continuously, researchers also find that deep learning models attempt to distinguish illness by using features that are related to a sample's demographic attributes, especially sensitive ones, such as skin tone or gender. The biased performance due to sensitive attributes within different subgroups is defined as unfairness <ref type="bibr" target="#b15">[16]</ref>. For example, Seyyed-Kalantari et. al. <ref type="bibr" target="#b20">[21]</ref> find that their models trained on chest X-Ray dataset show a significant disparity of True Positive Ratio (TPR) between male and female subgroups. Similar evaluations are done on brain MRI <ref type="bibr" target="#b16">[17]</ref>, dermatology <ref type="bibr" target="#b11">[12]</ref>, and mammography <ref type="bibr" target="#b14">[15]</ref>, which shows that unfairness issues exist extensively in medical applications. If the unfairness of deep learning models is not handled properly, healthcare disparity increases, and human fundamental rights are not guaranteed. Thus, there is a pressing need on investigating unfairness mitigation to eliminate critical biased inference in deep learning models.</p><p>There are two groups of methods to tackle unfairness. The first group proceeds implicitly with fairness through unawareness <ref type="bibr" target="#b6">[7]</ref> by leaving out sensitive attributes when training a single model or deriving invariant representation and ignoring them subjectively when making a decision. However, plenty of evaluations prove that this may lead to unfairness, due to the entangled correlation between sensitive attributes and other variables in the data, and statistical difference between features of different subgroups. The second group explicitly takes sensitive attributes into consideration when training models, for example, train independent models for unfairness mitigation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref> with no parameters shared between subgroups. However, this may result in degraded performance because the amount of data for model building is reduced (see Table <ref type="table" target="#tab_2">1</ref>).</p><p>It is natural to consider whether it is possible to inherit the advantages from both worlds, that is, learning a single model on the whole dataset yet still with explicit modeling of sensitive attributes. Therefore, we propose a framework with a powerful adapter termed Fair Adaptive Batch Normalization (FairAd-aBN). Specifically, FairAdaBN is designed to mitigate task disparity between subgroups captured by the neural network. It integrates the common information of different subgroups dynamically by sharing part of network parameters, and enables the differential expression of feature maps for different subgroups, by adding only a few parameters compared with backbones. Thanks to FairAdaBN, the proposed architecture can minimize statistical differences between subgroups and learn subgroup-specific features for unfairness mitigation, which improves model fairness and reserves model precision at the same time. In addition, to intensify the models' ability for balancing performance and fairness, a new loss function named Statistical Disparity Loss (L SD ), is introduced to optimize the statistical disparity in mini-batches and specify fairness constraints on network optimization. L SD also enhances information transmission between subgroups, which is rare for independent models. Finally, a perfect model should have both higher precision and fairness compared to current well-fitted models. However, most of the existing unfairness mitigation methods sacrifice overall performance for building a fairer model <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>. Therefore, following the idea of discovering the fairness-accuracy Pareto frontier <ref type="bibr" target="#b31">[32]</ref>, we propose a novel metric for evaluating the Fairness-Accuracy Trade-off Efficiency (FATE), urging researchers to pay attention to the performance and fairness simultaneously when building prediction models. We evaluate the proposed method based on its application to mitigating unfairness in dermatology diagnosis.</p><p>To sum up, our contributions are as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>According to <ref type="bibr" target="#b3">[4]</ref>, unfairness mitigation can be categorized into pre-processing, in-processing, and post-processing based on the instruction stage.</p><p>Pre-processing. Pre-processing methods focus on the quality of the training set, by organizing fair datasets via datasets combination <ref type="bibr" target="#b20">[21]</ref>, using generative adversarial networks <ref type="bibr" target="#b10">[11]</ref> or sketching model <ref type="bibr" target="#b26">[27]</ref> to generate extra images, or directly resampling the train set <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>. However, most methods in this category need huge effort due to the preciousness of medical data.</p><p>Post-processing. Although calibration has been widely used in unfairness mitigation in machine learning tasks, medical applications prefer to use pruning strategies. For example, Wu et. al <ref type="bibr" target="#b25">[26]</ref> mitigate unfairness by pruning a pretrained diagnosis model considering the difference of feature importance between subgroups. However, their method needs extra time except for training a precise classification model, while our FairAdaBN is a one-step method.</p><p>In-Processing. In-processing methods mainly consist of two folds. Some studies mitigate unfairness by directly adding fairness constraints to the cost functions <ref type="bibr" target="#b27">[28]</ref>, which often leads to overfitting. Another category of research mitigates unfairness by designing complex network architectures like adversarial network <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref> or representation learning <ref type="bibr" target="#b4">[5]</ref>. This family of methods relies heavily on the accuracy of sensitive attribute classifiers in the adversarial branch, leads to bigger models and cannot make full use of pre-trained weights. While our method does not increase the number of parameters significantly and can be applied to several common backbones for dermatology diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FairAdaBN</head><p>Problem Definition. We assume a medical imaging dataset D = {d 1 , d 2 , ..., d N } with N samples, the i-th sample d i consists of input image X i , sensitive attributes A i and classification ground truth label Y i . i.e.</p><formula xml:id="formula_0">d i = {X i , A i , Y i }.</formula><p>A is a binary variable (e.g., skin tone, gender), which splits the dataset into the unprivileged group, D A=0 , which has a lower average performance than the overall performance, and the privileged group, D A=1 , which has a higher average performance than the overall performance. Using accuracy as the performance metric for example, for a neural network f θ (•), our goal is to minimize the accuracy gap between D A=0 and D A=1 by finding a proper θ.  In this paper, we propose FairAdaBN, which replaces normalization layers in vanilla models with adaptive batch normalization layers, while sharing other layers between subgroups. The overview of our method is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><formula xml:id="formula_1">θ = arg min θ E {Xi,Yi}∼DA=1 I(f θ (X i ) = Y i ) -E {Xi,Yi}∼DA=0 I(f θ (X i ) = Y i )<label>(1)</label></formula><p>Batch normalization (BN) is a ubiquitous network layer that normalizes minibatch features using statistics <ref type="bibr" target="#b9">[10]</ref>. Let x ∈ R C×W ×H denote a given layer's output feature map, where C, W, H is the number of channels, width, and height of the feature map. The BN function is defined as:</p><formula xml:id="formula_2">BN(x) = γ • x -μ(x) σ(x) + β, (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where μ(x), σ(x) is the mean and standard deviation of the feature map computed in the mini-batch, β and γ denotes the learnable affine parameters. We implant the attribute awareness into BN, named FairAdaBN, by parallelizing multiple normalization blocks that are carefully designed for each subgroup. Specifically, for subgroup D A=a , its adaptive affine parameter γ a and β a are learnt by samples in D A=a . Thus, the adaptive BN function for subgroup D A=a is given by Eq. 3.</p><formula xml:id="formula_4">FairAdaBN a (x) = γ a • x -μ a (x) σ a (x) + β a , (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where a is the index of the sensitive attribute corresponding to the current input image, μ α , σ α are computed across subgroups independently.</p><p>The FairAdaBN acquires subgroup-specific knowledge by learning the affine parameter γ and β. Therefore, the feature maps of subgroups can be aligned and the unfair representation between privileged and unprivileged groups can be mitigated. By applying FairAdaBN on vanilla backbones, the network can learn subgroup-agnostic feature representations by the sharing parameters of convolution layers, and subgroup-specific feature representations using respective BN parameters, resulting in lower fairness criteria. The detailed structure of FairAdaBN is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we display the minimum unit of ResNet for simplification. Note that the normalization layer in the residual branch is not changed for faster convergence.</p><p>In this paper, we aim to retain skin lesion classification accuracy and improve model fairness simultaneously. The loss function consists of two parts: (i) the cross-entropy loss, L CE , constraining the prediction precision, and (2) the statistical disparity loss L SD as in Eq. 4, aiming to minimize the difference of prediction probability between subgroups and give extra limits on fairness.</p><formula xml:id="formula_6">L SD = Ncg y=1 E Xi∼DA=0 I(f θ (X i ) = y) -E Xi∼DA=1 I(f θ (X i ) = y) 2 , (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where N cg means the number of classification categories.</p><p>The overall loss function is given by the sum of the two parts, with a hyperparameter α to adjust the degree of constraint on fairness.</p><formula xml:id="formula_8">L = L CE + α • L SD .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Metrics</head><p>Lots of fairness criteria are proposed including statistical parity <ref type="bibr" target="#b6">[7]</ref>, equalized odds <ref type="bibr" target="#b8">[9]</ref>, equal opportunity <ref type="bibr" target="#b8">[9]</ref>, counterfactual fairness <ref type="bibr" target="#b12">[13]</ref>, etc. In this paper, we use equal opportunity and equalized odds as fairness criteria. For equal opportunity, we split it into EOpp0 and EOpp1 considering the ground truth label.</p><formula xml:id="formula_9">EOpp0 = |P ( Ŷ = 0 | Y = 0, A = 1) -P ( Ŷ = 0 | Y = 0, A = 0)| (5) EOpp1 = |P ( Ŷ = 1 | Y = 1, A = 1) -P ( Ŷ = 1 | Y = 1, A = 0)| (6) EOdd = |P ( Ŷ = 1 | Y = y, A = 1) -P ( Ŷ = 1 | Y = y, A = 0)|, y ∈ {0, 1}<label>(7)</label></formula><p>However, these metrics only evaluate the level of fairness while do not consider the trade-off between fairness and accuracy. Therefore, inspired by <ref type="bibr" target="#b5">[6]</ref>, we propose FATE, a metric that evaluates the balance between normalized improvement of fairness and normalized drop of accuracy. The formulas of FATE on different fairness criteria are shown below:</p><formula xml:id="formula_10">FATE F C = ACC m -ACC b ACC b -λ FC m -FC b FC b , (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where F C can be one of EOpp0, EOpp1, EOdd. ACC denotes accuracy. The subscript m and b denote the mitigation model and baseline model, respectively. λ is a weighting factor that adjusts the requirements for fairness pre-defined by the user considering the real application, here we define λ = 1.0 for simplification. A model obtains a higher FATE if it mitigates unfairness and maintains accuracy.</p><p>Note that FATE should be combined with utility metrics and fairness metrics, rather than independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset and Network Configuration</head><p>We use two well-known dermatology datasets to evaluate the proposed method. The Fitzpatrick-17k dataset <ref type="bibr" target="#b7">[8]</ref> contains 16,577 dermatology images in 9 diagnostic categories. The skin tone is labeled with Fitzpatrick's skin phenotype. In this paper, we regard Skin Type I to III as light, and Skin Type IV to VI as dark for simplicity, resulting in a ratio of dark : light ≈ 3 : 7. The ISIC 2019 dataset [1,2,24] contains 25,331 images among 9 different diagnostic categories. We use gender as the sensitive attribute, where female : male ≈ 4.5 : 5.5. Based on subgroup analysis, dark and female are treated as the privileged group, and light and male are treated as the unprivileged group. We randomly split the dataset into train, validation, and test with a ratio of 6:2:2. The models are trained for 600 epochs and the model with the highest validation accuracy is selected for testing. The images are resized or cropped to 128 × 128 for both datasets. Random flipping and random rotation are used for data augmentation. The experiments are carried out on 8 × NVIDIA 3090 GPUs, implemented on PyTorch, and are repeated 3 times. Pre-trained weights from ImageNet are used for all models. The networks are trained using AdamW optimizer with weight decay. The batch size and learning rate are set as 128 and 1e-4, respectively. The hyper-parameter α = 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We compare FairAdaBN with Vanilla (ResNet-152), Resampling <ref type="bibr" target="#b17">[18]</ref>, Ind (independently trained models for each subgroup) <ref type="bibr" target="#b17">[18]</ref>, GroupDRO <ref type="bibr" target="#b18">[19]</ref>, EnD <ref type="bibr" target="#b22">[23]</ref>, and CFair <ref type="bibr" target="#b28">[29]</ref>, which are commonly used for unfairness mitigation.</p><p>Results on Fitzpatrick-17k Dataset. Table <ref type="table" target="#tab_2">1</ref> shows the result of these seven methods on Fitzpatrick-17k dataset. Compared to the Vanilla model, Resampling has a comparable utility, but cannot improve fairness. FairAdaBN achieves the lowest unfairness with only a small drop in accuracy. Besides, FairAdaBN has the highest FATE on all fairness criteria. This is because Ind does not share common information between subgroups, and only part of the dataset is used for training. GroupDRO and EnD rely on the discrimination of features from different subgroups, which is indistinguishable for this task. CFair is more efficient on balanced datasets, while the ratio between light and dark is skewed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on ISIC 2019 Dataset.</head><p>Table <ref type="table" target="#tab_2">1</ref> shows the results on ISIC 2019 dataset. FairAdaBN is the fairest method among the seven methods. Resampling improves fairness sightly but does not outperform ours. GroupDRO mitigates EOpp0 while increasing unfairness on Eopp1 and Eodd. Ind and CFair cannot mitigate unfairness in ISIC 2019 dataset and EnD increases unfairness on EOpp0. The FATE Metric. Figure <ref type="figure" target="#fig_1">2</ref> shows the values of FATE. According to <ref type="bibr" target="#b2">[3]</ref>, the closer the curve is to the top left corner, the smaller the fairness-accuracy   <ref type="table" target="#tab_6">2</ref>, we find that by adding L SD , Eopp 0 decreases significantly, from 1.07 × 10 -2 to 0.48 × 10 -2 . Besides, although adding L SD on ResNet alone increases fairness criteria unexpectedly, fairness criteria decrease when using FairAdaBN and L SD simultaneously. The reason could be the potential connection between FairAd-aBN and L SD , due to the similar form dealing with subgroups.</p><p>Hyper-parameter α. Our experiments show that α = 1.0 has the best fairness scores and FATE compared to α = 0.1 and α = 2.0. Therefore we select α = 1.0 as our final setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose FairAdaBN, a simple but effective framework for unfairness mitigation in dermatological disease classification. Extensive experiments illustrate that the proposed framework can mitigate unfairness compared to models without fair constraints, and has a higher fairness-accuracy trade-off efficiency compared with other unfairness mitigation methods. By plugging FairAdaBN into several backbones, its generalization ability is proved. However, the current study only evaluates the effectiveness of FairAdaBN on dermatology datasets, and its generalization ability on other datasets (chest X-Ray, brain MRI) or tasks (segmentation, detection), where unfairness issues also exist, needs to be evaluated in the future. We also plan to explore the unfairness mitigation effectiveness for other universal models <ref type="bibr" target="#b30">[31]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of Our Method. (a) Compared to ResNet (Top), ResNet w/ AdaBN (Bottom) has a smaller performance disparity (Δp) between light samples and dark samples. (b) Details of FairAdaBN (use Residual block as an example).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. FATE on different fairness criteria. The data point of baseline (fairness, accuracy) splits the space into four parts: MF: Fairer; MA: More Accurate; LF: Less Fair; LA: Less Accurate. Points on the left of the line have positive FATE, while points on the right of the line have negative FATE.</figDesc><graphic coords="8,56,13,54,02,322,18,119,11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Result on Fitzpatrick-17k and ISIC 2019 Dataset (Mean Std × 10 -2 ). Best and Second-best are highlighted.</figDesc><table><row><cell cols="2">Fitzpartrick-17k Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="2">Accuracy↑ Precision↑ Recall↑</cell><cell>F1↑</cell><cell>EOpp0↓ EOpp1↓ Eodd↓</cell><cell>E0 ↑</cell><cell>E1 ↑</cell><cell>E2 ↑</cell></row><row><cell>Vanilla</cell><cell>87.53 0.14</cell><cell cols="4">79.60 0.33 80.22 0.19 78.41 0.15 1.00 0.30 10.40 1.43 10.54 0.98 /</cell><cell>/</cell><cell>/</cell></row><row><cell cols="2">Resampling [18] † 87.73 0.27</cell><cell cols="6">79.21 0.40 80.01 0.35 78.27 0.42 1.11 0.26 10.43 1.91 10.78 2.06 -10.86 -0.03 -2.05</cell></row><row><cell>Ind [18] †</cell><cell>86.33 0.12</cell><cell cols="4">76.11 0.38 77.48 0.18 75.20 0.09 0.78 0.33 10.13 0.51 9.72 0.94 20.63</cell><cell>1.23</cell><cell>6.41</cell></row><row><cell cols="2">GroupDRO [19] † 86.62 0.19</cell><cell cols="4">77.21 0.62 78.29 0.52 76.56 0.56 0.94 0.34 8.04 0.90 8.23 1.25 5.07</cell><cell>21.66</cell><cell>20.91</cell></row><row><cell>EnD [23] †</cell><cell>86.80 0.52</cell><cell cols="5">77.32 0.60 78.58 0.53 76.90 0.66 1.22 0.31 9.01 1.60 9.20 1.59 -22.83 12.53</cell><cell>11.88</cell></row><row><cell>CFair [29] †</cell><cell>87.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>91 0.35 78</head><label></label><figDesc>.62 0.49 79.73 0.37 78.12 0.38 0.93 0.28 9.83 1.65 10.17 1.57 10.03</figDesc><table><row><cell></cell><cell></cell><cell>12.15</cell><cell>10.09</cell></row><row><cell>FairAdaBN</cell><cell>84.72 0.40</cell><cell>74.43 0.22 75.74 0.33 73.31 0.48 0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>.48 0.09 7.67 3.86 7.73 3.95 48.79 23.04 23.45 ISIC 2019 Dataset</head><label></label><figDesc></figDesc><table><row><cell>Method</cell><cell cols="2">Accuracy↑ Precision↑ Recall↑</cell><cell>F1↑</cell><cell>EOpp0↓ EOpp1↓ Eodd↓</cell><cell>E0 ↑</cell><cell>E1 ↑</cell><cell>E2 ↑</cell></row><row><cell>Vanilla</cell><cell>92.52 0.12</cell><cell cols="4">82.64 0.31 82.94 0.36 82.60 0.32 0.85 0.12 6.12 1.83 6.02 1.66 /</cell><cell>/</cell><cell>/</cell></row><row><cell cols="2">Resampling [18] † 92</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>.81 0.28 83.15 0.50 83.42 0.51 83.12 0.52</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>0.86 0.15 5.65 2.83 5.76 2.78 -0.80</cell><cell cols="2">-2.48 -5.49</cell></row><row><cell>Ind [18] †</cell><cell>92.43 0.11</cell><cell>82.16 0.15 82.46 0.12 82.11 0.08 0.85 0.11 7.04 0.96 7.37 0.77 -0.10</cell><cell cols="2">-15.13 -22.52</cell></row><row><cell cols="2">GroupDRO [19] † 91.86 0.22</cell><cell>81.30 0.52 81.44 0.47 81.17 0.50 0.82 0.12 6.78 3.20 6.62 3.21 2.41</cell><cell cols="2">-22.99 -22.01</cell></row><row><cell>EnD [23] †</cell><cell>92.13 0.08</cell><cell cols="2">81.42 0.48 81.64 0.35 81.36 0.38 0.98 0.09 5.18 0.99 5.10 1.06 -15.72 14.94</cell><cell>14.86</cell></row><row><cell>CFair [29] †</cell><cell>87.39 0.77</cell><cell cols="3">72.39 2.67 72.60 2.22 71.28 2.12 2.83 1.09 9.21 3.53 10.80 4.15 -238.49 -56.03 -84.95</cell></row><row><cell>FairAdaBN</cell><cell>89.11 0.09</cell><cell>74.24 0.13 74.79 0.18 74.18 0.14 0.69 0.07 4.85 2.50 4.76 2.73 15.14</cell><cell cols="2">17.07 17.24</cell></row><row><cell cols="3">* E0, E1, E2 denotes FATEEOpp0, FATEEOpp1, FATE EOdd , respectively.</cell><cell></cell></row><row><cell cols="3">† Private implementation.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 .</head><label>2</label><figDesc>Ablation Study (Mean Std × 10 -2 ). Best in each group are highlighted.</figDesc><table><row><cell>Method</cell><cell>Accuracy↑ Precision↑ Recall↑</cell><cell>F1↑</cell><cell>EOpp0↓ EOpp1↓ Eodd↓</cell><cell>E0 ↑</cell><cell>E1 ↑</cell><cell>E2 ↑</cell></row><row><cell>VGG</cell><cell>88.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>11 0.51 79.18 0.56 80.07 0.49 78.55 0.56</head><label></label><figDesc>1.42 0.25 10.64 2.15 11.78 2.34 / / / VGG + FairAdaBN 83.55 0.24 69.73 0.83 72.09 0.41 70.15 0.69 1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>09 0.04 10.58 1.80 10.48 1.97</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>18.06</cell><cell>-4.61 5.86</cell></row><row><cell>DenseNet</cell><cell>87.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>32 0.06 78.12 0.52 79.08 0.38 77.37 0.24 1.18 0.37 10</head><label></label><figDesc>.961.34 11.47 1.16 / / / DenseNet + FairAdaBN 80.40 0.23 65.32 0.57 69.42 0.40 65.25 0.60 1.43 0.79 7.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>70 1.06 8.30 1.58</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>-29.11 21.82</cell><cell>19.71</cell></row><row><cell>ResNet</cell><cell>87.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>53 0.14 79.60 0.33 80.22 0.19 78.41 0.15</head><label></label><figDesc>1.00 0.30 10.40 1.43 10.54 0.98 /</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>/</cell><cell>/</cell></row><row><cell>Ours w/o LSD</cell><cell>87.18 0.50</cell><cell>78.50 0.75 79.24 0.68 77.40 0.71 1.07 0.16 9.33 0.23</cell><cell>9.91 0.29</cell><cell cols="2">-7.87 9.88</cell><cell>5.55</cell></row><row><cell>Ours w/o FairAdaBN</cell><cell>85.02 0.03</cell><cell cols="3">73.76 0.11 75.67 0.05 73.63 0.16 1.39 0.45 15.30 1.91 15.05 1.37 42.15</cell><cell cols="2">-49.94 -45.62</cell></row><row><cell>Ours (α = 0.1)</cell><cell>84.82 0.79</cell><cell cols="5">73.44 1.11 75.15 0.98 73.17 0.95 1.26 0.18 13.39 2.98 12.76 3.28 -29.10 -31.85 -24.16</cell></row><row><cell>Ours (α = 1.0)</cell><cell>84.72 0.40</cell><cell>74.43 0.22 75.74 0.33 73.31 0.48 0</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>.48 0.09 7.67 3.86 7.73 3.95 48.79 23.04 23.45</head><label></label><figDesc>Compared with other methods, FairAdaBN needs to use sensitive attributes in the test stage, which is unnecessary for EnD and CFair. Although this might be easy to acquire in real applications, improvements could be done to solve this problem.</figDesc><table><row><cell>Ours (α = 2.0)</cell><cell>84.57 0.38</cell><cell>74.26 0.22 75.40 0.11 72.91 0.87 1.10 0.60 8.53 2.79</cell><cell>8.40 2.75</cell><cell>-13.38 14.60</cell><cell>16.92</cell></row><row><cell cols="6">trade-off it has. The figure demonstrates that FATE has the same trend as this</cell></row><row><cell cols="6">argument. We prefer an algorithm that obtains a higher FATE since a higher</cell></row><row><cell cols="6">FATE denotes higher unfairness mitigation and a low drop in utility, and a neg-</cell></row><row><cell cols="6">ative FATE denotes that the mitigation model cannot decrease unfairness while</cell></row><row><cell cols="3">reserving enough accuracy (not beneficial).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Limitation. 4.4 Ablation Study</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Different Backbones. Firstly, we test FairAdaBN's compatibility on different backbones, by applying FairAdaBN on VGG-19-BN and DenseNet-121. Note that the first and last BN in DenseNet are not changed. The result is shown in</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 2 .</head><label>2</label><figDesc>The experiments are carried out on Fitzpatrick-17k dataset. The result shows that our FairAdaBN is also effective on these two backbones, except Eopp 0 when using DenseNet-121, showing well model compatibility. However, we also observe a larger drop in model precision compared with the baseline, which needs to be taken into consideration in future work. We train ResNet by only replacing BNs with FairAd-aBNs (the second row of the last part), and ResNet adding L SD on the total loss (the third row of the last part). The effectiveness of AdaBN is illustrated by comparing the first and second rows of the last part in Table2. By replacing BNs with FairAdaBN, ResNet can normalize subgroup feature maps using specific affine parameters, which reduce Eopp 1 and Eodd by 1.07 × 10 -2 and 0.63 × 10 -2 , respectively. Comparing the second and fourth row of the last part in Table</figDesc><table><row><cell>Different Loss Terms.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. Supported by <rs type="funder">Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62271465</rs> and <rs type="funder">Open Fund Project of Guangdong Academy of Medical Sciences, China</rs> (No. <rs type="grantNumber">YKY-KF202206</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_x2GSd7t">
					<idno type="grant-number">62271465</idno>
				</org>
				<org type="funding" xml:id="_dQxbBGN">
					<idno type="grant-number">YKY-KF202206</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Skin lesion analysis toward melanoma detection: a challenge at the 2017 international symposium on biomedical imaging (ISBI), hosted by the international skin imaging collaboration (ISIC)</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Combalia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02288</idno>
		<title level="m">BCN20000: Dermoscopic lesions in the wild</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Flexibly fair representation learning by disentanglement</title>
		<author>
			<persName><forename type="first">E</forename><surname>Creager</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1436" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How do the existing fairness metrics and unfairness mitigation algorithms contribute to ethical learning analytics?</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">B</forename><surname>Deho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le Duy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br. J. Educ. Technol</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="822" to="843" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On fairness of medical image classification with multiple sensitive attributes via learning orthogonal representations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.01481</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PASS: protected attribute suppression system for mitigating bias in face recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15087" to="15096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Innovations in Theoretical Computer Science Conference</title>
		<meeting>the 3rd Innovations in Theoretical Computer Science Conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards transparency in dermatology image datasets with skin tone annotations by experts, crowds, and an algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Daneshjou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Badri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koochek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.02942</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">AI fairness via domain adaptation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burlina</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01109</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fairness of classifiers across skin tones in dermatology</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kinyanjui</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-231" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="320" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Estimating and improving fairness with adversarial learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.04243</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lemay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hoebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02716</idno>
		<title level="m">Evaluating subgroup disparity using epistemic uncertainty in mammography</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Translation tutorial: 21 fairness definitions and their politics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Fairness Accountability Transp</title>
		<meeting>Conf. Fairness Accountability Transp<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1170</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Feature robustness and sex differences in medical imaging: a case study in MRI-based Alzheimer&apos;s disease detection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D C</forename><surname>Zemsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Henriksen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">E W</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ganz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.01737</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fairness in cardiac MR image analysis: an investigation of bias due to data imbalance in deep learning based segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Puyol-Antón</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-439" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="413" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distributionally robust neural networks for group shifts: on the importance of regularization for worst-case generalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08731</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the fairness of privacypreserving representations in medical applications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Sarhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-60548-3_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-60548-314" />
	</analytic>
	<monogr>
		<title level="m">DART/DCL -2020</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12444</biblScope>
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CheXclusion: fairness gaps in deep chest X-ray classifiers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seyyed-Kalantari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BIOCOMPUTING 2021: Proceedings of the Pacific Symposium</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="232" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chasing your long tails: differentially private prediction in health care settings</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Suriyakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="723" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">EnD: entangling and disentangling deep representations for bias correction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tartaglione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Barbano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grangetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13508" to="13517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mitigating bias in face recognition using skewness-aware reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9322" to="9331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">FairPrune: achieving fairness through pruning for dermatological disease diagnosis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02110</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving fairness in image classification via sketching</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.00168</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving the fairness of chest x-ray classifiers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dullerud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oakden-Rayner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Health, Inference, and Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="204" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07162</idno>
		<title level="m">Conditional learning of fair representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training confounder-free deep learning models for medical applications</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Pohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A review of deep learning in medical imaging: imaging traits, technology trends, case studies with progress highlights, and future promises</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="820" to="838" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Leveling down in computer vision: pareto inefficiencies in fair deep classifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zietlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10410" to="10421" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
