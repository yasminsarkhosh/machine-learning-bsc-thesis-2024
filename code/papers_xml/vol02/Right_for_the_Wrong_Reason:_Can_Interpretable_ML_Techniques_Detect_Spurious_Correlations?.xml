<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?</title>
				<funder>
					<orgName type="full">Hertie Foundation</orgName>
				</funder>
				<funder ref="#_NBQJnKU">
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</orgName>
				</funder>
				<funder ref="#_Ngd5pcU">
					<orgName type="full">Carl Zeiss Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">International Max Planck Research School for Intelligent Systems</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Susu</forename><surname>Sun</surname></persName>
							<email>susu.sun@uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Cluster of Excellence -ML for Science</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lisa</forename><forename type="middle">M</forename><surname>Koch</surname></persName>
							<email>lisa.koch@uni-tuebingen.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Hertie Institute for AI in Brain Health</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute of Ophthalmic Research</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><forename type="middle">F</forename><surname>Baumgartner</surname></persName>
							<email>christian.baumgartner@uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Cluster of Excellence -ML for Science</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EE313B5BA8286234F34E5024DF006361</idno>
					<idno type="DOI">10.1007/978-3-031-43895-040.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Interpretable machine learning • Confounder detection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique's ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net provide the best performance and can be used to reliably identify faulty model behavior.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Black-box neural network classifiers offer enormous potential for computer-aided diagnosis and prediction in medical imaging applications but, unfortunately, they also have a strong tendency to learn spurious correlations in the data <ref type="bibr" target="#b10">[11]</ref>. For the development and safe deployment of machine learning (ML) systems it is essential to understand what information the classifiers are basing their decisions on, such that reliance on spurious correlations may be identified.</p><p>Spurious correlations arise when the training data are confounded by additional variables that are unrelated to the diagnostic information we want to predict. For instance, older patients in our training data may be more likely to present with a disease than younger patients. A classifier trained on this data may inadvertently learn to base its decision on image features related to age rather than pathology. Crucially, such faulty behavior cannot be identified using classification performance metrics such as area under the ROC curve (AUC) if the testing data contains the same confounding information as the training data, since the classifier predicts the right thing, but for the wrong reason. If undetected, however, such spurious correlations may lead to serious safety implications after deployment. Fig. <ref type="figure">1</ref>. Overview. We train classifiers on datasets with three types of artificially added confounders highlighted by arrows. We then evaluate the ability of explanation techniques to correctly identify reliance on these confounders (shown Attri-Net <ref type="bibr" target="#b23">[24]</ref>).</p><p>Interpretable ML approaches may be used as a powerful tool to detect spurious correlations during development or after deployment of an ML system. Currently, the most widely used explanation modality are visual explanations, which highlight the pixels in the input image that are responsible for a particular decision. Common strategies include methods which leverage the gradient of the prediction with respect to the input image <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>, explain the predictions by counterfactually generating an image of the opposite class <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24]</ref>, interpret the feature map of the last layer before the classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27]</ref>, or methods that build a local approximation of the decision function such as LIME <ref type="bibr" target="#b15">[16]</ref>, or SHAP <ref type="bibr" target="#b14">[15]</ref>.</p><p>The majority of visual explanation methods are post-hoc techniques, meaning a heuristic is applied to any trained model (e.g. a ResNet <ref type="bibr" target="#b12">[13]</ref>) to approximately understand the decision mechanism for a given data point. However, post-hoc techniques are by definition only approximations and many techniques have been found to suffer from serious limitations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">26]</ref>. Inherently interpretable techniques on the other hand build custom architectures that are designed to directly reveal the reasoning of the classifier to the user without the need for approximations. This class of methods does not suffer from the same limitations as post-hoc methods, and it has been argued that inherently interpretable approaches should be preferred in high-stakes applications such as medical image analysis <ref type="bibr" target="#b16">[17]</ref>. For instance, if a classifier bases its decision on a spurious signal, an inherently interpretable classifier should by definition reveal this relationship.</p><p>Inherently interpretable visual explanation approaches are much less widely explored than post-hoc techniques, but there has recently been an increased interest in the topic. Two recently proposed methods in this category are the attribution network (Attri-Net) <ref type="bibr" target="#b23">[24]</ref>, and convolutional dynamic alignment networks (CoDA-Nets) <ref type="bibr" target="#b5">[6]</ref>. Attri-Net first produces human-interpretable feature attribution maps for each disease category using a GAN-based counterfactual generator <ref type="bibr" target="#b23">[24]</ref>. Then makes the final prediction with simple logistic regression classifiers based on those feature attribution maps. CoDA-Nets express neural networks as input dependent linear transformation <ref type="bibr" target="#b5">[6]</ref>. Both approaches produce explanations on the pixel level of the input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work on Comparing Explanation Techniques.</head><p>A number of works have studied the quality of post-hoc explanation techniques. The vast majority of work focuses exclusively on gradient-based approaches (e.g. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>).</p><p>In their landmark study, Adebayo et al. <ref type="bibr" target="#b0">[1]</ref> find that commonly used gradientbased explanation techniques do not pass some basic sanity checks. Arun et al. <ref type="bibr" target="#b4">[5]</ref> extends this work to weakly supervised localisation in one of the few papers in this domain focusing on medical data. Both papers, however, do not consider other types of commonly used approaches such as counterfactual methods, or local function approximations such as LIME or SHAP.</p><p>A small number of works specifically investigate explanations' sensitivity to spurious correlations. In closely related work to ours, Adebayo et al. <ref type="bibr" target="#b2">[3]</ref> explore a large library of post-hoc explanation techniques including LIME and SHAP, for detecting spurious image backgrounds in a bird versus dog classification task and find that many techniques are in fact able to detect the spurious background. In subsequent work, the same authors explore the usefulness of four post-hoc gradient-based explanation methods for identifying spurious correlations in hand and knee radiographs <ref type="bibr" target="#b1">[2]</ref> and come to the conclusion that the examined methods are ineffective at identifying spurious correlations. We note that prior work is inconclusive on the usefulness of explanation techniques for identifying spurious correlations. In particular, in the medical context it is still unclear if commonly used explanation techniques are suitable for the detection of spurious correlations. Moreover, there is, to our knowledge, no evidence for the supposition that inherently interpretable techniques are better suited for this task.</p><p>Contributions. We present a rigorous evaluation of post-hoc explanations and inherently interpretable techniques for the identification of spurious correlations in a medical imaging task. Specifically, we focus on the task of diagnosing cardiomegaly from chest x-ray data with three types of synthetically generated spurious correlations (see Fig. <ref type="figure">1</ref>). To identify whether an explanation correctly identifies a model's reliance on spurious correlations, we propose two quantitative metrics which are highly reflective of our qualitative findings. In contrast to the majority of prior work we focus on a wide range of different explanation approaches including counterfactual techniques and local function approximations, as well as post-hoc techniques and an inherently interpretable approach. Our analysis yields actionable insights which will be useful for a wide audience of ML practitioners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Framework for Evaluating Explanation Techniques</head><p>In the following, we introduce our evaluation strategy and proposed evaluation metrics, the studied confounders, as well as the evaluated explanation techniques. The strategy and evaluation metrics are generic and can also be applied to different problems. The confounders are engineered to correspond to realistic image artifacts that can appear in chest x-ray imaging<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Evaluation Strategy</head><p>We assume a setting in which the development data for a binary neural network based classifier contains an unknown spurious correlation with the target label. To quantitatively study this setting, we create training data with artificial spurious correlations by adding a confounding effect (e.g. a hospital tag) in a percentage of the cases with a positive label, where we vary the percentage p ∈ {0, 20, 50, 80, 100}. E.g., for p = 100% all of the positive images in the training set will have an artificial confounder, and for p = 0% there is no spurious signal. With increasing p the reliance on a spurious signal becomes more likely. The images with a negative label remain untouched.</p><p>In the evaluation, we consider a scenario in which the test data contain the same confounder type with the same proportion p used in the respective trainings. In this case, we can not tell if a classifier relies on the confounded features from classification performance. Our aim, therefore, is to investigate whether explanation techniques can identify that the classifier predicts the right thing for the wrong reason.</p><p>We perform all experiments on chest x-ray images from the widely used CheXpert dataset <ref type="bibr" target="#b13">[14]</ref>, where we focus on the binary classification task on disease cardiomegaly. We divided our dataset into a training (80%), validation (10%) test (10%) set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Studied Confounders</head><p>We study three types of confounders inspired by real-world artefacts. Firstly, we investigate a hospital tag placed in the lower left corner of the image (see Fig. <ref type="figure">1a</ref>). Secondly, we add vertical lines of hyperintense signal that can be caused by foreign materials on the light path assembly (see Fig. <ref type="figure">1b</ref>). Lastly, we consider an oblique occlusion of the image in the lower part of the image, which is an artefact that we observed for many images in the CheXpert dataset (see Fig. <ref type="figure">1c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Metrics for Measuring Confounder Detection</head><p>We propose two novel metrics which reflect an explanation's ability to correctly identify spurious correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confounder Sensitivity (CS).</head><p>Firstly, the explanations should be able to correctly attribute the confounder if classifier bases its decision on it. We assess this property by summing the number of true positive attributions divided by the total number of confounded pixels for each test image. We consider a pixel a true positive if it is part of the pixels affected by the confounder and in the top 10% attributed pixels according to a visual explanation. Thus the maximum sensitivity of 1 is obtained if all confounded pixels are in the top 10% of the attributions. Note that we do not penalise attributions outside of the confounding label as those can still also be correct. To guarantee that we only evaluate on samples for which the prediction is actually influenced by the confounder, we only include images for which the prediction with and without the confounding label is of the opposite class. To reduce computation times we use a maximum of 100 samples for each evaluation. An optimal explanation methods should obtain a CS score of 0 if the data contains p = 0% confounded data points, since in that case the spurious signal should not be attributed. For increasing p the confounder sensitivity should increase, i.e. the explanation should reflect the classifiers increasing reliance on the confounder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity to Prediction Changes via Explanation NCC.</head><p>Secondly, the explanations should not be invariant to changes in classifier prediction. That is, if the classifier's prediction for a specific image changes when adding or removing a confounder, then the explanations should also be different. We measure this property using the average normalised cross correlation (NCC) between explanations of test images when confounders were either present or absent.Again, we only evaluate on images for which the prediction changes when adding the confounder as in these cases, we know the classifier is relying on confounders, and we evaluate a maximum of 100 samples. An optimal explanation method should obtain a high NCC score if the training data contains p = 0% confounded data points, since in that case the explanation with and without the confounder should be similar. For increasing p the NCC score should decrease to reflect the classifiers increasing reliance on the confounder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Evaluated Explanation Methods</head><p>We evaluated five post-hoc techniques with representative examples from the approaches mentioned in the introduction: Guided Backpropgation <ref type="bibr" target="#b22">[23]</ref> and Grad-CAM <ref type="bibr" target="#b18">[19]</ref> (gradient-based), Gifsplanation (counterfactual), and LIME <ref type="bibr" target="#b15">[16]</ref> and SHAP partition explainer <ref type="bibr" target="#b14">[15]</ref> (local linear approximations). All post-hoc techniques were applied to a standard black-box ResNet50 model. We furthermore investigated the interpretable visual explanation method Attri-Net <ref type="bibr" target="#b23">[24]</ref>. We used the default parameters for all methods. We found CoDA-Nets <ref type="bibr" target="#b5">[6]</ref> required lengthy hyperparameter tuning for each type of experiment, and decided to exclude it in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>We first established the classifiers' performance in the presence of confounders, then compared all techniques in their ability to identify such confounders.</p><p>Classification Performance. Both investigated classifiers, the ResNet50 and the inherently interpretable Attri-Net, performed similarly in terms of classification AUC (first row of Fig. <ref type="figure" target="#fig_0">2</ref>). For all three confounders, classification AUC consistently increased with increasing contamination p of the training dataset. This indicated that the classifiers increasingly relied on the spurious signal. For p = 100% contamination, where the confounder was present on all positive training examples, both classifiers reached almost a perfect classification AUC of 1.</p><p>Explanations. We analysed the explanations' ability to identify confounders by reporting confounder sensitivity (CS, middle row in Fig. <ref type="figure" target="#fig_0">2</ref>) and explanation NCC (bottom row in Fig. <ref type="figure" target="#fig_0">2</ref>). Out of the investigated methods Attri-Net and SHAP were closest to the ideal behaviour of high confounder sensitivity and low explanation NCC for p &gt; 0%. We found that SHAP performed extremely well in detecting tag confounders, but struggled with hyperintensities confounders. This can be explained by the fact that the tag confounder is relatively small and thus is more likely to be completely covered by the superpixels in SHAP. Overall, the inherently interpretable Attri-Net technique achieved the best balance. In agreement with related literature we found that gradient-based explanation methods performed poorly. In particular, Guided Backpropagation displayed similar CSscores no matter if the classifier relies on a spurious signal (p &gt; 0%) or not (p = 0%). Note that some results for p = 100% were missing because no data points fulfilled the criterion of the prediction being flipped with and without the confounders.</p><p>Figures <ref type="figure" target="#fig_2">3, 4</ref> &amp;<ref type="figure"></ref> 5 contain examples explanations for the hyperintensity, tag, and edge confounder, respectively. Our qualitative analysis of the results confirms the quantitative findings, with SHAP and Attri-Net providing the most intuitive explanations. In particular, in the challenging hyperintensities scenario (see Fig. <ref type="figure" target="#fig_1">3</ref>) AttriNet was the only method able to highlight the confounders in a human-interpretable fashion. We note that in all examples when a confounder was present, SHAP tended to highlight only the confounder, while Attri-Net also highlighted features related to Cardiomegaly. This may reflect the different decision mechanisms of the ResNet50 and the Attri-Net. The explanation techniques' ability to identify confounders in terms of confounder sensitivity (middle row) and explanation NCC (bottom row, lower is better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>In this paper, we proposed an evaluation strategy to assess the ability of visual explanations to correctly identify a classifier's reliance on a spurious signal. We specifically focused on the scenario where the classifier is predicting the right thing, but for the wrong reason, which is highly significant for the safe development of ML-basd diagnosis and prediction systems. Using this strategy, we assessed the performance of five post-hoc explanation techniques and one inherently interpretable technique with three realistic confounding signals. We found that the inherently interpretable Attri-Net technique, as well as the post-hoc SHAP technique performed the best, with Attri-Net yielding the most balanced performance. Both techniques are suitable for finding false reliance on a spurious signals. We also observed that the variation in the explanations' sparsity makes them perform differently in detecting spurious signals of different sizes and   shapes. In agreement with prior work, we found that gradient based techniques performed less robustly in our experiments.</p><p>From our experiments we draw two main conclusions. Firstly, practitioners looking to check for spurious correlations in a trained black-box model such as a ResNet should give preference to SHAP which provided the best performance out of the post-hoc techniques in our experiments. Secondly, an inherently interpretable technique, namely Attri-Net, performed the best in our experiments providing evidence to the supposition by Rudin et al. <ref type="bibr" target="#b16">[17]</ref> that inherently interpretable techniques may provide a fruitful avenue for future work.</p><p>A major limitation of our study is the limited number of techniques we examined. Thus a primary focus of future work will be to scale our experiments to a wider range of techniques. Future work will also focus on human-in-the-loop experiments, as we believe, this will be the ultimate assessment of the usefulness of different explanation techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (top row) Classification AUC of Attri-Net and Resnet50 on images containing hospital tags (left), hyperintensities (middle) or obstruction confounders (right column). The classifiers were trained with a varying proportion of confounders present in the positive examples in the training set (shown on the x-axes). (bottom rows)The explanation techniques' ability to identify confounders in terms of confounder sensitivity (middle row) and explanation NCC (bottom row, lower is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Explanations for one example image with and without hyperintensities confounders. We show results for models trained on 20% (top rows) and 80% (bottom rows) confounded data points, respectively.</figDesc><graphic coords="8,60,63,69,26,318,82,174,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Explanations for one example image with (top) and without (bottom) a tag confounder for models trained on 50% confounded data points.</figDesc><graphic coords="8,59,55,322,82,319,60,83,83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Explanation for one example image with (top) and without (bottom) an obstruction confounder for models trained on 50% confounded data points.</figDesc><graphic coords="8,58,80,474,05,320,62,83,83" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our code can be found under https://github.com/ss-sun/right-for-the-wrongreason.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. Funded by the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</rs>) under Germany's Excellence Strategy -EXC number 2064/1 -Project number <rs type="grantNumber">390727645</rs>. The authors acknowledge support of the <rs type="funder">Carl Zeiss Foundation</rs> in the project "<rs type="projectName">Certification and Foundations of Safe Machine Learning Systems in Healthcare</rs>" and the <rs type="funder">Hertie Foundation</rs>. The authors thank the <rs type="funder">International Max Planck Research School for Intelligent Systems</rs> (<rs type="affiliation">IMPRS-IS</rs>) for supporting <rs type="person">Susu Sun</rs>, <rs type="person">Lisa M. Koch</rs>, and <rs type="person">Christian F. Baumgartner</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NBQJnKU">
					<idno type="grant-number">390727645</idno>
				</org>
				<org type="funded-project" xml:id="_Ngd5pcU">
					<orgName type="project" subtype="full">Certification and Foundations of Safe Machine Learning Systems in Healthcare</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sanity checks for saliency maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Post hoc explanations may be ineffective for detecting unknown spurious correlation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Liccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.05429</idno>
		<title level="m">Debugging tests for model explanations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08049</idno>
		<title level="m">On the robustness of interpretability methods</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Assessing the trustworthiness of saliency maps for localizing abnormalities in medical imaging</title>
		<author>
			<persName><forename type="first">N</forename><surname>Arun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiol.: Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2021">200267. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional dynamic alignment networks for interpretable classifications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bohle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10029" to="10038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual explanations for the detection of diabetic retinopathy from retinal fundus images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Boreiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet</title>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00760</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gifsplanation via Latent Shift: a simple autoencoder approach to counterfactual generation for chest X-rays</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<biblScope unit="page" from="74" to="104" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Djoumessi</surname></persName>
		</author>
		<idno>arXiv:TODO</idno>
		<title level="m">Sparse activations for interpretable disease grading</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Which explanation should i choose? a function approximation perspective to characterizing post hoc explanations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.01254</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">why should i trust you? explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ExplainGAN: model explanation via decision boundary crossing transformations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="666" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Grad-CAM: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Explanation by progressive exaggeration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pollack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00483</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">When explanations lie: why many modified BP attributions fail</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sixt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Granz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Landgraf</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9046" to="9057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03825</idno>
		<title level="m">SmoothGrad: removing noise by adding noise</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<title level="m">Striving for simplicity: the all convolutional net</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Woerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Baumgartner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.00500</idno>
		<title level="m">Inherently interpretable multi-label classification using class-specific counterfactuals</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Measurable counterfactual local explanations for any classifier</title>
		<author>
			<persName><forename type="first">A</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Garcez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03020</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Precognition</title>
		<meeting>the IEEE Conference on Computer Vision and Precognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
