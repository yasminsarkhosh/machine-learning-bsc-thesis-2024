<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos</title>
				<funder>
					<orgName type="full">Agency for Science, Technology and Research (A*STAR) Central Research Fund</orgName>
					<orgName type="abbreviated">CRF</orgName>
				</funder>
				<funder ref="#_xD5MeRn">
					<orgName type="full">National Research Foundation, Singapore</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Chao</forename><surname>Qin</surname></persName>
							<email>chao.qin@mbzuai.ac.ae</email>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of Artificial Intelligence</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">United Arab Emirates</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiale</forename><surname>Cao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tianjin University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Agency for Science, Technology and Research</orgName>
								<orgName type="institution">Institute of High Performance Computing</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rao</forename><forename type="middle">Muhammad</forename><surname>Anwer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of Artificial Intelligence</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">United Arab Emirates</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zayed University of Artificial Intelligence</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">United Arab Emirates</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Linköping University</orgName>
								<address>
									<settlement>Linköping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="479" to="488"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">55DA446CB8FB0A265FFC16E5C50E28C0</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_45</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Breast lesion detection</term>
					<term>Ultrasound videos</term>
					<term>Spatial-temporal deformable attention</term>
					<term>Multi-frame prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Detecting breast lesion in videos is crucial for computeraided diagnosis. Existing video-based breast lesion detection approaches typically perform temporal feature aggregation of deep backbone features based on the self-attention operation. We argue that such a strategy struggles to effectively perform deep feature aggregation and ignores the useful local information. To tackle these issues, we propose a spatial-temporal deformable attention based framework, named STNet. Our STNet introduces a spatial-temporal deformable attention module to perform local spatial-temporal feature fusion. The spatial-temporal deformable attention module enables deep feature aggregation in each stage of both encoder and decoder. To further accelerate the detection speed, we introduce an encoder feature shuffle strategy for multi-frame prediction during inference. In our encoder feature shuffle strategy, we share the backbone and encoder features, and shuffle encoder features for decoder to generate the predictions of multiple frames. The experiments on the public breast lesion ultrasound video dataset show that our STNet obtains a state-of-the-art detection performance, while operating twice as fast inference speed. The code and model are available at https://github.com/AlfredQin/STNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ultrasound imaging is a very effective technique for breast lesion diagnosis, which has high sensitivity. Automatically detecting breast lesions is a challenging problem with a potential to aid in improving the efficiency of radiologists in ultrasound-based breast cancer diagnosis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref>. Some of the challenges associated with automatic breast lesion detection include blurry boundaries and changeable sizes of breast lesions.</p><p>Most existing breast lesion detection methods can be categorized into imagebased <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref> and video-based <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref> breast lesion detection approaches. Image-based breast lesion detection approaches perform detection in each frame independently. Compared to image-based breast lesion detection approaches, methods based on videos are capable of utilizing temporal information for improved detection performance. For instance, Chen et al. <ref type="bibr" target="#b0">[1]</ref> exploited temporal coherence for semi-supervised video-based breast lesion detection. Recently, Lin et al. <ref type="bibr" target="#b8">[9]</ref> proposed a feature aggregation network, termed as CVA-Net, that executes intra-video and inter-video fusions at both video and clip levels based on attention blocks. Although the recent CVA-Net aggregates clip and video level features, we distinguish two key issues that hamper its performance. First, the self-attention based cross-frame feature fusion is a global-level operation and it operates once before the encoder-decoder, thereby ignoring the useful local information and in turn missing an effective deep feature fusion. Second, CVA-Net only performs one-frame prediction based on multiple frame inputs, which is very time-consuming.</p><p>To address the aforementioned issues, we propose a spatial-temporal deformable attention based network, named STNet, for detecting the breast lesions in ultrasound videos. Within our STNet, we introduce a spatial-temporal deformable attention module to fuse multi-scale spatial-temporal information among different frames, and further integrate it into each layer of the encoder and decoder. In this way, different from the recent CVA-Net, our proposed STNet performs both deep and local feature fusion. In addition, we introduce multiframe prediction with encoder feature shuffle operation that shares the backbone and encoder features, and only perform multi-frame prediction in the decoder. This enables us to significantly accelerate the detection speed of the proposed approach. We conduct extensive experiments on a public breast lesion ultrasound video dataset, named BLUVD-186 <ref type="bibr" target="#b8">[9]</ref>. The experimental results validate the efficacy of our proposed STNet that has a superior detection performance. For example, our proposed STNet achieves a mAP of 40.0% with an absolute gain of 3.9% in terms of detection accuracy, while operating at two times faster, compared to the recent CVA-Net <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Here, we describe our proposed spatial-temporal deformable attention based framework, named STNet, for detecting breast lesions in the ultrasound videos. Figure <ref type="figure" target="#fig_0">1</ref>(a) presents the overall architecture of our proposed STNet, which is built on the end-to-end detector deformable DETR <ref type="bibr" target="#b21">[22]</ref>. Within our STNet, we introduce spatial-temporal deformable attention into the encoder and the decoder. As in CVA-Net <ref type="bibr" target="#b8">[9]</ref>, we take six frames I k-1 , I k , I k+1 , I r1 , I r2 , I r3 from one ultrasound video as inputs, where there are three neighboring frames I k-1 , I k , I k+1 and three randomly-selected frames I r1 , I r2 , I r3 . Given these input frames, we use the backbone, such as ResNet-50 <ref type="bibr" target="#b5">[6]</ref>, to extract deep multi-scale features F k-1 , F k , F k+1 , F r1 , F r2 , F r3 . Afterwards, we introduce a spatial-temporal deformable attention based encoder (ST-Encoder) to perform intra-frame and inter-frame multi-scale feature fusion. Then, we introduce a spatial-temporal deformable attention based decoder (ST-Decoder) to generate output feature embeddings P k , which are fed to a classifier and a box predictor for classification and bounding-box regression. During inference, we take three neighboring frames and three randomly-selected frames as the inputs, and simultaneously predict the results of three neighboring frames using our encoder feature shuffle strategy. As a result, our approach operates at a faster inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spatial-Temporal Deformable Attention</head><p>Given a reference point, deformable attention <ref type="bibr" target="#b21">[22]</ref> aggregates the features of a group of key sampling points near it. Compared to original transformer selfattention <ref type="bibr" target="#b12">[13]</ref>, deformable attention has low-complexity along with a faster convergence speed. Motivated by this, we adopt deformable attention for breast lesion detection and extend it to spatial-temporal deformable attention (STDA). Our STDA not only aggregates the features of current frame, but also aggregates the features of the rest of the frames. Figure <ref type="figure" target="#fig_1">2</ref> presents the structure of our Given a query feature and reference point, our STDA not only fuses multi-scale features within a frame, but also aggregates multi-scale features between different frames.</p><p>proposed STDA. Let F t = F l t L l=1 represent the set of multi-scale feature maps at frame t, where F l t ∈ R C×H l ×W l is the feature map at level l. Given the query features p q and corresponding reference points z q , the spatio-temporal multiscale attention is given as:</p><formula xml:id="formula_0">STDA z q , p q , {F t } T t=0 = M m=1 W m T t=1 L l=1 K k=1 A tlqk F l t (φ l (p q ) + Δp tlqk ),<label>(1)</label></formula><p>where m represents multi-head index and k is sampling point index. W m is a linear layer, A tlqk indicates attention weight of sampling point, and Δp tlqk indicates sample offset of sampling point. φ l normalizes the coordinates p q by the scale of feature map F l t . The sampling offset Δp tlqk is predicted by the query feature z q with a linear layer. The attention weight A tlqk is predicted by feeding query feature z q to a linear layer and a softmax layer. As a result, the sum of attention weights is equal to one as</p><formula xml:id="formula_1">T t=1 L l=1 K k=1 A tlqk = 1.</formula><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>Compared to the standard deformable attention, the proposed spatial-temporal deformable attention fully exploits spatial information within frame and temporal information across frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spatial-Temporal Deformable Attention Based Encoder and Decoder</head><p>Here, we integrate the proposed spatial-temporal deformable attention (STDA) into encoder and decoder (called ST-Encoder and ST-Decoder). As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(b), ST-Encoder takes deep multi-scale feature maps F k-1 , F k , F k+1 , F r1 , F r2 , F r3 as inputs. Afterwards, we employ STDA to perform spatial and temporal fusion and generate the fused multi-scale feature maps</p><formula xml:id="formula_3">F k-1 , F k , F k+1 , F r1 , F r2 , F r3</formula><p>, where the query corresponds to each pixel in multi-scale feature maps. Then, the fused feature map goes through a feed-forward network (FFN) to generate the output feature maps</p><formula xml:id="formula_4">E k-1 , E k , E k+1 , E r1 , E r2 , E r3 .</formula><p>Similar to the original deformable DETR, we adopt cascade structure to stack six STDA and FFN layers in ST-Encoder.</p><p>The ST-Decoder takes the output feature maps E k-1 , E k , E k+1 , E r1 , E r2 , E r3 and a set of learnable queries Q ∈ R N ×C as inputs. The learnable queries first go through a self-attention layer. Afterwards, STDA performs cross-attention operation between these feature maps and the queries, where the key elements are these output feature maps of ST-Encoder. Then, we employ a FFN layer to generate the prediction features P k ∈ R N ×C . We also stack six self-attention, STDA, and FFN layers in ST-Decoder for deep feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-frame Prediction with Encoder Feature Shuffle</head><p>As discussed above, the proposed STNet adopts six frames to predict the results of one frame. Although STNet fully exploits temporal information for improved breast lesion detection, it becomes time-consuming for multi-frame prediction. To accelerate the detection speed, we introduce multi-frame prediction with encoder feature shuffle during inference. Instead of going through the entire network several times, we first share deep multi-scale feature maps before encoder and second perform the decoder several times for multi-frame prediction. To perform multi-frame prediction only in the decoder, we propose the encoder feature shuffle operation shown in Fig. <ref type="figure" target="#fig_0">1(d)</ref>. By exchanging the order of neighboring frame I k-1 , I k , I k+1 , the decoder can predict the results of three neighboring frames, respectively. Compared to the original STNet, the proposed encoder feature shuffle strategy only employs decoder forward three frames and accelerates the inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>Dataset. We conduct the experiments on the public BLUVD-186 dataset <ref type="bibr" target="#b8">[9]</ref>, comprising 186 videos including 112 malignant and 74 benign cases. The dataset has totally 25,458 ultrasound frames, where the number of frames in a video ranges from 28 to 413. The videos encompass a comprehensive tumor scan, from its initial appearance to its largest section and eventual disappearance. All videos were captured using PHILIPS TIS L9-3 and LOGIQ-E9. The grounding-truths in a frame, including breast lesion bounding-boxes and corresponding categories, are labeled by two pathologists, which have eight years of professional background in the field of breast pathology. We adopt the same dataset splits as in Table <ref type="table">1</ref>. State-of-the-art quantitative comparison of our approach with existing methods in literature on the BLUVD-186 dataset. Our approach achieves a superior performance on three different metrics. Compared to the recent CVA-Net <ref type="bibr" target="#b8">[9]</ref>, our approach obtains a gain of 3.9% in terms of overall AP. We show the best results in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Type Backbone AP AP50 AP75 GFL <ref type="bibr" target="#b6">[7]</ref> image ResNet-50 23.4 46.3 22.2 Cascade RPN <ref type="bibr" target="#b13">[14]</ref> image ResNet-50 24.8 42.4 27.3 Faster R-CNN <ref type="bibr" target="#b11">[12]</ref> image ResNet-50 25.2 49.2 22.3 VFNet <ref type="bibr" target="#b19">[20]</ref> image ResNet-50 28.0 47.1 31.0 RetinaNet <ref type="bibr" target="#b7">[8]</ref> image ResNet-50 29.5 50.4 32.4</p><p>DFF <ref type="bibr" target="#b23">[24]</ref> video ResNet-50 25.8 48.5 25.1 FGFA <ref type="bibr" target="#b22">[23]</ref> video ResNet-50 26.1 49.7 27.0 SELSA <ref type="bibr" target="#b14">[15]</ref> video ResNet-50 26.4 45.6 29.6 Temporal ROI Align <ref type="bibr" target="#b4">[5]</ref> video ResNet-50 29.0 49.9 33.1 MEGA <ref type="bibr" target="#b1">[2]</ref> video ResNet-50 32.3 57.2 35.7 CVA-Net <ref type="bibr" target="#b8">[9]</ref> video ResNet-50 36. the previous work CVA-Net <ref type="bibr" target="#b8">[9]</ref>, to guarantee a fair comparison. Specifically, the testing set comprises 38 videos randomly selected from all 186 videos, while the rest of the videos are used as the training set.</p><p>Evaluation Metrics. Three commonly-used metrics are employed for performance evaluation of breast lesion detection methods on the ultrasound videos, namely average precision (AP), AP 50 , and AP 75 .</p><p>Implementation Details. We employ the ResNet-50 <ref type="bibr" target="#b5">[6]</ref> pre-trained on Ima-geNet <ref type="bibr" target="#b2">[3]</ref>, and use Xavier <ref type="bibr" target="#b3">[4]</ref> to initialize the remaining network parameters. To enhance the diversity of training data, all videos are randomly subjected to horizontal flipping, cropping, and resizing. Similar to that of CVA-Net, we employ a two-phase training strategy to achieve better convergence. In the first phase, we employ Adam optimizer to train the model for 8 epochs. We then fine-tune the model for another 20 epochs with the SGD optimizer. Throughout both phases of training, we adopt the consistent hyper-parameters, where the learning rate is 5 × 10 -5 and the weight decay is 1 × 10 -4 . We train the model on a single NVIDIA A100 GPU and set the batch size as 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">State-of-the-Art Comparison</head><p>Our proposed approach is compared with eleven state-of-the-art methods, comprising image-based and video-based methods. We report the detection performance of these state-of-the-art methods generated by CVA-Net <ref type="bibr" target="#b8">[9]</ref>. Specifically, CVA-Net acquires the detection performance of these methods by utilizing their publicly available codes or re-implementing them if no publicly available codes. Quantitative Comparisons. Table <ref type="table">1</ref> presents the state-of-the-art quantitative comparison of our approach with the eleven existing breast lesion video detection methods in literature. As a general trend, video-based methods tend to yield higher average precision (AP), AP50, and AP75 scores compared to image-based breast lesion detection methods. Among the eleven existing methods, the recent CVA-Net <ref type="bibr" target="#b8">[9]</ref> achieves the best overall AP score of 36.1, AP50 score of 65.1, and AP75 score of 38.5. Our proposed STNet method consistently outperforms CVA-Net <ref type="bibr" target="#b8">[9]</ref> on all three metrics (AP, AP50, and AP75). Specifically, our STNet achieves a significant improvement in the overall AP score from 36.1 to 40.0, the AP50 score from 65.1 to 70.3, and the AP75 score from 38.5 to 43.3. The significant improvement demonstrates the efficacy of our approach for detecting breast lesions in ultrasound videos.</p><p>Qualitative Comparisons. Figure <ref type="figure" target="#fig_3">3</ref> presents the qualitative breast lesion detection comparison between CVA-Net and our proposed approach on an ultrasound video containing the benign breast lesions. Moreover, we show the ground truth of each frame on the third row for reference. The first row of the figure shows that CVA-Net struggles to identify the breast lesions in the second and third frames. Further, although CVA-Net manages to identify the breast lesions in the first and fifth frames, the classification results are inaccurate (as highlighted by the blue rectangle in Fig. <ref type="figure" target="#fig_3">3</ref>). In contrast, our STNet method in the second row of Fig. <ref type="figure" target="#fig_3">3</ref> accurately detects the breast lesions in all video frames and achieves accurate classification performance for each frame. Inference Speed Comparison. We present the inference speed comparison between our proposed STNet and CVA-Net on an NVIDIA RTX 3090 GPU using the same environment. We use FPS (frames per second) as the performance metric. Specifically, our proposed STNet achieves an averaged inference speed of 21.84 FPS, while CVA-Net achieves an averaged speed of 12.17 FPS. Our model operates around two times faster than CVA-Net, which we attribute to the ability of our model to predict three frames simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>Effectiveness of STDA: To show the efficacy of our proposed STDA, we perform different ablation studies. The first baseline network, referred as "Baseline + Single-frame", uses the original deformable DETR and takes a single frame as input. The second baseline network, referred as "Baseline + Multi-frame", uses modified deformable DETR with multi-head attention module to fuse six input frames. For the third study, labeled "ST-Encoder + DA-Decoder", we retain the encoder with STDA in our model but replace the STDA in the decoder with the conventional deformable attention. Similarly, in the fourth study, labeled "DA-Encoder + ST-Decoder", we retain the decoder with STDA in our model but replace the STDA in the encoder with the conventional deformable attention. As shown in Table <ref type="table" target="#tab_0">2</ref>, the results show that "ST-Encoder + DA-Decoder" and "DA-Encoder + ST-Decoder" improve the AP by 4.7 and 5.6, respectively, compared to "Baseline + Single-frame". This demonstrates that STDA can effectively perform intra-frame and inter-frame multi-scale feature fusion, even when only partially adopted in the encoder or decoder. Furthermore, our proposed STNet improves the AP by 5.1 and 4.2 compared to "ST-Encoder + DA-Decoder" and "DA-Encoder + ST-Decoder", respectively, indicating that the integration of STDA in both the encoder and decoder is crucial for achieving superior detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a novel breast lesion detection approach for ultrasound videos, termed as STNet, which performs local spatial-temporal feature fusion and deep feature aggregation in each stage of both encoder and decoder using our spatial-temporal deformable attention module. Additionally, we introduce the encoder feature shuffle strategy that enables multi-frame prediction during inference, thereby enabling us to accelerate the inference speed while maintaining better detection performance. The experiments conducted on a public breast lesion ultrasound video dataset show the efficacy of our STNet, resulting in a superior detection performance while operating at a fast inference speed. We believe STNet presents a promising solution and will help further promote future research in the direction of efficient and accurate breast lesion detection in videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Overall architecture of the proposed STNet. The proposed STNet takes six frames as inputs and extracts multi-scale features of each frame. Afterwards, the proposed STNet utilizes a spatial-temporal deformable attention (STDA) based encoder (b) and decoder (c) for spatial-temporal multi-scale information fusion. Finally, the proposed STNet performs classification and regression. (d) During inference, we introduce a encoder feature shuffle strategy for multi-frame prediction.</figDesc><graphic coords="3,55,98,54,41,340,18,220,30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Structure of our proposed Spatial-temporal deformable attention (STDA).Given a query feature and reference point, our STDA not only fuses multi-scale features within a frame, but also aggregates multi-scale features between different frames.</figDesc><graphic coords="4,41,79,54,62,340,24,140,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1 65.1 38.5 STNet (Ours) video ResNet-50 40.0 70.3 43.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Qualitative breast lesion detection comparison on example ultrasound video frames between the recent CVA-Net [9] and our proposed STNet. We also show the ground truth as reference. Our STNet achieves improved detection performance, compared to CVA-Net. Best viewed zoomed in. (Color figure online)</figDesc><graphic coords="7,55,98,54,50,340,18,175,21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Ablation study with different design choices. Our proposed STNet achieves a superior performance compared to the baseline and some different designs. We show the est results in bold.</figDesc><table><row><cell></cell><cell>AP AP50 AP75</cell></row><row><cell>Baseline + Single-frame</cell><cell>30.2 55.0 31.7</cell></row><row><cell>Baseline + Multi-frame</cell><cell>35.1 61.6 37.4</cell></row><row><cell cols="2">ST-Encoder + DA-Decoder 34.9 59.8 37.7</cell></row><row><cell cols="2">DA-Encoder + ST-Decoder 35.8 60.4 38.0</cell></row><row><cell>STNet (Ours)</cell><cell>40.0 70.3 43.3</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This research is supported by the <rs type="funder">National Research Foundation, Singapore</rs> under its <rs type="programName">AI Singapore Programme (AISG Award</rs> No: <rs type="grantNumber">AISG2-TC-2021-003</rs>) and <rs type="funder">Agency for Science, Technology and Research (A*STAR) Central Research Fund (CRF)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xD5MeRn">
					<idno type="grant-number">AISG2-TC-2021-003</idno>
					<orgName type="program" subtype="full">AI Singapore Programme (AISG Award</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semi-supervised breast lesion detection in ultrasound video based on temporal coherence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06941</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory enhanced global-local aggregation for video object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10337" to="10346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Temporal Rol align for video object recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1442" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalized focal loss: learning qualified and distributed bounding boxes for dense object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21002" to="21012" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new dataset and a baseline model for breast lesion detection in ultrasound videos</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_59</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_59" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automated analysis of ultrasound videos for detection of breast lesions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Movahedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Parsaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakoli Golpaygani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Haghighi Poya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Middle East J. Cancer</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="90" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated diagnosis of breast ultrasonography images using deep neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cascade RPN: delving into highquality region proposal network with adaptive convolution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">X</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06720</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence level semantics aggregation for video object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9217" to="9225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Global guidance network for breast lesion segmentation in ultrasound images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">101989</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A temporal sequence dual-branch network for classifying hybrid ultrasound data of breast cancer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="82688" to="82699" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated breast ultrasound lesions detection using convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1218" to="1226" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BIRADS features-oriented semisupervised deep learning for breast ultrasound computer-aided diagnosis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">125005</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VarifocalNet: an IoU-aware dense object detector</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sunderhauf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8510" to="8519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A second-order subregion pooling network for breast lesion segmentation in ultrasound</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-2_16" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="160" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deformable DETR: deformable transformers for end-to-end object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2349" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
