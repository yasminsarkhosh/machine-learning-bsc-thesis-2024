<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models</title>
				<funder>
					<orgName type="full">New Energy and Industrial Technology Development Organization</orgName>
					<orgName type="abbreviated">NEDO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ze</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Innovative Research</orgName>
								<orgName type="laboratory">Biomedical Artificial Intelligence Research Unit</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maolin</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Innovative Research</orgName>
								<orgName type="laboratory">Biomedical Artificial Intelligence Research Unit</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fahad</roleName><forename type="first">Yuqiao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Innovative Research</orgName>
								<orgName type="laboratory">Biomedical Artificial Intelligence Research Unit</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fahad</forename><forename type="middle">Parvez</forename><surname>Mahdi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tianyi</forename><surname>Qu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Innovative Research</orgName>
								<orgName type="laboratory">Biomedical Artificial Intelligence Research Unit</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ren</forename><surname>Sasage</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Innovative Research</orgName>
								<orgName type="laboratory">Biomedical Artificial Intelligence Research Unit</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Kenji</forename><surname>Suzuki</surname></persName>
							<email>suzuki.k.di@m.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Innovative Research</orgName>
								<orgName type="laboratory">Biomedical Artificial Intelligence Research Unit</orgName>
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<settlement>Kanagawa</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="713" to="722"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">5B060026C3569D652B6A42E371B34F6B</idno>
					<idno type="DOI">10.1007/978-3-031-43895-0_67</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Explainable AI (XAI)</term>
					<term>Visualizing Functions</term>
					<term>Liver Tumor Segmentation</term>
					<term>Unsupervised Hierarchical Clustering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this study, we proposed a novel explainable artificial intelligence (XAI) technique to explain massive-training artificial neural networks (MTANNs). Firstly, we optimized the structure of an MTANN to find a compact model that performs equivalently well to the original one. This enables to "condense" functions in a smaller number of hidden units in the network by removing "redundant" units. Then, we applied an unsupervised hierarchical clustering algorithm to the function maps in the hidden layers with the single-linkage method. From the clustering and visualization results, we were able to group the hidden units into those with similar functions together and reveal the behaviors and functions of the trained MTANN models. We applied this XAI technique to explain the MTANN model trained to segment liver tumors in CT. The original MTANN model with 80 hidden units (F1 = 0.6894, Dice = 0.7142) was optimized to the one with nine hidden units (F1 = 0.6918, Dice = 0.7005) with almost equivalent performance. The nine hidden units were clustered into three groups, and we found the following three functions: 1) enhancing liver area, 2) suppressing non-tumor area, and 3) suppressing the liver boundary and false enhancement. The results shed light on the "black-box" problem with deep learning (DL) models; and we demonstrated that our proposed XAI technique was able to make MTANN models "transparent".</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Artificial intelligence (AI) research has evolved rapidly, and unprecedented breakthroughs have been made in many fields. Applications of AI products can be witnessed in our daily life, such as autonomous driving, computer-aided diagnosis, automatic voice customer service, etc. The development of AI is undoubtedly a revolution in the course of human history.</p><p>The most effective and commonly used AI model is the one based on deep neural networks <ref type="bibr" target="#b0">[1]</ref>. However, with continuous research being held in methodologies, DL models are becoming more and more complicated. Researchers found that the deeper and more complex DL models are, the better the performance they could achieve for the tasks that traditional AI algorithms could not work well. The complexity of DL models reduces interpretability and transparency substantially; therefore, the current DL models are "black-box" <ref type="bibr" target="#b1">[2]</ref>. It is difficult to find how the model works in a way that humans can understand. Because of that, what researchers can do is only to prepare enough data and spend time training a model to obtain a high performance. Therefore, researchers or users can hardly find the reason why a DL model made a wrong decision.</p><p>XAI is an old area in AI research, but was named relatively recently <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, focusing now on the explainability of DL models. The final goal of XAI is to develop methods for revealing a basis for the decision made by a DL model and how the decision was made by the model to let users understand and trust the decision and model. Many XAI methods have been proposed to explain a trained DL model (i.e., post-hoc methods). Representative XAI methods include class activation mapping (CAM) <ref type="bibr" target="#b4">[5]</ref>, grad-CAM, layer-wise relevance propagation (LRP) <ref type="bibr" target="#b5">[6]</ref>, DL important features (DeepLIFT) <ref type="bibr" target="#b6">[7]</ref>, local interpretable model-agnostic explanations (LIME) <ref type="bibr" target="#b7">[8]</ref>, and SHapley additive explanations (SHAP) <ref type="bibr" target="#b8">[9]</ref>. These XAI methods offer post-hoc explanations that indicate which areas in a given input image the trained model focuses on and identify which areas in the image have a positive or negative impact on the model decision. In other words, those XAI methods are "instance-based" and limited to the visual explanation of model's attentions in a given input image (i.e., an instance). However, they do not offer explanations of the learned functions of the network.</p><p>In this study, we developed and presented an original XAI approach that can reveal the learned functions of groups of neurons in a neural network, which we call "functional explanations" and define as explanations of the model behavior by a combination of functions, as opposed to the visualization of a pattern to which a neuron responds. To our knowledge, there is no XAI method that offers functional explanations. Thus, our method is a post-hoc method that offers both instance-based and model-based functional explanations. We applied our XAI method to an MTANN model to emphasize the explainability and trustability of the MTANN, so that users can trust the MTANN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MTANN Deep Learning</head><p>In the field of image processing, supervised nonlinear filters and edge enhancers based on an artificial neural network (ANN) <ref type="bibr" target="#b9">[10]</ref> have been investigated for the reduction of the quantum noise in angiograms and supervised semantic segmentation of the left ventricles in angiography <ref type="bibr" target="#b10">[11]</ref>, which are called neural filters and neural edge enhancers, respectively. By extending the neural filter and edge enhancer, massive-training artificial neural networks (MTANNs) have been developed to reduce false positives in the computerized detection of lung nodules in computed tomography (CT) <ref type="bibr" target="#b11">[12]</ref>. The MTANNs have also shown promising performance in pattern recognition and classification tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>An MTANN is a deep learning model consisting of linear-output artificial neural network regression model that directly operates on pixels in an input image, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. A large number of patches are extracted from input images; and corresponding pixels at the same positions in desired output images, named as teaching images, are extracted for the MTANN to learn. This patch-based training leads to the fact that the MTANN can be trained with only a small number of input and teaching images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sensitivity-Based Structure Optimization</head><p>The numbers of hidden layers and their units in an MTANN model are adjustable hyperparameters. A relatively large structure is used to ensure that the model performs well on a specific task. A trained large model, however, may contain redundant units, and functions of neurons for the task would be "distributed and diluted" in many neurons in the model. This makes the analysis of the functions of neurons very difficult <ref type="bibr" target="#b14">[15]</ref>.</p><p>To address this issue, we applied our sensitivity-based structure optimization algorithm <ref type="bibr" target="#b15">[16]</ref> to a trained large MTANN model to "consolidate" the diluted functions of neurons in the MTANN model. With this algorithm, redundant hidden units of the model are gradually removed; and a compact model with equivalent performance is obtained. The algorithm is described as the following steps: Train on until the loss value converges the loss value of on other necessary evaluation metrics of on (like PSNR, dice coefficient, etc., which depend on the task) (Initialize the maximum loss value after removing a hidden unit from , and the loss value is supposed to be between 0 and 1) (Initialize the index of the hidden layer where the hidden unit belongs)</p><p>(Initialize the index of the hidden layer until in the -th hidden layer)</p><p>for in do (Go through each hidden layer) if do (This layer has only one unit which cannot be removed) Skip to the next iteration for in do (Go through each hidden unit in the -th hidden layer) Remove the -th hidden unit in the -th hidden layer from temporarily the loss value of on if do</p><p>Put the -th hidden unit in the -th hidden layer back to ← (Copy current model's weights and structure) Remove the -th hidden unit in the -th hidden layer from permanently return With the proposed optimization algorithm, the hidden units of MTANN could be gradually removed until the performance drops greatly when any of the rest unit is deleted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Calculation of Weighted Function Maps</head><p>After applying the structure optimization algorithm, every hidden unit in the compact model is expected to have an essential function for the target task. To understand the functions of the hidden units, function maps were obtained by performing the MTANN convolution of a hidden unit over a given input image. For better discrimination between enhancement and suppression, the function maps were normalized and then multiplied by the sign of the weight between the hidden units and the output unit. Weighted function maps were finally generated by shifting the range of the function map by 0.5. Namely, for a given hidden unit, in the weighted function map, a pixel value &gt;0.5 means enhancement of patterns in the input image, whereas a pixel value &lt;0.5 means suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Unsupervised Hierarchical Clustering</head><p>To group similar functions of the hidden units of the MTANN, we applied an unsupervised hierarchical clustering algorithm <ref type="bibr" target="#b16">[17]</ref> to the weighted functional visualization maps. With this algorithm, the hidden units were automatically divided into several groups based on the following distance function between the weighted function maps of the hidden units:</p><formula xml:id="formula_0">distance(x, y) = α(1 -SSIM (x, y)) + NRMSE(x, y) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where SSIM is the structural similarity index, and NMRSE is the normalized root mean square error. With the unsupervised hierarchical clustering algorithm, we visualize the function maps of the hidden units group by group to explain the behavior of each group of the hidden units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dynamic Contrast-Enhanced Liver CT</head><p>Our XAI technique was applied to explain the MTANN model's decision in a liver tumor segmentation task <ref type="bibr" target="#b19">[20]</ref>. Dynamic contrast-enhanced liver CT scans consisting of 42 patients with 194 liver tumors in the portal venous phase from the LiTS database <ref type="bibr" target="#b20">[21]</ref> were used in this study. Each slice of the CT volumes in the dataset has a matrix size of 512 × 512 pixels, with in-plane pixel sizes of 0.60-1.00 mm and thicknesses of 0.20-0.70 mm. The dataset consists of the original hepatic CT image with the liver mask and the "gold-standard" liver tumor region manually segmented by a radiologist, as illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. Firstly, to have the same physical scale on spatial coordinates, bicubic interpolation was applied on the original hepatic CT images together with the corresponding liver mask and "gold-standard" tumor segmentation to obtain isotropic images with a voxel size of 0.60 × 0.60 × 0.60 mm 3 . Then, to unify the image size into the same size, the isotropic image was cropped to obtain the liver region volume of interest (VOI) with an in-plane matrix size of 512 × 512. An anisotropic diffusion filter was applied to reduce the quantum noise, which could substantially reduce the noise while major structures such as tumors and vessels maintained <ref type="bibr" target="#b21">[22]</ref>. Finally, a Z-score normalization was applied to unify complex histograms of tumors in different cases. The final pre-processed CT images were used as the input images.</p><p>In addition, since most liver tumors' shape is ellipsoidal, the liver tumors can also be enhanced by the Hessian-based method and utilized in the model to improve the performance <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Hence, the model consisted of these two input channels: segmented liver CT image and its Hessian-enhanced image. Also, the patches were extracted from input images from both channels: a 5 × 5 × 5 sized patch in the same spatial position was extracted to form a training patch with a size of 2 × 5 × 5 × 5 pixels.</p><p>Seven cases and 24 cases in the dynamic contrast-enhanced CT scans dataset were used for training and testing, respectively. 10,000 patches were randomly selected from the liver mask region in each case, summing up to a total of 70,000 training samples for training. The number of input units in the MTANN model with one hidden layer was 250. The structure optimization process started with 80 hidden units in the hidden layer. The binary cross-entropy (BCE) loss function was used to train the model. The MTANN model classified the input patches into tumor or non-tumor classes, and the output pixels represented the probability of being a tumor class. During the structure optimization process, the F1 score on the training patches and the Dice coefficient on the training images were also calculated as the reference to select a suitable compact model that performed equivalently to the original large model.</p><p>As observed in the four evaluation metric curves in Fig. <ref type="figure" target="#fig_3">3</ref>, as the number of hidden units was reduced from 80 to 9, the performance of the model fluctuated up and down, and after it was reduced below 9, the performance of the model dropped dramatically. Therefore, we chose a number of hidden units of 9 as the optimized structure.</p><p>Then, we applied the unsupervised hierarchical clustering algorithm to the weighted function maps from the optimized compact model with 9 hidden units. Figure <ref type="figure" target="#fig_4">4</ref> shows that the 9 hidden units are clearly divided into 3 different groups. We denote hidden units 3, 4, and 7 as group A, hidden units 2, 6, 1, and 8 as group B, and hidden units 0 and 5 as group C. The hidden units in the same group should have a similar function, and the function maps from each group should show the function of the group.  As illustrated in Fig. <ref type="figure" target="#fig_5">5</ref>, the low-intensity areas in the function maps of hidden units 0 and 5 in group C match the high-intensity areas in the Hessian-enhanced input image, which means they suppress the high-intensity areas. Likewise, group A enhances the liver area, and group B suppresses the non-tumor area. We also understood that groups A and B worked together to enhance the tumor area, and group C suppressed the liver's boundary as well as reduced the false enhancements inside the liver. Thus, our XAI method was able to reveal the learned functions of groups of neurons in the neural network, which we call "functional explanations" and define as the explanations of the model behavior by a combination of functions. Our method is a post-hoc method that offers both instance-based and model-based functional explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this study, we proposed a novel XAI approach to explain the functions and behavior of an MTANN model for semantic segmentation of liver tumors in CT. Our structure optimization algorithm refined the structure and made every hidden unit in the model have a clear, meaningful function by removing redundant hidden units and "condensing" the functions into fewer hidden units, which solved the issue of unstable XAI results with conventional XAI methods. The unsupervised hierarchical clustering algorithm in our XAI approach grouped the hidden units with a similar function into one group so as to explain their functions by group. Through the experiments, we successfully proved that the MTANN model was explainable by functions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the structure of MTANN, extracting a patch from an input image and a desired pixel from a teaching image.</figDesc><graphic coords="3,55,98,148,82,340,15,74,41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Structure optimization for the MTANN. Require: : The training data Require: = : The numbers of units in each hidden layer ← 0 (Initialize timestamp) Initialize the weights in the model While do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An example from the dynamic contrast-enhanced liver CT dataset.</figDesc><graphic coords="5,77,46,396,02,297,22,91,27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance change of an MTANN segmentation scheme (in terms of BCE loss, F1 score, raw dice, and post dice) in the structure optimization process.</figDesc><graphic coords="7,91,98,57,32,268,66,231,07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Result of the unsupervised hierarchical clustering process for the function visualization maps for 9 hidden units.</figDesc><graphic coords="7,124,47,335,24,203,98,103,93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Functional visualization maps for the 9 hidden units in groups A, B, and C obtained by using our XAI method, and the comparison with the input, teaching, and output images.</figDesc><graphic coords="8,46,80,57,11,330,73,470,02" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This paper is based on results obtained from a project commissioned by the <rs type="funder">New Energy and Industrial Technology Development Organization (NEDO)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Can we open the black box of AI?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Castelvecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. News</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7623</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DARPA&apos;s explainable artificial intelligence (XAI) program</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Aha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Peeking inside the black-box: a survey on explainable artificial intelligence (XAI)</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="52138" to="52160" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Conference Machine Learning</title>
		<meeting>International Conference Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3145" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Why should i trust you? Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural edge enhancer for supervised edge enhancement from noisy images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sugie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural filter with selection of input features and its application to image quality improvement of medical image sequences</title>
		<author>
			<persName><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sugie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1710" to="1718" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extraction of left ventricular contours from left ventriculograms by means of a neural edge detector</title>
		<author>
			<persName><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="330" to="339" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Computer-aided diagnostic scheme for distinction between benign and malignant nodules in thoracic low-dose CT by use of massive training artificial neural network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1138" to="1150" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CT colonography: advanced computer-aided detection scheme utilizing MTANNs for detection of &apos;missed&apos; polyps in a multicenter clinical trial</title>
		<author>
			<persName><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Rockey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Dachman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="21" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On overfitting and the effective number of hidden units</title>
		<author>
			<persName><forename type="first">A</forename><surname>Weigend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1993 Connectionist Models Summer School</title>
		<meeting>the 1993 Connectionist Models Summer School</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple neural network pruning algorithm with application to filter synthesis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sugie</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1009639214138</idno>
		<ptr target="https://doi.org/10.1023/A:1009639214138" />
	</analytic>
	<monogr>
		<title level="j">Neural Process. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="53" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast optimal leaf ordering for hierarchical clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bar-Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Gifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An empirical comparison of voting classification algorithms: bagging, boosting, and variants</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1007515423169</idno>
		<ptr target="https://doi.org/10.1023/A:1007515423169" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="105" to="139" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic segmentation of liver tumor in contrast-enhanced hepatic CT by using deep learning with hessian-based enhancer with small training dataset size</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="34" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A large annotated medical image dataset for the development and evaluation of segmentation algorithms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Antonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv Prepr. ArXiv</title>
		<imprint>
			<date type="published" when="2019">190209063. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully automated MR liver volumetry using watershed segmentation coupled with active contouring</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Le-Trong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Suzuki</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-016-1498-9</idno>
		<ptr target="https://doi.org/10.1007/s11548-016-1498-9" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="243" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tissue classification based on 3D local intensity structures for volume rendering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="160" to="180" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An ellipsoid convex enhancement filter for detection of asymptomatic intracranial aneurysm candidates in CAD frameworks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kakeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Korogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="951" to="960" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
