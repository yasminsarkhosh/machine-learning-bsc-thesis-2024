Paper Title,Header Number,Header Title,Text
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,1,Introduction,"Head and Neck (H&N) cancer refers to malignant tumors in H&N regions, which is among the most common cancers worldwide  Traditional survival prediction methods are usually based on radiomics  Firstly, existing deep survival models are underdeveloped in utilizing complementary multi-modality information, such as the metabolic and anatomical information in PET and CT images. For survival prediction in H&N cancer, existing methods usually use single imaging modality  Secondly, although deep survival models have advantages in performing end-to-end survival prediction without requiring tumor masks, this also incurs difficulties in extracting region-specific information, such as the prognostic information in Primary Tumor (PT) and Metastatic Lymph Node (MLN) regions. To address this limitation, recent deep survival models adopted multi-task learning for joint tumor segmentation and survival prediction, to implicitly guide the model to extract features related to tumor regions  In this study, we design an X-shape merging-diverging hybrid transformer network (named XSurv, Fig. "
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,2,Method,Figure 
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,2.1,PET-CT Merging Encoder,"Assuming N conv , N self , and N cross are three architecture parameters, each encoder branch consists of N conv Conv blocks, N self Hybrid Parallel Self-Attention (HPSA) blocks, and N cross HPCA blocks. Max pooling is applied between blocks and the features before max pooling are propagated to the decoder through skip connections. As shown in Fig.  The idea of adopting convolutions and transformers in parallel has been explored for segmentation "
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,2.2,PT-MLN Diverging Decoder,As shown in Fig. 
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,2.3,Multi-task Learning,Following existing multi-task deep survival models 
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,2.4,Radiomics Enhancement,Our XSurv also can be enhanced by leveraging radiomics features (denoted as Radio-XSurv). Following 
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3,Experimental Setup,
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3.1,Dataset and Preprocessing,"We adopted the training dataset of HECKTOR 2022 (refer to https://hecktor.grand-cha llenge.org/), including 488 H&N cancer patients acquired from seven medical centers  We resampled PET-CT images into isotropic voxels where 1 voxel corresponds to 1 mm 3 . Each image was cropped to 160 × 160 × 160 voxels with the tumor located in the center. PET images were standardized using Z-score normalization, while CT images were clipped to "
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3.2,Implementation Details,"We implemented our XSurv using PyTorch on a 12 GB GeForce GTX Titan X GPU. Our XSurv was trained for 12,000 iterations using an Adam optimizer with a batch size of 2. Each training batch included the same number of censored and uncensored samples. The learning rate was set as 1e-4 initially and then reset to 5e-5 and 1e-5 at the 4,000 th and 8,000 th training iteration. Data augmentation was applied in real-time during training to minimize overfitting, including random affine transformations and random cropping to 112 × 112 × 112 voxels. Validation was performed after every 200 training iterations and the model achieving the highest validation result was preserved. In our experiments, one training iteration (including data augmentation) took roughly 4.2 s, and one inference iteration took roughly 0.61 s."
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3.3,Experimental Settings,"We compared our XSurv to six state-of-the-art survival prediction methods, including two traditional radiomics-based methods and four deep survival models. The included traditional methods are CoxPH  We also performed two ablation studies on the encoder and decoder separately: (i) We replaced HPCA/HPSA blocks with Conv blocks and compared different strategies to combine PET-CT images. (ii) We removed RAG blocks and compared different strategies to extract PT/MLN-related information. "
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,4,Results and Discussion,The comparison between our XSurv and the state-of-the-art methods is presented in Table  The ablation study on the PET-CT merging encoder is shown in Table  The ablation study on the PT-MLN diverging decoder is shown in Table 
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,5,Conclusion,"We have outlined an X-shape merging-diverging hybrid transformer network (XSurv) for survival prediction from PET-CT images in H&N cancer. Within the XSurv, we propose a merging-diverging learning framework, a Hybrid Parallel Cross-Attention (HPCA) block, and a Region-specific Attention Gate (RAG) block, to learn complementary information from multi-modality images and extract region-specific prognostic information for survival prediction. Extensive experiments have shown that the proposed framework and blocks enable our XSurv to outperform state-of-the-art survival prediction methods on the well-benchmarked HECKTOR 2022 dataset."
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,,Fig. 1 .,
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,,Fig. 2 .,
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,,,
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,,Table 1 .,
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,,Table 2 .,
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,,Table 3 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,1,Introduction,"Cardiovascular disease is a leading cause of death in the world. Accurate quantification of left ventricular wall thicknesses (LVWT) from ultrasound images is among the most clinically important and significant indices for evaluating cardiac function and diagnosis of cardiac diseases  Existing work can be divided into two categories: segmentation-based and direct-regression methods. The direct-regression methods to learn the regress LVWTs from cardiac images directly without identifying the contours first.  Segmentation-based methods segment the myocardium first and then calculate cardiac parameters. To the best of our knowledge, existing segmentation work mainly focus on apical views to evaluate the ejection fraction, and rare work exists for short-axis views.  To overcome the above mention challenges and inspired "
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,2,Methods,"The structures of the myocardium in ultrasound SAX images generally follow a circular ring shape, which is a vital characteristic of prior knowledge in shortaxis myocardial segmentation, especially for ultrasound images with heavy noisy myocardium boundaries. In this paper, we propose a novel method, Temporal Compatibility Deformation Learning, named TC-Deformer, to achieve accurate and plausible myocardium contours and LVWTs estimation. The details are described as follows. Global Affine Transformation. Our deformation learning consists of two stages: global affine transformation and local deformation learning. In this subsection, we will describe our global affine transformation. Considering the diversity of the cardiac structure and data gaps from different machines, we compute the mean value of the thickness measurements and the center position of the circle according to the annotated information to generate the prior template. As shown in Fig. "
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,2.1,Deformation Learning,"where S is the ground truth of the myocardium segmentation. However, the shapes warping from the prior template with global affine parameters are far from precise due to the individual variation. So, we introduce the local deformation learning to get a precise myocardium shape with the learned dense deformation field. Local Deformation Learning. In this part, we aim to learn a dense deformation field to adjust the AT prior locally to match the myocardium boundaries. As shown in Fig.  where S is the ground truth of the myocardium segmentation. In this work, only frames at the end-systolic (ED) and end-diastolic (ES) phases are annotated by cardiologists for each ultrasound SAX video. To make the template-deformed myocardium boundaries compatible across the whole sequence, we propose a bi-direction deformation learning to guarantee that the deformation fields across the whole sequence can be applied to both the myocardium boundaries and the ultrasound images in the sequence."
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,2.2,TC-Deformer,"As shown in Fig.  Similarly, for the MAT prior S MAT , we can have the shape consistency constraint when applied to the bi-direction deformation procedure: As the temporally compatible deformation started with a common template, we assume that when warped backward, all the frames will have a similar appearance. So, we introduce a centralization loss, to minimize the deviation between those backward deformed frames: where T represents the time. The total loss function of our TC-Deformer is as follows: where α, β and γ is the hyper-parameters. After myocardial segmentation, we use neural networks to determine two key points, combined with the centroid of the segmentation, to divided it into 16segments according to the 16-segment model of the American Heart Association and calculated the corresponding LVWTs."
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,3,Experiments and Results,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,3.1,Dataset Description and Experimental Setup,"Datasets. In this experiment, we trained our method on ultrasound SAX videos of 141 participants and tested with 60 participants. All the data was collected with the GE Vivid E95 from the Shenzhen People's Hospital and this study was approved by local institutional review boards. For each participant, videos of the basal, middle, and apical SAX views were acquired with multiple cardiac cycles. The mask of the myocardium at the ED and ES phases from one cardiac cycle was annotated by experts (including two senior, three middle-level, and three junior cardiologists). We compare the LVWT of each group with the average wall thickness of the senior doctors to get the average of the errors for different doctors as result. For the training dataset, the senior cardiologists conducted quality control for junior and middle-level cardiologists. All images were annotated by experienced doctors using the Pair annotation software package  Experimental Setup. We resized the images to the same size 256 × 256 and used the Adam optimization strategy during model training. The training is in two stages and the initial learning rate was 0.0001, the total epoch number is 100, and the batch size is 4. The hyperparameters α, β, and γ were set to be 0.1, 0.5, and 0.5, respectively, according to a small validation set. The models are implemented with PyTorch on the NVIDIA A100 Tensor Core GPU. Table "
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,3.2,Results,Table 
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,4,Conclusion,"In this paper, We propose a Temporally Compatible Deformation learning network, named TC-Deformer, to detect the myocardium boundaries and estimate regional left ventricle wall thickness. Our method is designed to avoid the irregular contours that can happen in ultrasound images with heavy noise and can incorporate the temporal dynamics of the myocardium in one cardiac cycle into the deformation field learning. When validated with a dataset of 201 patients, our method achieves less than 1.00 mm estimation error for all 16 myocardium segments and outperforms existing state-of-the-art methods."
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Fig. 1 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Fig. 2 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Fig. 3 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Fig. 4 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Fig. 5 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Fig. 6 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Table 1 .,
Wall Thickness Estimation from Short Axis Ultrasound Images via Temporal Compatible Deformation Learning,,Table 2 .,
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,1,Introduction,"Medical image analysis plays an indispensable role in clinical therapy because of the implications of digital medical imaging in modern healthcare  Recently, Denoising Diffusion Probabilistic Models (DDPM)  Motivated by the achievements of diffusion probabilistic models in generative image modeling, 1) we present a novel Denoising Diffusion-based model named DiffMIC for accurate classification of diverse medical image modalities. As far as we know, we are the first to propose a Diffusion-based model for general medical image classification. Our method can appropriately eliminate undesirable noise in medical images as the diffusion process is stochastic in nature for each sampling step. 2) In particular, we introduce a Dual-granularity Conditional Guidance (DCG) strategy to guide the denoising procedure, conditioning each step with both global and local priors in the diffusion process. By conducting the diffusion process on smaller patches, our method can distinguish critical tissues with fine-grained capability. 3) Moreover, we introduce Condition-specific Maximum-Mean Discrepancy (MMD) regularization to learn the mutual information in the latent space for each granularity, enabling the network to model a robust feature representation shared by the whole image and patches. 4) We evaluate the effectiveness of DiffMIC on three 2D medical image classification tasks including placental maturity grading, skin lesion classification, and diabetic retinopathy grading. The experimental results demonstrate that our diffusion-based classification method consistently and significantly surpasses state-of-the-art methods for all three tasks. Figure "
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,2,Method,"where θ is parameters of the denoising UNet, N (•, •) denotes the Gaussian distribution, and I is the identity matrix."
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,2.1,Dual-Granularity Conditional Guidance (DCG) Strategy,"DCG Model. In most conditional DDPM, the conditional prior will be a unique given information. However, medical image classification is particularly challenging due to the ambiguity of objects. It is difficult to differentiate lesions and tissues from the background, especially in low-contrast image modalities, such as ultrasound images. Moreover, unexpected noise or blurry effects may exist in regions of interest (ROIs), thereby hindering the understanding of high-level semantics. Taking only a raw image x as the condition in each diffusion step will be insufficient to robustly learn the fine-grained information, resulting in classification performance degradation. To alleviate this issue, we design a Dual-granularity Conditional Guidance (DCG) for encoding each diffusion step. Specifically, we introduce a DCG model τ D to compute the global and local conditional priors for the diffusion process. Similar to the diagnostic process of a radiologist, we can obtain a holistic understanding from the global prior and also concentrate on areas corresponding to lesions from the local prior when removing the negative noise effects. As shown in Fig.  where ∼ N(0, I), ᾱt = t α t , α t = 1β t with a linear noise schedule {β t } t=1:T ∈ (0, 1) T . After that, we feed the concatenated vector of the noisy variable y t and dual priors into our denoising model UNet θ to estimate the noise distribution, which is formulated as: where f (•) denotes the projection layer to the latent space. [•] is the concatenation operation. E(•) and D(•) are the encoder and decoder of UNet. Note that the image feature embedding ρ(x) is further integrated with the projected noisy embedding in the UNet to make the model focus on high-level semantics and thus obtain more robust feature representations. In the forward process, we seek to minimize the noise estimation loss L : Our method improves the vanilla diffusion model by conditioning each step estimation function on priors that combine information derived from the raw image and ROIs."
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,2.2,Condition-Specific MMD Regularization,"Maximum-Mean Discrepancy (MMD) is to measure the similarity between two distributions by comparing all of their moments  To be specific, we sample the noisy variable y g t from the diffusion process at time step t conditioned only by the global prior and then compute an MMDregularization loss as: where K(•, •) is a positive definite kernel to reproduce distributions in the Hilbert space. The condition-specific MMD regularization is also applied on the local prior, as shown in Fig. "
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,2.3,Training and Inference Scheme,"Total Loss. By adding the noise estimation loss and the MMD-regularization loss, we compute the total loss L dif f of our denoising network as follows: where λ is a balancing hyper-parameter, and it is empirically set as λ=0.  Training Details. The diffusion model in this study leverages a standard DDPM training process, where the diffusion time step t is selected from a uniform distribution of [1, T ], and the noise is linearly scheduled with β 1 = 1 × 10 -4 and β T = 0.02. We adopt ResNet18 as the image encoder ρ(•). Following "
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,3,Experiments,"Datasets and Evaluation: We evaluate the effectiveness of our network on an in-home dataset and two public datasets, e.g., PMG2000, HAM10000 "
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,,Visualization of Our Diffusion Procedure:,"To illustrate the diffusion reverse process guided by our dual-granularity conditional encoding, we used the t-SNE tool to visualize the denoised feature embeddings at consecutive time steps. Figure "
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,4,Conclusion,"This work presents a diffusion-based network (DiffMIC) to boost medical image classification. The main idea of our DiffMIC is to introduce dual-granularity conditional guidance over vanilla DDPM, and enforce condition-specific MMD regularization to improve classification performance. Experimental results on three medical image classification datasets with diverse image modalities show the superior performance of our network over state-of-the-art methods. As the first diffusion-based model for general medical image classification, our DiffMIC has the potential to serve as an essential baseline for future research in this area."
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,,Fig. 1 .,
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,,Fig. 2 .,
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,,Table 1 .,
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,,Comparison with State-of-the-Art Methods: InTable 1,
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,,Table 2 .,
DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 10.
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,1,Introduction,"Posterior eyeball shape (PES) is related to various ophthalmic diseases, such as glaucoma, high myopia, and retinoblastoma  In ophthalmic clinics, existing representations of posterior eyeball shape are mainly based on two medical imaging devices, including Magnetic Resonance Imaging (MRI) and Optical Coherence Tomography (OCT). MRI cannot become the preferred screening device in ophthalmic clinics due to its being expensive and time-consuming. Even most ophthalmic hospitals do not equip MR devices. Moreover, the resolution of ocular MRI is 0.416 × 0.416 × 0.399 mm 2 , while the depth of the retina is around 250 µm  As far as we know, there is no existing work on 3D eyeball reconstruction just using local OCT images. We refer several 3D reconstruction methods using natural images. For example, NeRF and Multi-View Stereo "
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,2,Method,The overview of our proposed Posterior Eyeball Shape Network (PESNet) is shown in Fig. 
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,2.1,Dual-Branch Architecture of PESNet,"The input point cloud is derived from limited FOV OCT images, and the structures between the local area and the complete posterior eyeball are totally different. Thus, considering the structural regularity of ocular shape in OCT images, we propose to aggregate structural prior knowledge for the reconstruction. As shown in Fig.  where O(u, v) and Ô(u, v) denote the predicted and ground truth (GT) OSM, respectively. The (u, v) and U, V represent the positional index and size of OSM. The α denotes a manual set weight value."
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,2.2,Polar Voxelization Block and R-Wise Fusion Block,"Polar Voxelization Block (PVB). The curve shape of the eyeball determines the sparse layer labels of OCT in cartesian coordinates. Sparse convolution significantly increases memory. Inspired by the shape of the eyeball, we propose to use polar transformation to obtain the dense representation of the input. Due to the hierarchical and regular distribution of RPE points in polar grid, the 3D grid is parameterized into a 2D OSM by anisotropic convolution. Therefore, we design the PVB to perform polar transformation and voxelization jointly for two input point clouds P s and P t and output two 3D grids of voxels G s and G t . Given the center coordinate c = (c x , c y , c z ) of the top en-face slice of OCT images as polar origin, the transformation between the cartesian coordinate (x, y, z) and the polar coordinates (r, u, v) can be expressed as Eq. 2. Besides, given a polar point set P : {(r i , u i , v i )|i = 1, . . . , n} and a 3D polar grid G(r, u, v) ∈ {0, 1}, the voxelization process of PVB is defined in Eq. 3: where u,v and r are the elevation angle, azimuth angle, and radius in polar coordinates. The extra π 2 term is added to make the angle range correspond to the integer index of height and width of OSM. Besides, R,U ,V denote the depth, height, and width of the 3D grid (R = 64, U and V = 180 in our paper), which make grid G hold enough resolution with acceptable memory overhand. In addition, φ is a manual set number to control the depth-dimension of G. Since we choose the center of the top en-face slice as the origin, the RPE voxels distribute in layers along the R-axis of G. Therefore, we can compress the 3D polar grid G(r, u, v) into a 2D OSM along the R-axis of grid using Eq. 4, where OSM (u, v) denotes the pixel value at position (u, v), which represents the radius from the polar origin to an ocular surface point along the elevation and azimuth angles specified by index (u, v). The entire posterior eyeball shape can be reconstructed once given OSM and the center coordinate C. This representation of ocular shape is not only much denser than a conventional voxel-based grid (370 * 300 * 256 in our paper) in cartesian coordinates but also maintains more points and spatial structure than common point-based methods."
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,R-wise Fusion Block (RFB),The conventional feature fusion methods (denoted as F in Table 
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,3,Experiments,"Dataset: We build a Posterior Ocular Shape (POS) dataset based on Ultrawidefield (24 × 20 mm 2 ) swept-source OCT device (BM-400K BMizar; TowardPi Medical Technology). The POS dataset contains 55 eyes of thirty-three healthy participants. Simulating local regions captured by common OCT devices, we sample five subareas from every ultra-widefield sample. Both subarea point clouds and ground truth of OSM are derived from segmented RPE label images from all samples (more details are in supplementary). The POS dataset is randomly split into training, validation, and test sets with 70%, 20% and 10%, respectively. Referring to "
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Implementation Details:,"The intensity values of all GT OSMs are normalized from [0, 63] to [0, 1] for loss calculation. We use unified center coordinates as the polar origin for polar transformation since all OCT samples are aligned to the macular fovea during acquisition. Since the background pixels in predicted OSM are hardly equal to 0, which cause many noise points in reconstructed point cloud, only intensity values that are not equal to OSM  Evaluation Metrics: Since our proposal employs 2D OSM regression in polar coordinates to achieve 3D reconstruction in cartesian coordinates, we evaluate its performance using both 2D and 3D metrics. Specifically, we use Chamfer Distance (CD) to measure the mean distance between predicted point clouds and its GT point cloud, and Structural Similarity Index Measure (SSIM) as 2D metrics to evaluate the OSM regression performance. Additionally, we introduce point density (Density) as an extra metric to evaluate the number ratio of total points between predicted and GT point clouds, defined as Density = |1 -N pred /N GT |, where N pred is the number of predicted point clouds, N GT is the number of GT point clouds. The lower Density, the fewer defects in the valid region of OSMs."
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Method Comparison and Ablation Study:,"The effectiveness of the key components of PESNet is investigated through a comparison and ablation study, as presented in Fig.  In our PESNet, we adopt unshare-weight for two branches, which produces the best metrics in the last row. Therefore, our proposed algorithm reconstructs the eyeball close to GT considering the specificity of the eyeball."
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Ablation Study About Loss Function:,"We compare different loss functions applied in our task, and the results as shown in Table  Validation on Disease Sample: Although our POS dataset only contains the data of healthy eyeballs, we still test our PESNet on two disease cases: one is high myopia and the other is Diabetic Macular Edema. As shown in Fig. "
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,4,Conclusion,"In this study, we introduced the task of posterior eyeball shape 3D reconstruction using small-FOV OCT images, and proposed a Posterior Eyeball Shape Network (PESNet) that utilizes polar coordinates to perform 2D ocular surface map (OSM) regression. Our experiments on a self-made posterior ocular shape dataset demonstrate the feasibility of this novel task and confirm that PESNet is capable of leveraging local and anatomical prior information to reconstruct the complete posterior eyeball shape. Additionally, our validation experiment with disease data highlights the potential of PESNet for reconstructing the eyeballs of patients with ophthalmic diseases."
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Fig. 1 .,
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Fig. 2 .,
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Fig. 3 .,
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Table 1 .,
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Table 2 .,
Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 18.
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,1,Introduction,"The examination of tissue and cells using microscope (referred to as histology) has been a key component of cancer diagnosis and prognostication since more than a hundred years ago. Histological features allow visual readout of cancer biology as they represent the overall impact of genetic changes on cells  The great rise of deep learning in the past decade and our ability to digitize histopathology slides using high-throughput slide scanners have fueled interests in the applications of deep learning in histopathology image analysis. The majority of the efforts, so far, focus on the deployment of these models for diagnosis and classification  In the machine learning domain, patient prognostication can be treated as a weakly supervised problem, which a model would predict the outcome (e.g., time to cancer recurrence) based on the histopathology images. Their majority have utilized Multiple Instance Learning (MIL)  To address this issue, graph neural networks (GNN) have recently received more attention in histology. They can model patch relations  This paper aims to investigate the potential of extracting fine and coarse features from histopathology slides and integrating them for risk stratification in cancer patients. Therefore, the contributions of this work can be summarized as: 1) a novel graph-based model for predicting survival that extracts both local and global properties by identifying morphological super-nodes; 2) introducing a fine-coarse feature distillation module with 3 various strategies to aggregate interactions at different scales; 3) outperforming SOTA approaches in both risk prediction and patient stratification scenarios on two datasets; 4) publishing two large and rare prostate cancer datasets containing more than 220 graphs for active surveillance and 240 graphs for brachytherapy cases. The code and graph embeddings are publicly available at https://github.com/pazadimo/ALL-IN 2 Related Works"
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,2.1,Weakly Supervised Learning in Histopathology,Utilizing Weakly Supervised Learning for modeling histopathology problems has been getting popular due to the high resolution of slides and substantial time and financial costs associated with annotating them as well as the development of powerful deep discriminative models in the recent years  Such models are used to perform nuclei segmentation 
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,2.2,Survival Analysis and GNNs in Histopathology,MIL-based models have been utilized for outcome prediction 
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3,Method,Figure 
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.1,Problem Formulation,"For P n , which is the n-th patient, a set of patches {patch j } M j=1 is extracted from the related whole slide images. In addition, a latent vector z j ∈ R 1×d is extracted from patch j using our encoder network (described in Sect. 3.2) that results in feature matrix Z n ∈ R M ×d for P n . Finally, a specific graph (G n ) for the n-th patient (P n ) can be constructed by assuming patches as nodes. Also, edges are connected based on the patches' k-nearest neighbour in the spatial domain resulting in an adjacency matrix A n . Therefore, for each patient such as P n , we have a graph defined by adjacency matrix A n with size M × M and features matrix Z n (G n = graph(Z n , A n )). We estimate K super-nodes as matrix S n ∈ R K×d representing groups of local nodes with similar properties as coarse features for P n 's slides. The final model ( θ ) with parameters θ utilizes G n and S n to predict the risk associated with this patient:"
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.2,Self-supervised Encoder,"Due to computational limits and large number of patches available for each patient, we utilize a self-supervised approach to train an encoder to reduce the inputs' feature space size. Therefore, We use DINO "
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.3,Local Graph Neural Network,"GNN's objective is to find new nodes' embeddings via integrating local neighbors' interactions with individual properties of patches. By exploiting the message passing mechanism, this module iteratively aggregates features from neighbors of each vertex and generates the new node representations. We employ two graph convolution isomorphism operators (GINconv)  where is a small positive value and I is the identity matrix. Also, φ denotes the weights of two MLP layers. X n ∈ R M ×d and X n ∈ R M ×d are GINconv's input and output feature matrices for P n , which X n equals Z n for the first layer."
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.4,Super-Nodes Extractor,"In order to find the coarse histo-morphological patterns disguised in the local graph, we propose extracting K Super-nodes, which each represents a weighted cluster of further processed local features. Intuitively, the number of super-nodes K should not be very large or small, as the former encourages them to only represent local clusters and the latter leads to larger clusters and loses subtle details. We exploit the minCUT  where W 1 , W 2 are MLPs' weights. Hence, the extracted nodes are directly dependent on the final survival-specific loss. In addition, two additional unsupervised weighted regularization terms are optimized to improve the process: MinCut Regularizer. This term is motivated by the original minCUT problem and intends to solve it for the the patients' graph. It is defined as: where D n is the diagonal degree matrix for A n . Also, T r(.) represents the trace of matrix and A n,norm is the normalized adjacency matrix. R minCU T 's minimum value happens when T r( Therefore, minimizing R minCU T causes assigning strongly similar nodes to a same super-node and prevent their association with others. Orthogonality Regularizer. R minCU T is non-convex and potent to local minima such as assigning all vertexes to a super-node or having multiple super-nodes with only a single vertex. R orthogonal penalizes such solutions and helps the model to distribute the graph's features between super-nodes. It can be formulated as: where ||.|| F is the Frobenius norm, and I is the identity matrix. This term pushes the model's parameters to find coarse features that are orthogonal to each other resulting in having the most useful global features. Overall, utilizing these two terms encourages the model to extract supernodes by leaning more towards the strongly associated vertexes and keeping them against weakly connected ones [5], while the main survival loss still controls the global extraction process."
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,3.5,Fine-Coarse Distillation,"We propose our fine-coarse morphological feature distillation module to leverage all-scale interactions in the final prediction by finding a local and a global patientlevel representations ( ĥl,n , ĥg,n ). Assume that X n ∈ R M ×d and S n ∈ R K×d are the feature matrices taken from local GNN (Sect. 3.3) and super-nodes for P n , respectively. We explore 3 different attention-based feature distillation strategies for this task, including: -Dual Attention (DA): Two gated self-attention modules for local and global properties with separate weights (W φ,l , W φ,g , W k,l , W k,g , W q,l , W q,g ) are utilized to find patches scores α l ∈ R 1×M and α g ∈ R 1×K and the final features ( ĥl,n , ĥg,n ) as: ) where x n,i and s n,i are rows of X n and S n , respectively, and the final representation ( ĥ) is generated as ĥ = cat( ĥl , ĥg ). -Mixed Guided Attention (MGA): In the first strategy, the information flows from local and global features to the final representations in parallel without mixing any knowledge. The purpose of this policy is the heavy fusion of fine and coarse knowledge by exploiting shared weights (W φ,shared , W k,shared , W q,shared , W v,shared ) in both routes and benefiting from the guidance of local representation on learning the global one by modifying Eq. (  -Mixed Co-Attention (MCA): While the first strategy allows the extreme separation of two paths, the second one has the highest level of mixing information. Here, we take a balanced policy between the independence and knowledge mixture of the two routes by only sharing the weights without using any guidance."
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,4,Experiments and Results,
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,4.1,Dataset,"We utilize two prostate cancer (PCa) datasets to evaluate the performance of our proposed model. The first set (PCa-AS) includes 179 PCa patients who were managed with Active Surveillance (AS). Radical therapy is considered overtreatment in these patients, so they are instead monitored with regular serum prostate-specific antigen (PSA) measurements, physical examinations, sequential biopsies, and magnetic resonance imaging  The second dataset (PCa-BT) includes 105 PCa patients with low to high risk disease who went through brachytherapy. This treatment involves placing a radioactive material inside the body to safely deliver larger dose of radiation at one time "
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,4.2,Experiments,"We evaluate the models' performance in two scenarios utilizing several objective metrics. Implementation details are available in supplementary material. Hazard (Risk) Prediction. We utilize concordance-index (c-index) that measures the relative ordering of patients with observed events and un-censored cases relative to censored instances  Patient Stratification. The capacity of stratifying patients into risk groups (e.g., low and high risk) is another criterion that we employ to assess the utility of models in clinical practice. We evaluate model performances via Kaplan-Meier curve  Ablation Study. We perform ablation study (Table "
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,,Fig. 1 .,
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,,Fig. 2 .,
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,,Table 1 .,
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,,Table 2 .,
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 74.
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,1,Introduction,"Bone marrow (BM) lineage classification and differential counting are at the very base of most diagnoses or clinical management of hematopoietic disorders  In order to provide a more quantitative BM cell characterization, different strategies have been proposed, either extracting hand-crafted features or applying deep learning based approaches. Importantly, most of them has been focused on leukemia detection  Unlike previous works, the presented approach is focused in quantifying changes in the most common pathological atypical BM cell subtypes, namely myelocytes (MYB), blasts (BLA), promyelocytes (PMO), and erythroblast (EBO), achieving a quantitative strategy to differentiate atypical BM cells regardless the disease they are associated with. Differentiating these classes is crucial since the proposed 4 cell subtypes are the ones which must be counted for a correct diagnosis and morphological characterization of acute myeloid leukemia and myelodysplasic disease "
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,2,Materials and Methods,
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,2.1,The BM Smear Image Dataset,"All experiments in the presented work were performed using a subset of the public image database ""An Expert-Annotated Dataset of Bone Marrow Cytology in Hematologic Malignancies"" "
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,,Data Use Declaration.,"The complete version of the public database used for validating the proposed strategy is provided by Matek et. al., under the TCIA Data Usage Policy and Restrictions, and it is publicly available at TCIA platform, https://doi.org/10.7937/TCIA.AXH3-T579. No ethical compliance statement is presented in this document since it is covered by the original dataset publication "
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,2.2,OCVAE Atypical BM Cell Differentiation Method,"Single Cell Image Pre-processing. The first step of the presented approach is to apply a color-space transformation from RGB to Lab, for reducing possible illumination and stain variability effects. In the Lab color representation, the luminance component is isolated in the L-channel, and the remaining components (a and b) are more robust to the mentioned sources of variability  Atypical BM Cell OCVAE Latent Space Representation. The main part of the introduced atypical BM cell characterization is based on a latent space representation of a variational autoencoder (VAE) bottleneck  . The latent space is described by a set of parameterized distributions (mean-μ and variance-σ), and imposes these distributions to be as close as possible to unitary Gaussians. This guarantees a continuous approximation of the latent space by the Parzen theorem, i.e. z∼ N (0, 1), in terms of the Kullback-Leibler divergence (KLD) between the parametric posterior and the true posterior distributions. Furthermore, to enhance VAE cell description, the presented approach takes advantage of the dedicated one-class characterization to force inter-class separability but increasing intra-class proximity. One-class classification strategies have been mainly used to find outlier samples in a given data space  In a more detailed description, the implemented VAE encoder is composed of 3 convolutional and 2 max-pooling, intercalated layers, followed by a flatten, a fully connected, and a lambda layer which is customized for sampling the latent space in terms of means μ and variances σ. Here the bottleneck is set to compress the input layer dimension (256 × 256 × 1) in terms of 64 Gaussian distributions, i.e., μ i:{1-64} and σ i:{1-64} . The decoder architecture follows similar encoder's layer organization (3 convolutional and 3 up-sampling layers), but returning the original dimensionality to the reconstructed images. Evaluation. The introduced concatenated OCVAE representation is quantitatively evaluated by classifying the four proposed classes (PMO, BLA, MYB, EBO). The OCVAE cascade training is carried out by using 80% of the dataset (n = 20, 800), i.e., parameters of each specialized OCVAE model are found by using 5, 200 images coming from each of the proposed classes. The remaining 20% of the dataset (n = 5, 200), composed of equal number of cells per class (n = 1, 300), is used to obtain the feature space for evaluating the presented methodology while assuring independence to the parameterization image set. This independent data partition feeds the previously obtained specialized OCVAE encoder models, and the bottleneck values are concatenated to build an atypical BM cell representation. Afterward, the feature concatenation is used to train three different classifiers (SVM with linear and RBF kernels, and Random Forest) for differentiating cell types that compose the sample space. This experiment uses five iterations of a five-fold cross validation over the obtained feature space (20% of images), for reducing possible batch effect. Mean accuracy, precision, recall and f1-score, with their correspondent standard deviations, are presented as performance metrics. Finally, the best classifier, selected based on the performance metrics, is optimized by using different OCVAE parameter combinations as inputs, i.e., latent-space distribution means together with the corresponding variances, only means or only variances. A second experiment is done to provide a baseline comparison in classifying the 4 atypical cell classes, by using different strategies reported in the literature for this task, including classical handcrafted image features and deep neural networks. For this evaluation procedure, classical image processing descriptors were included for providing a classification strategy that depends on both, the interpretability of the feature representation space and the classifier, like the one introduced in this work. This handcrafted representation comprises a set of 144 image features obtained from nucleus and cytoplasm as separated cell elements, and includes RGB-color space intensity statistics (mean, variance, kurtosis, skweness, entropy), Gray-level-Co-occurrence matrix statistics (contrast, dissimilarity, homogeneity, energy, correlation, angular second moment, Minkowski-Bouligand dimension), and shape descriptors (convexity, compactness, elongation, eccentricity, roundness, solidity, area, perimeter). All these features were used to train different classifiers from which an SVM classifier (linear kernel) provides the best performance with the proposed setup. Regarding deep learning approximations, different architectures were used, including two bench marking options Xception "
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,3,Results and Discussion,"The results of the first experiment are shown in Table  Finally, the baseline experiment results, presented in the table 2, demonstrates the performance achieved by ResNet50 (accuracy = 0.624, precision = 0.24, recall = 0.25 and f1-score = 0.248) and Xception network (accuracy = 0.708, precision = 0.697, recall = 0.899, f1-score = 0.72) are lower than the obtained with the proposed strategy. In contrast, handcrafted features (accuracy = 0.843, precision = 0.688, recall = 0.684, f1-score = 0.685) ResNext network (accuracy = 0.79, precision = 0.78, recall = 0.74, f1-score = 0.72), and   transformer-integrated network (CoAtnet-20, 000 images per class), according to the corresponding authors report  Besides the classification performance, it is important to highlight that these results are obtained by the OCVAE bottleneck regularization, which reduces variance inflation and maximises the compactness of the feature space "
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,4,Conclusions,"This work presents an atypical BM cell characterization strategy, which uses a concatenation of OCVAE's bottleneck parameters as a cell representation space. The combination of this space and an SVM classifier, demonstrates to successfully discriminate the 4 most common atypical cell types (PMO, BLA, MYO, EBO), while outperforms previously published strategies, with lower requirement of training images. This approach provides a tool for identifying these cell classes regardless the disease they are coming from, increasing the possibility of aiding differential blood counting in the presence of pathological conditions. Future work includes an independent evaluation by using other public databases and a more exhaustive baseline experimentation."
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,,Fig. 1 .,
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,,Fig. 2 .,
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,,Table 1 .,
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,,SVM linear kernel 0.881 (0.006) 0.882 (0.005) 0.881 (0.006) 0.882 (0.005),
A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types,,Table 2 .,
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,1,Introduction,"Medical image segmentation has been one of the key aspects in developing automated assisted diagnosis systems, which aims to separate objects or structures in medical images for independent analysis and processing. Normally, segmentation needs to be performed manually by professional physicians, which is time-consuming and error-prone. In contrast, developing computer-aided segmentation algorithms can be faster and more accurate for batch processing. The approach represented by U-Net  Skip connection is the most basic operation for fusing shallow and deep features in U-shaped networks. Considering that this simple fusion does not fully exploit the information, researchers have proposed some novel ways of skip connection  By reviewing the above multiple successful cases based on U-shaped structure, we believe that the efficiency and performance of U-shaped networks can be improved by improving the following two aspects: (i) local-global interactions. Often networks need to deal with objects of different sizes in medical images, and local-global interactions can help the network understand the content of the images more accurately. (ii) Spatial connection between encoder-decoder. Semantically stronger and positionally more accurate features can be obtained using the spatial information between encoder-decoders. Based on the above analysis, this paper rethinks the design of the U-shaped network. Specifically, we construct lightweight SegNetr (Segmentation Network with Transformer) blocks to dynamically learn local-global information over non-overlapping windows and maintain linear complexity. We propose information retention skip connection (IRSC), which focuses on the connection between encoder and decoder spatial locations, retaining more original features to help recover the resolution of the feature map in the up-sampling phase. In summary, the contributions of this paper can be summarized as follows: 1) We propose a lightweight U-shape SegNetr segmentation network with less computational cost and better segmentation performance. 2) We investigate the potential deficiency of the traditional U-shaped framework for skip connection and improve a skip connection with information retention. 3) When we apply the components proposed in this paper to other U-shaped methods, the segmentation performance obtains a consistent improvement. "
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,2,Method,As shown in Fig. 
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,2.1,SegNetr Block,"The self-attention mechanism with global interactions is one of the keys to Transformer's success, but computing the attention matrix over the entire space requires a quadratic complexity. Inspired by the window attention method  Local interaction can be achieved by calculating the attention matrix of non-overlapping small patches (P for patch size). First, we divide X MBConv into a series of patches ( H×W P ×P , P, P, C) that are spatially continuous (Fig.  Considering that local interactions are not sufficient and may have underfitting problems, we also design parallel global interaction branches. First, we use the global partition (GP) operation to aggregate non-contiguous patches on the space. GP adds the operation of window displacement to LP with the aim of changing the overall distribution of features in space (The global branch in Fig.  The local and global branches are finally fused by weighted summation, before which the feature map shape needs to be recovered by LP and GP reversal operations (i.e., local reverse (LR) and global reverse (GR)). In addition, our approach also employs efficient designs of Transformer, such as Norm, feedforward networks (FFN) and residual connections. Most Transformer models use fixed-size patches "
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,2.2,Information Retention Skip Connection,"Figure  . The essence of the PM operation is to convert the information in the spatial dimension into a channel representation without any computational cost and retaining all the information of the input features. The patch reverse (PR) in IRSC is used to recover the spatial resolution of the encoder, and it is a reciprocal operation with PM. We alternately select half the number of channels of X P M (i.e., H 2 × W 2 × 2C) as the input of PR, which can reduce the redundant features in the encoder on the one hand and align the number of feature channels in the decoder on the other hand. PR reduces the problem of information loss to a large extent compared to traditional upsampling methods, while providing accurate location information. Finally, the output features X P R ∈ R H×W × C 2 of PR are fused with the up-sampled features of the decoder for the next stage of learning."
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,3,Experiments and Discussion,"Datasets. To verify the validity of SegNetr, we selected four datasets, ISIC2017  Implementation Details. We implement the SegNetr method based on the PyTorch framework by training on an NVIDIA 3090 GPU with 24 GB of memory. Use the Adam optimizer with a fixed learning rate of 1e-4. All networks use a cross-entropy loss function and an input image resolution of 224 × 224, and training is stopped when 200 epochs are iteratively optimized. We use the source code provided by the authors to conduct experiments with the same dataset, and data enhancement strategy. In addition, we use the IoU and Dice metrics to evaluate the segmentation performance, while giving the number of parameters and GFLOPs for the comparison models.   In addition, Fig. "
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,3.2,Ablation Study,"Effect of Local-Global Interactions. The role of local-global interactions in SegNetr can be understood from Table  Effect of Patch Size. As shown in Table  Based on this ablation study, we recommend the use of [ Resolution   14   ] patches size at different stages. Effect of IRSC. Table "
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,4,Conclusion,"In this study, we introduce a novel framework SegNetr for medical image segmentation, which achieves segmentation performance improvement by optimizing local-global interactions and skip connections. Specifically, the SegNetr block implements dynamic interactions based on non-overlapping windows using parallel local and global branches, and IRSC enables more accurate fusion of shallow and deep features by providing spacial information. We evaluated the proposed method using four medical image datasets, and extensive experiments showed that SegNetr is able to obtain challenging experimental results while maintaining a small number of parameters and GFLOPs. The proposed framework is general and flexible that we believe it can be easily extended to other U-shaped networks."
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,,Fig. 1 .,
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,,Fig. 2 .,
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,,Fig. 3 .,
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,,Table 1 .,
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,,Table 2 .,
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,,Table 3 .,
SegNetr: Rethinking the Local-Global Interactions and Skip Connections in U-Shaped Networks,,Table 4 .,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,1,Introduction,"Automatic nucleus segmentation has captured wide research interests in recent years due to its importance in pathological image analysis  In recent years, the research on domain generalization (DG) has attracted wide attention. Most existing DG works are proposed for classification tasks  It is a consensus that a generalizable nucleus segmentation model should be robust to image appearance variation caused by the change in staining protocols, scanner types, and tissues, as illustrated in Fig. "
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,2,Method,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,2.1,Overview,"In this paper, we adopt a U-Net-based model similar to that in  Our proposed Distribution-Aware Re-Coloring model (DARC) is illustrated in Fig. "
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,2.2,Nucleus Image Re-Coloring,"We propose the Re-Coloring (RC) method to overcome the color change in different domains. Specifically, given a RGB image I, e.g., an H&E or IHC stained image, we first obtain its grayscale image I g . We then feed I g into a simple module T that consists of a single residual block and a 1 × 1 convolutional layer with output channel number of 3. In this way, we obtain an initial re-colored image I r . However, de-colorization results in the loss of fine-grained textures and may harm the segmentation accuracy. To handle this problem, we compensate I r with the original semantic information contained in I. Recent works "
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Algorithm 1. Re-Coloring,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Input:,"The input RGB image I ∈ R H×W ×3 ; The module T whose input and output channel numbers are 1 and 3, respectively; Output: The re-colored image Io ∈ R AssignV alue denotes re-assembling the sorted values according to the provided indices. Details of the module T are included in the supplementary material. Via RC, the original fine-grained structure information from I g is recovered in I r . In this way, the re-colored image is advantageous in two aspects. First, the appearance difference between pathological images caused by the change in scanners and staining protocols is eliminated. Second, the re-colored image preserves fine-grained structure information, enabling precise instance segmentation to be possible."
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,2.3,Distribution-Aware Instance Normalization,"Due to dramatic domain gaps, feature statistics may differ significantly between domains "
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Algorithm 2. Distribution-Aware Instance Normalization,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Input:,"Original feature maps X ∈ R H×W ×C . The C-dimensional feature vector on its pixel (i, j) is denoted as xij; The modules Eμ and E δ that re-estimate feature statistics; Δsra ∈ R 1×1×C that is obtained via running mean of Δs in the training stage; The momentum factor α used to update Δsra ; (Optional) Δs = f (ρ); Output: Normalized feature maps Y ∈ R H×W ×C ; To verify the above viewpoint, we evaluate the baseline model under different foreground-background ratios. Specifically, we first remove the foreground pixels via in-painting  The above problem is common in nucleus segmentation because pathological images from different organs or tissues tend to have significantly different foreground-background ratios. However, this phenomenon is often ignored in existing research. To handle this problem, we propose the Distribution-Aware Instance Normalization (DAIN) method to re-estimate feature statistics that account for different ratios of foreground and background pixels. Details of DAIN is presented in Algorithm 2. The structures of E μ and E δ are included in the supplemental materials. As shown in Fig.  The training of RPH requires an extra loss term L rph , which is formulated as bellow: where ρ g denotes the ground truth foreground-background ratio, and L BCE and L MSE denote the binary cross entropy loss and the mean squared error, respectively."
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,3,Experiments,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,3.1,Datasets,"The proposed method is evaluated on four datasets, including two H&E stained image datasets CoNSeP "
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,3.2,Implementation Details,"In the training stage, patches of size 224 × 224 pixels are randomly cropped from the original samples. During training, the batch size is 4 and the total number of training iterations is 40,000. We use Adam algorithm for optimization, and the learning rate is initialized as 1e -3 , which is gradually decreased to 1e -5 during training. We adopt the standard augmentation, like image color jittering and Gaussian blurring. In all experiments, the segmentation and contour detection predictions are penalized using the binary cross entropy loss. "
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,3.3,Experimental Results and Analyses,"In this paper, the models are compared using the AJI  In this paper, we re-implement some existing popular domain generalization algorithms for comparisons under the same training conditions. Specifically, we re-implement the TENT  We separately evaluate the effectiveness of RC and DAIN, and present the results in Table "
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,4,Conclusion,"In this paper, we propose the DARC model for generalizable nucleus segmentation. To handle the domain gaps caused by varied image acquisition conditions, DARC first re-colors the input image while preserving its fine-grained structures as much as possible. Moreover, we find that the performance of instance normalization is sensitive to the varied ratios in foreground and background pixel numbers. This problem is well addressed by our proposed DAIN. Compared with existing works, DARC achieves significantly better performance on average across four benchmarks."
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Fig. 1 .,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Fig. 2 .,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Fig. 3 .,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Table 1 .,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Table 2 .,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Table 3 .,
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 57.
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,1,Introduction,"Accurate spatial characterization of tumor immune microenvironment is critical for precise therapeutic stratification of cancer patients (e.g. via immunotherapy). Currently, this characterization is done manually by individual pathologists on standard hematoxylin-and-eosin (H&E) or singleplex immunohistochemistry (IHC) stained images. However, this results in high interobserver variability among pathologists, primarily due to the large (> 50%) disagreement among pathologists for immune cell phenotyping  In this paper, we introduce a new dataset that can be readily used out-ofthe-box with any artificial intelligence (AI)/deep learning algorithms for spatial characterization of tumor immune microenvironment and several other use cases. To date, only two denovo stained datasets have been released publicly: BCI H&E and singleplex IHC HER2 dataset "
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2,Dataset,The complete staining protocols for this dataset are given in the accompanying supplementary material. Images were acquired at 20× magnification at Moffitt Cancer Center. The demographics and other relevant information for all eight head-and-neck squamous cell carcinoma patients is given in Table 
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.1,Region-of-Interest Selection and Image Registration,"After scanning the full images at low resolution, nine regions of interest (ROIs) from each slide were chosen by an experienced pathologist on both mIF and mIHC images: three in the tumor core (TC), three at the tumor margin (TM), and three outside in the adjacent stroma (S) area. The size of the ROIs was standardized at 1356×1012 pixels with a resolution of 0.5 µm/pixel for a total surface area of 0.343 mm 2 . Hematoxylin-stained ROIs were first used to align all the mIHC marker images in the open source Fiji software using affine registration. After that, hematoxylin-and DAPI-stained ROIs were used as references to align mIHC and mIF ROIs again using Fiji and subdivided into 512×512 patches, resulting in total of 268 co-registered mIHC and mIF patches (∼33 co-registered mIF/mIHC images per patient)."
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.2,Concordance Study,We compared mIF and mIHC assays for concordance in marker intensities. The results are shown in Fig. 
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,3,Use Cases,"In this section, we demonstrate some of the use cases enabled by this high-quality AI-ready dataset. We have used publicly available state-of-the-art tools such as Adaptive Attention Normalization (AdaAttN) "
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,3.1,IHC CD3/CD8 Scoring Using mIF Style Transfer,We generate a stylized IHC image (Fig.  (a) Marker Generation: This sub-network is used for generating mIF marker data from the generated stylized image. We use a conditional generative adversarial network (cGAN) 
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,(b) Style Transfer:,"This sub-network creates the stylized IHC image using an attention module, given (1) the input hematoxylin and the mIF marker images and (2) the style and its corresponding marker images. For synthetically generating stylized IHC images, we follow the approach outlined in AdaAttN  For the stylized IHC images with ground truth CD3/CD8 marker images, we also segmented corresponding DAPI images using our interactive deep learning ImPartial  We evaluated the effectiveness of our synthetically generated dataset (stylized IHC images and corresponding segmented/classified masks) using our generated dataset with the NuClick training dataset (containing manually segmented CD3/CD8 cells)  We also tested the trained models on 1,500 randomly selected images from the training set of the Lymphocyte Assessment Hackathon (LYSTO) "
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,3.2,Virtual Translation of Cheap mIHC to Expensive mIF Stains,"Unlike clinical DAB staining, as shown in style IHC images in Fig. "
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,3.3,Virtual Cellular Phenotyping on Standard Hematoxylin Images,There are several public H&E/IHC cell segmentation datasets with manual immune cell annotations from single pathologists. These are highly problematic given the large (> 50%) disagreement among pathologists on immune cell phenotyping 
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,4,Conclusions and Future Work,"We have released the first AI-ready restained and co-registered mIF and mIHC dataset for head-and-neck squamous cell carcinoma patients. This dataset can be used for virtual phenotyping given standard clinical hematoxylin images, virtual clinical IHC DAB generation with ground truth segmentations (to train highquality segmentation models across multiple cancer types) created from cleaner mIF images, as well as for generating standardized clean mIF images from neighboring H&E and IHC sections for registration and 3D reconstruction of tissue specimens. In the future, we will release similar datasets for additional cancer types as well as release for this dataset corresponding whole-cell segmentations via ImPartial https://github.com/nadeemlab/ImPartial."
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Data use Declaration and Acknowledgment:,"This study is not Human Subjects Research because it was a secondary analysis of results from biological specimens that were not collected for the purpose of the current study and for which the samples were fully anonymized. This work was supported by MSK Cancer Center Support Grant/Core Grant (P30 CA008748) and by James and Esther King Biomedical Research Grant (7JK02) and Moffitt Merit Society Award to C. H. Chung. It is also supported in part by the Moffitt's Total Cancer Care Initiative, Collaborative Data Services, Biostatistics and Bioinformatics, and Tissue Core Facilities at the H. Lee Moffitt Cancer Center and Research Institute, an NCI-designated Comprehensive Cancer Center (P30-CA076292)."
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Fig. 1 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Fig. 2 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Fig. 3 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Fig. 4 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Fig. 5 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Fig. 6 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Table 1 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Table 2 .,
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 68.
Detection of Basal Cell Carcinoma in Whole Slide Images,1,Introduction,"Skin cancer, the most prevalent cancer globally, has seen increasing incidences over recent decades  In this study, we utilized the NAS approach to identify the optimal network for skin cancer detection. To improve the efficiency and accuracy of the search, we developed a new framework named SC-net, which focuses on identifying highly valuable architectures. We observed that conventional NAS methods often overlook fairness ranking during the search, hindering the search for optimal solutions. Our SC-Net framework addresses this by ensuring fair training and precise ranking. The efficacy of SC-net was confirmed by our experimental results, with our ResNet50 achieving 96.2% top-1 accuracy and 96.5% AUC, outperforming baseline methods by 4.8% and 4.7% respectively. Figure "
Detection of Basal Cell Carcinoma in Whole Slide Images,2,Methods,The proposed method (Fig. 
Detection of Basal Cell Carcinoma in Whole Slide Images,2.1,One-Shot Channel Number Search,"To extract an optimal architecture γ ∈ G from a vast search space G, a weightsharing strategy  where U (D) is a uniform distribution of network widths, E is the expected value of random variables, and L t is the training loss function. Then, the optimal network width d * corresponds to the network width with the best performance (e.g. classification accuracy) on the validation dataset, i.e., where F p is the resource budget of FLOPs. The search for Eq. 2 can be efficiently performed by various algorithms, such as random or evolutionary search "
Detection of Basal Cell Carcinoma in Whole Slide Images,2.2,SC-Net as a Balanced Supernet,"Current approaches for neural architecture search  where γ A (d) means the selected d channels from the left (smaller-index) side. However, the UA principle leads to channel training imbalance in the supernet due to its constraints, as illustrated in Fig.  Correspondingly, the probability of i -th channel being trained can be expressed as P i = n-i+1 n . Therefore, channels closer to the left will get more attempts during training, which leads the degree of training to vary widely between channels. This introduces evaluation bias and leads to sub-optimal results. To mitigate evaluation bias on width, we propose a new SC-net that promotes the fairness of channels during training. As shown in Fig.  where represents the union of two lists with repeatable elements. In detail, the left channel in S l follows the same UA principle setup as in Eq. (  (nd)]. Therefore, the training degree of each channel is the sum of the two supernets S l and S r . Since the channels are counted from the right within S r , the training degree of the d-th channel on the left corresponds to the training degree of the (n-d+1)-th channel on the right in Eq. (  Therefore, the training degree T for each channel will always be equal to the same constant value of the width, independent of the channel index, ensuring fairness in terms of channel (filter) levels. Thus the network width can be fairly ranked using our network."
Detection of Basal Cell Carcinoma in Whole Slide Images,2.3,Balanced Evolutionary Search with SC-Net,"Using a trained SC-net, the architecture can be evaluated. However, the search space involved in NAS is large, with more than 10 20 possible architectures, requiring an evolutionary search using the multi-objective NSGA-II algorithm to improve the search performance. During the evolutionary search, the width d of each network is represented by the average precision of its corresponding left and right paths in the supernet S, as shown in Eq. ( "
Detection of Basal Cell Carcinoma in Whole Slide Images,,"Acc(W, d, D","3 Experiments The dataset, comprised of 194 skin slides acquired from the Southern Sun Pathology laboratory, includes 148 BCC cases and 46 other types (common nevus, SCC), all manually annotated by a dermatopathologist. BCC slides served as positive samples and the rest as negatives. These slides were scanned at ×20 magnification with a 0.44 µm pixel size using a Leica Aperio AT2 Scanner. The patient data were separated between training and testing to prevent overlap. Details are shown in Table "
Detection of Basal Cell Carcinoma in Whole Slide Images,3.1,Experiment Settings,
Detection of Basal Cell Carcinoma in Whole Slide Images,3.2,Performance Evaluation,"We validated our algorithm using the curated skin cancer dataset and SC-net as a supernet, testing both heavy and light models. We performed a search on ResNet50 and MobileNetV2 models, compared against original ResNet50 (ori ResNet50) and MobileNetV2 (ori MobileNetV2) models as baselines. The resulting models are denoted as s ResNet50 and s MobileNetV2. Comparison with Related Methods. To ensure a fair comparison on our dataset, we selected several papers in the field of pathological image analysis, such as  Evaluation Metrics. Our model was evaluated on: As shown in Table "
Detection of Basal Cell Carcinoma in Whole Slide Images,3.3,Ablation Study,Effect of SC-net as a Supernet.  
Detection of Basal Cell Carcinoma in Whole Slide Images,4,Conclusion and Future Work,"In this paper, we introduce SC-net, a novel NAS framework for skin cancer detection in pathology images. By formulating SC-net as a balanced supernet, we ensure fair ranking and treatment of all potential architectures. With SCnet and evolutionary search, we obtained optimal architectures, achieving 96.2% Top-1 and 96.5% accuracy on a skin cancer dataset, improvements of 4.8% and 4.7% over baselines. Future work will apply our approach to larger datasets for wider-scale validation."
Detection of Basal Cell Carcinoma in Whole Slide Images,5,Compliance with Ethical Standards,"This study was performed in line with the principles of the Declaration of Helsinki. Ethics approval was granted by CSIRO Health and Medical Human Research Ethics Committee (CHMHREC). The ethics approval number is 2021 030 LR, and the validity period is from 07 Apr 2021 to 31 Dec 2024."
Detection of Basal Cell Carcinoma in Whole Slide Images,,Fig. 1 .,
Detection of Basal Cell Carcinoma in Whole Slide Images,,Fig. 2 .,
Detection of Basal Cell Carcinoma in Whole Slide Images,,Fig. 3 .,
Detection of Basal Cell Carcinoma in Whole Slide Images,,Fig. 4 .,
Detection of Basal Cell Carcinoma in Whole Slide Images,,( 1 ),
Detection of Basal Cell Carcinoma in Whole Slide Images,,Table 1 .,
Detection of Basal Cell Carcinoma in Whole Slide Images,,Table 2 .,
Detection of Basal Cell Carcinoma in Whole Slide Images,,Table 3 .,
Detection of Basal Cell Carcinoma in Whole Slide Images,,Table 3,
Detection of Basal Cell Carcinoma in Whole Slide Images,,Table 4 .,
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,1,Introduction,"Pathology is widely recognized as the gold standard for disease diagnosis  Current MIL methods can be broadly categorized into two types: bag-level MIL and instance-level MIL. Bag-level MIL  Considering these conventional MIL methods usually utilize either bag-level or instance-level supervision, leading to suboptimal performance. In this paper, we format the instance-level MIL as a noisy label learning task and propose to solve it by designing an instance-level supervision based on the label disambiguation  1) We propose a novel MIL method for pathological image analysis that leverages a specially-designed residual Transformer backbone and organically integrates both Transformer-based bag-level and label-disambiguation-based instancelevel supervision for performance enhancement. 2) We develop a label-disambiguation module that leverages prototypes and confidence bank to tackle the weakly supervised nature of instance-level supervision and reduce the impact of assigned noisy labels. 3) The proposed framework outperforms state-of-the-art (SOTA) methods on public datasets and in a practical clinical task, demonstrating its superiors in WSI analysis. Besides, ablation studies illustrate the superiority of our co-supervision design compared to using only one type of supervision. 2 Method"
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,2.1,Overview,The overall framework of the proposed IIB-MIL is shown in Fig. 
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,2.2,Problem Formulation,"Assume there is a set of N WSIs denoted by S = {S 1 , S 2 , ..., S N }. Each WSI S i has a WSI-level label Y i ∈ {1, ..., C}, where C represents category number. In each S i , there exist M i tiled patches without patch-level labels. To reduce the computational cost, we used a frozen pre-trained encoder to transform patches into K dimensional embeddings {e i, Our proposed IIB-MIL comprehensively integrates obtained embeddings {e i,j , ...} to generate accurate WSI classification."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,2.3,Backbone Network,"Before bag-level and instance-level supervision channels, we design a residual transformer backbone T (•) : R K → R D to calibrate the obtained patch embeddings and encode the context information and correlation of patches. T (•) maps patch embeddings {e i,j , ...} to a lower-dimensional feature space, denoted as {x i,j , ...}, where x i,j = T (e i,j ), x i,j ∈ R D is the calibrated embedding, T (•) is composed of transformer layers and skip connections (Details are given in the supplementary.)."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,2.4,Instance-Level Supervision,"At the core of instance-level supervision is the label disambiguation module, which serves to rectify the imprecise labels that have been assigned to patches. It comprises prototypes and a confidence bank, takes instance features and instance classifier predictions as inputs, and generates soft labels as outputs (Fig.  Step 1: Obtain the instance classifier output. The instance-level classifier, denoted as F inst (•), takes x i,j ∈ R D as input and outputs the predicted instance probability prob inst i,j ∈ R C , as: The probability that x i,j is predicted as class c is denoted as Step 2: Obtain the prototype labels. At t time, the prototype vector for the category c is P c,t ∈ R D . To update P c,t , we select a set of instance features Set c,t that have the highest probabilities prob inst i,j,c of belonging to category c. Specifically, we define Set c,t as: where K is the number of top instance features to select. Then, we use a momentum-based update rule to obtain P c,t+1 : where α is the momentum coefficient that automatically decreases from α = 0.95 to α = 0.8 across epochs. Then, we can obtain prototype labels z i,j ∈ R C using the following equation: The resulting prototype label z i,j ∈ R C is a one-hot vector that indicates the category of the j-th instance in the i-th WSI. Step 3: Obtain Soft Labels from the Confidence Bank Specifically, at time t, the pseudo-target B i,j,t ∈ R C of the instance embedding e i,j is updated by the following: where β is the momentum update parameter with a default value of β = 0.99. Step 4: Calculate Instance-Level Loss. We compute instance-Level Loss using the cross-entropy function: Here, B i,j,c and prob inst i,j,c are the c-th component of the pseudo-target B i,j and predicted probability prob inst i,j , respectively."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,2.5,Bag-Level Supervision,"For bag-level supervision, instance features x i ∈ R M ×D go through a transformer-based aggregator A(•) : R M ×D → R D and a WSI classifier F bag (•) : R D → R C in turn (Architecture details are given in the supplementary.). Then we obtain the predicted probability of WSI S i as: The bag-level loss function is given by: where Y i ∈ R C is the label of WSI S i ."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,2.6,Training,"In the training phase, We employ a warm-up strategy in which we update only the Prototypes and do not update the Confidence Bank during the first few epochs. Our approach is trained end-to-end, and the total loss function is : where λ is the hyperparameter that controls the relative importance of the two losses."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,3,Experiments,
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,3.1,Dataset,"We evaluate our model with three datasets. (1) LUAD-GM Dataset: The objective is to predict the epidermal growth factor receptor (EGFR) gene mutations in patients with lung adenocarcinoma (LUAD) using 723 Whole Slide Image (WSI) slices, where 47% of cases have EGFR mutations. (2) TCGA-NSCLC and TCGA-RCC Datasets: Cancer type classification is performed using The Cancer Genome Atlas (TCGA) dataset. The TCGA-NSCLC dataset comprised two subtypes, lung squamous cell carcinoma (LUSC) and lung adenocarcinoma (LUAD), while the TCGA-RCC dataset included three subtypes: renal chromophobe cell carcinoma (KICH), renal clear cell carcinoma (KIRC), and renal papillary cell carcinoma (KIRP)."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,3.2,Experiment Settings,"The dataset was randomly split into three parts: training, validation, and testing, with 60%, 20%, and 20% of the samples, respectively. WSIs were preprocessed by cropping them into 1120 × 1120 patches, without overlap. The proposed model was implemented in Pytorch, trained on a 32GB TESLA V100 GPU, using AdamW "
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,4.2,Ablation Studies,"We conducted ablation studies to assess the efficacy of each component in IIB-MIL. The results, in Table "
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,4.3,Model Interpretation,Figure 
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,5,Conclusion,"This paper presents IIB-MIL, a novel MIL approach for pathological image analysis. IIB-MIL utilizes a label disambiguation module to establish more precise instance-level supervision. It then combines the instance-level and bag-level supervision to enhance the performance of the IIB-MIL. Experimental results demonstrate that IIB-MIL surpasses current SOTA techniques on publicly available datasets, and holds significant potential for addressing more complex clinical applications, such as predicting gene mutations. Furthermore, IIB-MIL can identify highly relevant patches, providing pathologists with valuable insights into underlying mechanisms."
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,,Fig. 1 .,
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,,Fig. 2 .,
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,,Table 1 .,
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,,Table 2 .,
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,,Results and Discussion 4.1 Comparison with State-of-the Art Methods,
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 54.
Multi-scale Prototypical Transformer for Whole Slide Image Classification,1,Introduction,"Histopathological images are regarded as the 'gold standard' in the diagnosis of cancers. With the advent of the whole slide image (WSI) scanner, deep learning has gained its reputation in the field of computational pathology  To address this issue, multiple instance learning (MIL) has been successfully applied to the WSI classification task as a weakly supervised learning problem  Recently, prototypical learning is applied in WSI analysis to identify representative instances in the bag  On the other hand, when pathologists analysis the WSIs, they always observe the tissues at various resolutions  In this work, we propose a Multi-Scale Prototypical Transformer (MSPT) for WSI classification. The MSPT includes two key components: a prototypical Transformer (PT) and a multi-scale feature fusion module (MFFM). The specifically developed PT uses a clustering algorithm to extract instance prototypes from the bags, and then re-calibrates these prototypes at each scale with the self-attention mechanism in Transformer  The contributions of this work are summarized as follows: 1) A novel prototypical Transformer (PT) is proposed to learn superior prototype representation for WSI classification by integrating prototypical learning into the Transformer architecture. It can effectively re-calibrate the cluster prototypes as well as reduce the computational complexity of the Transformer. 2) A new multi-scale feature fusion module (MFFM) is developed based on the MLP-Mixer to enhance the information communication among phenotypes. It can effectively capture multi-scale information in WSI to improve the performance of WSI classification."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,2,Method,
Multi-scale Prototypical Transformer for Whole Slide Image Classification,2.1,MIL Problem Formulation,"MIL is a typical weakly supervised learning method, where the training data consists of a set of bags, and each bag contains multiple instances. The goal of MIL is to learn a classifier that can predict the label of a bag based on the instances in it. In binary classification, a bag can be marked as negative if all in-stances in the bag are negative, otherwise, the bag is labeled as positive with at least one positive instance. In the MIL setting, a WSI is considered as a bag and the numerous cropped patches in WSI are regarded as instances in the bag. A WSI dataset T can be defined as: where x i denotes a patient, y i the label of x i , I j i is the j-th instance of x i , N is the number of patients and n is the number of instances."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,2.2,Multi-scale Prototypical Transformer (MSPT),"The overall architecture of MSPT is shown in Fig.  Pre-training. It is a time consuming and tedious task for pathologists to annotate the patch-level labels in gigapixel WSIs, thus, a common practice is to use a pre-trained encoder network to extract instance-level features, such as an ImageNet pre-trained encoder or a self-supervised pre-trained encoder. In this work, we follow "
Multi-scale Prototypical Transformer for Whole Slide Image Classification,,Prototypical Transformer (PT).,"Most tissues in WSIs are redundancy, and therefore, we introduce the instance prototypes to reduce redundant instances. Specifically, for each instance bag X bag ∈ R n×d k , the K-means clustering algorithm is applied on all instances to get K centers (prototypes). These cluster prototypes can be used as instances to represent a new bag P bag ∈ R k×d k . However, the K-means clustering algorithm is sensitive to the initial selection of cluster centers, i.e. different initializations can lead to different results, and the final result may not be the global optimal solution. It is essential to try different initializations and choose the one with the lowest error. However, the WSI dataset generally has a long sequence of instances, which makes the clustering algorithms computationally expensive and slow down as the size of the bag increases. To solve the issue above, we propose to apply the self-attention (SA) mechanism in Transformer to re-calibrate these cluster prototypes. As shown in Fig.  where W q , W k , W v ∈ R d k ×d k are trainable matrices of query P bag and the key-value pair (X bag , X bag ), respectively, and A map ∈ R k×n is the attention matrix to compute the weight of X bag . Thus, the computational complexity of SA is O(nm) instead of O n 2 , and the k is much less than n. Specifically, for a single clustering prototype p k ∈ P, the SA layer scores the pairwise similarity between p k and x n for all x n ∈ X, which can be written as a row vector [a k1 , a k2 , a k3 , . . . , a kn ] in A map . These attention scores are then weighted to X bag to update the p k ∈ R 1×d k for completing the calibration of the clustering prototypes P ∈ R k×d k . As mentioned above, existing clustering-based MIL methods use the K-means clustering to identify instances prototypes in the bag, where the most important instances that contain the key semantic information may be ignored. On the contrary, our PT can efficiently use all the instances to update the cluster prototypes multiple times. Therefore, the combination of bag instances is no longer static and fixed, but diverse and dynamic. It means that different new bags can be fed into the MFFM each time. In addition, by applying PT to each scale, the number of cluster prototypes obtained at different scales is consistent, so there is no need for additional operations to align multi-scale features. Multi-scale Feature Fusion Module (MFFM). To fuse the output clustered prototypes at different scales in MSPT, we proposed an MFFM, which consists of an MLP-Mixer and a Gated Attention Pooling (GAP). The MLP-Mixer is used to enhance the information communication of the prototype representation, and the GAP is used to get the WSI-level representation for WSI classification. As shown in Fig.  We first perform the feature concatenation operation on the multi-scale output clustering prototypes P20× , P10× , P5× to construct a feature pyramid P: where d k is the feature vector dimension of the prototypes. Then, the P is fed to the MLP-Mixer to obtain the corresponding hidden feature representation H ∈ R k×3d k as follows: where LN denotes the layer normalization, σ denotes the activation function implemented by GELU, k are the weight matrices of MLP layers.c and d s are tunable hidden widths in the token-mixing and channel-mixing MLP, respectively. Finally, the H is fed to the gated attention pooling (GAP)  where Y ∈ R 1×d out is the class label probability of the bag, and d out is the number of classes."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,3,Experiments and Results,
Multi-scale Prototypical Transformer for Whole Slide Image Classification,3.1,Datasets,"To evaluate the effectiveness of MSPT, we conducted experiments on two public dataset, namely Camelyon16 "
Multi-scale Prototypical Transformer for Whole Slide Image Classification,3.2,Experiment Setup and Evaluation Metrics,"In WSI pre-processing, each slide is cropped into non-overlapping 256 × 256 patches at different magnifications, and a threshold is set to filter out background ones. After patching, we use a pre-trained ResNet18 model to convert each 256 × 256 patch into a 512-dimensional feature vector. We selected accuracy (ACC) and area under curve (AUC) as evaluation metrics. For Camelyon16 dataset, we reported the results of the official testing set. For TCGA-NSCLC, we conducted five cross-validation on the 854 slides, and the results are reported in the format of mean ±SD (standard deviation)."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,3.3,Implementation Details,"For the feature extractor, we employed the SimCLR encoder trained by Lee et al. "
Multi-scale Prototypical Transformer for Whole Slide Image Classification,3.4,Comparisons Experiment,"Comparison Algorithms. The proposed MSPT was compared to state-of-the-art MILbased algorithms: 1) The traditional pooling operators, such as mean-pooling and maxpooling; 2) the attention-based algorithms, including ABMIL  In TCGA-NSCLC, the proposed MSPT algorithm again outperforms all the compared algorithms on all indices. It achieves the best classification performance of 0.9289 ± 0.011 and 0.9622 ± 0.015 on the ACC and AUC. Moreover, MSPT improves at least 0.78% and 1.03%, respectively, on the corresponding indices compared with all other algorithms."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,3.5,Ablation Study,"To evaluate the contribution of PT and MFFM in the proposed MSPT, we further conducted a series of ablation studies. Investigation of the Number of Prototypes in PT. To evaluate the effectiveness of the PT, we first changed the number of prototypes K in the range of {1, 2, 4, 8, 16, 32} to get the optimal K for each dataset. Then, the following two variants were compared with PT: (1) Full-bag: the first variant was only trained on all the instances; (2) Prototype-bag: the second variant was only trained on the cluster prototypes. As shown in Fig.  Table "
Multi-scale Prototypical Transformer for Whole Slide Image Classification,4,Conclusion,"In summary, we propose an MSPT for WSI classification that combine the prototypebased learning and multi-scale learning to generate powerful WSI-level representation. The MSPT reduces redundant instances in WSI bags by replacing instances with updatable instance prototypes, and avoids complicated procedures to align patch features at different scales. Extensive experiments validate the effectiveness of the proposed MSPT. In the future, we will develop an attention mechanism based on the magnification level to re-weight the features from different scales before fusion in MSPT."
Multi-scale Prototypical Transformer for Whole Slide Image Classification,,Fig. 1 .,
Multi-scale Prototypical Transformer for Whole Slide Image Classification,,Fig. 2 .,
Multi-scale Prototypical Transformer for Whole Slide Image Classification,,Fig. 3 .,
Multi-scale Prototypical Transformer for Whole Slide Image Classification,,Table 1 .,
Multi-scale Prototypical Transformer for Whole Slide Image Classification,,Table 2 .,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,1,Introduction,"Abdominal Aortic Calcification (AAC) is an established marker of atherosclerotic cardiovascular disease (CVD)  Very few attempts have been made for automated AAC scoring in VFA DXA scans  Contrastive representation learning has shown promising results in medical image classification  To this end, we propose a novel Supervised Contrastive Ordinal Loss (SCOL), considering AAC scoring as an ordinal regression problem. We integrate a labeldependent distance metric with the supervised contrastive loss. Unlike AdaCon  To the best of our knowledge, this is the first framework that explores contrastive learning for automated detection of AAC. Our contributions are summarized as follows: 1) We propose a novel supervised contrastive ordinal loss by incorporating distance metric learning with the supervised contrastive loss to improve inter-class separability and handle intra-class diversity among the AAC genera. 2) We design a Dual-Encoder Contrastive Ordinal Learning framework using the proposed loss to learn separable feature embeddings at global and local levels. 3) We achieve state-of-the-art results on two clinical datasets acquired using DXA machines from multiple manufacturers, demonstrating the generalizability and efficacy of our approach. 4) We compare the Major Adverse Cardiovascular Event (MACE) outcomes for machine-predicted AAC scores and the human-measured scores to explore the clinical relevance of our method. This work aims to contribute clinically in refining automated AAC prediction methods using low-energy VFA scans. Our code is available at "
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,2,Methodology,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,2.1,Supervised Contrastive Ordinal Learning,"Consider a training set T of M image-label pairs, such that T = {(x i , y i )} M i=1 where x i is the i th VFA DXA scan and y i is the corresponding AAC score. Let a, p and n denote the indices of anchor, positive sample, and negative samples in a batch I. P (i) is the set of indices of all the positive samples, i.e., having the same AAC score as y a and N (i) is the set of all other negative indices. Consider an encoder-projector network that maps the anchor image x a in the embedding space such that z a = P roj(Enc(x a )), then the similarity between any two projections z i and z k in the latent space is: sim(z i , z k ) = z T i .z k . Supervised contrastive loss  where τ is a scaling hyper-parameter for contrastive loss and r (a,n) is the distance metric between two labels y (a) and y (n) . If C is the number of ordinal labels in training set T, then r (a,n) is calculated as: From the above equation, it can be seen that our ordinal distance metric is monotonically increasing, i.e., if a < b < c, then r a,b must be less than r a,c and vice versa. This property allows the metric to maintain the ordinality of the data while improving the class separability in the latent space."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,2.2,Dual-Encoder Contrastive Ordinal Learning,The proposed Dual-Encoder Contrastive Ordinal Learning (DCOL) framework consists of two stages: Stage-I: contrastive ordinal feature learning and Stage-II: relevant AAC risk class prediction via AAC-24 score regression (Fig. 
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Stage-I: It consists of two modules: Local Contrastive Ordinal Learning (LCOL),"and Global Contrastive Ordinal Learning (GCOL). In these modules, we train the global and local encoder-projector networks individually in an end-to-end manner to extract contrastive feature embeddings."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Local Contrastive Ordinal Learning:,"In practice, to quantify AAC, clinicians focus on the aortic region adjacent to lumbar vertebrae L1-L4. Following this, in the Localized feature-based Contrastive Ordinal Learning module, LCOL, we integrate a simple yet effective localized attention block with the baseline encoder E l to roughly localize the aorta's position using only regression labels. This attention block is attached with E l after extracting the deep feature map f m from the last convolution layer. Our localized attention block consists of two 2D convolutional layers, followed by batch normalization and ReLu activation layers. This set of layers is then followed by an average pooling layer and sigmoid activation to create an activation map f s for the most salient features in the given image. Multiplying this activation map f s with the initial feature map f m results in extracting the most significant features from f m . These features are then projected into the latent space for processing by our SCOL loss. SCOL encourages the local contrastive embeddings with the same AAC score to move closer and the dissimilar ones apart based on the distance between their labels."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Global Contrastive Ordinal Learning: In the Global Contrastive Ordinal,"Learning module, we extract the global representation of a given VFA DXA scan. In encoder E g , we replace the fully connected layers of the vanilla CNN model with a global average pooling (GAP ) layer for feature extraction. These feature embeddings are then passed to the projection network P g . SCOL operates on projected embeddings extracted from the whole lumbar region to maximize the feature separability while preserving the ordinal information in latent space. Both projector networks, P l and P g , consist of two Dense layers with 1280 and 128 neurons each, followed by ReLu activation."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Stage-II:,"In AAC-24 score regression, a small change in pixel-level information can move the patient from low to moderate or moderate to high-risk AAC class. Thus, to decrease the effect of intra-class variations and to increase the inter-class separability, we assimilate the features extracted from encoders E l and E g . The resultant feature vector is fed as input to a feed-forward network consisting of two Dense layers with 1280 and 128 neurons each, followed by ReLu activation. Finally, a linear layer predicts the final AAC regression score. This module is trained using root mean squared error loss L rmse calculated as: , where m is the number of samples, y i are actual and y i are predicted AAC scores. The resulting AAC scores are then further classified into three AAC risk classes."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,3,Experiments and Results,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Dataset and Annotations:,"We conducted experiments on two de-identified clinical datasets acquired using the Hologic 4500A and GE iDXA scanners. Both datasets are manually annotated by clinicians using the AAC-24 scale  The Hologic Dataset  Implementation Details: Each VFA scan in both datasets contains a full view of the thoracolumbar spine. To extract the region of interest (ROI), i.e., the abdominal aorta near the lumbar spine, we crop the upper half of the image, resize it to 300×300 pixels and rescale it between 0 and 1. We apply data augmentations including rotation, shear and translation. We implement all experiments in TensorFlow "
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Evaluation Metrics:,"The performance of the AAC regression score is evaluated in terms of Pearson's correlation, while for AAC risk classification task Accuracy, F1-Score, Sensitivity, Specificity, Negative Predictive Value (NPV) and Positive Predictive Value (PPV) are used in One Vs. Rest (OvR) setting. Baseline: EfficientNet-V2S model trained in regression mode using RMSE loss. Ablation Study: Table  Comparison with the Baseline: In Table "
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Clinical Analysis and Discussion:,"To signify the clinical significance, we estimate the AUCs (Fig. For the iDXA GE Dataset, in the cohort of 1877 patients with clinical followup, 160 experienced a MACE event. The AUCs of predicted AAC-24 scores were similar AUC to human AAC-24 (0.64 95%CI 0.60-0.69 vs. 0.63 95%CI 0.59-0.68). The predicted AAC groups had 877 (46.7%), 468 (24.9%), and 532 (28.3%) of people in the low, moderate, and high AAC groups, respectively, with MACE events occurring in 5.1%, 7.5%, and 15.0% of these groups, respectively. The age and sex-adjusted HR for MACE in the moderate AAC group was 1.21 95%CI 0.77-1.89, and 2.64 95% CI 1.80-3.86 for the high AAC group, compared to the low predicted AAC group, which were similar to the HRs of human AAC groups, i.e., for moderate and high AAC groups HR 1.15 95%CI 0.72-1.84 and 2.32 95% CI 1.59-3.38, respectively, compared to the human low AAC group."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,4,Conclusion,"We propose a novel Supervised Contrastive Ordinal Loss and developed a Dualencoder Contrastive Ordinal Learning framework for AAC scoring and relevant AAC risk classification in low-energy VFA DXA scans. Our framework learns contrastive feature embeddings at the local and global levels. Our results demonstrate that the contrastive ordinal learning technique remarkably enhanced interclass separability and strengthened intra-class consistency among the AAC-24 genera, which is particularly beneficial in handling challenging cases near the class boundaries. Our framework with SCOL loss demonstrates significant performance improvements, compared to state-of-the-art methods. Moreover, the ablation studies also establish the effectiveness of our dual-encoder strategy and localized attention block. These results suggest that our approach has great clinical potential for accurately predicting AAC scores and relevant risk classes."
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Fig. 1 .,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Fig. 2 .,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Table 1 .,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Table 2 .,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Table 3 .,
SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans,,Acknowledgement and Data Use Declaration. De-identified labelled images,
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,1,Introduction,"Contrast-enhanced ultrasound (CEUS) as a modality of functional imaging has the ability to assess the intensity of vascular perfusion and haemodynamics in the thyroid nodule, thus considered a valuable new approach in the determination of benign vs. malignant nodules  Here, we propose an explanatory framework for the diagnosis of thyroid nodules based on dynamic CEUS video, which considers the dynamic perfusion characteristics and the amplification of the lesion region caused by microvessel infiltration. Our contributions are twofolds. First, the Temporal Projection Attention (TPA) is proposed to complement and interact with the semantic information of microvessel perfusion from the time dimension. Second, we adopt a group of confidence maps instead of binary masks to perceive the infiltrative expansion area from gray US to CEUS of microvessels for improving diagnosis."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,2,Method,"The architecture of the proposed framework is shown in Fig.  where ω iden , ω cls are the learnable weights."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,2.1,Temporal-Based Lesions Area Recognition (TLAR),"The great challenge of automatic recognition of lesion area from CEUS video is that the semantic information of the lesion area is different in the CEUS video of the different microvessel perfusion periods. Especially in the perfusion period and the regression period, the semantic information of lesions cannot be fully depicted in an isolated CEUS frame. Thus, the interactive fusion of semantic information of the whole microvessel perfusion period will promote the identification of the lesion area, and we design the Temporal Projection Attention (TPA) to realize this idea. We use V-Net as the backbone, which consists of four encoder/decoder blocks for TLAR, and the TPA is used in the bottleneck of the V-Net."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Temporal Projection Attention (TPA). Given a feature,"16 after four down-sampling operations in encoder, its original 3D feature map is projected  16 , and we use global average pooling (GAP) and global maximum pooling (GMP) as temporal projection operations. Here, 16 is obtained by a single convolution. This operation can also filter out the irrelevant background and display the key information of the lesions. After the temporal projection, a group convolution with a group size of 4 is employed on K to extract the local temporal attention L ∈ R C× H 16 × W 16 . Then, we concatenate L with Q to further obtain the global attention G ∈ R C×1× H 16 × W 16 by two consecutive 1 × 1 2D convolutions and dimension expend. Those operations are described as follows: where Gonv(•) is the group convolution, σ denotes the normalization, ""⊕"" is the concatenation operation. The global attention G encodes not only the contextual information within isolated query-key pairs but also the attention inside the keys  16 to enhance the feature representation. Meanwhile, to make better use of the channel information, we use 3 16 . Then, we use parallel average pooling and full connection operation to reweight the channel information of F 4th to obtain the reweighted feature The obtained global temporal fusion attention maps M are fused with the reweighted feature F 4th to get the output features F fin . Finally, we input F fin into the decoder of the TLAR to acquire the feature map of lesion."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,2.2,Microvessel Infiltration Awareness (MIA),"We design a MIA module to learn the infiltrative areas of microvessel. The tumors and margin depicted by CEUS may be larger than those depicted by gray US because of continuous infiltrative expansion. Inspired by the continuous infiltrative expansion, a series of flexible Sigmoid Alpha Functions (SAF) simulate the infiltrative expansion of microvessels by establishing the distance maps from the pixel to lesion boundary. Here, the distance maps "
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Sigmoid Alpha Function (SAF).,"It is generally believed that the differentiation between benign and malignant thyroid nodules is related to the pixels around the boundaries of the lesion  where α is the conversion factor for generating initial probability distribution P D (when α → ∞, the generated P D is binary mask); C is used to control the function value within the range of [0, 1]; (i, j) is the coordinate point in feature map; D (i, j) indicates the shortest distance from (i, j) to lesion's boundaries. Iterative Probabilistic Optimization (IPO) Unit. Based on the fact that IncepText  where ""⊕"" represents the concatenation operation;ConvBlock consists of a group of asymmetric convolutions (e.g., Conv1 × 5, Conv5 × 1 and Conv1 × 1); n denotes the number of the layers of IPO unit. With the lesion's feature map f 0 from the TLAR module, the initial distribution P D obtained by SAF is fed into the first optimize layer of IPO unit to produce the first optimized probability map p1 . Then, p1 is contacted with f 0 , and used to generate optimized probability map p2 through the continuous operation based on SAF and the second optimize layer of IPO unit. The optimized probability map pi-1 provides prior information for producing the next probability map pi . In this way, we can get a group of probability map P to aware the microvascular infiltration."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,2.3,Loss Function,"With continuous probability map P obtained from MIA, P are multiplied with the feature F cls . Then, these maps are fed into a lightweight C3D to predict the probability of benign and malignant, as shown in Fig.  where g i is the label of pi , which is generated by the operation of SAF(D (i,j) , α i ); pi denotes pixel in the image domain Ω, σ is a learnable parameter to eliminate the hidden uncertainty information. For differentiating malignant and benign, we employ a hybrid loss L total that consists of the cross-entropy loss L cls , the loss of L MSE computing optimized probability maps P , and task focus loss L ta . The L total is denoted as follows: where λ 1 , λ 2 , λ 3 are the hyper-parameters to balance the corresponding loss. As the weight parameter, we set λ 1 , λ 2 , λ 3 are 0.5,0.2,0.3 in the experiments."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,3,Experiments,"Dataset. Our dataset contained 282 consecutive patients who underwent thyroid nodule examination at Nanjing Drum Tower Hospital. All patients performed dynamic CEUS examination by an experienced sonographer using an iU22 scanner (Philips Healthcare, Bothell, WA) equipped with a linear transducer L9-3 probe. These 282 cases included 147 malignant nodules and 135 benign nodules. On the one hand, the percutaneous biopsy based pathological examination was implemented to determine the ground-truth of malignant and benign. On the other hand, a sonographer with more than 10 years of experience manually annotated the nodule lesion mask to obtain the pixel-level groundtruth of thyroid nodules segmentation. All data were approved by the Institutional Review Board of Nanjing Drum Tower Hospital, and all patients signed the informed consent before enrollment into the study. Implementation Details. Our network was implemented using Pytorch framework with the single 12 GB GPU of NVIDIA RTX 3060. During training, we first pre-trained the TALR backbone via dice loss for 30 epochs and used Adam optimizer with learning rate of 0.0001. Then, we loaded the pre-trained weights to train the whole model for 100 epochs and used Adam optimizer with learning rate of 0.0001. Here, we set batch-size to 4 during the entire training process The CEUS consisted the full wash-in and wash-out phases, and the resolution of each frame was (600 × 800). In addition, we carried out data augmentation, including random rotation and cropping, and we resize the resolution of input frames to (224 × 224). We adopted 5-fold cross-validation to achieve quantitative evaluation. Three indexes including Dice, Recall, and IOU, were used to evaluate the lesion recognition task, while five indexes, namely average accuracy (ACC), sensitivity (Se), specificity (Sp), F1-score (F1), and AUC, were used to evaluate the diagnosis task. Experimental Results. As in Table "
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,4,Conclusion,"The microvessel infiltration leads to the observation that the lesions detected on CEUS tend to be larger than those on gray US. Considering the microvessel infiltration, we propose an method for thyroid nodule diagnosis based on CEUS videos. Our model utilizes a set of confidence maps to recreate the lesion expansion process; it effectively captures the ambiguous information caused by microvessel infiltration, thereby improving the accuracy of diagnosis. This method is an attempt to eliminate the inaccuracy of diagnostic task due to the fact that gray US underestimates lesion size and CEUS generally overestimates lesion size. To the best of our knowledge, this is the first attempt to develop an automated diagnostic tool for thyroid nodules that takes into account the effects of microvessel infiltration. The way in which we fully exploit the information in time dimension through TPA also makes the model more clinically explanatory."
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Fig. 1 .,
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Fig. 2 .,
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Fig. 3 .,
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Table 1 .,
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Table 2 .,
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,,Ours 88.79 ± 1.40 94.26 ± 1.68 88.37 ± 1.80 90.41 ± 1.85 94.54 ± 1.54,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,1,Introduction,"Mitral regurgitation (MR) is the most common heart valve disease. The incidence increases significantly with age, with more than 13% prevalence in the population over 75 years old  In the clinical diagnosis of MR, physicians assess the extent of MR by calculating the effective regurgitant orifice area (EROA) and mitral regurgitant stroke volume (MRSV) of MR patients from ultrasound images, including continuous wave Doppler images (CW) and color Doppler image (CD). The most commonly applied method for calculating EROA and MRSV uses measurements derived from the proximal isovelocity surface area (PISA) method  Most of the existing methods for the automatic detection of Doppler image contours were based on noise reduction and boundary tracking algorithms. In  The contribution of the paper can be summarized as follows: First, we propose the first fully automatic pipeline for MR quantification from multi-channel ultrasound images based on the modified PISA method. The pipeline includes ECGbased cycle detection, Doppler spectrum segmentation, PISA radius segmentation, and MR quantification; Secondly, we propose a novel adaptive-weighting multi-channel segmentation network, PISA-net, to identify the lower and upper contours of the PISA radius from the complementary and coupled images, i.e., M2D and MCD. The network can adaptively select the related information of the corresponding input image and lead to an accurate estimation of the radius contour. Thirdly, after calculation based on the modified method, our method achieves accurate estimation of MRSV and EROA, with a Pearson correlation of 0.994 with the ground truths for both MR parameters. "
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,2,Method,The overview of the proposed method illustrate in Fig. 
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,2.1,ECG-Based Cardiac Cycle Detection,Multiple cardiac cycles appear in the above-mentioned ultrasound images (Fig. 
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,2.2,Doppler Spectrum and PISA Radius Segmentation,"We proposed the PISA-net to obtain the Doppler spectrum and PISA radius segmentation from multi-channel images. The architecture of the PISA-net is shown in Fig.  Adaptive-Weighting Multi-channel Attention. PISA radius segmentation requires complementary information of both M2D and MCD images, and may also be affected by the image quality. We utilize the SE block "
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,2.3,MR Quantification,"As shown in Fig.  where V a is a constant representing the aliasing velocity, and T denotes the duration length of the regurgitation."
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,3,Experiment and Results,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,3.1,Experimental Configuration,"We obtained Doppler ultrasound images of 205 MR patients from a local hospital, and 157 of them were collected by GE VividE95 while the rest 48 were collected by PHILIPS CX50. For each patient, three images were included: CW, M2D, and MCD, as shown in Fig.  Our method was implemented using Pytorch 1.7.1 and trained on an NVIDIA A100 GPU. The size of the input image is 3 × 256 × 256. The model was optimized by minimizing the binary cross-entropy loss function and using the Adam optimization algorithm. The learning rate was set as 0.001."
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,3.2,Results and Analysis,Effect of Adaptive Weighting and Multiple Channel. We test the effect of the adaptive weighting mechanism in PISA-net when placed in different positions of the encoder. Table 
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,4,Conclusion,"In this work, we proposed the first fully automatic pipeline for MR quantification from multi-channel ultrasound images (CW, M2D, and MCD), based on the modified PISA method. An adaptive weighting mechanism and a spatial attention mechanism weighting were used to combine features of multi-channel inputs and enhance the local feature with a global context. Extensive experiments demonstrate that the proposed method is capable of delivering good segmentation results and excellent quantification of MR parameters, and has great potential in the clinical application of MR diagnosis."
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Fig. 1 .,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Fig. 2 .,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Fig. 3 .,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Fig. 4 .,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Fig. 5 .,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Table 1 .,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Table 2 .,
Mitral Regurgitation Quantification from Multi-channel Ultrasound Images via Deep Learning,,Table 3 .,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,1,Introduction,"Nuclei segmentation is a fundamental step in histology image analysis. In recent advances, with a large amount of labeled data, fully-supervised learning methods can easily achieve reasonable results  Unsupervised learning (UL) methods achieved great success in the data dependency problem for nuclei segmentation, which learns from the structural properties in the data without any manual annotations. Based on the character of these methods, we can group them into two categories: the traditional UL methods and the deep learning UL methods. Traditional UL nuclei segmentation methods include watershed  Therefore, some researchers  To address the above issues and motivated by the IISS property, we hereby propose a novel self-similarity-driven segmentation network (SSimNet) for unsupervised nuclei segmentation. As shown in Fig.  To validate the effectiveness of our method, we conduct extensive experiments on the MoNuSeg dataset "
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,2,Method,As shown in Fig. 
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,2.1,Candidate Nucleus Generation,"Channel Decomposition. Suppose that we are given a training set I S = {I S i } N i=1 of histopathology images without any manual annotation. For each image, stained tissue colors are results from light attenuation, which depends on the type and amount of dyes that the tissues have absorbed. This property is prescribed by the Beer-Lambert law: where I ∈ R 3×n represents the histology image with three color channels and n pixels, I 0 is the illuminating light intensity of sample with I 0 = 255 for 8-bit images in our cases, W ∈ R 3×r is the stain color matrix that encodes the color appearance of each stain with r representing the number of stains, and H ∈ R r×n is the stain density map. In this work, we follow the sparse non-negative matrix factorization in  Clustering and Active Contour. We transform I T into CIELAB color space and invoke the Fuzzy C-Means method (FCM) with 2 clusters to obtain the candidate foreground pixels. To reduce the noise in clustering results, we use active contour method as a smoothing operation to get hard candidate labels: Label Smoothing. Since hard label is overconfident at the border of nuclei, which is detrimental to the training of the network, we soften the hard label one by one for each connected component in P i using the following formulation: where  "
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,2.2,Data Purification and SSimNet Learning,"So far, soft candidate labels Pi have been acquired for each image I S i . However, it is common that adjacent nuclei are merged into one candidate due to imperfect staining and imaging conditions, which violate the IISS property. To this, we conduct data purification to build a reliable training set for subsequent learning. Data Purification. We sample K patches with overlap from original image I S i . The sampled results are expressed as patch tissue We design the Unsupervised Shape Measure Index (USMI) and calculate it using the algorithm in Fig.  ). Figure  To further separate possible adjacent nuclei in a blue patch, we follow  SSimNet Learning and Finetuning. By denoting our segmentation network as F , our final loss function to supervise the network training can be formulated as: where L BCE is the binary cross-entropy loss and L CE is the cross-entropy loss. Also, we can obtain tissue patches and corresponding pseudo labels for each image in the test set termed as Before evaluation, we first fine tune our network F using SET k for several epochs. As shown in the ablation study, this operation is simple but effective. And this fine tuning process can help the network capture the size and shape information in the current test slice. "
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,3,Experiments,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,3.1,Datasets and Settings,MoNuSeg. Multi-organ nuclei segmentation  CPM17. The CPM17 dataset  Settings. We compare our SSimNet with several current unsupervised segmentation methods. We follow the DCGN 
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,3.2,Experimental Results,"To evaluate the effectiveness of SSimNet, we compare it with several deep learning based and conventional unsupervised segmentation methods on the mentioned datasets, including minibatch K-Means (termed as mKMeans), Gaussian Mixture Model  For the methods without public codes, we report the results from the original publications for a fair comparison. The results are shown in Table  As Table  Besides, we conduct an additional comparison experiment based on CPM17 dataset to demonstrate the generalization of our method. As shown in Table "
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,3.3,Ablation Study,We perform ablation studies by disabling each component to the SSimNet framework to evaluate their effectiveness. As shown in Table 
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,4,Conclusion,"In this paper, we propose an SSimNet framework for label-free nuclei segmentation. Motivated by the intra-image self similarity (IISS) property, which characterize the histology images and nuclei, we design a series of operations to capture the prior knowledge and generate pseudo labels as supervision signal, which is used to learn the SSimNet for final nuclei segmentation. The IISS property renders us a tighter prior constraint for better model building compared to other unsupervised nuclei segmentation. Comprehensive experimental results demonstrate that SSimNet achieves the best performances on the benchmark MoNuSeg and CPM17 datasets, outperforming other unsupervised segmentation methods."
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,Fig. 1 .,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,Fig. 2 .,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,Fig. 3 .,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,Fig. 4 .,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,Table 1 .,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,Table 2 .,
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,,Table 3 .,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,1,Introduction,"The tumor microenvironment (TME) is comprised of cancer, immune (e.g. B lymphocytes, and T lymphocytes), stromal, and other cells together with noncellular tissue components "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,2,Previous Related Work and Novel Contributions,"Many studies have only looked at the density of a single biomarker (e.g. TILs), to show that a high density of TILs is associated with improved patient survival and treatment response in NSCLC  (1) TriAnGIL is a computational framework that characterizes the architecture and relationships of different cell types simultaneously. Instead of measuring only simple two-by-two relations between cells, it seeks to identify triadic spatial relations (hyperedges "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,3,Description of TriAnGIL Methodology,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,3.1,Notation,"Our approach consists of constructing heterogeneous graphs step by step and quantifying them by extracting features from them. The graphs are defined by G = (V, E), where V is the set of vertices (nodes) V = {v 1 , ...v N } with τ n vertex types, and E is the collection of pairs of vertices from V, E = {e 1 , ...e M }, which are called edges and φ n is the mapping function that maps every vertex to one of n differential marker expressions in this dataset φ n : V → τ n . G is represented by an adjacency matrix A that allows one to determine edges in constant time."
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,3.2,Node Linking and Computation of Graph-Interplay Features,"The inputs of TriAnGIL are the coordinates of nuclear centroids and the corresponding cell types. In the TriAnGIL procedure, the centroid of each nucleus in a family is represented as a node of a graph. TriAnGIL is agnostic of the method used for identifying the coordinates and types. Once the different cell families are identified (Fig.  1) Quantifying in absence of one family: First, we build a proximity graph (G 1 ) on nodes of α, β, γ based on the Euclidean distance of every two nodes. Two nodes will be connected if their distance is shorter than a given ""interaction rate"", θ regardless of their family and cell type (Fig.  We then exclude the nodes of α from all the interactions by removing its edges from G1 and characterize the relationship of β and γ (Fig.  We then extract 10 features relating to edge length and vertex count. Next, we prune long edges (D 1 ) where the Euclidean distance between connected nodes is more than the ""interaction rate"" (Fig.  3) Triangular interactions: As illustrated in Algorithm 1, from the unpruned Delaunay triangulation that includes the nodes of α, β, γ, we select those triangles (closed triads "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4,Experimental Results and Discussion,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.1,Dataset,"The cohort employed in this study was composed of pre-treatment tumor biopsy specimens from patients with NSCLC from five centers (two centers for training (S t ) and three centers for independent validation (S v )). The entire analysis was carried out using 122 patients in Experiment 1 (73 in S t , and 49 in S v ) and 135 patients in Experiment 2 (81 in S t , and 54 in S v ). Specimens were analyzed with a multiplexed quantitative immunofluorescence (QIF) panel using the method described in "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.2,Comparative Approaches,The efficacy of TriAnGIL was compared against five different approaches.
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,TIL density (DenTIL):,"For every patient, multiple density measures including the number of different cells types and their ratios are calculated "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,"GG: A Delaunay triangulation, a Voronoi diagram, and a Minimum Spanning",Tree were constructed 
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,CCG:,"For every patient, subgraphs are built on nuclei regardless of their type and only based on their Euclidean distance. Local graph metrics (e.g. clustering coefficient) "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,GNN:,A recent study 
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.3,Experiment 1: Immunotherapy Response Prediction in Lung Cancer,"Design: TriAnGIL was also trained to differentiate between patients who responded to IO and those who did not. For our study, the responders to IO were identified as those patients with complete response, partial response, and stable disease, and non-responders were patients with progressive disease. A linear discriminant analysis (LDA) classifier was trained on S t to predict which patients would respond to IO. For creating the model, the minimum redundancy maximum relevance (mRMR) method "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,Results:,"The two top predictive TriAnGIL features were found to be the number of edges between stroma and CD4+ cells, and the number of edges between stroma and tumor cells with more interactions between stromal cells and both CD4+ and tumor cells being associated with response to IO. This finding is concordant with other studies  Result: Figure "
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,5,Concluding Remarks,"We presented a new approach, Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL), to quantitatively chartacterize the spatial arrangement and relative geographical interplay of multiple cell families across pathological images. Compared to previous spatial graph-based methods, TriAnGIL quantifies the spatial interplay between multiple cell families, providing a more comprehensive portrait of the tumor microenvironment. TriAnGIL was predictive of response after IO (N = 122) and also demonstrated a strong correlation with OS in NSCLC patients treated with IO (N = 135). TriAnGIL outperformed other graph-and DL-based approaches, with the added benefit of provoding interpretability with regard to the spatial interplay between cell families. For instance, TriAnGIL yielded the insight that more interactions between stromal cells and both CD4+ and tumor cells appears to be associated with better response to IO. Although five cell families were studies in this work, TriAnGIL is flexible and could include other cell types (e.g., macrophages). Future work will entail larger validation studies and also evaluation on other use cases."
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,Algorithm 1 :,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,Fig. 1 .,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,Fig. 2 . 4 . 4 Experiment 2 :,
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,,Fig. 3 .,
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,1,Introduction,"Histopathology relies on hematoxylin and eosin (H&E) stained biopsies for microscopic inspection to identify visual evidence of diseases. Hematoxylin has a deep blue-purple color and stains acidic structures such as DNA in cell nuclei. Eosin, alternatively, is red-pink and stains nonspecific proteins in the cytoplasm and the stromal matrix. Pathologists then examine highlighted tissue characteristics to diagnose diseases, including different cancers. A correct diagnosis, therefore, is dependent on the pathologist's training and prior exposure to a wide variety of disease subtypes "
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,2,Related Work,"Deep learning based generative models for histopathology images have seen tremendous progress in recent years due to advances in digital pathology, compute power, and neural network architectures. Several GAN-based generative models have been proposed to generate histology patches "
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,3,Method,"In this paper, we describe our framework for generating tissue patches conditioned on semantic layouts of nuclei. Given a nuclei segmentation mask, we intend to generate realistic synthetic patches. In this section, we (1) describe our data preparation, (2) detail our stain-normalization strategy, (3) review conditional denoising diffusion probabilistic models, (4) outline the network architecture used to condition on semantic label map, and (5) highlight the classifier-free guidance mechanism that we employ at sampling time."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,3.1,Data Processing,We use the Lizard dataset 
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,3.2,Stain Normalization,A common issue in deep learning with H&E stained histopathology slides is the visual bias introduced by variations in the staining protocol and the raw materials of chemicals leading to different colors across slides prepared at different labs 
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,3.3,Conditional Denoising Diffusion Probabilistic Model,"In this section, we describe the theory of conditional denoising diffusion probabilistic models, which serves as the backbone of our framework. A conditional diffusion model aims to maximize the likelihood p θ (x 0 | y), where data x 0 is sampled from the conditional data distribution, x 0 ∼ q(x 0 | y), and y represents the conditioning signal. A diffusion model consists of two intrinsic processes. The forward process is defined as a Markov chain, where Gaussian noise is gradually added to the data over T timesteps as where {β} t=1:T are constants defined based on the noise schedule. An interesting property of the Gaussian forward process is that we can sample x t directly from x 0 in closed form. Now, the reverse process, p θ (x 0:T | y), is defined as a Markov chain with learned Gaussian transitions starting from pure noise, p(x T ) ∼ N(0, I), and is parameterized as a neural network with parameters θ as ( Hence, for each denoising step from t to t -1, It has been shown that the combination of q and p here is a form of a variational auto-encoder  Note that the above loss function provides no signal for training Σ θ (x t , y, t). Therefore, following the strategy in improved DDPMs  ). This optimization is done while applying a stop gradient to (x t , y, t) such that L vlb can guide Σ θ (x t , y, t) and L simple is the main guidance for (x t , y, t). Overall, the loss is a weighted summation of the two objectives described above as follows: (5)"
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,3.4,Conditioning on Semantic Mask,"NASDM requires our neural network noise-predictor θ (x t , y, t) to effectively process the information from the nuclei semantic map. For this purpose, we leverage a modified U-Net architecture described in Wang et al. "
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,3.5,Classifier-Free Guidance,"To improve the sample quality and agreement with the conditioning signal, we employ classifier-free guidance "
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,4,Experiments,"In this section, we first describe our implementation details and training procedure. Further, we establish the robustness of our model by performing an ablative study over objective magnification and classifier-guidance scale. We then perform quantitative and qualitative assessments to demonstrate the efficacy of our nuclei-aware semantic histopathology generation model. In all following experiments, we synthesize images using the semantic masks of the held-out dataset at the concerned objective magnification. We then compute Fréchet Inception Distance (FID) and Inception Score (IS) metrics between the synthetic and real images in the held-out set."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,4.1,Implementation Details,"Our diffusion model is implemented using a semantic UNet architecture (Sect. 3.4), trained using the objective in "
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,4.2,Ablation over Guidance Scale (s),"In this study, we test the effectiveness of the classifier-free guidance strategy. We consider the variant without guidance as our baseline. As seen in Fig. "
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,4.3,Ablation over Objective Magnification,
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,4.4,Quantitative Analysis,"To the best of our knowledge, ours is the only work that is able to synthesize histology images given a semantic mask, making a direct quantitative comparison tricky. However, the standard generative metric Fréchet Inception Distance (FID) measures the distance between distributions of generated and real images in the Inception-V3  Table "
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,4.5,Qualitative Analysis,"We perform an expert pathologist review of the patches generated by the model. We use 30 patches, 17 synthetic and 13 real for this review. We have two experts assess the overall medical quality of the patches as well as their consistency with the associated nuclei masks on likert scale. The survey used for the review can be found on a public google survey"
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,5,Conclusion and Future Works,"In this work, we present NASDM, a nuclei-aware semantic tissue generation framework. We demonstrate the model on a colon dataset and qualitatively Fig.  and quantitatively establish the proficiency of the framework at this task. In future works, further conditioning on properties like stain-distribution, tissuetype, disease-type, etc. would enable patch generation in varied histopathological settings. Additionally, this framework can be extended to also generate semantic masks enabling an end-to-end tissue generation framework that first generates a mask and then synthesizes the corresponding patch. Further, future works can explore generation of patches conditioned on neighboring patches, as this enables generation of larger tissue areas by composing patches together."
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,,Fig. 1 .,
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,,Fig. 2 .,
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,,Fig. 3 .,
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,,Table 1 .,
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,1,Introduction,"Radiotherapy, one of the mainstream treatments for cancer patients, has gained notable advancements in past decades. For promising curative effect, a high-quality radiotherapy plan is demanded to distribute sufficient dose of radiation to the planning target volume (PTV) while minimizing the radiation hazard to organs at risk (OARs). To achieve this, radiotherapy plans need to be manually adjusted by the dosimetrists in a trial-and-error manner, which is extremely labor-intensive and time-consuming  Recently, the blossom of deep learning (DL) has promoted the automatic medical image processing tasks  Although the above methods have achieved good performance in predicting dose distribution, they suffer from the over-smoothing problem. These DL-based dose prediction methods always apply the L 1 or L 2 loss to guide the model optimization which calculates a posterior mean of the joint distribution between the predictions and the ground truth  In this paper, we investigate the feasibility of applying a diffusion model to the dose prediction task and propose a diffusion-based model, called DiffDP, to automatically predict the clinically acceptable dose distribution for rectum cancer patients. Specifically, the DiffDP consists of a forward process and a reverse process. In the forward process, the model employs a Markov chain to gradually transform dose distribution maps with complex distribution into Gaussian distribution by progressively adding pre-defined noise. Then, in the reverse process, given a pure Gaussian noise, the model gradually removes the noise in multiple steps and finally outputs the predicted dose map. In this procedure, a noise predictor is trained to predict the noise added in the corresponding step of the forward process. To further ensure the accuracy of the predicted dose distribution for both the PTV and OARs, we design a DL-based structure encoder to extract the anatomical information from the CT image and the segmentation masks of the PTV and OARs. Such anatomical information can indicate the structure and relative position of organs. By incorporating the anatomical information, the noise predictor can be aware of the dose constraints among PTV and OARs, thus distributing more appropriate dose to them and generating more accurate dose distribution maps. Overall, the contributions of this paper can be concluded as follows: "
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2,Methodology,An overview of the proposed diffDP model is illustrated in Fig. 
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2.1,Diffusion Model,"The framework of DiffDP is designed following the Denoising Diffusion Probabilistic Models (DDPM)  Forward Process. In the forward process, the DiffDP model employs the Markov chain to progressively add noise to the initial dose distribution map y 0 ∼ q(y 0 ) until the final disturbed image y T becomes completely Gaussian noise which is represented as y T ∼ N (y T | 0, I ). This forward process can be formulated as: where α t is the unlearnable standard deviation of the noise added to y t-1 . Herein, the α t (t = 1, . . . , T ) could accumulate during the forward process, which can be treated as the noise intensity γ t = t i=1 α i . Based on this, we can directly obtain the distribution of y t at any step t from y 0 through the following formula: where the disturbed image y t is sampled using: in which ε t ∼ N (0, I ) is random noise sampled from normal Gaussian distribution. Reverse Process. The reverse process also harnesses the Markov chain to progressively convert the latent variable distribution p θ (y T ) into distribution p θ (y 0 ) parameterized by θ . Corresponding to the forward process, the reverse one is a denoising transformation under the guidance of structure images x that begins with a standard Gaussian distribution y T ∼ N (y T | 0, I ). This reverse inference process can be formulated as: where μ θ (x, y t , t) is a learned mean, and σ t is a unlearnable standard deviation. Following the idea of  where ε t,θ is a function approximator intended to predict ε t from the input x, y t and γ t . Consequently, the reverse inference at two adjacent steps can be expressed as: where z t ∼ N (0, I ) is a random noise sampled from normal Gaussian distribution. More derivation processes can be found in the original paper of diffusion model "
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2.2,Structure Encoder,"Vanilla diffusion model has difficulty preserving essential structural information and produce unstable results when predicting dose distribution maps directly from noise with a simple condition mechanism. To address this, we design a structure encoder g that effectively extracts the anatomical information from the structure images guiding the noise predictor to generate more accurate dose maps by incorporating extracted structural knowledge. Concretely, the structure encoder includes five operation steps, each with a residual block (ResBlock) and a Down block, except for the last one. The ResBlock consists of two convolutional blocks (ConvBlock), each containing a 3 × 3 convolutional (Conv) layer, a GroupNorm (GN) layer, and a Swish activation function. The residual connections are reserved for preventing gradient vanishment in the training. The Down block includes a 3 × 3 Conv layer with a stride of 2. It takes structure image x as input, which includes the CT image and segmentation masks of PTV and OARs, and evacuates the compact feature representation in different levels to improve the accuracy of dose prediction. The structure encoder is pre-trained by L 1 loss and the corresponding feature representation x e = g(x) is then fed into the noise predictor."
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2.3,Noise Predictor,"The purpose of the noise predictor f (x e , y t , γ t ) is to predict the noise added on the distribution map y t with the guidance of the feature representation x e extracted from the structure images x and current noise intensity γ t in each step t. Inspired by the great achievements of UNet  In the encoding procedure, to guide the noise predictor with essential anatomical structure, the feature representations respectively extracted from the structure images x and noisy image y t are simultaneously fed into the noise predictor. Firstly, y t is encoded into feature maps through a convolutional layer. Then, these two feature maps are fused by element-wise addition, allowing the structure information in x to be transferred to the noise predictor. The following two down-sampling operations retain the addition operation to complete information fusion, while the last three use a cross-attention mechanism to gain similarity-based structure guidance at deeper levels. In the decoding procedure, the noise predictor restores the feature representations captured by the encoder to the final output, i.e., the noise ε t,θ = f (x e , y t , γ t ) in step t. The skip connections between the encoder and decoder are reserved for multi-level feature reuse and aggregation."
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2.4,Objective Function,"The main purpose of the DiffDP model is to train the noise predictor f and structure encoder g, so that the predicted noise ε t,θ = f (g(x), y t , γ t ) in the reverse process can approximate the added noise ε t in the forward process. To achieve this, we define the objective function as: For a clearer understanding, the training procedure is summarized in Algorithm 1."
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,,Algorithm 1:,"Training procedure 1: Input: Input image pairs where is the structure image and is the corresponding dose distribution map, the total number of diffusion steps 2: Initialize: Randomly initialize the noise predictor and pre-trained structure encoder 3: Repeat 4: Sample 5: Sample 6: Perform the gradient step on Equation (9) 7: until converged"
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2.5,Training Details,"We accomplish the proposed network in the PyTorch framework. All of our experiments are conducted through one NVIDIA RTX 3090 GPU with 24 GB memory and a batch size of 16 with an Adaptive moment estimation (Adam) optimizer. We train the whole model for 1500 epochs (about 1.5M training steps) where the learning rate is initialized to 1e-4 and reset to 5e-5 after 1200 epochs. The parameter T is set to 1000. Additionally, the noise intensity is initialized to 1e-2 and decayed to 1e-4 linearly along with the increase of steps."
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,3,Experiments and Results,"Dataset and Evaluations. We measure the performance of our model on an in-house rectum cancer dataset which contains 130 patients who underwent volumetric modulated arc therapy (VMAT) treatment at West China Hospital. Concretely, for every patient, the CT images, PTV segmentation, OARs segmentations, and the clinically planned dose distribution are included. Additionally, there are four OARs of rectum cancer containing the bladder, femoral head R, femoral head L, and small intestine. We randomly select 98 patients for model training, 10 patients for validation, and the remaining 22 patients for test. The thickness of the CTs is 3 mm and all the images are resized to the resolution of 256 × 256 before the training procedure. We measure the performance of our proposed model with multiple metrics. Considering Dm represents the minimal absorbed dose covering m% percentage volume of PTV, we involve D 98 , D 2 , maximum dose (D max ), and mean dose (D mean ) as metrics. Besides, the heterogeneity index (HI) is used to quantify dose heterogeneity  Comparison with State-of-the-Art Methods. To verify the superior accuracy of our proposed model, we select multiple state-of-the-art (SOTA) models in dose prediction, containing UNet (2017)  Besides the quantitative results, we also present the DVH curves derived by compared methods in Fig.  Ablation Study. To study the contributions of key components of the proposed method, we conduct the ablation experiments by 1) removing the structure encoder from the proposed method and concatenating the anatomical images x and noisy image y t together as the original input for diffusion model (denoted as Baseline); 2) the proposed DiffDP model. The quantitative results are given in Table "
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,4,Conclusion,"In this paper, we introduce a novel diffusion-based dose prediction (DiffDP) model for predicting the radiotherapy dose distribution of cancer patients. The proposed method involves a forward and a reverse process to generate accurate prediction by progressively transferring the Gaussian noise into a dose distribution map. Moreover, we propose a structure encoder to extract anatomical information from patient anatomy images and enable the model to concentrate on the dose constraints within several essential organs. Extensive experiments on an in-house dataset with 130 rectum cancer patients demonstrate the superiority of our method."
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,,Fig. 1 .,
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,,Fig. 2 .,
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,,Fig. 3 .,
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,,Fig. 4 .,
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,,Table 1 .,
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,,Table 2 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,1,Introduction,"With the rapid development of 3D scanning devices, the application of computeraided diagnosis in orthodontics has gradually developed. In orthodontic treatment, an essential step for patients is to wear retainers. Orthodontists need to remove the brackets, utilize the intra-oral scanners to acquire the digital models, and then print models to fabricate retainers. However, there is a long waiting time for patients and the position of teeth would change, which affects the effectiveness of orthodontic treatment. As shown in Fig.  Since 3D dental models can be transformed into point clouds, methods proposed for point cloud segmentation would provide guidance. Point cloud segmentation methods can be mainly summarized as MLP-based  As graph-based network  The contributions of this paper are as follows: 1) We propose a graph-based network named BSegNet for bracket segmentation on 3D dental models which can reduce the burden on orthodontists; 2) An effective method is proposed to reconstruct the tooth surface where brackets are removed; 3) The low reconstruction error of the automatically processed models suggests that the framework integrating the segmentation and reconstruction can be used as a powerful tool to assist orthodontists."
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,2,Method,"We propose a network named BSegNet for bracket segmentation on 3D dental models. BSegNet regards each mesh cell as a graph node and updates the nodewise feature via several local modules. For dental models with brackets removed, a simple yet effective reconstruction method is adopted to reconstruct the tooth surface. We will describe the network architecture of BSegNet (Fig. "
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,2.1,Bracket Segmentation Network,"Compared to point clouds, more geometric spatial information can be obtained from 3D mesh data. Different input features will have different effects on the subsequent network training. In this paper, we use a 24-dimension vector as the initial input. The 24-dimensional input vector corresponds to the coordinates of three vertices, the normal vectors of three vertices, the normal vectors of the mesh cell, and the coordinates of the mesh cell centroid. We use a MLP to map the input feature Then a transform net is adopted to transform the feature F 1 into a canonical space to improve the robustness. The transformed feature is fed to several local modules which are designed to encode the local features of each mesh cell in semantic space. In each local module, the first step is to construct the graph G(V, E) where V = {c 1 , c 2 , ..., c N } denotes the set of mesh cells and E represents the  After building the graph, we construct the local feature of each cell and update the feature through the graph convolution layers. Let the f l i ∈ R 64 denote the feature vector of the mesh cell c i in the l-th layer and denotes the edge feature which is used to capture the geometric relationship between each cell and its neighbors. Then the local feature f local = ( f l ij ⊕ f l i ) goes through two Conv2D layers to further encode the feature. The update process of the mesh cell features (graph node) is defined as where h ϑ denotes the Conv2D layers and R(.) stands for the feature aggregation function. To avoid the over-smoothing problem in the graph network, we use the residual connection in each local module the same as DeepGCN  As the prediction results of the network may have some isolated labeled mesh cells, we use the graph-cut method to refine the results. The post-processing stage minimizes an energy function by combining the probability term and the smoothness term. The energy function to be optimized is defined as where p i (l i ) denotes the probability belongs to the l i , is the minimal probability threshold, and λ denotes the smooth parameter. The local smoothness term is defined as where θ ij denotes the dihedral angle of two adjacent facets and d ij denotes the distance between the centroids of two adjacent facets."
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,2.2,Tooth Surface Reconstruction,"First, we identify all the holes and extract their boundaries. Then we project the vertices of the boundary into the 2D plane. We triangulate the projected polygon without inserting new vertices. If the line segment inside the polygon exceeds the preset length, it should be n-equally divided. To get more uniform triangles, we use the optimal delaunay triangulation algorithm  where p is the vertex needs to be optimized, Ω(p) denotes the set of first-ring neighborhood mesh cells of p, |T j | denotes the area of mesh cell T j , c j is the circumcentre of T j . After optimizing the positions of the vertices, the triangles have almost the same angles and the distribution is closer to the original models. After the polygon triangulation, the x-y coordinates are determined and a layer-by-layer procedure is employed to estimate the corresponding z-values. To avoid the vertices at the gingiva, we only use the information of the first-ring neighborhoods of the boundary vertices. We first compute the slopes of the boundary vertices which are defined as where N 1 (i) denotes the set of first-ring neighborhood vertices of the boundary point b i . Then the calculation of the z-coordinate value is denoted as where N b (i) denotes the set of adjacent boundary points and k b j denotes the slope of boundary vertex b j . Then we regard the added points as new boundary vertices and repeat the above process until the z-values of all vertices are calculated. Before transforming back to the original space, the extreme z values are removed by median filtering. Then we use a rotation matrix to obtain the coordinates in the original space. Finally, we employ Laplacian smoothing to enhance the smoothness of the reconstructed surface. To make the reconstructed surface blend better with the boundary, we need to reduce the effect of smoothing on the first-ring neighborhood of the boundary. The calculation equation is as follows where p b and p s denote the vertex before and after smoothing, d b is the average distance between the vertex and the adjacent boundary vertices, and d h is the average length of the hole boundary."
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,3,Experiments and Results,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,3.1,Datasets and Implementation Details,"We collect 80 dental mesh models in STL format from different patients. The number of mesh cells in each dental model is approximately 100,000 and all the dental models are down-sampled to nearly 24,000 mesh cells. The ground truth segmentations are annotated by professional orthodontists on the downsampled dental models. We divide the dataset into a training, validation, and test set which consists of 45, 14, and 21 subjects, respectively. The performance of bracket segmentation is evaluated by mean Intersection-over-Union (mIoU) and Overall Accuracy (OA). The performance of reconstruction is evaluated by the Mean Distance (MD) and Standard Deviation (SD) of the distance between the models reconstructed by our method and by Geomagic Studio. We also evaluate the reconstruction error of models processed by the automatic framework and manually by orthodontists. We train all the networks by minimizing the crossentropy loss for 400 epochs except for MeshSegNet which minimizes the dice loss. We use the Adam optimizer and set the mini-batch as 5. The initial learning rate is 0.001, and we anneal the learning rate using the cosine functions. "
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,3.2,Experimental Evaluation,"Comparison Results. We compare our method with SOTA point cloud segmentation methods and teeth segmentation methods. All the results are shown in Table  As shown in Table  Ablation Study and Analysis. In this section, we analyze different operations and modules in the BSegNet. As shown in Table "
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,4,Conclusion,"In this paper, we propose a network named BSegNet for bracket segmentation on 3D dental models which can reduce the burden on orthodontists. BSegNet is a graph-based network that employs dynamic dilated neighborhood construction and residual connections to improve segmentation results. With label optimization, the segmentation results can be further refined. Experimental results on a clinical dataset demonstrate our method significantly outperforms related stateof-the-art methods. We also propose a simple yet effective method to reconstruct the tooth surface which can better recover the feature of the teeth. The whole framework achieves a low reconstruction error and can be used as a powerful tool to assist doctors in orthodontic diagnosis."
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Fig. 1 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Fig. 2 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Fig. 3 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Table 1 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Table 2 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Table 3 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Table 4 .,
Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 40.
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,1,Introduction,"With the advent of advanced medical imaging technologies, such as computed tomography (CT) and magnetic resonance (MR), non-invasive visualizations of various human organs and tissues become feasible and are widely utilized in clinical practice  Empowered by the great feature extraction ability of deep neural networks (DNNs) and the strong parallel computing power of graph processing units (GPUs), automated visualizations of cardiac organs have been extensively explored in recent years  In such context, automated approaches that can directly and efficiently generate cardiac shapes from medical imaging data are highly desired. Recently, various DNN works  To solve this issue, we introduce a novel surface loss that inherently considers the topology of the two to-be-compared meshes in the loss function, with a goal of optimizing the anatomical topology of the reconstructed mesh. The surface loss is defined by a computable norm on currents "
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,2,Methodology,"Figure  For an input CT or MR volume, it passes into the voxel feature extraction module to predict binary segmentation for the to-be-reconstructed ROIs. Meanwhile, the initial spherical meshes enter into the first deformation block along with the trilinearly-interpolated voxel features to predict the vertex-wise displacements of the initial meshes. Then, the updated meshes go through the following blocks for subsequent deformations. The third deformation block finally outputs the reconstructed whole-heart meshes. The three deformation blocks follow the same process, except for the meshes they deform and the trilinearlyinterpolated voxel features they operate on. In the first deformation block, we use high-level voxel features, f 3 and f 4 , obtained from the deepest layers of the volume encoder. In the second deformation block, the middle-level voxel features, f 1 and f 2 , are employed. As for the last deformation block, its input meshes are usually quite accurate and only need to be locally refined. Thus, low-level voxel features are employed to supervise this refining process. Surface Representation as Currents. Keeping in line with  where for each x ∈ S u 1 x and u 2 x form an orthonormal basis of the tangent plane at x. ω(x) is a skew-symmetric bilinear function on R 3 . dσ(x) represents the basic element of surface area. Subsequently, a surface can be represented as currents in the following expression where S(ω) denotes the currents representation of the surface. f denotes each face of S and σ f is the surface measure on f . ω(x) is the vectorial representation of ω(x), with • and × respectively representing dot product and cross product. After the currents representation is established, an approximation of ω over each face can be obtained by using its value at the face center. Let is the center of the face and N (f ) = 1  2 (e 2 × e 3 ) is the normal vector of the face with its length being equal to the face area. Then, ω can be approximated over the face by its value at the face center, resulting in S(ω) ≈ f ω(c(f ))•N (f ). In fact, the approximation is a sum of linear evaluation functionals C(S) = f δ N (f ) c(f ) associated with a Reproducing Kernel Hilbert Space (RKHS) under the constraints presented elsewhere  Thus, S ε , the discrepancy between two surfaces S and T , can be approximately calculated via the RKHS as below where W * is the dual space of a Hilbert space (W, •, • W ) of differential 2-forms and || || 2 is l 2 -norm. () T denotes the transpose operator. f , g index the faces of S and q, r index the faces of T . k W is an isometry between W * and W , and we have . The first and third terms enforce the structural integrity of the two surfaces, while the middle term penalizes the geometric and spatial discrepancies between them. With this preferable property, Eq. 3 fulfills the topology correctness purpose, the key of this proposed pipeline."
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,,Surface Loss.,"As in  ), where x and y are the centers of two faces and σ W is a scale controlling parameter that controls the affecting scale between the two faces. Therefore, the surface loss can be expressed as where t 1 , t 2 , t and p 1 , p 2 , p respectively index faces on the reconstructed surfaces S R and those on the corresponding ground truth surfaces S T . L surf ace not only considers each face on the surfaces but also its corresponding direction. When the reconstructed surfaces are exactly the same as the ground truth, the surface loss L surf ace should be 0. Otherwise, L surf ace is a bounded positive value  Minimizing L surf ace enforces the reconstructed surfaces to be progressively close to the ground truth as the training procedure develops. Figure  Loss Function. In addition to the surface loss we introduce above, we also involve two segmentation losses L BCE and L Dice , one point cloud loss L CD , and three regularization losses L laplace , L edge , and L normal that comply with  where w s is the weight for the segmentation loss, and w 1 , w 2 , w 3 and w 4 are respectively the weights for the surface loss, the Chamfer distance, the Laplace loss, and the edge loss. The geometric mean is adopted to combine the five individual mesh losses to accommodate their different magnitudes. L seg ensures useful feature learning of the ROIs. L surf ace enforces the integrity of the reconstructed meshes and makes them topologically similar to the ground truth. L CD makes the point cloud representation of the reconstructed meshes to be close to that of the ground truth. Additionally, L laplace , L edge , and L normal are employed for the smoothness consideration of the reconstructed meshes."
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,3,Experiments,"Datasets and Preprocessing. We evaluate and validate our method on a publicly-accessible dataset MM-WHS (multi-modality whole heart segmentation)  Evaluation Metrics. In order to compare with existing state-of-the-art (SOTA) methods, four metrics as in  Table  Table  Ablation Study. For the ablation study, we train a model without the surface loss while keeping the rest the same. Table "
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,4,Conclusion,"In this work, we propose and validate a whole-heart mesh reconstruction method incorporating a novel surface loss. Due to the intrinsic and favorable property of the currents representation, our method is able to generate accurate meshes with the correct topology."
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,,Fig. 1 .,
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,,Fig. 2 .,
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,,Fig. 3 .,
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,,,
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,,Table 2 .,
Whole-Heart Reconstruction with Explicit Topology Integrated Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 11.
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,1,Introduction,"In the past few years, the development of histopathological whole slide image (WSI) analysis methods has dramatically contributed to the intelligent cancer diagnosis  Generally, multiple instance learning (MIL) is one of the most popular solutions for WSI analysis  However, HIPT is a hierarchical learning framework based on a greedy training strategy. The bias and error generated in each level of the representation model will accumulate in the final decision model. Moreover, the ViT  In this paper, we propose a novel whole slide image representation learning framework named position-aware masked autoencoder (PAMA), which achieves slide-level representation learning by reconstructing the local representations of the WSI in the patch feature space. PAMA can be trained end-to-end from the local features to the WSI-level representation. Moreover, we designed a position-aware cross-attention mechanism to guarantee the correlation of localto-global information in the WSIs while saving computational resources. The proposed approach was evaluated on a public TCGA-Lung dataset and an in-house Endometrial dataset and compared with 6 state-of-the-art methods. The results have demonstrated the effectiveness of the proposed method. The contribution of this paper can be summarized into three aspects. (1) We propose a novel whole slide image representation learning framework named position-aware masked autoencoder (PAMA). PAMA can make full use of abundant unlabeled WSIs to learn discriminative WSI representations. (2) We propose a position-aware cross-attention (PACA) module with a kernel reorientation (KRO) strategy, which makes the framework able to maintain the spatial integrity and semantic enrichment of slide representation during the selfsupervised training. "
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,2,Methods,
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,2.1,Problem Formulation and Data Preparation,MAE 
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,2.2,Masked WSI Representation Autoencoder,Figure 
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,2.3,Position-Aware Cross-Attention,"To preserve the structure information of the tissue, we propose the positionaware cross-attention (PACA) module, which is the core of the encoder and decoder blocks. The structure of PACA is shown in Fig. "
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,(III).,"The message passing between the anchors and patches is achieved by a bidirectional cross-attention between the patches and anchors. First, the anchors collect the local information from the patches, which is formulated as where W l ∈ R d f ×de , l = q, k, v are learnable parameters with d e denoting the dimension of the head output, σ represents the softmax function, and ϕ d and ϕ p are the embedding functions that respectively take the distance and polar angle as input and output the corresponding trainable embedding values. Symmetrically, each patch token catches the information of all anchors into their own local representations by the equations The two-way communication makes the patches and anchors timely transmit local information and perceive the dynamic change of global information. The embedding of relative distance and polar angle information helps the model maintain the semantic and structural integrity of the WSI and meanwhile prevents the WSI representation from collapsing to the local area throughout the training process. In terms of efficiency, the computational complexity of self-attention is O(n p 2 ) where n p is the number of patch tokens. In contrast, our proposed PACA's complexity is O(n k × n p ) where n k is the number of anchors. Notice that n k << n p , the complexity is close to O(n p ), i.e. linear correlation with the size of the WSI."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,2.4,Kernel Reorientation,"As for the polar angle matrix P ∈ N n k ×np , we specify the horizontal direction of all the anchors as the initial polar axis. In natural scene images, there is natural directional conspicuousness of semantics. For instance, in the case of a church, it is most likely to find a door below the windows rather than be located above them. But histopathology images have no absolute definition of direction. The semantics of WSI will not change with rotation and flip. Namely, it is isotropic. Embedding the orientation information with a fixed polar axis will lead to ambiguities in various slides. To address this problem, we design a kernel reorientation (KRO) strategy to dynamically update the polar axis during the training. As shown in Fig. "
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,3,Experiments and Results,
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,3.1,Datasets,"We evaluated the proposed method on two datasets, the public TCGA-Lung and the in-house Endometrial dataset, which are introduced as follows. Algorithm 1: Kernel Reorientation algorithm. Input: P (n) ∈ N H×n k ×np : The relative polar angle matrix of n-th block, where H is the head number of multi-head attention, n k is the number of anchors in the WSI, np is the number of patches in the WSI; A (n) ∈ R H×n k ×np : The attention matrix from anchors to patches, defined as D score : A dictionary taking the angle as KEY for storing attention scores; Output: P (n+1) ∈ R H×n k ×np : The updated polar angle matrix. h,i,max = arg max D score ; // Find the orientation that has the highest attention score. for j in np do P h,i,max ;// Reorientation."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,end end end,"TCGA-Lung dataset is collected from The Cancer Genome Atlas (TCGA) Data Portal. The dataset includes a total of 3,064 WSIs, which consist of three categories, namely Tumor-free (Normal), Lung Adenocarcinoma (LUAD), and Lung Squamous Cancer (LUSC), Endometrial dataset includes 3,654 WSIs of endometrial pathology, which includes 8 categories, namely Well/Moderately/Low-differentiated endometrioid adenocarcinoma, Squamous differentiation carcinoma, Plasmacytoid carcinoma, Clear cell carcinoma, Mixed-cell adenocarcinoma, and benign tumor. Each dataset was randomly divided into training, validation and test sets according to 6:1:3 while keeping each category of data proportionally. We conducted WSI multi-type classification experiments on the two datasets. The validation set was used to perform an early stop. The results of the test set were reported for comparison."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,3.2,Implementation Details,"The WSI representation pre-training stage uses all training data and does not involve any supervised information. During the downstream classification task,  the pre-trained encoder is utilized as the slide representation extractor, and the [CLS ] token is fed into the following classifier consisting of a multilayer perceptron (MLP) and a fully connected layer. Following the protocol in selfsupervised learning  To ensure the uniformity of patch features, we choose DINO "
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,3.3,Effectiveness of the WSI Representation Learning,We first conducted experiments on the Endometrial dataset to verify the effectiveness of self-supervised learning for WSI analysis under label-limited conditions. The results are shown in Fig. 
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,3.4,Ablation Study,"Then, we conducted ablation experiments to verify the necessity of the proposed structural embedding strategy. The detailed results are shown in Table "
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,3.5,Comparison with SOTA Methods,"Finally, we additionally compared the proposed PAMA with four weaklysupervised methods, DSMIL "
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,4,Conclusion,"In this paper, we proposed an effective self-supervised representation learning framework for WSI analysis. The experiments on two large-scale datasets have demonstrated the effectiveness of PAMA in the condition of limited-label. The results have shown superiority to the existing weakly-supervised and selfsupervised MIL methods. Future work will focus on training the WSI representation model based on datasets across multiple organs, thus promoting the generalization ability of the model for different downstream tasks."
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,Fig. 1 .,
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,Fig. 2 .,
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,Table 1 .,
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,Table 2 .,
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 69.
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,1,Introduction,"Cervical cancer is a common and severe disease that affects millions of women globally, particularly in developing countries  Several computer-aided cervical cancer screening methods have been proposed for whole slide images (WSIs) in the literature. Most of them are detectionbased methods, which typically contain a detection model as well as some postprocessing modules in their frameworks. For instance, Zhou et al.  Some methods improve the final classification performance by improving the detection model to identify positive cells more reliably. Cao et al.  These methods have achieved good results through continuous improvement on the detection-based pipeline, but there are some common drawbacks. First, they are not able to get rid of their reliance on detection models, which means they have a high need for expensive detection data labeling to train the detection model. Cervical cancer cell detection datasets involve labeling individual and small bounding boxes in a large number of cells. It often requires multiple experienced pathologists to annotate  To address the aforementioned issues, we propose a detection-free pipeline in this paper, which does not rely on any detection model. Instead, our pipeline requires only sample-level diagnosis labels, which are naturally available in clinical scenarios and thus get rid of additional image labeling. To attain this goal, we have designed a two-stage pipeline as in Fig. "
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,2,Methodology,Fig. 
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Two-Stage Pipeline with Attention Guided Selection.,"The overview of our two-stage pipeline is shown in Fig.  To complete sample-level classification, both stages share basically the same network architecture. The input images are first processed by a CNN encoder to extract features. Then, we propose the pooling transformer, which is modified from the basic transformer module in Sect. 2, to integrate these features for WSI classification. Additionally, the input images for both stages are 256 × 256. In the coarse-grained stage, in order to allow the model to examine as many local images as possible, we resize the cropped local images from 1024 × 1024 to 256 × 256. In the fine-grained stage, we enlarge suspicious local abnormality and thus crop input images to 256 × 256 from 1024 × 1024. For the coarse-grained stage, after passing the resized local images through encoder and pooling transformer, we obtain a rough prediction result at the sample level. We then use the Cross-Entropy (CE) loss to minimize the difference between the predicted WSI label and the ground truth. In addition, we calculate the attention score to identify the local image inputs that are most likely to yield positive reading. We describe the attention score as where x 0 represents classification token (which is a commonly used setting in transformer  Next, in the fine-grained stage, each local image that has passed attention guided selection is cropped into 16 patches of the size 256 × 256. We expect that those patches contain positive cells and are thus critical to diagnosis at the sample level. The network of the fine-grained stage is the same as that of the coarse-grained stage, but the weights of the encoder is pre-trained in an unsupervised manner (Sect. 2). The same CE loss supervised by sample-level ground truth is used for the fine-grained stage here. For inference, the output of the fine-grained stage will be treated as the final result of the test WSI. Pooling Transformer. We use a transformer network to aggregate features of multiple inputs and to derive the sample-level outcome in both coarse-grained and fine-grained stages. We have observed that different local images of the same sample often have patterns of grouped similarity (such as the first two images in the upper-right of Fig.  Therefore, inspired by "
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Contrastive Pre-training of Encoder.,"To make full use of WSI data and provide a better feature encoder, inspired by MoCo  Specifically, in the same training batch, a patch (256 × 256, the same to the input size of the fine-grained stage) and its augmented patch are treated as a positive pair (note that here ""positive/negative"" is defined in the context of contrastive learning), and their features are required to be as similar as possible. Meanwhile, their features are required to be as dissimilar as possible from those of other patches. So the loss function can be described as f i and f ia represent the positive pair, and f j represents another patch negatively paired with f i . Using this method, we can pre-train a feature encoder in an unsupervised manner and initialize it into our encoder for the fine-grained stage."
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,3,Experiment and Results,"Dataset and Experimental Setup. In this study, we have collected 5384 cervical cytopathological WSI by 20x lens, each with 20000 × 20000 pixels, from our collaborating hospitals. Among them, there 2853 negative samples, and 2531 positive samples (962 ASCUS, and 1569 high-level positive samples). All WSIs only have diagnosis labels at the sample level, without annotation boxes at the cell level. And all sample labels are strictly diagnosed according to the TBS  Comparison to SOTA Methods. In this section, we experiment to compare our method with popular state-of-the-art (SOTA) methods, which are all fully supervised and detection-based. To the best of our knowledge, there are few good methods to train cervical cancer classification models in weakly supervised or unsupervised learning ways. No methods can achieve the detection-free goal either. All the detection-based methods are evaluated in the following way. First, we label a dataset with cell-level bounding boxes to train a detection model. The detection dataset has 3761 images and 7623 cell-level annotations. After obtaining the suspicious cell patches provided by the detection model, we use the subsequent classification models used in these SOTA works to classify them and obtain the final classification results. As shown in Table  Ablation Study. In this section, we experiment to demonstrate the effectiveness of all the proposed parts in our pipeline. We divide all 5384 samples into five independent parts for five-fold cross-validation, and the results are shown in Table "
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Sample Numbers and Inference Time.,"In order to further demonstrate the huge potential of our method, we also perform an ablation study on the number of samples used for training and compare the time consuming of the different methods. For the experiment of sample numbers, We compare the best fully supervised detection-based method (Retinanet+GAT "
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,4,Conclusion and Discussion,"In this paper, we propose a novel two-stage detection-free pipeline for WSI classification of cervical abnormality. Our method does not rely on detection models and eliminates the need for expensive cell-level data annotation. By leveraging just sample-level diagnosis labels, we achieve results that are competitive with fully supervised detection-based methods. Through the use of the proposed pooling transformer and unsupervised pre-training, our method makes full use of information within WSIs, resulting in improved efficiency in the use of pathological images. Importantly, our method offers even greater advantages with increasing amounts of data. And also, by utilizing attention weights, we can calculate attention scores to visually represent the importance of each image in the sample, making it easier for doctors to make judgments. Relevant visualization results can be found on our project homepage. Admittedly, our method has some limitations, such as slow training. Accelerating the training of massive data can be our next optimization direction."
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Fig. 2 .,
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Fig. 3 .,
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,,
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Table 1 .,
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Table 2 .,
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,,Table 3 .,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,1,Introduction,"Multi-class cell segmentation is an essential technique for analyzing tissue samples in digital pathology. Accurate cell quantification assists pathologists in identifying and diagnosing diseases  Previous works proposed several computer vision tools to perform automated or semi-automated cell segmentation on pathological images  In this paper, we proposed a holistic molecular-empowered learning scheme that democratizes AI pathological image segmentation by employing only lay annotators (Fig.  • We propose a molecular-empowered learning scheme for multi-class cell segmentation using partial labels from lay annotators; • The molecular-empowered learning scheme integrates (1) Giga-pixel level molecular-morphology cross-modality registration, (2) molecular-informed annotation, and (3) molecular-oriented segmentation model to achieve statistically a significantly superior performance via lay annotators as compared with experienced pathologists; • A deep corrective learning method is proposed to further maximize the cell segmentation accuracy using partially annotated noisy annotation from lay annotators."
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,2,Methods,The overall pipeline of the entire labeling and auto-quantification pipeline is presented in Fig. 
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,2.1,Morphology-Molecular Multi-modality Registration,"Multi-modality, multi-scale registration is deployed to ensure the pixel-to-pixel correspondence (alignment) between molecular IF and PAS images at both the WSI and regional levels. To maintain the morphological characteristics of the functional unit structure, a slide-wise multi-modality registration pipeline (Map3D)  To achieve a more precise pixel-level correspondence, Autograd Image Registration Laboratory (AIRLab)  In Eq. ( "
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,2.2,Molecular-Informed Annotation,"After aligning molecular images with PAS images, an automatic multi-class functional units segmentation pipeline Omni-Seg "
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,2.3,Molecular-Oriented Corrective Learning for Partial Label Segmentation,"The lack of molecular expertise as well as the variability in the quality of staining in molecular images can cause annotations provided by non-specialists to be unreliable and error-prone. Therefore, we propose a corrective learning strategy (in Fig.  Inspired by confidence learning  where k denotes the number of selected embedding features. E is the embedding map from the last layer of the decoder, while Y is the lay annotation. We then implement a cosine similarity score S between the embedding from an arbitrary pixel to those from critical embedding features as Eq. "
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,"S(e i , e top","where m denotes the channel of the feature embeddings. Since the labels from lay annotators might be noisy and erroneous, the W and S are applied in following Eq. ( "
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,3,Data and Experiments,"Data. 11 PAS staining WSIs, including 3 injured glomerulus slides, were collected with pair-wise IF images for the process. The stained tissues were scanned at a 20× magnification. After multi-modality multi-scale registration, 1,147 patches for podocyte cells, and 789 patches for mesangial cells were generated and annotated. Each patch has 512×512 pixels."
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Morphology-Molecular Multi-modality Registration.,"The slide-level global translation from Map3D was deployed at a 5× magnification, which is 2 µm per pixel. The 4096×4096 pixels PAS image regions with 1024 pixels overlapping were tiled on anatomical WSIs at a 20× magnification, which is 0.5 µm per pixel. Molecular-Empowered Annotation. The automatic tuft segmentation and molecular knowledge images assisted the lay annotators with identifying glomeruli and cells. ImageJ (version v1.53t) was used throughout the entire annotation process. ""Synchronize Windows"" was used to display cursors across the modalities with spatial correlations for annotation. ""ROI Manager"" was used to store all of the cell binary masks for each cell type. Molecular-Oriented Corrective Learning. Patches were randomly split into training, validation, and testing sets -with a ratio of 6:1:3, respectively -at the WSI level. The distribution of injured glomeruli and normal glomeruli were balanced in the split. Experimental Setting. 2 experienced pathologists and 3 lay annotators without any specialized knowledge were included in the experiment. All anatomical and molecular patches of glomerular structures are extracted from WSI on a workstation equipped with a 12-core Intel Xeon W-2265 Processor, and NVIDIA RTXA6000 GPU. An 8-core AMD Ryzen 7 5800X Processor workstation with XP-PEN Artist 15.6 Pro Wacom is used for drawing the contour of each cell. Annotating 1 cell type on 1 WSI requires 9 h, while staining and scanning 24 IF WSIs (as a batch) requires 3 h. The experimental setup for the 2 experts and the 3 lay annotators is kept strictly the same to ensure a fair comparison. Evaluation Metrics. 100 patches from the testing set with a balanced number of injuries and normal glomeruli were captured by the pathologists for evaluating morphology-based annotation and molecuar-informed annotation. The annotation from one pathologist (over 20 years' experience) with both anatomical and molecular images as gold standard (Fig. "
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,4,Results,Figure 
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,4.1,Performance on Multi-class Cell Segmentation,"In Table  To evaluate the performance of molecular-oriented corrective learning on imperfect lay annotation, we also implemented two noisy label learning strategies Confidence Learning (CL) "
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,4.2,Ablation Study,"The purpose of corrective learning is to alleviate the noise and distillate the correct information, so as to improve the model performance using lay annotation. Four designs of corrective learning with different utilization of similarity losses and confidence losses were evaluated with lay annotation in Table "
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,5,Conclusion,"In this work, we proposed a holistic, molecular-empowered learning solution to alleviate the difficulties of developing a multi-class cell segmentation deep learning model from the expert level to the lay annotator level, enhancing the accuracy and efficiency of cell-level annotation. An efficient corrective learning strategy is proposed to offset the impact of noisy label learning from lay annotation. The results demonstrate the feasibility of democratizing the deployment of a pathology AI model while only relying on lay annotators."
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Fig. 1 .,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Fig. 2 .,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Fig. 3 .,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Fig. 4 .,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Fig. 5 .,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Table 1 .,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Table 2 .,
Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning,,Table 3 .,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,1,Introduction,"The use of panoramic X-rays to diagnose numerous dental diseases has increased exponentially due to the demand for precise treatment planning  To circumvent the limitations of the existing methods, we propose a novel diffusion-based hierarchical multi-label object detection method to point out each abnormal tooth with dental enumeration and associated diagnosis concurrently on panoramic X-rays, see Fig.  The contributions of our work are three-fold. "
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,2,Methods,Figure 
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,2.1,Base Model,"Our method employs the DiffusionDet  where z 0 represents the input bounding box b, and b ∈ R N ×4 is a set of bounding boxes, z t represents the latent noisy boxes, and ᾱt represents the noise variance schedule. The DiffusionDet model "
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,2.2,Proposed Framework,"To improve computational efficiency during the denoising process, Diffusion-Det  Image Encoder. Our method utilizes a Swin-transformer  Multi-label Detection. We utilize three classification heads as quadrantenumeration-diagnosis for each bounding box and freeze the heads for the unlabeled classes, shown in Fig.  where y i q , y i e , and y i d represent the bounding box classifications for quadrant, enumeration, and diagnosis, respectively, and h q , h e , and h d represent binary indicators of whether the labels are present in the training dataset. By adapting this approach, we leverage the full range of available information and improve our ability to handle partially labeled data. This stands in contrast to conventional object detection methods, which rely on a single classification head for each bounding box "
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,3,Experiments and Results,"We evaluate models' performances using a combination of Average Recall (AR) and Average Precision (AP) scores with various Intersection over Union (IoU) thresholds. This included AP [0.5,0.95] , AP 50 , AP 75 , and separate AP scores for large objects (AP l ), and medium objects (AP m ). Data. All panoramic X-rays were acquired from patients above 12 years of age using the VistaPano S X-ray unit (Durr Dental, Germany). To ensure patient privacy and confidentiality, panoramic X-rays were randomly selected from the hospital's database without considering any personal information. To effectively utilize FDI system  Experimental Design. To evaluate our proposed method, we conduct two experiments: (1) Comparison with state-of-the-art object detection models, including DETR  Evaluation. Fig. "
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,4,Discussion and Conclusion,"In this paper, we introduce a novel diffusion-based multi-label object detection framework to overcome one of the significant obstacles to the clinical application of ML models for medical and dental diagnosis, which is the difficulty in getting a large volume of fully labeled data. Specifically, we propose a novel bounding box manipulation technique during the denoising process of the diffusion networks with the inference from the previously trained model to take advantage of hierarchical data. Moreover, we utilize a multi-label detector to learn efficiently from partial annotations and to assign all necessary classes to each box for treatment planning. Our framework outperforms state-of-the-art object detection models for training with hierarchical and partially annotated panoramic X-ray data. From the clinical perspective, we develop a novel framework that simultaneously points out abnormal teeth with dental enumeration and associated diagnosis on panoramic dental X-rays with the help of our novel diffusion-based hierarchical multi-label object detection method. With some limits due to partially annotated and limited amount of data, our model that provides three necessary classes for treatment planning has a wide range of applications in the real world, from being a clinical decision support system to being a guide for dentistry students."
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,Fig. 1 .,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,Fig. 2 .,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,Fig. 3 .,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,5 .,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,Fig. 4 .,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,Table 1 .,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,.4 61.3 47.9 49.7 39.5,
Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 38.
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,1,Introduction,"Histological staining is regarded as the standard protocol in clinical pathological examination, which is used to label biological structures and morphological changes in tissues  The rapidly emerging field of digital virtual staining has shown great promise to revolutionize the decade-old staining workflow. Zhang et al.  In this paper, we propose MulHiST, a novel Multiple Histological Staining model for Thick biological tissues, which is not feasible in the traditional histochemical staining workflow. To our knowledge, this is the first attempt to achieve multiple histological staining generations for thick tissues. "
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,2,Methodology,"To represent different staining types, we follow the idea of the domain-specific attribute vector proposed in  In general, the image of thick tissues contains several-layer information, and information from different layers interferes with each other, which will lead to ambiguity in the determination of pathological features, e.g., cell boundary and tissue content. We believe that all the image domains, i.e., LS images and H&E/PAS/MT-stained images, share some domain-invariant features that can facilitate the model training. Moreover, our generator learns the mapping between every two domains. Then, a reconstruction loss can be employed in the single-generator model (orange dashed arrows in Fig.  Unlike starGAN, we add the style code into the model with the Adaptive Instance Normalization (AdaIN) Layer  During the training, we incorporate multiple domain images and shuffle the ensembled dataset. A style code c can be generated randomly to indicate the target domain for the generator. The generator will transfer the input image into an image with the target style indicated by c, and the semantic content of the original input will not change. The model forward can be expressed by: where c is the style code that is randomly generated during the training. R is residual block. Enc and Dec are the encoder and decoder in the generator, respectively. The AdaIN layer in the decoder can be computed as: where x here is the feature map results of the previous layer, and s is the output of MLP. AdaIN aims to align the mean and variance of the input to match those of the desired style. The overall loss formulation of the generator is: where the L G adv is the adversarial loss and L rec is the image reconstruction loss. The c trg is the generated target style code and c org is the style code of the original input images."
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,3,Experimental Results,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,3.1,Dataset and Implementation Details,"In this work, we prepared six thick tissue slabs and obtained scanned images of ∼15,000 × 15,000 pixels using an open-top LS microscope with an excitation wavelength of 266 nm. After imaging, the specimens were sectioned and histologically processed with the standard protocol to obtain HS images for the model training. We chose one set of scanned images as training data, and the others were used for the testing. For the training, we extracted small image patches with the size of 128 × 128 randomly. During testing, we divided the tested whole-slide image into 256 × 256 patches with 16 pixels overlap to avoid artifacts. There are 11,643 extracted image patches in the testing set. Our model was implemented in PyTorch on a single NVIDIA GeForce RTX 3090 GPU. We trained our model with the Adam optimizer (with β 1 = 0.5 and β 2 = 0.999) "
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,3.2,Evaluation Metrics,"We quantitatively evaluate our model and results with Kernel Inception Distance (KID)  In this work, we observed that LS images of thick tissues share a similar style as fluorescence images of thin sections i.e., the cell nuclei are highlighted with positive contrast for both thin and thick tissues with the help of fluorescent labels. In this case, we used the model trained on LS images of thick tissues to test the fluorescence images of thin sections. Then, we could prepare the ground truth of thin sections for comprehensive comparison, as well as quantitative indicators, such as mean square error (MSE), structural similarity (SSIM), and peak signalto-noise ratio (PSNR). It is worth mentioning that when tested on the thinsection images, the model trained with thick-tissue images would underperform the model trained with thin-section images. Therefore, for virtual staining on thin slices, it is better to train on thin-section images as there are still some differences in details between data from thick tissues and thin sections. Here, we use the thin-section data only for model validation."
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,3.3,Quantitative and Qualitative Results,"In this paper, we select cycleGAN  Moreover, we quantitatively evaluate the model performance with FID and KID scores, as shown in Table  As no ground truth can be provided for the virtual staining of thick tissue. We collected 2 sets of thin mouse kidney sections for the model validation. We used the model trained with LS images of thick tissues to test the scanned images of prepared thin sections. In this situation, we can perform traditional histological staining to obtain ground truth for further comparison. Figure "
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,3.4,Ablation Analysis,"In this paper, we have two main hypotheses, one is that the model can benefit from data fusion from multiple domains due to the domain-invariant features, and the other is that the AdaIN-based style transfer is better in source image feature extraction compared with channel-wise style code concatenation.  We first verify the importance of domain-invariant features shared by multiple domains. The starGAN "
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,4,Conclusion,"This paper proposes a multiple histological image generation model for thick biological samples. This is undesirable in the traditional histopathology workflow as the chemical histological staining should be performed on the thin sections, and the same section cannot be stained with various stains simultaneously. We use slide-free microscopy to capture the thick tissues and translate the scanned images into multiple stained-versions. The model is optimized in an unsupervised manner, fitting the issue of large morphological mismatches between the scanned thick tissue and histologically stained images. Experiment results demonstrated the superiority and the great promise of the proposed method in developing a slide-free, cost-effective, and chemical-free histopathology staining pipeline."
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,,Fig. 1 .,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,,Fig. 2 .,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,,Fig. 3 .,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,,Fig. 4 .,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,,Fig. 5 .,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,,Table 1 .,
MulHiST: Multiple Histological Staining for Thick Biological Samples via Unsupervised Image-to-Image Translation,,90 0.039 82.49 0.048 84.44 0.055 1217.36 0.6475 17.50,
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,1,Introduction,"Colorectal cancer is a prevalent form of cancer characterized by colorectal adenocarcinoma, which develops in the colon or rectum's inner lining and exhibits glandular structures  Recently, diffusion model  In this paper, we propose a new method for gland instance segmentation based on the diffusion model. (1) Our method utilizes a diffusion model to perform denoising and tackle the task of gland instance segmentation in histology images. The noise boxes are generated from Gaussian noise, and the predicted ground truth (GT) boxes and segmentation masks are performed during the diffusion process. (2) To improve segmentation, we use instance-aware techniques to recover lost details during denoising. This includes employing a filter and a multi-scale Mask Branch to create a global mask and refine finer segmentation details. (3) To enhance object-background differentiation, we utilize Conditional Encoding to augment intermediate features with the original image encoding. This method effectively integrates the abundant information from the original image, thereby enhancing the distinction between the objects and the surrounding background. Our proposed method was trained and tested on the 2015 MIC-CAI Gland Segmentation (GlaS) Challenge dataset "
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,2,Method,"In this section, we present the architecture of our proposed method, which includes an Image Encoder, an Image Decoder, and a Mask Branch. The network structure is shown in Fig. "
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,2.1,Image Encoder,"We propose to perform subsequent operations on the features of the original image, so we use an Image Encoder for advanced feature extraction. The Image Encoder takes the original image as input and we use a convolutional neural network such as ResNet  The input image is x and the output is a high-level feature F R . where F is the ResNet. The Image Encoder operates only once and uses the F R as condition to progressively refine and generate predictions from the noisy boxes."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,2.2,Image Decoder,"We designed our model based on the diffusion model  A variance schedule β t ∈ (0, 1), t ∈ {1, ..., T } determines the amount of noise that is introduced at each stage. Alternatively, we can obtain a sample of z t from direct z 0 as follows: where ᾱt = t s=0 (1β s ), ∼ N (0, I). Our Image Decoder is based on diffusion model, which can be viewed as a noise-to-GT denoising process. In this setting, the data samples consist of a set of bounding boxes represented as z 0 , where z 0 is a set of N boxes. The neural network f θ (z t , t) is trained to predict z 0 from the z t based on the corresponding image x. In addition, to achieve complementary information by integrating the segmentation information from z t into the original image encoding, we introduce Conditional Encoding, which uses the encoding features of the current step to enhance its intermediate features. where D represent the decoder, E represent the encoder and m ∈ {1, ..., T }. We use Instance Aware Filters (IAF) during iterative sampling, which allows sharing parameters between steps. where F t f is the output feature of the filter."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,2.3,Mask Branch,"We have also utilized dynamic mask head  In this stage, we use the Mask Branch to fuse the different scale information of the FPN and output the mask feature F mask . The diffusion process decodes RoI features into local masks, and multi-scale features can be supplemented with more detailed information for predicting global masks to compensate for the detail lost in the diffusion process, and we believe that instance masks require a larger perceptual domain because of the higher demands on instance edges. Specifically, the instance mask can be generated by convolving the feature map F mask from the Mask Branch and F t f from the IAF , which is calculated as follows: where the predicted instance mask is denoted by s ∈ R H×W . The Mask FCN Head, denoted by MF H, is comprised of three 1 × 1 convolutional layers. We enhance our loss function by incorporating two components, L d and L s , and utilize the γ parameter to optimize the balance between these two losses. where the L s in our model represents the measure of overlap between the predicted instance mask and the ground truth s GT "
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,3,Experiments and Results,We presented the segmentation results of our model compared to the ground truth in Fig. 
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Data and Evaluation Metrics:,"We evaluated the effectiveness of the proposed model on two datasets: the GlaS dataset and the CRAG dataset. The GlaS dataset comprises 85 training and 80 testing images, divided into 60 images in Test A and 20 images in Test B. The CRAG dataset consists of 173 training and 40 testing images. We have adopted Vahadane method for stain normalization  We assessed the segmentation results using three metrics from the GlaS Challenge: (1) Object F1, which measures the accuracy of detecting individual glands, (2) Object Dice, which evaluates the volume-based accuracy of gland segmentation, and (3) Object Hausdorff, which assesses the shape similarity between the segmentation result and the ground truth. We assigned each method three ranking numbers based on these metrics and computed their sum to determine the final ranking for each method's overall performance."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Implementation Details:,"In our experiments, we choose the ResNet-50 with FPN as the backbone in the proposed method. The backbone is pretrained on ImageNet. Image decoder, Mask Branch and Mask FCN Head are trained end-toend. We trained on the GlaS and CRAG datasets in a Python 3.8.3 environment on Ubuntu 18.04, using PyTorch 1.10 and CUDA 11.4. During training, we utilized an SGD optimizer with a learning rate of 2.5 × 10 -5 and the weight decay as 10 -4 . We set diffusion timesteps T = 1000 and chose a linear schedule from β 1 = 10 -4 to β T = 0.02. Training was performed on A100 GPU with a batch size of 2."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Results on the GlaS Challenge Dataset:,"We conducted experiments to evaluate the performance of our proposed model by comparing it with the DSE model  Our proposed model demonstrated a enhancement in performance, surpassing the second-best method on both Test A and Test B datasets. Specifically, on Test A, we observed an improvement of 0.006, 0.01, and 1.793 in Object F1, Object Dice, and Object Hausdorf. Similarly, on Test B, resulting in an improvement of 0.022, 0.014 and 3.694 in Object F1, Object Dice, and Object Hausdorf, respectively. Although Test B presented a more challenging task due to the presence of complex morphology in the images, our proposed model demonstrated accurate segmentation in all cases. The experimental results highlighted the effectiveness of our approach in improving the accuracy of gland instance segmentation."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Results on the CRAG Dataset:,"The proposed model was additionally evaluated on the CRAG dataset by comparing it against the GCSBA-Net, DoubleU-Net, DSE model, MILD-Net, and DCAN. The average performance of these models is shown in Table  Ablation Studies: Our network utilizes the Mask Branch and Conditional Encoding to enhance performance and segmentation quality. Ablation studies on the GlaS and CRAG datasets confirm the effectiveness of these modules (Table "
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,4,Conclusion and Discussion,"In this paper, we propose a diffusion model based method for gland instance segmentation. By considering instance segmentation as a denoising process based on diffusion model. Our model contains three main parts: Image Encoder, Image Decoder, and Mask Branch. By utilizing a diffusion model with Conditional Encoding for denoising, we are able to improve the precision of instance localization while compensating for the missing details in the diffusion model. By incorporating multi-scale information fusion, our approach results in more accurate segmentation outcomes. Experimental results on the GlaS dataset and CRAG dataset show that our method surpasses state-of-the-art approach, demonstrating its effectiveness. Although our method demonstrates excellent performance in gland instance segmentation, challenges arise in certain scenarios characterized by irregular shapes, flattening, and overlapping. In such cases, our network tends to classify multiple small targets with unclear boundaries as a single object, indicating limitations in segmentation accuracy when dealing with high aggregation or overlap. This limitation may stem from the difficulty in accurately distinguishing fine details between instances and the incorrect identification of boundaries. To address these limitations, future work will focus on improving segmentation performance in challenging scenarios by specifically targeting three identified limitations: (1) Incorporate random noise during training to reduce reliance on bounding box information for denoising; (2) Explore more efficient methods for cross-step denoising in the diffusion model to improve processing time without compromising segmentation accuracy; and (3) Develop a more effective Conditional Encoding method to provide accurate instance context for noise filtering in discriminative tasks like nuclear segmentation."
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Fig. 1 .,
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Fig. 2 .,
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Fig. 3 .,
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Table 1 .,
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Table 2 .,
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,,Table 3 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,1,Introduction,"Immunohistochemical (IHC) staining is a widely used technique in pathology for visualizing abnormal cells that are often found in tumors. IHC chromogens highlight the presence of certain antigens or proteins by staining their corresponding antibodies. For instance, the HER2 (human epidermal growth factor receptor 2) biomarker is associated with aggressive breast tumor development and is essential in forming a precise treatment plan. Despite its capability to provide highly valuable diagnostic information, the process of IHC staining is very labor-intensive, time-consuming and requires specialized histotechnologists and laboratory equipments  At the other end of the spectrum, H&E (Hematoxylin and Eosin) staining, as the gold standard in histological staining, highlights the tissue structures and cell morphology. In routine diagnostics, on account of its much lower cost, an H&E-stained slide is prepared by pathologists in order to determine whether or not to also apply the IHC stains for a more precise assessment of the disease. Therefore, it is of great interest to have an algorithm that can automatically translate an H&E-stained slide into one that could be considered to have been stained with IHC while accurately predicting the target expression levels. To that end, researchers have recently proposed to use GAN-based Image-to-Image Translation (I2IT) algorithms for transforming H&E-stained slides into IHC. Despite the progress, the outstanding challenge in training such I2IT frameworks is the lack of aligned H&E-IHC image pairs, or in other words, the inconsistencies in the H&E-IHC groundtruth pairs. To explain, since re-staining a slice is physically infeasible, a matching pair of H&E-IHC slices are taken from two depth-wise consecutive cuts of the same tissue then stained and scanned separately. This inevitably prevents pixel-perfect image correspondences due to the slice-to-slice changes in cell morphology, staining-induced degradation (e.g. tissue-tearing), imaging artifacts that may vary among slices (e.g. camera out-offocus) and multi-slice registration errors. An example pair of patches is shown in Fig.  As a result, recent advances in H&E-to-IHC I2IT have mostly avoided using the inconsistent GT pairs and instead have imposed the cycle-consistency constraint  In this paper, we argue that the IHC slides, despite the disparities vis-a-vis their H&E counterparts, can still serve as useful targets for stain translation. The work we present in this paper is based on the important realization that even when pairs of consecutive tissue slices do not yield images that are pixel-perfect aligned, it is highly likely that the corresponding patches in the two stains share the same diagnostic label. For example, if the levels of expression in a region of the HER2 slide are high, the corresponding region in the H&E slide is highly likely to contain a high density of cancerous cells. Therefore, we set our goal to meaningfully leverage such correlations to benefit the H&E-to-IHC I2IT while being resilient to any inconsistencies. Toward this goal, we propose a supervised patchwise contrastive loss named the Adaptive Supervised PatchNCE (ASP) loss. Our formulation of this loss was inspired by the recent research findings that contrastive loss benefits model robustness under label noise  Lastly, to support further research in virtual IHC-restaining, we present the Multi-IHC Stain Translation (MIST) as a new public dataset. The MIST dataset contains 4k+ training and 1k testing aligned H&E-IHC patches for each of the following IHC stains that are critical for breast cancer diagnostics: HER2, Ki67, ER (Estrogen Receptor) and PR (Progesterone Receptor). We evaluated existing I2IT methods and ours for multiple IHC stains and demonstrate the superior performance achieved by our method both qualitatively and quantitatively. "
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,2,Method Description,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,2.1,The Supervised PatchNCE (SP) Loss,"Before getting to our ASP loss, we need to first introduce the SP loss as a robust means to learning from inconsistent GT image pairs. The SP loss was inspired by the findings in recent literature that demonstrate the positive effect of contrastive learning on boosting model robustness against label noise  The goal of the PatchNCE loss is to ensure the content is consistent across translation by maximizing the mutual information between the input and the corresponding output. It does so by minimizing a patch-based InfoNCE loss  where v, v + and v -are the embeddings of the anchor, positive and negative samples, respectively. With InfoNCE, the PatchNCE loss is set up as follows: given the anchor embedding ẑY of a patch in the output image, the positive z X is the embedding of the corresponding patch from the input image, while the negatives z X are embeddings of the non-corresponding ones, i.e. As for the SP loss, given the embedding of an output patch ẑY as anchor, we now designate the embedding of the corresponding patch in the groundtruth image z Y as the positive and the embeddings of the non-corresponding ones z Y as the negatives. We then use the same InfoNCE-based contrastive learning objective, i.e. L SP = L InfoNCE (ẑ Y , z Y , z Y ). A depiction of both the PatchNCE loss and the SP loss is given in Fig. "
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,2.2,The Adaptive Supervised PatchNCE (ASP) Loss,"To learn selectively from more consistent groundtruth locations, we further propose to augment the Supervised PatchNCE loss in an adaptive manner. The key idea here is to automatically recognize patch locations that are inconsistent and adapt the SP loss so that the severely inconsistent patch locations will have lesser effects on training. To measure the consistency at a given patch location, we use the cosine similarity between the embeddings of the generated IHC patch and the corresponding GT patch. In Fig.  Directly motivated by this observation, we first propose a weighting scheme for the SP loss. More specifically, we assign lower weights to patch locations that have low anchor-positive similarity values to alleviate the negative impacts the inconsistent targets may have on training. At training time t, the weight is a function of the anchor-positive cosine similarity. Examples of the weight function h(•) are shown in Fig.  In order to make the weighting scheme work in practice, we must also account for the phase of training. The intuition is that, during the initial phase of training, the network is not going to be able to discriminate between consistent patch locations from those that are inconsistent. Additionally, as shown in Fig.  To that end, we further augment the weight so that it is also a function of the training iterations. Such scheduling of the weights is done so that in the beginning of the training, the weights are uniform in order not to wrongly bias the network when the embeddings are still indiscriminative. And as training progresses, the selective weighting scheme is gradually enforced so that the inconsistent patch locations are treated with reduced weights. We call this gradual process of shifting the learning focus weight scheduling. Let t denote the current iteration and T the total number of training iterations. Then weight scheduling is achieved by using a scheduling function g( t T ). Various options of g(•) are shown in Fig.  We refer to the new augmented Supervised PatchNCE loss as the Adaptive Supervised PatchNCE (ASP) loss, which can be expressed as:   where W l t = s w l,s t is a normalization factor that maintains the total magnitude of the loss after applying the weights. Finally, the overall learning objective for our generator is as follows: where L GP is the Gaussian Pyramid based reconstruction loss from "
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,3,Experiments,"Datasets. The following datasets are used in our experiments: the Breast Cancer Immunohistochemical (BCI) challenge dataset  Implementation Details. For all of our models, we used ResNet-6Blocks as the generator and a 5-layer PatchGAN as the discriminator. We trained our networks with random 512 × 512 crops and a batch size of one. The Adam optimizer  Evaluation Metrics. We compare the methods using both paired and unpaired evaluation metrics. To compare a pair of images, generated and groundtruth, we use the standard SSIM (Structural Similarity Index Measure) and PHV (Perceptual Hash Value) as described in  Qualitative Evaluations. In Fig. "
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,4,Conclusion,"In this paper, we have proposed the Adaptive Supervised PatchNCE (ASP) loss for learning H&E-to-IHC stain translation with inconsistent GT image pairs. The adaptive logic in ASP is based on the intuition that inconsistent patch locations should contribute less to learning. We demonstrated that our proposed framework is able to achieve significant improvements both qualitatively and quantitatively over the existing approaches for translations to multiple IHC stains. Finally, we have made public our Multi-IHC Stain Translation dataset with the hope to assist further research towards accurate H&E-to-IHC stain translation."
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Fig. 1 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Fig. 2 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Fig. 3 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Fig. 4 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Fig. 5 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Table 1 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Table 2 .,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,,
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 61.
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,1,Introduction,"Deep neural networks (DNNs) have achieved remarkable success in medical image classification. However, the great success of DNNs relies on a large amount of training data with high-quality annotations, which is practically infeasible. The annotation of medical images requires expert domain knowledge, and suffers from large intra-and inter-observer variability even among experts, thus noisy annotations are inevitable in clinical practice. Due to the strong memorization ability, DNNs can easily over-fit the corrupted labels and degrade performance  An effective paradigm in learning with noisy labels (LNL) first selects clean samples, then formulates the LNL problem as semi-supervised learning (SSL) task by regarding the clean samples as a labeled set and noisy samples as an unlabeled set  In the SSL stage, most existing studies  In this paper, we present a novel two-stage framework to combat noisy labels in medical image classification. In the clean data selection stage, we propose a gradient and feature conformity-based method to identify the samples with clean labels. Specifically, the Gradient Conformity-based Selection (GCS) criterion selects clean samples that show higher conformity with the principal gradient of its labeled class. The Feature Conformity-based Selection (FCS) criterion identifies clean samples that show better alignment with the feature eigenvector of its labeled class. In the SSL stage, we propose a Sample Reliability-based Mixup (SRMix) to augment the training data without aggravating the error accumulation of pseudo labels. Specifically, SRMix interpolates each sample with reliable mixup partners which are selected based on their spatial reliability, temporal stability, and prediction confidence. Our main contributions are as follows: -We devise two novel criteria (i.e., GCS and FCS) to improve clean data selection by exploring the topological correlation and contextual information in the gradient and feature spaces. -We propose a novel SRMix method that selects reliable mixup partners to mitigate the error accumulation of pseudo labels and improve model training. -Extensive experiments show that our proposed framework is effective in combating label noise and outperforms state-of-the-art methods on two medical datasets with both synthetic and real-world label noise."
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,2,Method,"An overview of our proposed two-stage framework is shown in Fig.  , where the given label y i could be noisy or clean. In the clean data selection stage, we propose a gradient and feature conformity-based method to distinguish clean samples from the noisy dataset. As shown in Fig. "
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,2.1,Gradient and Feature Conformity-Based Clean Data Selection,Inspired by previous studies that optimization dynamics in the gradient space reflects the true class information 
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,,Gradient Conformity-Based Selection (GCS).,"The GCS aims to distinguish clean samples from noisy ones by exploring their optimization dynamics in the gradient space. Since training samples from the same class usually exhibit similar optimization dynamics  where f (x i ) is the feature vector of the sample x i , and x j denotes the K-Nearest Neighbors (KNN) of x i . p (x i ) is the probability of the most likely true class predicted by its KNN neighbors. Therefore, the gradient g(x i ) is very likely to reflect the true class information and optimization dynamics of x i . For each class, we select α% samples with the smallest loss as an anchor set A c , which is depicted in the shaded areas of the GCS in Fig.  which is the average gradient of all samples in the anchor set A c of the c-th class. Then, we can measure the similarity between the gradient of the sample x i and the principal gradient of class y i with the cosine similarity s g (x i ) = cos < g(x i ), g yi >. For the sample x i , if y i is a noisy label, g(x i ) should be consistent with the principal gradient of its true class and diverge from g yi , thus yielding small s g (x i ). By fitting Gaussian mixture models (GMM) on the similarity score s g (x i ), we can get c g (x i ) = GM M (s g (x i )), which represents the clean probability of the sample x i decided by the GCS criterion. To the best of our knowledge, this is the first work that explores gradient conformity for clean data selection. Feature Conformity-Based Selection (FCS). Since feature space is more robust to noisy labels than the output space  where f (x i ) denotes the feature vector of the sample x i in the anchor set A c of the c-th class. Then, we perform eigen-decomposition on the gram matrix: , where U c is the eigenvector matrix and Σ c is a diagonal matrix composed of eigenvalues. The principal eigenvector u c of U c is utilized to represent the distribution and contextual information of the c-th class. Then, for each sample x i , we measure its label quality based on the conformity of its feature f (x i ) with the principal eigenvector u yi of its given label: s f (x i ) = cos < f (x i ), u yi >. Samples that better align with the principal eigenvectors of their labeled class are more likely to be clean. According to the FCS criterion, the clean probability of the sample x i is obtained by Compared with existing methods that utilize classwise average features to represent contextual information  Integration of GCS and FCS. Finally, we average the clean probabilities estimated by the GCS and FCS criteria to identify clean data. As shown in Fig. "
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,2.2,Sample Reliability-Based Mixup (SRMix),"By regarding D clean as a labeled set and D noisy as an unlabeled set, we can formulate the LNL task into an SSL problem and employ Mixup  Intuitively, samples with reliable pseudo labels should have consistent predictions with their neighboring samples, stable predictions along sequential training epochs, and high prediction confidence. As shown in Fig.  where p(x j ) and p(x k ) are the pseudo labels of sample x j and its neighbor x k . Normalize() denotes the min-max normalization over all samples in each batch. If the pseudo label of a sample is more consistent with its neighbors, a higher R spatial (x j ) will be assigned, and vice versa. Second, for each sample x j , we keep the historical sequence of its predictions in the past T epochs, e.g., the prediction sequence at the t-th epoch is defined as P t (x j ) = [p t-T +1 (x j ), ..., p t-1 (x j ), p t (x j )]. The temporal stability of x j can be defined as: where p(x j ) is the average prediction of the historical sequence. According to Eq. (  , where max(p(x j )) denotes the prediction confidence of the pseudo label of the sample x j . The possibility of x j being chosen as a mixup partner is set as where τ R is a predefined threshold to filter out unreliable mixup partners. For each sample x i , we select a mixup partner x j with the probability p m (x j ) defined in Eq. (  Compared with the traditional Mixup, the SRMix can effectively mitigate error accumulation of the pseudo labels and promote model training."
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,3,Experiments,
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,3.1,Datasets and Implementation Details,"WCE Dataset with Synthetic Label Noise. The Wireless Capsule Endoscopy (WCE) dataset  The symmetric noise rate is set as 20%, 40%, 50%, and the pairflip noise rate is set as 40%. The model performance is measured by the average Accuracy (ACC) and Area Under the Curve (AUC) on the 5-fold test data. Histopathology Dataset with Real-World Label Noise. The histopathology image dataset is collected from Chaoyang Hospital "
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,3.2,Experimental Results,"Comparison with State-of-the-Art Methods. We first evaluate our method on the WCE dataset under diverse synthetic noise settings and show the results in Table  We then evaluate our method on the histopathology dataset with real-world label noise. As shown in Table  Ablation Study. To quantitatively analyze the contribution of the proposed components (i.e., GCS, FCS, and SRMix) in combating label noise, we perform an ablation study on the WCE dataset under 40% symmetric and pairflip noise. As shown in Table  More comprehensive analysis of the GCS and FCS criteria is provided in the supplementary material. Figure "
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,4,Conclusion,"In this paper, we present a two-stage framework to combat label noise in medical image classification tasks. In the first stage, we propose two novel criteria (i.e., GCS and FCS) that select clean data based on their conformity with the class-wise principal gradients and feature eigenvectors. By exploring contextual information and high-order topological correlations in the gradient space and feature space, our GCS and FCS criteria enable more accurate clean data selection and benefit LNL tasks. In the second stage, to mitigate the error accumulation of pseudo labels, we propose an SRMix method that interpolates input samples with reliable mixup partners which are selected based on their spatial reliability, temporal stability, and prediction confidence. Extensive experiments on two datasets with both diverse synthetic and real-world label noise indicate the effectiveness of our method."
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,,Fig. 1 .,
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,,Table 1 .,
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,,Table 2 .,
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,,Table 3 .,
Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_8.
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,1,Introduction,"Computational pathology powered by artificial intelligence (AI) shows promising applications in various clinical studies  Although diverse deep-learning approaches have been proposed in clinical studies to process and analyze histopathological images  In this paper, we introduce a deep computational pathology framework (dubbed as FPath) for forensic histopathological analysis. As shown in Fig.  The main technical contributions of our work are: 1) We design a double-tier backbone and a dedicated self-supervised learning strategy to capture discriminative instance-level histopathological patterns of postmortem organ tissues. The double-tier backbone combines CNN and transformer for local and non-local information fusion. To effectively train such a backbone to handle images acquired with varying microscopic magnifications, the dedicated self-supervised learning strategy leverages multiple complementary contrastive losses and regularization terms to concurrently maximize global and spatially fine-grained similarities between different views of the same instances/patches in an informative representation space. 2) We design a context-aware MIL branch to produce the bag-level discriminative representations for accurate and efficient postmortem histopathological recognition. Our MIL branch first refines instance embedding by leveraging a self-attention mechanism integrating positional embedding to model crosspatch associations for contextual information enhancement. Thereafter, an adaptive pooling operation is designed to learn deformable spatial attention to distill from contextually enhanced patch-level representations a holistic image-level representation for recognition. 3) Our FPath was applied to recognize postmortem organ tissues, a fundamental task in forensic pathology. To this end, we established a relatively largescale multi-domain database consisting of an experimental rat postmortem dataset and a real-world human decedent dataset, each with 19, 607 and 3, 378 images acquired at a specific microscopic magnification (e.g., 5×, 10×, 20×, and 40×), respectively. On such a multi-domain database, our FPath led to promising cross-domain generalization and state-of-the-art accuracy in recognizing seven different postmortem organs. "
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,2,Method,"The schematic diagram of our FPath is shown in Fig.  2.1 Self-supervised Contrastive Patch Embedding Double-Tier Backbone. Given patches from a postmortem histopathological image acquired at a specific magnification (i.e., 5×, 10×, 20×, or 40×), we adopt a backbone with a local branch (LB) and a global branch (GB) for instance/patch feature embedding. The LB is a ResNet50  Self-supervised Contrastive Learning Strategy. We leverage the idea of self-supervised representation learning to establish the double-tier backbone. Referring to MoCo  where m = 0.99 is the momentum parameter. Another key issue that determines the quality of the embedding from such a self-supervised strategy is the formulation of respective contrastive loss functions and regularization terms. Accordingly, we design a thorough contrastive learning strategy to capture fine-grained discriminative patterns of postmortem tissues under varying microscopic magnifications. That is, let X s and X t be two different views of an image patch X generated by a random data augmentation process. Our contrastive learning strategy concurrently encourages the global similarity and spatially fine-grained similarity between the corresponding feature embedding M s = f θ (X s ) and M t = f η (X t ) (∈ R h×w×C ). Also, two regularization terms are applied as auxiliary guidance to protect the informativeness and avoid collapses of the embedding learned by the backbone. Specifically, the global similarity between M s and M t is encouraged by minimizing a general cosine contrastive loss, such as where z g s = p sg (g sg (GAP(M s ))) and z g t = g tg (GAP(M t )), with GAP(•) standing for the global average pooling that produces feature vectors. In practice, forensic pathologists typically infer postmortem tissue type by evaluating the cellular compositions in multiple local regions. Accordingly, inspired by cross-view learning  , where ⊗ denotes tensor multiplication. Finally, the spatially fine-grained contrastive loss is quantified as where Besides, two additional regularization terms are further included to stabilize contrastive representation learning. Following  where is a small scalar to stabilize numerical computation, Z g s [d] denotes the dth dimension of Z g s , and th element in such a covariance matrix. According to  Overall, we combine Eqs. ( "
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,2.2,Context-Aware MIL,"Given the patch/instance-level representations of a histopathological image from the double-tier backbone, we further design a context-aware MIL framework to aggregate their information for postmortem tissue recognition. Given patch embeddings of a Microscope image, our context-aware MIL part contains two main steps, i.e., a multi-head self-attention to refine each patch's feature and an adaptive pooling step to distill all patches' information. In detail, we first adopt a multi-head self-attention (MSA) mechanism  where h ω1 (•) and h ω2 (•) are two linear projections with the same number of output units, symbol • denotes the Hadamard product between two tensors, and sof tmax(•) is performed across different instances to filter out uninformative patches and preserve discriminative patches in quantifying z bag for classification."
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,3,Experiments,
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,3.1,Data and Experimental Setup,"Rat Postmortem Histopathology Dataset. Ninety Sprague-Dawley adult male rats were executed by the spinal cord dislocation and placed in a constant temperature and humidity environment for 6-8 h. The animal experiments were approved by the Laboratory Animal Care Committee of the anonymous institution. Seven organs, i.e., brain, heart, kidney, liver, lung, pancreas, and spleen, were removed and placed in the formalin solution. Briefly, paraffin sections of these organ tissues were stained with the H&E solution. The H&E-stained sections were then analyzed by three forensic pathologists, who used Lercai LAS EZ microscopes to record the areas according to their expertise. Overall, five to ten images were recorded from a section at each magnification (i.e., 5×, 10×, 20×, and 40×). Finally, we split the 90 rats as training, validation, and test sets of 60, 10, and 20 rats, respectively, each with 13, 137, 2, 235, and 4, 325 images. Human Forensic Histopathology Dataset. The real forensic images were provided by the Forensic Judicial Expertise Center of the anonymous institution, after getting the informed consent of relatives. All procedures followed the requirements of local laws and institutional guidelines, and were approved and supervised by the Ethics Committee. A total of 32 decedents participated in this study. Four to six images were recorded at each of three magnifications (5×, 10×, and 20×) per H&E stained section. Similar to the rat dataset, the human dataset was selected from the same seven organs. Finally, the training, validation and test sets contain 1, 691 images, 628, and 1059 images, corresponding to 16, 6, and 10 different decedents, respectively. Experimental Details. Notably, the double-tier backbone was self-supervised and learned on the rat training set for 100 epochs by setting the mini-batch size as 1024, with the parameters initialized by the ImageNet pre-trained models. The training data were augmented by a histopathology-oriented strategy by combining different kinds of staining jitters, random affine transformation, Gaussian blurring, resizing, etc. The image(patch) dimension in our implementation was 224*224. The tuning parameters γ and λ in L all were set as 5 and 0.005, respectively. Thereafter, the MIL blocks on two different datasets were both trained by minimizing the cross-entropy loss for 20 epochs with the minibatch size setting as 32. The experiments were conducted on three PCs with twenty NVIDIA GEFORCE RTX 3090 GPUs."
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,3.2,Results of Self-supervised Contrastive Learning,"Our self-supervised double-tier backbone was compared with other state-ofthe-art self-supervised learning approaches, including balow twins "
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,3.3,Results of Multiple-Instance Learning,"Based upon the double-tier backbone learned on the rat training set, we compared our context-aware MIL with other MIL methods, including the gated attention-based approach (i.e., AB-MIL  In addition, we conducted LayerCAM-based analysis "
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,4,Conclusion,"In this study, we have proposed a context-aware MIL framework powered by self-supervised contrastive learning to learn fine-grained discriminative representations for postmortem histopathological recognition. The dedicated selfsupervised learning strategy concurrently maximizes multiple contrastive losses and regularization terms to deduce informative and discriminative instance embedding. Thereafter, the context-aware MIL framework adopts MSA followed by an adaptive pooling operation to distill from all instances a holistic bag/image-level representation. The experimental results on a relatively largescale database suggest the state-of-the-art postmortem recognition performance of our method."
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,,Fig. 1 .,
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,,Fig. 2 .,
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,,Table 1 .,
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,,Table 2 .,
Forensic Histopathological Recognition via a Context-Aware MIL Network Powered by Self-supervised Contrastive Learning,,Table 3 .,
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,1,Introduction,"Aortic stenosis (AS) is a common heart valve disease characterized by the calcification of the aortic valve (AV) and the restriction of its movement. It affects 5% of individuals aged 65 or older  To alleviate this issue, deep neural network (DNN) models have been proposed for automatic assessment of AS directly from two-dimensional B-mode echo, a modality more commonly used in point-of-care settings. Huang et al.  Explainable AI (XAI) methods can provide explanations of a DNN's decision making process and can generally be categorized into two classes. Post-hoc XAI methods explain the decisions of trained black-box DNNs. For example, gradientbased saliency maps  There are two limitations to applying current prototype-based methods to the task of classifying AS severity from echo cine series. First, prototypes should be spatio-temporal instead of only spatial, since AS assessment requires attention to small anatomical regions in echo (such as the AV) at a particular phase of the heart rhythm (mid-systole). Second, user variability in cardiac view acquisition and poor image quality can complicate AV visualization in standard PLAX and PSAX views. The insufficient information in such cases can lead to more plausible diagnoses than one. Therefore, a robust solution should avoid direct prediction and notify the user. These issues have been largely unaddressed in previous work. We propose ProtoASNet (Fig. "
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,2,Methods,
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,2.1,Background: Prototype-Based Models,"Prototype-based models explicitly make their decisions using similarities to cases in the training set. These models generally consist of three key components structured as h(g(f (x))). Firstly, f (.) is a feature encoder such as a ConvNet that maps images x ∈ R Ho×Wo×3 to f (x) ∈ R H×W ×D , where H, W , and D correspond to the height, width, and feature depth of the ConvNet's intermediate layer, respectively. Secondly, g(.) ∈ R H×W ×D → R P is a prototype pooling function that computes the similarity of encoded features f (x) to P prototype vectors. There are K learnable prototypes defined for each of C classes, denoted as p c k . Finally, h(.) ∈ R P → R C is a fully-connected layer that learns to weigh the input-prototype similarities against each other to produce a prediction score for each class. To ensure that the prototypes p  Such models are inherently interpretable since they are enforced to first search for similar cases in the training set and then to compute how these similarities contribute to the classification. As a result, they offer a powerful approach for identifying and classifying similar patterns in data."
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,2.2,ProtoASNet,"Feature Extraction. The overall structure of ProtoASNet is shown in Fig.  The feature extraction layer consists of a convolutional backbone, in our case the first three blocks of a pre-trained R(2+1)D-18  where |.| is the absolute value and • is the Hadamard product. Prototype Pooling. The similarity score of a feature vector f p c k and prototype p c k is calculated using cosine similarity, which is then shifted to [0, 1]: ). ( Prototypes for Aleatoric Uncertainty Estimation. In Fig.  where σ denotes Softmax normalization in the output of h(.), y and ŷ are the ground truth and the predicted probabilities, respectively, and λ abs is a regularization constant. When projecting p u k to the nearest extracted feature from training examples, we relax the requirement in Eq. (  Class-Wise Similarity Score. The fully connected (FC) layer h(.) is a dense mapping from prototype similarity scores to prediction logits. Its weights, w h , are initialized to be 1 between class c and the corresponding prototypes and 0 otherwise to enforce the process to resemble positive reasoning. h(.) produces a score for membership in each class and for α. Loss Function. As in previous prototype-based methods  3 Experiments and Results"
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,3.1,Datasets,We conducted experiments on a private AS dataset and the public TMED-2 dataset 
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,3.2,Implementation Details,"To better compare the results with TMED-2 dataset, we adopted their labeling scheme of no AS (normal), early AS (mild), and significant AS (moderate and severe) in our private dataset. We split longer cines into 32-frame clips which are approximately one heart cycle long. In both layers of the feature module, we used D convolutional filters, while the three layers in the ROI module had D, D  2 , and P convolutional filters, preventing an abrupt reduction of channels to the relatively low value of P . In both modules, we used kernel size of 1×1×1. We set D = 256 and K = 10 for AS class and aleatoric uncertainty prototypes. Derived from the hyperparameter selection of ProtoPNet "
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,3.3,Evaluations on Private Dataset,Quantitative Assessment. In Table  Table 
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,,Method,"Ablation Study. We assessed the effect of removing distinct components of our design: uncertainty prototypes (L abs , p u k ), clustering and separation (L clst , L sep ), and push mechanism. As shown in Table "
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,3.4,"Evaluation on TMED-2, a Public Dataset","We also applied our method to TMED-2, a public image-based dataset for AS diagnosis. Consistent with "
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,4,Conclusion,"We introduce ProtoASNet, an interpretable method for classifying AS severity using B-mode echo that outperforms existing black-box methods. ProtoASNet identifies clinically relevant spatio-temporal prototypes that can be visualized to improve algorithmic transparency. In addition, we introduce prototypes for estimating aleatoric uncertainty, which help flag difficult-to-diagnose scenarios, such as videos with poor visual quality. Future work will investigate methods to optimize the number of prototypes, or explore out-of-distribution detection using prototype-based methods."
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,,Fig. 1 .,
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,,,
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,,Fig. 2 .,
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,,Table 2 .,
ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 36.
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,1,Introduction,"Histopathology is considered the gold standard for diagnosing and treating many cancers  Different resolution levels in the WSI pyramids contain different and complementary information  In this paper, we present a novel Hierarchical Interaction Graph-Transformer framework (i.e., HIGT) to simultaneously capture both local and global information from WSI pyramids with a novel Bidirectional Interaction module. Specifically, we abstract the multi-resolution WSI pyramid as a heterogeneous hierarchical graph and devise a Hierarchical Interaction Graph-Transformer architecture to learn both short-range and long-range correlations among different image patches within different resolutions. Considering that the information from different resolutions is complementary and can benefit each other, we specially design a Bidirectional Interaction block in our Hierarchical Interaction ViT mod- ule to establish communication between different resolution levels. Moreover, a Fusion block is proposed to aggregate features learned from the different levels for slide-level prediction. To reduce the tremendous computation and memory cost, we further adopt the efficient pooling operation after the hierarchical GNN part to reduce the number of tokens and introduce the Separable Self-Attention Mechanism in Hierarchical Interaction ViT modules to reduce the computation burden. The extensive experiments with promising results on two public WSI datasets from TCGA projects, i.e., kidney carcinoma (KICA) and esophageal carcinoma (ESCA), validate the effectiveness and efficiency of our framework on both tumor subtyping and staging tasks. The codes are available at https:// github.com/HKU-MedAI/HIGT."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,2,Methodology,Figure 
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,2.1,Graph Construction,"As shown in Fig.  where t, r i , p i,j ∈ R 1×C correspond to the feature embeddings of each patch in thumbnail, region, and patch levels, respectively. N is the total number of the region nodes and M is the number of patch nodes belonging to a certain region node, and C denotes the dimension of feature embedding (1,024 in our experiments). Based on the extracted feature embeddings, we construct a hierarchical graph to characterize the WSI, following previous H 2 -MIL work "
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,2.2,Hierarchical Graph Neural Network,"To learn the short-range relationship among different patches within the WSI pyramid, we propose a new hierarchical graph message propagation operation, called RAConv+. Specifically, for any source node j in the hierarchical graph, we define the set of it all neighboring nodes at resolution k as N k and k ∈ K. Here K means all resolutions. And the h k is the mean embedding of the node j's neighboring nodes in resolution k. And h j is the embedding of the neighboring nodes of node j in resolution k and h j ∈ N k . The formula for calculating the attention score of node j in resolution-level and node-level: where α j,j is the attention score of the node j to node j and h j is the source node j embedding. And U , V , a and b are four learnable layers. The main difference from H2-MIL  where A represents the attention score matrix, and the attention score for the j-th row and j -th column of the matrix is given by Eq. ( "
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,2.3,Hierarchical Interaction ViT,"We further propose a Hierarchical Interaction ViT (HIViT) to learn long-range correlation within the WSI pyramids, which includes three key components: Patch-level (PL) blocks, Bidirectional Interaction (BI) blocks, and Region-level (RL) blocks."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,,Patch-Level Block.,"Given the patch-level feature set P = N i=1 P i , the PL block learns long-term relationships within the patch level: where l = 1, 2, ..., L is the index of the HIViT block. P L(•) includes a Separable Self Attention (SSA)  Bidirectional Interaction Block. We propose a Bidirectional Interaction (BI) block to establish communication between different levels within the WSI pyramids. The BI block performs bidirectional interaction, and the interaction progress from region nodes to patch nodes is: where the SE(•) means the Sequeeze-and-Excite layer  where the MEAN(•) is the operation to get the mean value of patch nodes set P l+1 i associated with the i-th region node and P l+1 1 ∈ R 1×C and the C is the feature channel of nodes, and Rl+1 is the region nodes set after interaction. Region-Level Block. The final part of this module is to learn the long-range correlations of the interacted region-level nodes: where l = 1, 2, ..., L is the index of the HIViT module, and RL(•) has a similar structure to P L(•)."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,2.4,Slide-Level Prediction,"In the final stage of our framework, we design a Fusion block to combine the coarse-grained and fine-grained features learned from the WSI pyramids. Specifically, we use an element-wise summation operation to fuse the coarse-grained thumbnail feature and patch-level features from the Hierarchical Interaction GNN part, and then further fuse the fine-grained patch-level features from the HIViT part with a concatenation operation. Finally, a 1 × 1 convolution and mean operation followed by a linear projection are employed to produce the slide-level prediction."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,3,Experiments,"Datasets and Evaluation Metrics. We assess the efficacy of the proposed HIGT framework by testing it on two publicly available datasets (KICA and ESCA) from The Cancer Genome Atlas (TCGA) repository. The datasets are described below in more detail: -KICA dataset. The KICA dataset consists of 371 cases of kidney carcinoma, of which 279 are classified as early-stage and 92 as late-stage. For the tumor typing task, 259 cases are diagnosed as kidney renal papillary cell carcinoma, while 112 cases are diagnosed as kidney chromophobe. -ESCA dataset. The ESCA dataset comprises 161 cases of esophageal carcinoma, with 96 cases classified as early-stage and 65 as late-stage. For the tumor typing task, there are 67 squamous cell carcinoma cases and 94 adenocarcinoma cases. Experimental Setup. The proposed framework was implemented by PyTorch  Ablation Analysis. We further conduct an ablation study to demonstrate the effectiveness of the proposed components. The results are shown in Table "
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,4,Conclusion,"In this paper, we propose HIGT, a framework that simultaneously and effectively captures local and global information from the hierarchical WSI. Firstly, the constructed hierarchical data structure of the multi-resolution WSI is able to offer multi-scale information to the later model. Moreover, the redesigned H2-MIL and HIViT capture the short-range and long-range correlations among varying magnifications of WSI separately. And the bidirectional interaction mechanism and fusion block can facilitate communication between different levels in the Transformer part. We use IHPool and apply the Separable Self Attention to deal with the inherently high computational cost of the Graph-Transformer model. Extensive experimentation on two public WSI datasets demonstrates the effectiveness and efficiency of our designed framework, yielding promising results. In the future, we will evaluate on other complex tasks such as survival prediction and investigate other techniques to improve the efficiency of our framework."
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,,Fig. 1 .,
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,,Table 1 .,
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,,Table 2 .,
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,,Table 3 .,
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,,.90 ± 0.60 97.90 ± 1.40 Fig,
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 73.
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,1,Introduction,"Image classification is a significant challenge in medical image analysis. Although some classification methods achieve promising performance on balanced and clean medical datasets, balanced datasets with high-accuracy annotations are time-consuming and expensive. Besides, pruning clean and balanced datasets require a large amount of crucial clinical data, which is insufficient for large-scale deep learning. Therefore, we focus on a more practical yet unexplored setting for handling imbalanced medical data with noisy labels, utilizing all available lowcost data with possible noisy annotations. Noisy imbalanced datasets arise due to the lack of high-quality annotations  Existing approaches for non-ideal medical image classification can be summarized into noisy classification, imbalanced recognition, and noisy imbalanced identification. Noisy classification approaches  In this work, we propose a multi-stage noise removal framework to address these concerns jointly. The main contributions of our work include: 1) We decompose the negative effects in practical medical image classification, 2) We minimize the invariant risk to tackle noise identification influenced by multiple factors, enabling the classifier to learn causal features and be distribution-invariant, 3) A re-scaling class-aware Gaussian Mixture Modeling (CGMM) approach is proposed to distinguish noise labels under various class hardness, 4) We evaluate our method on two medical image datasets, and conduct thorough ablation studies to demonstrate our approach's effectiveness."
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,2,Method,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,2.1,Problem Formulation,"In the noisy imbalanced classification setting, we denote a medical dataset as {(x i , y i )} N i=1 where y i is the corresponding label of data x i and N is the total amount of instances. Here y i may be noisy. Further, we split the dataset according to class categories. Then, we have {D j } M j=1 , where M is the number of classes; D j denotes the subset for class j. In each subset containing N j samples, the data pairs are expressed as {(x j i , y j i )} Nj i=1 . Without loss of generality, we order the classes as N 1 > N 2 > ... > N M -1 > N M . Further, we denote the backbone as H(•; θ), X → Z mapping data manifold to the latent manifold, the classifier head as G(•; γ), Z → C linking latent space to the category logit space, and the identifier as F(•; φ), Z → C. We aim to train a robust medical image classification model composed of a representation backbone and a classifier head on label noise and imbalance distribution, resulting in a minimized loss on the testing dataset:"
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,2.2,Mapping Correction Decomposition,"We decompose the non-linear mapping p(y = c|x) as a product of two space mappings p G (y = c|z) • p H (z|x). Given that backbone mapping is independent of noisy imbalanced effects, we conduct further disentanglement by defining e as the negative effects and P as constant for fixed probability mappings: The induction derives from the assumption that the incorrect mapping p G (y = c|z, e) conditions on both pure latent to logits mapping p G (y = c|z) and adverse effects p G (y = c|e). By Bayes theorem, we decompose the effect into imbalance, noise, and mode (hardness), where the noise effect depends on skew distribution and hardness effect; and the hardness effect is noise-invariant. Currently, noise removal methods only address pure noise effects (p G (e n |y = c)), while imbalance recognition methods can only resolve imbalanced distribution, which hinders the co-removal of adverse influences. Furthermore, the impact of hardness effects has not been considered in previous studies, which adds an extra dimension to noise removal. In essence, the fundamental idea of noisy classification involves utilizing clean data for classifier training, which determines the importance of noise identification and removal. To address these issues, we propose a mapping correction approach that combines independent noise detection and removal techniques to identify and remove noise effectively."
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,2.3,Minimizing Invariant Risk Across Multi-distributions,"Traditional learning with noisy label methods mainly minimize empirical risk on training data. However, they fail to consider the influence of imbalanced distributions, which might cause a biased gradient direction on the optimization subspace. Following  where ε represents an environment (distribution) for classifier F φ and backbone H θ ; and L denotes the empirical loss for classification. Since the incorrect mapping is not caused by feature representation, the backbone H θ is fixed during the optimization. By transferring the constraints into a penalty in the optimizing object, we solve this problem by learning the constraint scale ω  Ideally, the noise removal process is distribution-invariant if data is uniformly distributed w.r.t. classes. By the law of large numbers, all constructed distributions should be symmetric according to the balanced distribution to obtain a uniform expectation. To simplify this assumption, we construct three different data distributions "
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,2.4,Rescaling Class-Aware Gaussian Mixture,"Existing noise labels learning methods  which produces more accurate and independent measurements of label quality. Rather than relying on the assumption that confidence distributions of training samples depend solely on their label quality, RCGM solves the effect of class hardness in noisy detection by individually clustering the scores in each category. This overcomes the limitations of global clustering methods and significantly enhances the accuracy of noise identification even when class hardness varies. Instead of assigning a hard label to the potential noisy data as  which is then multiplied by a hyperparameter s if the instance is predicted as noise to reduce its weight in the finetuning. With a pre-defined noise selection threshold as τ , we have the final clean score as:"
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,2.5,Overall Learning Framework for Imbalanced and Noisy Data,"In contrast to two-stage noise removal and imbalance classification techniques, our approach applies a multi-stage protocol: warm-up phases, noise removal phases, and fine-tuning phases as shown in Fig.  where α kl := v(x k ) v(x l ) denotes the balanced scale; and {(x kl , ŷkl )} are the mixed clean data for classifier fine-tuning. Sqrt sampler is applied to re-balance the data, and cross-stage KL "
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,3,Experiment,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,3.1,Dataset and Evaluation Metric,"We evaluated our approach on two medical image datasets with imbalanced class distributions and noisy labels. The first dataset, HAM10000 "
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,3.2,Implementation Details,We mainly follow the training settings of FCD 
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,3.3,Comparison with State-of-the-Art Methods,We compare our model with state-of-the-art methods which contain noisy methods (including DivideMix 
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,3.4,Ablation Studies,As shown in Fig. 
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,4,Conclusion and Discussion,"We propose a multi-step framework for noisy long-imbalanced medical image classification. We address three practical adverse effects including data noise, imbalanced distribution, and class hardness. To solve these difficulties, we conduct Multi-Environment Risk Minimization (MER) and rescaling class-aware Gaussian Mixture Modeling (RCGM) together for robust feature learning. Extensive results on two public medical image datasets have verified that our framework works on the noisy imbalanced classification problem. The main limitation of our work is the manually designed multi-stage training protocol which lacks simplicity compared to end-to-end training and warrants future simplification."
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,,Fig. 1 .,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,,Fig. 2 .,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,,Fig. 3 .,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,,Table 1 .,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,,,
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 30.
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,1,Introduction,"Colorectal cancer is the third most common malignant tumor, and nearly half of all patients with colorectal cancer develop liver metastasis during the course of the disease  Extensive existing works have demonstrated the power of deep learning on various spatial-temporal data, and can potentially be applied towards the problem of CRLM. For example, originally designed for natural data, several mainstream models such as E3D-LSTM  However, all these methods have only demonstrated their effectiveness towards 3D/4D data (i.e., time-series 2D/3D images), and it is not clear how to best extend them to work with the 5D CECT data. Part of the reason is due to the lack of public availability of such data. When extending these models towards 5D CECT data, some decisions need to be made, for example: 1) What is the most effective way to incorporate the phase information? Simply concatenating different phases together may not be the optimal choice, because the positional information of the same CT slice in different phases would be lost. 2) Shall we use uni-directional LSTM or bi-direction LSTM? E3D-LSTM  In this paper, we investigate how state-of-art deep learning models can be applied to the CRLM prediction task using our 5D CECT dataset. We evaluate the effectiveness of bi-directional LSTM and explore the possible method of incorporating different phases in the CECT dataset. Specifically, we show that the best prediction accuracy can be achieved by enhancing E3D-LSTM "
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,2,Dataset and Methodology,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,2.1,Dataset,"Our dataset follows specific inclusion criteria: -No tumor appears on the CT scans. That means patients have not been diagnosed as CRLM when they took the scans. -Patients were previously diagnosed with colorectal cancer TNM stage I to stage III, and recovered from colorectal radical surgery. -Patients have two or more times of CECT scans. -We already determined whether or not the patients had liver metastases within 2 years after the surgery, and manually labeled the dataset based on this. -No potential focal infection in the liver before the colorectal radical surgery. -No metastases in other organs before the liver metastases. -No other malignant tumors. Our retrospective dataset includes two cohorts from two hospitals. The first cohort consists of 201 patients and the second cohort includes 68 patients. Each scan contains three phases and 100 to 200 CT slices with a resolution of 512×512. Patients may have different numbers of CT scans, ranging from 2 to 6, depending on the number of follow-up visits. CT images are collected with the following acquisition parameters: window width 150, window level 50, radiation dose 120 kV, slice thickness 1 mm, and slice gap 0.8 mm. All images underwent manual quality control to exclude any scans with noticeable artifacts or blurriness and to verify the completeness of all slices. Additional statistics on our dataset are presented in Table "
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,2.2,Methods,"Numerous state-of-the-art deep learning models are available to effectively process 4D data. In this paper, we will evaluate some of the most popular ones: 1) SaConvLSTM, introduced by Lin et al.  All of these models need to be modified to handle 5D CECT datasets. A straightforward way to extend them is simply concatenating the A phase and V phase together, thus collapsing the 5D dataset to 4D. However, such an extension may not be the best way to incorporate the 5D spatiotemporal information, because the positional information of the same CT slice in different phases would be lost. Below we explore an alternative modification multi-plane bi-directional LSTM (MPBD-LSTM), based on E3D-LSTM, to handle the 5D data."
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,MPBD-LSTM.,"The most basic building block in MPBD-LSTM is the 3D-LSTM modules. Each 3D-LSTM module is composed of two E3D-LSTM cells  where -→ h v,t and ←h v,t are the output hidden state of the forward pass and backward pass of phase v at timestamp t, and σ is the function which is used to combine these two outputs, which we choose to use a summation function to get the summation product of these two hidden states. Therefore, the output of the bi-directional LSTM module presented in Fig.  in which ⊕ stands for summation. After this, the output y v,t0 is passed into the bi-directional LSTM module in the next layer and viewed as input for this module. Figure  An alternative approach is to additionally connect two planes by combining the hidden states of 3D-LSTM modules and taking their average if a module receives two inputs. However, we found that such design actually resulted in a worse performance. This issue will be demonstrated and discussed later in the ablation study. In summary, the MPBD-LSTM model comprises two planes, each of which contains three 3D-LSTM stacks with two modules in each stack. It modifies E3D-LSTM by using bi-directional connected LSTMs to enhance communication between different timestamps, and a multi-plane structure to simultaneously process multiple phases."
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,3,Experiments,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,3.1,Data Augmentation and Selection,"We selected 170 patients who underwent three or more CECT scans from our original dataset, and cropped the images to only include the liver area, as shown in Fig. "
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,3.2,Experiment Setup,"As the data size is limited, 10-fold cross-validation is adopted, and the ratio of training and testing dataset is 0.9 and 0.1, respectively. Adam optimizer "
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,4,Results and Discussion,Error Analysis. In Fig. 
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,5,Conclusion,"In this paper, we put forward a 5D CECT dataset for CRLM prediction. Based on the popular E3D-LSTM model, we established MPBD-LSTM model by replacing the uni-directional connection with the bi-directional connection to better capture the temporal information in the CECT dataset. Moreover, we used a multiplane structure to incorporate the additional phase dimension. MPBD-LSTM achieves the highest AUC score of 0.790 among state-of-the-art approaches. Further research is still needed to improve the AUC."
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,Fig. 1 .,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,Fig. 2 .,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,Table 1 .,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,Table 2 .,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,Table 3 .,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,Table 4 .,
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 37.
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,1,Introduction,Accurate diagnosis plays an important role in achieving the best treatment outcomes for people with cancer  The introduction of digital pathology (DP) has enabled application of machine learning approaches to extract otherwise inaccessible diagnostic and prognostic information from H&E-stained whole slide images (WSIs) 
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,2,Embedding Extraction Scheme,"Transfer learning using backbones pretrained on natural images is a common method that addresses the challenge of using data sets that largely lack annotation. However, using backbones pretrained on natural images is not optimal for classification of clinical images "
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,2.1,Tile-Level Embeddings,"Following standard practice, we extracted tiles with dimensions of 256 × 256 pixels from WSIs (digitized at 40 × magnification) on a spatial grid without overlap. Extracted tiles that contained artifacts were discarded (e.g., tiles that had an overlap of >10% with background artifacts such blurred areas or pen markers). We normalized the tiles for stain color using a U-Net model for stain normalization  To create the tile-level embeddings, we used the method proposed by "
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,2.2,Cell-Level Embeddings,"Tiles extracted from WSIs may contain different types of cells, as well as noncellular tissue such as stroma and blood vessels and nonbiological features (e.g., glass). Celllevel embeddings may be able to extract useful information, based on the morphological appearance of individual cells, that is valuable for downstream classification tasks but would otherwise be masked by more dominant features within tile-level embeddings. We extracted deep cell-level embeddings by first detecting individual cellular boundaries using StarDist  Because of heterogeneity in the size of cells detected, each 32 × 32-pixel cellpatch image contained different proportions of cellular and noncellular features. Higher proportions of noncellular features in an image may cause the resultant embeddings to be dominated by noncellular tissue features or other background features. Therefore, to limit the information used to create the cell-level embeddings to only cellular features, we removed portions of the cell-patch images that were outside of the segmented nuclei by setting their pixel values to black (RGB 0, 0, 0). Finally, to prevent the size of individual nuclei or amount of background in each cell-patch image from dominating over the celllevel features, we modified the ResNet50 Global Average Pooling layer to only average the features inside the boundary of the segmented nuclei, rather than averaging across the whole output tensor from the CNN."
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,2.3,Combined Embeddings,"To create a combined representation of the tile-level and cell-level embeddings, we first applied a nuclei segmentation network to each tile. Only tiles with ≥ 10 cells per tile, excluding any cells which overlapped the tile border, were included for embedding extraction. For the included tiles, we extracted the tile-level embeddings as described in Sect. 2.1 and for each detected cell we extracted the cell-level embeddings as described in Sect. 2.2. We then calculated the mean and standard deviation of the vectors of the cell-level embeddings for each tile and concatenated those to each tile-level embedding. This resulted in a combined embedding representation with a total size of 1536 pixels (1024 + 256 + 256). In addition to the WSI classification results presented in the next sections, we also performed experiments to compare the ability of combined embeddings and tile-level embeddings to predict nuclei-related features that were manually extracted from the images and to identify tiles where nuclei had been ablated. The details and results of these experiments are available in supplementary materials and provide further evidence of the improved ability to capture cell-level information when using combined embeddings compared with tile-level embeddings alone."
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,3,WSI Classification Tasks,For each classification task we compared different combinations of tile-level and celllevel embeddings using a MIL framework. We also compared two different MIL architectures to aggregate the embeddings for WSI-level prediction. The first architecture used an attention-MIL (A-MIL) network  Transformer (Xformer) was used as the second MIL architecture 
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,3.1,Data,"We tested our feature representation method in several classification tasks involving WSIs of H&E-stained histopathology slides. The number of slides per class for each classification task are shown in Fig.  For these two tasks we used artifact-free tiles from tumor regions detected with an in-house tumor detection model. For breast cancer metastasis detection in lymph node tissue, we used WSIs of H&Estained healthy lymph node tissue and lymph node tissue with breast cancer metastases from the publicly available CAMELYON16 challenge data set  For cell of origin (COO) prediction of activated B-cell like (ABC) or germinal center B-cell like (GCB) tumors in diffuse large B-cell lymphoma (DLBCL), we used data from the phase 3 GOYA (NCT01287741) and phase 2 CAVALLI (NCT02055820) clinical trials, hereafter referred to as CT1 and CT2, respectively. All slides were H&E-stained and scanned using Ventana DP200 scanners at 40× magnification. CT1 was used for training and testing the classifier and CT2 was used only as an independent holdout data set. For these data sets we used artifact-free tiles from regions annotated by expert pathologists to contain tumor tissue."
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,4,Model Classification Performance,"For the HER2 prediction, ER prediction, and metastasis detection classification tasks, combined embeddings outperformed tile-level only embeddings irrespective of the downstream classifier architecture used (Fig.  For COO classification in DLBCL, not only did the combined embeddings achieve better performance than the tile-level only embeddings with both the Xformer and A-MIL architectures (Fig. "
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,4.1,Model Explainability,Tile-based approaches in DP often use explainability methods such as Gradient-weighted Class Activation Mapping 
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Cellular Explainability Method. The cellular average embedding is,"where e ij ∈ R 256 is the cellular embedding extracted from every detected cell in the tile j i ∈ 1, 2, . . . , N j where N j is the number of cells in the tile j. This can be rewritten as a weighted average of the cellular embeddings where w i ∈ R 256 are the per cell attention weights that if initialized to 0 result in the original cellular average embedding. The re-formulation does not change the result of the forward pass since w i are not all equal. Note that the weights are not learned through training but calculated per cell at inference time to get the per cell contribution. We computed the gradient of the output category (of the classification method applied on top of the computed embedding) with respect to the attention weights w i : grad i = ∂Score i /∂w i and visualized cells that received positive and negative gradients using different colors. Visual Example Results. Examples of our cellular explainability method applied to weakly supervised tumor detection on WSIs from the CAMELYON16 data set using A-MIL are shown in Fig.  In this case, cells with positive attention gradients that shifted the output towards a classification of GCB were labeled green and cells with negative attention gradients that shifted the classification towards ABC were labeled red. Cells with positive attention gradients were mostly smaller lymphoid cells with low grade morphology or were normal lymphocytes, whereas cells with negative attention gradients were more frequently larger lymphoid cells with high grade morphology (Fig. "
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,5,Conclusions,"We describe a method to capture both cellular and texture feature representations from WSIs that can be plugged into any MIL architecture (e.g., CNN or Xformer-based), as well as into fully supervised models (e.g., tile classification models). Our method is more flexible than other methods (e.g., Hierarchical Image Pyramid Transformer) that usually capture the hierarchical structure in WSIs by aggregating features at multiple levels in a complex set of steps to perform the final classification task. In addition, we describe a method to explain the output of the classification model that evaluates the contributions of histologically identifiable cells to the slide-level classification. Tilelevel embeddings result in good performance for detection of tumor metastases in lymph nodes. However, introducing more cell-level information, using combined embeddings, resulted in improved classification performance. In HER2 and ER prediction tasks for breast cancer we demonstrate that addition of a cell-level embedding summary to tilelevel embeddings can boost model performance by up to 8%. Finally, for COO prediction in DLBCL and breast cancer metastasis detection in lymph nodes, we demonstrated the potential of our explainability method to gain insights into previously unknown associations between cellular morphology and disease biology."
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Fig. 1 .,
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Fig. 2 .,
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Fig. 3 .,
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Fig. 4 .,
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Fig. 5 .,
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,,Fig. 6 .,
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,1,Introduction,"Periodontal disease is a set of inflammatory gum infections damaging the soft tissues in the oral cavity, and one of the most common issues for oral health  If not diagnosed and treated promptly, it can develop into irreversible loss of the bone and tissue that support the teeth, eventually causing tooth loosening or even falling out. Thus, it is of great significance to accurately classify periodontal disease in an early stage. However, in clinics, dentists have to measure the clinical attachment loss (CAL) of each tooth by manual probing, and eventually determine the severity and progression of periodontal disease mainly based on the most severe area  With the development of computer techniques, computer-aided diagnosis has been widely applied for lesion detection  To address the aforementioned challenges and limitations of previous methods, we propose HC-Net, a novel hybrid classification framework for automatic periodontal disease diagnosis from panoramic X-ray images, which significantly learns from clinical probing measurements instead of any radiographic manual annotations. The framework learns upon both tooth-level and patient-level with three major components, including tooth-level classification, patient-level classification, and an adaptive noisy-OR gate. Specifically, tooth-level classification first applies tooth instance segmentation, then extracts features from each tooth and predicts a tooth-wise score. Meanwhile, patient-level classification provides patient-wise prediction with a multi-task strategy, simultaneously learning a classification activation map (CAM) to show the confidence of local lesion areas upon the panoramic X-ray image. Most importantly, a learnable adaptive noisy-OR gate is designed to integrate information from both levels, with the tooth-level scores and patient-level CAM. Note that our classification is only supervised by the clinical golden standard, i.e., probing measurements. We provide comprehensive learning and integration on both tooth-level and patient-level classification, eventually contributing to confident and stable diagnosis. Our proposed HC-Net is validated on the dataset from real-world clinics. Experiments have demonstrated the outstanding performance of our hybrid structure for periodontal disease diagnosis compared to state-of-the-art methods."
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,2,Method,"An overview of our proposed framework, HC-Net, is shown in Fig. "
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,2.1,Task Formulation and Method Overview,"In this paper, we aim to classify each patient into seriously periodontal (including periodontitis stage II-IV) or not (including health, gingivitis, and periodontitis stage I), abbreviated below as 'positive' or 'negative'. We collect a set of panoramic X-ray images X where Y i ∈ {0, 1} indicates whether the i-th patient is negative (0) or positive  Briefly, our goal is to build a learning-based framework to predict the probability P i ∈ [0, 1] of the i-th patient from panoramic X-ray image. An intuitive solution is to directly perform patient-level classification upon panoramic X-ray images. However, it fails to achieve stable and satisfying results (See Sect. 3.2), mainly for the following two reasons. Firstly, evidence is subtle to be recognized in the large-scale panoramic X-ray image (notice that clinical diagnosis relies on tedious probing around each tooth). Secondly, as we supervise the classification with clinical golden standard (i.e., probing measurements), a mapping should be well designed and established from radiography to this standard, since the extracted discriminative features based on radiography may not be well consistent with the golden standard. Therefore, as shown in Fig. "
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,2.2,Tooth-Level Classification,"Given the panoramic X-ray image of the i-th patient, we propose a two-stage structure for tooth-level classification, which first captures each tooth with tooth instance segmentation, and then predicts the classification of each tooth. Tooth instance segmentation aims to efficiently detect each tooth with its centroid, bounding box, and mask, which are later used to enhance tooth-level learning. It introduces a detection network with Hourglass  where we denote p = {p c + e i } 8 i=1 as the set of 8-adjacent, where p c is the center point and {e i } 8 i=1 is the set of direction vectors. The second branch then uses the center points and image features generated by the backbone to regress the bounding box offsets. The third branch utilizes each bounding box to crop the original panoramic X-ray image and segment each tooth. Eventually, with the image patch and corresponding mask A j i for the j-th tooth of the i-th patient, we employ a classification network (i.e., feature extractor and MLP) to predict the probability T j i , if the tooth being positive. To train the tooth-level framework, we design a multi-term objective function to supervise the learning process. Specifically, for tooth center regression, we employ the focal loss of "
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,2.3,Patient-Level Classification,"As described in Sect. 2.1, although patient-level diagnosis is our final goal, direct classification is not a satisfying solution, and thus we propose a hybrid classification network on both tooth-level and patient-level. Additionally, to enhance patient-level classification, we introduce a multi-task strategy that simultaneously predicts the patient-level classification and a classification activation map (CAM). The patient-level framework first utilizes a backbone network to extract image features for its following two branches. One branch directly determines whether the patient is positive or negative through an MLP, which makes the extracted image features more discriminative. We mainly rely on the other branch, which transforms the image features into CAM to provide local confidence upon the panoramic X-ray image. Specifically, for the i-th patient, with the predicted area {A j i } Ki j=1 of each tooth and the CAM M i , the intensity I of the j-th tooth can be obtained, described as where C(•, * ) denotes the operation that crops • with the area of * . To supervise the CAM, we generate a distance map upon the panoramic X-ray image, based on Euclidean Distance Transform with areas of positive tooth masks. In this way, we train patient-level classification in a multi-task scheme, jointly with direct classification and CAM regression, which increases the focus on possible local areas of lesions and contributes to accurate classification. We introduce two terms to train the patient-level framework, including a cross-entropy loss L clsp to supervise the classification, and a mean squared loss L CAM to supervise the regression for CAM. Eventually, the total loss of the patient-level classification is"
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,2.4,Learnable Adaptive Noisy-OR Gate,"We finally present a learnable adaptive noisy-OR gate  where Φ denotes the pooling operation. In this way, as shown in Fig.  where Ỹi is the final prediction of the i-th patient, G i is the subset of tooth numbers. We employ the binary cross entropy loss L gate to supervise the learning of adaptive noisy-OR Gate. Eventually, the total loss L of our complete hybrid classification framework is formulated as 3 Experiments"
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,3.1,Dataset and Evaluation Metrics,"To evaluate our framework, we collect 426 panoramic X-ray images of different patients from real-world clinics, with the same size of 2903 × 1536. Each patient has corresponding clinical records of golden standard, measured and diagnosed by experienced experts. We randomly split these 426 scans into three sets, including 300 for training, 45 for validation, and 81 for testing. To quantitatively evaluate the classification performance of our method, we report the following metrics, including accuracy, F1 score, and AUROC. Accuracy directly reflects the performance of classification. F1 score further supports the accuracy with the harmonic mean of precision and recall. AUROC additionally summarizes the performance over all possible classification thresholds."
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,3.2,Comparison with Other Methods,"We mainly compare our proposed HC-Net with several state-of-the-art classification networks, which can be adapted for periodontal disease diagnosis. ResNet "
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,3.3,Ablation Studies,"We conduct ablative experiments to validate the effectiveness of each module in HC-Net, including patient-level multi-task strategy with classification activation map (CAM) and hybrid classification with adaptive noisy-OR gate. We first define the baseline network, called B-Net, with only patient-level classification. Then, we enhance B-Net with the multi-task strategy, denoted as M-Net, which involves CAM for joint learning on the patient level. Eventually, we extend B-Net to our full framework HC-Net, introducing the tooth-level classification and the adaptive noisy-OR gate. Effectiveness of Multi-task Strategy with CAM. We mainly compare M-Net to B-Net to validate the multi-task strategy with CAM. We show the classification activation area of both methods as the qualitative results in Fig. "
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,3.4,Implementation Details,Our framework is implemented based on the PyTorch platform and is trained with a total of 200 epochs on the NVIDIA A100 GPU with 80GB memory. The feature extractors are based on DenseNet 
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,4,Conclusion,"We propose a hybrid classification network, HC-Net, for automatic periodontal disease diagnosis from panoramic X-ray images. In tooth-level, we introduce instance segmentation to help extract features for tooth-level classification. In patient-level, we adopt the multi-task strategy that jointly learns the patientlevel classification and CAM. Eventually, a novel learnable adaptable noisy-OR gate integrates both levels to return the final diagnosis. Notice that we significantly utilize the clinical golden standard instead of unconvincing radiographic annotations. Extensive experiments have demonstrated the effectiveness of our proposed HC-Net, indicating the potential to be applied in real-world clinics."
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,,Fig. 1 .,
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,,Fig. 2 .,
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,,Fig. 3 .,
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,,Table 1 .,
HC-Net: Hybrid Classification Network for Automatic Periodontal Disease Diagnosis,,Table 2 .,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,1,Introduction,"Anatomical landmark detection has been used successfully in parametric modeling  Generally, deep learning-based anatomical landmark detection is based on heatmap regression approaches  In this study, we propose a multiresolution heatmap learning strategy that derives the predicted landmark coordinate from multiresolution heatmaps to balance the bias and variance of the predicted landmarks. Moreover, leveraging multiresolution feature representations to generate the heatmap can effectively increase the localization accuracy of the deep learning network. Typically, the existing methods of anatomical landmark detection are formulated using CNN-based or transformer-based encoder-decoder architecture  The main contributions of this paper are as follows: -Introduction of a multiresolution heatmaps learning strategy, which increases the detection ability of the network by leveraging multiresolution information to derive the predicted landmark. -Development of a hierarchical hybrid transformer and CNN architecture named the HTC, which sequentially combines transformer and convolutional modules. -Our proposed HTC model trained with a multiresolution heatmap learning approach clearly outperforms previous state-of-the-art models on three datasets: XCAT 2D projections of head CBCT volumes, X-ray dataset from ISBI2023 Challenge "
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,2,Methods,"In this section, we introduce our anatomical landmark detector, which consists of the proposed multiresolution heatmap learning method and the HTC backbone network, as shown in Fig. "
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,2.1,Multiresolution Heatmap Learning,"As depicted in Fig.   where Conv 1 and Conv 3 are 1ˆ1 and 3ˆ3 convolutional operations, respectively. In addition, K is the number of landmarks. During training, we use mean squared error (MSE) as the loss function between the predicted and ground-truth heatmaps for each resolution. Moreover, we enforce the detector to learn global and local information from the heatmap generated by the high-resolution coarse feature and the low-resolution fine-grained features, through the weighted summation of the heatmap loss from each resolution heatmap as follows: where L H1 and L H2 are the respective losses calculated from the predicted heatmap H 1 and ground truth heatmap of size H 4 ˆW 4 ˆK as well as the predicted heatmap H 2 and ground truth heatmap of size H 2 ˆW 2 ˆK. Additionally, λ is the loss weight, which is set as three in all the experiments. During inference, we calculate the output landmark prediction coordinates of the model by averaging the corresponding coordinates decoded from the heatmap at each resolution."
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,2.2,Hybrid Transformer-CNN (HTC),"We introduce a novel hierarchical encoder architecture named HTC, which consists of four stages of a stack of the convolutional ConvU i and transformer mod-ules T ransU i to generate multi-level feature representation. The overall architecture design of the HTC is shown in Fig.  In the first stage, an input image of size H ˆW ˆ3 is fed to the patch embedding, which is a convolutional 3 ˆ3 operation to obtain a patch token of size H 2 ˆW 2 ˆC1 . Then, the patch token is passed through the bilinear pooling attention module, which requires three inputs: a query Q, a key K, and a value V , which are the patch token that passes through the 1 ˆ1 convolutional operations, separately. Then, Q, K, and V are flattened to size HW 2 2 ˆC1 . The key and query are then fed to bilinear pooling  where N is the set of spatial locations (combinations of rows and columns). We further applied the softmax function to the output of bilinear pooling F to generate the attention weighting vector. Thereafter, matrix multiplication was performed between F and V , and the output of the bilinear pooling attention module was passed through to the feed-forward layer  (3) Similarly, using the feature representations from the previous stages as inputs, we obtained E 2 , E 3 , and E 4 with spatial reduction ratios of 4, 8, and 16 pixels, respectively, with respect to the input image."
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,3,Experiments,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,3.1,Dataset,"To evaluate our method, we conducted experiments on a total of three datasets, including one 4D XCAT phantom CT dataset and two public X-ray datasets. Here, we generated head models from the 4D XCAT dataset  We also evaluated our method on the public X-ray dataset from the IEEE ISBI 2023 Challenge  Finally, we performed experiments on a public hand dataset containing Xray images from 895 patients and having 37 landmarks "
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,3.2,Implementation Details,"In our experiments, we implemented our framework using MMPose "
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,3.3,Performance Evaluation,"In this section, we compare the performances of the proposed and state-of-theart methods as well as analyze ablation studies on the proposed method. In each of the tables below, the metrics showing the best and second-best performances are indicated by boldface and underlined, respectively.  "
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Ablation Study:,"We conducted an additional study to observe the effects of the proposed multiresolution heatmap learning and HTC. For this study, we compared the HTC with Conformer "
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,4,Conclusion,"This study presents a new feature extraction architecture, referred to as the hybrid transformer-CNN (HTC), along with multiresolution heatmap learning for automatic anatomical landmark detection. The HTC architecture comprises of multiple stages of stacked transformer modules, which incorporate a bilinear pooling attention module to capture the global information of images and convolutional modules to extract local and specific feature representations relevant to landmarks. Additionally, we introduced multiresolution heatmap learning to improve the network's ability to capture global and local representations more accurately than learning from a single heatmap resolution, thereby enhancing network localization. Our experimental evaluations on three benchmark datasets demonstrate that the proposed method surpasses state-of-the-art approaches across various modalities and anatomical regions. These findings highlight the potential of our method for automatic anatomical landmark detection in various medical applications."
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Fig. 1 .,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Fig. 2 .,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Fig. 3 .,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Table 1 .,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Table 2 .,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,.56(0.58) 96.84 99.63 100 Comparisons with State-of-the-art Methods:,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Table 3 .,
Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model,,Acknowledgements,. This research was partly supported by the 
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,1,Introduction,"Volumetric medical scans allow for comprehensive diagnosis, but their manual interpretation is time consuming and error prone  Among approaches for handling variable input size, Multiple Instance Learning (MIL) is commonly used. There, a model classifies each slice or subgroup of slices individually, and the final prediction is determined by aggregating subdecisions via maximum or average pooling  Vision Transformers (ViTs)  Despite the merits of the aforementioned approaches, three fundamental challenges still remain. First, the model should be able to process inputs with variable volume resolutions, where throughout the paper we refer to the resolution in the dimension across slices (number of slices), and simultaneously capture the size-independent characteristics of the volume and similarities among the constituent slices. The second challenge is the scalability and the ability of the model to adapt to unseen volume-wise resolutions at inference time. Lastly, the training of deep learning models with high resolution volumes is both computationally expensive and memory-consuming. In this paper, we propose a late fusion Transformer-based end-to-end framework for 3D volume classification whose local-similarity-aware PEs not only improve the model performance, but also make it more robust to interpolation of PEs sequence. We first embed each slice by a spatial feature extractor and then aggregate the corresponding sequence of slice-wise embeddings with a Feature Aggregator Transformer (FAT) module to capture 3D intrinsic characteristics of the volume and produce a volume-level representation. To enable the model to process volumes with variable resolutions, we propose a novel training strategy, Variable Length FAT (VLFAT), that enables FAT module to process volumes with different resolutions both at training and test times. VLFAT can be trained with a proportionally few #slices, an efficient trait in case of training time/memory constraints. Consequently, even with drastic slice subsampling during training, the model will be robust against extreme PEs interpolation for high-resolution volumes at the test time. The proposed approach is modelagnostic and can be deployed with Transformer-based backbones. VLFAT beats the state-of-the-art performance in retinal OCT volume classification on a private dataset with nine disease classes, and achieves competitive performance on a two-class public dataset."
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,2,Methods,Our end-to-end Transformer-based volume classification framework (Fig. 
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,,Slice Feature Extractor (SFE).,"To obtain the slice representations, we use ViT as our SFE due to its recent success in medical interpretation tasks  Volume Feature Aggregator (VFA). The output of the previous step is a sequence of slice-wise embeddings, to which we append a learnable volumelevel classification token  Volume Classification. Finally, the volume-level classification token is fed to a Fully Connected (FC) layer, which produces individual class scores. As a loss function, we employ the weighted cross-entropy."
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,3,Experiments,"We tested our model for volume classification of macula-centered retinal OCT scans, where large variation in volume resolution (#B-scans) between samples is very common. For multiclass classification performance metrics, we relied on Balanced Accuracy (BAcc) and one-vs-all Area Under the Receiver Operating Curve (AUROC). The source code is available at: github.com/marziehoghbaie/VLFAT. Comparison to State-of-the-Art Methods. We compared the performance of the proposed method with two state-of-the-art video ViTs (ViViT)  Ablation Studies. To investigate the contribution of SFE module, we deployed ViT and ResNet18  Robustness Analysis. We investigate the robustness of VFLAT and FAT to PEs sequence interpolation at inference time by changing the volume resolution. To process inputs with volume resolutions different from FAT's and VLFAT's input size, we linearly interpolate the sequence of PEs at the test time. For 9C dataset, we only assess samples with minimum #slices of 128 to better examine the PE's scalability to higher resolutions. Implementation Details. The volume input size was 25 × 224 × 224 for all experiments except for FSA ViViT where #slices was set to 24 based on the corresponding tublet size of 2 × 16 × 16. During VLFAT training, the #slices varied between {5, 10, 15, 20, 25}, specified according to memory constraints. We randomly selected slices using a normal distribution with its mean at the central slice position, thus promoting the inclusion of the region near the fovea, essential for the diagnosis of macular diseases. Our ViT configuration is based on ViT-Base "
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,4,Results and Discussion,"In particular, on large 9C dataset our VLFAT achieved 21.4% and 22.51% BAcc improvement compared to FE ViViT and FSA ViViT, respectively. Incorporating our training strategy, VLFAT, improved FAT's performance by 16.12% on 9C, and 8.79% on OLIVES, which verifies the ability of VLFAT in learning more location-aware PEs, something that is also reflected in the increase of AUROCs (0.96→ 0.98 on 9C dataset and 0.95→ 0.97 on OLIVES). Per-class AUROCs are shown in Table "
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,5,Conclusions,"In this paper, we propose an end-to-end framework for 3D volume classification of variable-length scans, benefiting from ViT to process volume slices and FAT to capture 3D information. Furthermore, we enhance the capacity of PE in FAT to capture sequential dependencies along volumes with variable resolutions. Our proposed approach, VLFAT, is more scalable and robust than vanilla FAT at classifying OCT volumes of different resolutions. On a large-scale retinal OCT datasets, our results indicate that this effective method performs in the majority of cases better than other common methods for volume classification. Besides its applicability for volumetric medical data analysis, our VFLAT has potential to be applied on other medical tasks including video analysis (e.g. ultrasound videos) and high-resolution imaging, as is the case in histopathology. Future work would include adapting VLFAT to ViViT models to make them less computationally expensive. Furthermore, PEs in VLFAT could be leveraged for improving the visual interpretation of decision models by collecting positional information about the adjacent slices sharing anatomical similarities."
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,,Fig. 1 .,
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,,Fig. 2 .,
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,,Fig. 3 .,
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,,Table 1 .,
Transformer-Based End-to-End Classification of Variable-Length Volumetric Data,,Table 2 .,
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,1,Introduction,"Vulvovaginal candidiasis (VVC) is a type of fungal infection caused by candida, which results in discomforting symptoms, including itching and burning in the genital area  Previous studies for computer-aided VVC diagnosis were mainly based on pap smears rather than WSIs. For example, Momenzadeh et al.  Computer-aided diagnosis for candidiasis through WSI is highly challenging (see examples in Fig.  In this paper, we find that the attention for a deep network to focus on candida is the key to the high performance of the screening task. And we propose a series of strategies to make the model focus on candida progressively. Our contributions are summarized into three parts: "
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,2,Method,"We use a hierarchical framework for cervical candida screening, concerning the huge size of WSI and the infeasibility of handling a WSI scan in one shot. The overall pipeline of our framework is presented in Fig. "
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,2.1,Detection Task for Pre-training,"We use a pre-trained detection model as prior to initialize the classification model. In experimental exploration, we find that, if we train the detection network directly, the bounding-box annotation indicates the location of candida and can rapidly establish a rough understanding of the morphology of candida. However, the positioning task coming with detection lacks enough granularity, resulting in relatively low precision to discern cell edges or folding from candida. Meanwhile, directly training a classification model is usually easier to converge. However, in such a task, as candida occupies only a few pixels in an image, it is difficult for the classifier to focus on the target. That, the attention of the classifier may spread across the entire image, leading to overfitted training quickly. Therefore, we argue that the detection and classification tasks are complementary to solve our problem. Particularly, we pre-train a detector and inherit its advantages in the classifier. We use Retinanet  Retinanet. We then initialize the classification network by directly loading the encoder parameters and freezing the first few layers during the training of the classification network. Note that pre-training not only discards the complex positioning task but also makes it easier for the classification network to converge especially in the early stage of training."
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,2.2,Transformer with Skip Self-Attention (SSA),"We design a novel skip self-attention (SSA) module to fuse discriminative features of candida from different scales. At a fine-grained level, the hyphae and spores of candida are usually the basis for judging. Yet we need to distinguish them from easily distorting factors such as contaminants in WSIs and edges of nearby cells. At a coarse-grained level, there is the phenomenon that a candida usually links multiple host cells and yields a string of them. Thus it is necessary to combine long-range visual cues that span several cells to derive the decision related to candida. CNN-based methods have achieved excellent performance in computer-aided diagnosis including cervical cancer "
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,2.3,Contrastive Learning,"As mentioned in Sect. 1, the style gap is another problem, which makes overfitting more severe. In this part, we adopt the strategy of contrastive learning to alleviate such problems and further optimize the attention of the network. Our approach has two key goals: (1) to ensure that the features from the original image remain consistent after undergoing various image augmentations, and (2) to construct an image without the region of candida, resulting in highly dissimilar features compared to the original. Inspired by a weakly supervised learning segmentation method  To achieve this, we use augmentation and the attention map generated during the training process to construct three types of images and apply contrastive learning to the features extracted from them. For a given image I, we use image augmentation to generate I aug and use the encoder attached with SSA to extract feature, F c aug . The attention map A is transformed from F c aug by an attention extractor. The attention extractor uses FC (the same params as that of the classifier) to reduce the channels of features (except the class token) to 2 (candida and others), then reshape the features representing candida to a feature map, and applies bilinear interpolation to upsample it to the same size of I. Equation 1 normalizes A to the interval [0,1], obtaining M to represent the likelihood of candida distribution. We get the masked image I masked by subtracting M from I. where σ and s are used to adjust the range of values, set to 0.5 and 10. As shown in Fig.  In addition, we use two constraints, leading to more stable and robust training. If our network has effective attention, the masked image should not contain any candida, so the score of the Candida category after the mask S(I masked ) should be minimized. We use the attention mining loss to handle this, as shown in the second part of Eq. 2. Additionally, we need the attention to cover only the partial area around candida, without false positive regions. Otherwise, attention maps that cover the whole image can also result in low L tri . We take the average grayscale of attention map M as a restriction, as shown in the last part of Eq. 2. L tri ,L am , and L focus are combined as L cl to constrain each other and take full advantage of contrastive learning, as shown in Eq. 2. Finally, we use the cross-entropy loss L ce to calculate the classification loss with labels. The total loss during training can be expressed as shown in Eq. 3. α is a hyper-parameter, set to 0.1. (3)"
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,2.4,Aggregated Classification for WSI,"With the strategies above, we have built the classifier for all cropped images in a WSI. Then we can finish the pipeline of WSI-level classification, which is shown as part of Fig.  The image-level classification also produces a score, as well as the feature representation of the image under consideration. Then, we reorganize features from all images by ranking their scores and preserving that with top-k scores. We complete the aggregation of the top-k features by the transformer and make the WSI-level decision by an FC layer in the final."
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,3,Experimental Results,"Datasets and Experimental Setup. Our samples were collected by a collaborating clinical institute in 2021. Each sample is scanned into a WSI following standard cytology protocol, which can be further cropped to 500 images sized 1024×1024. For pre-training the detector, we prepare 1467 images with the size of 1024×1024 pixels, all of which have bounding-box annotations. The ratio of training and validation is 4:1. For training of the image-level classification model, we use 1940 positive images (1467 of which are used in detector pre-training) and 2093 negative images. All images used to pre-train the detector are categorized as training data here. The rest 473 images are split in 5-fold cross-validation, from which we collect experimental results and report later. The ratio of training, validation, and testing is 3:1:1. At the WSI level, we use two datasets. Dataset-Small is balanced with 100 positive WSIs and 100 negative WSIs. We conduct a 5-fold cross-validation, and the ratio of training, validation, and testing is 3:1:1. We further validate upon an imbalanced Dataset-Large of 7654 WSIs. There are only 140 positive WSIs in this dataset, which is closer to real world. These two WSI-level datasets have no overlay with the data used to train the above detection and classification tasks. For implementation details, the models are implemented by PyTorch and trained on 4 Nvidia Tesla V100S GPUs. All parameters are optimized by Adam for 100 epochs with the initial learning rate of 3 × 10 -4 . The batch sizes of the detection task, image-level classification, and WSI-level classification are 8, 8, 16, respectively. To aggregate WSI classification, we use top-10 cropped images and their features. We report the performance using five common metrics: area under the receiver operating characteristic curve (AUC), accuracy (ACC), sensitivity (Sen), specificity (Spe), and F1-score. Comparisons for Image-Level Classification. We conduct an ablation study to evaluate the contribution of pre-training (PT), skip self-attention (SSA), and contrastive learning (CL) for the image-level classification, as shown in Table  To verify whether our model focuses on important regions of the input image for accurate classification, we visualize the model's attention using Grad-CAM  Comparisons for WSI-Level Classification. We compare our proposed method to other methods in the whole slide of cervical disease screening. To save computation, we did not verify the performance of the methods that performed too poorly on Dataset-Small. The detection-based method "
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,4,Conclusion,"We introduced a novel attention-guided method for VVC screening, which can progressively correct the attention of the model. We pre-train a detection task for the initialization, then add SSA to fuse features from coarse and fine-grained, and finally narrower attention with contrastive learning. After obtaining accurate attention and good generalization for the image-level classifier, we reorganized and ensemble features from slices, and make a diagnosis. Both numerical metrics and visualization results show the effectiveness of our model. In the future, we would like to explore the method of weakly supervised learning to make use of a huge number of unlabeled images and jointly train the image-level and WSI-level models."
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,,Fig. 1 .,
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,,Fig. 2 .,
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,,Fig. 4 .,
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,,,
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,,Table 1 .,
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,,Table 2 .,
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,1,Introduction,"Due to the superiority in image representation, tremendous success has been achieved in medical image segmentation through recent advancements of deep learning  Consequently, domain adaptation (DA) and DG  To circumvent the above challenges, a frequency-mixed single-source domain generalization strategy, called FreeSDG, is proposed in this paper to learn generalizable segmentation models from a single-source domain. Specifically, the impact of frequency on domain discrepancy is first explored to test our hypotheses on domain augmentation. Then based on the hypotheses, diverse frequency views are extracted from medical images and mixed to augment the single-source domain. Simultaneously, a self-supervised task is posed from frequency views to learn robust context-aware representations. Such that the representations are injected into the vanilla segmentation task to train segmentation networks for out-of-domain inference. Our main contributions are summarised as follows: -We design an efficient SDG algorithm named FreeSDG for medical image segmentation by exploring the impact of frequency on domain discrepancy and mixing frequency views for domain augmentation. -Through identifying the frequency factor for domain discrepancy, a frequencymixed domain augmentation (FMAug) is proposed to extend the margin of the single-source domain. -A self-supervised task is tailored with FMAug to learn robust context-aware representations, which are injected into the segmentation task. -Experiments on various medical image modalities demonstrate the effectiveness of the proposed approach, by which data dependency is alleviated and superior performance is presented when compared with state-of-the-art DG algorithms in medical image segmentation."
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,2,Methodology,"Aiming to robustly counter clinical data from unknown domains, an SDG algorithm for medical image segmentation is proposed, as shown in Fig. "
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,2.1,Frequency-controlled Domain Discrepancy,"Generalizable algorithms have been developed using out-of-domain knowledge to circumvent the clinical performance dropping caused by domain shifts. Nevertheless, extra data dependency is often inevitable in developing the generalizable algorithms, limiting their clinical deployment. To alleviate the data dependency, a single source generalization strategy is designed inspired by the Fourier domain adaption  According to  1) uniformly removing the LFS reduces inter-and inner-domain shifts; 2) discriminatively removing the LFS from a single domain increases innerdomain discrepancy. Various frequency views are thus extracted from medical images with changing parameters to verify the above hypotheses. Denote the frequency filter with parameters θ n as F n (•), where n ∈ R N +1 refers to the index of parameters. Following  As shown in Fig. "
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,2.2,Frequency-mixed Domain Augmentation,"Motivated by the hypotheses, domain augmentation is implemented by F n (•) with perturbed parameters. Moreover, the local-frequency-mix is executed to further extend the domain margin, as shown in Fig.  where M ∈ 0, 1 W ×H is a binary mask controlling where to drop out and fill in from two images, and is element-wise multiplication. k = (i -1) × N + (j -1) denotes the index of the augmentation outcomes, where i, j ∈ R N , i = j. Notably, self-supervision is simultaneously acquired from FMAug, where only patches from N frequency views xn , n ∈ R N are mixed, and the rest one x0 is cast as a specific view to be reconstructed from the mixed ones, where (r n , σ n ) = (27, 9). Under the self-supervision, an objective function for learning contextaware representations from view reconstruction is defined as ( where xk refers to the view reconstructed from xk , K = N × (N -1). Consequently, FMAug not only extends the domain discrepancy and margin, but also poses a self-supervised pretext task to learn generalizable context-aware representations from view reconstruction."
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,2.3,Coupled Segmentation Network,"As the FMAug promises domain-augmented training data and generalizable context-aware representations, a segmentation model capable of out-of-domain inference is waiting to be learned. To inject the context-aware representations into the segmentation model seamlessly, a coupled network is designed with attention mechanisms (shown in the purple block of Fig.  Concretely, the network comprises an encoder E and two decoders D sel , D seg , where skip connection bridges E and D sel while D sel marries D seg using attention mechanisms. For the above pretext task, E and D sel compose a U-Net architecture to reconstruct x0 from xk with the objective function given in Eq. 2. On the other hand, the segmentation task shares E with the pretext task, and introduces representations from D sel to D seg . The features outcomes from the l-th layer of D seg are given by where f l sel refers to the features from the l-th layer of D sel . Additionally, attention modules are implemented to properly couple the features from D sel and D seg . D l seg imports and concatenates f l- Then convolutional layers are used to generate the final outcome f l seg . Accordingly, denote the segmentation result from xk as mk , the objective function for segmentation task is given by where m denotes the ground-truth segmentation mask corresponding to the original source sample x. Therefore, the overall objective function for the network is defined as where α is the hyper-parameter to balance L sel and L seg ."
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,3,Experiments,"Implementation: Five image datasets of three modalities were collected to conduct segmentation experiments on fundus vessels and articular cartilage. For fundus vessels, training was based on 1) DRIVE The image data were resized to 512 × 512, the training batch size was 2, and Adam optimizer was used. The model was trained according to an earlystop mechanism, which means the optimal parameter on the validation set was selected in the total 200 epochs, where the learning rate is 0.001 in the first 80 epochs and decreases linearly to 0 in the last 120 epochs. The encoder and two decoders are constructed based on the U-net architecture with 8 layers. The comparisons were conducted with the same setting and were quantified by DICE and Matthews's correlation coefficient (Mcc)."
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,,Comparison and Ablation Study:,The effectiveness of the proposed algorithm is demonstrated in comparison with state-of-the-art methods and an ablation study. The Fourior-based DG methods FACT  (1) Comparison. Quantified comparison of our algorithm with the competing methods is summarized in Table  Visualized comparison is shown in Fig.  (2) Ablation Study. According to Table 
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,4,Conclusion,"Pixel-accurate annotations have long been a common bottleneck for developing medical image segmentation networks. Segmentation models learned from a single-source dataset always suffer performance dropping on out-of-domain data. Leveraging DG solutions bring extra data dependency, limiting the deployment of segmentation models. In this paper, we proposed a novel SDG strategy called FreeSDG that leverages a frequency-based domain augmentation technique to extend the single-source domain discrepancy and injects robust representations learned from self-supervision into the network to boost segmentation performance. Our experimental results demonstrated that the proposed algorithm outperforms state-of-the-art methods without requiring extra data dependencies, providing a promising solution for developing accurate and generalizable medical image segmentation models. Overall, our approach enables the development of accurate and generalizable segmentation models from a single-source dataset, presenting the potential to be deployed in real-world clinical scenarios."
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,,Fig. 1 .,
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,,Fig. 2 .,
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,,Fig. 3 .,
Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation,,Table 1 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,1,Introduction,"Despite the rapid development of new detection and treatment methods, the prevalence of cardiovascular disease continues to increase  Optical coherence tomography (OCT)  2) It is difficult to distinguish between different lesions, even for senior radiologists. This is because these lesions vary in size and appearance within the same class, and some of them do not have regular form, as shown in Fig.  Overall, the contribution of this work can be summarized as follows: 1) We propose a new IVOCT dataset that is the first multi-class IVOCT dataset with bounding box annotations for macrophages, cavities/dissections, and thrombi. 2) We design a multi-class lesion detection model with a novel self-attention module that exploits the relationship between adjacent frames in IVOCT, resulting in improved performance. 3) We explore different data augmentation strategies for this task. 4) Through extensive experiments, we demonstrate the effectiveness of our proposed model."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,2,Dataset,"We collected and annotated a new IVOCT dataset consisting of 2,988 IVOCT images, including 2,811 macrophages, 812 cavities and dissections, and 1,111 thrombi. The collected data from 69 patients are divided into training/validation/test sets in a 55:7:7 ratio, respectively. Each split contains 2359/290/339 IVOCT frames. In this section, we will describe the data collection and annotation process in detail."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,2.1,Data Collection,"We collaborated with the Cardiovascular and Cerebrovascular Research Center of Sichuan Provincial People's Hospital, which provided us with IVOCT data collected between 2019 and 2022. The data include OCT examinations of primary patients and post-coronary stenting scenarios. Since DICOM is the most widely-used data format in medical image analysis, the collecting procedure was exported to DICOM, and the patient's name and other private information contained in DICOM were desensitized at the same time. Finally, the 69 DICOM format data were converted into PNG images with a size of 575 × 575 pixels. It is worth noting that the conversion from DICOM to PNG did not involve any downsampling operations to preserve as much information as possible."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,2.2,Data Annotation,"In order to label the lesions as accurately as possible, we designed a two-step annotation procedure. The first round was annotated by two expert physicians using the one-stop medical image labeling software Pair. Annotations of the two physicians may be different. Therefore, we asked them to discuss and reach agreement on each annotation. Next, the annotated data was sent to senior doctors to review. The review starts with one physician handling the labeling, including labeling error correction, labeling range modification, and adding missing labels. After that, another physician would continue to check and review the previous round's results to complete the final labeling. Through the above two steps, 2,988 IVOCT images with 4,734 valid annotations are collected."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,3,Methodology,"Recently, object detection models based on Vision Transformers have achieved state-of-the-art (SOTA) results on various object detection datasets, such as the MS-COCO dataset. Among them, the Swin Transformer "
Vision Transformer Based Multi-class Lesion Detection in IVOCT,3.1,G-Swin Transformer,"In traditional object detection datasets such as the MS-COCO dataset, the images are typically isolated from each other without any correlation. However, in our proposed IVOCT dataset, each IVOCT scan contains around 370 frames with a strong inter-frame correlation. Specifically, for example, if a macrophage lesion is detected at the [x, y, w, h] position in frame F i of a certain IVOCT scan, it is highly likely that there is also a macrophage lesion near the [x, y, w, h] position in frame F i-1 or F i+1 , due to the imaging and pathogenesis principles of IVOCT and ACS. Doctors also rely on the adjacent frames for diagnosis rather than a single frame when interpreting IVOCT scans. But, the design of the Swin-Transformer did not consider the utilization of inter-frame information. Though global modeling is enabled by using the sliding window mechanism. In the temporal dimension, it still has a locality because the model did not see adjacent frames. Based  feature pyramid network (FPN) for fusion of features at different resolutions. The RPN Head is then applied to obtain candidate boxes, and finally, the ROI Head is used for classification and refinement of candidate boxes to obtain class and bbox (bounding box) predictions. The inter-frame feature fusing is happend in the attention block, introduced in the next subsection."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,3.2,Grid Attention,"To better utilize information from previous and future frames and perform feature fusion, we propose a self-attention calculation mode called ""Grid Attention"". The structure shown in Fig.  Based on the feature map of the key frame (orange color), the feature maps of the previous (blue) and next (green) frames first do a dimensional reduction from [H, W, C] to [H, W, C/2]. Then they are down-sampled and a grid-like feature map are reserved. The grid-like feature map are then added to key-frame feature map, and the fusion progress finishes. In the W-MSA module, the self-attention within the local window and that between adjacent local windows are calculated, and the inter-frame information is fully used. The local window of key-frame has contained information from other frames, and self-attention calculation happens in inter-frames. The frame-level feature modeling can thus be achieved, simulating the way that doctors view IVOCT by combining information from previous and next frames. During feature fusion with Grid Attention, the feature maps from different frames are fused together in a grid-like pattern (as shown in the figure). The purpose of this is to ensure that when dividing windows, half of the grid cells within a window come from the current frame, and the other half come from other frames. If the number of channels in the feature map is C, and the number of frames being fused is 3 (current frame + previous frame + next frame), then the first C/2 channels will be fused between the current frame and the previous frame, and the last C/2 channels will be fused between the current frame and the next frame. Therefore, the final feature map consists of 1/4 of the previous frame, 1/2 of the current frame, and 1/4 of the next frame. The impact of the current frame on the new feature map remains the largest, as the current frame is the most critical frame."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,4,Experiments,"Baseline Methods and Evaluation Metrics. The baseline is based on a PyTorch implementation of the open-source object detection toolbox MMDetection. We compare our proposed approach with Swin Transformer and four CNNbased network models including Faster-RCNN  To ensure objective comparison, all experiments were conducted in the MMdetection framework. The metric we used is the AP /AR for each lesion and the mAP , based on the COCO metric and the COCO API (the default evaluation method in the MMdetection framework). We trained the model for 60 epochs with an AdamW optimizer following Swin Transformer. The learning  rate and weight decay is set to be 1e-4 and 1e-2, respectively. The batch size is set to be 2. Quantitative and Qualitative Results. All experiments are conducted on our newly-collected dataset. Each model is trained on the training set, selected based on the performance of the validation set, and the reported results are obtained on the test set. Table  Effect of Different Hyper-parameters. Table  Effect of Fusion Methods. In addition to Grid Attention, there are other methods of feature fusion. The first method is like 2.5D convolution, in which multiple frames of images are mapped into 96-dimensional feature maps directly through convolution in the Linear Embedding layer. This method is the simplest, but since the features are fused only once at the initial stage of the network, the use of adjacent frame features is very limited. The second method is to weight and sum the feature maps of different frames before each Attention Block, giving higher weight to the current frame and lower weight to the reference frames. Table "
Vision Transformer Based Multi-class Lesion Detection in IVOCT,5,Conclusion,"In this work, we have presented the first multi-class lesion detection dataset of IVOCT scans. We have also proposed a Vision Transformer-based model, called G-Swin Transformer, which uses adjacent frames as input and leverages the temporary dimensional information inherent in IVOCT data. Our method outperforms traditional detection models in terms of accuracy. Clinical evaluation shows that our model's predictions provide significant value in assisting the diagnosis of acute coronary syndrome (ACS)."
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Fig. 1 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Fig. 2 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Fig. 3 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Fig. 4 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Table 1 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Table 2 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Table 3 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Table 4 .,
Vision Transformer Based Multi-class Lesion Detection in IVOCT,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 32.
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,1,Introduction,"Cervical cancer accounts for 6.6% of the total cancer deaths in females worldwide, making it a global threat to healthcare  Nowadays, thin-prep cytologic test (TCT)  After scanning whole-slide images (WSIs) from TCT samples, automatic TCT screening is highly desired due to the large population versus the limited number of pathologists. As the WSI data per sample has a huge size, the idea of identifying abnormal cells in a hierarchical manner has been proposed and investigated by several studies using deep learning  To alleviate the shortage of sufficient data to supervise classification, one may adopt traditional data augmentation techniques, which yet may bring little improvement due to scarcely expanded data diversity  Aiming at augmenting the performance of cervical abnormality screening, we develop a novel conditional generative adversarial network in this paper, namely CellGAN, to synthesize cytopathological images for various cell types. We leverage FastGAN "
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,2,Method,"The dilemma of medical image synthesis lies in the conflict between the limited availability of medical image data and the high demand for data amount to train reliable generative models. To ensure the synthesized image quality given relatively limited training samples, the proposed CellGAN is built upon FastGAN "
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,2.1,Architecture of the Generator,"The generator of CellGAN has two input vectors. The first input of the class label y, which adopts one-hot encoding, provides class-conditional information to indicate the expected cervical cell type in the synthesized image I syn . The second input of the 128-dimensional latent vector z represents the remaining image information, from which I syn is gradually expanded. We stack six UpBlocks to form the main branch of the generator. To inject cell class label y into each UpBlock, we follow a similar design to StyleGAN  We further introduce the Skip-layer Global Context (SGC) module into the generator (see Fig. "
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,2.2,Discriminator and Adversarial Training,"In an adversarial training setting, the discriminator forces the generator to faithfully match the conditional data distribution of real cervical cytopathological images, thus prompting the generator to produce visually and semantically realistic images. For training stability, the discriminator is trained as a feature encoder with two extra decoders. In particular, five ResNet-like  where T denotes the image processing (i.e., 1 2 downsampling and 1 4 random cropping) on real image I real , f is the processed intermediate feature map from the discriminator Dis, and Dec stands for the reconstruction decoder. This simple self-supervised technique provides a strong regularization in forcing the discriminator to extract a good image representation. To provide more detailed feedback from the discriminator, PatchGAN  For the objective function, we use the hinge version  where λ reg is empirically set to 0.01 in our experiments. 3 Experimental Results"
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,3.1,Dataset and Experimental Setup,"Dataset. In this study, we collect 14,477 images with 256 × 256 pixels from three collaborative clinical centers. All the images are manually inspected to contain different cervical squamous cell types. In total, there are 7,662 NILM, 2,275 ASC-US, 2,480 LSIL, 1,638 ASC-H, and 422 HSIL images. All the 256×256 images with their class labels are selected as the training data. Implementation Details. We use the learning rate of 2.5 × 10 -4 , batch size of 64, and Adam optimizer "
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,3.2,Evaluation of Image Synthesis Quality,"We compare CellGAN with the state-of-the-art generative models for classconditional image synthesis, i.e., BigGAN "
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,3.3,Evaluation of Augmentation Effectiveness,"To validate the data augmentation capacity of the proposed CellGAN, we conduct 5-fold cross-validations on the cell classification performances of two classi- In each fold, one group is selected as the testing data while the other four are used for training. For different data settings, we synthesize 2,000 images for each cell type using the corresponding generative method, and add them to the training data of each fold. We use the learning rate of 1.0 × 10 -4 , batch size of 64, and SGD optimizer  The experimental accuracy, precision, recall, and F1 score are listed in Table "
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,4,Conclusion and Discussion,"In this paper, we propose CellGAN for class-conditional cytopathological image synthesis of different cervical cell types. Built upon FastGAN for training stability and computational efficiency, incorporating class-conditional information of cell types via non-linear mapping can better represent distinguishable cellular features. The proposed SGC module provides the global contexts of cell spatial relationships by capturing long-range dependencies. We have also found that the PatchGAN-based discriminator can prevent potential color distortion. Qualitative and quantitative experiments validate the semantic realism as well as the data augmentation effectiveness of the synthesized images from CellGAN. Meanwhile, our current CellGAN still has several limitations. First, we cannot explicitly control the detailed attributes of the synthesized cell type, e.g., nucleus size, and nucleus-cytoplasm ratio. Second, in this paper, the synthesized image size is limited to 256×256. It is worth conducting more studies for expanding synthesized image size to contain much more cells, such that the potential applications can be extended to other clinical scenes (e.g., interactively training pathologists) in the future."
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,,Fig. 1 .,
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,,Fig. 2 .,
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,,Fig. 3 .,
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,,Table 1 .,
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,,Table 3 .,
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_47.
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,1,Introduction,"Thyroid cancer is the most common cancer of the endocrine system, accounting for 2.1% of all malignant cancers  However, nuclei segmentation in thyroid cytopathology is still challenged by the varying cellularity of images from different TBSRTC categories  To narrow the gap discussed, we propose a novel TBSRTC-category-aware nuclei segmentation framework. Our contributions are three-fold. "
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,2,Methodology,"Overview. We propose a novel TBSRTC-category Aware Segmentation Network (TCSegNet) to segment nuclei boundaries in cytopathology images, which is guided by TBSRTC-category label to learn from unbalanced data. Our model uses a CNN and Transformer dual-path U-shape architecture, where the CNN captures the local features, and the Transformer extracts the global features for a more comprehensive representation of nuclei allocation  where ŷ is the prediction from TCSegNet and y is and pixel-wise annotation, respectively. Subscript ni and nb denote the nuclei area and boundary, respectively. Superscripts cnn and trans write for the CNN branch and Transformer branch, respectively. We set the balancing coefficient γ ni to 1 and γ nb to 10. Additionally, to ensure the consistency between the two branches, we impose a dice consistency loss (L cons ) between the nuclei instance predictions from the CNN branch and the Transformer branch, namely TBSRTC-Category Label Guidance Block. In TCSegNet, we introduce a TBSRTC-category label guidance block to address the learning issue from unbalanced routine datasets. This block consists of two learnable fully connected layers that process the feature extracted by the CNN and Transformer branches separately, which obtains image-wise TBSRTC-category prediction denoted as ŷcnn cls and ŷtrans cls . Correspondingly, to train this block, we use a cross-entropy loss function (CE) that provides an extra supervision signal to help the network learn from unbalanced datasets, defined as follows: where y cls is the image-wise TBSRTC-category label, and the balancing coefficient γ cls is set to 3, as the global feature captured by the Transformer branch is tightly correlated with the image-level classification tag. Finally, the overall loss for TCSegNet becomes Extension to Semi-supervised Learning. To leverage images that only have image-wise labels, we extend to a semi-supervised mean teacher  where e max is the maximum epoch number. HSV-Intensity Noise. The traditional method of integrating Gaussian noise in the mean teacher  Specifically, X v is the pixel value of the image's V channel in HSV space, and hyper-parameter λ v is set as 0.5 to control the amplitude of the intensity-based noise. Finally, the value of the obtained noise is clamped to [-0.2, 0.2] before being added to the images."
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,3,Experiments,Image Dataset. We construct a clinical thyroid cytopathology dataset with images of both image-wise and pixel-wise labels as a benchmark (appear in GitHub upon acceptance) Some representative images are presented in Fig. 
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,,Method Dice IoU,"Fully-supervised Mask R-CNN  Semi-supervised PseudoSeg  Compared Methods and Evaluation Metrics. We compared TCSegNet with the fully-supervised counterparts, including method specific for segmentation in general image "
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,4,Conclusion,"In this paper, we propose a TBSRTC-category aware nuclei segmentation framework TCSegNet, that leverages easy-to-obtain image-wise diagnostic category to facilitate nuclei segmentation. Importantly, it addresses the challenge of distinguishing nuclei across different cell scales in an unbalanced dataset. We also extend the framework to a semi-supervised learning fashion to overcome the issue of lacking annotated training samples. Moreover, we construct the first thyroid cytopathology dataset with both image-wise and pixel-wise labels, which we believe can it facilitate future research in this field. As the spatial distribution, shape, and area information from nuclear segmentation is supportive of diagnostic decisions, we will further leverage the segmentation result for malignancy analysis and also explore the potential of spatial information for unlabeled data exploration in the future."
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,,Fig. 1 .,
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,,),
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,,Fig. 2 .,
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,,Fig. 3 .,
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,,Fig. 4 .,
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,,Table 2 .,
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,1,Introduction,"In the field of medical image processing, nuclei detection on Hematoxylin and Eosin (H&E)-stained images plays a crucial role in various areas of biomedical research and clinical applications  Yet, newly developed large-scale visual-language pre-trained models (VLPMs) have provided another possible unsupervised learning paradigm  Among VLPMs, Grounded Language-Image Pre-training (GLIP) model  Building upon this concept, our goal is to establish a zero-shot nuclei detection framework based on VLPM. However, directly applying VLPM for this task poses two challenges.  To address the first challenge, Yamada et al. have analyzed and revealed that under the pre-training of vast text-image pairs, VLPM establishes a strong association binding between the object and its semantic attributes regardless of image domain, i.e., associated attribute text can fully describe the corresponding objects in an image through VLPM  Through GLIP's strong object retrieval performance, we can obtain preliminary boxes. The precision of these preliminary boxes is relatively high, but there is still considerable room for improvement in recall. Therefore, we further establish a self-training framework. We use the preliminary boxes generated by GLIP as pseudo labels for further training YOLOX  The contributions of this paper are threefold. (1) A novel zero-shot labelfree nuclei detection framework is proposed based on VLPMs. Our method outperforms all existing unsupervised methods and demonstrates excellent transferability. (2) We leverage GLIP, which places more emphasis on object-level representation learning and generates more high-quality language-aware visual representations compared to Contrastive Language-Image Pre-training (CLIP) model, to achieve better nuclei retrieval. (3) An automatic prompt design process is established based on the association binding trait of VLPM to avoid non-trivial empirical manual prompt engineering."
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,2,Method,Our approach aims to establish a zero-shot nuclei detection framework based on VLPMs by directly using text prompts. We utilize GLIP for better object-level representation extraction. The overview of our framework is shown in Fig. 
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,2.1,Object-Level VLPM -GLIP,"Recently, large VLPMs such as CLIP  A typical VLPM framework comprises two encoders: the text encoder, which encodes text prompts to semantic-rich text embeddings, and the image encoder, which encodes images to visual embeddings. These VLPMs use a vast amount of web-originated text-image pairs {(X, T )} i to learn the text-to-image alignment through contrastive loss over text and visual embeddings. Denoting the image encoder as E I , the text encoder as E T , the cosine similarity function as cos(•), and assuming that K text-image pairs are used in each training epoch, the objective of the contrastive learning can be formulated as: Through this aligning contrastive learning, VLPM aligns text and image in a common feature space, allowing one to directly transfer a trained VLPM to downstream tasks via manipulating text, i.e., prompt engineering. The visual representations of input images are semantic-rich and interpretability-friendly with the help of aligned text-image pairs. However, the conventional VLPM pre-training process only aligns the image with the text from a whole perspective, which results in a lack of emphasis on object-level representations. Therefore, Li et al. proposed GLIP  where L is the total number of layers, R 0 denotes the visual features from swintransformer-large "
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,2.2,Automatic Prompt Design,"The text input, known as the prompt, plays a crucial role in the zero-shot transfer of VLPM for downstream tasks. GLIP originally uses concatenated object nouns such as ""object noun 1. Object noun 2..."" as default text prompts for detection, and also allows for manual engineering to improve performance  As shown in Fig.  Our automatic prompt design leverages the powerful text-to-image aligning capabilities of GLIP and BLIP. This approach also enables the automatic generation of the most appropriate attribute words for the specific domain, embodying excellent interpretability."
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,2.3,Self-training Boosting,"Leveraging the strong object retrieval performance of GLIP, we obtain preliminary detection boxes with high precision but low recall. These boxes suffer from missed detection, false detection, and overlapping. To fully exploit the zeroshot potential of GLIP, a self-training framework is established. The automatic prompts are inputted into GLIP to generate the initial results which served as pseudo labels for training YOLOX "
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,3,Experiments,
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,3.1,Dataset and Implementation,"The dataset used in this study is the MoNuSeg dataset  In terms of experimental settings, four Nvidia RTX 3090 GPUs were utilized, each with 24 GB of memory. In the automatic prompt generating process, the VQA weights of BLIP finetuned on ViT-B and CapFilt-L  Table "
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,3.2,Comparison,"Our proposed method was compared with the representative fully-supervised method YOLOX  Referring to the table, it is evident that our GLIP-based approach outperforms all unsupervised techniques, including domain adaptation-based and clipbased methods. Figure "
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,3.3,Ablation Studies,"Automatic Prompt Design. Firstly, we conducted an ablation study specifically targeting the prompt while ensuring that other conditions remained constant, the results are presented in Table  The first row of the table displays the default noun-concatenation prompt GLIP originally adopted, i.e. ""nuclei. nucleus. cyteblast. karyon"". The second row represents the same set of nouns with some manual property descriptions added, like ""Nuclei. Nucleus. cyteblast. karyon, which are round or oval, and purple or magenta"". It is noteworthy that this manual approach is empirically  subjective and therefore prone to significant biases. The subsequent rows in the table demonstrate the combination of attributes generated by our automatic prompt design method. In these experiments, M was set to 3. It is worth noting that the predictions generated by the prompts shown in the first and second rows also employed the self-training strategy until convergence. However, the results of the first row contain a significant amount of noise, implying that the intrinsic gap between medical nouns and natural nouns impedes the directly zero-shot transfer of GLIP. The second row improves obviously, indicating the effectiveness of attribute description. But manual design is empirical and tedious. As for the automatically generated prompts, it is evident that as the description of attributes becomes comprehensive, from only including shape or color solely to encompassing both, GLIP's performance improves gradually. Furthermore, the second-to-last row indicates that even without nouns, attribute words alone can achieve good results, which also demonstrates the ability of BLIP-generated attribute words to effectively describe the target nuclei. Through the utilization of VLPM's text-to-image alignment capabilities, the proposed automatic prompt design method generates the most suitable attribute words for a given domain automatically, with a high degree of interpretability. Please refer to the supplement for a detailed list of [shape] and [color] attributes. We further looked into the effect of word augmentation. The results are shown in Table "
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,,Self-training and YOLOX.,"The box optimization process of the self-training stage is recorded, and the corresponding results are presented in the supplement. YOLOX and self-training are not the essential reasons for the superior performance of our method. The true key is the utilization of semantic-information-rich VLPMs. To illustrate this point, we employed another commonly used unsupervised detection method, superpixels "
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,4,Conclusion,"In this work, we propose to use the object-level VLPM, GLIP, to realize zeroshot nuclei detection on H&E images. An automatic prompt design pipeline is proposed to avoid empirical manual prompt design. It fully utilizes the text-toimage alignment of BLIP and GLIP, and enables the automatic generation of the most suitable attribute describing words, offering excellent interpretability. Furthermore, we utilize the self-training strategy to polish the predicted boxes in an iterative manner. Our method achieves a remarkable performance for labelfree nuclei detection, surpassing other comparison methods. We demonstrate that VLPMs pre-trained on natural image-text pairs still exhibit astonishing potential for downstream tasks in the medical field."
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,,Fig. 1 .,
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,,Fig. 2 .,
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,,Table 2 .,
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,,Table 3 .,
Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 67.
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,1,Introduction,"Orthodontic treatment aims to correct misaligned teeth and restore normal occlusion. Patients are required to wear dental braces or clear aligners for a duration of one to three years, reported by  Deep learning methods have achieved great success in image-related tasks. In the field of tooth semantic segmentation, there exist plenty of studies targeting on different data modalities, such as dental mesh scanning  When it comes to 3D teeth reconstruction, both template-based and deeplearning-based frameworks offer unique benefits. Wu et al. employed a templatebased approach by adapting their pre-designed teeth template to match teeth contours extracted from a set of images  Predicting the smiling portrait after orthodontic treatment has recently gained much attention. Yang et al. developed three deep neural networks to extract teeth contours from smiling images, arrange 3D teeth models, and generate images of post-treatment teeth arrangement, respectively  In this study, we propose an explainable generative framework, which is semantic-guided and knowledge-based, to predict teeth alignment after orthodontic treatment. Previous works have either required 3D teeth model as additional input and predicted its alignment using neural networks "
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,2,Method,The proposed generative framework (Fig. 
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,2.1,Semantic Segmentation in Frontal Images,"The tooth areas and mouth cavity are our region of interest for semantic segmentation in each frontal image. As a preprocessing step, the rectangle mouth area is firstly extracted from frontal images by the minimal bounding box that encloses the facial key points around the mouth, which are detected by dlib toolbox  The tooth semantic segmentation model (Fig.  To generate a more precise tooth segmentation map, we introduce the regionboundary feature fusion module, which merges the tooth region and boundary information, i.e., the last hidden feature maps of the two decoders. The module is constructed as a stack of convolutional layers, which incorporates an improved atrous spatial pyramid pooling (ASPP) module "
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,2.2,Template-Based 3D Teeth Reconstruction,"To achieve 3D tooth reconstruction from a single frontal image, we deform the parametric templates of the upper and lower tooth rows to match the projected contours with the extracted teeth contours in semantic segmentation. The parametric tooth-row template is a statistical model that characterizes the shape, scale, and pose of each tooth in a tooth row. To describe the shape of each tooth, we construct a group of morphable shape models  The optimization-based 3D teeth reconstruction following  The point correspondences are established by Eq. (  We use L as the objective function to minimize, expressed in Eq. (  The regularization term L prior in Eq. (  During optimization, we first optimize the camera parameters and the relative pose of tooth rows for 10 iterations and optimize all parameters for 20 iterations. Afterward, we use Poisson surface reconstruction "
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,2.3,Orthodontic Treatment Simulation,"We implement a naive teeth alignment algorithm to mimic orthodontic treatment. The symmetrical beta function (Eq. (  We assume that the established dental arch curves are parallel to the occlusal plane. Each reconstructed tooth is translated towards its expected position in the dental arch and rotated to its standard orientation while preserving its shape. The teeth gaps are then reduced along the dental arch curve with collision detection performed. The relative pose between tooth rows is re-calculated to achieve a normal occlusion. Finally, the aligned 3D teeth models are projected with the same camera parameters to generate the semantic image output of the simulation."
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,2.4,Semantic-Guided Image Generation,"The idea behind the semantic-guided image generation model is to decompose an image into an orthogonal representation of its style and structure. By manipulating the structural or style information, we can control the characteristics of the generated image. Improving upon "
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,3,Experiments and Results,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,3.1,Dataset and Implementation Details,"We collected 225 digital dental scans with labelled teeth and their intra-oral photos, as well as 5610 frontal intra-oral images, of which 3300 were labelled, and 4330 smiling images, of which 2000 were labelled, from our partner hospitals. The digital dental scans were divided into two groups, 130 scans for building morphable shape models and tooth-row templates and the remaining 95 scans for 3D teeth reconstruction evaluation. The labelled 3300 intra-oral images and 2000 smiling images were randomly split into training (90%) and labelled test (10%) datasets. The segmentation accuracy was computed on the labelled test data, and the synthetic image quality was evaluated on the unlabelled test data. All the models were trained and evaluated on an NVIDIA GeForce RTX 3090 GPU. We trained the segmentation models for 100 epochs with a batch size of 4, and trained the image generation models for 300 epochs with a batch size of 8. The input and output size of the image segmentation and generation models are 256 × 256. The training was started from scratch and the learning rate was set to 10 -4 . We saved the models with the minimal loss on the labelled test data. At the inference stage, our method takes approximately 15 s to run a single case on average, with the 3D reconstruction stage accounting for the majority of the execution time, tests performed solely on an Intel 12700H CPU."
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,3.2,Evaluation,"Ablation Study of Tooth Segmentation Model. We conduct an ablation study to explore the improvement of segmentation accuracy brought by the dualbranch network architecture and the region-boundary feature fusion module. Segmentation accuracy is measured by the mean intersection over union (mIoU)  metric for different groups of tooth labels. The results listed in Table  Accuracy of 3D Teeth Reconstruction. We reconstruct the 3D teeth models of the 95 test cases from their intra-oral photos. The restored teeth models are aligned with their ground truth by global similarity registration. We compare the reconstruction error using different metrics, shown in Table  Image Generation Quality. We use Fréchet inception distance (FID) "
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,4,Conclusion,"In conclusion, we develop a semantic-guided generative framework to predict the orthodontic treatment visual outcome. It comprises tooth semantic segmentation, template-based 3D teeth reconstruction, orthodontic treatment simulation, and semantic-guided mouth cavity generation. The results of quantitative tests show that the proposed framework has a potential for orthodontic application."
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Fig. 1 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Fig. 2 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Fig. 3 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Fig. 4 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Fig. 5 .Fig. 6 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Table 1 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Table 2 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Table 3 .,
A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 14.
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,1,Introduction,"The voice is an essential aspect of human communication and plays a critical role in expressing emotions, conveying information, and establishing personal connections. An impaired function of the voice can have significant negative impacts on an individual. Malign changes of human vocal folds are conventionally observed by the use of (high-speed) video endoscopy that measures their 2D deformation in image space. However, it was shown that their dynamics contain a significant vertical deformation. This led to the development of varying methods for the 3D reconstruction of human vocal folds during phonation. In these works, especially active reconstruction systems have been researched that project a pattern onto the surface of vocal folds "
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,2,Related Work,"Our work can properly be assumed to be simultaneously a heatmap regression as well as a semantic segmentation approach. Deep learning based medical semantic segmentation has been extensively studied in recent years with novel architectures, loss-functions, regularizations, data augmentations, holistic training and optimization approaches "
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,3,Method,"Given an isotropic light source and a material having subsurface scattering properties, the energy density of the penetrating light follows an exponential falloff  Feature Extraction. Current heatmap regression approaches use the argmax(.) or topk(.) functions to estimate channel-wise global maxima, i.e. a single point per channel. In case of an 31 by 31 LPU this would necessitate such an approach to estimate 961 channels; easily exceeding maintainable memory usage. Thus our method needs to allow multiple points to be on a single channel. However, this makes taking the argmax(.) or topk(.) functions to extract local maxima infeasible, or outright impossible. Thus, to extract an arbitrary amount of local maxima adhering to a certain quality, we use dilation filtering on a thresholded and Gaussian blurred image. More precisely we calculate where T (x, y) depicts a basic thresholding operation, G a Gaussian kernel, B a general box kernel with a 0 at the center, and ⊕ the dilation operator. Finally we can easily retrieve local maxima by just extracting every non-zero element of I T . We then span a window I ij of size k ∈ N around the non-zero discrete points p i . For improved comprehensibility, we are dropping the subscripts in further explanations, and explain the algorithm for a single point p. Next, we need to estimate a Gaussian function. Note that, a Gaussian function is of the form f (x) = Ae -(x-μ) 2 /2σ 2 , where x = μ is the peak, A the peaks height, and σ defines the functions width. Gaussian fitting approaches can generally be separated into two types: the first ones employ non-linear least squares optimization techniques  Caruanas Algorithm. As stated previously, Caruanas algorithm is based on the observation that by taking the logarithm of the Gaussian function, we generate a polynomial equation (Eq. 1)."
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,ln(f (x)) = ln(,"Note that the last equation is in polynomial form ln(y) = ax 2 + bx + c, with c = ln(A) --μ 2 2σ 2 , b = μ σ 2 and a = -1 2σ 2 . By defining the error function δ = ln(f (x)) -(ax 2 + bx + c) and differentiating the sum of residuals gives a linear system of equations (Eq. 2). ⎡ After solving the linear system, we can finally retrieve μ, σ and A with μ = -b/2c, σ = -1/2c, and A = e a-b 2 /4c ."
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Guos Algorithm.,"Due to the logarithmic nature of Caruanas algorithm, it is very susceptible towards outliers. Guo "
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,= y[ln(y,"The parameters of the Gaussian function μ, σ and A can be calculated similar to Caruanas algorithm. Recall that polynomial regression has an analytical solution, where the vector of estimated polynomial regression coefficients is β = (X T X) -1 X T ŷ. This formulation is easily parallelizable on a GPU and fully differentiable. Hence, we can efficiently compute the Gaussian coefficients necessary for determining the subpixel position of local maxima. Finally, we can calculate the points position p by simple addition using Eq. 5."
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,4,Evaluation,"We implemented our code in Python (3.10.6), using the PyTorch (1.12.1) "
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,HLE++.,"The HLE dataset is a publicly available dataset consisting of 10 labeled in-vivo recordings of human vocal folds during phonation  , where F i is the vocal fold segmentation at frame i and G i the glottal segmentation, respectively. Quantitative and Qualitative Evaluation. Table "
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,5,Conclusion,"We presented a method for the simultaneous segmentation and laser dot localization in structured light high-speed video laryngoscopy. The general idea is that we can dedicate one channel of the output of a general multiclass segmentation model to learn Gaussian heatmaps depicting the locations of multiple unlabeled keypoints. To robustly handle noise, we propose to use Gaussian regression on a per local-maxima basis that estimates sub-pixel accurate keypoints with negligible computational overhead. Our pipeline is very accurate and robust, and can give feedback about the success of a recording within a fraction of a second. Additionally, we extended the publicly available HLE Dataset to include segmentations of the human vocal fold itself. For future work, it would be beneficial to investigate how this method generalizes to arbitrary projection patterns and non-healthy subjects."
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Fig. 1 .,
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Fig. 2 .,
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,,
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Fig. 3 .,
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Fig. 4 .,
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Fig. 5 .,
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Table 1 .,
Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 4.
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,1,Introduction,"Breast cancer (BC) is one of the most common malignant tumors in women worldwide and it causes nearly 0.7 million deaths in 2020  Recently, with the development of Transformer, multi-modal pre-training has achieved great success in the fields of computer vision (CV) and natural language processing (NLP). According to the data format, there are two main multi-modal pre-training approaches, as shown in Fig.  In this paper, we propose a multi-modal pre-training method based on masked autoencoders for BC downstream tasks. Our model consists of three parts, i.e., the modal-fusion encoder, the mixed attention, and the modal-specific decoder. We choose paired H&E and IHC (only HER2) staining images, which are cropped into non-overlapped patches as the input of our model. We randomly mask some patches by a ratio and feed the remaining patches into the modalfusion encoder to get corresponding tokens. Then the mixed attention module is used to take the intra-modal and inter-modal correlation into account. Finally, we use modal-specific decoders to reconstruct the original H&E and IHC staining images respectively. Our contributions are summarized as follows: We propose a Multi-Modal Pre-training via Masked AutoEncoders MMP-MAE for BC diagnosis. To our best knowledge, this is the first pre-training work based on multi-modal pathological data. We evaluate the proposed method on two public datasets as HEROHE Challenge and BCI challenge, which shows that our method achieves state-of-theart performance. i=1 and {yi} λ 2 N i=1 into the modal-fusion encoder to extract the patch tokens {fi} λ 1 N i=1 and {gi} λ 2 N i=1 . Then we use intra-modal attention and inter-modal attention to take patch correlation into account. X and Y are reconstructed by modal-specific decoders respectively."
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,2,Method,
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,2.1,Overview,"The proposed MMP-MAE consists of three modules, i.e., the modal-fusion encoder, the mixed attention, and the modal-specific decoder, as shown in Fig. "
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,2.2,MMP-MAE,"Modal-Fusion Encoder. We use ViT-base  Algorithm 1. Transformer processing flow. F l ← FFN(LN(F l )) + F l 10: end for Output: Class token and patch tokens F l ∈ R (N +1)×D Fig.  In inter-modal attention, we replace MHSA with the multi-head cross-attention (MHCA) module. We use MHCA to leverage diverse complementary information between two modalities. Modal-Specific Decoder. Each modal-specific decoder is a shallow block with two transformer layers. Different from the transformer encoder, the target of the transformer decoder is used to reconstruct the original image. Reconstruction Loss. Given a pair of H&E image X and HER2 image Y , which is cut into 16 × 16 non-overlapping patches {x i } N i=1 and {y i } N i=1 . We mask some of the patches randomly with the ratio λ 1 and λ 2 (λ 1 + λ 2 = 1). The remained patches are fed into the modal-fusion encoder and the output is corresponding patch tokens {f i } λ1N i=1 and {g i } λ2N i=1 . We randomly generate masked patch tokens {e x j } (1-λ1)N j=1 and {e y j } (1-λ2)N j=1 , which are learnable vectors for masked patch prediction. The input of the mixed attention module is the full set of tokens {f i , e and {g i , e y j }"
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,,"i=λ2N,j=(1-λ2)N i=1",", which include both the remaining patch tokens and the masked patch tokens. After the process of the mixed attention module, H&E and HER2 patch tokens are fed into the modal-specific decoders respectively to reconstruct the original H&E image X and HER2 image Y . The reconstruction loss is computed by the mean squared error between the original images X, Y and the generative images X , Y , which is computed as We use an adjustable hyperparameter θ to balance the losses of two modalities. The final loss L is defined as"
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,2.3,Downstream Tasks,"The pre-trained encoder could be used for downstream tasks, as shown in Fig.  In the HER2 status prediction task, we replace the universal extractor ResNet-50  3 Experimental Results"
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,3.1,Datasets,ACROBAT Challenge. The AutomatiC Registration Of Breast cAncer Tissue (ACROBAT) Challenge  BCI Challenge. Breast Cancer Immunohistochemical Image Generation Challenge 
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,3.2,Experimental Setup,"Experiments are implemented in PyTorch  In the HER2 staining image generation task, we use 2 GPUs with a batch size of 4. The learning rate is 2e -4 and the optimizer is Adam. We use the learning rate decay strategy for stable training. Peak Signal to Noise Ratio (PSNR) and Structural Similarity (SSIM) are used as the evaluation indicators for the quality of the HER2 generated images. In the HER2 status prediction task, we use 1 GPU with a batch size of 1 (WSI level). The learning rate is 1e -4 and the Adam optimizer is used. Four standard metrics are used to measure the HER2 status prediction results, including the area under the receiver operator characteristic curve (AUC), Precision, Recall, and F1-score."
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,3.3,Method Comparison,"HER2 Staining Image Generation. Three methods on BCI datasets are compared in our experiments, as shown in Table  HER2 Status Prediction. We compare our method with the top five methods reported in HEROHE challenge review "
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,4,Conclusion,"In this paper, we propose a novel multi-modal pre-training framework, MMP-MAE for BC diagnosis. MMP-MAE use paired H&E and HER2 staining images for pre-training, which could be used for several downstream tasks such as HER2 staining image generation and HER2 status prediction only by H&E modality. Both the experiment results on BCI and HEROHE datasets show our pretrained MMP-MAE demonstrates strong transfer ability. Our future work will expand our work to more modalities."
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,,Fig. 1 .,
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,,Fig. 2 .,
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,,Fig. 3 .,
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,,Fig. 5 .,
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,,Table 1 .,
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,,Table 2 .,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,1,Introduction,"Many clinical imaging workflows require the patient's height and weight to be estimated in the beginning of the workflow. This information is essential for patient's safety and scan optimizations across modalities and workflows. It is used for accurate prediction of the Specific Absorption Rate (SAR) in Magnetic Resonance Imaging (MRI), contrast dose calculations in Computed Tomography (CT), and drug dose computations in Emergency Room (ER) workflows. Contrary to its importance, there are no widely established methods for estimating the patient's height and weight. Measuring these values using an actual scale is not a common clinical practice since: 1) a measurement scale is not available in every scan room, 2) manual measurements add an overhead to the clinical workflow, and 3) manual measurements may not be feasibly for some patients with limited mobility. Alternative methods such as the Lorenz formulae  In this paper we present a deep-learning based method for accurately and robustly estimating the patient's height and weight in challenging and unrestricted clinical environments using depth images from a 3-dimensional (3D) camera. We aim to cover the patient demographics in common diagnostic imaging workflows. Our method is trained and validated on a very large dataset of more than 1850 volunteers and/or patients, captured in more than 7500 clinical scenarios, and consists of nearly 170k depth images. We achieve a PH5 (percentage of the height estimates within 5% error) of 98.4% and a PH15 of 99.9% for height estimation, and a PW10 (percentage of the weight estimates withing 10% error) of 95.6% and a PW20 of 99.8% for weight estimation, making the proposed method state-of-the-art in clinical setting. In addition to the clinical significance, our method has the following primary technical novelties: 1) we formulate the problem as an end-to-end single-value regression problem given only depth images as input (i.e. no error-prone intermediate stages such as volume computations), 2) we present a multi-stage training approach to ensure robustness in training (i.e. no need for hyper-parameter tunings at any stage), and 3) we evaluate our method on a very large dataset of both volunteers and patients, using 23-fold cross validation to ensure field generalization."
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,2,Related Work,"A large number of previous methods have been proposed and independently evaluated for patient height and weight estimation. Since patient height estimation is considered to be an easier problem, the primary focus of the previous work has been on patient weight estimation. Most of the existing work in patient weight estimation are formulae-based approaches where one or more anthropometric measurements are used for estimating the patient's weight. The Mercy method uses the humeral length and the mid-arm circumference (MAC) for estimating the paediatric body weight  Formulae-based approaches are independently evaluated in numerous studies, both for paediatric patients  More recently several methods have been proposed that leverage 3D camera input for estimating the patient's weight. In  Estimation of the patient weight by the clinical staff remains to be the most common approach in the clinical workflow. In  As a clinical acceptance criteria, Wells et al. "
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,3,Approach,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,3.1,"Obtaining ""Normalized"" Depth Images","When training deep neural networks it is more data efficient to eliminate as much of the foreseen variances in the problem as possible in the preprocessing steps. Conceptually this can be thought as reducing the ""dimensionality"" of the problem before the model training even starts. Camera view-point is a good example when model inputs are images. With a known system calibration, a ""virtual camera"" may be placed in a consistent place in the scene (e.g. with respect to the patient table) and the ""re-projected"" depth images from this virtual camera may be used instead of the original depth images. This way the network training does not need to learn an invariance to the camera view-point, since this variance will be eliminated in the preprocessing. This process forms the first step in our depth image normalization. Figure  For table subtraction, top surfaces extracted from 3D models of the corresponding patient tables are used. For a given input image, it is assumed that the corresponding patient table is known since this information is readily available in an integrated system. Even though we leveraged the actual 3D models of the patient tables, since the proposed approach only requires the top surface, this information may also be obtained during calibration by taking a depth image of the empty patient table. Figure "
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,3.2,Learning Accurate Low-Level Features,"Single-value regression problems require more attention during the training since the limited feedback provided through the loss function may result in the collapse of some of the features in the lower levels of the network, especially if a larger model is being trained. In order to ensure that the learned low-level features are high-quality, we start with the training of a standard image-in image-out encoder-decoder network using the landmark localization as an auxiliary task. With this task, the network is expected to both capture local features (for getting precise landmark locations) and holistic features (for getting globally consistent landmark locations) better than the case where the network was to asked only to regress a single-value such as the height or the weight of the patient. Once the encoder-decoder network is trained, we disregard the decoder part and use the encoder part as the pre-trained feature-extractor."
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,3.3,Task-Specific Decoders for Height and Weight Estimation,"The final stage of our approach is the training of two separate single-value regression networks, one for the estimation of the patient's height and the other for the weight. In this stage, we attach untrained decoder heads to the pre-trained encoder bases. During the training we allow all parameters of the model, including the pre-trained encoder, to be fine-tuned. The motivation for a complete fine-tuning comes from two primary reasons: 1) pre-training of the encoder part allows the network to start with already good low-level features, so it is less-likely to turn these good low-level features to degenerate features during the fine-tuning, and 2) by still allowing changes in the low-level features we have the potential to squeeze further performance from the networks."
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,4,Experiments,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,4.1,Dataset,"For training and validation of our method we have collected a very large dataset of 1899 volunteers and/or patients captured in 7620 clinical workflows, corresponding to nearly 170k depth images. Within this large dataset, we did not have the patient table information for 909 patients and the corresponding 909 workflows, so this subset was used only for the training of the height estimation network. Some example depth snapshots of this extensive dataset is provided in Fig.  This dataset is collected from multiple sites in multiple countries, over a span of several years. The target clinical workflows such as a variety of coils, a variety of positioning equipment, unrestricted covers (light and heavy blankets), and occlusions by technicians, are covered in this dataset. Due to volunteer and patient consents, this dataset cannot be made publicly available. We consider the following inclusion criteria for the training and validation: patient weight between 45 kg to 120 kg, patient height between 140 cm to 200 cm, and patient body mass index (BMI) between 18.5 to 34.9. The distribution of the samples in our dataset, together with the above inclusion criteria, is illustrated in Fig. "
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,4.2,Training,"Training of the feature extraction network is performed using a subset of nearly 2000 workflows. For these workflows, we also acquired the corresponding fullbody 3D medical volumes (MRI acquisitions). A set of 10 anatomical landmarks corresponding to major joints (i.e. knees, elbows, shoulders, ankles, and wrists) are manually annotated by a group of experts in these 3D medical volumes. These 3D annotations are transferred to depth image coordinates for training using the known calibration information. We use a modified version of ResNet  Training of the feature extraction network is done using the ADAM optimizer  where A t is the actual value and P t is the predicted value. We train for 250 epochs with a relatively large patience of 50 epochs. For height estimation we omitted the second step of depth normalization through table subtraction since the patient back-surface does not affect the height estimation significantly."
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,4.3,Results,"We evaluate our model using 23-fold cross-validation. Since we have multiple workflows and depth images corresponding to the same volunteer or patient (e.g. same volunteer captured both in a ""knee-scan"" acquisition and a ""hip-scan""  Table  We also investigated the performance of our method for weight estimation for patient BMI groups outside our inclusion criteria. For patients with BMI < 18.5, our method achieved a PW10 of 89.2% and a PW20 of 98.9%. For patients with BMI > 34.9, our method achieved a PW10 of 96.2% and a PW20 of 99.8%. Even though the performance drops a little bit for underweight population, the main reason for keeping these populations outside the inclusion criteria is not the performance, but rather the limited support in the training dataset. Accurate and rapid estimation of the patient's height and weight in clinical imaging workflows is essential for both the patient's safety and the possible patient-specific optimizations of the acquisition. In this paper, we present a deeplearning based method trained on a very large dataset of volunteer and patient depth images captured in unrestricted clinical workflows. Our method achieves a PH5 of 98.4% and a PH15 of 99.9% for height estimation, and a PW10 of 95.6% and a PW20 of 99.8% for weight estimation. These results out-perform all alternative methods to the best of our knowledge, including family estimates and clinical staff estimates. Disclaimer. The concepts and information presented in this paper are based on research results that are not commercially available. Future availability cannot be guaranteed."
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,,Fig. 1 .,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,,Fig. 2 .,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,,Fig. 3 .,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,,Fig. 4 .,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,,Fig. 5 .,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,,Fig. 6 .,
Accurate and Robust Patient Height and Weight Estimation in Clinical Imaging Using a Depth Camera,,Table 1 .,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,1,Introduction,"Accurate quantification of immunohistochemistry (IHC) membrane staining images is a crucial aspect of disease assessment  This study proposes a novel point-based cell membrane segmentation method, which can significantly reduce the cost of pixel-level annotation required in conventional methods, as shown in Fig. "
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,2,Related Works,"Deep learning-based segmentation methods have been widely developed for cell nuclei segmentation from H&E images in recent years, ranging from convolutional neural networks  For the task of analyzing IHC membrane-stained images, due to the challenge of pixel-level annotation, existing methods mostly adopt traditional unsupervised algorithms, such as watershed  To reduce the annotation cost of nuclei segmentation in histopathological images, weakly supervised segmentation training methods have received attention, including: 1) Unsupervised cell nuclei segmentation methods represented by adversarial-based methods  Therefore, this paper proposes a novel point-supervised cell membrane segmentation method, addressing a major challenge in the field. The paper also explores the feasibility of point supervision for the segmentation of two types of objects (cell nuclei and cell membranes) for the first time."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,3,Method,"This study aims to explore how to perform membrane and nucleus segmentation in IHC membrane-stained images using only point-level supervision. Nuclei segmentation is performed for cell localization and counting, while membrane quantification provides clinical evidence for diagnosis. "
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,3.1,Formulation of the Point-Level Supervised Segmentation Problem,"Given an input cell image set {I i } N i=1 , where N is the number of images in this set, I i ∈ R H×W ×3 with H, W representating the height and width of the image, respectively, and 3 being the number of channels of the image. Our goal is to obtain the mask of membranes ( , where F θ is a segmentation network (Seg-Net) and with trainable parameters θ, and σ is the sigmoid activation function. We have point annotations P i ∈ R H×W ×(c+1) for image I i , in which c is the number of semantic categories used to describe the states of membrane staining. In order to train F θ to segment membranes M i and nuclei S i using point annotations P i in image I i , we need to establish the relationship from input to segmentation, and then to point annotation, as shown in Eq. (  where G ω is the transition network (Tran-Net) with parameters ω. G ω transforms M i , S i to semantic points P i ∈ R H×W ×(c+1) , it should be noted that M i and S i respectively provide the semantic and spatial information to G ω for semantic points prediction, so that the segmentation performance is crucial for G ω . So that, by fitting P i to P i ( P i ∼= P i ), the segmentation can be supervised."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,3.2,Network Architecture,We adopt the U-Net 
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,3.3,Decouple the Membranes and Nuclei Segmentation,"Our goal is to use Seg-Net to generate masks for both membranes ( M i ∈ R H×W ×1 ) and nuclei ( S i ∈ R H×W ×1 ),i.e., M i , S i = σ(F θ (I i )). However, the two Seg-Net channels are interdependent, which can result in nuclei and membranes being inseparably segmented. To overcome this issue, we enforce one channel to output the nuclei segmentation using a supervised mask S i ∈ R H×W ×1 . In this study, we create S i by expanding each point annotation to a circle with a radius of 20. Thus, to provide semantic information to Tran-Net for predicting semantic points, the other channel must contain information describing the staining status of the membrane, which in turn decouples membrane segmentation. Because both S i and S i are single-channel, we employ the naive L 1 loss to supervise the segmentation of the nuclei, as shown in Eq. (  (2)"
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,3.4,Constraints for Membranes Segmentation,"As there are no annotations available for pixel-level membrane segmentation, the network could result in unwanted over-segmentation of membranes. This oversegmentation can take two forms: (1) segmentation of stained impurities, which can restrict the network's generalization performance by learning simple color features, and (2) segmentation of nuclei. To address these issues, we propose a normalization loss term, norm i , which is an L 1 normalization and is defined in Eq. (  However, relying solely on the norm i normalization term might lead to a trivial solution, as it only minimizes the average value of the prediction result, potentially resulting in a minimal maximum confidence for the cell membrane segmentation (e.g., 0.03). To prevent this issue, we introduce a hinge-loss-like loss function, presented in Eq. (  (4)"
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,3.5,Point-Level Supervision,"Using G ω to detect the central point of the cells is a typical semantic points detection task. The difference is that the input of G ω is the output of F θ rather than the image. Nevertheless, we can also employ the cross-entropy function for point-level supervision: To enhance the spatial supervision information of the point annotations P i , it is worth noting that we extended each point annotation to a Gaussian circle with a radius of 5 pixels."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,3.6,Total Loss,"By leveraging the advantages of the above items, we can obtain the total loss as follows: where norm i and hinge i are antagonistic, and their values are close to 0.5 in the ideal optimal state, which can be achieved at τ = 1 in hinge i ."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,4,Experiments,"In order to comprehensively verify the proposed method, we conduct extensive experiments on two IHC membrane-stained data sets, namely the PDL1 (Programmed cell depth 1 light 1) and HER2 (human epidermal growth factor receiver 2) datasets. The HER2 experiment is dedicated to validate segmentation performance, while the PDL1 experiment is utilized to verify the effectiveness of converting segmentation results into clinically relevant indicators."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,4.1,Dataset,"We collected 648 HER2 and 1076 PDL1 images at 40x magnification with a resolution of 1024 × 1024 from WSIs. The PDL1 data only has point-level annotations, where pathologists categorize cells as positive or negative based on membrane staining. The HER2 data has both pixel-level and point-level annotations, where pathologists delineate the nuclei and membranes for pixel-level annotations and categorize cells based on membrane staining for point-level annotations. Pixel-level annotations are used for testing and fully supervised experiments only. We split the data into training and test sets in a 1:1 ratio and do not use a validation set since our method is trained without pixel-level annotations."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,4.2,Implementation Details and Evaluation Metric,We totally train the networks 50 epochs and employ Adam optimizer 
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,4.3,Result Analysis,HER2 Results. Table 
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,PDL1 Results,. We chose to compare our method with unsupervised segmentation methods because existing point-supervised segmentation methods are unable to segment cell membranes. We present their results in Table 
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,5,Conclusion,"In this paper, we present a novel method for precise segmentation of cell membranes and nuclei in immunohistochemical (IHC) membrane staining images using only point-level supervision. Our method achieves comparable performance to fully supervised pixellevel annotation methods while significantly reducing annotation costs, only requiring one-tenth of the cost of pixel-level annotation. This approach effectively reduces the expenses involved in developing, deploying, and adapting IHC membrane-stained image analysis algorithms. In the future, we plan to further optimize the segmentation results of cell nuclei to further boost the performance of the proposed method, and extend it to the whole slide images (WSIs)."
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Fig. 1 .,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Fig. 2 .,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Fig. 3 .,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Fig. 4 .,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Table 1 .,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Table 2 .,
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Acknowledgements,. This work is supported by the 
Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_52.
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,1,Introduction,"Hip fractures represent a life-changing event and carry a substantial risk of decreased functional status and death, especially in elderly patients  Recently, Deep Learning (DL) methods for radiography analysis have gained popularity and shown promising results  Since the introduction of vision transformer models  1. We propose a cross-view deformable transformer framework for non-displaced hip fracture classification, in which we take advantage of discriminative features of Frontal-view as a guidance to localize informative representations of Lateral-view for cross-view feature fusion. 2. For each view, we adopt the deformable self-attention module to select pivotal tokens in a data-dependent way. 3. We build a new non-displaced hip fracture X-ray dataset which includes both Frontal and Lateral views for each case to valid the proposed method. Our approach surpasses the state of the art in accuracy by over 1.5%."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,2,Method,
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,2.1,Overview,The detailed architecture of the proposed cross-view deformable attention framework is shown in Fig. 
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,2.2,View-Specific Deformable Transformer Network,"To discover the task-related regions of each view, the view-specific branch is designed as a deformable transformer network consisting of four stages. In each branch, the first two stages explore the local representations of the input images with shift-window attention modules, followed by the last two stages exploit local tokens relation using deformable self-attention modules. Specifically, our framework takes an image of size H×W ×3 as input. After the first two stages, the input image will be embedded into feature maps where the C denotes the channel number. The f layer3 will be passed to a query projection network W q , which is a light network to obtain the query feature maps f layer3;q . Moreover, a uniform grid p original ∈ R H/4×W/4×2 is generated as a position reference of points in f layer3 . The values of p original are linearly spaced and normalized to 2D coordinates range in ) refers to the horizontal and vertical coordinates for reference points respectively. In the meanwhile, reference points offset p offsets ∈ R H/4×W/4×2 are generated from the f layer3;q by a light offset network consisted of two convolutional layers followed normalization layer, which are also normalized into (-4/H, -4/W ), ). Then the deformed features of each point are sampled at the shifted position, which could be denoted as f = S(f, p), where S represents a bilinear interpolation function. Therefore, the deformed multi-head self-attention module with M heads can be described as: where σ(•) denotes the softmax function, and d is the dimension of each head. z (m) is the embedding output from the m-th attention head, and {q (m) , k(m) , v(m) } ∈ R N ×d represents query, deformed key and value embeddings, respectively. Also, W q , W k , W v , W o are the projection networks. Features passed to the 4th stage are conducted a same operation as in 3rd stage with different feature dimensions."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,2.3,Cross-View Deformable Transformer Framework,"The proposed cross-view framework is consisted of two joint view-specific branches, with a pair of X-ray images (Frontal-view and Lateral-view) from the same patient taken as the input of two individual view-specific branches, respectively. These input images will be embedded into primary representations in the first stage of view-specific network, then these primary features will be sent to the second stage to get representations with larger receptive field. To observe correlations between Frontal-view and Lateral-view, we opt for a simple solution to share queries from the Frontal-view to model token relations of Lateral-view in a self-attention manner as the Frontal-view contains dominated diagnosis features  in which (•) fr and (•) la represent the features of Frontal-view and Lateral-view, respectively. While for the Frontal-view branch, the multi-head self-attention can be denoted as: It is worth noting that view-specific reference points offset are derived from corresponding view-specific query feature maps which contain global view-specific position relations. By taking query feature maps from the Frontal-view as an informative clue, it makes sense to search relevant task-related features in the Lateral-view deformed values and keys embedding which are also discriminative features of Lateral-view. In this way, the cross-view transformer framework manages to localize task-related features in both views while exploring the crossview related representations for feature aggregation. Then the final output can be denoted as outputs =MLP(Concat(f fr , f la )), where the f fr and f la represent the output features of the last layer of the Frontal-view and Lateral-view branches, respectively. The Concat is a concatenation operation and MLP is a projection head consisted of two fully connected layers to generate logit predictions."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,2.4,Technical Details,"The view-specific model shares a similar pyramid structure with DTA-T  The first stage consists of one shift-window block whose head number is set as 3, followed the second stage with one shift-window block whose head number is set as 6. We adopt three deformable attention block with 12 heads in the 3rd stage and one deformable attention block with 24 heads in the 4th stage. To optimize the whole framework, we calculate the cross entropy loss between the label and final output of the cross-view deformable transformer for training."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,3,Experiment,
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,3.1,Experiment Setup,"Dataset. The dataset used in this study includes 768 paired hip X-ray images (329 non-displaced fractures, 439 normal hips) from 4 different manufacturers of radiologic data sources: GE Healthcare, Philips Medical Systems, Kodak and Canon. All the hip radiographs are collected and labeled by experts with nondisplaced fractures or normal for classification task."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,,Implementation Details.,"For experiments of our dataset, we manually locate the hip region and crop a 224 × 224 image that is centered on the original hip region whose size is 400 × 600. The learning rate is set as 3e-3 for the endto-end training of the framework with a batch size of 32. We adopt a 10-fold cross-validation and report the average performance of 10 folds. For each fold, we further divide the data (the other 9 folds) into a training set (90%) and a validation set (10%) and take the best model on the validation part for testing. Evaluation Metric. We evaluate our method with Accuracy (Acc), Precision, Recall and F1 score. The Precision and Recall are calculated with one-classversus-all-other-classes and then calculate F1 score F 1 = 2•P recision•Recall P recision+Recall ."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,3.2,Experimental Results,"Comparisons with the State of the Art. The proposed method is also compared with other cross-view fusion methods; results are reported in Table  2) DualNet: an ensemble of two DenseNet-121  As shown in Table  Ablation Study. We also conduct ablation experiments to validate the design of our proposed different components. We compare the following different settings. 1) Frontal: take the Frontal image as input of the view-specific deformable transformer network to generate the prediction. 2) Frontal swin: take the Frontal image as input of the swin-transformer network to generate the prediction. 3) Lateral: take the Lateral image as input of the view-specific deformable transformer network to generate the prediction. 4) Lateral swin: take the Lateral image as input of the swin-transformer network to generate the prediction. 5) Ours w/o q: the proposed framework without cross-view deformable attention. Table  Visualization Results. To verify the effectiveness of the proposed framework, we visualize the interest regions of the model as shown in Fig. "
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,4,Conclusion,"This paper innovatively introduces a cross-view deformable transformer framework for non-displaced hip fracture classification from paired hip X-ray images. We adopt the deformable self-attention module to locate the interested regions of each view, while exploring feature relations among Lateral-view with the guidance of Frontal-view characteristics. In addition, the proposed deformable crossview learning method is general and has great potential to boost the performance of detecting other complicated disease. Our future work will focus on more effective training strategies and extend our framework to other cross-view medical image analysis problems."
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,,Fig. 1 .,
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,,Fig. 2 .,
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,,Fig. 3 .,
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,,Table 1 .,
Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair,,Computational Pathology,
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,1,Introduction,"Automatic segmentation of tumor lesions from pathological images plays an important role in accurate diagnosis and quantitative evaluation of cancers. Recently, deep learning has achieved remarkable performance in pathological image segmentation when trained with a large and well-annotated dataset  Semi-Supervised Learning (SSL) is a potential technique to reduce the annotation cost via learning from a limited number of labeled data along with a large amount of unlabeled data. Existing SSL methods can be roughly divided into two categories: consistency-based  In this work, we propose a novel and efficient method based on Cross Distillation with Multiple Attentions (CDMA) for semi-supervised pathological image segmentation. Firstly, a Multi-attention Tri-branch Network (MTNet) is proposed to efficiently obtain diverse outputs for a given input. Unlike MC-Net+  The contribution of this work is three-fold: 1) A novel framework named CDMA based on MTNet is introduced for semi-supervised pathological image segmentation, which leverages different attention mechanisms for generating diverse and complementary predictions for unlabeled images; 2) A Cross Decoder Knowledge Distillation method is proposed for robust and efficient learning from noisy pseudo labels, which is combined with an average prediction-based uncertainty minimization to improve the model's performance; 3) Experimental results show that the proposed CDMA outperforms eight state-of-the-art SSL methods on the public DigestPath dataset "
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,2,Methods,As illustrated in Fig. 
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,2.1,Multi-attention Tri-Branch Network (MTNet),"Attention is an effective network structure design in fully supervised image segmentation  CA branch uses channel attention blocks to calibrate the features in the first decoder. A channel attention block highlights important channels in a feature map and it is formulated as: where F represents an input feature map. P ool S avg and P ool S max represent average pooling and max-pooling across the spatial dimension, respectively. MLP and σ denote multi-layer perception and the sigmoid activation function respectively. F c is the output feature map calibrated by channel attention. SA branch leverages spatial attention to highlight the most relevant spatial positions and suppress the irrelevant regions in a feature map. An SA block is: where Conv denotes a convolutional layer. P ool C avg and P ool C max are average and max-pooling across the channel dimension, respectively. ⊕ means concatenation. CSA branch calibrates the feature maps using a CSA block for each convolutional block. A CSA block consists of a CA block followed by an SA block, taking advantage of channel and spatial attention simultaneously. Due to the different attention mechanisms, the three decoder branches pay attention to different aspects of feature maps and lead to different outputs. To further improve the diversity of the outputs and alleviate over-fitting, we add a dropout layer and a feature noise layer η "
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,2.2,Cross Decoder Knowledge Distillation (CDKD),"Since the three branches have different decision boundaries, using the predictions from one branch as pseudo labels to supervise the others would avoid each branch over-fitting its bias. However, as the predictions for unlabeled training images are noisy and inaccurate, using hard or sharpened pseudo labels  where z c represents the logit prediction for class c of a pixel, and pc is the soft probability value for class c. Temperature T is a parameter to control the softness of the output probability. Note that T = 1 corresponds to a standard Softmax function, and a larger T value leads to a softer probability distribution with higher entropy. When T < 1, Eq. 3 is a sharpening function. Let PCA , PSA and PCSA represent the soft probability map obtained by T-Softmax for the three branches, respectively. With the other two branches being the teachers, the KD loss for the CSA branch is: where KL() is the Kullback-Leibler divergence function. Note that the gradient of L CSA kd is only back-propagated to the CSA branch, so that the knowledge is distilled from the teachers to the student. Similarly, the KD losses for the CA and SA branches are denoted as L CA kd and L SA kd , respectively. Then, the total distillation loss is defined as:"
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,2.3,Average Prediction-Based Uncertainty Minimization,"Minimizing the uncertainty (e.g., entropy)  where P = (P CSA + P CA + P SA )/3 is the average probability map. C and N are the class number and pixel number respectively. P c i is the average probability for class c at pixel i. Note that when L um for a pixel is close to zero, the average probability for class c of that pixel is close to 0.0 (1.0), which drives all the decoders to predict it as 0.0 (1.0) and encourages inter-decoder consistency. Finally, the overall loss function for our CDMA is: where L sup = (L CSA sup + L CA sup + L SA sup )/3 is the average supervised learning loss for the three branches on the labeled training images, and the supervised loss for each branch calculates the Dice loss and cross entropy loss between the probability prediction (P CSA , P CA and P SA ) and the ground truth label. λ 1 and λ 2 are the weights of L cdkd and L um respectively. Note that L cdkd and L um are applied on both labeled and unlabeled training images. "
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,3,Experiments and Results,"Dataset and Implementation Details. We used the public DigestPath dataset  At inference time for segmenting a WSI, we used a sliding window of size 256×256 with a stride of 192 × 192. The CDMA framework was implemented in PyTorch, and all experiments were performed on one NVIDIA 2080Ti GPU. MTNet was implemented by extending DeepLabv3+  Quantitative evaluation of these methods is shown in Table "
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,4,Conclusion,"We have presented a novel semi-supervised framework based on Cross Distillation of Multiple Attentions (CDMA) for pathological image segmentation. It employs a Multi-attention Tri-branch network to generate diverse predictions based on channel attention, spatial attention, and simultaneous channel and spatial attention, respectively. Different attention-based decoder branches focus on various aspects of feature maps, resulting in disparate outputs, which is beneficial to semi-supervised learning. To eliminate the negative impact of incorrect pseudo labels in training, we employ a Cross Decoder Knowledge Distillation (CDKD) to enforce each branch to learn from soft labels generated by the other two branches. Experimental results on a colonoscopy tissue segmentation dataset demonstrated that our CDMA outperformed eight state-of-the-art SSL methods. In the future, it is of interest to apply our method to multi-class segmentation tasks and pathological images from different organs."
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,,Fig. 1 .,
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,,Fig. 2 .,
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,,Table 1 .,
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,,69.72±22.06 72.24±21.21 57.09±21.23 60.17±21.98,
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,,Table 2 .,
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,1,Introduction,"Automatic identification of lesions from dermoscopic images is of great importance for the diagnosis of skin cancer  One approach to address the above problem is novel class discovery (NCD)  In this paper, we propose a new novel class discovery framework to automatically discover novel disease categories. Specifically, we first use contrastive learning to pretrain the model based on all data from known and unknown categories to learn a robust and general semantic feature representation. Then, we propose an uncertainty-aware multi-view cross-pseudo-supervision strategy to perform clustering. It first uses a self-labeling strategy to generate pseudo-labels for unknown categories, which can be treated homogeneously with ground truth labels. The cross-pseudo-supervision strategy is then used to force the model to maintain consistent prediction outputs for different views of unlabeled images. In addition, we propose to use prediction uncertainty to adaptively adjust the contribution of the pseudo labels to mitigate the effects of noisy pseudo labels. Finally, to encourage local neighborhood alignment and further refine the pseudo labels, we propose a local information aggregation module to aggregate the information of the neighborhood samples to boost the clustering performance. We conducted extensive experiments on the dermoscopy dataset ISIC 2019, and the experimental results show that our method outperforms other state-of-the-art comparison algorithms by a large margin. In addition, we also validated the effectiveness of different components through extensive ablation experiments."
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,2,Methodology,"Given an unlabeled dataset {x u i } N u i=1 with N u images, where x u i is the ith unlabeled image. Our goal is to automatically cluster the unlabeled data into C u clusters. In addition, we also have access to a labeled dataset {x l i , y l i } N l i=1 with N l images, where x l i is the ith labeled image and y l i ∈ Y = 1, . . . , C l is its corresponding label. In the novel class discovery task, the known and unknown classes are disjoint, i.e., C l ∩ C u = ∅. However, the known and unknown classes are similar, and we aim to use the knowledge of the known classes to help the clustering of the unknown classes. The overall framework of our proposed novel class discovery algorithm is shown in Fig.  Contrastive Learning. To achieve a robust feature representation for the NCD task, we first use noise contrastive learning  where z i = E(x i ) is the deep feature representation of the image x i , E is the feature extractor network, and τ is the temperature value. 1 is the indicator function. In addition, to help the feature extractor learn semantically meaningful feature representations, we introduce supervised contrastive learning  where N (i) represents the sample set with the same label as x i in a mini-batch data. |N (i)| represents the number of samples. The overall contrastive loss can be expressed as: , where μ denotes the balance coefficient. B l is the labeled subset of mini-batch data. Uncertainty-Aware Multi-view Cross-Pseudo-Supervision. We now describe how to train uniformly on known and unknown categories using the uncertainty-aware multi-view cross-pseudo-supervision strategy. Specifically, we construct two parallel classification models M 1 and M 2 , both of them composed of a feature extractor and two category classification heads, using different initialization parameters. For an original image x i , we generate two augmented versions of x i , x v1 i and x v2 i . We then feed these two augmented images into M 1 and M 2 to obtain the predictions for x v1 i and x v2 i : The prediction outputs are obtained by concatenating the outputs of the two classification heads and then passing a softmax layer  Next, we need to obtain training targets for all data. For an input image x i , if x i is from the known category, we construct the training target as one hot vector, where the first C l elements are ground truth labels and the last C u elements are 0. If x i is from the unknown category, we set the first C l elements to 0 and use pseudo labels for the remaining C u elements. We follow the self-labeling method in  where Y = y u 1 ; . . . ; y u Bu ∈ R Bu×C u will assign B u unknown category samples to C u category prototypes uniformly, i.e., each category prototype will be selected B u /C u times on average. S is the search space. H is the entropy function used to control the smoothness of Y. δ is the hyperparameter. The solution to this objective can be calculated by the Sinkhorn-Knopp algorithm  To mitigate the effect of noisy pseudo labels, we propose to use prediction uncertainty  where E represents the expected value. If the variance of the model's predictions for different augmented images is large, the pseudo label may be of low quality, and vice versa. Then, based on the prediction variance of the two models, the multi-view cross-pseudo supervision loss can be formulated as: where L ce denotes the cross-entropy loss. y v1 and y v2 are the training targets. Local Information Aggregation. After the cross-pseudo-supervision training described above, we are able to assign the instances to their corresponding clustering centers. However, it ignores the alignment between local neighborhood samples, i.e., the samples are susceptible to interference from some irrelevant semantic factors such as background and color. Here, we propose a local information aggregation to enhance the alignment of local samples. Specifically, as shown in Fig.  during the training process, which contains the features of N m most recent samples and their pseudo labels. For each sample in the current batch, we compute the similarity between its features and the features of each sample in the memory bank: Then based on this feature similarity, we obtain the final pseudo labels as: k , where ρ is the balance coefficient. By aggregating the information of the neighborhood samples, we are able to ensure consistency between local samples, which further improves the clustering performance."
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,3,Experiments,"Dataset. To validate the effectiveness of the proposed algorithm, we conduct experiments on the widely used public dermoscopy challenge dataset ISIC 2019  Following  Comparison with State-of-the-Art Methods. We compare our algorithms with some state-of-the-art NCD methods, including RankStats  Ablation Study of Each Key Component. We performed ablation experiments to verify the effectiveness of each component. As shown in Table  Ablation Study of Contrastive Learning. We further examined the effectiveness of each component in contrastive learning. Recall that the contrastive learning strategy includes supervised contrastive learning for the labeled known category data and unsupervised contrastive learning for all data. As shown in Table  Uncertainty-Aware Multi-view Cross-Pseudo-Supervision. We also examine the effectiveness of uncertainty-aware multi-view cross-pseudosupervision. We compare it with 1) w/o CPS, which does not use cross-pseudosupervision, and 2) CPS, which uses cross-pseudo-supervision but not the uncertainty to control the contribution of the pseudo label. As shown in Table "
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,4,Conclusion,"In this paper, we propose a novel class discovery framework for discovering new dermatological classes. Our approach consists of three key designs. First, contrastive learning is used to learn a robust feature representation. Second, uncertainty-aware multi-view cross-pseudo-supervision strategy is trained uniformly on data from all categories, while prediction uncertainty is used to alleviate the effect of noisy pseudo labels. Finally, the local information aggregation module further refines the pseudo label by aggregating the neighborhood information to improve the clustering performance. Extensive experimental results validate the effectiveness of our approach. Future work will be to apply this framework to other medical image analysis tasks."
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,,Fig. 1 .,
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,,Table 1 .,
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,,Table 2 .,
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,,Table 3 .,
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 3.
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,1,Introduction,"World Bank data from 2020 suggests that while the infant mortality rate in high-income countries is as low as 0.4%, the number is over ten times higher in low-income countries (approximately 4.7%). This stark contrast underlines the necessity for accessible healthcare. The placenta, as a vital organ connecting the fetus to the mother, has discernable features such as meconium staining, infections, and inflammation. These can serve as indicators of adverse pregnancy outcomes, including preterm delivery, growth restriction, respiratory or neurodevelopmental conditions, and even neonatal deaths  In a clinical context, these adverse outcomes are often signaled by morphological changes in the placenta, identifiable through pathological analysis "
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Related Work. Considerable progress has been made in segmenting,"With growing research in vision-and-language and contrastive learning  However, these methods are still not suitable for variable-length reports and are inefficient in low-resource settings. Our Contributions. We propose a novel framework for more accurate and efficient computer-aided placenta analysis. Our framework introduces two key enhancements: Pathology Report Feature Recomposition, a first in the medical VLC domain that captures features from pathology reports of variable lengths, and Distributional Feature Recomposition, which provides a more robust, distribution-aware representation. We demonstrate that our approach improves representational power and surpasses previous methods by a significant performance margin, without additional data. Furthermore, we boost training and testing efficiency by eliminating the large language model (LLM) from the training process and incorporating more efficient encoders. To the best of our knowledge, this is the first study to improve both the efficiency and performance of VLC training techniques for placenta analysis."
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,2,Dataset,We use the exact dataset from Pan et al. 
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,3,Method,"This section aims to provide an introduction to the background, intuition, and specifics of the proposed methods. An overview is given in Fig. "
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,3.1,Problem Formulation,"Our tasks are to train an encoder to produce placenta features and a classifier to classify them. Formally, we aim to learn a function f v using a learned function f u , such that for any pair of input (x i , t i ) and a similarity function sim, we have where sim(u, v) represents the cosine similarity between the two feature vectors u = f u (x), v = f v (t). The objective function for achieving inequality (1) is: where τ is the temperature hyper-parameter and N is the mini-batch size. To train a classifier, we aim to learn a function f c t using the learned function"
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,3.2,Pathology Report Feature Recomposition,"Traditional VLC approaches for medical image and text analysis, such as Con-VIRT  Our approach addresses the limitations of traditional VLC methods in the medical domain by first decomposing the placenta pathology report into set T of arbitrary size, where each t i ∈ T represents a distinct placental feature; the individual items depicted in the pathology report in Fig. "
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,3.3,Distributional Feature Recomposition,"Since our pathology reports are decomposed and encoded as a set of feature vectors, to ensure an accurate representation, it is necessary to consider potential limitations associated with vector operations. In the context of vector summation, we anticipate similar representations when two sets differ only slightly. However, even minor changes in individual features within the set can significantly alter the overall representation. This is evident in the substantial difference between v1 and v2 in Fig.  Implementation-wise, we employ bootstrapping to estimate the distribution of the mean vector. We assume that the vectors adhere to a normal distribution with zero covariance between dimensions. During each training iteration, we randomly generate a new bootstrapped sample set Ṽ from the estimated normal distribution N (μ(V), σ(V)). Note that a slightly different sample set is generated in each training epoch to cover the variations in the feature distribution. We can therefore represent this distribution by the vector ṽ = v∈ Ṽ v, the sum of the sampled vectors, which captures the mean feature distribution in its values and carries the feature variation through epochs. By leveraging a sufficient amount of training data and running multiple epochs, we anticipate achieving a reliable estimation. The distributional feature recomposition not only inherits the scalability and efficiency of the traditional sum of vector approach but also provides a more robust estimate of the distribution of the mean vector, resulting in improved representational power and better generalizability."
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,3.4,Efficient Neural Networks,"Efficient models, which are smaller and faster neural networks, facilitate easy deployment across a variety of devices, making them beneficial for low-resource communities. EfficientNet "
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,4,Experiments,
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,4.1,Implementation,"We implemented the proposed methods and baselines using the Python/PyTorch framework and deployed the system on a computing server. For input images, we used PlacentaNet  The encoder in the last epoch was saved and evaluated on their task-specific performance on the test set, measured by the AUC-ROC scores (area under the ROC curve). To ensure the reliability of the results, each evaluation experiment was repeated five times using different fine-tuning dataset random splits. The same testing procedure was adopted for all our methods. We masked all iPad images using the provided manual segmentation masks. For more information, please refer to the supplementary material."
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,4.2,Results,"We compare our proposed methods (Ours) with three strong baselines: a ResNet-50 classification network, the ConVIRT  Table "
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,4.3,Ablation,"To better understand the improvements, we conduct a component-wise ablation study. We use the ConVIRT method (instead of Pan et al.) as the starting point to keep the loss function the same. We report the mean AUC-ROC across all tasks to minimize the effects of randomness. As shown in Table "
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,5,Conclusions and Future Work,"We presented a novel automatic placenta analysis framework that achieves improved performance and efficiency. Additionally, our framework can accommodate architectures of different sizes, resulting in better-performing models that are faster and smaller, thereby enabling a wider range of applications. The framework demonstrated clear performance advantages over previous work without requiring additional data, while significantly reducing the model size and computational cost. These improvements have the potential to promote the clinical deployment of automated placenta analysis, which is particularly beneficial for resource-constrained communities. Nonetheless, we acknowledge the large variance and performance drop when evaluating the iPad images. Hence, further research is required to enhance the model's robustness, and a larger external validation dataset is essential. Moreover, the performance of the image encoder is heavily reliant on the pre-trained language model, and our framework does not support online training of the language model. We aim to address these limitations in our future work."
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Fig. 1 .,
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Fig. 2 .,
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Table 1 .,
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Table 2 .,
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Table 1 .,
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Table 3 .,
Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 12.
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,1,Introduction,"Now that the hardware performance has achieved a certain level, 3D rendering of biomedical volumes is becoming very popular, above all, with the younger generation of clinicians that uses to use the forefront technologies in their everyday life. 3D representation has proven useful for faster comprehension of traumas in areas of high anatomic complexity, for surgical planning, simulation, and training  The most popular techniques for volume data rendering are maximum-intensity projection (MIP), iso-surface rendering, and direct volume rendering (DVR). These techniques have their pros and cons, but essentially, they suffer from a lack of photo realism. Our novel Advanced Realistic Rendering Technique (AR 2 T) is based on Monte-Carlo path tracing (MCPT). Historically, MCPT is thought of as a technique suited to (iso)surfaces rendering  In this paper, we present a practical framework that includes different visualization techniques, including AR 2 T. Our framework allows the user to interact with the data in high quality for the deterministic algorithms (iso-surface, MIP, DVR), and in low quality for the stochastic AR 2 T. Moreover, the framework supports a mixed modality that works as follows. By default, the data is rendered by DVR. It allows to interact with the data, adjust the transfer function, and apply clip planes. However, a high-quality AR 2 T image can be generated at any moment by the user request without explicitly switching between rendering algorithms. The quality improves progressively, and the process can be stopped as soon as the desired quality is achieved. As an alternative, the improvement stops automatically, when the algorithm converged. The framework permits to compare different rendering techniques directly, i.e., using the same view/light position and transfer functions. It, therefore, promotes further research on the importance of realism in visualising biomedical volumes, providing medical experts with an immediate one-to-one visual comparison between different data representations. Related Work. Various deterministic approaches were applied in an attempt to increase the realism of volume rendering. Above all, the direct volume rendering technique has been enriched with local and global illumination, combined with ambient occlusion and shadowing  One interesting technique for improved DVR, which includes realistic effects and physically based lighting, was proposed by  Recently, the cinematic rendering (CR) 3D technique was introduced "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2,Methods,"The entire framework is written from scratch in C++ using Qt. The whole rendering runs on GPU and is implemented in OpenGL Shading Language (GLSL). In the following subsections, we describe the techniques we used within the framework, giving the details only for AR 2 T for the sake of brevity. The quality of the images produced with the different techniques is difficult to assess with a numerical index; following "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.1,Deterministic Rendering Algorithms,"First of all, we implemented the most popular rendering techniques for biomedical volumetric data: iso-surface, MIP, and DVR. They gave us the basis for the direct comparison of proposed methods. Then, we enriched our DVR model with local and global illumination, applying various approaches "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.2,Advanced Realistic Rendering Technique,"There are two modalities of AR 2 T visualization: pure AR 2 T and mixed DVR-AR 2 T. When pure AR 2 T is active, the interactivity is achieved by the execution of just one iteration of the algorithm. When the interaction is finished (e.g., the mouse button is released), 10 iterations of AR 2 T are executed. To improve the quality, Gaussian blur filter  Our AR 2 T is inspired by MCPT applied to analytically generated surfaces and isotropic volumes  GPU Implementation. To be independent in the choice of hardware to run our framework, we implement AR 2 T in GLSL. Unfortunately, there are two main issues to resolve for Monte-Carlo path tracing in OpenGL: 1. recursion, and 2. random number generation. Recursion. As GLSL memory model does not allow for recursive function calls, which are essential for MCPT, we simulated the recursion by exploiting multiple render targets feature of modern GPUs. This feature allows the rendering pipeline to render images to multiple render target textures at once. Indeed, the information we need after every scatter of a ray is the resulting colour, the position where the scatter occurred, and the direction in which the ray scatters. Therefore, three target textures are necessary for every rendering step. Moreover, two frame buffers are used in a ping pong blending manner (as described in  On any step, three situations are possible: (1) The ray does not hit the volume or the light source. In this case, a zero-length vector is saved to direction render target -it indicates that the ray scattering finishes here, and the resulting colour components are set to zeros (for ulterior speed up, and considering that the usual background for biomedical visualization is black, we do not model the Cornel box outside the volume). ( "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Random Number Generation.,"To provide the uniformly distributed random numbers on the fragment stage of the OpenGL pipeline, we generate a pool of 50 additional two-dimensional textures of viewport size and fill them with uniformly distributed random numbers generated with std::uniform int distribution. In each step, we randomly choose four textures from the pool to provide random numbers: two for advanced Woodcock tracking, one for scattering, and one for sampling direction. Every 100 iterations of the algorithm, we regenerate the pool of random numbers to avoid the quality improvement stuck. On Intel(R) Core(TM) i5-7600K CPU @ 3.80 GHz, the generation of the pool takes ∼ 1600 ms, for viewport size 1727 × 822. It occupies ∼270 Mb of RAM. Advanced Woodcock Tracking. For volume sampling, we implemented the advanced Woodcock tracking technique, described in  Phase Functions. In AR 2 T, we use four well-known phase functions, described in  where α( -→ v ) ∈ [t 0 , t 1 ] is the voxel density (or opacity), m( -→ v ) is the normalized gradient magnitude, and s is the hybrid scattering factor. Image Generation. When the first iteration of AR 2 T is completed, the result contained in the colour texture (see Recursion for rehearse) is blit into the output rendering frame buffer to be immediately displayed. Moreover, it is saved locally to be summed with the results of the next iterations. On the next iterations, the accumulated result is saved into the local buffer, and then the medium (e.g. the sum divided by the iterations number) is blit into the output frame buffer and displayed. Convergence. As a convergence criterion, we use mean square displacement (MSD) between the iterations "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3,Results,"In our experiments, we have used spiral CT (Computed Tomography), MRI (Magnetic Resonance Imaging), and CBCT (Cone-Beam Computed Tomography) publicly available data sets. The CBCT data sets were acquired by SeeFactorCT3 TM (human) and Vimago TM GT30 (vet) Multimodal Medical Imaging Platforms in our layout and are available on https://kaggle.com/ datasets/imaginar2t/cbctdata. To validate the superiority of the AR 2 T over the other methods implemented in our platform, we ask a group of clinicians to participate into the survey. 22 participants (7 orthopedic surgeons, 1 trauma surgeon, 1 neurosurgeon, 7 interventional radiologists, 6 veterinaries) evaluated the data sets on Fig.  According to Table "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,4,Conclusions,"The main result of our research is the novel advanced realistic rendering technique-AR 2 T (see Fig.  Despite our model supports any number of light sources, all images (except Fig.  At the moment of writing this paper, we are evaluating free access to our framework's executable to share our results, facilitate the comparison with other approaches, and stimulate further research on the usefulness of photo-realistic 3D images in medicine."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 1 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 2 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 3 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 4 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 5 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 6 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Table 1 .,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,1,Introduction,"Although cardiovascular events are highly prevalent worldwide, it was estimated 75-80% of cardiovascular events in high-risk patients could be prevented through lifestyle changes and medical/dietary interventions  Carotid intima media thickness (IMT) is an early imaging biomarker measured from two-dimensional ultrasound (2DUS) images. The use of IMT in serial monitoring is limited by the small annual change (∼0.015 mm)  A convolutional neural network (CNN) is typically represented as a blackbox function that maps images to an output. However, a biomarker should be interpretable for it to be trusted by clinicians. One approach to promote the interpretability of the biomarker is to allow the visualization of regions that have a prominent effect on the biomarker. Class activation map (CAM) "
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,2,Materials and Methods,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,2.1,3DUS Imaging and Preprocessing,"In this work, we assessed the sensitivity of the proposed biomarker in evaluating the effect of pomegranate juice and tablets. Pomegranate is anti-oxidative, and previous studies have established that plaque texture features "
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,2.2,Siamese Change Biomarker Generation Network (SCBG-Net),"Network Architecture. Figure  j=3i z fu j , for i = 0, 1, . . . , 15. The slice-wise cosine ""dissimilarity"" for a baseline-follow-up slice pair was defined by d c (•, •) = 1-cos(•, •). To represent disease progression and regression, the sign of the slice-wise vessel wall volume change ΔV ol i from baseline to follow-up was used to determine the sign of the slice-wise score. ΔV ol i of each smoothed slice pair was computed by averaging vessel wall volume change (i.e., area change × 1mm ISD) for a group of three neighbouring slices involved in the smoothing operation. The slice-wise score was obtained by: where sgn represents the signed function. The use of ReLU in FC layers results in non-negative z i , thereby limiting d c ( zi bl , zfu i ) and s(z bl i , zfu i ) to the range of [0, 1] and [-1, 1], respectively. Defined as such, s i integrates vessel-wall-plusplaque volume change with textural features extracted by the network. Finally, the AutoVT biomarker was obtained by averaging 16 slice-wise scores (i.e., AutoV T = 1 16 15 i=0 s i ). Loss Functions. We developed a treatment label contrastive loss (TCL) to promote discrimination between the pomegranate and placebo groups and a plaque-focus (PF) constraint that considers slice-based volume change. (i) Treatment Label Contrastive Loss. The contrastive loss  where y is the group label of the input subject (pomegranate = 1, placebo = 0). For pomegranate subjects, instead of assigning a penalty based on the squared distance as in  (ii) Plaque-focus Constraint. We observe that slice pairs with high volume change are typically associated with a large plaque change (Supplementary Fig.  The overall loss L is a weighted combination of L tcl and L pf (i.e., L = L tcl +wL pf , where w is the weight)."
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,2.3,Change Biomarker Activation Map (CBAM),"Figure  with M p,k L,i = Norm(Up(A p,k L,i )), • representing the Hadamard product and (p, q) ∈ {(bl, f u), (fu, bl)}. Up(•) upsamples A p,k L,i into the size of x p i , and Norm(•) is a min-max normalization function mapping each element in the matrix into [0, 1]. A p,k L,i is first upsampled and normalized to M p,k L,i , which serves as an activation map to highlight regions in the input image. The importance of A p,k L,i to the slicewise score s is quantified by the cosine dissimilarity between the feature vectors generated by SCBG-Net for the highlighted input image and the corresponding image slice in the baseline-follow-up image pair. If the input image is a baseline image, the corresponding slice would be from the follow-up image, and vice versa. For each slice x p i , the activation map from the convolutional layer L was generated as"
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,3,Experiments and Results,"Statistical Evaluation. The discriminative power of biomarkers was evaluated by p-values from two-sample t-tests for normally distributed measurements or Mann-Whitney U tests for non-normally distributed measurements. P-values quantify the ability of each biomarker to discriminate the change exhibited in the pomegranate and placebo groups. Experimental Settings. Our model was developed using Keras on a computer with an Intel Core i7-6850K CPU and an NVIDIA RTX 1080Ti GPU. The ResNet50 was initialized by the ImageNet pretrained weights. The SGD optimizer was applied with an initial learning rate of 3 × 10 -3 . An exponential decay learning rate scheduler was utilized to reduce the learning rate by 0.8 every 10 epochs. We set the number of slices with top/last |ΔV ol i | in the definition of the PF constraint as K l = K s = 3. All models were evaluated by three-fold cross-validation with 80 labeled subjects and 40 test subjects. Labeled subjects are further partitioned into training and validation sets with 60 and 20 subjects, respectively. For the proposed SCBG-Net, the margin m and loss function weight w were tuned using the validation set. In all three trials, the optimized m and w were 0.8 and 0.15, respectively."
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Comparison with Traditional Biomarkers. Table 1 shows p-values for,"AutoVT and traditional biomarkers. The proposed biomarker based on the overall loss function L, AutoVT(L), was the most sensitive to the effect of pomegranate with the lowest p-value. This biomarker has learned the volumetric information from the input images, as demonstrated by the correlation coefficient of 0.84 between AutoVT(L) and ΔV W V . AutoVT(L) has also learned the texture information, as demonstrated in Fig.  Comparison with Different Losses. We compared our proposed loss with another two losses, including cross-entropy loss and bi-direction contrastive loss. Cross-entropy loss is expressed as , where σ(•) is a sigmoid function. The bi-direction contrastive loss is a symmetric version of L tcl , expressed as L bd = y max(m + AutoV T, 0) 2 + (1y) max(m -AutoV T, 0) 2 . The margin m in L bd was tuned in the same way as the proposed L, with m = 0.4 being the optimized parameter in all three cross-validation trials. Table "
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Comparison with Other Activation Maps,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,4,Conclusion,"We, for the first time, developed a deep biomarker to quantify the serial change of carotid atherosclerosis by integrating the vessel wall and plaque volume change and the change of textural features extracted by a CNN. We showed that the proposed biomarker, AutoVT, is more sensitive to treatment effect than vessel wall and plaque volume measurements. SCBG-Net involves slice-based comparison of textural features and vessel wall volume (Eq. 1) and we showed that this architecture results in a biomarker that is more sensitive than Aver-Net and Atten-Net that quantify global change for the left and right arteries. This result is expected as atherosclerosis is a focal disease with plaques predominantly occurring at the bifurcation. For the same reason, PF constraint that involves local slice-based assessment further improves the sensitivity of AutoVT in detecting treatment effects. We developed a technique to generate activation maps highlighting regions with a strong influence on AutoVT. The improvement in the interpretability of AutoVT afforded by the activation maps will help promote clinical acceptance of AutoVT ."
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Fig. 1 .,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Fig. 2 .,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Fig. 3 .,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Fig. 4 .,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Table 1 .,
Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 29.
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,1,Introduction,"Cancers are a group of heterogeneous diseases reflecting deep interactions between pathological and genomics variants in tumor tissue environments  The major goal of multimodal data learning is to extract complementary contextual information across modalities  To tackle above challenges, we propose a pathology-and-genomics multimodal framework (i.e., PathOmics) for survival prediction (Fig. "
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,2,Methodology,"Overview. Figure  For group-wise WSIs representation, we first cropped all tissue-region image tiles from the entire WSI and extracted CNN-based (e.g., ResNet50) d idimensional features for each image tile k as h k ∈ R 1×di , where d i = 1, 024, k ∈ K and K is the number of image patches. We construct the group-wise WSIs representation by randomly splitting image tile features into N groups (i.e., the same number as genomics categories). Therefore, group-wise image representation could be defined as I n ∈ R kn×1024 , where n ∈ N and k n represents tile k in group n. Then we apply an attention-based refiner (ABR)  where w,V1 and V2 are the learnable parameters. Patient-Wise Multimodal Feature Embedding. To aggregate patient-wise multimodal feature embedding from the group-wise representations, as shown in Fig.  In each stream, we use the same architecture with different weights, which is updated separately in each modality stream. In the pathological image stream, the patient-wise image representation is aggregated by N group representations as , where p ∈ P and P is the number of patients. Similarly, the patient-wise genomics representation is aggregated as G p ∈ R N ×256 . After generating patient-wise representation, we utilize two transformer layers  where MSA denotes Multi-head Self-attention  Multimodal Fusion in Pretraining and Finetuning. Due to the domain gap between image and molecular feature heterogeneity, a proper design of multimodal fusion is crucial to advance integrative analysis. In the pretraining stage, we develop an unsupervised data fusion strategy by decreasing the mean square error (MSE) loss to map images and genomics embeddings into the same space. Ideally, the image and genomics embeddings belonging to the same patient should have a higher relevance between each other. MSE measures the average squared difference between multimodal embeddings. In this way, the pretrained model is trained to map the paired image and genomics embeddings to be closer in the latent space, leading to strengthen the interaction between different modalities. In the single modality finetuning, even if we use image-only data, the model is able to produce genomic-related image feature embedding due to the multimodal knowledge aggregation already obtained from the model pretraining. As a result, our cross-modal information aggregation relaxes the modality requirement in the finetuning stage. As shown in Fig. "
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,3,Experiments and Results,"Datasets. All image and genomics data are publicly available. We collected WSIs from The Cancer Genome Atlas Colon Adenocarcinoma (TCGA-COAD) dataset (CC-BY-3.0)  Experimental Settings and Implementations. We implement two types of settings that involve internal and external datasets for model pretraining and finetuning. As shown in Fig  The number of epochs for pretraining and finetuning are 25, the batch size is 1, the optimizer is Adam "
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,,Results.,In Table 
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,,Fig. 1 .,
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,,Fig. 2 .,
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,,Table 1 .,
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 60. Ablation Analysis. We verify the model efficiency by using fewer amounts of finetuning data in finetuning. For TCGA-COAD dataset, we include 50%, 25%, and 10% of the finetuning data. For the TCGA-READ dataset, as the number of uncensored patients is limited, we use 75%, 50%, and 25% of the finetuning data to allow at least one uncensored patient to be included for finetuning. As shown in Fig. "
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,4,Conclusion,"Developing data-efficient multimodal learning is crucial to advance the survival assessment of cancer patients in a variety of clinical data scenarios. We demonstrated that the proposed PathOmics framework is useful for improving the survival prediction of colon and rectum cancer patients. Importantly, our approach opens up perspectives for exploring the key insights of intrinsic genotypephenotype interactions in complex cancer data across modalities. Our finetuning"
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,1,Introduction,"MRI is an essential diagnostic and investigative tool in clinical and research settings. Expert radiologists rely on multiple MRI series of varying acquisition parameters and orientations to capture different aspects of the underlying anatomy and diagnose any defect or pathology that may be present. For a knee study, it is typical to acquire MRI series with coronal, sagittal, and axial orientations using proton density (PD), proton density fat suppressed (PDFS) or T2-weighted fat suppressed series (T2FS) for each study. When series are analyzed in concert, a radiologist can make a more effective diagnosis and mark down the location of any corresponding defect in each series. The defect location is typically represented as a single point  In recent years, convolutional neural networks (CNNs) have achieved promising results in pathology localization. Many approaches rely on generating a multi-variate Gaussian heatmap, where the peak of the distribution represents the pathology localization. Hourglass  To do this, we design a framework that utilizes self-attention across multiple series and we further add a mask to allow the model to focus on relevant areas, which we term as Masked Self-Attention (MSA). To predict the pathology location, we use a transformer decoder with an encoder-based initialization of the reference points. This approach provides a strong initial guess of the pathology location, improving the accuracy of the model's predictions. Overall, our framework leverages the strengths of both self-attention and encoder-decoder architectures to enhance the performance of pathology localization. Specifically, our contributions are: -We introduce a framework that enables the simultaneous use of multiple series from an MRI study, allowing for the sharing of pathology information across different series through Masked Self-Attention. -We design a transformer-based decoder model to predict consistent locations across series in an MRI study, which reduces the network's parameters compared to standard heatmap-based approaches. -Through extensive experiments on three knee pathologies, we demonstrate the effectiveness and efficiency of our framework, showing the benefits of Masked Self-Attention and a Pathology localization decoder to accurately predict pathology locations. Overall, our framework represents a promising step towards more consistent and accurate localization, which could have important applications in medical diagnosis and treatment. Fig. "
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,2,Methods,
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,2.1,Our Architecture,"We aim to produce a reliable pathology location for each series in a given study if a location is available for that series. More formally, we assume that we are given a dataset, D = {X i , Y i } N i=1 , with N denoting the total number of studies in the dataset, X i and Y i denoting the set of series and corresponding location for each series. Due to different acquisition protocols, the number of series in each X i can vary. Similarly, each Y i can have a different number of location. Our goal is to predict a pathology location for each series and its corresponding confidence score. Figure "
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,2.2,Backbone,"Our framework contains a backbone, which is responsible for generating multilevel feature maps. The multi-level feature maps are then fed into the pathology localization decoder. We use a 3D ResNet50  for each series k. We adhere to common standards by initializing the 3D ResNet50 backbone with pretrained weights. Prior work fine-tunes weights from the ImageNet dataset, but this may not be optimal if the target dataset has different characteristics. Our pretrained model for medical image analysis is based on ConVIRT "
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,2.3,Masked Self-attention,"To explore the complementary information between different series, we use Masked Self-Attention inspired from  representing the number of channels, d representing the depth dimension, and w and h representing the width and height dimensions, respectively. We concatenate the features F j l along the depth dimension d and add position embedding on the concatenated features. The transformer uses a linear projection for computing the set of queries, keys and values Q, K and V respectively. We adhere to the naming conventions used in  where The self-attention is calculated by taking the dot products between Q and K and then aggregating the values for each query, where, the attention mask M l-1 ∈ {0, 1} is a binarized output (thresholded at δ t ) of the the resized mask prediction of the previous (l -1)-th layer. δ t is empirically set to 0.15. The attention mask ignores the features that are not relevant to the pathology and attends to pathological features. B is a mask to handle missing series and it shares the same equation as 3. Finally, the transformer uses a non-linear transformation to calculate the output features, R l+1 , which shares the same resolution as that of R l . The transformer applies the attention mechanism 3 L times to generate a deep representation learning among the features. This approach allows the transformer model to effectively capture the relationships between different input positions."
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,2.4,Pathology Localization Decoder,"The localization decoder follows the transformer decoder paradigm, using a query, reference points, and input feature maps to predict a location and corresponding score. The decoder has N identical layers, each consisting of crossattention and feed forward networks (FFNs). The query Q ∈ R 1×256 and reference points R ∈ R 3 go through each layer, generating an updated Q as input for the next layer. Unlike Deformable DETR "
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,2.5,Loss Functions,"The model generates a single location ŷl ∈ R 3 , score y s and auxiliary heatmap outputs H for each series in a given study. The goal of our framework is to generate one reasonable location and its corresponding score for each series. Since there may be multiple locations annotated for a series, we use the Hungarian Matching function  where K is the number of intermediate heatmaps generated, x and h i are ground truth heatmap and predicted heatmap. To penalize the predicted location, we use the Huber loss defined as, where δ is empirically set to 0.3. The distance of a pathology does not differ more than λ (which can be calculated from the dataset) across series. With this information, we enforce proximity between the world coordinates which can be converted from the predicted volume coordinates across different series. We employ a Margin L1 loss, which penalizes the distance between points if they exceed the margin. Formally, where N is the number of series in a given study, wc ŷl is the world coordinates converted from volume coordinates. We then formulate the confidence score loss by considering the sum over the series of the binary cross entropy between the ground truth confidence score and predicted confidence score, formally defined as, Overall, the entire loss for a given study is formulated as, We set the hyper parameter w 1 , w 2 , w 3 and w 4 as 10, 1, 0.1, 1 respectively. These values are empirically set based on the validation loss."
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,3,Experiment,
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,3.1,"Implementation Details, Datasets and Evaluation Protocols","Implementation Details. Our model was implemented in Pytorch 1.13.1 on a NVIDIA A6000 GPU. We used an AdamW  Other hyper-parameters are mentioned in the supplementary paper. Datasets. The study is limited to secondary use of existing HIPPA-based deidentified data. No IRB required. We primarily conduct our experiments using knee MRI datasets, with a specific focus on MM tear, MM displaced fragment flap (DF), and MCC defect. Studies were collected at over 25 different institutions, and differed in scanner manufacturers, magnetic field strengths, and imaging protocols. The pathological locations were annotated by American Board certified sub-specialists radiologists. The most common series types included fat-suppressed (FS) sagittal (Sag), coronal (Cor) and axial (Ax) orientations, using either T2-weighted (T2) or proton-density (PD) protocols. For pathology detection, we use CorFS, SagFS, and SagPD. The dataset statistics that we use for training, validation and test are shown in Table "
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,3.2,Comparison with SOTA Methods,"Heatmap-Based Architectures. The proposed architecture was compared to two other models, the Gaussian Ball approach  Regression-Based Architectures. We compared our proposed architecture with several other methods: 1) a simple regression method that removes the pathology localization decoder and uses a fully connected layer to predict the pathology locations, 2) DETR, 3) deformable DETR "
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,3.3,Ablation Study,"We first analyze the importance of MSA to our framework by training models with and without MSA. As MSA is a variant of self-attention, we also experiment with self-attention and with an attention mechanism "
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,4,Conclusion,"We propose MOAT, a framework for performing localization in multi-series MRI studies which benefits from the ability to share relevant information across series via a novel application of self-attention. We increase the efficiency of the MOAT model by using a pathology localization decoder which is a variant of deformable decoder and initializes the reference points from the backbone of the model. We evaluate the effectiveness of our proposed framework (MOAT) on three challenging pathologies from knee MRI and find that it represents a significant improvement over several SOTA localization techniques. Moving forward, we aim to apply our framework to pathologies from other body parts with multiple series."
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,,Table 1 .,
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,,Table 3 .,
Improving Pathology Localization: Multi-series Joint Attention Takes the Lead,,Table 4 .,
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,1,Introduction,"Recent years have witnessed the great success of deep learning techniques in various applications on computer-aided diagnosis  Technically, the issue of class imbalance is formulated as a long-tailed problem in existing works  Although the aforementioned decoupling methods  To address the above two challenges, we propose the MRC-VFC framework that adopts the decoupling strategy to enhance the first-stage representation learning with Multi-view Relation-aware Consistency (MRC) and recalibrate the classifier using Virtual Features Compensation (VFC). Specifically, in the first stage, to boost the representation learning under limited samples, we build a twostream architecture to perform representation learning with the MRC module, which encourages the model to capture semantic information from images under different data perturbations. In the second stage, to recalibrate the classifier, we propose to generate virtual features from multivariate Gaussian distribution with the expectation-maximization algorithm, which can compensate for tail classes and preserves the correlations among features. In this way, the proposed MRC-VFC framework can rectify the biases in the encoder and classifier, and construct a balanced and representative feature space to improve the performance for rare diseases. Experiments on two public dermoscopic datasets prove that our MRC-VFC framework outperforms state-of-the-art methods for long-tailed diagnosis."
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,2,Methodology,As illustrated in Fig. 
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,2.1,Multi-view Relation-Aware Consistency,"The representation learning towards the decoupling models is insufficient  To motivate the student model to learn from the data representations but the ill distributions, we propose multi-view constraints on the consistency of two models at various phases. A straightforward solution is to encourage identical predictions for different augmentations of the same input image, as follows: where KL(•, •) refers to the Kullback-Leibler divergence to measure the difference between two outputs. As this loss function calculates the variance of classifier output, the supervision for the encoders is less effective. To this end, the proposed MRC measures the sample-wise and channel-wise similarity between the feature maps of two encoders to regularize the consistency of the encoders. We first define the correlations of individuals and feature channels as indicates the similarities across feature channels. Thus, the consistency between the feature maps of two models can be defined as: Furthermore, we also adopt the cross-entropy loss , where y denotes the ground truth, between the predictions and ground truth to ensure that the optimization will not be misled to a trivial solution. The overall loss function is summarized as where λ 1 , λ 2 and λ 3 are coefficients to control the trade-off of each loss term. By introducing extra semantic constraints, the MRC can enhance the representation capacity of encoders. The feature space generated by the encoders is more balanced with abundant semantics, thereby facilitating the MRC-VFC framework to combat long-tails in medical diagnosis."
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,2.2,Virtual Features Compensation,"Recalling the introduction of decoupling methods, the two-stage methods  where X k denotes the set of all samples in the k-th class, and g I (•) denotes the encoder trained in the first stage on the imbalanced dataset and N k is the sample number of the k-th class. We then randomly sample R feature vectors for each category from the corresponding Gaussian distribution N (μ k , Σ k ) to build the unbiased feature space, as {V k ∈ R R×C } K k=1 . We re-initialize the classifier and then calibrated it under cross-entropy loss, as follows: where K is the number of categories in the dataset. As the Gaussian distribution is calculated according to the statistics from the first-stage feature space, to further alleviate the potential bias, we employ the expectation-maximization algorithm  where q is a hyper-parameter to control the trade-off between the imbalance calibration and the classification task. At the maximization step, we freeze the encoder and train the classifier on the impartial feature space. By enriching the semantic features with balanced virtual features, our MRC-VFC framework can improve the classification performance in long-tailed datasets, especially the performance of minority categories. "
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,3,Experiments,
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,3.1,Datasets,"To evaluate the performance on long-tailed medical image classification, we construct two dermatology datasets from ISIC , where the imbalance factor r = N 0 /N k-1 is defined as the sample number of the head class N 0 divided by the tail one N k-1 . We adopt three imbalance factors for ISIC-2019-LT, as r = {100, 200, 500}. Furthermore, the ISIC-Archive-LT dataset "
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,3.2,Implementation Details,We implement the proposed MRC-VFC framework with the PyTorch library 
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,3.3,Comparison on ISIC-2019-LT Dataset,"We evaluate the performance of our MRC-VFC framework with state-of-the-art methods for long-tailed medical image classification, including (i) baselines: finetuning classification models with cross-entropy loss (CE), random data resampling methods (RS), and MixUp "
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,3.4,Comparison on ISIC-Archive-LT Dataset,"To comprehensively evaluate our MRC-VFC framework, we further perform the comparison with state-of-the-art algorithms on a more challenging ISIC-Archive-LT dataset for long-tailed diagnosis. As illustrated in Table  Performance Analysis on Head/Tail Classes. We further present the performance of several head and tail classes in Fig. "
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,4,Conclusion,"To address the long-tails in computer-aided diagnosis, we propose the MRC-VFC framework to improve medical image classification with balanced perfor-mance in two stages. In the first stage, we design the MRC to facilitate the representation learning of the encoder by introducing multi-view relation-aware consistency. In the second stage, to recalibrate the classifier, we propose the VFC to train an unbias classifier for the MRC-VFC framework by generating massive virtual features. Extensive experiments on the two long-tailed dermatology datasets demonstrate the effectiveness of the proposed MRC-VFC framework, which outperforms state-of-the-art algorithms remarkably."
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,,Fig. 1 .,
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,,,
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,,Fig. 2 .,
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,,Table 1 .,
Combat Long-Tails in Medical Classification with Relation-Aware Consistency and Virtual Features Compensation,,Table 2 .,
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,1,Introduction,"Classification and segmentation are two common tasks that use deep learning techniques to solve clinical problems  sample size, which can lead to poor performance across different sources. Different sources refer to the same modality collected from different scanners. In medical imaging, one of the main reasons for poor performance is the variation in the imaging process, such as the type of scanner, the settings, the protocol, etc.  To prevent overfitting and improve generalization, data augmentation  To address the above challenges, we propose a novel framework that combines the advantages of data augmentation and style transfer to enhance the model's segmentation and classification performance on ultrasound images from different sources. Our contributions (Fig. "
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,2,Methods,"Our proposed framework for ultrasonic image style augmentation consists of three stages, as illustrated in Fig.  "
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,2.1,Mixed Style Augmentation (MixStyleAug),"To improve the performance of the multi-task network, we design MixStyleAug, combining traditional transformations and style transfer to incorporate image information from target sources during training (Fig. "
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,2.2,Network Architecture and Feature Augmentation (FeatAug),"To address the limitation of MixStyleAug in small-size medical datasets, FeatAug is applied for augmenting image styles at the feature level during the network training (Fig.  The architecture of our designed multi-task network (Fig.  Previous studies reported that changing the mean and standard deviation of the feature maps could lead to different image styles  where A indicates the feature map, A indicates the augmented feature map, μ A indicates the mean of feature map A, σ A indicates the standard deviation of feature map A, and N (μ, σ) indicates a value randomly generated from a normal distribution with mean μ and standard deviation σ. In this study, the μ and σ of the normal distribution were empirically set to 0 and 0.1 according to preliminary experimental results, respectively."
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,2.3,Mask-Based Style Augmentation (MaskAug),"In general, the style transfer uses the style information of the entire image, but this approach may not be ideal when the regions outside of the ROIs contain conflicting style information as compared to the regions within the ROIs, as illustrated in Fig.  Figure "
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,2.4,Loss Function and Implementation Details,"We utilized cross-entropy (CE) as the primary loss function for segmentation and classification during the training stage. Additionally, Dice loss  where L CE denotes CE loss, L Dice denotes Dice loss, L m denotes the loss for the multi-task network optimization, L Seg denotes the loss computed from the segmentation result, and L Cls denotes the loss computed from the classification result. We adopted Pytorch to implement the proposed framework, and the multitask network was trained on Nvidia RTX 3070 with 8 GB memory. During training, the batch size was set to 16, the maximum epoch number was 300, and the initial learning rate was set to 0.0005. We decayed the learning rate with cosine annealing "
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,3,Experimental Results and Discussion,"Datasets and Evaluation Metrics. We evaluated our framework on five ultrasound datasets (each representing a source) collected from multiple centers using different ultrasound scanners, including three liver datasets and two thyroid nodules datasets. A detailed description of the collected datasets is provided in Table  AUROC is used to evaluate the classification performance. DSC is used to assess the performance of the segmentation. The DSC is defined as: where T P refers to the pixels where both the predicted results and the gold standard are positive, F P refers to the pixels where the predicted results are positive and the gold standard are negative, and F N refers to the pixels where the predicted results are negative and the gold standard are positive. Ablation Study. We evaluated the effects of MixStyleAug, FeatAug, and MaskAug by training a multi-task network with different combinations of these augmentation strategies. Table  Comparison with Previous Studies. We compared our proposed method with BigAug "
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,4,Conclusion,"We proposed an augmentation framework based on style transfer method to improve the segmentation and classification performance of the network on ultrasound images from multiple sources. Our framework consists of MixStyleAug, FeatAug, and MaskAug. MixStyleAug integrates the image information from various sources for well generalization, while FeatAug increases the number of styles at the feature level to compensate for potential style variations. MaskAug uses the segmentation results to guide the network to focus on the style information of the ROI in the ultrasound image. We evaluated our framework on five datasets from various sources, and the results showed that our framework improved the segmentation and classification performance across different sources."
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,,2 .,
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,,Fig. 2 .,
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,,Fig. 3 .,
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,,,
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,,Table 1 .,
A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images,,Table 2 .,
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,1,Introduction,"Cervical cancer is the second most common cancer among adult women. If diagnosed early, it can be effectively treated and cured  With the development of deep learning  Although the above-mentioned attempts can improve the screening performance significantly, there are several issues that need to be addressed: 1) Object detection methods often require accurate annotated data to guarantee performance with robustness and generalization. However, due to legal limitations, the scarcity of positive samples, and especially the subjectivity differences between cytopathologists for manual annotations  To address these issues, we propose a novel method for cervical abnormal cell detection using distillation from local-scale consistency refinement. Inspired by knowledge distillation, we construct a pre-trained Patch Correction Network (PCN), which is designed to exploit the supervised information from the PCN to reduce the impact of noisy labels and utilize the contextual relationships between cells. In our approach, we begin by utilizing RetinaNet "
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,2,Method,The proposed framework is shown in Fig. 
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,2.1,Patch Correction Network(PCN),"In Fig.  More specifically, the input image is processed by the base detector F d (•) firstly to obtain the primary proposal information. The proposed PCN F c (•) takes the top-K patches as inputs, which are cropped from original images according to the proposal location, denoted as I p = Cr(I, p), where Cr(•) denotes the crop function, I and p denote input image and proposal boxes predicted by F d (•), respectively. Similar to the RetinaNet proposal classifier in F d (•), the PCN F c (•) outputs a classification distribution vector s c . Therefore, the proposed PCN F c (•) can be represented as: ( The key idea is to augment the base detector F d (•) with the PCN F c (•) in parallel to enhance the proposal classification capability."
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,2.2,Classification Ranking Loss,"Due to the inaccurate confidence scores output by RetinaNet, false positive cells are inevitable after detection. Hence, a good correction network is required to generate more precise scores. In this work, the suspicious ranking of the detected patches is updated by applying PCN to them. The detector is optimized by interscale pairwise ranking loss. Specifically, the ranking loss is given by: where s c is the classification refinement score and s d is the detection score, which enforces s d > s c + margin in training. We set margin = 0.05. Such a design can enable RetinaNet to take the prediction score as references, and utilize refined scores from PCN to obtain more confident predictions. The ranking loss optimizes the detection to generate higher confidence scores than the previous prediction, thereby suppressing false positives and enabling the detection network to better distinguish between positive and negative cells."
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,2.3,ROI-Correlation Consistency (RCC) Learning,"In order to solve the problem of mismatched inputs to the detection and classification models, we add the ROI Align layer to the output of the FPN. However, for cervical abnormal cell detection, normal and abnormal cells may have very similar appearances, which might not be sufficient for conducting effective differentiation. In clinical practice, to determine whether a cervical cell is normal or abnormal, cytopathologists usually compare it to the surrounding reference cells. Therefore, we studied the correlation between the top K ROIs to help more accurate classification of abnormal cells. Based on the consistency strategy  We model the structured relation among different patches with a case-level Gram Matrix  where G ij is the inner product between the vectorized activation map A R i and A R j , whose intuitive meaning is the similarity between the activations of i th ROI and j th ROI within the input mini-batch. The final ROI relation matrix R R is obtained by conducting the L2 normalization for each row G R i of G R , which is expressed as: The proposed PCN F c (•) takes the B×K proposals of box regressor as inputs, we denote the local-scale feature map by PCN as F C ∈ R B×K×H ×W ×C , and set H = 56, W = 56. We perform average pooling on the feature map F C across the spatial dimension and then reshape it into A C ∈ R BK×HW C , the Case-wise Gram Matrix G C ∈ R BK×BK and the final relation matrix R C are computed as: The RCC requires the correlation matrix to be stable under ROI features and local-scale features to preserve the semantic relation between patches. We then define the proposed RCC loss as: where X is the proposals from the sampled mini-batch, R C (X) and R R (X) are the correlation matrices computed on X under different network. By minimizing L RCC during the training process, the network could be enhanced to capture the intrinsic relation between patches, thus helping to extract additional semantic information from cells."
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,2.4,Optimization,"To better optimize the Retinanet detector in a reinforced way, we take the following training strategy, which consists of three major stages. In the first stage, we collect images with doctors' labels for training and initialized the detection net. In the second stage, we train PCN with cross-entropy loss until convergence. In the last stage, we freeze the PCN and optimize the detector. The detector is optimized using the total objective function, which is written as follows: where L cls and L reg are the ordinary detection loss for each detection head in RetinaNet. L cls is a Cross-Entropy loss for classification and L reg is a Smooth-L 1 loss for bounding box regression. L Rank is the classification ranking loss,L RRC is the RCC loss. α and β are hyper-parameters that denote the different weights of loss. During inference, only the optimized detector is used to output the final detection results without any additional modules. 3 Experimental Results"
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,3.1,Dataset and Experimental Setup,"Dataset. For cervical cell detection, our dataset includes 3761 images of 1024 × 1024 pixels cropped from WSIs. Our private dataset was collected and qualitycontrolled according to a standard protocol involving three pathologists: A, B, and C. Pathologist A had 33 years of experience in reading cervical cytology images, while pathologists B and C had 10 years of experience each. Initially, the images were randomly assigned to pathologist B or C for initial labeling. Later, the assigned pathologist's annotations were reviewed and verified by the other pathologist. Any discrepancies found were checked and re-labeled by pathologist A. These images were divided into the training set and the testing set according to the ratio of 9:1. We also collect a new dataset of 5000 positive and negative 224 × 224 cell patches to train the PCN. Implementation Details. The backbone of the suspicious cell detection network is RetinaNet with ResNet-50 "
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,3.2,Evaluation of Cervical Abnormal Cell Detection,"Comparison with SOTA Methods. We compare the performance of our proposed method against known methods for cervical lesion detection as well as representative methods for object detection. Table  Ablation Study. We also perform an ablation study to further evaluate the contributions of each part in our method.   In addition, to further show the effectiveness of our method, we visualize the feature maps of Retinanet and the proposed method in Fig. "
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,4,Conclusion,"In this paper, we integrate a distillation strategy that uses the knowledge learned from the pre-trained PCN to guide the training of the detection model to minimize the effects of noisy labels and explore the feature interaction between cells. Our method constructs RetinaNet with the PCN module which provides the refined scores and local-scale features of extracted patches. Specifically, we propose the ranking loss by utilizing refined scores to optimize the RetinaNet proposal classifier by reducing the impact of noisy labels. In addition, the ROI features generated by the detector and local-scale features from the PCN are used for correlation consistency learning, which explores the extracted cells' relationship. Our work can achieve better performance without adding new modules during inference. Experiments demonstrate the effectiveness and robustness of our method on the task of cervical abnormal cell detection."
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,,Fig. 1 .,
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,,Fig. 2 .,
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,,Table 1 .,
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,,Table 2,
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,,Table 2 .,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,1,Introduction,"Staining is a vital process in preparing tissue samples for histology studies. Specifically, with dyes such as Hematoxylin and Eosin, transparent tissue elements can be transformed into distinguishable features  To address the issue of stain variations between different domains, stain style transfer has been proposed. While the conventional color matching  The major contributions are three-fold, summarized as follows. (1) We propose StainDiff, which is the first attempt at a pure denoising diffusion probabilistic model for stain transfer. More innovatively, unlike existing diffusion models, StainDiff is capable of learning from unpaired histology images, making it a more flexible and practical solution. The model is superior to GAN-based methods as the training of additional discriminators is free, and also spares for the difficulty in the alignment of posterior probabilities in AE-based approaches. (2) We also propose a self-ensemble scheme to further improve and stabilize the style transfer performance in StainDiff. This scheme utilizes the stochastic property of the diffusion model to generate multiple slightly different outputs from one input at the inference stage. (3) A broad range of histology tasks, such as stain normalization between multiple clients, can be conveniently achieved with minor adjustment to the loss in StainDiff."
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,2,Methods,"Overview. The goal of this work is to design a diffusion model  T thorough the following equation: where N (•) denotes the Gaussian distribution, I is the identity matrix. The hyper-parameters β t s follow a linear rule as defined in DDPM  are designed to learn the transfer between the latent variables across the two domains in an unsupervised fashion, using a novel cycle-consistency constraint. Formally, this constraint ensures that two cycles as depicted in Fig.  where • denotes the composition of operations. It follows the cycle-consistency constraint formulated by where xA t+1 and xB t+1 are defined by Eq. (  Extension to Stain Normalization. The stain transfer primarily addresses the domain gap between two stain styles, which is mathematically formulated as a one-to-one mapping. Meanwhile, in some clinical settings, multiple institutions or hospitals are involved, where stain normalization is usually employed for multiple stain styles to one style alignment. The proposed symmetric StainDiff structure can be easily adapted to support stain normalization, with minimal change to the loss in Eq. ( "
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,3,Experiments,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Datasets. Evaluations of StainDiff are conducted on two datasets. (1),Dataset-A: MITOS-ATYPIA 14 Challenge
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Implementations.,"All experiments are implemented in Python 3.8.13 with Pytorch 1.12.1 on two NVIDIA GeForce RTX 3090 GPU cards with 24GiB of memory each in parallel. We leverage the Adam optimizer with a learning rate of 2 × 10 -4 , and a batch size of 4. The learning scheme follows previous work "
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,4,Conclusion,"In this paper, we propose StainDiff, a denoising diffusion model for histological stain style transfer, hence a model can get rid of the challenging issues in mainstream networks, such as the mode collapses in GANs or alignment between posterior distributions in AEs. Innovatively, by imposing a cycle-consistent constraint imposed on latent spaces, StainDiff enables learning from unpaired histology images, making it widely applicable to real clinical settings. One future work will explore efficient sampling diffusion models, e.g., DDIM "
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Fig. 1 .,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Fig. 2 .,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Fig. 3 .,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Table 1 .Fig. 4 .,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Reverse Process and Cycle-Consistency Constraint.,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,,
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,,Table 2 .,
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,1,Motivation,"Whole slide imaging is capable of effectively digitizing specimen slides, showing both the microscopic detail and the larger context, without any significant manual effort. Due to the enormous resolution of the whole slide images (WSIs), a classification based on straight-forward convolutional neural network architectures is not feasible. Multiple instance learning  Here, we consider feature-level data augmentation directly applied to the representation extracted using a convolutional neural network. These methods can be easily combined with image-based augmentation and show the advantage of a high computational efficiency (since operations are efficient and pre-computed features can be used)  The main contribution of this work is a set of novel data augmentation strategies for MIL, based on the interpolation of patch descriptors. Inspired by the (linear) MixUp approach "
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,2,Methods,"In this paper, we consider MIL approaches relying on separately trained feature extraction and classification stages  In the original MixUp formulation of Zhang et al.  A single input (corresponding to a WSI) of a MIL approach with a separate feature extraction stage "
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,2.1,Inter-MixUp and Intra-MixUp,"Inter-MixUp refers to the generation of synthetic feature vectors by linearly combining feature vectors of a pair of WSIs (see Fig.  with α being a uniformly sampled random weight (α ∈ [0, 1]). The WSI indexes v and w are uniformly sampled from the set of indexes. The index u ranges from the 1 to the number of extracted WSI descriptors. Since the new synthetic descriptors are individually generated in each epoch, there is no benefit if the number of extracted WSI descriptors is increased. We fix this number to the number of WSIs in the training data set, in order to keep the number of training iterations per epoch consistent. Two different configurations are considered. Firstly, we investigate the interpolation between WSIs of the same class (V1). Secondly, interpolation between all WSIs is performed, which also includes the interpolation between the labels (V2). In the case of V2, also the one-hot-encoded label vectors are linearly combined, such that y The random values, α, v and w are selected individually for each individual WSI and each epoch. Before applying the MixUp operation, the vector tupel is randomly shuffled (as performed in all experiments). Intra-WSI combinations (Intra-MixUp) refers to the generation of synthetic descriptors by combining feature vectors within an individual WSI (see Fig. "
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,2.2,Experimental Setting,"As experimental architecture, use the dual-stream MIL approach proposed by Li et al  As comparison, several other augmentation methods on feature level are investigated including random sampling, selective random sampling and random noise. Random sampling corresponds to the random selection of patches (feature vectors) from each WSI. Thereby the amount of investigated data per WSI is reduced with the benefit of increasing the variability of the data. In the experiments, we adjust the sample ratio q between the patch-based features for training and testing. A q of 50 % indicates that 512 descriptors are used for training while for testing always a fixed number of 1024 is used. Selective random sampling corresponds to the random sampling strategy, with the difference that the ratio of features is not fixed but drawn from a uniform random distribution (U (q, 100 %)). Here, a q of 50 % indicates that for each WSI, between 512 and 1024 feature vectors are selected. In the case of the random noise setting, to each feature vector x i , a random noise vector r is added (x i = x i + r). The elements of r are randomly sampled (individually for each x i ) from a normal distribution N (0, σ ). To incorporate for the fact that the feature dimensions show different magnitudes, σ is computed as the product of the meta parameter σ and the standard deviation of the respective feature dimension. In this work, we aimed at distinguishing different nodular lesions of the thyroid, focusing especially on benign follicular nodules (FN) and papillary carcinomas (PC). This differentiation is crucial, due to the different treatment options, in particular with respect to the extent of surgical resection of the thyroid gland  The patches were randomly extracted from the WSI, based on uniform sampling. For each patch, we checked that at least 75 % of the area was covered with tissue (green color channel) in order to exclude empty areas "
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,3,Results,Figure 
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,4,Discussion,"In this work, we proposed and examined novel data augmentation strategies based on the idea of interpolations of feature vectors in the MIL setting. Instance-based MIL did not show any competitive scores. Obviously the model reducing each patch to a single value is not adequate for the classification of frozen or paraffin sections from thyroid cancer tissues. The considered dual-stream approach, including an embedding and instance-based stream, exhibited slightly improved average scores, compared to embedding-based MIL only. In our analysis, we focused on the embedding-based configuration and on the balanced combined approach (referred to as 2/2). With the baseline data augmentation approaches, the maximum improvements were 0.03, and 0.02 for the frozen, and 0.01, and 0.05 for the paraffin data set. The Inter-MixUp approach did not show any systematic improvements. Independently of the chosen strategy (V1, V2), concerning the combination within or between classes, we did not notice any positive trend. The multilinear Intra-MixUp method, however, exhibited the best scores for 3 out of 4 combinations and the best overall mean accuracy for both, the frozen and the paraffin data set. Also a clear trend with increasing scores in the case of an increasing ratio of augmented data (β) is visible. The linear method showed a similar, but less pronounced trend. Obviously, the straightforward application of the MixUp scheme (as in case of the Inter-MixUp approach), is inappropriate for the considered setting. An inhibiting factor could be a high inter-WSI variability leading to incompatible feature vectors (which are too far away from realistic samples in the feature space). To particularly investigate this effect, we performed 2 different Inter-MixUp settings (V1 & V2), with the goal of identifying the effect of mixed (and thereby more dissimilar) or similar classes during interpolation. The analysis of the distance distributions between patch representations confirmed that, the variability between WSIs is clearly larger than the variability within WSIs. In addition, the results showed that the variability between classes is, on patch-level, not clearly larger than the variability within a class. Obviously variability due to the acquisition outweigh any disease specific variability. This could provide an explanation for the effectiveness of Intra-MixUp approach compared to the (similarly) poorly performing Inter-MixUp settings. We expect that stain normalization methods (but not stain augmentation) could be utilized to align the different WSIs to provide a more appropriate basis for inter-WSI interpolation. With regard to the different data sets, we noticed a stronger, positive effect in case of the frozen section data set. This is supposed to be due to the clearly higher variability of the frozen sections corresponding with a need for a higher variability in the training data. We also noticed a stronger effect of the solely embedding-based architecture (also showing the best overall scores). We suppose that this is due to the fact that the additional loss of the dual-stream architecture exhibits a valuable regularization tool to reduce the amount of needed training data. With the proposed Intra-MixUp augmentation strategy, this effect diminishes, since the amount and quality of training data is increased. To conclude, we proposed novel data augmentation strategies based on the idea of interpolations of image descriptors in the MIL setting. Based on the experimental results, the multilinear Intra-MixUp setting proved to be highly effective, while the Inter-MixUp method showed inferior scores compared to a state-of-the-art baseline. We learned that there is a clear difference between combinations within and between WSIs with a noticeable effect on the final classification accuracy. This is supposedly due to the high variability between the WSIs compared to a rather low variability within the WSIs. In the future, additional experiments will be conducted including stain normalization methods and larger benchmark data sets to provide further insights."
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,,Fig. 1 .,
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,,Fig. 2 .,
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,,Fig. 3 .,
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,1,Introduction,"Breast cancer (BC) is the most common cancer diagnosed among females and the second leading cause of cancer death among women after lung cancer  Among different types of imaging biomarkers, histopathological images are generally considered the golden standard for BC prognosis since they can confer important cell-level information that can reflect the aggressiveness of BC  To deal with the above challenges, several researchers began to design domain adaption algorithms, which utilize the labeled data from a related cancer subtype to help predict the patients' survival in the target domain. Specifically, Alirezazadeh et al  Although much progress has been achieved, most of the existing studies applied the feature alignment strategy to reduce the distribution difference between source and target domains. However, such transfer learning methods neglected to take the interaction among different types of tissues into consideration. For example, it is widely recognized that tumor-infiltrating lymphocytes (TILs) and its correlation with tumors reveal a similar role in the prognosis of different BRCA subtypes. For instance, Kurozumi et al  Based on the above considerations, in this paper, we proposed a TILs-Tumor interactions guided unsupervised domain adaptation (T2UDA) algorithm to predict the patients' survival on the target BC subtype. Specifically, T2UDA first applied the graph attention network (GATs) to learn node embeddings and the spatial interactions between tumor and TILs patches in WSI. In order to preserve the node-level and interaction-level similarities across different domains, we not only aligned the embedding for different types of nodes but also designed a novel Tumor-TILs interaction alignment (TTIA) module to ensure that the distribution of the interaction weights are similar in both domains. We evaluated the performance of our method on the Breast Invasive Carcinoma (BRCA) cohort derived from the Cancer Genome Atlas (TCGA), and the experimental results indicated that T2UDA outperforms other domain adaption methods for predicting patients' clinical outcomes."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,2,Method,We summarized the proposed T2UDA network in Fig. 
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Source Graph Construction,"Here, given patches as nodes V , we first calculated the pairwise distance among different nodes, and select the top 10 percent connections with the smallest distance values as edges E. For each node in V , we followed the study in "
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Calculating TILs-Tumor Interaction via Graph Attention Networks(GATs).,"To characterize the interaction between different TILs and tumor patches, we employed GAT  we calculated the attention coefficients among different nodes, which can be formulated as: Furthermore, a softmax function was then adopted to normalize the attention coefficients e ij : where N i represents all neighbors of node i. The new feature vector v i for node i was calculated via a weighted sum: Finally, the output features of each GAT layer were aggregated in the readout layer. We fed the generated output features from each readout layer into the Cox hazard proportional regression model for the final prognosis predictions. Feature Alignment. In the proposed GAT-based transfer learning framework, the feature alignment component was employed on its first two layers. Then, for the node embeddings with different types (TILs and Tumor) in both the source and target domain, we performed a mean pooling operation to obtain their aggregated features. Next, we aligned the aggregated tumor or TILs features from the two domains separately using Maximum Mean Discrepancy(MMD)  Here, we adopted MMD for feature alignment due to its ability to measure the distance between two distributions without explicit assumptions on the data distribution, we showed the objective function of MMD in our method as follows: where H is a Hilbert space, f represents the features from the source, f represents the feature from the target, r represents the layer number, k ∈ {L, T } referred to TILs or tumor node. In addition, n denotes the number of source samples, while m refers to the number of target samples.  TILs-Tumor Interaction Alignment. To accurately characterize the interaction between TILs and tumors, we further analyzed the extracted interaction weights by dividing them into 10 intervals (i.e., bins). For each interval, we calculated the sum of all source domain interaction weights as i s k and the sum of all target domain interaction weights as i t k , where k represents the k-th interval. Consequently, we obtained two vectors and applied softmax on each of them for normalization that can be denoted as . In order to measure the dissimilarity between p i and q i , the Kullback-Leibler (KL) divergence is adapted on the third layer of GAT, which can be formulated as: According to Eq.( "
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Prognosis Prediction by the Cox Proportional Hazard Model.,"The Cox proportional hazard model was applied to predict the patients' clinical outcome  where x i represents the output of the last layer for the prognosis task and R (t i ) is the risk set at time t i , which represents the set of patients that are still under risk before time t. In addition, δ i is an indicator variable. Sample i refers to censored patient if Overall Objective. To achieve domain-adaptive prognosis prediction, the final loss function included the Cox loss, FA loss, and TTIA loss as the following formula: where α and β represent the weights assigned to the importance of FA component and TTIA component respectively."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,3,Experiments and Results,"Datasets. We conducted our experiments on the breast invasive carcinoma (BRCA) dataset from The Cancer Genome Atlas (TCGA). Specifically, the BRCA dataset includes 661 patients with hematoxylin and eosin (HE)-stained pathological imaging and corresponding survival information. Among the collected BRCA patients in TCGA, the number of ER positive(ER+) and ER negative(ER-) patients are 515 and 146, respectively. We hope to investigate if the proposed T2UDA could be used to help improve the prognosis performance of (ER+) or (ER-) with the aid of the survival information on its counterpart."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,3.1,Implementation Details and Evaluation Metrics,"The dimension of intermediate layers in GAT was 256. The pooling ratio in SagPool was set to 0.7. α and β were tuned from {0.01, 0.1}. During training, the model was trained for 150 epochs for both the main experiment and all comparative experiments. We used the Adam optimizer with a learning rate tuned from {1e -5, 1e -4}. We evaluated the performance of our model using the Concordance Index (CI) and Area Under the Curve (AUC) as performance metrics. Both CI and AUC range from 0 to 1, with larger values indicating better prediction performance and vice versa "
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,3.2,Result and Discussion,"In this study, we compared the performance of our proposed model with several existing domain adaptation methods, including 1) DDC  The results presented in Table  We also evaluated the contributions of the key components of our framework and found that T2UDA performed better than Source only and T2UDA-v1, which shows the advantage of minimizing differences in TILs-Tumor interaction weights. In addition, we also evaluated the patient stratification performance of different methods. As shown in Fig.  We also examined the consistency of important edges in each group of stratified patients based on the TILs-Tumor interaction weights calculated by the GAT-based framework in the source and target domains. As seen in Fig. "
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,4,Conclusion,"In this paper, we presented an unsupervised domain adaptation algorithm that leverages TILs-Tumor interactions to predict patients' survival in a target BC subtype(T2UDA). Our results demonstrated that the relationship between TILs and tumors is transferable and can be effectively used to improve the accuracy of survival prediction models. To the best of our knowledge, T2UDA was the first method to successfully achieve interrelationship transfer between TILs and tumors across different cancer subtypes for prognosis tasks."
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Fig. 1 .,
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Fig. 2 .,
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Fig. 3 .,
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Fig. 4 .,
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,,
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,,Table 1 .,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,1,Introduction,"Sarcopenia is a progressive and skeletal muscle disorder associated with loss of muscle mass, strength, and function  The development of effective, reproducible, and cost-effective algorithms for reliable quantification of muscle mass is critical for diagnosing sarcopenia. However, automatically identifying sarcopenia is a challenging task due to several reasons. First, the subtle contrast between muscle and fat mass in the leg region makes it difficult to recognize sarcopenia from X-ray images. Second, although previous clinical studies  Deep learning attracted intensive research interests in various medical diagnosis domains  In this work, we propose a multi-modality contrastive learning (MM-CL)"
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,2,Data Collection,"In this retrospective study, we collected anonymized data from patients who underwent sarcopenia examinations at the Taipei Municipal Wanfang Hospital. The data collection was approved by an institutional review board. The demographic and clinical characteristics of this dataset are shown in Table "
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,3,Methodology,As shown in Fig. 
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,3.1,Non-local CAM Enhancement,"Considering the large proportion of muscle regions in hip X-ray images, capturing longrange dependencies is of great importance for sarcopenia screening. In this work, we adopt the non-local module  CAM Enhancement: First, each training image X ∈ R 3×H×W is sent to the CAM generator as shown in Fig.  where sigmoid denotes the Sigmoid function. The downstream main encoder is identical to ResNet18."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Non-local Module:,"Given a hip X-ray image X and the corresponding CAM map X m , we apply the backbone of ResNet18 to extract the high-level feature maps x ∈ R C×H ×W . The feature maps are then treated as inputs for the non-local module. For output xi from position index i, we have where concat denotes concatenation, w f is a weight vector that projects the concatenated vector to a scalar, ReLU is the ReLU function, a ij denotes the non-local feature attention that represents correlations between the features at two locations (i.e., x i and x j ), θ, φ, and g are mapping functions as shown in Fig. "
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,3.2,Visual-Text Feature Fusion,"After capturing the global information, we aim to fuse the visual and text features in the high-level latent space. We hypothesize that the clinical data may have a positive effect to boost the visual prediction performance. The overall structure of this strategy is given in Fig. "
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Visual-Text Fusion Module:,"In order to learn from clinical data, we first encode 5 numerical variables as a vector and send it to TextNet. As shown in Fig.  and W v are part of the model parameters to be learned. We compute the visual-text self-attentive feature ẑvt i at position i as The softmax operation indicates the attention across each visual and text pair in the multi-modality feature."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,3.3,Auxiliary Contrastive Representation,"Inspired by unsupervised representation learning  During the training stage, given N samples in a mini-batch, we obtain 2N samples by applying different augmentations (AutoAugment  where N is the set of negative counterparts of ôvt , the sim(•, •) is the cosine similarity between two representations, and τ is the temperature scaling parameter. Note that all the visual-text embeddings in the loss function are 2 -normalized. Finally, we integrate the auxiliary contrastive learning branch into the main Classification Head as shown in Fig. "
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,4,Experiments and Results,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,4.1,Implementation Details and Evaluation Measures,"Our method is implemented in PyTorch using an NVIDIA RTX 3090 graphic card. We set the batch size to 32. Adam optimizer is used with a polynomial learning rate policy, where the initial learning rate 2.5 × 10 -4 is multiplied by 1 -epoch total_epoch power with power as 0.9. The total number of training epochs is set to 100, and early stopping is adopted to avoid overfitting. Weight factor β is set to 0.01. The temperature constant τ is set to 0.5. Visual images are cropped to 4/5 of the original height and resized to 224 × 224 after different online augmentation. The backbone is initialized with the weights pretrained on ImageNet. Extensive 5-fold cross-validation is conducted for sarcopenia diagnosis. We report the diagnosis performance using comprehensive quantitative metrics including area under the receiver operating characteristic curve (AUC), F1 score (F1), accuracy (ACC), sensitivity (SEN), specificity (SPC), and precision (PRE)."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,4.2,Quantitative and Qualitative Comparison,"We implement several state-of-the-art single-modality (ResNet, ARLNet, MaxNet  We further visualize the AUC-ROC and Precision-Recall curves to intuitively show the improved performance. As shown in Fig.  We have three observations: (1) Multi-modality based models outperform singlemodality based methods, and we explain this finding that multiple modalities complement each other with useful information. (2) MaxNet "
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,4.3,Ablation Study of the Proposed Method,"Our first finding is that fusing visual and text knowledge brings significant improvement, which demonstrates that the extra tabular information could help substantially in learning. Second, incorporating unsupervised contrastive learning in the supervised learning framework could also improve the feature representation ability of the model."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,5,Conclusions,"In conclusion, we propose a multi-modality contrastive learning model for sarcopenia screening using hip X-ray images and clinical information. The proposed model consists of a Non-local CAM Enhancement module, a Visual-text Feature Fusion module, and an Auxiliary contrastive representation for improving the feature representation ability of the network. Moreover, we collect a large dataset for screening sarcopenia from heterogeneous data. Comprehensive experiments and explanations demonstrate the superiority of the proposed method. Our future work includes the extension of our approach to other multi-modality diagnosis tasks in the medical imaging domain."
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Fig. 1 .,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Fig. 2 .,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Fig. 3 .,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Table 1 .,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Table 2 .,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Table 2 outlines,
Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information,,Table 3 .,
Gene-Induced Multimodal Pre-training for Image-Omic Classification,1,Introduction,"Pathological image-omic analysis is the cornerstone of modern medicine and demonstrates promise in a variety of different tasks such as cancer diagnosis and prognosis  Though deep learning techniques have revolutionized medical imaging, designing a task-specific algorithm for image-omic multi-modality analysis is challenging.  Specifically, to our knowledge, most multi-modality techniques have been designed for modalities such as chest X-ray and reports  In this paper, we propose a task-specific framework dubbed Gene-induced Multimodal Pre-training (GiMP) for image-omic classification. Concretely, we first propose a transformer-based gene encoder, Group Multi-head Self Attention (GroupMSA), to capture global structured features in gene expression cohorts. Next, we design a pre-training paradigm for WSIs, Masked Patch Modeling (MPM), masking random patch embeddings from a fixed-length contiguous subsequence of a WSI. We assume that one patch-level feature embedding can be reconstructed by its adjacent patches, and this process enhances the learning ability for pathological characteristics of different tissues. Our MPM only needs to recover the masked patch embeddings in a fixed-length subsequence rather than processing all patches from WSIs. Furthermore, to model the high-order relevance of the two modalities, we combine CLS tokens of paired image and genomic data to form unified representations and propose a triplet learning module to differentiate patient-level positive and negative samples in a mini-batch. It is worth mentioning that although our unified representation fuses features from the whole gene expression cohort and partial WSIs in a mini-batch, we can still learn high-order relevance and discriminative patient-level information between these two modalities in pre-training thanks to the triplet learning module. In addition, note that our proposed method is different from self-supervised pre-training. Specifically, we focus not only on superior representation learning capability, but also category-related feature distributions, w.r.t. intra-and inter-class variation. With the training process going on, complete information from WSIs can be integrated and the fused multimodal representations with high discrimination will make it easier for the classifier to find the classification hyperplane. Experimental results demonstrate that our GiMP achieves significant improvement in accuracy than other image-omic competitors, and our multimodal framework shows competitive performance even without pre-training."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,2,Method,"Given a multimodal dataset D consisting of pairs of WSI pathological images and genomic data (X I , X G ), our GiMP learns feature representations via accomplishing masked patch modeling and triplets learning. As shown in Fig. "
Gene-Induced Multimodal Pre-training for Image-Omic Classification,2.1,Group Multi-head Self Attention,"In this section, we propose Group Multi-head Self Attention (GroupMSA), a specialized gene encoder to capture structured features in genomic data cohorts. Specifically, inspired by tokenisation techniques in natural language processing "
Gene-Induced Multimodal Pre-training for Image-Omic Classification,2.2,Patch Aggregator with Efficient Attention Operation,"Let's denote the whole slide pathological image with H×W spatial resolution and C channels by X I ∈ R H×W ×C . We follow the preprocessing strategy of CLAM  are downsampling matrices obtained from clustering tokens in Q l and K l for layer l ∈ {0, 1}."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,2.3,Gene-Induced Multimodal Fusion,"In this section, we first describe the formulation of masked patch modeling. Then we introduce the overall pipeline of our pre-training framework and illustrate how to apply it to downstream classification tasks. Masked Patch Modeling. In WSIs, the foreground patches are spatially contiguous, which means the adjacent patches have similar feature embeddings. Thus, we propose a Masked Patch Modeling (MPM) pre-training strategy that masks random patch embeddings from a fixed-length contiguous subsequence j=i in H p and reconstruct the invisible information. The fixed subsequence length L is empirically set to 6,000 and the sequences shorter than L are duplicated to build mini batches. Besides, the masking ratio is set to 50% and the set of masked subscripts is denoted as M ∈ R 0.5L . Next, a two-layer Nystrom-based patch aggregator followed by a lightweight reconstruction decoder are adopted to process the masked sequence H mpm and the reconstructed sequence is denoted as . Note that we reconstruct the missing feature embeddings rather than the raw pixels of the masked areas, which is different from traditional MIM methods like SimMIM  where 1[•] is the indicator function. Gene-Induced Triplet Learning. The transformer-based backbones in the classification task require the CLS token to be able to extract accurate global information, which is even more important yet difficult in WSIs due to the long sequence challenge. In addition, in order to construct the mini-batch, the subsequences we intercept in the MPM pre-training phase may not be sufficiently representative of the image-level characteristics. To overcome these issues, we further propose a gene-induced triplet learning module, which uses pathological images and genomic data as input and extracts high-order and discriminative features via CLS tokens. Firstly, we pre-train the GroupMSA module by patientlevel annotations in advance and froze it in the following iterations. Next, a learnable CLS token CLS img for WSIs is added to the input masked sequence H mpm . After extracting the input patch embeddings and gene sequence separately, we concatenate CLS img and CLS ge as CLS pat ∈ R 2d to represent patient-level characteristics. Suppose we obtain a triplet list {x, x + , x -} during current iteration, where x, x + , x -are concatenated tokens of anchor CLS pat , positive CLS pat , and negative CLS pat , respectively. To enhance the global modeling capability, i.e., extracting more precise patient-level features, we expect that the distance between the anchor and the positive sample gets closer, while the negative sample is farther away. The loss function for optimizing triplet learning is computed by: δ indicates a threshold, e.g., δ = 0.8. Finally, the loss function for GiMP pretraining is: Multimodal Fine-Tuning. Applying the pre-trained backbone to image-omic classification task is straightforward, since GiMP pre-training allows it to learn representative patient-level features. We use a simple Multi-Layer Perceptron (MLP) head to map CLS pat to the final class predictions P , which can be written as P = softmax(MLP(CLS pat )). 3 Experiments"
Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.1,Experimental Setup,"Datasets. We verify the effectiveness of our method on The Caner Genome Atlas (TCGA) non-small cell lung cancer (NSCLC) dataset, which contains two cancer subtypes, i.e., Lung Squamous Cell Carcinoma (LUSC) and Lung Adenocarcinoma (LUAD). After pre-processing  Implementation Details. The pre-training process of all algorithms is conducted on the training set, without any extra data augmentation. Note that our genetic encoder, GroupMSA, is fully supervised pre-trained on unimodal genetic data to accelerate convergence and it is frozen during GiMP training process. The maximum pre-training epoch for all methods is set to 100 and we finetune the models at the last epoch. During fine-tuning, we evaluate the model on the validation set after every epoch, and save the parameters when it performs the best. AdamW "
Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.2,Comparison Between GiMP and Other Methods,"We conduct comparisons between GiMP and three competitors under different settings. Firstly, we compare our proposed patch aggregator with the current state-of-the-art deep MIL models on unimodal TCGA-NSCLC dataset, i.e., only pathological WSIs are included as input. As shown in Table "
Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.3,Ablation Study,Table 
Gene-Induced Multimodal Pre-training for Image-Omic Classification,4,Conclusion,"In this paper, we propose a novel multimodal pre-training method to exploit the complementary relationship of genomic data and pathological images. Concretely, we introduce a genetic encoder with structured learning capabilities and an effective gene-induced multimodal fusion module which combines two pretraining objectives, triplet learning and masked patch modeling. Experimental results demonstrate the superior performance of the proposed GiMP compared to other state-of-the-art methods. The contribution of each proposed component of GiMP is also demonstrated in the experiments."
Gene-Induced Multimodal Pre-training for Image-Omic Classification,,Fig. 1 .,
Gene-Induced Multimodal Pre-training for Image-Omic Classification,,Fig. 2 .,
Gene-Induced Multimodal Pre-training for Image-Omic Classification,,Table 1 .,
Gene-Induced Multimodal Pre-training for Image-Omic Classification,,Table 2 .,
Gene-Induced Multimodal Pre-training for Image-Omic Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_49.
Histopathology Image Classification Using Deep Manifold Contrastive Learning,1,Introduction,"Whole slide image (WSI) classification is a crucial process to diagnose diseases in digital pathology. Owing to the huge size of a WSI, the conventional WSI classification process consists of patch decomposition and per-patch classification, followed by the aggregation of per-patch results using multiple instance learning (MIL) for the final per-slide decision  The main motivation of this work is to extend the current contrastive learning to represent the nonlinear feature manifold inspired by manifold learning. Owing to the manifold distribution hypothesis  In this study, we propose a hybrid method that combines manifold learning and contrastive learning to generate a good feature extractor (encoder) for histopathology image classification. Our method uses the sub-classes and prototypes as in conventional contrastive learning, but we propose the use of geodesic distance in generating the sub-classes to represent the non-linear feature manifold more accurately. By doing this, we achieve better separation between features with large margins, resulting in improved MIL classification performance. The main contributions of our work can be summarized as follows: -We introduce a novel integration of manifold geodesic distance in contrastive learning, which results in better feature representation for the non-linear feature manifold. We demonstrate that the proposed method outperforms conventional cosine-distance-based contrastive learning methods. -We propose a geodesic-distance-based feature clustering for efficient contrastive loss evaluation using prototypes without brute-force pairwise feature similarity comparison while approximating the overall manifold geometry well, which results in reduced computation. -We demonstrate that the proposed method outperforms other state-of-theart (SOTA) methods with a much smaller number of sub-classes without complicated prototype assignment (e.g., hierarchical clustering). To the best of our knowledge, this work is the first attempt to leverage manifold geodesic distance in contrastive learning for histopathology WSI classification."
Histopathology Image Classification Using Deep Manifold Contrastive Learning,2,Method,The overview of our proposed model is illustrated in Fig. 
Histopathology Image Classification Using Deep Manifold Contrastive Learning,2.1,Deep Manifold Embedding Learning,"As illustrated in Fig.  The output is then passed through two different paths, namely, deep manifold and softmax paths. Loss Functions. For the deep manifold training, we adopted two losses: (1) intra-subclass loss L intra and (2) inter-subclass loss L inter . The main idea in intra-subclass loss is to make the samples from the same sub-class stay near their respective sub-class prototype. L intra is formulated as follows: where x i j is the i-th patch in the j-th batch, J represents the total number of batches, I represents the total number of patches per batch, f (•) is the feature extractor, and p + indicates the positive prototype of the patch (i.e., the prototype of the subclass containing x i j ). The prototype of each sub-class is computed by simply taking the mean of all the patch features that belong to each sub-class. Inter-subclass loss L inter is proposed to make the sub-classes from a different class far apart from one another. The formulation of L inter is as shown below: where  Another path via softmax is simply trained on outputs from the feature extractor with the ground truth slide-level labels y by the cross-entropy loss L CE , which is defined as follows: where y is the ground truth slide-level label and ŷ is predicted label. Finally, the total loss for the first stage is defined as follows:"
Histopathology Image Classification Using Deep Manifold Contrastive Learning,2.2,MIL Classification,"As illustrated in Fig.  After the classification, majority voting is applied to the predicted labels of the bags to derive the final predicted label for each WSI."
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3,Result,
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.1,Datasets,"We tested our proposed method on two different tasks: (1) intrahepatic cholangiocarcinomas(IHCCs) subtype classification and (2) liver cancer type classification. The dataset for the former task was collected from 168 patients with 332 WSIs from Seoul National University hospital. IHCCs can be further categorized into small duct type (SDT) and large duct type (LDT). Using gene mutation information as prior knowledge, we collected WSIs with wild KRAS and mutated IDH genes for use as training samples in SDT, and WSIs with mutated KRAS and wild IDH genes for use in LDT. The rest of the WSIs were used as testing samples. The liver cancer dataset for the latter task was composed of 323 WSIs, in which the WSIs can be further classified into hepatocellular carcinomas (HCCs) (collected from Pathology AI Platform "
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.2,Implementation Detail,"We used a pre-trained VGG16 with ImageNet as the initial encoder, which was further modified via deep manifold model training using the proposed manifold and cross-entropy loss functions. The number of nearest neighbors k and the number of sub-classes n were set to 5 and 10, respectively. In the deep manifold embedding learning model, the learning rates were set to 1e-4 with a decay rate of 1e-6 for the IHCCs subtype classification and to 1e-5 with a decay rate of 1e-8 for the liver cancer type classification. The k-nearest neighbors graph and the geodesic distance matrix are updated once every five training epochs, which is empirically chosen to balance running time and accuracy. To train the MIL classifier, we set the learning rate to 1e-3 and the decay rate to 1e-6. We used batch sizes 64 and 4 for training the deep manifold embedding learning model and the MIL classification model, respectively. The number of epochs for the deep manifold embedding learning model was 50, while 50 and 200 epochs for the IHCCs subtype classification and liver cancer type classification, respectively. As for the optimizer, we used stochastic gradient decay for both stages. The result shown in the tables is the average result from 10 iterations of the MIL classification model."
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.3,Experimental Results,"The performance of different models from two different datasets is reported in this section. For the baseline model, we chose the pre-trained VGG16 feature extractor with an MIL classifier, which is the same as our proposed model except that the encoder is retrained using the proposed loss. Two SOTA methods using contrastive learning and clustering, PCL  Since one of our contributions is the use of geodesic distance, we assessed the efficacy of the method by comparing it with the performance using cosine distance, as shown in Table "
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.4,Conclusion and Future Work,"In this paper, we proposed a novel geodesic-distance-based contrastive learning for histopathology image classification. Unlike conventional cosine-distancebased contrastive learning methods, our method can represent nonlinear feature manifold better and generate better discriminative features. One limitation of the proposed method is the extra computation time for graph generation and pairwise distance computation using the Dijkstra algorithm. In the future, we plan to optimize the algorithm and apply our method to other datasets and tasks, such as multi-class classification problems and natural image datasets."
Histopathology Image Classification Using Deep Manifold Contrastive Learning,,Fig. 1 .,
Histopathology Image Classification Using Deep Manifold Contrastive Learning,,Fig. 2 .,
Histopathology Image Classification Using Deep Manifold Contrastive Learning,,,
Histopathology Image Classification Using Deep Manifold Contrastive Learning,,Table 1 .,
Histopathology Image Classification Using Deep Manifold Contrastive Learning,,Table 2 .,
Histopathology Image Classification Using Deep Manifold Contrastive Learning,,Table 3 .,
Histopathology Image Classification Using Deep Manifold Contrastive Learning,,Acknowledgements,. This study was approved by the institutional review board of 
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,1,Introduction,"Prostate cancer (PCa) diagnosis and grading rely on histopathology analysis of biopsy slides  Field effect refers to the spread of genetic and epigenetic alterations from a primary tumor site to surrounding normal tissues, leading to the formation of secondary tumors. Understanding field effect is essential for cancer research as it provides insights into the mechanisms underlying tumor development and progression. Tumor-associated stroma, which consists of various cell types, such as fibroblasts, smooth muscle cells, and nerve cells, is an integral component of the tumor microenvironment that plays a critical role in tumor development and progression. Reactive stroma, a distinct phenotype of stromal cells, arises in response to signaling pathways from cancerous cells and is characterized by altered stromal cells and increased extracellular matrix components  Manual review for tumor-associated stroma is time-consuming and lacks quantitative metrics  Analyzing tumor-associated stroma in prostate cancer requires combining whole-mount and biopsy histopathology slides. Biopsy slides provide information on the presence of PCa, while whole-mount slides provide information on the extent and distribution of PCa, including more information on tumor-associated stroma. Combining the information from both modalities can provide a more accurate understanding of the tumor microenvironment. In this work, we explore the field effect in prostate cancer by analyzing tumor-associated stroma in multimodal histopathological images. Our main contributions can be summarized as follows: -To the best of our knowledge, we present the first deep-learning approach to characterize prostate tumor-associated stroma by integrating histological image analysis from both whole-mount and biopsy slides. Our research offers a promising computational framework for in-depth exploration of the field effect and cancer progression in prostate cancer. -We proposed a novel approach for stroma classification with spatial graphs modeling, which enable more accurate and efficient analysis of tumor microenvironment in prostate cancer pathology. Given the spatial nature of cancer field effect and tumor microenvironment, our graph-based method offers valuable insights into stroma region analysis. -We developed a comprehensive pipeline for constructing tumor-associated stroma datasets across multiple data sources, and employed adversarial training and neighborhood consistency regularization techniques to learn robust multimodal-invariant image representations."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2,Method,
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.1,Stroma Tissue Segmentation,"Accurately analyzing tumor-associated stroma requires a critical pre-processing step of segmenting stromal tissue from the background, including epithelial tissue. This segmentation task is challenging due to the complex and heterogeneous appearance of the stroma. To address this, we propose utilizing the PointRend model "
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.2,Stroma Classification with Spatial Patch Graphs,"To capture the spatial nature of field effect and analyze tumor-associated stroma, modeling spatial relationships between stroma patches is essential. The spatial relationship can reveal valuable information about the tumor microenvironment, and neighboring stroma cells can undergo similar phenotypic changes in response to cancer. Therefore, we propose using a spatial patch graph to capture the highorder relationship among stroma tissue regions. We construct the stroma patch graph using a K-nearest neighbor (KNN) graph and neighbor sampling. The KNN graph connects each stroma patch to its K nearest neighboring patches. Given a central stroma patch, we iteratively add neighboring patches to construct  the patch graph until we reach a specified layer number L to control the subgraph size. This process results in a tree-like subgraph with each layer representing a different level of spatial proximity to the central patch. The use of neighbor sampling enables efficient processing of large images and allows for stochastic training of the model. To predict tumor-associated binary labels of stroma patches, we employ a message-passing approach that propagates patch features in the spatial graph. To achieve this, we use Graph Convolutional Networks with attention, also known as Graph Attention Networks (GATs)  where W ∈ R M ×N is a learnable matrix transforming N -dimensional features to M -dimensional features. N E vi is the neighborhood of the node v i connected by E in G. GAT uses attention mechanism to construct the weighting coefficients as: where T represents transposition, is the concatenation operation, and ρ is LeakyReLU function. The final output of GAT module is the tumor-associated probability of the input patch. And the module was optimized using the crossentropy loss L GAT in an end-to-end fashion."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.3,Neighbor Consistency Regularization for Noisy Labels,"The labeling of tumor-associated stroma can be affected by various factors, which can result in noisy labels. One of the reasons for noisy labels is the irregular distribution of the field effect, which makes it challenging to define a clear boundary between the tumor-associated and normal stroma regions. Additionally, the presence of tumor heterogeneity and the varied distribution of tumor foci can further complicate the labeling process. To address this issue, we propose applying Neighbor Consistency Regularization (NCR)  where D KL is the KL-divergence loss to quantify the discrepancy between two probability distributions, T represents the temperature and NN k (v i ) is the set of k nearest neighbors of v i in the feature space."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.4,Adversarial Multi-modal Learning,"Biopsy and whole-mount slides provide complementary multi-modal information on the tumor microenvironment, and combining them can provide a more comprehensive understanding of tumor-associated stroma. However, using data from multiple modalities can introduce systematic shifts, which can impact the performance of a deep learning model. Specifically, whole-mount slides typically contain larger tissue sections and are processed using different protocols than biopsy slides, which can result in differences in image quality, brightness, and contrast. These technical differences can affect the pixel intensity distributions of the images, leading to systematic shifts in the features that the deep learning model learns to associate with tumor-associated stroma. For instance, a model trained on whole-mount slides only may not generalize well to biopsy slides due to systematic shifts, hindering model performance in the clinical application scenario. To address the above issues, we propose an Adversarial Multi-modal Learning (AML) module to force the feature extractor to produce multimodal-invariant representations on multiple source images. Specifically, we incorporate a source discriminator adversarial neural network as auxiliary classifier. The module takes the stroma embedding as an input and predicts the source of the image (biopsy or whole-mount) using Multilayer Perceptron (MLP) with cross-entropy loss function L AML . The overall loss function of the entire model is computed as: where hyper-parameters α and β control the impact of each loss term. All modules were concurrently optimized in an end-to-end manner. The stroma classifier and source discriminator are trained simultaneously, aiming to effectively classify tumor-associated stroma while impeding accurate source prediction by the discriminator. The optimization process aims to achieve a balance between these two goals, resulting in an embedding space that encodes as much information as possible about tumor-associated stroma identification while not encoding any information on the data source. By adopting the adversarial learning strategy, our model can maintain the correlated information and shared characteristics between two modalities, which will enhance the model's generalization and robustness."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,3,Experiment,
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,3.1,Dataset,"In our study, we utilized three datasets for tumor-associated stroma analysis. (1) Dataset A comprises 513 tiles extracted from the whole mount slides of 40 patients, sourced from the archives of the Pathology Department at Cedars-Sinai Medical Center (IRB# Pro00029960). It combines two sets of tiles: 224 images from 20 patients featuring stroma, normal glands, low-grade and highgrade cancer  (2) Dataset B included 97 whole mount slides with an average size of over 174,000×142,000 pixels at 40x magnification. The prostate tissue within these slides had an average tumor area proportion of 9%, with an average tumor area of 77 square mm. An expert pathologist annotated the tumor region boundaries at the region-level, providing exhaustive annotations for all tumor foci. (3) Dataset C comprised 6134 negative biopsy slides obtained from 262 patients' biopsy procedures, where all samples were diagnosed as negative. These slides are presumed to contain predominantly normal stroma tissues without phenotypic alterations in response to cancer. Dataset A was utilized for training the stroma segmentation model. Extensive data augmentation techniques, such as image scaling and staining perturbation, were employed during the training process. The model achieved an average test Dice score of 95.57 ± 0.29 through 5-fold cross-validation. This model was then applied to generate stroma masks for all slides in Datasets B and C. To precisely isolate stroma tissues and avoid data bleeding from epithelial tissues, we only extracted patches where over 99.5% of the regions were identified as stroma at 40X magnification to construct the stroma classification dataset. For positive tumor-associated stroma patches, we sampled patches near tumor glands within annotated tumor region boundaries, as we presumed that tumor regions represent zones in which the greatest amount of damage has progressed. For negative stroma patches, we calculated the tumor distance for each patch by measuring the Euclidean distance from the patch center to the nearest edge of the labeled tumor regions. Negative stroma patches were then sampled from whole mount slides with a Gleason Group smaller than 3 and a tumor distance larger than 5 mm. This approach aims to minimize the risk of mislabeling tumor-associated stroma as normal tissue. Setting a 5mm threshold accounts for the typically minimal inflammatory responses induced by prostate cancers, particularly in lower-grade cases. To incorporate multi-modal information, we randomly sampled negative stroma patches from all biopsy slides in Dataset C. Overall, we selected over 1.1 million stroma patches of size 256×256 pixels at 40x magnification for experiments. During model training and testing, we performed stain normalization and standard image augmentation methods."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,3.2,Model Training and Evaluation,"For constructing KNN-based patch graphs, we limited the graph size by setting K = 4 and layer number L = 3. We controlled the strength of the NCR and AML terms by setting α = 0.25 and β = 0.5, respectively. The Adam optimizer with a learning rate of 0.0005 was used for model training. All models were implemented using PyTorch on a single Tesla V100 GPU. To evaluate the model performance, we perform 5-fold cross-validation, where all slides are stratified by source origin and divided into 5 subsets. In each cross-validation trial, one subset was taken as the test set while the remaining subsets constituted the training set. We measure the prediction performance using the area under the receiver operating characteristic (AUROC), F1 score, precision, and recall. To evaluate the effectiveness of our proposed method, we conducted an ablation study by comparing the performance of different model variants presented in Table "
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,4,Results and Discussions,"In conclusion, our study introduced a deep learning approach to accurately characterize the tumor-associated stroma in multi-modal prostate histopathology slides. Our experimental results demonstrate the feasibility of using deep learning algorithms to identify and quantify subtle stromal alterations, offering a promising tool for discovering new diagnostic and prognostic biomarkers of prostate cancer. Through exploring field effect in prostate cancer, our work provides a computational system for further analysis of tumor development and progression. Future research can focus on validating our approach on larger and more diverse datasets and expanding the method to a patient-level prediction system, ultimately improving prostate cancer diagnosis and treatment."
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,,Fig. 1 .,
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,,Fig. 2 .,
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,,Table 1 .,
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,,53 ± 0.38 79.26 ± 0.36 78.45 ± 0.49 80.10 ± 0.32,
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,1,Introduction,"The ability to predict the future risk of patients with cancer can significantly assist clinical management decisions, such as treatment and monitoring  Over the years, deep learning has greatly promoted the development of computational pathology, including WSI analysis  In summary, to better capture the prognosis-related information in WSI, two technical key points should be fully investigated: "
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2,Methodology,"Figure  However, conventional patch-level analysis cannot model complex pathological patterns (e.g., tumor lymphocyte infiltration, immune cell composition, etc.), resulting in limited cancer survival prediction performance. To this end, we proposed a novel learning network, i.e., HGT, which utilized the spatial and semantic priors mined by a multi-scope analysis strategy (i.e., in-slide superpixel and cross-slide clustering) to represent and capture the contextual interaction of pathological components. Our framework consists two modules: a hierarchical graph convolutional network and a Transformer-based prediction head."
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2.1,Hierarchical Graph Convolutional Network,"Unlike cancer diagnosis and subtyping, cancer survival prediction is a quite more challenging task, as it is a future event prediction task which needs to consider complex pathological structures  Patch Graph Convolutional Layer. Based on the spatial topology extracted by in-slide superpixel, the patch graph convolutional layer (Patch GCL) is designed to learn the feature of the fine-grained microenvironment (e.g., cell) through the message passing between adjacent patches, which can be represented as: where σ(•) denotes the activation function, such as ReLU. GraphConv denotes the graph convolutional operation, e.g., GCNConv  Tissue Graph Convolutional Layer. Third, based on the spatial assignment matrix A spa , the learned patch-level features can be aggregated to the tissue-level features which contain the information of necrosis, epithelium, etc. where [•] T denote the matrix transpose operation. The tissue graph convolutional layer (Tissue GCL) is further designed to learn the feature of this coarse-grained microenvironment, which can be represented as:"
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2.2,Transformer-Based Prediction Head,"Clinical studies have shown that cancer survival prediction requires considering not only the biological morphology but also the contextual interactions of tumor and surrounding tissues  Cross-Slide Clustering. As shown in Fig.  Transformer Architecture. Under the guidance of the semantic prior identified by cross-slide clustering, the learned tissue features V tissue can be further aggregated, forming a series meaningful component embeddings P specific to the cancer type. Then we employed a Transformer  where P out is the output of Transformer, MHSA is the Multi-Headed Self-Attention "
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,,Loss Function and Training Strategy.,"For the network training, Cox loss  where δ i denote the censorship of i-th patient, O(i) and O(j) denote the survival output of i-th and j-th patient in a batch, respectively. GPUs. Our graph convolutional model is implemented by Pytorch Geometric "
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,3,Experiments,"The initial number of superpixels of SLIC algorithm is set to {600, 700, 600}, and the number of clusters of k-means algorithm is set to {16, 16, 16} for CRC, TCGA-LIHC and TCGA-KICA cohorts. The non linearity of GCN is ReLU. The number of Transformer heads is 8, and the attention scores of all heads are averaged to produce the heatmap of contextual interactions. HGT is trained with a mini-batch size of 16, and a learning rate of 1e -5 with Adam optimizer for 30 epochs. Evaluation Metric. The concordance index (CI) "
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,3.2,Comparative Results,"We compared seven state-of-the-art methods (SOTAs), i.e., DeepSets "
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,3.3,Interpretability of the Proposed Framework,"We selected the CRC dataset for further interpretable analysis, as it is one of the leading causes of mortality in industrialized countries, and its prognosis-related factors have been widely studied "
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,4,Conclusion,"In this paper, we propose a novel learning framework, i.e., multi-scope analysis driven HGT, to effectively represent and capture the contextual interaction of pathological components for improving the effectiveness and interpretability of WSI-based cancer survival prediction. Experimental results on three clinical cancer cohorts demonstrated our model achieves better performance and richer interpretability over the existing models. In the future, we will evaluate our framework on more tasks and further statistically analyze the interpretability of our model to find more pathological biomarkers related to cancer prognosis."
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,,Fig. 1 .,
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,,Fig. 2 .Fig. 3 .,
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,,Table 1 .,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,1,Introduction,"According to a systematic research study  Recently, deep learning-based methods have achieved great success in the task of implant position estimation. Kurt et al.  In recent years, great success has been witnessed in Vision-Language Pretraining (VLP). For example, Radford  Motivated by the above observation in dental implant and the property of CLIP, in this paper, we integrate a text condition from the CLIP to assist the implant position regression. According to the natural distribution, we divide teeth regions into three categories in Fig.  Main contributions of this paper can be summarized as follows: 1) To the best of our knowledge, the proposed TCEIP is the first text condition embedded implant position regression network that integrates a text embedding of CLIP to guide the prediction of implant position. (2) A cross-modal interaction that consists of a cross-modal attention (CMA) and knowledge alignment module (KAM) is devised to facilitate the interaction between features that representing image and text. ( "
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,2,Method,"Given a tooth crown image with single or multiple implant regions, the proposed TCEIP aims to give a precise implant location conditioned by text indications from the dentist, i.e., a description of position like 'left', 'right', or 'middle'. An overview of TCEIP is presented in Fig. "
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,2.1,Encoder and Decoder,We employ the widely used ResNet 
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,2.2,Conditional Text Embedding,"To integrate the text condition provided by a dentist, we utilize the CLIP to extract the text embedding. Specifically, additional input of text, e.g., 'left', 'middle', or 'right', is processed by the CLIP Text Encoder to obtain a conditional text embedding v t ∈ R 1×D . As shown in Fig.  where f (•) repeats text embedding v t from R 1×D to R 1×HW and g(•) then reshapes it to R H×W ×1 . This operation ensures better interaction between image and text in the same feature space. "
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,2.3,Cross-Modal Interaction,"High-resolution features from the aforementioned decoder can be directly used to regress the implant position. However, it cannot work well in situations of multiple teeth loss or sparse teeth disturbance. In addition, although we have extracted the conditional text embedding from the CLIP to assist the network regression, there exists a big difference with the feature of encoder-decoder in the feature space. To tackle these issues, we propose cross-modal interaction, including i) Cross-Modal Attention and ii) Knowledge Alignment module, to integrate the text condition provided by the dentist.  C+1) . The CMA module can be formulated as follows:"
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Cross-Modal Attention,"where four independent 1×1 convolutions α, β, θ, and γ are used to map image and fusion features to the space for cross-modal attention. At first, M and F are passed into θ(•), β(•) and α(•) for channel transformation, respectively. Following the transformed feature, M θ and F β perform multiplication via a Softmax activation function to take a cross-attention with F α . In the end, the output feature of the cross-attention via γ(•) for feature smoothing is added with F. Given the above operations, the cross-modal features O et and O dt are obtained and passed to the next layer. Knowledge Alignment Module. The above operations only consider the interaction between features in two modalities. A problem is that text embeddings from pre-trained text encoder of CLIP are not well aligned with the image features initialized by ImageNet pre-training. This knowledge shift potentially weakens the proposed cross-modal interaction to assist the prediction of implant position. To mitigate this problem, we propose the knowledge alignment module (KAM) to gradually align image features to the feature space of pre-trained CLIP. Motivated by knowledge distillation  where m e 4 ∈ R 1×D is the transformed feature of M e 4 after attention pooling operation "
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,2.4,Heatmap Regression Network,"The heatmap regression network is used for locating the implant position, which consists of the heatmap and the offset head. The output of the heatmap head is the center localization of implant position, which is formed as a heatmap G ∈ [0, 1] H×W . Following  where σ is an object size-adaptive standard deviation. The predicted heatmap is optimized by the focal loss  where λ and ϕ are the hyper-parameters of the focal loss, Ĝ is the predicted heatmap and N is the number of implant annotation in image. The offset head computes the discretization error caused by the downsampling operation, which is used to further refine the predicted location. The local offset loss L o is optimized by the L1 loss. The overall training loss of network is:"
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,2.5,Coordinate Projection,"The output of TCEIP is the coordinate of implant at the tooth crown. To obtain the real implant location at the tooth root, we fit a centerline of implant using the predicted implant position of TCEIP and then extend the centerline to the root area, which is identical as "
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,3.1,Dataset and Implementation Details,The dental implant dataset was collected by 
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,3.2,Performance Analysis,We use the same evaluation criteria in 
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Comparison to the Mainstream Detectors.,"To demonstrate the superior performance of the proposed TCEIP, we compare the AP value with the mainstream detectors in Table "
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,4,Conclusions,"In this paper, we introduce TCEIP, a text condition embedded implant position regression network, which integrate additional condition from the CLIP to guide the prediction of implant position. A cross-modal attention (CMA) and knowledge alignment module (KAM) is devised to facilitate the interaction between features in two modalities. Extensive experiments on a dental implant dataset through five-fold cross-validation demonstrated that the proposed TCEIP achieves superior performance than the existing methods."
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Fig. 1 .,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Fig. 2 .,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Fig. 3 .,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Fig. 4 .,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Table 1 .,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Table 2 .,
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Acknowledgments,. This work was supported by the 
TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 31.
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,1,Introduction,"Ultrasound is a widely-used imaging modality for clinical cancer screening. Deep Learning has recently emerged as a promising approach for ultrasound lesion detection. While previous works focused on lesion detection in still images  Previous general-purpose detectors  To address this issue, we propose a novel UltraDet model to leverage NTC. For each Region of Interest (RoI) R proposed by a basic detector, we extract temporal contexts from previous frames. To compensate for inter-frame motion, we generate deformed grids by applying inverse optical flow to the original regular RoI grids, illustrated in Fig.  Our contributions are four-fold. "
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,2,Related Works,"Real-Time Video Object Detection is typically achieved by single-frame detectors, often with temporal information aggregation modules. One-stage detectors  Ultrasound Lesion Detection  Optical Flow "
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,3,Method,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,3.1,Basic Real-Time Detector,"The basic real-time detector comprises three main components: a lightweight backbone (e.g. ResNet34  where To aggregate temporal information, proposals from all T + 1 frames are fed into the Temporal Relation head and updated with inter-lesion information extracted via a relation operation  where l = 1, • • • , L represent layer indices, B and Q are the concatenation of all B τ and Q τ , and Q 0 = Q. We call this basic real-time detector BasicDet. The BasicDet is conceptually similar to RDN "
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,3.2,Negative Temporal Context Aggregation,"In this section, we present the Negative Temporal Context Aggregation (NTCA) module. We sample T ctxt context frames from T previous frames, then extract temporal contexts (TC) from context frames and aggregate them into proposals. We illustrate the NTCA module in Fig.  to transform the RoIs from frame t to τ : O t→τ = FlowNet(I t , I τ ) where H, W represent height and width of feature maps. The FlowNet(I t , I τ ) is a fixed network  where IOFAlign(F τ , B t , O t→τ ) extracts context features in F τ from deformed grids generated by applying offsets O t→τ to the original regular grids in B t , which is illustrated in the Fig.  Temporal Aggregation. We concatenate C t,τ in all T ctxt context frames to form C t and enhance proposal features by fusing C t into Q t : where "
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,3.3,UltraDet for Real-Time Lesion Detection,"We integrate the NTCA module into the BasicDet introduced in Sect. 3.1 to form the UltraDet model, which is illustrated in Fig.  During training, we apply regression and classification losses L = L reg + L cls to the current frame. To improve training efficiency, we apply auxiliary losses L aux = L to all previous T frames. During inference, the UltraDet model uses the current frame and T previous frames as inputs and generates predictions only for the current frame. This design endows the UltraDet with the ability to perform real-time lesion detection."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,4,Experiments,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,4.1,Dateset,"CVA-BUS Dateset. We use the open source CVA-BUS dataset that consists of 186 valid videos, which is proposed in CVA-Net  High-Quality Labels. The bounding box labels provided in the original CVA-BUS dataset are unsteady and sometimes inaccurate, leading to jiggling and inaccurate model predictions. We provide a new version of high-quality labels that are re-annotated by experienced radiologists. We reproduce all baselines using our high-quality labels to ensure a fair comparison. Visual comparisons of two versions of labels are available in supplementary materials. To facilitate future research, we will release these high-quality labels. "
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,4.2,Evaluation Metrics,"Pr80, Pr90. In clinical applications, it is important for detection models to be sensitive. So we provide frame-level precision values with high recall rates of 0.80 and 0.90, which we denote as Pr80 and Pr90, respectively. FP80, FP90. We further report lesion-level FP rates as critical metrics. Framelevel FPs are linked by IoU scores to form FP sequences  R@16. To evaluate the highest achievable sensitivity, we report the frame-level average recall rates of Top-16 proposals, denoted as R@16."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,4.3,Implementation Details,UltraDet Settings. We use FlowNetS  Shared Settings. All models are built in PyTorch framework and trained using eight NVIDIA GeForce RTX 3090 GPUs. We use ResNet34 
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,4.4,Main Results,"Quantitative Results. We compare performances of real-time detectors with the UltraDet in Table  Especially, the Pr90 of UltraDet achieves 90.8%, representing a 5.4% absolute improvement over the best competitor, PTSEFormer  Inference Speed. We run inference using one NVIDIA GeForce RTX 3090 GPU and report the inference speed in Table  Qualitative Results.  Effectiveness of Each Sub-module. We ablate the effectiveness of each submodule of the NTCA module in Table "
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,4.5,Ablation Study,"We find RoI-level aggregation is more effective than feature-level, and bothlevel aggregation provides no performance gains. This conclusion agrees with radiologists' skills to focus more on local regions instead of global information."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,5,Conclusion,"In this paper, we address the clinical challenge of real-time ultrasound lesion detection. We propose a novel Negative Temporal Context Aggregation (NTCA) module, imitating radiologists' diagnosis processes to suppress FPs. The NTCA module leverages negative temporal contexts that are essential for FP suppression but ignored in previous works, thereby being more effective in suppressing FPs. We plug the NTCA module into a BasicDet to form the UltraDet model, which significantly improves the precision and FP rates over previous state-ofthe-arts while achieving real-time inference speed. The UltraDet has the potential to become a real-time lesion detection application and assist radiologists in more accurate cancer diagnosis in clinical practice."
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Fig. 1 .,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Fig. 2 .,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Fig. 3 .,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Fig. 4 .,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Figure 4 (,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Table 1 .,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Table 2 .,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Table 3 .,
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_1.
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,1,Introduction,"Whole slide scanning is increasingly used in disease diagnosis and pathological research to visualize tissue samples. Compared to traditional microscopebased observation, whole slide scanning converts glass slides into gigapixel digital images that can be conveniently stored and analyzed. However, the high resolution of WSIs also makes their automated classification challenging  In contrast, MIL-based methods have become increasingly preferred due to their only demand for slide-level labels  In summary, our contributions are: "
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,2,Methodology,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,2.1,Iterative Coupling of Embedder and Bag Classifier in ICMIL,"The general idea of ICMIL is shown in Fig.  Next, we freeze the weights of f (•) and fine-tune g(•) with the generated pseudo-labels (step 2 in Fig. "
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,2.2,Instance Aggregation Method in ICMIL,"Although most instance aggregators are compatible with ICMIL, they still have an impact on the efficiency and effectiveness of ICMIL. In addition to that a(•) has to project the bag representations to the same hidden space as the instance representations, it also should avoid being over-complicated. Otherwise, a(•) may lead to larger difference between the decision boundaries of bag-level classifer f (•) and instance-level classifier f (•), which may cause ICMIL taking more time to converge. Therefore, in our experiments, we choose to use the attention-based instance aggregation method  where a k is the attention score for the k-th instance h k in the bag. Obviously, H and h k remains in the same hidden space, satisfying the prerequisite of ICMIL."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,2.3,Label Propagation from Bag Classifier to Embedder,"We propose a novel teacher-student model for accurate and robust label propagation from f (•) to g(•). The model's architecture is depicted in Fig.  where f (•) and f (•) are teacher classifer and student classifier respectively, f (•) c indicates the c-th channel of f (•), and C is the total number of channels. Additionally, during training, a learnable instance-level classifier is used on the student to back-propagate the gradients to g (•). The initial weights of f (•) are the same as those of f (•), as the differences in the instance-and bag-level classification boundaries is expected to be minor. To make f (•) not so different from f (•) during training, a weight similarity loss, L w , is further imposed to constrain it by drawing closer their each layer's outputs under the same input. By applying L w , the patch embeddings from g (•) can still suit the bag-level classification task well, rather than being tailored solely for the instance-level classifier f (•). L w is defined as: where f (•) l c indicates the c-th channel of l-th layer's output in f (•). The overall loss function for this step is L c + αL w , with α set to 0.5 in our experiments."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,3,Experiments,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,3.1,Datasets,"Our experiments utilized two datasets, with the first being the publicly available breast cancer dataset, Camelyon16  The second dataset is a private hepatocellular carcinoma (HCC) dataset collected from Sir Run Run Shaw Hospital, Hangzhou, China. This dataset comprises a total of 1140 valid tumor WSIs scanned at 40× magnification, and the objective is to identify the severity of each case based on the Edmondson-Steiner (ES) grading. The ground truth labels are binary classes of low risk and high risk, which were provided by experienced pathologists."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,3.2,Implementation Details,"For Camelyon16, we tiled the WSIs into 256×256 patches on 20× magnification using the official code of "
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Ours,"(w/ AB-MIL) 90.0 firstly embedded into a 1024-dimension vector, and then be projected to a 512dimension hidden space for further bag-level training. For the training of bag classifier f (•), we used an initial learning rate of 2e-4 with Adam "
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,3.3,Experimental Results,"Ablation Study. The results of ablation studies are presented in Table  Comparison with Other Methods. Experimental results are presented in Table  The results indicate that one iteration of g(•) fine-tuning in ICMIL significantly improves the instance-level representations, leading to a better aggregated baglevel representation naturally. Besides, the bag-level representations are also more closely aligned with the instance representations, proving that ICMIL can reduce the inconsistencies between g(•) and f (•) by coupling them together for training, resulting in a better separability."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,4,Conclusion,"In this work, we propose ICMIL, a novel framework that iteratively couples the feature extraction and bag classification stages to improve the accuracy of MIL models. ICMIL leverages the category knowledge in the bag classifier as pseudo supervision for embedder fine-tuning, bridging the loss propagation from classifier to embedder. We also design a two-stream model to efficiently facilitate such knowledge transfer in ICMIL. The fine-tuned patch embedder can provide more accurate instance embeddings, in return benefiting the bag classifier. The experimental results show that our method brings consistent improvement to existing MIL backbones."
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Fig. 1 .,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Fig. 2 .,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Fig. 3 .,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Fig. 4 .,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Fig. 5 .,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Table 1 .,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Table 2 .,
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Acknowledgements,. This work was supported by the 
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 45.
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,1,Introduction,"Deep learning is becoming increasingly popular in modern orthodontic treatments for tooth segmentation in intraoral scans (IOS), cone-beam CT (CBCT) and panoramic X-ray  There are two main categories for tooth segmentation in IOS: conventional methods that handle 2D image projections  Inspired by the success of transformers in various tasks  We collect a large-scale, high-resolution and heterogeneous 3D IOS dataset with 16,000 dental models where each contains over 100,000 triangular faces. To the best of our knowledge, it is the largest IOS dataset to date. Experimental results show that TSegFormer has reached 97.97% accuracy, 94.34% mean intersection over union (mIoU) and 96.01% dice similarity coefficient (DSC) on the large-scale dataset, outperforming previous works by a significant margin. To summarize, our main contributions are: -We design a novel framework for 3D tooth segmentation with a tailed 3D transformer and a multi-task learning paradigm, aiming at distinguishing the permanent teeth with divergent anatomical structures and noisy boundaries. -We design a geometry guided loss based on a novel point curvature for endto-end boundary refinement, getting rid of the two-stage and time-consuming post-processing for boundary smoothing. -We collect the largest ever 3D IOS dataset for compelling evaluation. Extensive experiments, ablation analysis and clinical applicability test demonstrate the superiority of our method, which is appealing in real-world applications. "
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,2.1,Overview,The overall pipeline is illustrated in Fig. 
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,2.2,TSegFormer Network Architecture,"Feature Extraction. We first transform input meshes to point clouds as directly handling meshes with deep nets is computationally expensive, especially for high-resolution IOSs. To compensate for potential topology loss, we extract 8-dimensional feature vectors h in ∈ R N ×8 /R N ×8 for each point to preserve sufficient geometric information, including the point's 3D Cartesian coordinates, 3-dimensional normal vector of mesh face, the Gaussian curvature and a novel point ""curvature"" m i . The m i is defined as , where n i is the i-th point's normal vector, K(i) is the second-order neighborhood of the i-th point, |K(i)| is the number of points in K(i), and θ(•, •) denotes the angle in radians between two vectors. By definition, the curvature of a point reflects how much the local geometric structure around this point is curved, i.e., the local geometry on 3D tooth point clouds. Backbone Network. Delineating complicated tooth-tooth or tooth-gingiva boundaries requires decent knowledge of local geometry in IOS. Hence, we first learn local dependencies from the input h in . In particular, we design a point embedding module composed of two linear layers and two EdgeConv layers  Meantime, in view of the inherently sophisticated and inconsistent shapes and structures of the teeth, and the ability of attention mechanism to capture long-range dependencies and suitability for handling unordered point cloud data  Geometry Guided Loss. Previous methods are usually unsatisfactory to delineate the complicated tooth-tooth boundaries. Observing that points with high point curvatures often lie on the upper sharp ends of tooth crowns and the teeth boundaries (Fig.  where γ is the modulating factor (empirically set to 2 in experiments); y S(r)i ∈ R 33 represents the gold label of the i-th point in the point set S(r); pgeo ic denotes the predicted probability of the i-th point belonging to the c-th class, and Φ(y i , c) is an indicator function which outputs 1 if y i = c and 0 otherwise. Concretely, S(r) is a set of points whose point curvatures m i are among the top r • 100% (0 < r ≤ 1) of all N points, i.e., S(r The experimental results (Fig.  We employ the cross entropy loss as the loss of main segmentation head (L seg ) and the loss of auxiliary segmentation head (L aux ). The total loss L total is computed by combining L seg , L aux for all points and L geo for hard points: L total = L seg +ω geo •L geo +ω aux •L aux . We set the weights ω geo = 0.001, ω aux = 1 and the ratio r = 0.4, and detailed hyperparameter search results in SM Fig. "
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,3,Experiments,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,3.1,Dataset and Experimental Setup,"We construct a large-scale 3D IOS dataset consisting of 16,000 IOS meshes with full arches (each with 100,000 to 350,000 triangular faces) collected between 2018-2021 in China, with evenly distributed maxillary and mandible scans labeled by human experts. Detailed data statistics are presented in SM Table "
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,3.2,Main Results on Tooth Segmentation,"To our best knowledge, there has been no prior work on Transformer-based segmentation on non-Euclidean 3D tooth point clouds/meshes. Hence, we compare our TSegFormer to seven representative and state-of-the-art baselines from three categories: 1) neural networks for point clouds, including PointNet++  It is important to integrate advanced architectures with domain-specific design for superior performance. We can notice that though MeshSegNet, TSGC-Net, and DCNet are all domain-specific 3D tooth segmentation models, their performance, though on par with PVT and DGCNN, is worse than the point transformer. This is also consistent with the superior performance of transformerbased models on standard point cloud processing tasks, which could be mainly attributed to the larger dataset and powerful attention mechanism that better capture global dependencies. Hence, though models like MeshSegNet adopt some task-specific designs to achieve good performance, they still lag behind point transformer when a huge amount of data samples are available. In contrast, our TSegFormer employs the attention mechanism for point representation learning, and meanwhile, adopted task-specific architectures and geometry guided loss to further boost the performance. More statistical results are in SM Table "
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,3.3,Ablation Studies,Effectiveness of Geometry Guided Loss. Table  Effectiveness of the Auxiliary Segmentation Head. The auxiliary segmentation head is designed to rectify the inaccuracy brought by mislabeling teeth and gingiva near their boundaries. Adding a loss for the auxiliary branch leads to about 0.4% mIoU performance improvement (Table 
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Effectiveness on,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,3.4,Clinically Applicability Test and Visualization,"To show the effectiveness of TSegFormer in real-world scenarios, we conducted a clinical applicability test (Table  By visualization, we show the superiority of TSegFormer on various complicated dental diseases in Fig. "
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,4,Conclusion,"We propose TSegFormer, a 3D transformer-based model for high-resolution IOS segmentation. It combines a point embedding module and attention mechanism to effectively capture local and global features, and introduces a geometry guided loss based on a novel point curvature to handle boundary errors and multi-task segmentation heads for boundary refinement. Results of comprehensive experiments on a large-scale dataset and clinical applicability tests demonstrate TSeg-Former's state-of-the-art performance and its great potential in digital dentistry."
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Fig. 1 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Fig. 2 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Fig. 3 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Table 1 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Table 2 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Table 3 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Table 4 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Table 5 .,
TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_41.
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,1,Introduction,"Histology is critical for accurately diagnosing all cancers in modern medical imaging analysis. However, the complex scanning procedure for histological wholeslide images (WSIs) digitization may result in the alteration of tissue structures, due to improper removal, fixation, tissue processing, embedding, and storage  In real clinical practice, rescanning the WSIs that contain artifacts can partially address this issue. However, it may require multiple attempts before obtaining a satisfactory WSI, which can lead to a waste of time, medical resources, and deplete tissue samples. Discarding the local region with artifacts for deep learning models is another solution, but it may result in the loss of critical contextual information. Therefore, learning-based artifact restoration approaches have gained increasing attention. For example, CycleGAN  The major contributions are two-fold. "
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,2,Methodology,"Overall Pipeline. The proposed histology artifact restoration diffusion model ArtiFusion, comprises two stages, namely the training, and inference. During the training stage, ArtiFusion learns to generate regional histology tissue structures based on the contextual information from artifact-free images. In the inference stage, ArtiFusion formulates the artifact restoration as a gradual denoising process. Specifically, it first replaces the artifact regions with Gaussian noise, and then gradually restores them to artifact-free images using the contextual information from nearby regions. Diffusion Training Stage. The proposed ArtiFusion learns the capability of generating local tissue representation from contextual information during the training stage. To achieve this, we follow the formulations of DDPM  , where β t ∈ (0, 1) are predefined hyper-parameters  This formulation is extended in DDPM  where Artifact Restoration in Inference Stage. During the inference stage, we first use a threshold method to detect the artifact region in the input image x 0 . Then, unlike the conventional diffusion models  where ). Consequently, the final restored image is obtained as Swin-Transformer Denoising Network. To capture the local-global correlation and enable the denoising network to effectively restore the artifact regions, we propose a novel Swin-Transformer-basedr "
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,3,Experiments,"Dataset. To evaluate the performance of artifact restoration, a training set is curated from a subset of Camelyon17  Implementations. We implement the proposed ArtiFusion and its counterpart in Python 3.8.10 and PyTorch 1.10.0. All experiments are carried out in parallel on two NVIDIA RTX A4000 GPU cards with 16 GiB memory. Hyperparameters are as follows: a learning rate of 10 -4 with Adam optimizer, the total timesteps is set to 250. Compared Methods and Evaluation Metrics. As a proof-of-concept attempt at a generative-models-based artifact restoration framework in the histology domain, currently, there are limited available literature works and opensourced codes for comparison. Consequently, we leverage the prevalent Cycle-GAN  Table "
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,4,Conclusion,"In this paper, we propose ArtiFusion, the first attempt at a diffusion-based artifact restoration framework for histology images. With a novel Swin-Transformer denoising backbone, ArtiFusion is able to restore regional artifacts using the context information, while preserving the tissue structures in artifact-free regions as well as the stain style. Experimental results on a public histological dataset demonstrate the superiority of our proposed method over the state-of-the-art GAN counterpart. Consequently, we believe that our proposed method has the potential to benefit the medical community by enabling more accurate diagnosis or treatment planning as a pre-processing method for histology analysis. Future work includes investigating the extension of ArtiFusion to more advanced diffusion models such as score-based or score-SDE models "
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,,Fig. 1 .,
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,,Fig. 2 .,
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,,Fig. 3 .,
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,,,
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,,,
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,,Table 2 .,
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,,Table 3 .,
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,1,Introduction,"Cardiovascular disease (CVD) is the most common cause of mortality among patients with Chronic Kidney disease (CKD), with CVD accounting for 40-50% deaths in patients with acute CKD  Radiomic based interpretable features have been used to model cardiac morphology for predicting disease outcomes "
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,2,Prior Work and Novel Contributions,"The relationship between Chronic Kidney disease (CKD) and Cardiovascular disease (CVD) is well established  Echo coupled with machine learning, has been a potent tool for analysis of CVD in patients with CKD  In this study, for the first time, we employ echo video based spatiotemporal analysis of LVW to prognosticate CVD outcomes in kidney disease patients. Our code is available here -https://github.com/rohand24/STAR Echo Key contributions of our work are as follows: * STAR-Echo, a novel biomarker that combines spatiotemporal radiomics and transformer-based models to capture morphological differences in shape and texture of LVW and their longitudinal evolution over time. * Unique interpretable features: spatiotemporal evolution of sphericity and perimeter (shape-based) and LongRunHighGrayLevelEmphasis (texturebased), prognostic for CVD risk in CKD patients * Demonstrated the superiority of STAR-Echo in the prognosis of CVD in CKD patients, compared to individual spatiotemporal models and clinical biomarkers. * An end-to-end automated pipeline for echo videos that can identify heartbeat cycles, segment the LVW, and predict a prognostic risk score for CVD in CKD patients."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,3,STAR-ECHO: Methodological Description,
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,3.1,Notation,"We denote a patient's echo video as a series of two-dimensional frames, represented as V = [I 1 , I 2 , ..., I N ], where I 1 , I 2 , ..., I N represent the N phases (frames) "
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,3.3,Systolic Phase Extraction,"The heart-beat cycle is captured using the consecutive end-diastolic phases identified by the CNN+LSTM based multi-beat echo phase detection model  The heart-beat cycle frame lengths vary with individual patients. To achieve a standard frame length of 30 frames, the videos are processed with the following strategy -in videos with extra frames, random frame dropping based on normal distribution is performed, while in videos with fewer frames, a video-frame interpolation model "
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,3.4,Automated LVW Segmentation,"Since LVW annotations were only available for the end-diastolic phase in the dataset, a weakly supervised segmentation approach is employed to obtain annotations for all phases of the echo video. Combining the available annotations with the publicly available CAMUS dataset  Masking the input echo video with LVW mask provides the echo video V , a 30-frame sequence of masked LVW image frames I N , to be input to the spatiotemporal models M R and M T (Fig. "
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,3.5,Spatiotemporal Feature Representation,"Radiomic Time-Series Model: To build this model, we employ a two-stage feature extraction process."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,Radiomic Feature Extraction:,"In first stage, radiomic feature R(I N ) is extracted on each I N of the LVW for each phase (frame) of the echo video, V as given in "
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,Time Series Feature Extraction:,"In the second stage, to model the temporal LVW motion, we consider the sequence of radiomic features from each phase of one heartbeat cycle as individual time-series. Thus, a radiomic feature timeseries t R (V ) is given by (2). Time-series feature T R , as given by (3), is extracted for each radiomic feature time-series using the TSFresh library "
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,R(I,"Thus, the radiomic time-series model M R is trained on time-series features T R (V ) obtained for each V of the patient to predict the outcome O T ."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,Video Transformer Model:,The Video Vision Transformer (ViViT) model 
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,3.6,Model Fusion,"The predictions from radiomic time-series model, M R and video transformer model, M T are fused at the junction D j linear discriminant analysis (LDA) based fusion, with the output prediction probability P given by (  where ω k , ω k0 are the learned parameters of the model and input X i for patient i is the linear combination of the predictions of M R and M T given as in "
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,4,Experimental Results and Discussion,
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,4.1,Dataset Description,"The dataset consisted of echo videos from patients with CKD (N = 150) and their composite CVD outcomes and clinical biomarker measurements for Ejection Fraction (EF), B-type natriuretic peptide (BNP) and N-terminal pro-B-type natriuretic peptide (NT-proBNP). It was stratified into 70% training (S t = 101) and 30% holdout set (S v = 44) The reported cardiovascular outcome, O T , is composite of following cardiovascular events -chronic heart failure, myocardial infarction and stroke. The patients were participants of the Chronic Renal Insufficiency cohort (CRIC)  The study followed the patients for CVD outcomes. Additional details along with inclusion-exclusion criteria of the dataset are included in the supplementary materials. In the dataset, a total of 46 patients experienced the composite event outcome. The dataset also included survival time information about the patients. The median survival time for patients, O T was 6.5 years (median O - = 6.7 years, median O + = 3.5 years)."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,4.2,Statistical and Survival Analysis,"Statistical Feature Selection -Extracted times-series features (T R ) are normalized and multicollinear features are excluded. Analysis of Variance (ANOVA) based stable feature selection (p < 0.002) and Brouta  Survival Analysis -KM plots with log-rank test analyzed the prognostic ability of models. The curves showed survival time (in years) on the horizontal axis and probability of survival on the vertical axis, with each point representing survival probability of patients. Hazard ratios and p-values were reported for significance between low-risk and high-risk cohorts."
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,4.3,Evaluating Ability of STAR-ECHO to MACE Prognosis,MACE prognosis is a necessary task in CKD patients management due to high CVD events in CKD patients  The top radiomic features (Fig. 
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,5,Conclusion,"In this study, we introduced STAR-Echo, a novel biomarker combining radiomics and video transformer-based descriptors to evaluate spatiotemporal changes in LVW morphology. STAR-Echo identifies novel features based on longitudinal changes in LVW shape (perimeter and sphericity) and texture (intensity variations) over a heartbeat cycle, similar to recent clinical pathophysiological findings of CVD in CKD "
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,Fig. 1 .,
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,Fig. 2 .,
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,Fig. 3 .,
STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models,,Table 1 .,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,1,Introduction,"Gout is the most common inflammatory arthritis and musculoskeletal ultrasound (MSKUS) scanning is recommended to diagnose gout due to the non-ionizing radiation, fast imaging speed, and non-invasive characteristics of MSKUS  In medical image analysis, recent works have attempted to inject the recorded gaze information of clinicians into deep CNN models for helping the models to predict correctly based on lesion area. Mall et al.  Different from the existing studies, we propose a novel framework to adjust the general CNNs to ""think like sonographers"" from three different levels. "
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,2,Method,Fig.  Figure 
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,2.1,Where to Adjust,"It is essential to obtain the gaze map corresponding to each MSKUS to emphasize the region where gouty features are obvious. Inspired by studies of saliency model  The MSKUS image I 0 ∈ R H×W ×3 is first input into CNN encoder that contains five convolution blocks. The output feature maps from the deeper last three convolution blocks are denoted as F 0 , F 1 , F 2 and are respectively fed into transformer encoders to enhance the long-range and contextual information. During the transformer-encoder, we first flatten the feature maps produced by the CNN encoder into a 1D sequence. Considering that flatten operation leads to losing the spatial information, the absolute position encoding  In the CNN decoder part, a pure CNN architecture progressively up-samples the feature maps into the original image resolution and implements pixel-wise prediction for modeling sonographers' gaze map. The CNN decoder part includes five convolution blocks. In each block, 3 × 3 convolution operation, Batch normalization (BN), RELU activation function, and 2-scale upsampling that adopts nearest-neighbor interpolation is performed. In addition, the transformer's output is fused with the feature map from the decoding process by an element-wise product operation to further enhance the long-range and multi-scale visual information. After five CNN blocks, a 3 × 3 convolution operation and Sigmoid activation is performed to output the predicted sonographers' gaze map. We use the eye gaze information of the sonographers which is collected by the Eye-Tracker to restrain the predicted sonographers' gaze map. The loss function is the sum of the Normalized Scanpath Saliency (NSS), the Linear Correlation Coefficient (CC), Kullback-Leibler divergence (KLD) and Similarity (SIM) "
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,2.2,What to Adjust,"Common CNN classification models for gout diagnosis often fail to learn the gouty MSKUS features including the double contour sign, tophus, and snowstorm which are key factors for sonographers' decision. A CAM for a particular category indicates the discriminative regions used by the CNN to identify that category. Inspired by CAM technique, it is needed to decide whether the attention region given to an CNN model is reasonable for diagnosis of gout. We firstly use the Grad-CAM technique  Our target of adjustment is to reduce imprecise and unreasonable predictions. In this way, CNNs not only finish correct gout diagnosis, but also acquire the attention region that agreements with the sonographers' gaze map."
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,2.3,How to Adjust,"We proposed a training mechanism (Algorithm 1) which can strike the balance between the gout diagnosis error and the reasonability error of attention region to promote the CNNs to ""think like sonographers"". In addition to reducing the diagnosis error, we also want to minimize the difference between sonographers' gaze map S sono and normalized salient attention region S CAM , which directly leads to our target:  The total loss function can be expressed as the weighted sum the gout diagnosis error and the reasonability error, as follows: The gout diagnosis error L diagnosis is calculated by the Cross-entropy loss, and the reasonability is calculated by the L1-loss. This training mechanism uses the quadrant of instances to identify whether samples' attention needs to adjusted. For MSKUS sample in the quadrant of UP, α can be set 0.2 to control the CNN pay more attention to reasonability. Correspondingly, for sample in RIP, α can be set 0.8 to make CNN pay more attention to precise. For sample in RP and UIP, α can be set 0.5 to strike the balance between accuracy and reasonability. Gaze Data Collection. We collected the eye movement data with the Tobii 4C eye-tracker operating at 90 Hz. The MSKUS images were displayed on a 1920 × 1080 27-inch LCD screen. The eye tracker was attached beneath the screen with a magnetic mounting bracket. Sonographers were seated in front of the screen and free to adjust the chair's height and the display's inclination. Binary maps of the same size as the corresponding MSKUS images were generated using the gaze data, with the pixel corresponding to the point of gaze marked with a'1' and the other pixels marked with a'0'. A sonographer gaze map S was generated for each binary map by convolving it with a truncated Gaussian Kernel G(σ x,y ), where G has 299 pixels along x dimension, and 119 pixels along y dimension."
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,3,Experiments,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,MSKUS,"Evaluation Metrics. Five metrics were used to evaluate model performance: Accuracy (ACC), Area Under Curve (AUC), Correlation Coefficient (CC), Similarity (SIM) and Kullback-Leibler divergence (KLD)  Evaluation of ""Thinking like Sonographers"" Mechanism. To evaluate the effectiveness of our proposed mechanism of ""Thinking like Sonographers"" (TLS) that combines ""where to adjust"", ""what to adjust"" and ""how to adjust"", we compared the gout diagnosis results of several classic CNN classification "
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Stability Under Different Gaze Maps via t-Test.,"To evaluate the prediction's stability under the predicted gaze map from the generation model in ""Where to adjust"", we conducted three t-test studies. Specifically, we trained two classification models (M C and M P ), using the actual collected gaze maps, and the predicted maps from the generation model, respectively. During the testing, we used the collected maps as input for M C and M P to get classification results R CC and R P C . Similarly, we used the predicted maps as input for M C andM P As shown in Table "
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,4,Conclusion,"In this study, we propose a framework to adjust CNNs to ""think like sonographers"", and diagnose gout from MSKUS images. The mechanism of ""thinking like sonographers"" contains three levels: where to adjust, what to adjust, and how to adjust. The proposed design not only steers CNN models as we intended, but also helps the CNN classifier focus on the crucial gout features. Extensive experiments show that our framework, combined with the mechanism of ""thinking like sonographers"" improves performance over the baseline deep classification architectures. Additionally, we can bypass the need to collect the real gaze maps of the doctors during the classification of newly acquired MSKUS images, thus our method has good clinical application values."
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Fig. 1 .,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Fig. 3 .,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Algorithm 1 :,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Fig. 4 .,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Table 1 .,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Table 2 .,
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 16.
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,1,Introduction,"As the largest organ in the human body, the skin is an important barrier protecting the internal organs and tissues from harmful external substances, such as sun exposure, pollution, and microorganisms  Among non-invasive skin imaging techniques, dermoscopy is currently widely used in the diagnosis of many skin diseases  Although these approaches have achieved impressive results, most of them neglect the domain knowledge of dermatology and lack interpretability in diagnosis basis and results. In a typical inspection, dermatologists give an initial evaluation with the consideration of both global information, e.g. body part, and local information, e.g. the attributes of skin lesions, and further information including the patient's medical history or additional examination is required to draw a diagnostic conclusion from several possible skin diseases. Recognizing skin diseases from clinical images presents various challenges that can be summarized as follows: (1) Clinical images taken by portable electronic devices (e.g. mobile phones) often have cluttered backgrounds, posing difficulty in accurately locating lesions. (2) Skin diseases exhibit high intra-class variability in lesion appearance, but low inter-class variability, thereby making discrimination challenging. (3) The diagnostic reasoning of dermatologists is empirical and complicated, which makes it hard to simulate and model. To tackle the above issues and leverage the domain knowledge of dermatology, we propose a novel multi-task model, namely DermImitFormer. The model is designed to imitate the diagnostic process of dermatologists (as shown in Fig. "
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,2,Method,"The architecture of the proposed multi-task model DermImitFormer is shown in Fig.  Lesion Selection Module. As introduced above, skin diseases have high variability in lesion appearance and distribution. Thus, it requires the model to concentrate on lesion patches so as to describe the attributes and associated diseases precisely. The multi-head self-attention (MHSA) block in ViT generates global attention, weighing the informativeness of each token. Inspired by  The mutual attention score s n is calculated across all attention heads. Thereafter, we select the top K tokens according to s n for two task heads as l k d and l k a , k ∈ {1, 2, ..., K}. "
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,Cross-Interaction Module.,"A diagnostic process of skin diseases takes multiple visual information into account, which is relatively complicated and difficult to model in an analytical way. Simple fusion operations such as concatenation are insufficient to simulate the diagnostic logic. Thus, partially inspired by  where g B , g D are the class token, p i b , p i d , i ∈ {1, 2, ..., N p } the corresponding patch tokens. GAP and LN denote the global average pooling and layer normalization, respectively. W Q BD , W K BD , W V BD ∈ R F ×F denote learnable parameters. F denotes the dimension of features. g D B is computed from the patch tokens of disease and the class token of body-part in the same fashion. Similarly, we can obtain the fused class tokens (g D A and g A D ) and the fused local class tokens (l D A and l A D ) between attribute and disease. Note that the disease class token g D is replaced by g B D in the later computations, and local class tokens l A and l D in Fig.  Learning and Optimization. We argue that joint training can enhance the feature representation for each task. Thus, we define a multi-task loss as follows:  where N s denotes the number of samples, n x , n h the number of classes for each task, and p ij , y ij the prediction and label, respectively. Notably, body parts and attributes are defined as multi-label classification tasks, optimized with the binary cross-entropy loss, as shown in Eq. 6. The correspondence of x and h is shown in Fig. "
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,3,Experiment,"Datasets. The proposed DermImitFormer is evaluated on three different clinical skin image datasets including an in-house dataset and two public benchmarks. (1) Derm-49: We establish a large-scale clinical image dataset of skin diseases, collected from three cooperative hospitals and a teledermatology platform. The 57,246 images in the dataset were annotated with the diagnostic ground truth of skin disease, body parts, and lesions attributes from the patient records. We clean up the ground truth into 49 skin diseases, 15 body parts, and 27 lesion attributes following the ILDS guidelines  (2) SD-198  Each sample contains a clinical image and a set of metadata with labels such as diseases and body parts. Implementation Details. The DermImitFormer is initialized with the pretrained ViT-B/16 backbone and optimized with SGD method (initial learning rate 0.003, momentum 0.95, and weight decay 10 -5 ) for 100 epochs on 4 NVIDIA Tesla V100 GPUs with a batch size of 96. We define the input size i.e. H = W = 384 that produces a total of 576 spatial tokens i.e. N p = 576 for a ViT-B backbone. K = 24 in the LSM module. For data augmentation, we employed the Cutmix "
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,4,Conclusion,"In this work, DermImitFormer, a multi-task model, has been proposed to better utilize dermatologists' domain knowledge by mimicking their subjective diagnostic procedures. Extensive experiments demonstrate that our approach achieves state-of-the-art recognition performance in two public benchmarks and a largescale in-house dataset, which highlights the potential of our approach to be employed in real clinical environments and showcases the value of leveraging domain knowledge in the development of machine learning models."
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,Fig. 1 .,
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,Fig. 2 .,
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,Fig. 3 .,
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,Fig. 4 .,
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,Table 1 .,
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,,
A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images,,Table 2 .,
