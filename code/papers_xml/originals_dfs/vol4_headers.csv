Paper Title,Header Number,Header Title,Text
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,1,Introduction,"Despite their remarkable success in numerous tasks, deep learning models trained on a source domain face the challenges to generalize to a new target domain, especially for segmentation which requires dense pixel-level prediction. This is attributed to a large semantic gap between these two domains. Unsupervised Domain Adaptation (UDA) has lately been investigated to bridge this semantic gap between labeled source domain, and unlabeled target domain  Compared to UDA, obtaining annotation for a few target samples is worthwhile if it can substantially improve the performance by providing crucial target domain knowledge. Driven by this speculation, and the recent success of semisupervised learning (SemiSL), we investigate semi-supervised domain adaptation (SSDA) as a potential solution. Recently, Liu et al.  Contrastive learning (CL) is another prospective direction where we enforce models to learn discriminative information from (dis)similarity learning in a latent subspace  To alleviate these three underlined shortcomings, we propose a novel contrastive learning with pixel-level consistency constraint via disentangling the style and content information from the joint distribution of source and target domain. Precisely, our contributions are as follows: "
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,2,Proposed Method,"Given the source domain image-label pairs {(x i s , y i s ) Ns i=1 ∈ S}, a few image-label pairs from target domain {(x i t1 , y i t1 ) Nt1 i=1 ∈ T 1}, and a large number of unlabeled target images {(x i t2 ) Nt2 i=1 ∈ T 2}, our proposed pre-training stage learns from images in {S ∪ T ; T = T 1 ∪ T 2} in a self-supervised way, without requiring any labels. The following fine-tuning in SSDA considers image-label pairs in {S ∪T 1} for supervised learning alongside unlabeled images T 2 in the target domain for unsupervised prediction consistency. Our workflow is shown in Fig. "
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,2.1,Gaussian Fourier Domain Adaptation (GFDA),"Manipulating the low-level amplitude spectrum of the frequency domain is the easiest way for style transfer between domains  where indicates element-wise multiplication. It generates an image preserving the semantic content from S but preserving the style from T . Reciprocal pair x t→s is also formulated using the same drill. The source and target images, and the style-transferred versions {x s , x s→t , x t , x t→s } are then used for contrastive pre-training below. Visualization of GFDA is shown in the supplementary file."
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,2.2,CL on Disentangled Domain and Content,"We aim to learn discriminative content-specific features that are invariant of the style of the source or target domain, for a better pre-training of the network for the task at hand. Hence, we propose to disentangle the style and content information from the images and learn them jointly in a novel disentangled CL paradigm: Style CL (SCL) and Content CL (CCL). The proposed SCL imposes learning of domain-specific attributes, whereas CCL enforces the model to identify the ROI, irrespective of the spatial semantics and appearance. In joint learning, they complement each other to render the model to learn domain-agnostic and content-specific information, thereby mitigating the domain dilemma. The set of images {x s , x s→t , x t , x t→s }, along with their augmented versions are passed through encoder E, followed by two parallel projection heads, namely style head (G S ) and content head (G C ) to obtain the corresponding embeddings. Two different losses: style contrastive loss L SCL and content contrastive loss L CCL , are derived below. Assuming {x s , x t→s } (along with their augmentations) having source-style representation (style A), and {x t , x s→t } (and their augmentations) having target-style representation (style B), in style CL, embeddings from the same domain (style) are grouped together whereas embeddings from different domains are pushed apart in the latent space. Considering the i th anchor point x i t ∈ T in a minibatch and its corresponding style embedding s i t ← G S (E(x i t )) (with style B), we define the positive set consisting of the same target domain representations as Λ + = {s j+ t , s j+ s→t } ← G S (E({x j t , x j s→t })), ∀j ∈ minibatch, and negative set having unalike source domain representation as Λ -= {s j- s , s j- t→s } ← G S (E({x j s , x j t→s })), ∀j ∈ minibatch. Following SimCLR  where {s i , s j+ } ∈ style B; s j-∈ style A, sim(•, •) defines cosine similarity, τ is the temperature parameter  where {c i , c j } ← G C (E({x i , x j })). These contrastive losses, along with the consistency constraint below enforce the encoder to extract domain-invariant and content-specific feature embeddings."
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,2.3,Consistency Constraint,"The disentangled CL aims to learn global image-level representation, which is useful for instance discrimination tasks. However, segmentation is attributed to learning dense pixel-level representations. Hence, we propose an additional Dense Feature Propagation Module (DFPM) along with a momentum encoder E with exponential moving average (EMA) of parameters from E. Given any pixel m of an image x, we transform its feature f m E obtained from E by propagating other pixel features from the same image: where K is a linear transformation layer, ⊗ denotes matmul operation. This spatial smoothing of learned representation is useful for structural sensitivity, which is fundamental for dense segmentation tasks. We enforce consistency between this smoothed feature fE from E and the regular feature f E from E as: where d(•, •) indicates the spatial distance, T h is a threshold. The overall pretraining objective can be summarized as:"
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,2.4,Semi-supervised Fine-Tuning,"The pre-training stage is followed by semi-supervised fine-tuning using a studentteacher framework  where CE indicates cross-entropy loss, E S , D S , E T , D T indicate the student and teacher encoder and decoder networks. The student branch is updated using a consolidated loss L = L Sup + λ 3 L Reg , whereas the teacher parameters (θ T ) are updated using EMA from the student parameters (θ S ): where t tracks the step number, and α is the momentum coefficient "
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,3,Experiments and Results,Datasets: We evaluate our work on two different DA tasks to evaluate its generalizability: (1) Polyp segmentation from colonoscopy images in Kvasir-SEG 
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,3.1,Performance on SSDA,Quantitative comparison of our proposed method with different SSDA methods 
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,3.2,Performance on UDA,"Unlike SSDA methods, UDA fully relies on unlabeled data for domain-invariant representation learning. To analyze the effectiveness of DA, we extend our model to the UDA setting (explained in Sect. 3 [Source → Target]) and compare it with SoTA methods "
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,3.3,Ablation Experiments,"We perform a detailed ablation experiment, as shown in Table "
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,4,Conclusion,"We propose a novel style-content disentangled contrastive learning, guided by a pixel-level feature consistency constraint for semi-supervised domain adaptive medical image segmentation. To the best of our knowledge, this is the first attempt for SSDA in medical image segmentation using CL, which is further extended to the UDA setting. Our proposed work, upon evaluation on two different domain adaptive segmentation tasks in SSDA and UDA settings, outperforms the existing SoTA methods, justifying its effectiveness and generalizability."
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,,Fig. 1 .,
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,,Table 2 .,
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,,Table 3 .,
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_25.
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,1,Introduction,"Medical image segmentation plays a crucial role in enabling better diagnosis, surgical planning, and image-guided surgery  Recently, diffusion models  To achieve accurate and diverse segmentation masks, we propose a novel Conditional Bernoulli Diff usion model for medical image segmentation (BerDiff). Instead of using the Gaussian noise, we first propose to use the Bernoulli noise as the diffusion kernel to enhance the capacity of the diffusion model for segmentation, resulting in more accurate segmentation masks. Moreover, by leveraging the stochastic nature of the diffusion model, our BerDiff randomly samples the initial Bernoulli noise and intermediate latent variables multiple times to produce a range of diverse segmentation masks, highlighting salient regions of interest (ROI) that can serve as a valuable reference for radiologists. In addition, our BerDiff can efficiently sample sub-sequences from the overall trajectory of the reverse diffusion based on the rationale behind the Denoising Diffusion Implicit Models (DDIM)  The contributions of this work are summarized as follows. 1) Instead of using the Gaussian noise, we propose a novel conditional diffusion model based on the Bernoulli noise for discrete binary segmentation tasks, achieving accurate and diverse medical image segmentation masks. 2) Our BerDiff can efficiently sample sub-sequences from the overall trajectory of the reverse diffusion, thereby speeding up the segmentation process. 3) Experimental results on LIDC-IDRI and BRATS 2021 datasets demonstrate that our BerDiff outperforms other state-of-the-art methods."
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,2,Methodology,"In this section, we first describe the problem definitions, and then demonstrate the Bernoulli forward and diverse reverse processes of our BerDiff, as shown in Fig. "
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,2.1,Problem Definition,"Let us assume that x ∈ R H×W×C denotes the input medical image with a spatial resolution of H × W and C channels. The ground-truth mask is represented as y 0 ∈ {0, 1} H×W , where 0 represents background while 1 ROI. Inspired by diffusion-based models such as denoising diffusion probabilistic model (DDPM) and DDIM, we propose a novel conditional Bernoulli diffusion model, which can be represented as p θ (y 0 |x) := p θ (y 0:T |x)dy 1:T , where y 1 , . . . , y T are latent variables of the same size as the mask y 0 . For medical binary segmentation tasks, the diverse reverse process of our BerDiff starts from the initial Bernoulli noise y T ∼ B(y T ; 1 2 •1) and progresses through intermediate latent variables constrained by the input medical image x to produce segmentation masks, where 1 denotes an all-ones matrix of the size H ×W ."
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,2.2,Bernoulli Forward Process,"In previous generation-related diffusion models, Gaussian noise is progressively added with increasing timestep t. However, for segmentation tasks, the groundtruth masks are represented by discrete values. To address this, our BerDiff gradually adds more Bernoulli noise using a noise schedule β 1 , . . . , β T , as shown in Fig.  where B denotes the Bernoulli distribution with the probability parameters (1 - Using the notation α t = 1β t and ᾱt = t τ =1 α τ , we can efficiently sample y t at an arbitrary timestep t in closed form: ( To ensure that the objective function described in Sect. 2.4 is tractable and easy to compute, we use the sampled Bernoulli noise ∼ B( ; 1-ᾱt 2 •1) to reparameterize y t of Eq. (  where ])."
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,2.3,Diverse Reverse Process,"The diverse reverse process p θ (y 0:T ) can also be viewed as a Markov chain that starts from the Bernoulli noise y T ∼ B(y T ; 1 2 • 1) and progresses through intermediate latent variables constrained by the input medical image x to produce diverse segmentation masks, as shown in Fig.  Specifically, we utilize the estimated Bernoulli noise ˆ (y t , t, x) of y t to parameterize μ(y t , t, x) via a calibration function F C , as follows: "
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,2.4,Detailed Procedure,"Here, we provide an overview of the training and sampling procedure in Algorithms 1 and 2. During the training phase, given an image and mask data pair {x, y 0 }, we sample a random timestep t from a uniform distribution {1, . . . , T }, which is used to sample the Bernoulli noise . We then use to sample y t from q(y t | y 0 ), which allows us to obtain the Bernoulli posterior q(y t-1 | y t , y 0 ). We pass the estimated Bernoulli noise ˆ (y t , t, x) through the calibration function F C to parameterize p θ (y t-1 | y t , x). Based on the variational upper bound on the negative log-likelihood in previous diffusion models  Finally, the overall objective function is presented as: During the sampling phase, our BerDiff first samples the initial latent variable y T , followed by iterative calculation of the probability parameters of y t-1 for different t. In Algorithm 2, we present two different sampling strategies from DDPM and DDIM for the latent variable y t-1 . Finally, our BerDiff is capable of producing diverse segmentation masks. By taking the mean of these masks, we can further obtain a saliency segmentation mask to highlight salient ROI that can serve as a valuable reference for radiologists. Note that our BerDiff has a novel parameterization technique, i.e. calibration function, to estimate the Bernoulli noise of y t , which is different from previous works "
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,3,Experiment,
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,3.1,Experimental Setup,Dataset and Preprocessing. The data used in this experiment are obtained from LIDC-IDRI 
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,3.2,Ablation Study,"We start by conducting ablation experiments to demonstrate the effectiveness of different losses and estimation targets, as shown in Table  Here, we conduct ablation experiments on our BerDiff with Gaussian or Bernoulli noise, and the results are shown in Table  Results on BRATS 2021. Here, we present the quantitative and qualitative results of BRATS 2021 in Table "
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,4,Conclusion,"In this paper, we proposed to use the Bernoulli noise as the diffusion kernel to enhance the capacity of the diffusion model for binary segmentation tasks, achieving accurate and diverse medical image segmentation results. Our BerDiff only focuses on binary segmentation tasks and takes much time during the iterative sampling process as other diffusion-based models; e.g. our BerDiff takes 0.4 s to segment one medical image, which is ten times of traditional U-net. In the future, we will extend our BerDiff to the multi-target segmentation problem and implement additional strategies for speeding up the segmentation process."
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,,Fig. 1 .,
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,,Fig. 3 .,
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,,3. 3,
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,,Table 1 .,
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,,Table 2 .,
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,,Table 3 .,
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,,Table 4 .,
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_47.
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,1,Introduction,"Medical image segmentation is a challenging task that requires accurate delineation of structures and regions of interest in complex and noisy images. Multiple expert annotators are often employed to address this challenge, to provide binary segmentation annotations for the same image. However, due to differences in experience, expertise, and subjective judgments, annotations can vary significantly, leading to inter-and intra-observer variability. In addition, manual annotation is a time-consuming and costly process, which limits the scalability and applicability of segmentation methods. To overcome these limitations, automated methods for multi-annotator prediction have been proposed, which aim to fuse the annotations from multiple annotators and generate an accurate and consistent segmentation result. Existing approaches for multi-annotator prediction include majority voting  In recent years, diffusion models have emerged as a promising approach for image segmentation, for example by using learned semantic features  In this work, we propose a novel method for multi-annotator prediction, using diffusion models for medical segmentation. The goal is to fuse multiple annotations of the same image from different annotators and obtain a more accurate and reliable segmentation result. In practice, we leverage the diffusionbased approach to create one map for each level of consensus. To obtain the final prediction, we average the obtained maps and obtain one soft map. We evaluate the performance of the proposed method on a dataset of medical images annotated by multiple annotators. Our results demonstrate the effectiveness and robustness of the proposed method in handling inter-and intra-observer variability and achieving higher segmentation accuracy than the state-of-the-art methods. The proposed method could improve the efficiency and quality of medical image segmentation and facilitate the clinical decision-making process."
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,2,Related Work,"Multi-annotator Strategies. Research attention has recently been directed towards the issues of multi-annotator labels  Diffusion Probabilistic Models (DPM).  Conditional Diffusion Probabilistic Models. In our work, we use diffusion models to solve the image segmentation problem as conditional generation, given the image. Conditional generation with diffusion models includes methods for class-conditioned generation, which is obtained by adding a class embedding to the timestep embedding  A previous study directly applied a diffusion model to generate a segmentation mask based on a conditioned input image "
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,3,Method,"Our approach for binary segmentation with multi-annotators employs a diffusion model that is conditioned on the input image I ∈ R W ×H , the step estimation t, and the consensus index c. The diffusion model updates its current estimate x t iteratively, using the step estimation function θ . See Fig.  Given a set of C annotations {A i k } C i=1 associated with input sample I k , we define the ground truth consensus map at level c to be During training, our algorithm iteratively samples a random level of the consensus c ∼ U [1, 2, ..., C] and an input-output pair (I k , M c k ). The iteration number 1 ≤ t ≤ T is sampled from a uniform distribution and X T is sampled from a normal distribution. We then compute x t from X T , M c k and t according to: where ᾱ is a constant that defines the schedule of added noise. The current step index t, and the consensus index c are integers that are translated to z t ∈ R d and z c ∈ R d , respectively with a pair of lookup tables. The embeddings are passed to the different networks F , D and E. In the next step, our algorithm encodes the input signal x t with network F and encodes the condition image I k with network G. We compute the conditioned signal u t = F (x t , z c , z t ) + G(I k ), and apply it to the networks E and D, where the output is the estimation of x t-1 . The loss function being minimized is: The training procedure is depicted in Algorithm 1. The total number of diffusion steps T is set by the user, and C is the number of different annotators in the dataset. Our model is trained using binary consensus maps (M c k ) as the ground truth, where k is the sample id, and c is the consensus index. The inference process is described in Algorithm 2. We sample our model for each consensus index, and then calculate the mean of all results to obtain our target, which is a soft-label map representing the annotator agreement. Mathematically, if the consensus maps are perfect, this is equivalent to assigning each image location with the fraction of annotations that consider this location to be part of the mask (if c annotators mark a pixel, it would appear in levels 1..c). In Sect. 4, we compare our method with other variants and show that estimating the fraction map directly, using an identical diffusion model, is far inferior to estimating each consensus level separately and then averaging."
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,,Employing Multiple Generations. Since calculating x t-1 during inference includes the addition of 1 [t>1],"β 1 is significant variability between different runs of the inference method on the same inputs, see Fig.  In order to exploit this phenomenon, we run the inference algorithm multiple times, then average the results. This way, we stabilize the results of segmentation and improve performance, as demonstrated in Fig.  Architecture. In this architecture, the U-Net's decoder D is conventional and its encoder is broken down into three networks: E, F , and G. The last encodes the input image, while F encodes the segmentation map of the current step x t . The two processed inputs have the same spatial dimensionality and number of channels. Based on the success of residual connections  The input image encoder G is built from Residual in Residual Dense Blocks  The encoder-decoder part of θ , i.e., D and E, is based on U-Net, similarly to  The residual block is composed of two convolutional blocks, where each convolutional block contains group-norm, SiLU activation, and a 2D-convolutional layer. The residual block receives the time embedding through a linear layer, SiLU activation, and another linear layer. The result is then added to the output of the first 2D-convolutional block. Additionally, the residual block has a residual connection that passes all its content. On the encoder side (network E), there is a downsample block after the residual blocks of the same depth, which is a 2D-convolutional layer with a stride of two. On the decoder side (network D), there is an upsample block after the residual blocks of the same depth, which is composed of the nearest interpolation that doubles the spatial size, followed by a 2D-convolutional layer. Each layer in the encoder has a skip connection to the decoder side."
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,4,Experiments,"We conducted a series of experiments to evaluate the performance of our proposed method for multi-annotator prediction. Our experiments were carried out on datasets of the QUBIQ benchmark The AdamW  Following  Results. We compare our method with FCN  We also compare with models that we train ourselves, using public code AMIS  Ablation Study. We evaluate alternative training variants as an ablation study in Table  Next, we study the effect of the number of generated images on performance. The results can be seen in Fig. "
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,5,Discussion,"In order to investigate the relationship between the annotator agreement and the performance of our model, we conducted an analysis by calculating the average Dice score between each pair of annotators across the entire dataset. The results of this pairwise Dice analysis can be found in Table "
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,6,Conclusions,"Shifting the level of consensus required to mark a region from very high to as low as one annotator, can be seen as creating a dynamic shift from a very conservative segmentation mask to a very liberal one. As it turns out, this dynamic is wellcaptured by diffusion models, which can be readily conditioned on the level of consensus. Another interesting observation that we make is that the mean (over the consensus level) of the obtained consensus masks is an effective soft mask. Applying these two elements together, we obtain state-of-the-art results on multiple binary segmentation tasks."
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,,Fig. 1 .,
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,,Fig. 2 .,
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,,Fig. 3 .,
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,,Fig. 4 .,
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,,Table 1 .,
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,,Table 2 .,
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,,Table 3 .,
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,1,Introduction,"Semantic segmentation aims to segment objects in an image by classifying each pixel into an object class. Training a deep neural network (DNN) for such a task is known to be data-hungry, as labeling dense pixel-level annotations requires laborious and expensive human efforts in practice  To alleviate the sample size and overfitting issues, diverse data augmentations have been recently developed. For example, CutMix  Although these augmentation approaches have been successful for natural images, their usage for medical image semantic segmentation is quite restricted as objects in medical images contain non-rigid morphological characteristics that should be sensitively preserved. For example, basalioma (e.g., pigmented basal cell carcinoma) may look similar to malignant melanoma or mole in terms of color and texture  In these regards, we tackle these issues with a novel augmentation method without distorting the semantics of objects in image space. This can be achieved by slightly but effectively perturbing target objects with adversarial noises at the object level. We first augment hard samples with adversarial attacks  We summarize our main contributions as follows: 1) We propose a novel online data augmentation method for semantic segmentation by imposing objectspecific consistency regularization between anti-adversarial and adversarial data."
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,,2),"Our method provides a flexible regularization between differently perturbed data such that a vulnerable network is effectively trained on challenging samples considering their ambiguities. 3) Our method preserves underlying morphological characteristics of medical images by augmenting data with quasiimperceptible perturbation. As a result, our method significantly improves sensitivity and Dice scores over existing augmentation methods on Kvasir-Seg "
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,2,Preliminary: Adversarial Attack and Anti-adversary,"Adversarial attack is an input perturbation method that adds quasiimperceptible noises into images to deceive a DNN. Given an image x, let μ be a noise bounded by l ∞ -norm. While the difference between x and the perturbed sample x = x + μ is hardly noticeable to human perception, a network f θ (•) can be easily fooled (i.e., f θ (x) = f θ (x + μ)) as the μ pushes x across the decision boundary. To fool a DNN, Fast Gradient Sign Method (FGSM)  Recently, anti-adversarial methods were proposed for the benign purpose to defend against such attacks. The work in "
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,3,Method,"Let {X i } N i=1 be an image set with N samples each paired with corresponding ground truth pixel-level annotations Y i . Our proposed method aims to 1) generate realistic images with adversarial attacks and 2) train a segmentation model f θ (X i ) = Y i for robust semantic segmentation with anti-adversarial consistency regularization (AAC). Figure  Data Augmentation with Object-Targeted Adversarial Attack. In many medical applications, false negatives (i.e., failing to diagnose a critical disease) are much more fatal than false positives. To deal with these false negatives, we mainly focus on training a network to learn diverse features at target ROIs (e.g., polyps) where disease-specific variations exist. To do so, we first exclude the background and perturb only the objects in the given image. Given o as the target object class and (p, q) as a pixel coordinate, a masked object is defined as Xi = {(p, q)|X i (p,q) = o}. As in PGD  where ), Y i ) is a quasi-imperceptible adversarial noise that fools f θ (•) and is a perturbation magnitude that limits the noise (i.e., |μ (p,q) | ≤ , s.t. (p, q) ∈ Xi ). Similarly, iterative anti-adversarial perturbation is defined as In contrast to the adversarial attack in Eq. 1, the anti-adversarial noise ), Y i ) manipulates samples to increase the classification score. Note that, generating noises and images are online and training-free as the loss derivatives are calculated with freezed network parameters. The adversaries X - i,1 , ..., X - i,K are used as additional training samples so that the network includes the non-discriminative yet object-relevant features for the prediction. On the other hand, as the anti-adversaries are sufficiently discriminative, we do not use them as training samples. Instead, only the K-th anti-adversary X + i,K (i.e., the most perturbed sample with the lowest loss) is used for downstream consistency regularization to provide informative guidance to the adversaries. Computing Adaptive Consistency Toward Anti-adversary. Let X i be either X i or X - i,k . As shown in Fig.  where l(•) is Dice loss  Training a Segmentation Network. Let Ŷ + i,K be a segmentation outcome, i.e., one-hot encoded pseudo-label from the network output P + i,K of anti-adversary X + i,K . Given X i and {X - i,k } K k=1 as training data, the supervised segmentation loss L sup and the consistency regularization R con are defined as Using the pseudo-label from anti-adversary as a perturbation of the ground truth, the network is supervised by diverse and realistic labels that contain auxiliary information that the originally given labels do not provide. With a hyperparameter α, the whole training loss L = L sup +αR con is minimized via backpropagation to optimize the network parameters for semantic segmentation."
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,4,Experiments,
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,4.1,Experimental Setup,Dataset. We conducted experiments on two representative public polyp segmentation datasets: Kvasir-SEG  Implementation. We implemented our method on Pytorch framework with 4 NVIDIA RTX A6000 GPUs. Adam optimizer with learning rates of 4e-3/1e-4 (Kavsir-SEG/ETIS) were used for 200 epochs with a batch size of 16. We set the number of perturbation steps K as 10 and the magnitude of perturbation as 0.001. The weight α for R con was set to 0.01.
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,,Baselines.,"Along with conventional augmentation methods (i.e., random horizontal and vertical flipping denoted as 'Basic' in Table "
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,,Evaluation.,"To verify the effectiveness of our method, evaluations are conducted using various popular backbone architectures such as U-Net "
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,4.2,Comparison with Existing Methods,As shown in Table 
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,4.3,Analysis on Anti-adversaries and Adversaries,In Fig. 
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,5,Conclusion,"We present a novel data augmentation method for semantic segmentation using a flexible anti-adversarial consistency regularization. In particular, our method is tailored for medical images that contain small and underrepresented key objects such as a polyp and tumor. With object-level perturbations, our method effectively expands discriminative regions on challenging samples while preserving the morphological characteristics of key objects. Extensive experiments with various backbones and datasets confirm the effectiveness of our method."
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,,Fig. 1 .,
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,,Fig. 2 .,
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,,Fig. 3 .,
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,,Fig. 4 .,
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,,Fig. 5 .,
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,,Fig. 6 .,
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,,Table 1 .,
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,1,Introduction,"The successes of deep convolutional neural networks (CNNs)  The development of FSL has derived many effective medical image segmentation methods, including pure CNNs  In this paper, we propose a prototype contrastive learning and semantic reasoning network based on multi-shot strategy (MPSNet). To our knowledge, this is a great improvement in medical image analysis. First, we design a multi-shot learning network (MLN) within the support set to generate prior semantic features and a prior segmentation model for the query image. Cross-validation mode is used to construct the support-query image pairs within the support set, and a complete FSS model is formed by prototype contrastive learning and supervised training. Second, we propose a prototype contrastive learning module (PCLM) to ascertain the positive contribution of multiple support images to the query image in segmentation guidance, leading to better segmentation performance of the query image. Third, we design a semantic reasoning network (SRN) that is convenient to directly transfer the prior semantic features and prior segmentation model to the query image to deduce its segmentation mask quickly. Our contributions can be summarized as follows: 1) A novel FSS method based on a multi-shot prototype strategy is proposed for the first time to replace the commonly used mean prototype method to improve the guidance ability of the support images to the query images in segmentation. 2) A multi-shot prototype contrastive learning network within the support set is constructed with supervised training to generate prior semantic features and a prior segmentation model, and transfer them to the query image to deduce its segmentation mask. 3) The proposed method achieves the latest performance on three public datasets that is superior to the state-of-the-art (SOTA) methods.   n=1 . The support images, masks and the query image constitute the model input Input model = {I s , M s , I q }, query mask M q as the supervisory information. At testing stage, we set episodes with the same mode, that is, we use the trained FSS model to segment the query set under the function of the support set. Since our research is carried out based on multi-shot strategy, we mainly take 5-shot as an example to verify the effectiveness of the proposed method."
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,2.2,Multi-shot Learning Network,"The framework of MLN is shown in Fig.  msi / mfq = P CLM(GAP (RN ( where RN (•) refers to the shared feature extractor ResNet101, GAP (•) refers to global average pooling, P CLM (•) refers to the proposed prototype contrastive learning module. x fsi , m fsi , x fq represent the fake support image, mask and query image. We use cross-entropy (CE) loss to supervise the training process: As shown in the lower part of Fig.  where support features F ea fs i are resized to the mask size (H, W ) , (h, w) denotes the shape of the feature maps, C denotes the channel number of the feature maps, denotes the Hadamard product. The obtained multi-shot prototypes P and query features F ea fq are sent into PCLM to get the final segmentation of the query image."
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,2.3,Prototype Contrastive Learning Module,"As shown in Fig.  The foreground prediction mask is then obtained: where (x, y) refers to the spatial location of the query image, and the prototype generates a similarity coefficient for each pixel. δ (•) denotes the Sigmoid function with a steepness parameter k = 0.5. T fq is the correlation threshold obtained by the query features F ea fq through the full connection layers g θ (•). Then, we assume that the prediction mask is no different from the real mask, and GAP it with F ea fq to obtain the hypothetical prototypes P h = P h i 4 i=1 : Theoretically, under the assumption that there is no pixel intensity and contrast difference between the query image and the support image, the hypothetical prototypes P h should be similar to the multi-shot prototypes P . But in reality, it is difficult to make the query image and the support images have a consistent feature hierarchy. That is to say, the prior knowledge provided by multiple support images for the query image is not equally important. We need to assign a weight factor w i to each support image to represent its positive contribution to query image segmentation. To this end, we determine the weight of each foreground prediction mask by comparing the hypothetical prototypes P h with real support prototypes P : At this time, the foreground and background segmentation mask can be modified as: we take m fq as the output of PCLM that is also the final segmentation mask."
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,2.4,Semantic Reasoning Network,"In our 5-shot segmentation task, the query image needs to be accurately segmented under the guidance of the given 5 support images. Such an FSS task can be realized through the migration of the prior semantic features and prior segmentation model from MLN. Specifically, as shown in Fig.  L q seg = CE( mq , m q ), (  so far, the overall loss of the proposed segmentation network MPSNet is L seg ."
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,3,Experiments,
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,3.1,Experimental Setup,Datasets: We evaluate the proposed MPSNet on three public datasets: the Combined (CT-MR) Healthy Abdominal Organ Segmentation (CHAOS) dataset  Implementation Details: We use 5-fold cross validation to conduct training and testing. We chose the same allocation strategy for the support slices and the query slices as Roy et al. 
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,3.2,Superior Performance over the SOTA Methods,"We compare the quantitative results of our method with the SOTA methods, including SE-Net "
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,3.3,Ablation Studies,"The specific implementation scheme of the ablation studies can be divided into three aspects: 1) MLN is removed from MPSNet to prove that the prior semantic   prototype network is reverted, that is, MLN and PCLM are removed, and the multi-shot support prototypes are directly averaged for similarity measurement with the query features (PNet). Note that SRN cannot be implemented when MLN and PCLM are absent. Table "
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,4,Conclusion,"In this paper, we propose a novel FSS method MPSNet for medical image segmentation to replace the approach of averaging multiple support prototypes in the existing FSS methods. This is the first time that a multi-shot learning pattern is built within the support set and applied to the query image to significantly improve segmentation performance. Compared with the SOTA methods, our method is evaluated on three public datasets and achieves improvements in DSC of 3.17%, 1.37% and 1%, respectively. Relevant ablation studies also demonstrate the necessity and validity of our method."
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,,Fig. 1 .,
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,,Fig. 2 .,
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,,Fig. 3 .,
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,,2 Method 2.1 Problem Setting In,
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,,Table 1 .,
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,,Table 2 .,
Multi-shot Prototype Contrastive Learning and Semantic Reasoning for Medical Image Segmentation,,MPSNet 81.88 87.26 82.10 76.80 82.01 81.13 69.56 70.26 77.96 74.72 87.98 65.08 80.63 77.90,
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,1,Introduction,"The precise processing and analysis of brain MR images are critical in advancing neuroscience research. As a widely used animal model with high systematic similarity to humans in various aspects, macaques play an indispensable role in understanding brain mechanisms in development, aging and evolution, exploring the pathogenesis of neurological diseases, and validating the effectiveness of clinical techniques and drugs  Deep learning has emerged as a powerful tool in medical image processing and analysis, with Convolutional Neural Networks (CNNs) showing remarkable performance in various applications, e.g., image segmentation  In this study, we present a 3D Collaborative Segmentation-Generation Framework (CSGF) for early-developing macaque MR images. The CSGF is designed in the form of multi-task collaboration and feature sharing, which enables it to complete the missing modality generation and tissue segmentation simultaneously. As depicted in Fig.  The proposed CSGF offers several advantages over existing methods: (1) The collaborative learning of both MGM and TSM enables the missing modality to be imputed in a tissue-oriented manner, and hence the generated neuroimages are more consistent with real neuroimages from an anatomical point of view; "
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,2,Method,
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,2.1,Dataset,The experimental dataset was from the public UNC-Wisconsin Neurodevelopment Rhesus Database 
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,2.2,Problem Formulation,"We construct this Collaborative Segmentation-Generation Framework (CSGF) based on the primary modality T1w (denoted as T 1 i ) and the auxiliary modality T2w (denoted as T 2 i ) data. Let {T 1 i , T 2 i , M i } i=1∼N be the dataset consisting of N scans, where T 1 i and T 2 i represent the T1w and T2w of i th scan, respectively; M i represents the tissue label of i th scan. This framework can be formulated as where Mi is the estimated tissue label of the i th scan. However, in practical application, it is often the case that some s only contain the T1w modality. Therefore we construct a modality generation module (MGM) by a mapping function G to generate the missing T2w modality as where T2 i is the generated T2w based on T1w modality for the i th scan. Then, the tissue segmentation module (TSM) constructs a mapping function S as"
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,2.3,Framework Overview and Collaborative Learning,"The proposed CSGF consists of two key modules: the MGM and the TSM. These two modules are linked through the cross-module feature sharing (CFS), thus being trained collaboratively, as illustrated in Fig.  The proposed MGM and TSM are trained collaboratively. Given a dataset {T 1  i , T 2 i , M i } i=1∼N , the generator in MGM can be trained as where * GAN represents the GAN loss and * MSE represents mean square error (MSE) loss. Meanwhile, the segmenter in TSM can be trained as where * CE represents the cross-entropy loss and * DICE represents dice coefficient loss. During testing, for each input scan, if it has both T1w and T2w, we predict its tissue label as Mi = S(T 1 i , T 2 i ). If it only has T1w modality, we first generate its T2w modality by T2 i = G(T 1 i ), and then predict its tissue label as Mi = S(T 1 i , G(T 1 i ))."
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,3,Results and Discussion,
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,3.1,Experimental Details,"The total scans of MRI data used in this study was 155, divided into training and test sets in a 4:1 ratio. Before training, the data had the redundant background removed and cropped into a size of 160 × 160 × 160, and then overlapping patches of size 64 were generated. The intensity values of all patches were normalized to [-1, 1]. Both MGM and TSM were optimized simultaneously during training. We employed the Adam optimizer with a learning rate of 0.0002 for the first 50 epochs, after which the learning rate gradually decreased to 0 over the following 50 epochs. In the testing phase, overlapping patches with stride 32 generated from testing volumes were input into the model, and the obtained results were rebuilt to a volume, during which the overlaps between adjacent patches were averaged. All experiments were implemented based on Pytorch 1.13.0 and conducted on NVIDIA RTX 4090 GPUs with 24GB VRAM in Ubuntu 18.04."
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,3.2,Evaluation of Framework,"Totally 31 scans of early-developing macaques were used for framework evaluation. To provide a comparison baseline, we used PTNet3D (PTNet), one stateof-the-art algorithm for medical image translation, and U-Net  Compared with the other two stages, the brain in the early infancy stage from 0 to 6 months is in a state of vigorous development, especially the extremely low contrast between tissues from 0 to 2 months, which brings great challenges to this study. In the evaluation of tissue segmentation, we assumed the practical scenario that the test scans contain only T1w images. We compared four results, including U-Net with T1w input, U-Net with T1w and generated T2w input from PTNet (i.e. independent-task mode), CSGF without CFS embedding and the full CSGF framework. We quantitatively evaluate the results by using the average surface distance (ASD) and the dice coefficient, as reported in Table  In the evaluation of the modality generation, we compared generated T2w images from T1w images based on three methods, including PTNet, CSGF w/o CFS (i.e. PTNet under the supervision of U-Net) and the final CSGF framework. We quantitatively evaluated the results by using the peak signal to noise ratio (PSNR) and structural similarity index (SSIM), as reported in Table "
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,4,Conclusion,"We propose the novel Collaborative Segmentation-Generation Framework (CSGF) to deal with the missing-modality brain MR images for tissue segmentation in early-developing macaques. Under this framework, the modality generation module (MGM) and tissue segmentation module (TSM) are jointly trained through cross-module feature sharing (CFS). The MGM is trained under the supervision of the TSM, while the TSM is trained with real and generated neuroimages. Comparative experiments on 155 scans of developing macaque data show that our CSGF outperforms conventional independent-task mode in both modality generation and tissue segmentation, showing its great potential in neuroimage research. Furthermore, as the proposed CSGF is a general framework, it can be easily extended to other types of modality generation, such as CT and PET, combined with other image segmentation tasks."
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,,Fig. 1 .,
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,,Fig. 2 .,
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,,Fig. 3 .,
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,,Fig. 4 .,
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,,Table 1 .,
Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images,,Table 2 .,
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,1,Introduction,"Automatic medical image segmentation is the implementation of data-driven image segmentation concepts to identify a specific anatomical structure's surface or volume in a medical image ranging from X-ray and ultrasonography to CT and MRI scans. Deep learning algorithms are exquisitely suited for this task because they can generate measurements and segmentations from medical images without the time-consuming manual work as in traditional methods. However, the performance of deep learning algorithms depends heavily on the availability of large-scale, high-quality, fully pixel-wise annotations, which are often expensive to acquire. To this end, few-shot learning is considered as a more promising approach and introduced into the medical image segmentation by  Through revisiting existing FSMS algorithms  The main reason affecting the representativeness of the prototype is the significant discrepancy between support and query. Specifically, in general, different protocols are taken for different patients, which results in a variety of superficial organ appearances, including the size, shape, and contour of features. In this case, the prototype generated from the support features may not accurately represent the key attributes of the target organ in the query image. In addition, it is also challenging to extract useful information (prototypes of novel classes) from the cluttered background due to the extremely heterogeneous texture between the target and its surroundings, which may contain information belonging to some novel classes or redundant information issue  To mitigate the impact of intra-class diversity, it considers subdividing the foreground of the supporting prototypes to produce some regional prototypes, which are then rectified to suppress or exclude areas inconsistent with the query targets, as illustrated in Fig.  -A Region-enhanced Prototypical Transformer (RPT) consisting of stacked Bias-alleviated Transformer (BaT) blocks is proposed to mitigate the effects of large intra-class variations present in FSMS through Search and Filter (S&F) modules devised based on the self-selection mechanism. -A subdivision strategy is proposed to perform in the foreground of the support prototype to generate multiple regional prototypes, which can be further iteratively optimized by the RPT to produce the optimal prototype. -The proposed method can achieve state-of-the-art performance on three experimental datasets commonly used in medical image segmentation."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,2,Methodology,
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,2.1,Overall Architecture,"Before introducing the overall architecture, it is necessary to briefly explain how data is processed. Specifically, the 3D supervoxel clustering method  As depicted in Fig. "
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,2.2,Regional Prototype Generation,"The core problem considered in this paper is what causes prototype bias. By examining the input data, it can be observed that images of healthy and diseased organs have a chance to be considered as support or query. This means that if there are lesioned or edematous regions in some areas of the support images, they will be regarded as biased information which in reality cannot be accurately transferred for the query images containing only healthy organs. When these prototypes that contain the natural heterogeneity of the input images are processed by the Masked Average Pooling (MAP) operation, they inevitably lead to significant intra-class biases. To cope with the above problems, we propose a Region Prototype Generation (RPG) module to generate multi-region prototypes by performing subdivisions in the foreground of the support images. Given an input support image I s and the corresponding foreground mask M f , the foreground of this image can be obtained by calculating their product. The foreground image then can be partitioned into N f regions, where N f is set to 10 by default. By using the Voronoi-based partition method  where F s ∈ R C×H×W is the feature extracted from the support images and V n denotes the regional masks."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,2.3,Query Prototype Generation,"Once a set of coarse regional prototypes Ps have been generated for the support images, we can employ the method introduced in  where F q ∈ R C×H×W is feature extracted from query images, S(a, b) = -αcos(a, b) is the negative cosine similarity with a fixed scaling factor α = 20, σ denotes the Sigmoid activation, and τ is obtained by applying one averagepooling and two fully-connected layers (FC) to the query feature, expressed as τ = FC(F q ). After this, the coarse query foreground prototype can be achieved by using Pq = MAP(F q,i , Mf q,i )."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,2.4,Region-Enhanced Prototypical Transformer,"The above received prototypes Ps and Pq are taken as input to the proposed Region-enhanced Prototypical Transformer (RPT) to rectify and regenerate the optimal global prototype P s . As shown in Fig.  where ξ is the selection threshold achieved by ξ = (min(A) + mean(A))/2, S indicates the chosen regions from the support image that performs compatible with the query at the prototypical level. Then, the heterogeneous or disturbing regions of support foreground will be weeded out with softmax(•) function. The preliminary rectified prototypes Po s ∈ R N f ×C is aggregated as: The refined Po s will be fed into the following components designed based on the multi-head attention mechanism to produce the output where Po+1 s ∈ R N f ×C is the intermediate generated prototype, LN(•) denotes the layer normalization, MHA(•) represents the standard multi-head attention module and MLP(•) is the multilayer perception. By stacking multiple BaT blocks, our RPT can iteratively rectify and update all coarse support and the query prototype. Given the prototypes P l-1 s and P l-1 q from the previous BaT block, the updates for the current BaT block are computed by: where P l s ∈ R N f ×C and P l q ∈ R 1×C (l = 1, 2, ..., L) are updated prototypes, GAP(•) denotes the global average pooling operation. The final output prototypes P s optimized by the RPT can be used to predict the foreground of the query image by using Eq. (2: Mf q = 1σ(S(F q , GAP(P 3 s ))τ ), while its background can be obtained by Mb q = 1 -Mf q accordingly."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,2.5,Objective Function,"The binary cross-entropy loss L ce is adopted to determine the error between the predict masks Mq and the given ground-truth M q . Formally, Considering the prevalent class imbalance problem in medical image segmentation, the boundary loss  where θ denotes the network parameters, Ω denotes the spatial domain, φG : Ω → R denotes the level set representation of the ground-truth boundary, φG(q) = -D G (q) if q ∈ G and φG(q) = D G (q) otherwise, D G is distance map between the boundary of prediction and ground-truth, and s θ (q) : Ω → [0, 1] denotes softmax(•) function. Overall, the loss used for training our RPT is defined as where L dice is the Dice loss  Evaluation: For a fair comparison, the metric used to evaluate the performance of 2D slices on 3D volumetric ground-truth is the Dice score used in  In addition, as in "
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,3.1,Quantitative and Qualitative Results,Table 
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,3.2,Ablation Studies,The ablation studies were conducted on Abd-MRI dataset under Setting 2. As can be seen from Fig. 
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,4,Conclusion,"In this paper, we introduced a Region-enhanced Prototypical Transformer (RPT) to mitigate the impact of large intra-class variations present in medical image segmentation. The model is mainly beneficial from a subdivision-based strategy used for generating a set of regional support prototypes and a self-selection mechanism introduced to the Bias-alleviated Transformer (BaT) blocks. The proposed RPT can iteratively optimize the generated regional prototypes and output a more precise global prototype for predictions. The results of extensive experiments and ablation studies can demonstrate the advancement and effectiveness of the proposed method."
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,,Fig. 1 .,
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,,Fig. 2 .,
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,,Fig. 3 .Fig. 4 .,
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,,Table 1 .,
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,,Table 2 .,
Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_26.
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,1,Introduction,Head and neck (HaN) cancer is a prevalent type of cancer 
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,,Related Work.,"A literature review by Zhang et al.  Motivation. When segmenting OARs in the HaN region for the purpose of RT planning, a multimodal segmentation model that can leverage the information from CT and MR images of the same patient might be beneficial compared to separate single-modal models. Firstly, as intuition suggests, such a model would rely on the CT image for bone structures and on the MR image for soft tissues, and therefore improve the overall segmentation quality by exploiting the complementary information from both modalities. Secondly, a multimodal model would facilitate cross-modality learning by extracting knowledge from one and applying that knowledge to the other modality, potentially improving the segmentation accuracy. Several studies indicated that such an approach is feasible, for example, for improving video classification by training a model on an auxiliary audio reconstruction task "
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,,Contributions.,"To tackle these considerations, we propose a mechanism named modality fusion module (MFM) that can generally be applied to any network architecture that learns features from multiple modalities, and shows promising performance also in the missing modality scenario. The advantages of the proposed MFM are the following: 1) it enables the spatial alignment of FMs from one with FMs from the other modality to further reduce errors that persist after deformable registration of input images, and enrich the FMs to improve the final OAR segmentation, 2) it significantly improves the performance of the missing modality scenario compared to other baseline fusion approaches, and 3) it performs well also on single modality out-of-distribution data, therefore facilitating cross-modality learning and contributing to better model generalizability."
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,2,Methods,"Backbone Architecture. Our chosen backbone network is based on nnU-Net, a publicly available framework for DL-based segmentation "
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,,Modality Fusion Module.,"The proposed MFM draws inspiration from the work of Jaderberg et al.  Baseline Comparison. We evaluate the performance of the proposed MFM nnU-Net against three baseline networks: 1) a single modality nnU-Net trained only on CT images, 2) a nnU-Net trained on concatenated CT and MR image pairs, and 3) a model with separate encoders for both modalities, but with a simple concatenation along the channel axis instead of the proposed MFM. In addition, we compare our model with the state-of-the-art modality-aware mutual learning nnU-Net (MAML) that was presented at MICCAI 2021 "
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,3,Experiments and Results,"Image Datasets. The proposed methodology was evaluated on two publicly available datasets: our recently released HaN-Seg dataset  Implementation Details. All models were trained for all OARs using the 3d fullres configuration of nnU-Net, with the only modification that we reduced rotation around the axial axis and disabled image flipping along the sagittal plane, which eliminated segmentation errors that were previously observed for the paired (left and right) OARs. The same modification was also used with the MAML model. To ensure a fair model comparison, we set the number of filters in the encoder of the single modality baseline model to match the number of filters of the entry-level concatenation encoder. We also halved the number of filters in networks that have separate encoders so that the overall number of parameters in the proposed model and the baselines remains approximately the same (excluding the parameters in the localization part of MFM  block). Note that the MAML model, which is composed of two U-Nets, had a considerably higher number of parameters. To address the challenge of a relatively small dataset, we adopted a 4-fold cross-validation strategy without using any external training images. All models were trained until convergence, i.e. when the validation loss plateaued, and we selected the model with the best validation loss for inference. Results. The quality of the obtained OAR segmentation masks was evaluated by computing the Dice similarity coefficient (DSC) and the 95 th -percentile Hausdorff distance (HD 95 ) against reference manual delineations, and the results for all OARs are presented in Figs. "
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,4,Discussion,"In this study, we evaluated the impact on the quality and robustness of automatic OAR segmentation in the HaN region caused by the incorporation of the MR modality into the segmentation framework. We devised a mechanism named MFM and combined it with nnU-Net as our backbone segmentation network. The choice of using nnU-Net as the backbone was based on the rationale that nnU-Net already incorporates numerous state-of-the-art DL innovations proposed in recent years, and therefore validation of the proposed MFM is more challenging in comparison to simply improving a vanilla U-Net architecture, and consequently also more valuable to the research community. Segmentation Results. The obtained results demonstrate that our model performs best in terms of DSC (Fig.  Missing Modality Scenario. The overall good performance on the HaN-Seg dataset suggests that all models are close to the maximal performance, which is bounded by the quality of reference segmentations. However, the performance on the PDDCA dataset that consists only of CT images allows us to test how the models handle the missing modality scenario and perform on an out-ofdistribution dataset, as images from this dataset were not used for training. As expected, the CT-only model performed best in its regular operating scenario, with a mean DSC of 74.7% (Fig. "
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,5,Conclusions,"In this study, we introduced MFM, a fusion module that aligns FMs from an auxiliary modality (e.g. MR) to FMs from the primary modality (e.g. CT). The proposed MFM is versatile, as it can be applied to any multimodal segmentation network. However, it has to be noted that it is not symmetrical, and therefore requires the user to specify the primary modality, which is typically the same as the primary modality used in manual delineation (i.e. in our case CT). We evaluated the performance of MFM combined with the nnU-Net backbone for segmentation of OARs in the HaN region, an important task in RT cancer treatment planning. The obtained results indicate that the performance of MFM is similar to other state-of-the-art methods, but it outperforms other multimodal methods in scenarios with one missing modality."
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,,Fig. 1 .,
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,,Fig. 2 .,
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,,Fig. 3 .,
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 71.
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,1,Introduction,"Atrial Fibrillation (AFib) is a prevalent cardiac arrhythmia affecting over 45 million individuals worldwide as of 2016  Intracardiac Echocardiography imaging utilizes an ultrasound probe attached to a catheter and inserted into the heart to obtain real-time images of its internal structures. In ablation procedures for Left Atrium (LA) AFib treatment, the ICE ultrasound catheter is inserted in the right atrium to image the left atrial structures. The catheter is rotated clockwise to capture image frames that show the LA body, the LA appendage and the pulmonary veins  The precise segmentation of cardiac structures, particularly the LA, is crucial for the success and safety of catheter ablation. However, segmentation of the LA is challenging due to the constrained spatial resolution of 2D ICE images and the manual manipulation of the ICE transducer. Additionally, the sparse sampling of ICE frames makes it difficult to train automatic segmentation models. Consequently, there is a persistent need to develop interactive editing tools to help experts modify the automatic segmentation to reach clinically satisfactory accuracy. During a typical ICE imaging scan, a series of sparse 2D ICE frames is captured and a Clinical Application Specialist (CAS) annotates the boundaries of the desired cardiac structure in each frame A natural remedy is to allow clinicians to edit the segmentation output and create a model that incorporates and follows these edits. In the case of ICE data, the user interacts with the segmentation output by drawing a scribble on one of the 2D frames (Fig.  In this paper, we present a novel interactive editing framework for the ICE data. This is the first study to address the specific challenges of interactive editing with ICE data. Most of the editing literature treats editing as an interactive segmentation problem and does not provide a clear distinction between interactive segmentation and interactive editing. We provide a novel method that is specifically designed for editing. The novelty of our approach is two-fold: 1) We introduce an editing-specific novel loss function that guides the model to incorporate user edits while preserving the original segmentation in unedited areas. 2) We present a novel evaluation metric that best reflects the editing formulation. Comprehensive evaluations of the proposed method on ICE data demonstrate that the presented loss function achieves superior performance compared to traditional interactive segmentation losses and training strategies, as evidenced by the experimental data."
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,2,Interactive Editing of ICE Data,
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,2.1,Problem Definition,"The user is presented first with an ICE volume, x ∈ R H×W ×D , and its initial imperfect segmentation, y init ∈ R H×W ×D , where H, W and D are the dimensions of the volume. To correct inaccuracies in the segmentation, the user draws a scribble on one of the 2D ICE frames. Our goal is to use this 2D interaction to provide a 3D correction to y init in the vicinity of the user interaction. We project the user interaction from 2D to 3D and encode it as a 3D Gaussian heatmap, u ∈ R H×W ×D , centered on the scribble with a standard deviation of σ enc  We train an editing model f to predict the corrected segmentation output ŷt ∈ R H×W ×D given x, y t init , and u t , where t is the iteration number. The goal is for ŷt to accurately reflect the user's correction near their interaction while preserving the initial segmentation elsewhere. Since y t+1 init ≡ ŷt , subsequent user inputs u {t+1,...,T } should not corrupt previous corrections u {0,...,t} (Fig. "
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,2.2,Loss Function,"Most interactive segmentation methods aim to incorporate user guidance to enhance the overall segmentation  We propose an editing-specific loss function L that encourages the model to preserve the initial segmentation while incorporating user input. The proposed loss function incentivizes the model to match the prediction ŷ with the ground truth y in the vicinity of the user interaction. In regions further away from the user interaction, the loss function encourages the model to match the initial segmentation y init , instead. Here, y represents the 3D mesh, which is created by deforming a CT template to align with the CAS contours y cas  We define the vicinity of the user interaction as a 3D Gaussian heatmap, A ∈ R H×W ×D , centered on the scribble with a standard deviation of σ edit . Correspondingly, the regions far from the interaction are defined as Ā = 1 -A. The loss function is defined as the sum of the weighted cross entropy losses L edit and L preserve w.r.t y and y init , respectively, as follows where The Gaussian heatmaps facilitate a gradual transition between the edited and unedited areas, resulting in a smooth boundary between the two regions."
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,2.3,Evaluation Metric,"The evaluation of segmentation quality typically involves metrics such as the Dice coefficient and the Jaccard index, which are defined for binary masks, or distancebased metrics, which are defined for contours  We propose a novel editing-specific evaluation metric that assesses how well the prediction ŷ matches the CAS contours y cas in the vicinity of the user interaction, and the initial segmentation y init in the regions far from the interaction. where, ∀(i, j, k) ∈ {1, . . . , H} × {1, . . . , W } × {1, . . . , D}, D edit is the distance from y cas to ŷ in the vicinity of the user edit, as follows 2 Contours are inferred from the predicted mask ŷ. where d is the minimum Manhattan distance from y cas i,j,k to any point on ŷ. For D preserve , we compute the average symmetric distance between y init and ŷ, since the two contours are of comparable length. The average symmetric distance is defined as the average of the minimum Manhattan distance from each point on y init contour to ŷ contour and vice versa, as follows The resulting D represents a distance map ∈ R H×W ×D with defined values only on the contours y cas , y init , ŷ. Statistics such as the 95 th percentile and mean can be computed on the corresponding values of these contours on the distance map. 3 Experiments"
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,3.1,Dataset,"Our dataset comprises ICE scans for 712 patients, each with their LA CAS contours y cas and the corresponding 3D meshes y generated by "
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,3.2,Implementation Details,"To obtain the initial imperfect segmentation y init , a U-Net model  The input of the editing model consists of three channels: the input ICE volume x, the initial segmentation y init , and the user input u. During training, the user interaction is synthesized on the frame with maximum error between y init and y. "
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,,Method,
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,3.3,Results,"We use the editing evaluation metric D (Sect. 2.3) for the evaluation of the different methods. For better interpretability of the results, we report the overall error, the error near the user input, and the error far from the user input. We define near and far regions by thresholding the Gaussian heatmap A at 0.5. We evaluate our loss (editing loss) against the following baselines: (1) No Editing: the initial segmentation y init is used as the final segmentation ŷ, and the overall error in this case is the distance from the CAS contours to y init . This should serve as an upper bound for error. (2) CE Loss: an editing model trained using the standard CE segmentation loss w.r.t y. (3) Dice Loss  Our results demonstrate that the proposed loss outperforms all baselines in terms of overall error. Although all the editing methods exhibit comparable performance in the near region, in the far region where the error is calculated relative to y init , our proposed loss outperforms all the baselines by a significant margin. This can be attributed to the fact that the baselines are trained using loss functions which aim to match the ground truth globally, resulting in deviations from the initial segmentation in the far region. In contrast, our loss takes into account user input in its vicinity and maintains the initial segmentation elsewhere. Sequential Editing. We also investigate the scenario in which the user iteratively performs edits on the segmentation multiple times. We utilized the same models that were used in the single edit experiment and simulated 10 editing iterations. At each iteration, we selected the furthest CAS contour from ŷ, ensuring that the same edit was not repeated twice. For the interCNN model, we aggregated the previous edits and input them into the model, whereas for all other models, we input a single edit per iteration. We assessed the impact of the number of edits on the overall error. In Fig.  Furthermore, in Fig. "
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,4,Conclusion,"We presented an interactive editing framework for challenging clinical applications. We devised an editing-specific loss function that penalizes the deviation from the ground truth near user interaction and penalizes deviation from the initial segmentation away from user interaction. Our novel editing algorithm is more robust as it does not compromise previously corrected regions. We demonstrate the performance of our method on the challenging task of volumetric segmentation of sparse ICE data. However, our formulation can be applied to other editing tasks and different imaging modalities."
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,,Fig. 1 .,
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,,Fig. 2 .,
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,,Fig. 3 .,
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,,Fig. 4 .,
From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_73.
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,1,Introduction,"Medical image segmentation is an essential task in computer-aided diagnosis. On this task, convolutional neural networks (CNNs) have demonstrated their effectiveness in an extensive literature  Most studies on DG attempt to alleviate the distribution discrepancy by standardizing the features  To address these issues, in this paper, we propose a simple but effective multisource DG method called Treasure in Distribution (TriD), which consists of two major steps: statistics randomization (SR) and style mixing (SM). SR aims to tap the potential of distribution by randomly sampling the augmented statistics from a uniform distribution to perturb the original intermediate features, which can expand the search space to cover more cases evenly (see Fig.  Our contributions are three-fold: (1) The proposed multi-source DG method called TriD can boost the robustness of model and alleviate the performance drop on the unseen target-domain data. (2) We focus on expanding the search space of feature styles and therefore devise the statistics-randomization strategy, which allows exploring in the feature-style space evenly. (3) Different from perturbing all feature channels, we introduce the original statistics to the augmented statistics to learn the domain-invariant representations explicitly."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,2,Method,
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,2.1,Preliminaries,"Let f ∈ R B×C×H×W be the intermediate features in a mini-batch, where B, C, H, and W respectively denote the mini-batch size, channel, height, and width. MixStyle  where λ m ∈ R B is a weight coefficient sampled from a Beta distribution  the features from two different images in a minibatch, and μ( * ), σ( * ) ∈ R B×C are the mean and standard deviation computed across the spatial dimension within each channel of each image."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,2.2,Treasure in Distribution (TriD),"The TriD is designed to perturb the intermediate feature styles by randomly changing the feature statistics (i.e., mean and standard deviation), as shown in Fig.  It can be implemented as a plug-and-play module inserted into any CNN-based architecture. In this study, we use ResNet-34 "
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Statistics Randomization (SR).,Inspired by effectiveness of unreal styles in the input space 
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Style Mixing (SM).,"To learn the domain-invariant representations explicitly, SM strategy is designed to randomly mix the augmented and original statistics along the channel wise. We first sample P ∈ R B×C from the Beta distribution: P ∼ Beta(α, α), and use P as the probability to generate the Bernoulli distribution from which to sample λ ∈ R B×C : λ ∼ Bern(P ), where α is set to 0.1 empirically  where f denotes the intermediate features. Finally, the mixed feature statistics is applied to perturb the normalized f similar to Eq. (  Different from MixStyle, we replace the batch-wise fusion with channel-wise mixing, which avoids the sampling preference and introduces original-feature reference, so as to learn the domain-invariant representations explicitly."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,2.3,Training and Inference,"Let D s = {(x di , y di ) N d i=1 } K d=1 be a set including K source domains, where x di is the i-th image in the d-th source domain, and y di is the corresponding segmentation mask of x di . Our goal is to train a segmentation model that can generalize well to an unseen target domain During training, we empirically set a probability of 0.5 to activate TriD in the forward pass  Inference. During inference, all the TriD modules are removed, and the segmentation network is tested on the unseen target domain D t ."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,3,Experiments and Results,
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,3.1,Datasets and Evaluation Metrics,"Two datasets are used for this study, whose details are summarized in Table  The first dataset contains 116 MRI cases from six domains for prostate segmentation "
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,3.2,Implementation Details,"We set the mini-batch size to 8 and adopt the SGD optimizer with a momentum of 0.99 for both tasks. The initial learning rate l 0 is set to 0.01 (prostate) and 0.001 (OD/OC) respectively and decays according to the polynomial rule l t = l 0 × (1t/T ) 0.9 , where l t is the learning rate of the t-th epoch and T is the number of total epochs that is set to 200 for prostate segmentation and 100 for joint segmentation of OD and OC. For both tasks, the leave-one-domain-out strategy was used to evaluate the performance of each DG method, i.e., training on K-1 source domains and evaluating on the left domain. We consistently apply the above implementation settings to our TriD and other competing methods."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,3.3,Results,"Comparing to Other DG Methods. We used the same segmentation network and loss function to compare our TriD with seven DG methods, including (1) DCAC: dynamic structure  Location of TriD. To discuss where to apply our TriD, we repeated the experiments in joint segmentation of OD and OC and listed the results in Table  Uniform Distribution vs. Normal Distribution. It is particularly critical to choose the distribution from which to randomly sample the augmented statistics. To verify the advantages of uniform distribution, we repeated the experiments in joint segmentation of OD and OC by replacing the uniform distribution with a normal distribution N (0.5, 1) and compared the effectiveness of them in Table  It shows that the normal distribution indeed results in performance drop due to the limited search space."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,4,Conclusion,"We proposed the TriD, a domain-randomization based multi-source domain generalization method, for medical image segmentation. To solve the limitations existing in preview methods, TriD perturbs the intermediate features with two steps: (1) SR: randomly sampling the augmented statistics from a uniform distribution to expand the search space of feature styles; (2) SM: mixing the feature styles for explicit domain-invariant representation learning. Through extensive experiments on two medical segmentation tasks with different modalities, the proposed TriD is demonstrated to achieve superior performance over the baselines and other state-of-the-art DG methods."
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Fig. 1 .,
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Fig. 2 .,
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Fig. 3 .,
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Table 1 .,
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Table 2 .,
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Table 3 .,
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Table 4 .,
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,,
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Table 5 .,
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Table 6 .,
Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_9.
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,1,Introduction,"Magnetic resonance imaging (MRI) plays a pivotal role in knee clinical examinations. It provides a non-invasive and accurate way to visualize various internal knee structures and soft tissues  Although multi-view 2D MRI scanning can provide sufficient image quality for clinical diagnosis in most cases  The cross-view consistency of multi-view knee MR scans provides the basis for generating 3D segmentation from 2D scans, as shown in Fig.  In this paper, we propose a novel framework, named Cross-view Aligned Segmentation Network (CAS-Net), to generate 3D knee segmentation from clinical multi-view 2D scans. Moreover, following the convention of radiologists, we require the supervising annotation for the sagittal segmentation only. We first align the multi-view knee MRI scans into an isotropic 3D volume by superresolution. Then we sample multi-view patches and construct a knee graph to cover the whole knee joint. Next, we utilize a graph-based network to derive a fine 3D segmentation. We evaluate our proposed CAS-Net on the Osteoarthritis Initiative (OAI) dataset, demonstrating its effectiveness in cross-view 3D segmentation.  "
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,2,Method,"The goal of CAS-Net is to generate 3D knee segmentation with multi-view 2D scans and sagittal segmentation annotation. The overall architecture is shown in Fig.  1 Knee Graph Construction. We conduct super-resolution on multi-view 2D scans and align them in a 3D volume. Then, we sample multi-view patches along bone-cartilage interfaces and construct a knee graph. 2 Graph-based Segmentation Network. We deal with the 3D multi-view patches on the knee graph by a 3D UNet "
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,2.1,Knee Graph Construction,"The 2D clinical multi-view scans are acquired from non-orthogonal angles and have large differences in inter-and intra-slice spacing (e.g. 3.5 mm v.s. 0.3 mm in clinical practice). Therefore, we use super-resolution to unify them into one 3D volume with a pseudo segmentation (with tolerable errors), and sample the volume into graph representation. Isotropic Super-Resolution. We first apply super-resolution  Specifically, we follow  Moreover, we train a 2D nnUNet  We start extracting points on the bone surface by calculating the bone boundaries on the pseudo-3D segmentation. Then, we uniformly sample from these points until the average distance between them is about 0.8 times the patch size. These sampled points are collected as the vertices V in the graph. V stores the center-cropped multi-view patches P and their WCS positional coordinates C. The distribution of the sampled points ensures that multi-view patches centercropped on V can fully cover the bone surfaces and bone-cartilage interfaces. Finally, we connect the vertices in V using edges E, which enables the vertex features to be propagated on the knee graph. The edges are established by connecting each vertex to its k (k = 10, following "
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,2.2,Graph-Based Segmentation Network,"Graph-Based UNet. The knee graph, which is constructed early, is input to a UNet combined with a 3-layer Graph Transformer Network (GTN)  To encode the multi-view patches P as vertex features H in the bottleneck of the UNet, we first apply average pooling to H and then add the linearlyprojected WCS coordinate C to enable convolution of H along the graph. Specifically, we compute H (0) = AvgPooling(H) + Linear(C). Next, we pass the resulting vertex features H (0) through a graph convolutional network. Note that the knee graphs have varying numbers of vertices and edges, we utilize the Graph Transformer Networks (GTN) to adapt it. Similar to Transformers  where α is the self-attention, denotes dot product, and d is the length of attention embeddings. Then, the local feature H (i) in the i-th layer can be computed as H (i) = σ(αV ), where σ is feedforward operation. After the attention-based graph representation, H (3) is repeated to match the shape of H and then summed up, serving as the input to the UNet decoder. And the UNet decoder derives the 3D patch segmentation according to H (3)  and skip connection. We further project the patch segmentations back into the pseudo-3D segmentation, thereby obtaining a rectified 3D knee segmentation."
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,,Random View Masking.,"To reduce impacts of the errors in the initial pseudo-3D segmentation, we randomly masked out views (channels) in the multi-view (multi-channel) patches during training. The 3D volume, obtained from 2D scans by super-resolution, has a clearer tissue texture in the plane parallel to its original scanning plane. Thus, the random view masking helps to produce more accurate segmentation boundaries by forcing the network to learn from different combinations of patch views (channels). We set the probability of each view to be masked as 0.25. During inference, complete multi-view patches are used, reducing errors caused by the pseudo-3D segmentation."
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,3,Experimental Results,
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,3.1,Data and Experimental Settings,"We have tested the CAS-Net on the Osteoarthritis Initiative (OAI) dataset  In the following, we refer to the femur bone and tibia bone as FB and TB for short, their attached cartilages as FC and TC, and meniscus as M. We use GTN and RM to denote the Graph Transformer Network and random masking strategies, respectively. SR and NN stand for super-resolution and nearest neighbor interpolation. To ensure the fairness of the comparison, we use the same training and testing settings for all experiments: PyTorch1.9.0, RTX 3090, learning rate is 3e-4, 100 epochs, cosine learning rate decay, Adam optimizer."
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,3.2,Ablation Study,"To find the optimal setting for the CAS-Net, we apply different hyper-parameters on it and test them on cross-view 2D segmentation. We evaluate the performance of the CAS-Net with different patch size, and then remove the GTN, random masking, and super-resolution to see the effect of each component. To ensure the multi-view slices can be aligned in the same space without super-resolution, we use trilinear interpolation as the alternative. The results are shown in Table "
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,3.3,Evaluation of 3D Segmentation Performance,"We report the performance of the CAS-Net for 3D knee segmentation. To the best of our knowledge, CAS-Net is the first method that can perform knee segmentation from multi-view 2D scans and sagittal segmentation annotation. Therefore, we compare CAS-Net with the following settings: (1) the sagittal annotation that is resampled to 3D DESS space by nearest neighbor interpolation; (2) the pseudo-3D segmentation generated in Sect. 2.1; (3) the 3D prediction from 3D nnUNet based on 3D DESS images and annotations, which is considered as the upper bound for this task. As can be seen from Table "
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,3.4,Evaluation of Cross-View Projection of 3D Segmentation,"The 3D segmentation results inferred by CAS-Net can be resampled onto unannotated 2D scans to achieve cross-view segmentation. We evaluate the cross-view segmentation performance on the coronal view, which is shown in Table  (2) the pseudo-segmentation generated in Sect. 2.1 projected to the coronal view, named as Pseudo Seg. in Table  Compared with nnUNet, there is still improvement room for CAS-Net in the case of small tissues, such as meniscus (79.9% vs. 92.9% Dice ratio). This is likely caused by subtle movements of subjects during the acquisition of multi-view 2D scans. As shown in Fig. "
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,4,Conclusion,"We have proposed a novel framework named CAS-Net for generating cross-view consistent 3D knee segmentation via super-resolution and graph representation with clinical 2D multi-view scans and sagittal annotations. By creating a detailed 3D knee segmentation from clinical 2D multi-view MRI, our framework provides significant benefits to morphology-based knee analysis, with promising applications in knee disease analysis. We believe that this framework can be extended to other 3D segmentation tasks, and we intend to explore those possibilities. However, it should be noted that the performance of the CAS-Net is affected by misaligned multi-view images, as shown in Fig. "
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,,Fig. 1 .,
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,,Fig. 2 .,
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,,Fig. 3 (,
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,,Fig. 3 .,
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,,( 3 ),
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,,Fig. 4 .,
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,,Resample Projection Knee Graph Construction Graph-Based Segmentation Network,
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,,Table 1 .,
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,,Table 2 .,
CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees,,Table 3 .,
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,1,Introduction,"Medical image segmentation is an essential task in clinical practice, enabling accurate diagnosis, treatment planning, and disease monitoring. However, existing medical segmentation methods often encounter challenges related to changes in imaging protocols and variations in patient populations. These challenges can significantly impact the performance and generalizability of segmentation models. For instance, a segmentation model trained on MRI images from a specific S. Bera and V. Ummadi-These authors contributed equally to this work and share the first authorship. patient population may not perform well when applied to a different population with distinct demographic and clinical characteristics. Similarly, variations in imaging protocols, such as the use of different contrast agents or imaging parameters, can also affect the model's accuracy and reliability. To ensure accurate segmentation, it is necessary to retrain or fine-tune the model with current data before deploying it. However, this process often leads to catastrophic forgetting, where the model loses previously acquired knowledge while being trained on the current data. Catastrophic forgetting occurs due to the neural network's inability to learn from a continuous stream of data without disregarding previously learned information. Retraining the network using the complete training set, including both old and current data, is not always feasible or practical due to reasons such as the unavailability of old data or data privacy concerns. Moreover, training the network from scratch every time for every perturbation is a resource-intensive and time-sensitive process. Continual learning aims to address this limitation of catastrophic forgetting by enabling the model to learn continuously from a stream of incoming data without the need to retrain the model from scratch. Continual learning algorithms have gained significant interest lately for computer vision tasks like image denoising, superresolution, and image classification. However, the development of efficient continual learning algorithms specifically designed for medical image segmentation has been largely overlooked in the literature. To address the above gap, our study proposes a continual learning based approach for medical image segmentation, which can be used to train any backbone network. In our approach, we leverage the recently proposed concept of the memory replay-based continual learning (MBCL)  We propose a simple yet effective algorithm for image selection while creating the memory bank. Two different ranking mechanisms, which address the bottlenecks related to medical images discussed above, are proposed to rank all the images present in the training set. Then, images to be stored in the memory bank are selected using a combined ranking. Further, we suggest the cropping of the images around the organ of interest in order to minimize the size of the memory bank. An extensive evaluation is performed on three different problems, i.e., continual prostrate segmentation, continual hippocampus segmentation, and task incremental segmentation of the prostate, hippocampus and spleen. We consider several baselines including EWC "
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,2,Proposed Methodology,"We are given a sequential stream of images from K sites, which are sequentially used to train a segmentation model. In a round k ∈ [1, K] of this continual learning procedure, we can only obtain images and ground truths {(x k,i , y k,i )} n k i=1 from a new incoming site (dataset) D k without access to old data from previous sites D k-1 . Due to catastrophic forgetting, this type of sequential learning results in a drop in performance for all the previous sites (≤ D k-1 ) after training with images from D k site as the parameters of the previous site or task are overwritten while learning a new task. In naive memory replay-based continual learning, a memory buffer, M, is used to store a small number of examples of past sites (≤ D k-1 ), which can be used to train the model along with the new data. Unlike other tasks like image classification or image restoration, for downstream tasks like medical image segmentation, the selection of images for storing in the M is very crucial. A medical image segmentation (like hippocampus segmentation) approach typically has a very small target organ. It is very likely that randomly selected images for storage in the M will have an under-representation of the positive (hippocampus) class. Further, the contribution of each training sample is not equal towards the learning, as a network usually learns more from examples that are challenging to segment. Based on the above observations, we propose two image ranking schemes to sort the images for storing in M (see Fig. "
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,2.1,Positive Class Based Ranking (PCR),"In this ranking scheme, we rank an input image volume according to the percentage of voxels corresponding to the positive class available in the volume. Let cr k,i be the positive class based ranking score for the sample input-ground truth pair (x k,i , y k,i ) from the dataset D k . We use the ground truth label y k,i to calculate the score of each volume. Let H, W and D respectively be the height, width and number of slices in the 3D volume of y k,i . The voxel value at location (h, w, d) in the ground truth label y k,i is represented by y k,i h,w,d ∈ 0, 1. If the voxel value at a location (h, w, d) is equal to 1, then the voxel belongs to the positive class. Let us use |y k,i | to represent the total number of voxels in the 3D volume. For a sample pair (x k,i , y k,i ), cr k,i is computed as follows: The rationale behind this ranking scheme is that by selecting volumes with a higher positive class occupancy, we can minimize the risk of underrepresentation of the positive class leading to a continuously trained network that remembers previous tasks more faithfully."
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,2.2,Gradient Based Ranking (GBR),"Here, we intend to identify the examples which the segmentation network finds hard to segment. In this ranking, we leverage the relationship between example difficulty and gradient variance, which has been previously observed in other studies "
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,2.3,Gradient Plus Class Score (GPCS) Sampling for Memory,"Once the score gr k,i and cr k,i are available for a dataset D k , they are normalized to [0, 1] range and stored for future use. If p samples are to be selected for the replay memory M, then p 2 will be selected using gr k,i and other p 2 using cr k,i . However, we do not store entire image volumes in the memory. We propose a straightforward yet effective strategy to optimally utilize the memory bank size for continual medical segmentation tasks. Typically, the organ of interest in these tasks occupies only a small portion of the entire image, resulting in significant memory wastage when storing the complete volume. For example, the hippocampus, which is a common region of interest in medical image segmentation, occupies less than 0.5% of the total area. We propose to store only a volumetric crop of the sequence where the region of interest is present rather than the complete volume. This enables us to make more efficient use of the memory, allowing significant amounts of memory for storing additional crops within a given memory capacity. Thus, we can reduce memory wastage and optimize memory usage for medical image segmentation. Consider that an image-label pair (x k,i , y k,i ) from a dataset D k has dimensions H ×W ×D (height×width×sequence length). Instead of a complete imagelabel pair, a crop-sequence of dimension h c ×w c ×d c with h c ≤ H, w c ≤ W, d c ≤ D is stored. We also use an additional hyperparameter, foreground background ratio fbr ∈ [0, 1] to store some background areas, as described below fbr ="
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,,Number of crops having RoI Number of crops not having RoI,"(2) Using only foreground regions in problems such as task incremental learning may result in a high degree of false positive segmentation. In such cases, having a few crops of background regions helps in reducing the forgetting in background regions as discussed in Sect. 3.3."
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,3,Experimental Details,
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,3.1,Datasets,"We conduct experiments to evaluate the effectiveness of our methods in two different types of incremental learning tasks. To this end, we use seven openly available datasets for binary segmentation tasks: four prostate datasets (Prostate158 "
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,3.2,Evaluation Metrics,"To evaluate the effectiveness of our proposed methods against baselines, we use the segmentation evaluation metric Dice Similarity Coefficient (DSC) along with standard continual learning (CL) metrics. The CL metrics "
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,3.3,Training Details,"In this work, we consider a simple segmentation backbone network UNet, which is widely used in medical image segmentation. Both the proposed and baseline methods were used to train a Residual UNet "
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,4,Results and Discussion,Ablation Study of Our Atypical Sample Selection:  Visually analyzing the predictions for a test sample in Fig. 
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,5,Conclusion,This paper proposes a novel approach to address the challenge of catastrophic forgetting in medical image segmentation using continual learning. The paper presents a memory replay-based continual learning paradigm that enables the model to learn continuously from a stream of incoming data without the need to retrain from scratch. The proposed algorithm includes an effective image selection method that ranks and selects images based on their contribution to the learning process and faithful representation of the task. The study evaluates the proposed algorithm on three different problems and demonstrates significant performance improvements compared to several relevant baselines.
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,,Fig. 1 .,
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,,Table 1 .,
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,,Table 2 .,
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,,Table 3 .,
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,,Table 4 .,
Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 49.
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,1,Introduction,"Benefiting from the prominent ability to model long-range dependency, transformers have become the de-facto standard for natural language processing  Inspired by this, transformers are introduced into medical image segmentation and arouse wide concerns  In this work, we propose a plug-and-play module named ConvFormer to address attention collapse by constructing a kernel-scalable CNN-style transformer. In ConvFormer, 2D images can directly build sufficient long-range dependency without being split into 1D sequences. Specifically, corresponding to tok-enization, self-attention, and feed-forward network in vanilla vision transformers, ConvFormer consists of pooling, CNN-style self-attention (CSA), and convolutional feed-forward network (CFFN) respectively. For an input image/feature map, its resolution is first reduced by applying convolution and max-pooling alternately. Then, CSA builds appropriate dependency for each pixel by adaptively generating a scalable convolutional, being smaller to include locality or being larger for long-range global interaction. Finally, CFFN refines the features of each pixel by applying continuous convolutions. Extensive experiments on three datasets across five state-of-the-art transformer-based methods validate the effectiveness of ConvFormer, outperforming existing solutions to attention collapse."
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,2,Related Work,"Recent transformer-based approaches for medical image analysis mainly focus on introducing transformers for robust features exaction in the encoder, crossscale feature interactive in skip connection, and multifarious feature fusion in the decoder "
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,3,Method,The comparison between the vision transformer (ViT) and ConvFormer is illustrated in Fig. 
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,3.1,Pooling vs. Tokenization,"The pooling module is developed to realize the functions of tokenization (i.e., making the input suitable to transformers in the channel dimension and shaping and reducing the input size when needed) while without losing details in the grid lines in tokenization. For an input X in ∈ R c×H×W , convolution with a kernel size of 3 × 3 followed by batch normalization and Relu, is first applied to capture local features. Then, corresponding to each patch size S in ViT, total d = log 2 S downsampling operations are applied in the pooling module to produce the same resolutions. Here, each downsampling operation consists of a max-pooling with a kernel size of 2 × 2 and a combination of 3 × 3 convolution, batch normalization, and Relu. Finally, through the pooling module where c m is corresponding to the embedding dimension in ViT."
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,3.2,CNN-Style vs. Sequenced Self-attention,"The building of long-range dependency in ConvFormer is relying on CNN-style self-attention, which creates an adaptive receptive field for each pixel by constructing a customized convolution kernel. Specifically, for each pixel x i, j of X 1 , the convolution kernel A i, j is constructed based on two intermediate variables: where E q and E k ∈ R c q ×c m ×3×3 are the learnable projection matrices and c q is corresponding to the embedding dimension of Q, K, and V in ViT, which incorporates the features of adjacent pixels in 3 × 3 neighborhood into x i, j . Then, the initial customized convolutional kernel 2 d for x i, j is calculated by computing the cosine similarity: Here, m,n corresponds to attention score calculation in ViT (constrained to be positive while I i, j m,n can be either positive or negative). Then, we dynamically determine the size of the customized convolution kernel for x i, j by introducing a learnable Gaussian distance map M: where θ ∈ (0, 1) is a learnable network parameter to control the receptive field of A and α is a hyper-parameter to control the tendency of the receptive field. θ is proportional to the receptive field. For instance, under the typical setting H = W = 256, d = 3, and α = 1, when θ = 0.003, the receptive field only covers five adjacent pixels, when θ > 0.2, the receptive field is global. The larger α is, the more likely A tends to have a global receptive field. Based on I i, j and M i, j , A i, j is calculated by A i, j = I i, j × M i, j . In this way, every pixel x i, j has a customized size-scalable convolution kernel A i, j . By multiplying A with V, CSA can build adaptive long-range dependency, where V can be formulated similarly according to Eq. ( "
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,3.3,Convolution Vs. Vanilla Feed-Forward Network,"The convolution feed-forward network (CFFN) is to refine the features produced by CSA, just consisting of two combinations of Approaches outperforming the state-of-the-art 2D approaches on the publiclyavailable ACDC (i.e., FAT-Net  ICH. A locally-collected dataset for hematoma segmentation. Totally 99 CT scans consisting of 2648 slices were collected and annotated by three radiologists. The dataset is randomly divided into the training, validation, and testing sets according to a ratio of 7:1:2. Implementation Details. For a fair comparison, all the selected state-of-theart transformer-based baselines were trained with or without ConvFormer under the same settings. All models were trained by an Adam optimizer with a learning rate of 0.0001 and a batch size of 4 for 400 rounds. Data augmentation includes random rotation, scaling, contrast augmentation, and gamma augmentation."
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,4.2,Results,"ConvFormer can work as a plug-and-play module and replace the vanilla transformer blocks in transformer-based baselines. To evaluate the effectiveness of ConvFormer, five state-of-the-art transformer-based approaches are selected as backbones, including SETR "
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,5,Conclusions,"In this paper, we construct the transformer as a kernel-scalable convolution to address the attention collapse and build diverse long-range dependencies for efficient medical image segmentation. Specifically, it consists of pooling, CNNstyle self-attention (CSA), and convolution feed-forward network (CFFN). The pooling module is first applied to extract the locality details while reducing the computational costs of the following CSA module by downsampling the inputs. Then, CSA is developed to build adaptive long-range dependency by constructing CSA as a kernel-scalable convolution, Finally, CFFN is used to refine the features of each pixel. Experimental results on five state-of-the-art baselines across three datasets demonstrate the prominent performance of ConvFormer, stably exceeding the baselines and comparison methods across three datasets."
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,,Fig. 1 .,
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,,Fig. 2 .,
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,,Fig. 3 .,
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,,Table 1 .,
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,,Table 2 .,
ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_61.
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,1,Introduction,"Deep learning can significantly reduce the workload of medical specialists during image segmentation tasks, which are essential for patient diagnosis and follow-up management  In general, one of the issues of these and other deep learning segmentation methods is that their performance strongly depends on the amount of annotated data  A possible alternative is to pre-train the models using a self-supervised learning (SSL) paradigm  Contributions. In this work, we propose a novel approach for label-efficient 3D→2D segmentation. In particular, our contributions are as follows: (1) As an alternative to state-of-the-art network architectures, we propose a 3D→2D segmentation CNN based on ReSensNet  Clinical Background. Geographic atrophy (GA) is an advanced form of agerelated macular degeneration (AMD) that corresponds to a progressive loss of retinal photoreceptors and leads to irreversible visual impairment. GA is typically assessed with OCT and/or fundus autofluorescence (FAF) imaging modalities  Reticular pseudodrusen (RPD) are accumulations of extracellular material that commonly occur in association with AMD. In OCT scans, these lesions are shown as granular hyperreflective deposits situated between the RPE layer and the ellipsoid zone. SLO visualizes RPD as a reticular pattern of iso-reflective round lesions surrounded by a hyporeflective border (see Fig. "
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,2,Methods and Experimental Setup,"The proposed approach, illustrated in Fig.  Network Architecture. The proposed network architecture (Fig.  Datasets. Experiments were performed using three datasets (Table  Training and Evaluation Details. OCT volumes were flattened along the Bruch's membrane, rescaled depth-wise to 128 voxels, and then Z-score normalized along the cross-sectional plane. To make FAF and GA masks more similar and thus facilitate fine-tuning, FAF images were inverted. In all cases, models were trained for 800 epochs using SGD with a learning rate of 0.1 and a momentum of 0.9. Batch size was set to 4 for reconstruction and 8 for segmentation. All datasets were split patient-wise into training (60%), validation (10%) and test (30%). For reconstruction, models were trained on GA-M. For GA segmentation, they were trained/fine-tuned on GA-S-1 and evaluated on GA-S-1, GA-S-2 and GA-M-S. For RPD segmentation, RPD-S was used. To evaluate the performance under label scarcity, we train with 5%, 10%, 20% and 100% of the data in GA-S, and 20% and 100%, in RPD-S. More details about the hardware used and the carbon footprint of our method are included in the Supplement. To reduce inference variability, we average the predictions of the top-5 checkpoints of the models in terms of Dice (validation). Segmentations are evaluated via Dice and absolute area difference (Area diff.) of predicted and manual masks."
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,3,Results and Discussion,Baseline Comparison. We compared our approach to current state-of-the-art methods (IPN  Reconstructed Modality Effect. We also conducted experiments to assess the effect of the reconstructed modality (SLO and FAF). Figure 
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,4,Conclusions,"Labeled data scarcity is one of the main limiting factors for the application of deep learning in medical imaging. In this work, we have proposed a new model and SSL strategy for label-efficient 3D→2D segmentation. The proposed approach was validated in two tasks with clinical relevance: the en-face segmentation of GA and RPD in OCT. The results demonstrate that: (1) the proposed CNN architecture clearly outperforms the state of the art when there is limited annotated data, (2) regardless of the architecture and the modality to be reconstructed, the proposed SSL strategy improves the performance of the models on the target tasks in those cases; (3) despite the greater diagnostic utility of FAF over SLO, SSL with FAF does not always result in a significant gain in model performance, with the advantage of the latter not requiring a supplementary registration method. On the other hand, although the proposed approach shows promising results in the en-face segmentation of RPD, further evaluation is needed. Based on our findings, we believe that the proposed approach has the potential to be used in other common 3D→2D tasks, such as the prediction of retinal sensitivity in OCT, the segmentation of different structures in OCT-A, or the segmentation of intravascular ultrasound (IVUS). In addition, we also believe that the proposed SSL strategy could be easily extended to other imaging domains, such as magnetic resonance, where multi-modal data is widely used."
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,,Fig. 1 .,
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,,Fig. 2 .,
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,,Fig. 3 .,
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,,Fig. 4 .,
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,,Fig. 5 .,
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,,Table 1 .,
Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_56.
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,1,Introduction,"Hand imaging examination is a standard clinical procedure commonly utilized for various medical purposes such as predicting biological bone age  To address this challenge, deep learning-based US image segmentation methods have been explored. For instance, Liu et al.  Fine-grained segmentation demands a model to maintain the inter-slice anatomical relationships while extracting intra-slice detailed features. The 2D convolution excels at capturing dense information but lacks inter-slice information, while the 3D convolution is complementary  To overcome the deficiencies of existing methods, this study proposes a novel Adaptive Multi-dimensional Convolutional Network (AMCNet) with an anatomy-constraint loss for fine-grained hand bone segmentation. Our contribution is three-fold. 1) First, to the best of our knowledge, this is the first work to address the challenge of automatic fine-grained hand bone segmentation in 3D US volume. We propose a novel multi-dimensional network to tackle the issue of multiple categories and insignificant feature differences. 2) Second, we propose an adaptive multi-dimensional feature fusion mechanism to dynamically adjust the weights of 2D and 3D convolutional feature layers according to different objectives, thus improving the fine-grained feature representation of the model. 3) Finally, we propose an anatomy-constraint loss that minimizes the anatomical error and mines hard samples, further improving the performance of the model."
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,2,Methods,
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,2.1,Network Design,As shown in Fig. 
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,2.2,Adaptive 2D/3D Convolutional Module (ACM),"To enable the model to capture the inter-layer anatomical connection and intralayer dense semantic feature, the ACM is proposed to adaptively fuse the 2D and 3D convolution at different levels. Figure  where w, h, and d are the width, height, and depth of the input feature map, respectively. Further, a local cross-channel information interaction attention mechanism is applied for the fusion of multi-dimensional convolutional features. Specifically, the feature map F G is squeezed to a one-dimension tensor of length c, which is the number of channels, and then a one-dimensional (1D) convolution with a kernel size of K is applied for information interaction between channels. The obtained feature layer is re-expanded into a 3D feature map F G , which can be expressed as: where C1D K denotes the 1D convolution with the kernel size of K, G S and G U denote the operation of squeezing and re-expanding respectively. To adaptively select the feature information from different convolutions, the softmax operation is performed channel-wise to compute the weight vectors α and β corresponding to F 2D and F 3D respectively, which can be expressed as: where A, B ∈ R c×c denote the learnable parameters, A i and B i denote to the i-th row of A and B respectively, α i and β i denote to i-th element of α and β respectively. where F o denotes the output feature map of the ACM."
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,2.3,Anatomy-Constraint Loss,"Fine-grained hand bone segmentation places stringent demands on the anatomical relationships between categories, but the lack of supervision during model training renders it the primary source of segmentation error. For instance, the epiphysis is highly prone to neglect due to its small and imbalanced occupation, while the index and ring phalanx bones can easily be mistaken due to their symmetrical similarities. To address this issue, we propose the anatomy-constraint loss to facilitate the model's learning of anatomical relations. Anatomical errors occur when pixels significantly deviate from their expected anatomical locations. Thus, we utilize the loss to compute and penalize these deviating pixels. Assume that the Y and P are the label and segmentation map respectively. First, the map representing anatomical errors is generated, where only pixels in the segmentation map that do not correspond to the anatomical relationship are activated. To mitigate subjective labeling errors caused by the unclear boundaries in US images, we perform morphological dilation on the segmentation map P. This operation expands the map and establishes an error tolerance, allowing us to disregard minor random errors and promote training stability. To make it differentiable, we implement this operation with a kernel=3 and stride=1 max-pooling operation. Subsequently, the anatomical error map F E is computed by pixel-wise subtracting the Y and the expanded segmentation map P. The resulting difference map is then activated by ReLU, which ensures that only errors within the label region and beyond the anatomically acceptable range are penalized. The process can be expressed as: where Y Ci and P Ci denote the i-th category maps of the label Y and segmentation map P respectively, G mp (•) denotes the max-pooling operation, and σ R (•) denotes the ReLU activation operation. Next, we intersect F E with the segmentation map P and label Y, respectively, based on which the cross entropy is computed, which is used to constrain the anatomical error: where Loss AC (•) denotes the proposed anatomy-constraint loss, Loss CE (•) denotes the cross-entropy loss, and denotes the intersection operation. To reduce the impact of class imbalance on model training and improve the stability of segmentation, we use a combination of Dice loss and anatomyconstraint loss function: where L is the overall loss, Loss Dice denotes the Dice loss, and γ denotes the weight-controlling parameter of the anatomy-constraint loss. 3 Experiments and Results"
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,3.1,Dataset and Implementation,"Our method is validated on an in-house dataset, which consists of 103 3D ultrasound volumes collected using device IBUS BE3 with a 12MHz linear transducer from pediatric hand examinations. The mean voxel resolution is 0.088×0.130×0.279 mm 3 and the mean image size is 512×1023×609. Two expert ultrasonographers manually annotated the data based on ITK-snap  The training and test phases of the network were implemented by PyTorch on an NVIDIA GeForce RTX 3090 GPU. The network was trained with Adam optimization with momentum of 0.9. The learning rate was set as 10e-3. For the hyperparameter, the kernel size K of 1D convolution in ACM was set as 3 and the weight-controlling parameter γ was set as 0.5. We used the Dice coefficient (DSC), Jaccard Similarity (Jaccard), Recall, F1-score, and Hausdorff Distance (HD95) as evaluation metrics."
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,3.2,Performance Comparison,"We compared our network with recent and outstanding medical image segmentation methods, which contain UNet  Table "
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,3.3,Ablation Study,"To validate the effect of anatomy-constraint loss, we compare the results of training with Loss Dice and the combination of Loss Dice and Loss AC on both 3D UNet and the proposed AMCNet. Table "
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,3.4,Software Development and Application,"Based on the method described above, a user-friendly and extensible module was developed on the 3D Slicer platform "
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,4,Conclusion,"In this work, we have presented an adaptive multi-dimensional convolutional network, called AMCNet, to address the challenge of automatic fine-grained hand bone segmentation in 3D US volume. It adopts an adaptive multi-dimensional feature fusion mechanism to dynamically adjust the weights of 2D and 3D convolutional feature layers according to different objectives. Furthermore, an anatomy-constraint loss is designed to encourage the model to learn anatomical relationships and effectively mine hard samples. Experiments show that our proposed method outperforms other comparison methods and effectively addresses the task of fine-grained hand bone segmentation in ultrasound volume. The proposed method is general and could be applied to more medical segmentation scenarios in the future."
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,,Fig. 1 .,
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,,Fig. 2 .,
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,,Fig. 3 .,
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,,Table 1 .,
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,,Table 2 .,
Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_38.
Diffusion Transformer U-Net for Medical Image Segmentation,1,Introduction,"Deep Learning (DL) methods like Convolutional Neural Networks (CNN) and Vision-Transformers (ViT) have been applied to medical image segmentation  Recently, Denoising Diffusion Probabilistic Model (DDPM)  Motivated by the underlined limitations, we propose a Diffusion Transformer U-Net, with the following contributions: -A conditional diffusion model with forward and backward processes is proposed to train segmentation networks. In the backward denoising process, the feature embedding of a noise image is aligned with that of the conditional source image by a new cross-attention module. Then, it is denoised into a segmentation mask of the source image by the segmentation network. -A transformer-based U-Net with multi-sized windows, named as MT U-Net, is designed to extract both pixel-level and global contextual features for achieving good segmentation performance. -The MT U-Net trained by the diffusion model has a great generalization capability on various imaging modalities, and outperforms all the current state-of-the-art on five benchmark datasets including polyp segmentation from colonoscopy images "
Diffusion Transformer U-Net for Medical Image Segmentation,2,Method,
Diffusion Transformer U-Net for Medical Image Segmentation,2.1,Diffusion Model,"The diffusion has two processes (Fig.  where f p M is the query (Q), the concatenation of f p M and f p I is the key (K) and value (V ). denotes the transpose. Fourthly, following "
Diffusion Transformer U-Net for Medical Image Segmentation,2.2,Multi-sized Transformer U-Net (MT U-Net),The proposed Multi-sized Transformer (MT) module (Fig. 
Diffusion Transformer U-Net for Medical Image Segmentation,2.3,Training and Inference,"During training, a source image and its segmentation ground truth map are given as input to the diffusion model. The diffusion model is trained using the noise prediction loss (L Noise )  During inference, a noise image sampled from the Gaussian distribution, along with the testing image, is given as the input to the reverse process."
Diffusion Transformer U-Net for Medical Image Segmentation,3,Experimental Results,
Diffusion Transformer U-Net for Medical Image Segmentation,3.1,Datasets and Evaluation Metrics,"To evaluate the effectiveness and generalization ability of the proposed method, different medical image segmentation tasks are tested, including: (1) Polyp segmentation from colonoscopy images (Kvasir-SEG (KSEG) "
Diffusion Transformer U-Net for Medical Image Segmentation,3.2,Implementation Details,"The number of branches in the MT module is set to 3 by cross-validation, with window sizes as 4, 8, and 16 respectively. The diffusion transformer U-Net is trained for 40, 000 iterations using SGD optimizer with a momentum of 0.6, with a batch size of 16, and the learning rate is set to 0.0005. In the diffusion, we use a linear noise scheduler with T = 1000 steps. For fair comparisons with the recent diffusion-based segmentation models "
Diffusion Transformer U-Net for Medical Image Segmentation,3.3,Performance Comparison,"First, we quantitatively compare our method with several well-known U-Net and/or Transformer-related segmentation models, including U-Net "
Diffusion Transformer U-Net for Medical Image Segmentation,3.4,Ablation Study,"We perform a set of ablation studies to evaluate the contribution of each module in our Diffusion Transformer U-Net, as shown in Table "
Diffusion Transformer U-Net for Medical Image Segmentation,4,Conclusion,"A Diffusion Transformer U-Net is proposed for medical image segmentation. Instead of a standard U-Net in the diffusion model, we propose a transformer based U-Net with multi-sized windows for enhancing the contextual information extraction and reconstruction. We also design a cross-attention module to align feature embeddings, providing a better conditioning from the source image to the diffusion model. The evaluation on various datasets of different modalities shows the effectiveness and generalization ability of the proposed method."
Diffusion Transformer U-Net for Medical Image Segmentation,,Fig. 1 .,
Diffusion Transformer U-Net for Medical Image Segmentation,,Fig. 2 .,
Diffusion Transformer U-Net for Medical Image Segmentation,,Figure 3 (,
Diffusion Transformer U-Net for Medical Image Segmentation,,Fig. 3 .,
Diffusion Transformer U-Net for Medical Image Segmentation,,Fig. 4 .,
Diffusion Transformer U-Net for Medical Image Segmentation,,Table 1 .,
Diffusion Transformer U-Net for Medical Image Segmentation,,Table 2 .,
Diffusion Transformer U-Net for Medical Image Segmentation,,Table 3 .,
Diffusion Transformer U-Net for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 59.
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,1,Introduction,"With the recent development of semi-supervised learning (SSL)  To address long-tail medical segmentation, our motivations come from the following two perspectives in CL training schemes "
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,2,Method,
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,2.1,Overview,"Problem Statement. Given a medical image dataset (X, Y ), our goal is to train a segmentation model F that can provide accurate predictions that assign each pixel to their corresponding K-class segmentation labels. Setup. Figure  where τ s and τ t are two temperature parameters. Finally, we minimize the unsupervised instance discrimination loss (i.e., Kullback-Leibler divergence KL) as: We formally summarize the pretraining objective as the equal combination of the global and local L inst , and supervised segmentation loss L sup (i.e., equal combination of Dice loss and cross-entropy loss). Anatomical Contrast Fine-Tuning. The underlying motivation for the finetuning stage is that it reduces the vulnerability of the pre-trained model to long-tailed unlabeled data. To mitigate the problem,  Then,  where C denotes a set of all available classes in the current mini-batch, and τ an is a temperature hyperparameter. For class c, we select a query representation set R c q , a negative key representation set R c k whose labels are not in class c, and the positive key r c,+ k which is the c-class mean representation. Given P is a set including all pixel coordinates with the same size as R, these queries and keys can be defined as: We formally summarize the fine-tuning objective as the equal combination of unsupervised L anco , unsupervised cross-entropy loss L unsup , and supervised segmentation loss L sup . For more details, we refer the reader to "
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,2.2,Supervised Adaptive Anatomical Contrastive Learning,"The general efficacy of anatomical contrast on long-tail unlabeled data has previously been demonstrated by the authors of  Anatomical Center Pre-computation. We first pre-compute the anatomical class centers in latent representation space. The optimal class centers are chosen as K positions from the unit sphere S d-1 = {v ∈ R d : v 2 = 1} in the ddimensional space. To encourage good separability and uniformity, we compute the class centers {ψ c } K c=1 by minimizing the following uniformity loss L unif : In our implementation, we use gradient descent to search for the optimal class centers constrained to the unit sphere S d-1 , which are denoted by {ψ c } K c=1 . Furthermore, the latent dimension d is a hyper-parameter, which we set such that d K to ensure the solution found by gradient descent indeed maximizes the minimum distance between any two class centers  Adaptive Allocation. As the second step, we explore adaptively allocating these centers among classes. This is a combinatorial optimization problem and an exhaustive search of all choices would be computationally prohibited. Therefore, we draw intuition from the empirical mean in the K-means algorithm and adopt an adaptive allocation scheme to iteratively search for the optimal allocation during training. Specifically, consider a batch B = {B 1 , • • • , B K } where B c denotes a set of samples in a batch with class label c, i∈Bc φ i 2 be the empirical mean of class c in current batch, where φ i is the feature embedding of sample i. We compute assignment π by minimizing the distance between pre-computed class centers and the empirical means: In implementation, the empirical mean is updated using moving average. That is, for iteration t, we first compute the empirical mean φ c (B) for batch B as described above, and then update by Adaptive Anatomical Contrast. Finally, the allocated class centers are wellseparated and should maintain the semantic relation between classes. To utilize these optimal class centers, we want to induce the feature representation of samples from each class to cluster around the corresponding pre-computed class center. To this end, we adopt a supervised contrastive loss for the label portion of the data. Specifically, given a batch of pixel-feature-label tuples where ω i is the i-th pixel in the batch, φ i is the feature of the pixel and y i is its label, we define supervised adaptive anatomical contrastive loss for pixel i as: where is the pre-computed center of class y i . The first term in Eq. 5 is supervised contrastive loss, where the summation over φ + i refers to the uniformly sampled positive examples from pixels in batch with label equal to y i . The summation over φ j refers to all features in the batch excluding φ i . The second term is contrastive loss with the positive example being the pre-computed optimal class center."
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,2.3,Anatomical-Aware Temperature Scheduler (ATS),"Training with a varying τ induces a more isotropic representation space, wherein the model learns both group-wise and instance-specific features "
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,3,Experiments,"Experimental Setup. We evaluate ACTION++ on two benchmark datasets: the LA dataset  Implementation Details. We use an SGD optimizer for all experiments with a learning rate of 1e-2, a momentum of 0.9, and a weight decay of 0.0001. Following  On ACDC, we use the U-Net model  We then theoretically show the superiority of our method in Appendix A. Finally, we conduct experiments to study the effects of cosine boundaries, cosine period, different methods of varying τ , and λ a in Table "
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,4,Conclusion,"In this paper, we proposed ACTION++, an improved contrastive learning framework with adaptive anatomical contrast for semi-supervised medical segmentation. Our work is inspired by two intriguing observations that, besides the unlabeled data, the class imbalance issue exists in the labeled portion of medical data and the effectiveness of temperature schedules for contrastive learning on longtailed medical data. Extensive experiments and ablations demonstrated that our model consistently achieved superior performance compared to the prior semisupervised medical image segmentation methods under different label ratios. Our theoretical analysis also revealed the robustness of our method in label efficiency. In future, we will validate CT/MRI datasets with more foreground labels and try t-SNE."
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,,Fig. 1 .,
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,,Fig. 2 .,
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,,,
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,,Table 1 .,
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,,,
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,,Table 2 .,
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,,Table 3 .,
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,,Table 4 .,
ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 19.
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,1,Introduction,"Integrating multi-modality medical images for tumor segmentation is crucial for comprehensive diagnosis and surgical planning. In the clinic, the consistent information and complementary information in multi-modality medical images provide the basis for tumor diagnosis. For instance, the consistent anatomical structure information offers the location feature for tumor tracking  Existing methods for multi-modality medical image integration can be categorized into three groups:  Dempster-Shafer theory (DST)  In this paper, we propose an evidence-identified DDPM (EI-DDPM) with contextual discounting for tumor segmentation via integrating multi-modality medical images. Our basic assumption is that we can learn the segmentation feature on single modality medical images using DDPM and parse the reliability of different modalities medical images by evidence theory with a contextual discounting mechanism. Specifically, the EI-DDPM first utilizes parallel conditional DDPM to learn the segmentation feature from a single modality image. Next, the evidence-identified layer (EIL) preliminarily integrates multi-modality images by comprehensively using the multi-modality uncertain information. Lastly, the contextual discounting operator (CDO) performs the final integration of multimodality images by parsing the reliability of information from multi-modality medical images. The contributions of this work are: -Our EI-DDPM achieves tumor segmentation by using DDPM under the guidance of evidence theory. It provides a solution to integrate multi-modality medical images when deploying the DDPM algorithm. -The proposed EIL and CDO apply contextual discounting guided DST to parse the reliability of information from different modalities of medical images. This allows for the integration of multi-modality medical images with learned weights corresponding to their reliability. -We conducted extensive experiments using the BraTS 2021 "
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,2,Method,"The EI-DDPM achieves tumor segmentation by parsing the reliability of multimodality medical images. Specifically, as shown in Fig. "
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,2.1,Parallel DDPM Path for Segmentation Feature Learning,"Background of DDPM: As an unconditional generative method, DDPM  The reverse process of denoising that the joint distribution p θ (x 0:T ), it can be defined as a Markov chain with learnt Gaussian transitions starting from p(x T ) = N (x T ; 0, I): where α t := 1β t , ᾱt := t s=1 α s , and Multi-modality Medical Images Conditioned DDPM: In Eq. 2 from DDPM  As shown in Fig.  where the conditional image x C ∈ {T 1, T 2, F lair, T 1ce} corresponding to the four parallel conditional DDPM path. And in each step t, the x C,t was also performed the operation of adding Gaussian noise to convert the distribution: where x C,0 presents the multi-modality medical images."
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,2.2,EIL Integrates Multi-modality Images,"As shown in Fig.  The hypothesis is to regard multi-modality images as independent and different sources knowledge. And then embedding multi-phase features into the combination rule of DST for evidence identification, which comprehensively parses the multi-phase uncertainty information for confident decision-making. The basic concepts of evidence identification come from DST  where the M(A) denotes the whole belief and evidence allocated to A. The associated belief (Bel) and plausibility (Pls) functions are defined as: And using the contour function pls to restrict the plausibility function P ls of singletons (i.e. pls(θ) = P ls({θ}) for all θ ∈ Θ)  where γ = B∩C=∅ M 1 (B)M 2 (C) is conflict degree between M 1 and M 2 . And the combined contour function P ls 12 corresponding to M 1 ⊕ M 2 is: The specific evidence identification layer mainly contains three sub-layers (i.e. activation layer, mass function layer, and belief function layer). For activation layer, the activation of i unit can be defined as: . The w i is the weight of i unit. λ i > 0 and a i are parameters. The mass function layer calculates the mass of each K classes using M i ({θ k }) = u ik y i , where K k=1 u ik = 1. u ik means the degree of i unit to class θ k . Lastly, the third layer yields the final belief function about the class of each pixel using the combination rule of DST (Eq. 8).  where M ? is a vacuous mass function defined by M(Θ) = 1, η M is a mixture of M and M ? . The coefficient η plays the role of weighting mass function η M. The corresponding contour function of η M is: Advantage: EIL and CDO parse the reliability of different modality medical images in different contexts. For example, if we feed two modality medical images like T 1 and T 2 into the EI-DDPM, with the discount rate 1η T 1 and 1η T 2 , we will have two contextual discounted contour functions ηT The combined contour function in Eq. 9 is proportional to the ηT 1 pls T 1 ηT 2 pls T 2 . In this situation, the η T 1 and η T 2 can be trained to weight the two modality medical images by parsing the reliability."
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,3,Experiment and Results,"Dataset. We used two MRI datasets that BraTS 2021  In EIL, the initial values of a i equal 0.5 and λ i is equal to 0.01. For CDO, the initial of parameter η k is equal to 0.5. With the Adam optimization algorithm, the denoising process was optimized using L 1 , and the EIL and CDO were optimized using Dice loss. The learning rate of EI-DDPM was set to 0.0001. The Quantitative and Visual Evaluation. The performance of EI-DDPM is evaluated by comparing with three methods: a classical CNN-based method (U-Net "
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,4,Conclusion,"In this paper, we proposed a novel DDPM-based framework for tumor segmentation under the condition of multi-modality medical images. The EIL and CDO enable our EI-DDPM to capture the reliability of different modality medical images with respect to different tumor regions. It provides a way of deploying contextual discounted DST to parse the reliability of multi-modality medical images. Extensive experiments prove the superiority of EI-DDPM for tumor segmentation on multi-modality medical images, which has great potential to aid in clinical diagnosis. The weakness of EI-DDPM is that it takes around 13 s to predict one segmentation image. In future work, we will focus on improving sampling steps in parallel DDPM paths to speed up EI-DDPM."
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,,Fig. 1 .,
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,,Fig. 2 .,
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,,Fig. 3 .,
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,,Table 1 .,
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,,Table 2 .,
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,,Table 3 .,
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,,Table 4 .,
Learning Reliability of Multi-modality Medical Images for Tumor Segmentation via Evidence-Identified Denoising Diffusion Probabilistic Models,,,
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,1,Introduction,"Deep convolutional neural networks (DCNNs) have significantly advanced medical image segmentation  To analyze the annotation process, it is assumed that a latent true segmentation, called meta segmentation for this study, exists as the consensus of annotators  Existing methods can be categorized into three groups. Annotator decision fusion  To address these issues, we advocate extracting the features, on which different preferences focus. Recently, Transformer  In this paper, we propose a Transformer-based Annotation Bias-aware (TAB) medical image segmentation model, which can characterize the annotator preference and stochastic errors and deliver accurate meta segmentation and annotator-specific segmentation. TAB consists of a CNN encoder, a Preference Feature Extraction (PFE) module, and a Stochastic Segmentation (SS) head. The CNN encoder performs image feature extraction. The PFE module takes the image feature as input and produces R + 1 preference-focused features in parallel for meta/annotator-specific segmentation under the conditions of R + 1 different queries. Each preference-focused feature is combined with the image feature and fed to the SS head. The SS head produces a multivariate normal distribution that models the segmentation and annotator-related error as the mean and variance respectively, resulting in more accurate segmentation. We conducted comparative experiments on a public dataset (two tasks) with multiple annotators. Our results demonstrate the superiority of the proposed TAB model as well as the effectiveness of each component. The main contributions are three-fold. (1) TAB employs Transformer to extract preference-focused features under the conditions of various queries, based on which the meta/annotator-specific segmentation maps are produced simultaneously. (2) TAB uses the covariance matrix of a multivariate normal distribution, which considers the correlation among pixels, to characterize the stochastic errors, resulting in a more continuous boundary. ( "
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,2,Method,
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,2.1,Problem Formalization and Method Overview,"Let a set of medical images annotated by R annotators be denoted by , where x i ∈ R C×H×W represents the i-th image with C channels and a size of H × W , and y ir ∈ {0, 1} K×H×W is the annotation with K classes given by the r-th annotator. We simplify the K -class segmentation problem as K binary segmentation problems. Our goal is to train a segmentation model on D so that the model can generate a meta segmentation map and R annotator-specific segmentation maps for each input image. Our TAB model contains three main components: a CNN encoder for image feature extraction, a PFE module for preference-focused feature production, and a SS head for segmentation prediction (see Fig. "
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,2.2,CNN Encoder,"The ResNet34  Moreover, f img is fed to a 1×1 convolutional layer for channel reduction, resulting in f re ∈ R d×H ×W , where d = 256."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,2.3,PFE Module,"The PFE module consists of an encoder-decoder Transformer and a multi-head attention block. Feeding the image feature f re to the PFE module, we have R+1 enhanced feature maps {f pref r } R r=0 , on which different preferences focus (r = 0 for meta segmentation and others for R annotator-specific segmentation). Note that meta segmentation is regarded as a special preference for simplicity. The Transformer Encoder is used to enhance the image feature f re . The Transformer encoder consists of a multi-head self-attention module and a feedforward network. Since the encoder expects a sequence as input, we collapse the spatial dimensions of f re and reshape it into the size of d × H W . Next, f re is added to the fixed positional encodings E pos and fed to the encoder. The output of the Transformer encoder is denoted by f E and its size is d × H W . The Transformer Decoder accepts f E and R+1 learnable queries {Q pref r } R r=0 of size d = 256 as its input. We aim to extract different preference-focused features based on the conditions provided by {Q pref r } R r=0 , which are called 'preference queries' accordingly. This decoder consists of a multi-head self-attention module for the intercommunication between queries, a multi-head attention module for feature extraction under the conditions of queries, and a feed-forward network. And it produces R + 1 features {f D r } R r=0 of size d = 256 in parallel. Multi-head Attention Block has m heads in it. It takes the output of the Transformer decoder {f D r } R r=0 as its input and computes multi-head attention scores of f D r over the output of the encoder f E , generating m attention heatmaps per segmentation. The output of this block is denoted as {f pref r } R r=0 , and the size of r=0 are individually decoded by SS head, resulting in R + 1 different preference-involved segmentation maps."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,2.4,SS Head,"The SS head aims to disentangle the annotator-related bias and produce meta and annotator-specific segmentation maps. Given an input image, we assume that the annotation distribution over annotators follows a multivariant normal distribution. Thus, we can learn the annotation distribution and disentangle the annotator-related bias by modeling it as the covariance matrix that considers pixel correlation. First, we feed the concatenation of f re and f pref r to a CNN decoder, which is followed by batch normalization and ReLU, and obtain the feature map f seg r ∈ R 32×H×W . Second, we establish the multivariant normal distribution N (μ(x), Σ(x)) via predicting the μ(x) ∈ R K×H×W and Σ(x) ∈ R (K×H×W ) 2 based on f seg r . However, the size of the covariance matrix scales with (K × H × W ) 2 , making it computationally intractable. To reduce the complexity, we adopt the low-rank parameterization of the covariance matrix [19] where P ∈ R (K×H×W )×α is the covariance factor, α defines the rank of the parameterization, D is a diagonal matrix with K × H × W diagonal elements. We employ three convolutional layers with 1 × 1 kernel size to generate μ(x), P , and D, respectively. In addition, the concatenation of μ(x) and f seg r is fed to the P head and D head, respectively, to facilitate the learning of P and D. Finally, we get R+1 distributions. Among them, N (μ MT (x), Σ MT (x)) is for meta segmentation and N (μ r (x), Σ r (x)), r = 1, 2, ..., R are for annotator-specific segmentation. The probabilistic meta/annotator-specific segmentation map (ŷ MT /ŷ r ) is calculated by applying the sigmoid function to the estimated μ MT /μ r . We can also produce the probabilistic annotator bias-involved segmentation maps ŷs MT /ŷ s r by applying the sigmoid function to the segmentation maps sampled from the established distribution N (μ MT (x), Σ MT (x))/N (μ r (x), Σ r (x))."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,2.5,Loss and Inference,"The loss of our TAB model contains two items: the meta segmentation loss L MT and annotator-specific segmentation loss L AS , shown as follows where L MT and L AS are the binary cross-entropy loss, y s is a randomly selected annotation per image, y r is the delineation given by annotator A r . During inference, the estimated probabilistic meta segmentation map ŷMT is evaluated against the mean voting annotation. The estimated probabilistic annotator-specific segmentation map ŷr is evaluated against the annotation y r given by the annotator A r ."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,3,Experiments and Results,
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,3.1,Dataset and Experimental Setup,"Dataset. The RIGA dataset  Implementation Details. All images were normalized via subtracting the mean and dividing by the standard deviation. The mean and standard deviation were counted on training cases. We set the batch size to 8 and resized the input image to 256 × 256. The Adam optimizer  is the maximum epoch. All results were reported over three random runs. Both mean and standard deviation are given. Evaluation Metrics. We adopted Soft Dice (D s ) as the performance metric. At each threshold level, the Hard Dice is calculated between the segmentation and annotation maps. Soft Dice is calculated via averaging the hard Dice values obtained at multiple threshold levels, i.e., (0.1, 0.3, 0.5, 0.7, 0.9) for this study. Based on the Soft Dice, there are two performance metrics, namely Average and Mean Voting. Mean Voting is the Soft Dice between the predicted meta segmentation and the mean voting annotation. A higher Mean Voting represents better performance on modeling the meta segmentation. The annotator-specific predictions are evaluated against each annotator's delineations, and the average Soft Dice of all annotators is denoted as Average. A higher Average represents better performance on mimicking all annotators."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,3.2,Comparative Experiments,"We compared our TAB to three baseline models and four recent segmentation models that consider the annotator-bias issue, including (1) the baseline 'Multi-Net' setting, under which R U-Nets (denoted by M r , r = 1, 2, ..., R, R = 6 for RIGA) were trained and tested with the annotations provided by annotator A r , respectively; (2) MH-UNet "
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,,Methods,
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,,A1,
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,4,Conclusion,"In this paper, we propose the TAB model to address the issue of annotatorrelated bias that existed in medical image segmentation. TAB leverages the Transformer with multiple learnable queries on extracting preference-focused features in parallel and the multivariate normal distribution on modeling stochastic annotation errors. Extensive experimental results on the public RIGA dataset with annotator-related bias demonstrate that TAB achieves better performance than all competing methods."
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,,Fig. 1 .,
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,,,
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,,,
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,,Table 1 .,
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,,Table 2 .,
Transformer-Based Annotation Bias-Aware Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_3.
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,1,Introduction,"Breast cancer is the leading cause of cancer-related fatalities among women. Currently, it holds the highest incidence rate of cancer among women in the U.S., and in 2022 it accounted for 31% of all newly diagnosed cancer cases  In the past decade, deep learning-based approaches achieved remarkable advancements in BUS tumor classification  Vision Transformer (ViT)  Accordingly, numerous prior studies introduced modifications to the original ViT network specifically designed for BUS image classification  Multitask learning leverages shared information across related tasks by jointly training the model. It constrains models to learn representations that are relevant to all tasks rather than learning task-specific details. Moreover, multitask learning acts as a regularizer by introducing inductive bias and prevents overfitting  In this study, we introduce a hybrid multitask approach, Hybrid-MT-ESTAN, which encompasses tumor classification as a primary task and tumor segmentation as a secondary task. Hybrid-MT-ESTAN combines the advantages of CNNs and Transformers in a framework incorporating anatomical tissue information in BUS images. Specifically, we designed a novel attention block named Anatomy-Aware Attention (AAA), which modifies the attention block of Swin Transformer by considering the breast anatomy. The anatomy of the human breast is categorized into four primary layers: the skin, premammary (subcutaneous fat), mammary, and retromammary layers, where each layer has a distinct texture and generates different echo patterns. The primary layers in BUS images are arranged in a vertical stack, with similar echo patterns appearing horizontally across the images. The kernels in the introduced AAA attention blocks are organized in rows and columns to capture the anatomical structure of the breast tissue. In the published literature, the closest approach to ours is the work by Iqbal et al.  where f l and f l are the output features of the MLP module and the (S)W-MSA module for block l, respectively; in the proposed Anatomy-Aware Attention (AAA) block, we redesigned the Swin blocks to enhance their ability to model both global and local features by adding an attention block based on the breast anatomy (see Fig.  Concretely, we first reconstruct the i-th feature map (y i ) by merging (M ) all patches, and afterward, we applied average pooling (AVG-P) and max pooling (MAX-P) layers with size (2, 2). The outputs of (AVG-P) and (MAX-P) layers are concatenated and up-sampled (U ) with size (2, 2) and stride (2, 2). Rowcolumn-wise kernels (A) with size (9 , 1) and "
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,2.3,Segmentation and Classification Branches/Tasks,The segmentation branch in Fig. 
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,2.4,Loss Function,"We applied a multitask loss function (L mt ) that aggregates two terms: a focal loss L F ocal for the classification task and dice loss L Dice for the segmentation task. Therefore, the composite loss function is L mt = w 1 • L F ocal + L Dice , where the weight coefficient w 1 is set to apply greater importance to the classification task as the primary task. Since in medical image diagnosis achieving high sensitivity places emphasis on the detection of malignant lesions, we employed the focal loss for the classification task to trade off between sensitivity and specificity. Because malignant tumors are more challenging to detect due to greater differences in margin, shape, and appearance in BUS images, focal loss forces the model to focus more on difficult predictions. Specifically, focal loss adds a factor (1 -p i ) γ to the cross-entropy loss where γ is a focusing parameter, resulting in In the formulation, α is a weighting coefficient, N denotes the number of image samples, t i is the target label of the i th training sample, and p i denotes the prediction. The segmentation loss is calculated using the commonly-employed Dice loss (L Dice ) function."
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,3,Experimental Results,
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,3.1,Datasets,"We evaluated the performance of Hybrid-MT-ESTAN using four public datasets, HMSS "
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,3.2,Evaluation Metrics,"For performance evaluation of the classification task, we used the following metrics: accuracy (Acc), sensitivity (Sens), specificity (Spec), F1 score, Area Under the Curve of Receiver Operating Characteristic (AUC), false positive rate (FPR), and false negative rate (FNR). To evaluate the segmentation performance, we used dice similarity coefficient (DSC) and Jaccard index (JI)."
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,3.3,Implementation Details,"The proposed approach was implemented with Keras and TensorFlow libraries. All experiments were performed on a machine with NVIDIA Quadro RTX 8000 GPUs and two Intel Xeon Silver 4210R CPUs (2.40GHz) with 512 GB of RAM. All BUS images in the dataset were zero-padded and reshaped to form square images. To avoid data leakage and bias, we selected the train, test, and validation sets based on the cases, i.e., the images from one case (patient) were assigned to only one of the training, validation, and test sets. Furthermore, we employed horizontal flip, height shift (20%), width shift (20%), and rotation (20 • C) for data augmentation. The proposed approach utilizes the building blocks of ResNet50 and Swin-Transformer-V2, pretrained on ImageNet dataset. Namely, MT-ESTAN uses pretrained ResNet50 as a base model for the five encoder blocks (the implementation details of MT-ESTAN can be found in "
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,3.4,Performance Evaluation and Comparative Analysis,"We compared the performance of Hybrid-MT-ESTAN for BUS classification to nine deep learning approaches commonly used for medical image analysis. The compared models include CNN-based, ViT-based, and hybrid approaches. CNNbased networks are SHA-MTL  We evaluated the segmentation performance of Hybrid MT-ESTAN and compared the results to five multitask approaches, including SHA-MTL "
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,3.5,Effectiveness of the Anatomy-Aware Attention (AAA) Block,"To verify the effectiveness of the Anatomy-Aware Attention (AAA) block, we conducted an ablation study that quantified the impact of the different components in Hybrid-MT-ESTAN on the classification and segmentation performance. Table  To evaluate the segmentation performance, we compared the proposed approach with and without the AAA block and Swin Transformer. As shown in Table "
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,4,Conclusion,"In this paper, we introduced the Hybrid-MT-ESTAN, a multitask learning approach for BUS image analysis that alleviates the lack of global contextual infor-mation in the low-level layers of CNN-based approaches. Hybrid-MT-ESTAN concurrently performs BUS tumor classification and segmentation, with a hybrid architecture that employs CNN-based and Swin Transformer layers. The proposed approach exploits multi-scale local patterns and global long-range dependencies provided by MT-ESTA and AAA Transformer blocks for learning feature representations, resulting in improved generalization. Experimental validation demonstrated significant performance improvement by Hybrid-MT-ESTAN in comparison to current state-of-the-art models for BUS classification."
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,,•,
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,,Fig. 1 .,
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,,Fig. 2 .,
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,,Fig. 3 .,
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,,Table 1 .,
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,,Table 2 .,
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,,Table 3 .,
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,1,Introduction,"Large open-access medical datasets are integral to the future of deep learning (DL) in medicine because they provide much-needed training data and a method of public comparison between researchers  We hypothesize that adaptable domain-aware model calibration that combines expert-level and data-level knowledge can effectively generalize to OOD data. DL calibration is correlated with better OOD generalizability  In this work, we introduce DOMINO++, an adaptable regularization framework to calibrate DL models based on expert-guided and data-guided knowledge. DOMINO++ builds on the work DOMINO  2 Dynamic Framework for DL Regularization"
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,2.1,DL Backbone,U-Net transformer (UNETR) 
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,2.2,DOMINO++ Loss Regularization,"Derivation. The original DOMINO's loss regularization is as follows: where L can be any uncalibrated loss function (e.g., DiceCE which is a hybrid of cross-entropy and Dice score  where β dynamically changes over epochs. s is adaptively updated to balance the data and regularization terms. W HCCM is the dual-guidance penalty matrix. Combining Expert-Guided and Data-Guided Regularization. DOMINO-HC regularizes classes by arranging them into hierarchical groupings based on domain. DOMINO-HC is data-independent and thus immune to noise. Yet, it becomes less useful without clear hierarchical groups. DOMINO-CM calculates class penalties using the performance of an uncalibrated model on a held-out dataset. The CM method does not require domain knowledge, but it can be more susceptible to messy data. Overall, DOMINO-HC is expert-crafted and DOMINO-CM is data-driven. These approaches have complementary advantages and both perform very well on medical image segmentation  The combined regulation (a.k.a. DOMINO-HCCM) requires first replicating DOMINO-HC. For this step, we recreate the exact hierarchical groupings from the DOMINO paper "
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,2.3,Adaptive Regularization Weighting,"Multiple loss functions must be balanced properly  3 Experiments and Results Reference Segmentations. The T1 MRIs are segmented into 11 different tissues, which include grey matter (GM), white matter (WM), cerebrospinal fluid (CSF), eyes, muscle, cancellous bone, cortical bone, skin, fat, major artery (blood), and air. Trained labelers performed a combination of automated segmentation and manual correction. Initially, base segmentations for WM, GM, and bone are obtained using Headreco  Out-of-Domain (OOD) Testing Data Most DL work selects a testing set by splitting a larger dataset into training and testing participants. This work also incorporates ""messy"" or fully independent data. Thus, three additional testing datasets are used along with the traditional testing data (Site A -Clean). Site A Noisy -MRI noise may be approximated as Gaussian for a signal-tonoise ratio (SNR) greater than 2  Site A Rotated -Rotated MRI data simulates other further disturbances or irregularities (e.g., head tilting) during scanning. The rotation dataset includes random rotation of 5-to 45 • clockwise or counter-clockwise with respect to each 3D axis. The rotation angles are based on realistic scanner rotation  Site B -Site A uses a 64-channel head coil and Site B uses a 32-channel head coil. The maximum theoretical SNR of an MRI increases with the number of channels "
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,3.2,Implementation Details,This study implements UNETR using the Medical Open Network for Artificial Intelligence (MONAI-1.1.0) in Pytorch 1.10.0 
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,3.3,Analysis Approach,"This section compares the results of 11 tissue head segmentation on each of the datasets using the baseline model, the best performing DOMINO approach, and the best performing DOMINO++ approach. The results are evaluated using Dice score "
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,3.4,Segmentation Results,"Qualitative Comparisons. Figure 3 features segmentation of one example slice from the Site A MRI dataset with Gaussian Noise. DOMINO substantially exaggerates the blood regions in this slice. In addition, DOMINO entirely misses a section of white matter near the eyes. However, DOMINO can also capture certain regions of the white matter, particularly in the back of the head, better than the baseline model. In general, all outputs have noisy regions where there appear to be ""specks"" of an erroneous tissue. For instance, grey matter is incorrectly identified as specks within the white matter. This issue is far more common in the DOMINO output compared to the baseline or DOMINO++ outputs. Quantitative Comparisons. Tables "
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,4,Conclusions,DOMINO  We will release DOMINO++ code to the community to support open science research.
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,,Fig. 1 .,
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,,Fig. 2 .,
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,,Fig. 3 .,
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,,,
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,,Table 1 .,
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,,Table 2 .,
DOMINO++: Domain-Aware Loss Regularization for Deep Learning Generalizability,,Table 3 .,
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,1,Introduction,"Medical image segmentation is a core step for quantitative and precision medicine. In the past decade, Convolutional Neural Networks (CNNs) became the SOTA method to achieve accurate and fast medical image segmentation  Although transformers have achieved certain success in medical imaging, the lack of inductive bias makes them harder to be trained and requires much more training data to avoid overfitting. The self-attentions are good at learning complicated relational interactions for high-level concepts  In this paper, we try to develop a new ""to-go"" transformer for 3D medical image segmentation, which is expected to exhibit strong performance under different data situations and does not require extensive hyperparameter tuning. SwinUNETR reaches top performances on several large benchmarks, making itself the current SOTA, but without effective pretraining and excessive tuning, its performance on new datasets and challenges is not as high-performing as expected. A straightforward direction to improve transformers is to combine the merits of both convolutions and self-attentions. Many methods have been proposed and most of them fall into two directions: 1) a new self-attention scheme to have convolutionlike properties "
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,2,Method,"Our SwinUNETR-V2 is based on the original SwinUNETR, and we focus on the transformer encoder. The overall framework is shown in Fig. "
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Fig. 2. The SwinUNETR-V2 architecture,"Swin-Transformer. We briefly introduce the 3D swin-transformer as used in Swin-UNETR  W-MSA and SW-MSA represent regular window and shifted window multi-head selfattention, respectively. MLP and LN represent multilayer perceptron and layernorm, respectively. A patch merging layer is applied after every swin transformer block to reduce each spatial dimension by half. Stage-Wise Convolution. Although Swin-transformer uses local window attention to introduce inductive bias like convolutions, self-attentions can still mess up with the local details. We experimented with multiple designs as in Fig.  Decoder. The decoder is the same as SwinUNETR "
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,3,Experiments,"We use extensive experiments to show its effectiveness and justify its design for 3D medical image segmentation. To make fair comparisons with baselines, we did not use any pre-trained weights. Datasets. The network is validated on five datasets of different sizes, targets and modalities: 1) The WORD dataset "
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Implementation Details,"The training pipeline is based on the publicly available SwinUNETR codebase (https://github.com/Project-MONAI/research-contributions/tree/main/SwinUNETR/BT CV, our training recipe is the same as that by SwinUNETR). We changed the initial learning rate to 4e-4, and the training epoch is adapted to each task such that the total training iteration is about 40k. Random Gaussian smooth, Gaussian noise, and random gamma correction are also added as additional data augmentation. There are differences in data preprocessing across tasks. MSD data are resampled to 1 × 1x1 mm resolution and normalized to zero mean and standard deviation (CT images are firstly clipped by .5% and 99.5% foreground intensity percentile). For WORD and FLARE preprocessing, we use the default transforms in SwinUNETR codebase (https://github. com/Project-MONAI/research-contributions/tree/main/SwinUNETR/BTCV, our training recipe is the same as that by SwinUNETR) and 3D UXNet codebase (see footnote 1). Besides these, all other training hyperparameters are the same. We only made those minimal changes for different tasks and show surprisingly good generalizability of the SwinUNETR-V2 and the pipeline across tasks. "
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Results,WORD Result. We follow the data split in  FLARE 2021 Result. We use the 5-fold cross-validation data split and baseline scores from 
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,MSD Results.,"For MSD datasets, we perform 5-fold cross-validation and ran the baseline experiments with our codebase using exactly the same hyperparameters as mentioned. nnunet2D/3D baseline experiments are performed using nnunet's original codebase "
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Variations of SwinUNetR-V2,"In this section, we investigate other variations of adding convolutions into swin transformer. We follow Fig. "
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,4,Discussion and Conclusion,"In this paper, we propose a new 3D medical image segmentation network SwinUNETR-V2. For some tasks, we found the original SwinUNETR with pure transformer backbones (or other ViT-based models) may have inferior performance and training stability than CNNs. To improve this, our core intuition is to combine convolution with window-based self-attention. Although existing window-based attention already has a convolution-like inductive bias, it is still not good enough for learning local details as convolutions. We tried multiple combination strategies as in Table "
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Fig. 1 .,
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Fig. 3 .,
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,"MSD Task05 prostate, Task06 lung tumour and Task07 pancreas.",
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Table 1 .,
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Table 2 .,
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Table 3 .,
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Table 4 .,
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,Table 5 .,
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,,,
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,1,Introduction,"Automatic mitochondria segmentation from electron microscopy (EM) volume is a fundamental task, which has been widely applied to basic scientific research and R. Sun and H. Mai-Equal Contribution. clinical diagnosis  In this work, we tackle the unsupervised domain adaptive (UDA) problem, where there are no accessible labels in the target volume. To alleviate this issue, representative and competitive methods  In this paper, we propose a Structure-decoupled Adaptive Part Alignment Network (SAPAN) including a structure decoupler and a part miner for robust mitochondria segmentation. "
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,2,Related Work,Domain adaptation in EM volume 
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,3,Method,
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,3.1,Overview,The domain adaptive mitochondria segmentation task aims at learning an accurate segmentation model in the target domain based on a labeled source domain EM volume As shown in Fig. 
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,3.2,Structure Decoupler,"In order to bridge the gap in mitochondrial structure between domains, a structure decoupler is designed to decouple the extracted feature map from domainspecific structure information, realized by a channel-wise modulation mechanism. To better model the structure information, we first apply attention-based spatial smoothing for adjacent sections. Concretely, given feature maps of adjacent sections F i and F i+1 , we define the spatial smoothing S i+1 (•) w.r.t F i+1 as: each pixel f i,j ∈ R 1×C (j = 1, 2, ..., hw) in feature map F i as query, the feature map of adjacent section F i+1 ∈ R h×w×C as keys and values. Formally, where the T refers to the matrix transpose operation and the √ C is a scaling factor to stabilize training. We compute the structure difference D i ∈ R h×w×C between F i and its adjacent F i+1 and F i-1 by: ( The final structure embedding e ∈ R 1×C for each domain is calculated by exponential momentum averaging batch by batch: where GAP(•) denotes the global average pooling, B denotes the batch size and θ ∈ [0, 1] is a momentum coefficient. In this way, e S and e T condense the structure information for the whole volume of the corresponding domain. To effectively mitigate the discrepancy between different domains, we employ channel-wise modulation to decouple the feature from the domain-specific structure information. Taking source domain data F S i as example, we first produce the channel-wise modulation factor γ S ∈ R 1×C and β S ∈ R 1×C conditioned on e S : where G γ (•) and G β (•) shared by the source domain and target domain, are implemented by two linear layers and an activation layer. Then the decoupled source feature can be obtained by: where denotes element-wise multiplication. The decoupled target feature F T i can be acquired in a similar way by Eq. ( "
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,3.3,Part Miner,"As far as we know, a good generator is inseparable from a powerful discriminator. To inspire the discriminator to focus on discriminative regions, we design a part miner to dynamically divide foreground and background into diverse parts in an adaptive manner, which are classified by the discriminator D part subsequently. To mine different parts, we first learn a set of part filters P = {p k } 2K k=1 , each filter p k is represented as a C-dimension vector to interact with the feature map F (omit the subscript i for convenience). The first half {p k } K k=1 are responsible for dividing the foreground pixels into K groups and vice versa. Take the foreground filters for example. Before the interaction between p k and F, we first filter out the pixels belonging to the background using downsampled prediction Y . Then we get K activation map A i ∈ R h×w by multiplying p k with the masked feature map: In this way, the pixels with a similar pattern will be highlighted in the same activation map. And then, the foreground part-aware prototypes P = { p k } K k=1 can be got by: Substituting Y with (1 -Y ), we can get the background part-aware prototypes P = { p k } 2K k=K+1 in the same manner."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,3.4,Training Objectives,"During training, we calculate the supervise loss with the provided label Y S i of the source domain by: where the CE(•, •) refers to the standard cross entropy loss. Considering the part-aware prototypes may focus on the same, making the part miner degeneration, we impose a diversity loss to expand the discrepancy among part-aware prototypes. Formally, where the cos(•, •) denotes cosine similarity between two vectors. The discriminator D part takes p k as input and outputs a scalar representing the probability that it belongs to the target domain. The loss function of D part can be formulated as: As a result, the overall objective of our SAPAN is as follows: where the λ div and λ part are the trade-off weights. The segmentation network and the D part are trained alternately by minimizing the L and the L part , respectively."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,4,Experiments,
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,4.1,Dataset and Evaluation Metric,Dataset. We evaluate our approach on three widely used EM datasets: the VNC III 
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,4.2,Implementation Details,"We adopt a five-stage U-Net with feature channel number of  During training, we randomly crop the original EM section into 512 × 512 with random augmentation including flip, transpose, rotate, resize and elastic transformation. All models are trained 20,000 iterations using Adam optimizer with batch size of 12, learning rate of 0.001 and β of (0.9, 0.99). The λ div , λ part and K are set as 0.01, 0.001 and 4, respectively."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,4.3,Comparisons with SOTA Methods,"Two groups of experiments are conducted to verify the effectiveness of our method. Note that ""Oracle"" means directly training the model on the target domain dataset with the ground truth and ""NoAdapt"" means training the model only using the source domain dataset without the target domain dataset. Table  On the MitoEM dataset with a larger structure discrepancy, our SAPAN can achieve 75.6% and 80.6% IoU on the two subsets respectively, outperforming DA-ISC by 0.8% and 1.2%. It demonstrates the remarkable generalization capacity of SAPAN, credited to the structure decoupler, which can effectively alleviate the domain gap caused by huge structural difference. Figure "
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,4.4,Ablation Study and Analysis,"To look deeper into our method, we perform a series of ablation studies on VNC III → Lucchi-Test to analyze each component of our SAPAN, including the Structure Decoupler (SD) and the Part Miner (PM). Note that we remove all modules except the U-Net and the pixel-wise discriminator as our baseline. Hyperparameters are discussed in the supplementary material."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,,Effectiveness of Components.,As shown in Table  Effectiveness of the Attention-Based Smoothing. As shown in Table  Effectiveness of the Ways Modeling Part-Aware Prototypes. In Table 
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,5,Conclusion,"We propose a structure decoupler to decouple the distribution and morphology, and a part miner to aggregate diverse parts for UDA mitochondria segmentation. Experiments show the effectiveness."
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,,,
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,,Fig. 2 .Fig. 3 .,
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,,Fig. 4 .,
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,,Table 1 .,
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,,Table 2 .,
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,,Table 3 .,
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,,Table 4 .,
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,,Table 5 .,
Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 50.
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,1,Introduction,"It has been widely recognized that the success of supervised learning approaches, such as deep learning, relies on the i.i.d. assumption for both training and test samples  To address this issue, unsupervised domain adaptation (UDA)  By contrast, single domain generalization (SDG)  Specifically, to disentangle the style representations, we train a segmentation model, i.e., the baseline, using single domain data and assess the impact of the features extracted by the first convolutional layer on the segmentation performance, since shallower features are believed to hold more style-sensitive information  In this paper, we incorporate contrastive feature disentanglement into a segmentation backbone and thus propose a novel SDG method called Channel-level Contrastive Single Domain Generalization (C 2 SDG) for joint optic cup (OC) and optic disc (OD) segmentation on fundus images. In C 2 SDG, the shallower features of each image and its style-augmented counterpart are extracted and used for contrastive training, resulting in the disentangled style representations and structure representations. The segmentation is performed based solely on the structure representations. This method has been evaluated against other SDG methods on a public dataset and improved performance has been achieved. Our main contributions are three-fold: (1) we propose a novel contrastive perspective for SDG, enabling contrastive feature disentanglement using the data from only a single domain; (2) we disentangle the style representations and structure representations explicitly and channel-wisely, and then diminish the impact of style-sensitive 'devil' channels; and (3) our C 2 SDG outperforms the baseline and six state-of-the-art SDG methods on the joint OC/OD segmentation benchmark."
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,2,Method,
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,2.1,Problem Definition and Method Overview,"Let the source domain be denoted by D s = {x s i , y s i } Ns i=1 , where x s i is the i-th source domain image, and y s i is its segmentation mask. Our goal is to train a segmentation model F θ : x → y on D s , which can generalize well to an unseen target domain D t = {x t i } Nt i=1 . The proposed C 2 SDG mainly consists of a segmentation backbone, a style augmentation (StyleAug) module, and a contrastive feature disentanglement (CFD) module. For each image x s , the StyleAug module generates its style-augmented counterpart x a , which shares the same structure but different style to x s . Then a convolutional layer extracts high-dimensional representations f s and f a from x s and x a . After that, f s and f a are fed to the CFD module to perform contrastive training, resulting in the disentangled style representations f sty and structure representations f str . The segmentation backbone only takes f str as its input and generates the segmentation prediction y. Note that, although we take a U-shape network "
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,2.2,Style Augmentation,"Given a batch of source domain data {x s n } NB n=1 , we adopt a series of stylerelated data augmentation approaches, i.e., gamma correction and noise addition in BigAug  , and {x F R n } NB n=1 in turn to perform contrastive training and segmentation."
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,2.3,Contrastive Feature Disentanglement,"Given x s and x a , we use a convolutional layer with parameter θ c to generate their shallow features f s and f a , which are 64-channel feature maps for this study. Then we use a channel mask prompt P ∈ R 2×64 to disentangle each shallow feature map f into style representation f sty and structure representation f str explicitly channel-wisely where SM (•) is a softmax function, the subscript i denotes i-th channel, and τ = 0.1 is a temperature factor that encourages P sty and P str to be binaryelement vectors, i.e., approximately belonging to {0, 1} 64 . After channel-wise feature disentanglement, we have {f s sty , f s str } from x s and {f a sty , f a str } from x a . It is expected that (a) f s sty and f a sty are different since we want to identify them as the style-sensitive 'devil' channels, and (b) f s str and f a str are the same since we want to identify them as the style-irrelevant channels and x s and x a share the same structure. Therefore, we design two contrastive loss functions L sty and L str where the P roj(•) with parameters θ p reduces the dimension of f str and f sty . Only f s str and f a str are fed to the segmentation backbone with parameters θ seg to generate the segmentation predictions y s and y a ."
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,2.4,Training and Inference,"Training. For the segmentation task, we treat OC/OD segmentation as two binary segmentation tasks and adopt the binary cross-entropy loss as our objective where y represents the segmentation ground truth and y is the prediction. The total segmentation loss can be calculated as During training, we alternately minimize L seg to optimize {θ c , P, θ seg }, and minimize L str + L sty to optimize {P, θ p }. Inference. Given a test image x t , its shallow feature map f t can be extracted by the first convolutional layer. Based on f t , the optimized channel mask prompt P can separate it into f t sty and f t str . Only f t str is fed to the segmentation backbone to generate the segmentation prediction y t ."
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,3,Experiments and Results,"Materials and Evaluation Metrics. The multi-domain joint OC/OD segmentation dataset RIGA+  Implementation Details. The images were center-cropped and normalized by subtracting the mean and dividing by the standard deviation. The input batch contains eight images of size 512 × 512. The U-shape segmentation network, whose encoder is a modified ResNet-34, was adopted as the segmentation backbone of our C 2 SDG and all competing methods for a fair comparison. The Table "
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,,Methods,"Comparative Experiments. We compared our C 2 SDG with two baselines, including 'Intra-Domain' (i.e., training and testing on the data from the same target domain using 3-fold cross-validation) and 'w/o SDG' (i.e., training on the source domain and testing on the target domain), and six SDG methods, including BigAug  Ablation Analysis. To evaluate the effectiveness of low-frequency components replacement (FR) in StyleAug and CFD, we conducted ablation experiments using BinRushed and Magrabia as the source domain, respectively. The average performance is shown in Table  Analysis of CFD. Our CFD is modularly designed and can be incorporated into other SDG methods. We inserted our CFD to ADS "
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,4,Conclusion,"In this paper, we propose a novel SDG method called C 2 SDG for medical image segmentation. In C 2 SDG, the StyleAug module generates style-augmented counterpart of each source domain image and enables contrastive learning, the CFD module performs channel-level style and structure representations disentanglement via optimizing a channel prompt P, and the segmentation is performed based solely on structure representations. Our results on a multi-domain joint OC/OD segmentation benchmark indicate the effectiveness of StyleAug and CFD and also suggest that our C 2 SDG outperforms the baselines and six completing SDG methods with a large margin."
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,,Fig. 1 .,
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,,Fig. 2 .,
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,,Fig. 3 .,
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,,Table 2 .,
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,,Table 3 .,
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,,Table 4 .,
Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_2.
Robust Segmentation via Topology Violation Detection and Feature Synthesis,1,Introduction,"Performance of automated medical image segmentation methods is widely evaluated by pixel-wise metrics, e.g., the Dice score or the Intersection-over-Union  To address the above challenge, several methods explore the idea of topology constraints in image segmentation. Many utilize persistent homology (PH)  In this work, we propose a novel deep learning (DL)-based topology-aware segmentation method, utilizing the concept of the Euler Characteristic (EC) "
Robust Segmentation via Topology Violation Detection and Feature Synthesis,,Contributions.,"(1) This paper presents a novel DL-based method for fast EC calculation, which, to the best of our knowledge, is the first paper introducing DL-compatible EC computation.  So far, topology in segmentation has only been explored through the use of PH "
Robust Segmentation via Topology Violation Detection and Feature Synthesis,2,Method,"Preliminaries. Algebraic topology utilizes algebraic techniques to study the properties of topological spaces. This field aims to identify and characterize topological invariants, such as the number of holes, the connectivity, or the homotopy groups of a space. For image segmentation, gray-scale images defined on grids I ∈ R h×w can be modeled as cubical complexes K, which is a set consisting of points, line segments, squares, and cubes. We show the modeling process in Fig.  Euler Characteristic (EC), denoted as χ ∈ Z, is a topological invariant that describes the structure and properties of a given topological space. For a cubical complex K defined in a n-dimensional space, EC χ is defined as the alternating sum of the number of k-dimensional cells N k , or alternating sum of the ranks of the k-dimensional homology groups, called Betti number β k . Mathematically, EC can be formalized as: Specifically, for 2-dimensional images, EC where N 0 , N 1 and N 2 are the number of vertices, edges, and faces; and β 0 and β 1 are the number of connected components and holes. Gray Algorithm for EC Calculation. The Gray algorithm is a conventional approach to calculate EC on binary images  In (1), we utilize a U-Net to predict a segmentation probability map S ∈ [0, 1] h×w , with the supervision of GT segmentation S ∈ {0, 1} h×w . The predicted map may contain topological errors. (  During training, we use the TVD block (red arrows in Fig.  Topology-violation Detection. TVD consists of two parts: an EC-Net and a Visualization Net (Fig.  EC-Net consists of three parts: 1) fixed kernel CNN layers, 2) an averaged pooling layer and 3) a linear transformation f . Following the Gray algorithm  as shown in Fig.  where l ∈ {1, 2, 3}, * represents the convolutional operation, and 1(•) is an indicator function that equals 1 if and only if the input is true. Note that the patch size of average pooling is the same as the patch size of the segmentation Δ. Finally, following Eq. 2, a linear transformation is used to calculate the EC χ. During training, we separately take both S and S as the input of EC-Net and obtain their corresponding Euler maps X and X. In the second part of TVD, we measure the Euler error by the L1 distance as e = X -X 1 . We can calculate the gradient of e with respect to the segmentation maps as the visualization of the EC error, called topology violation map V = ∂e/∂S, which is the output of TVD. Topology-Aware Feature Synthesis. In this module, we aim to improve the segmentation's topological correctness by utilizing the detected topology violation maps. We observe that topological errors are often caused by poor feature representation in the input image, e.g. blurry boundary regions. These errors are difficult to be corrected when trained from the image space. Therefore, we propose a TFS network that directly learns how to repair the topological structures from the segmentations. During training, we mask out the topological error regions in the segmentation map and send it as the input of the feature synthesis network. We then use the GT segmentation to supervise this network to learn to repair these error regions. The input S (with its element Si,j ) of the feature synthesis network is generated from the segmentation probability map S (with its element Ŝi,j ) and the topology violation map V (with its element V i,j ) as follows: We first filter out the coordinates {(i, j)} with severe topological errors if | V i,j |≥ t, where t is a filtration threshold. Then we replace the values of the segmentation at these coordinates by a random probability sampled from standard Gaussian distribution σ ∼ N (0, 1) and followed by a Sigmoid function to map it to (0, 1). The process of generating Si,j can be summarized as: During inference, we feed the feature synthesis network with pure predictions S. We show the effectiveness of our design in the next section."
Robust Segmentation via Topology Violation Detection and Feature Synthesis,3,Evaluation,Datasets: We conduct experiments on two datasets: CREMI for neuron boundary segmentation  The dHCP dataset has 242 fetal brain T2 Magnetic Resonance Imaging (MRI) scans with gestational ages ranging from 20.6 to 38.2 weeks. All MR images were motion corrected and reconstructed to 0.8 mm isotropic resolution for the fetal head region of interest (ROI) 
Robust Segmentation via Topology Violation Detection and Feature Synthesis,,Settings:,"The hyper-parameters in Eq. 3 and Eq. 4 are empirically chosen as Δ = 32 for the CREMI dataset, Δ = 8 for the dHCP dataset, and t = 0.6 for both datasets. We choose Δ to be higher for CREMI than for dHCP because: (1) the resolution of CREMI is higher and (2) the topology of the fetal cortex may change in smaller regions. We choose a default U-Net as the backbone for all methods for comparison, but our method can also be Incorporated into other segmentation frameworks. For the training process, we first use default crossentropy loss to train the first segmentation network, then the TFS network. Note that the parameters in TVD are fixed. Implementation: We use PyTorch 1.13.1 and calculate the Betti number with the GUDHI package  Evaluation Metrics. Segmentation performance is evaluated by Dice score and averaged surface distance (ASD), and the performance of topology is evaluated by Betti errors, which is defined as: e i =| β pred i -β gt i |, where i ∈ {0, 1} indicates the dimension. We also report the mean Betti error as e = e 0 + e 1 . Quantitative Evaluation. We compare the segmentation performance of our method with three baselines which are proposed to preserve shape and topology: cl-Dice loss  Ablation Study. We first evaluate the effectiveness of our TVD design. We train our pipeline without the TVD block. Instead, we use the difference map between the prediction and GT as a substitute for the topology violation map. We also summarize the qualitative results in Fig.  Discussion. This study sheds new light on improving and evaluation of topology-aware medical image segmentation. We observe that most existing methods either do not consider topological constraints, or are limited by their high computational complexity. As a computation-efficient block, our method can be easily integrated into existing segmentation methods to improve the topological structure. A limitation for topology-aware segmentation methods is that they are easy to be affected by noises, so they might be more suitable to datasets with clear topology structures."
Robust Segmentation via Topology Violation Detection and Feature Synthesis,4,Conclusion,"We propose a novel EC-based method to include topology constraints in the segmentation network. Different from PH-based approaches, our method has a distinct advantage in computational efficiency while providing improved performance. In this paper, we generate a topology violation map from TVD and employ a post-processing feature synthesis network to correct topological errors. We believe this map is valuable and could be explored for other scenarios, such as serving as a spatial prior to regularize various loss functions."
Robust Segmentation via Topology Violation Detection and Feature Synthesis,,Fig. 1 .,
Robust Segmentation via Topology Violation Detection and Feature Synthesis,,Fig. 2 .,
Robust Segmentation via Topology Violation Detection and Feature Synthesis,,Table 1 .,
Robust Segmentation via Topology Violation Detection and Feature Synthesis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_7.
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,1,Introduction,"Breast cancer is the most common cause of cancer-related deaths among women all around the world  To better support the radiologists with breast cancer diagnosis, various segmentation algorithms have been developed  Although  In this study, we propose a simple yet effective weakly-supervised strategy, by using extreme points as annotations (see Fig. "
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,2,Method,The proposed SimPLe strategy and the trainfine-tuneretrain procedure is illustrated in Fig. 
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,2.1,Generate Initial Pseudo-masks,"We use the extreme points to generate pseudo-masks based on random walker algorithm  where Ω is the spatial domain. To further increase the area of foreground, the voxel at location k is considered as new foreground seed if Y (k) is greater than 0.8 and new background seed if Y (k) is less than 0.1. Then we run the random walker algorithm repeatedly. After seven times iterations, we set foreground in the same way via the last output probability map. Voxels outside the bounding box are considered as background. The rest of voxels remain unlabeled. This is the way initial pseudo-masks Y init : Ω ⊂ R 3 → {0, 1, 2} generated, where 0, 1 and 2 represent negative, positive and unlabeled."
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,2.2,Train Network with Initial Pseudo-masks,"Let X : Ω ⊂ R 3 → R denotes a training volume. Let f and θ be network and its parameters, respectively. A simple training approach is to minimize the partial cross entropy loss L pce , which is formulated as: ( Moreover, supervised contrastive learning is employed to encourage voxels of the same label to gather around in feature space. It ensures the network to learn discriminative features for each category. Specifically, features corresponding to N negative voxels and N positive voxels are randomly sampled, then the contrastive loss L ctr is minimized: where P(k) denotes the set of points with the same label as the voxel k and N (k) denotes the set of points with the different label. Z(k) denotes the feature vector of the voxel at location k. sim(•, •) is the cosine similarity function. σ denotes sigmoid function. τ is a temperature parameter. To summarize, we employ the sum of the partial cross entropy loss L pce and the contrastive loss L ctr to train the network with initial pseudo-masks: (3)"
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,2.3,SimPLe-Based Fine-Tune and Retrain,"The performance of the network trained by the incomplete initial pseudo-masks is still limited. We propose to fine-tune the entire network using the pre-trained weights as initialization. The fine-tune follows the SimPLe strategy which evaluates the similarity between unlabeled voxels and positive voxels to propagate labels to unlabeled voxels. Specifically, N positive voxels are randomly sampled as the referring voxel. For each unlabeled voxel k, we evaluate its similarity with all referring voxels: where I(•) is the indicator function, which is equal to 1 if the cosine similarity is greater than λ and 0 if less. If S(k) is greater than αN , the voxel at location k is considered as positive. Then the network is fine-tuned using the partial cross entropy loss same as in the initial train stage. The loss function L f inetune is formulated as: where w is the weighting coefficient that controls the influence of the pseudo labels. To reduce the influence of possible incorrect label propagation, pseudo labels for unlabeled voxels are valid only for the current iteration when they are generated. After the fine-tune completed, the network generates binary pseudo-masks for every training data, which are expected to be similar to the ground-truths provided by radiologists. Finally the network is retrained from random initialization by minimizing the cross entropy loss with the binary pseudo-masks."
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,3,Experiments,"Dataset. We evaluated our method on an in-house breast DCE-MRI dataset collected from the Cancer Center of Sun Yat-Sen University. In total, we collected 206 DCE-MRI scans with biopsy-proven breast cancers. All MRI scans were examined with 1.5T MRI scanner. The DCE-MRI sequences (TR/TE = 4.43 ms/1.50 ms, and flip angle = 10 • ) using gadolinium-based contrast agent were performed with the T1-weighted gradient echo technique, and injected 0.2 ml/kg intravenously at 2.0 ml/s followed by 20 ml saline. The DCE-MRI volumes have two kinds of resolution, 0.379×0.379×1.700 mm 3 and 0.511×0.511×1.000 mm 3 . All cancerous regions and extreme points were manually annotated by an experienced radiologist via ITK-SNAP  Implementation Details. The framework was implemented in PyTorch, using a NVIDIA GeForce GTX 1080 Ti with 11GB of memory. We employed 3D Unet  • Train: The network was trained by stochastic gradient descent (SGD) for 200 epochs, with an initial learning rate η = 0.01. The ploy learning policy was used to adjust the learning rate, (1epoch/200) 0.9 . The batch size was 2, consisting of a random foreground patch and a random background patch located via initial segmentation Y init . Such setting can help alleviate class imbalance issue. The patch size was 128 × 128 × 96. For the contrastive loss, we set N = 100, temperature parameter τ = 0.1. • Fine-tune: We initialized the network with the trained weights. We trained it by SGD for 100 iterations, with η = 0.0001. The ploy learning policy was also used. For the SimPLe strategy, we set N = 100, λ = 0.96, α = 0.96, w = 0.1. Quantitative and Qualitative Analysis. We first verified the efficacy of our SimPLe in the training stage. Figure "
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,4,Conclusion,We introduce a simple yet effective weakly-supervised learning method for breast cancer segmentation in DCE-MRI. The primary attribute is to fully exploit the simple trainfine-tuneretrain process to optimize the segmentation network via only extreme point annotations. This is achieved by employing a similarityaware propagation learning (SimPLe) strategy to update the pseudo-masks. Experimental results demonstrate the efficacy of the proposed SimPLe strategy for weakly-supervised segmentation.
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,,Fig. 1 .,
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,,Fig. 2 .,
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,,Fig. 3 .Fig. 4 .,
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,,Fig. 5 .,
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,,Table 1 .,
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,1,Introduction,"Accurate and robust classification and segmentation of the medical image are powerful tools to inform diagnostic schemes. In clinical practice, the image-level classification and pixel-wise segmentation tasks are not independent  Considering the close correlation between the classification and segmentation, many researchers  The current uncertainty estimation method can roughly include the Dropoutbased  Based on the analysis presented above, we design a novel Uncertaintyinformed Mutual Learning (UML) network for medical image analysis in this study. Our UML not only enhances the image-level and pixel-wise reliability of medical image classification and segmentation, but also leverages mutual learning under uncertainty to improve performance. Specifically, we adopt evidential deep learning "
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,2,Method,"The overall architecture of the proposed UML, which leverages mutual learning under uncertainty, is illustrated in Fig.  Given an input medical image I, I ∈ R H,W , where H, W are the height and width of the image, separately. To maximize the extraction of specific information required for two different tasks while adequately mingling the common feature which is helpful for both classification and segmentation, I is firstly fed into the dual backbone network that outputs the classification feature maps f c i , i ∈ 1, ..., 4 and segmentation feature maps f s i , i ∈ 1, ..., 4, where i denotes the i th layer of the backbone. Then following "
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,2.1,Uncertainty Estimation for Classification and Segmentation,"Classification Uncertainty Estimation. For the K classification problems, we utilize Subjective Logic  where b c k ≥ 0 and U c ≥ 0 denote the probability belonging to the k th class and the overall uncertainty value, respectively. As shown in Fig.  where represents the Dirichlet strength. Actually, Eq. 2 describes such a phenomenon that the higher the probability assigned to the k th class, the more evidence observed for k th category should be. Segmentation Uncertainty Estimation. Essentially, segmentation is the classification for each pixel of a medical image. Given a pixel-wise segmentation result, following "
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,", . . . , α s(h,w) Q","], (h, w) ∈ (H, W ). We can compute the belief mass and uncertainty mass of the input image by where b s(h,w) q ≥ 0 and u s(h,w) ≥ 0 denote the probability of the pixel at coordinate (h, w) for the q th class and the overall uncertainty value respectively. We also define U s = {u s(h,w) , (h, w) ∈ (H, W )} as the pixel-wise uncertainty of the segmentation result."
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,2.2,Uncertainty-Informed Mutual Learning,"Uncertainty Navigator for Segmentation. Actually, we have already obtained an initial segmentation mask M = α s , M ∈ (Q, H, W ) through estimating segmentation uncertainty, and achieved lots of valuable features such as lesion location. In our method, appropriate uncertainty guided decoding on the feature list can obtain more reliable information and improve the performance of segmentation  Then, the reliable segmentation feature r s , which combines the trusted information in M r with the original features, is generated by: where f s 1 derives from jump connecting and f b 2 is the feature of the s 2 with one up-sample operation. Conv(•) represents the convolutional operation, Cat(•, •) denotes the concatenation. Especially, the U s is also used to guide the bottom feature with the dot product. The r s is calculated from the segmentation result s 1 and contains uncertainty navigated information not found in s 1 . Uncertainty Instructor for Classification. In order to mine the complementary knowledge of segmentation as the instruction for the classification and eliminate intrusive features, we devise an Uncertainty Instructor for classification (UI) following  where d n (•) denotes that the frequency of down-sampling operations is n. Then the produced features are transformed into a semantic feature vector by the global average pooling. The obtained vector is converted into the final result (belief values) of classification with uncertainty estimation."
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,2.3,Mutual Learning Process,"In a word, to obtain the final results of classification and segmentation, we construct an end-to-end mutual learning process, which is supervised by a joint loss function. To obtain an initial segmentation result M and a pixel-wise uncertainty estimation U s , following  where y s is the Ground Truth (GT) of the segmentation. The hyperparameters λ m 1 and λ m 2 play a crucial role in controlling the Kullback-Leibler divergence (KL) and Dice score, as supported by  where y c is the true class of the input image. The hyperparameter λ c serves as a crucial hyperparameter governing the KL, aligning with previous work  where υ n indicates the number of up-sampling is 2 n . Thus, the overall loss function of our UML can be given as: where w m , w c , w s denote the weights and are set 0.1, 0.5, 0.4, separately."
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,3,Experiments,"Dataset and Implementation. We evaluate the our UML network on two datasets REFUGE  For each case, we cut out the slices in the 3D image and totally got 1,570 2D images, which are randomly divided into the train, validation, and test datasets with 1,230, 170, and 170 slices, respectively.  We implement the proposed method via PyTorch and train it on NVIDIA GeForce RTX 2080Ti. The Adam optimizer is adopted to update the overall parameters with an initial learning rate 0.0001 for 100 epochs. The scale of the regularizer is set as 1 × 10 -5 . We choose VGG-16 and Res2Net as the encoders for classification and segmentation, separately. Compared Methods and Metrics. We compared our method with singletask methods and multi-task methods. (1) Single-task methods: (a) EC  Ablation Study. As illustrated in Table  MD represents the mutual feature decoder. It is clear that the performance of classification and segmentation is significantly improved when we introduce supervision of mutual features. As we thought, the introduction of UN and UI takes the reliability of the model to a higher level."
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,4,Conclusion,"In this paper, we propose a novel deep learning approach, UML, for joint classification and segmentation of medical images. Our approach is designed to improve the reliability and interpretability of medical image classification and segmentation, by enhancing image-level and pixel-wise reliability estimated by evidential deep learning, and by leveraging mutual learning with the proposed UN and UI modules. Our extensive experiments demonstrate that UML outperforms baselines and introduces significant improvements in both classification and segmentation. Overall, our results highlight the potential of UML for enhancing the performance and interpretability of medical image analysis."
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,Fig. 1 .,
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,,
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,Fig. 2 .,
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,Figure 2 (,
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,4,
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,Fig. 3 .,
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,Table 1 .,
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,Table 2 .,
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,Table 3 .,
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,.251 Comparison with Single-and Multi-task Methods.,
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,Acknowledgements,. This work was supported by the 
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 4.
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,1,Introduction,"Due to the broad success of U-Nets  One of the key elements of the U-Net are the skip-connections, which propagate information directly (i.e., without further processing) from the encoding to the decoding branch at different scales. Azad et al.  Currently, the U-Net is used more as a ""Swiss-army knife"" architecture across different image modalities and image quality ranges. In this paper, we describe the interplay between skip-connections and their effective role of ""transferring information"" into the decoding branch of the U-Net for different degrees of task complexity, based on controlled experiments conducted on synthetic images of varying textures as well as on clinical data comprising Ultrasound (US), Computed tomography (CT), and Magnetic Resonance Imaging (MRI). In this regard, the work of "
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,2,Materials and Methods,
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,2.1,Experiment Design,"By increasing α from zero to one, more of the foreground texture is added in the foreground mask, which otherwise is made up of the background texture (See Fig.  where T S refers to the level of texture similarity, H() corresponds to histogram, and L(I) {BG,F G} refers to LBP calculated for BG or FG. The LBP histogram was computed using a 3×3 neighbourhood with 8 points around each pixel in the image. Three U-Net models were trained featuring three different skip-connection strategies: NoSkipU-Net, U-Net, and AGU-Net, representing the absence of skipconnections, the use of an identity transform (i.e., information through skips is kept as is), and filtering information via attention through skip-connections, respectively. Models were trained at different levels of T S between the foreground and background regions, determined based on the Kullback-Leibler divergence of Local Binary Pattern (LBP) histograms, Eq. 1. For each level of α used to create a training set, we trained a model to be evaluated on a synthetic test set using the same α to measure within-domain performance and across a range of α, to measure their out-of-domain robustness. Next, using Eq. 1 and ground truth labels, we computed the T S of images from the test set of the medical data sets and applied corruptions by way of noise or blurring in order to increase and decrease T S depending on the imaging modality being analyzed. Then we evaluated the robustness of these models to texture changes in these data sets. We did this at two levels of task complexity (easier -where T S is higher, and harder, where T S is lower) and different from the original T S. We report all model performances using dice scores."
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,2.2,Description of Data,Synthetic Textures: We took two representative grayscale textures from the synthetic toy data set described in 
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,,Medical Data Sets:,We tested the three variants of the U-Net architecture on three medical binary segmentation data sets: a Breast Ultrasound 
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,2.3,Model Architecture and Training Settings,"The network architectures were created using MONAI  The NoSkipU-Net was identical to the U-Net except for severed skipconnections. This led to the number of channels in the decoder to be smaller as there is no concatenation from the corresponding encoder level. The AGU-Net was setup to be the same as the U-Net, except with attention gating through the skip-connections. The training parameters were kept constant across compared models for fair comparison. Our experiments We did not perform any data augmentation that could change the scale of the image content, thereby also changing the texture characteristics. Therefore, we only do a random rotation by 90 degrees with a probability of 0.5 for training, and no other augmentations. We also refrained from fine-tuning hyperparameters and did not perform any ensembling as our study design is not meant to achieve the best possible performance metric as much as it attempts to reliably compare performance across architecture variants while keeping confounding factors to a minimum. We therefore trained each model using the same random seeds (three times) and report the dice score statistics. Training and testing were performed on an NVIDIA A5000 GPU with 24 GB RAM and CUDA version 11.4."
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,3,Results,
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,3.1,On Synthetic Texture Variants,In-domain (Performance): Figure 
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,,Out-of-Domain (Robustness):,Rows in Fig. 
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,3.2,On Medical Image Textures,"In-Domain (performance): Looking at the ""In-domain"" rows in Table "
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,,Out-of-Domain (Robustness):,"Focusing on the rows ""Harder"" and ""Easier"" in Table "
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,4,Discussion and Conclusion,"Through extensive experiments using synthetic texture images at various levels of complexity and validating these findings on medical image data sets from three different modalities, we show in this paper that the use of skip-connections can both be beneficial as well as harmful depending on what can be traded off: robustness or performance. A limitation of our work is that we vary only the foreground in synthetic experiments but background variations could demonstrate unexpected asymmetric behavior. We envision the proposed analysis pipeline to be useful in quality assurance frameworks where U-Net variants could be compared to analyse potential failure modes."
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,,Figure 1,
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,,Fig. 1 .,
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,,Fig. 2 .,
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,,Fig. 3 .,
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,,Fig. 4 .,
Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity,,Table 1 .,
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,1,Introduction,"Automatic brain ROI segmentation for magnetic resonance images (MRI) of severe traumatic brain injuries (sTBI) patients is crucial in brain damage assessment and brain network analysis  Recently, one-shot medical image segmentation based on learned transformations (OSSLT) has shown great potential  However, despite the previous success, the generalization ability of these methods is challenged by two issues in traumatic brain segmentation: 1) Limited diversity of generated data due to the amount of available unlabeled images. Although several studies  To address the aforementioned issues, we propose a novel one-shot traumatic brain segmentation method that leverages adversarial training and uncertainty rectification. We introduce an adversarial training strategy that improves both the diversity of generated data and the robustness of segmentation, and incorporate an uncertainty rectification strategy that mitigates potential label errors in generated samples. We also quantify the segmentation difference of the same image with and without the appearance transform, which is used to estimate the uncertainty of segmentation and rectify the segmentation results accordingly. The main contributions of our method are summarized as follows: First, we develop an adversarial training strategy to enhance the capacity of data augmentation, which brings better data diversity and segmentation robustness. Second, we notice the potential label error introduced by appearance transform in current one-shot segmentation attempts, and introduce uncertainty rectification for compensation. Finally, we evaluate the proposed method on brain segmentation of sTBI patients, where our method outperforms current state-of-the-art methods. "
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,2,Method,
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,2.1,Overview of One-Shot Medical Image Segmentation,"After training the unsupervised deformable registration, one-shot medical image segmentation based on learned transformations typically consists of two steps: 1) Data augmentation on the atlas image by learned transformations; 2) Training the segmentation using the augmented images. The basic workflow is shown in Fig.  For spatial transform, given an atlas image x A and a spatial reference x s , the registration network performs the deformable registration between them and predicts a deformation field φ, which is used as the spatial transform to augment the atlas image spatially. For appearance transform, given an atlas image x A and an appearance reference x a , we warp x a to x A via an inverse registration φ -1  xa and generates a inverse-warped xa = x a •φ -1  xa , and appearance transform ψ = xa -x A is calculated by the residual of inverse-warped appearance reference xa and the atlas image x A . It should be noted that the registration here is diffeomorphic to allow for inverse registration. After acquiring both the spatial and appearance transform, the augmented atlas x g = (x A + ψ) • φ is generated by applying both transformations. The corresponding ground-truth y g = y A • φ is the atlas label warped by φ, as it is hypothesized that appearance transform does not alter the semantic labels in the atlas image. During segmentation training, a large amount of spatial and appearance reference images are sampled to ensure the diversity of x g , and the segmentation is trained with generated pairs of x g and y g . In this work, we focus on both of the two steps in OSSLT by adversarial training and uncertainty rectification, which is shown in Fig. "
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,2.2,Adversarial Training,"Although the diversity of generated pairs of x g and y g is ensured by the increased number of unlabeled images as references, such a setting requires a large amount of unlabeled data. Inspired by  Second, by applying the sampled transformations φ a and ψ a to the atlas x A , we acquire an adversarial generated image x ag . We expect x ag to add extra diversity of data augmentation and maintain realistic as well: Finally, both the original generated image x g and the adversarial generated image x ag are fed to the segmentation network, and the training objective is the min-max game of the adversarial network and the segmentation network. Thus, we ensure the diversity of generation and robustness of segmentation simultaneously by adversarial training: where f (•; θ g ) and g(•; θ h ) denote the adversarial network and the segmentation network, respectively. ŷg and ŷag are the segmentation predictions of x g and x ag . It should be noted that since the spatial transformation applied to x g and x ag is different, the loss calculation is performed in atlas space by inverse registration."
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,2.3,Uncertainty Rectification,"Most of the current methods hypothesize that appearance transformation does not alter the label of the atlas. However, in brain scans with abnormalities such as sTBI, the appearance transformation may include edema, lesions, and etc, which may affect the actual semantic labels of the atlas and weaken the accuracy of segmentation. Inspired by  The overall supervised loss consists of two items. First, the segmentation loss L seg = L ce (ŷ As , y g ) of spatial-augmented image x As guides the network to learn spatial variance only, where ŷAs is the prediction of x As , and L ce denotes cross-entropy loss. Second, the rectified segmentation loss L rseg of x g guides the network to learn segmentation under both spatial and appearance transformations. We adopt the KL-divergence D KL of the segmentation results ŷAs and ŷg as the uncertainty in prediction  Thus, the overall supervised loss L sup = L seg +L rseg is the segmentation loss L seg of x As and the rectified segmentation loss L rseg of x g . We apply the overall supervised loss L sup on both x g and x ag in practice. During segmentation training, the linear summation of supervised segmentation loss L sup and adversarial loss L adv is minimized."
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,3,Experiments,
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,3.1,Data,"We have collected 165 MR T1-weighted scans with sTBI from 2014-2017, acquired on a 3T Siemens MR scanner from Huashan hospital. Among the 165 MR scans, 42 scans are labeled with the 17 consciousness-related brain regions (see appendix for details) while the remaining are left unlabeled, since the manual labeling requires senior-level expertise. Informed consent was obtained from all patients for the use of their information, medical records, and MRI data. All MR scans are linearly aligned to the MNI152 template using FSL "
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,3.2,Implementation Details,"The framework is implemented with PyTorch 1.12.1 on a Debian Linux server with an NVIDIA RTX 3090 GPU. In practice, the registration network is based on VoxelMorph "
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,3.3,Evaluation,"We have evaluated our one-shot segmentation framework on the labeled sTBI MR scans in the one-shot setting, which means that only one labeled image is available to learn the segmentation. We explore the effectiveness of the proposed adversarial training and uncertainty rectification, and make the comparison with state-of-the-art alternatives, by reporting the Dice coefficients. First, we conduct an ablation study to evaluate the impact of adversarial training and uncertainty rectification, which is shown in Table  Then, we compare the proposed method with three cutting-edge alternatives in one-shot medical image segmentation, including BrainStorm  For qualitative evaluation, we have visualized the segmentation results of the proposed method and the above-mentioned alternatives, which are shown in Fig. "
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,4,Conclusion,"In this work, we present a novel one-shot segmentation method for severe traumatic brain segmentation, a difficult clinical scenario where limited annotated data is available. Our method addresses the critical issues in sTBI brain segmentation, namely, the need for diverse training data and mitigation of potential label errors introduced by appearance transforms. The introduction of adversarial training enhances both the data diversity and segmentation robustness, while uncertainty rectification is designed to compensate for the potential label errors. The experimental results on sTBI brains demonstrate the efficacy of our proposed method and its advantages over state-of-the-art alternatives, highlighting the potential of our method in enabling more accurate segmentation in severe traumatic brains, which may aid clinical pipelines."
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,,Fig. 1 .,
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,,Fig. 2 .,
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,,,
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,,Fig. 3 .,
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,,Table 1 .,
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,,Table 2 .,
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,,6±24.8 54.4±22.9 62.0±16.2 58.4±10.2 51.1±16.6 44.2±17.5,
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,,0±16.7 44.7±18.1 56.1±15.0 53.2±12.5 51.7±12.9 60.8±11.1,
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,,61.0±16.9 61.1±11.1 48.1±18.9 53.7±17.0 90.1±4.2 56.3±18.8,
One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 12.
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,1,Introduction,"The automated detection and analysis of small vessels from non-invasive imaging data is critical for many clinical studies such as the research on cerebral small vessel disease(CSVD)  (2) Small vessels often have weak intensities and low contrasts, which would be easily affected by noise or surrounding backgrounds. These characteristics are generally not well modeled by existing methods for vessel detection  Traditional vesselness filters typically characterize blood vessels based on hand-crafted features  In this work, we propose a novel self-supervised network that focus on challenges in small vessel segmentation with the following contributions: (1) Instead of assuming ideal tube-shaped vessels like "
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,2,Method,"Our proposed framework for small vessel detection is shown in Fig.  A projected flux response along a direction -→ ρ , which generalizes conventional flux measures for circular shaped tubes  after discretization and normalization, where v(•) is the image gradient, -→ The vesselness score of x can then be computed as where -→ ρ 1 = P (x) is the estimated principal direction at x and -→ ρ 2 , -→ ρ 3 are two orthogonal vectors in the cross-sectional plane of the vascular structure. In contrast to conventional flux measures for vessel segmentation, where only isotropic radius value is estimated, radius in different sampling directions d i are estimated adaptively in this work to fit the irregular-shaped vascular boundaries as f vs is maximized only if the sampling vectors -→ h i fit the vessel edges. Since the radius values can be different from each other, our proposed network is designed to handle the small vessels with irregular and non-circular cross-sections. Local Contrast Guided Attention. As shown in Fig.  On the other hand, the low contrast of small vessel region makes it hard to distinguish the vascular structures from the background clutters. To address this issue, Fig.  where With s = 1, D measures the intensity differences between x and x + -→ h i which locates on vascular boundaries for every sampling direction d i . We can contract or expand the vascular regions along its edges with different s. To measure the contrast inside and outside the vascular regions, we design two measures D in and D out as follows:   where is a small constant to prevent the numerical explosion. Since the ratio of D out and D in gives large value, D LC ≈ 1 for x inside a vascular structure, which means the D LC measure of small vessels is on a similar scale to large vessels. For the latter two situations, D LC ≈ σ(0) = 0.5 will stand since D in ≈ D out . Therefore, the proposed D LC measure can enhance the small vascular structures and suppresses backgrounds and edges simultaneously. Guided by the local contrast map of images, we propose to incorporate a novel spatial attention module in our DL network. Following CBAM  To include low-level contrast information, we insert our LCA and LCE module in the skip connections between encoders and decoders as shown in Fig.  Self-supervised Losses. Based on (2), we propose our flux-based loss L flux as the negative average vesselness score over the whole image spatial space. This cost function takes advantage of the clear edge separation of flux-based filter and robustness to irregular-shaped vessels due to our adaptive shape-aware flux computation. To ensure the vessel structure's continuity, we adopt the path continuity loss L path from  where N is the total number of voxels and R(x) is the average radius of R(x) so that the length of the walking path is relative to the vessel size at x. Mean square error loss is applied between the reconstructed image Î and original image I to make sure the network learns the semantic meaningful features based on the previous success of reconstruction-based self-supervised learning  where λ 1 , λ 2 and λ 3 correspond to the coefficients of each objective."
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,3,Experimental Results,"Datasets. We used two public datasets and two in-house datasets in our experiments for 3D vessel segmentation. The VESSEL12  The dataset was divided into a training set with 21 subjects (42 volumes) and a test set with 7 subjects (14 volumes). Implementation Details. We performed the vessel segmentation tasks for each dataset to evaluate our model's performance. For comparison, we selected 4 conventional filters, including the Frangi, Sato, Meijering and OOF filter and a flow-based DL method  To tune the network's hyper-parameters, we used 15% of the training data as the validation set. Finally, we set m = 128, λ 1 = 5 and λ 2 = λ 3 = 1 for our model. For both DL models, patch size is set to 64 × 64 × 64 and the networks are trained for 100 epochs. All experiments were conducted with Pytorch on one NVIDIA A5000 GPU with the Adam optimizer and a learning rate of 0.001. For all methods, the output is a vessel enhanced image. The enhanced image is binarized through a hard threshold. We thus found the best threshold by optimizing the metrics based on the validation sets and then applied the final threshold to test sets. We reported five metrics in Table  Results and Discussion. We can clearly observe from Table  Small Vessel Segmentation. From Fig.  Ablation Study. To investigate the contributions of each proposed module in our approach, we performed ablation studies by training the network with each proposed module using the 7T MRA dataset. The results are shown in Table "
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,4,Conclusions,"In this paper, we proposed a self-supervised network for the detection of small vessels from 3D imaging data. Our method is designed to address existing challenges in small vessel detection arising from their irregular appearance in relatively limited resolution and low-contrast conditions. In comparison to previous methods, we demonstrated that our method is able to achieve superior performance on small vessel detection. For future work, we will also apply it to various clinical datasets to examine its power for CSVD detection in brain images."
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,,Fig. 1 .,
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,,Fig. 2 .,
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,,Fig. 3 .,
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,,2 1D,
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,,Fig. 4 .,
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,,Fig. 5 .,
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,,Table 1 .,
Shape-Aware 3D Small Vessel Segmentation with Local Contrast Guided Attention,,Table 2 .,
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,1,Introduction,"The field of medical image segmentation has been increasingly drawn to semisupervised learning (SSL) due to the great difficulty and cost of data labeling. By utilizing both labeled and unlabeled data, SSL can significantly reduce the need for labeled training data and address inter-observer variability  Typical SSL approaches involve techniques such as self-training, uncertainty estimation, and consistency regularization. Self-training aims to expand labeled training set by selecting the most confident predictions from the unlabeled data to augment the labeled data  Despite current progress, the performance of pseudo-labeling and consistency constraints has been limited for two reasons. First, defining an appropriate quantification criterion for reliability across various tasks can be challenging due to the inherent complexity of uncertainty. Second, most of the consistency constraints are imposed at decision space with the assumption that the decision boundary must be located at the low-density area, while the latent feature space of unlabeled data has not been fully exploited, and the low-density assumption may be incapable to guide model learning in the correct way. Recently, prototype alignment has been introduced into SSL. Prototypebased methods have the potential of capturing underlying data structure including unlabeled information, and optimizing the distribution of feature embeddings across various categories  Overall, prototype learning has much room for improvement in semisupervised segmentation. As voxel-level averaging is only reliable for labeled data, current prototype learning approaches rely on labeled data and a small amount of unlabeled data, or learn prototypes separately for labeled and unlabeled data. In this way, they may not fully represent the distribution of the embedding space. Here raises the question:"
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,,"Can we capture the embedding distribution by considering all voxels, including both labeled and unlabeled, and exploit the knowledge of the entire dataset?","To answer it, we propose to learn fused prototypes through uncertainty-based attention pooling. The fused prototypes represent the most representative and informative examples from both the labeled and unlabeled data for each class. The main contributions of our work can be summarized as follows: 1) We develop a novel uncertainty-informed prototype consistency learning framework, UPCoL, by considering voxel-level consistency in both latent feature space (i.e., prototype) and decision space. 2) Different from previous studies, we design a fused prototype learning scheme, which jointly learns from labeled and unlabeled data embeddings. 3) For stable prototype learning, we propose a new entropy measure to qualify the reliability of unlabeled voxel and an attention-weighted strategy for fusion. 4) We apply UPCoL to two-class and three-class segmentation tasks. UPCoL outperforms the SOTA SSL methods by large margins. The labeled images go through the student model for supervised learning, while the unlabeled images go through the teacher model for segmentation and uncertainty estimation. In Uncertainty-informed Prototype Fusion module, we utilize the reliability map to fuse prototypes learned from labeled and unlabeled embeddings. The similarity between fused prototypes and feature embeddings at each spatial location is then measured for consistency learning."
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,2,Methodology,"Given a dataset D = {D l , D u }, the labeled set D l = {x l i , y l i } N i=1 contains N samples, and the unlabeled set x l i , x u i ∈ R H×W ×D represent the input with height H, width W , depth D, and y u i ∈ {0, 1, ..., C-1} H×W ×D . The proposed framework UPCoL includes a student model and a self-ensembling teacher model, each consisting of a representation head h and a segmentation head f . Figure "
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,2.1,Uncertainty Assessment,"To assess the uncertainty at both voxel-level and geometry-level, we adopt the same ensemble of classifiers as  ∈ R C denote the softmax probability for voxel at position (x, y, z) in i -th unlabeled image yielded by the segmentation head of the teacher model, where the segmentation result is the average over multiple classifiers (AMC), and C is the number of classes. The entropy is formulated in Eq. (  Intuitively, voxels with high entropy are ambiguous. Thus, a reliability map can be defined accordingly, denoted by Φ (x,y,z) i , which enables the model to assign varying degrees of importance to voxels, ) ). (2)"
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,2.2,Prototype Consistency Learning,"Uncertainty-Informed Prototype Fusion. The prototypes from labeled and unlabeled data are first extracted separately. Both of them originate from the feature maps of the 3rd-layer decoder, which are upsampled to the same size as segmentation labels by trilinear interpolation. Let h l s,i be the output feature by the representation head of the student model for the i-th labeled image, and h u t,i be the hidden feature by the teacher representation head for the i-th unlabeled image. B l and B u denote the batch sizes of labeled and unlabeled set respectively, and (x, y, z) denotes voxel coordinate. For labeled prototype, the feature maps are masked directly using ground truth labels, and the prototype of class c is computed via masked average pooling  For unlabeled data, instead of simply averaging features from the same predicted class, UPCoL obtains the prototypes in an uncertainty-informed manner, i.e., using a masked attention pooling based on each voxel's reliability: x,y,z 1 ŷu(x,y,z) Temporal Ensembling technique in Mean-Teacher architecture enhances model performance and augments the predictive label quality  where λ lab = 1 1+λcon , λ unlab = λcon 1+λcon , and λ con is the widely-used timedependent Gaussian warming up function  Consistency Learning. We adopt non-parametric metric learning to obtain representative prototypes for each semantic class. The feature-to-prototype similarity is employed to approximate the probability of voxels in each class, where the value of is fixed to 1e -8 , and CosSim(•) denotes cosine similarity. To ensure the accuarcy of the prototype, prototype-based predictions for labeled voxels expect to close to ground truth. And the prototype-based predictions for unlabeled voxels expect to close to segmentor prediction since prototype predictions are considered to be reliable aid. Then the prototype consistency losses for labeled and unlabeled samples are defined respectively as: where ŷu(x,y,z) i is the student model prediction of the i-th unlabeled sample at (x, y, z). Equation (  3 Experimental Results Datasets. We evaluate our approach on three datasets: the pancreas dataset (82 CTA scans), the left atrium dataset (100 MR images), and a multi-center dataset for type B aortic dissection (TBAD, 124 CTA scans). The pancreas and left atrium datasets are preprocessed following previous studies  The TBAD dataset is well-annotated by experienced radiologists, with 100 scans for training and 24 for test, including both public data  Results on the Left Atrium Dataset and Pancreas-CT Dataset. We compare UPCoL with nine SOTA SSL methods, including consistency-based (MT  Results on the Aortic Dissection Dataset. We compare with two SOTA SSL methods, the uncertainty-based method FUSSNet  Ablation Study. Here we investigate the contribution of key components, including the mean-teacher architecture (MT), the average multi-classifier (AMC) (to yield segmentation results), and the prototype learning (PL) strategy. As shown in Table "
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,4,Conclusion,"This paper presents a novel framework, UPCoL, for semi-supervised segmentation that effectively addresses the issue of label sparsity through uncertaintybased prototype consistency learning. To better utilize unlabeled data, UPCoL employs a quantitative uncertainty measure at the voxel level to assign degrees of attention. UPCoL achieves a careful and effective fusion of unlabeled data with labeled data in the prototype learning process, which leads to exceptional performance on both 2-class and 3-class medical image segmentation tasks. As future work, a possible extension is to allow multiple prototypes for a class with diversified semantic concepts, and a memory-bank-like mechanism could be introduced to learn prototypes from large sample pools more efficiently."
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,,Fig. 1 .,
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,,Fig. 2 .,
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,,Table 1 .,
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,,Table 2 .,
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,,Table 3 .,
UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_63.
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,1,Introduction,"Medical image segmentation is always a critical task as it can be used for disease diagnosis, treatment planning, and anomaly monitoring. Weakly supervised semantic segmentation attracts significant attention from medical image community since it greatly reduces the cost of dense pixel-wise labeling to get segmentation mask. In WSSS, the training labels are usually easier and faster to obtain, like image-level tags, bounding boxes, scribbles, or point annotations. This work only focuses on WSSS with image-level tags, like whether a tumor presents or not. In this field, previous WSSS works  Meanwhile, denoising diffusion models "
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,2,Methods,
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,2.1,Training Conditional Denoising Diffusion Models,"Suppose that we have a sample x 0 from distribution D(x|y), and y is the condition. The condition y can be various, like different modality  With fixed variance {β 1 , β 2 , ..., β t }, x t can be explicitly expressed by x 0 , where α t := 1β t , ᾱt := t s=1 α s . Then a conditional U-Net  The variance μ σ can be learnable parameters or a fixed set of scalars, and both settings achieve comparable results in  As for how to infuse binary condition y in the U-Net, we follow the strategy in  Algorithm 1. Generation of WSSS prediction mask using differentiate conditional model with DDIM sampling Input: input image x with label y 1 , noise level Q, inference stage R, noise predictor θ , τ Output: prediction mask of label y 1 for all t from 1 to Q do ) end for return a"
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,2.2,Gradient Map w.r.t Condition,"Inspired by the finding in  When given the same images at noise level Q, but with different conditions, the noises predicted by the network θ are supposed to reflect the localization of target objects, that is equivalently x t-1 (x t , t, y 1 )x t-1 (x t , t, y 0 ) . This idea is quite similar to  ) in which, τ is the moving weight from f (y 1 ) towards f (y 0 ). The weight τ can not be too close to 1, otherwise there is no noticeable gap between x t-1 and x t-1 , and we find τ = 0.95 gives the best performance. Algorithm 1 shows the detailed workflow of obtaining the segmentation mask of samples with label y 1 . Notice that we iterate the process (5) for R steps, and the default R in this work is set as 10, much smaller than Q = 400. The purpose of R is to amplify the change of x t-1 since the condition does not change the predicted noise a lot in one step. In addition, we find that the guidance of additional classifier can further boost the WSSS task, by passing the gradient ˆ ← θ (x t )s √ 1ᾱt ∇ xt logp φ (y|x t ), where p φ is the classifier and s is the gradient scale. Then, in Algorithm 1, ˆ 1 and ˆ 0 have additional terms guided by gradient of y 1 and y 0 , respectively. The ablation studies of related hyperparameters can be seen in Sect. 3.2."
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,3,Experiments and Results,"Brain Tumor Segmentation. BraTS (Brain Tumor Segmentation challenge)  Only classification labels are used during training the diffusion models, and segmentation masks are used for evaluation in the test stage. For both datasets, we repeat the evaluation protocols for four times and report the average metrics and their standard deviation on test set."
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,3.1,Implementation Details,"As for model architecture, we use the same setting as in "
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,,Types Methods,Dice↑ mIoU↑ HD95↓ infer time CAM GradCAM  CAM GradCAM 
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,3.2,Results,"Comparison with State of the Arts. We benchmark our methods against previous WSSS works on two datasets in Table  From the results, we can make several key observations. Firstly, our proposed method, even without classifier guidance, outperform all other WSSS methods including the classifier guided diffusion model CG-Diff on both datasets for all three metrics. When classifier guidance is provided, the improvement gets even bigger, and CG-CDM can beat other methods regarding segmentation accuracy. Secondly, all WSSS methods have performance drop on kidney dataset compared with BraTS dataset. This demonstrates that the kidney segmentation task is a more challenging task for WSSS than brain tumor task, which may be caused by the small training size and diverse appearance across slices in the CHAOS dataset. Time Efficiency. Regarding inference time for different methods, as shown in Table  Ablation Studies. There are several important hyperparameters in our framework, noise level Q, number of iterations R, moving weight τ , and gradient scale s. The default setting is CG-CDM on BraTS dataset with Q = 400, R = 10, τ = 0.95, and s = 10. We evaluate the influence of one hyperparameter at a time by keeping other parameters at their default values. As illustrated in Fig. "
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,4,Conclusion,"In this paper, we present a novel weakly supervised semantic segmentation framework based on conditional diffusion models. Fundamentally, the essence of generative approaches on WSSS is maximizing the change in class-related areas while minimizing the noise in the background. Our methods are designed around this rule to enhance the state-of-the-art. First, existing work that utilizes a trained classifier to remove target objects leads to unpredictable distortion in other areas, thus we decide to iterate the reverse denoising process for as few steps as possible. Second, to amplify the difference caused by different conditions, we extract the semantic information from gradient of the noise predicted by the diffusion model. Finally, this rule also applies to all other designs and choice of hyperparameters in our framework. When compared with latest WSSS methods on two public medical image segmentation datasets, our method shows superior performance regarding both segmentation accuracy and inference efficiency."
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,,Fig. 1 .,
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,,Fig. 2 .,
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,,Fig. 3 .,
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,,Table 1 .,
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,,Table 2 .,
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,,,
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 72.
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,1,Introduction,"Glioma is one of the most common malignant brain tumors with varying degrees of invasiveness  With the rise of deep learning, researchers have begun to study deep learning-based image analysis methods  In recent years, models based on the self-attention mechanism, such as Transformer, have received widespread attention due to their excellent performance in Natural Language Processing (NLP)  While Transformer-based models have shown effectiveness in capturing long-range dependencies, designing a Transformer architecture that performs well on the SAMM-BTS task remains challenging. First, modeling relationships between 3D voxel sequences is much more difficult than 2D pixel sequences. When applying 2D models, 3D images need to be sliced along one dimension. However, the data in each slice is related to three views, discarding any of them may lead to the loss of local information, which may cause the degradation of performance  The contributions of our proposed method can be described as follows: 1) Based on Transformer, we construct dual-branch encoder and decoder layers that assemble two attention mechanisms, being able to model close-window and distant-window dependencies without any extra computational cost. 2) In addition to the traditional skipconnection structure, in the dual-branch decoder, we also establish an extra path to facilitate the decoding process. We design a Shifted-W-MCA-based global branch to build a bridge between the decoder and encoder, maintaining affluent information of the segmentation target during the decoding process. 3) For the multi-modal data adopt in the task of SAMM-BTS, we improve the channel attention mechanism in SE-Net by applying SE-weights to features from both branches in the encoder and decoder layers. By this means, we implicitly consider the importance of multiple MRI modalities and two window-based attention branches, thereby strengthening the fusion effect of the multi-modal information from a global perspective."
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,2,Methodology,Figure 
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,2.1,Dual-Branch in Encoder,As shown in Fig. 
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,,Shifted-W-MSA-Based Local Branch. The image embedding e,"] is fed into the local branch in the encoder block. In the process of window partition (denoted as WP), e i 1 is split into non-overlapping windows after a layer normalization (LN) operation to obtain the window matrix m i 1 . Since we set M as 2, the input of 4×4×4 size is uniformly divided into 8 windows of 2 × 2 × 2. Following 3D Swin-Transformer  where ẑi 1 represents the attention score after W -MSA calculation, "" Shifted-"" represents that we use the shifted-window partition and restoration in the second block of every encoder layer. o i 1 is the final output of the local branch. Shuffle-W-MCA-Based Global Branch. Through the local branch, the network still cannot model the long-distance dependencies between non-adjacent windows in the same layer. Thus, Shuffle-W-MCA is designed to complete the complementary task. After the window-partition process that converts the embedding , inspired by ShuffleNet  ( In the second block, we get the final output o i 2 of the layer through the same process."
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,2.2,Dual-Branch in Decoder,As shown in Fig. 
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,,Shifted-W-MCA-Based Global Branch.,"Apart from employing Shifted-W-MSA to form the local branch of the decoder layer, we design a novel Shifted-W-MCA mechanism for the global branch to ease the information loss during the decoding process and take full advantage of the features from the encoder layers. The global branch receives the query matrix from the split feature map d j 2 ∈ R D j ×H j ×W j ×[C j /2] , while receiving key and value matrices from the encoder block in the corresponding stage, denoted as Q j 2 , K e i , and V e i . The process of Shifted-W-MCA can be formulated as follows: where ẑj 2 denotes the attention score after MCA calculation, o j 2 denotes the final output of the global branch."
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,2.3,Channel-Attention-Based Dual-Branch Fusion,"As shown in Fig.  where is the attention weight of a single branch. Then, the weight vectors of the two branches are re-calibrated using a Softmax function. Finally, the weighted channel attention is multiplied with the corresponding scale feature map to obtain the refined output feature map with richer multi-scale feature information: where "" "" represents the operation of element-wise multiplication and ""Cat"" represents the concatenation. The concatenated output O, serving as the dual-branch output of a block in the encoder or decoder, implicitly integrates attention interaction information within individual branches across different channels/modalities, as well as across different branches."
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,2.4,Training Details,"For the loss function, the widely used cross entropy (CE) loss and Dice loss "
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,3,Experiments and Results,"Datasets. We use the Multimodal Brain Tumor Segmentation Challenge (BraTS 2021  Comparative Experiments. To evaluate the effectiveness of the proposed DBTrans, we compare it with the state-of-the-art brain tumor segmentation methods including six Transformer-based networks Swin-Unet "
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,4,Conclusion,"In this paper, we innovatively proposed an end-to-end model named DBTrans for multimodal medical image segmentation. In DBTrans, first, we well designed the dual-branch structures in encoder and decoder layers with Shifted-W-MSA, Shuffle-W-MCA, and Shifted-W-MCA mechanisms, facilitating feature extraction from both local and global views. Moreover, in the decoder, we establish a bridge between the query of the decoder and the key/value of the encoder to maintain the global context during the decoding process for the segmentation target. Finally, for the multi-modal superimposed data, we modify the channel attention mechanism in SE-Net, focusing on exploring the contribution of different modalities and branches to the effective information of feature maps. Experimental results demonstrate the superiority of DBTrans compared with the state-of-the-art medical image segmentation methods."
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,,Fig. 1 .,
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,,Fig. 2 .,
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,,( 1 ),
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,,Table 1 .,
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,,Table 2 .,
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,1,Introduction,"Convolutional neural networks (CNNs) have been widely used for medical image segmentation because of their speed and accuracy  With the introduction of Transformers  As image size reduction is usually required for large images, it is desirable to have a model that is robust to training image resolution so that the trained model can be applied to higher-resolution images with decent accuracy. Furthermore, as self-attention of Transformers allows better expressiveness through high-order channel and sample mixing  Experimental results on the BraTS'19 dataset "
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,2,Methodology,
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,2.1,Fourier Neural Operator,"FNO is a deep learning model for learning mappings between functions in PDEs without the PDEs provided  where u t (x) ∈ R du t is a function of x. W ∈ R du t+1 ×du t is a learnable linear transformation and σ accounts for normalization and activation. In our work, D ⊂ R 3 represents the 3D imaging space, and u t (x) are the outputs of hidden layers with d ut channels. K is the kernel integral operator with κ ∈ R du t+1 ×du t a learnable kernel function. As (Ku t ) is a convolution, it can be efficiently solved by the convolution theorem which states that the Fourier transform (F) of a convolution of two functions is the pointwise product of their Fourier transforms: Therefore, each pointwise product at k is realized as a matrix multiplication. When the fast Fourier transform is used in implementation, k ∈ N 3 are non-negative integer coordinates, and each k has a learnable R(k). As mainly low-frequency components are required for image segmentation, only k i ≤ k max,i corresponding to the lower frequencies in each dimension i are used to reduce model parameters and computation time."
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,2.2,Hartley Neural Operator (HNO),"As the FNO requires complex number operations in the frequency domain, the computational requirements such as memory and floating point operations are higher than with real numbers. Therefore, we use the Hartley transform instead, which is an integral transform alternative to the Fourier transform  This is equivalent to applying a convolution layer with the kernel size of one in the frequency domain. We find that using (4) simplifies the computation and largely reduces the number of parameters without affecting the accuracy."
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,2.3,Hartley Multi-head Attention (MHA),"As real instead of complex numbers are used in (4), multi-head attention in  As k max can be much smaller than the image size for image segmentation, the sequence length (number of voxels) can be largely reduced. With (4), the query, key, and value matrices (Q, K, V ) of self-attention can be computed as: where Although N f can be relatively small, the computation and memory requirements of computing QK T can still be demanding. For example, k max = (14, 14, 10) corresponds to an attention matrix with around 246M elements. To remedy this, for each Q, K, and V , we group the feature vectors with a patch size of 2 × 2 × 2 voxels in the frequency domain and their matrix sizes become This reduces the number of elements in QK T by 64 times. The self-attention can then be computed as: where SELU represents the scaled exponential linear unit "
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,2.4,Network Architectures -HNOSeg and HartleyMHA,Figure 
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,2.5,Training Strategy,"The images of different modalities are stacked along the channel axis to provide a multi-channel input. As the intensity ranges across modalities can be quite different, intensity normalization is performed on each image of each modality. Image augmentation with rotation (axial, ±30 • ), shifting (±20%), and scaling ([0.8, 1.2]) is used and each image has an 80% chance to be transformed. The Adamax optimizer "
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,3,Experiments,
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,3.1,Data and Experimental Setups,"The dataset of BraTS'19 with 335 cases of gliomas was used, each with four modalities of T1, post-contrast T1, T2, and T2-FLAIR images with 240 × 240 × 155 voxels  1. V-Net-DS "
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,3.2,Results and Discussion,"Figure  For computation cost, Table "
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,4,Conclusion,"In this paper, based on the idea of FNO which has the properties of zeroshot super-resolution and global receptive field, we propose the HNOSeg and HartleyMHA models for resolution-robust and parameter-efficient 3D image segmentation. HNOSeg is FNO improved by the Hartley transform, residual connections, deep supervision, and shared parameters in the frequency domain. We further extend this concept for efficient multi-head attention in the frequency domain as HartleyMHA. Experimental results show that HNOSeg and HartleyMHA had similar accuracies as other tested segmentation models when trained with the original image resolution, but had superior performance when trained with images of much lower resolutions. HartleyMHA performed slightly better than HNOSeg and ran faster with less memory. With these advantages, HartleyMHA can be a promising alternative for 3D image segmentation especially when computational resources are limited."
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,,Fig. 1 .,
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,,Fig. 2 .,
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,,Fig. 3 .,
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,,,
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,,,
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,,Table 1 .,
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,,Table 2 .,
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,1,Introduction,"Stroke is the second leading cause of death worldwide, with over twelve million new cases reported annually. Projections estimate that one in four individuals will experience a stroke in their lifetime  Lesion delineation and quantification are critical components of stroke diagnosis, guiding clinical intervention and informing patient prognosis  To address these challenges, computational approaches have supported stroke lesion segmentation, including multi-context approaches from convolutional architectures and standard training schemes to achieve a latent lesion representation  To the best of our knowledge, this work is the first effort to explore and model complementary lesion annotations from DWI and ADC modalities, utilizing a novel multimodal cross-attentional autoencoder. The proposed architecture employs modality-specific encoders to extract stroke patterns and generate a cross-domain embedding that enhances lesion segmentations for both modalities. To mitigate noise propagation, cross-attention mechanisms were integrated with skip connections between encoder and decoder blocks with similar spatial dimensions. The approach was evaluated on 82 ADC and DWI sequences, annotated by two neuroradiologists, evidencing generalization capabilities and outperforming typical unimodal models. The proposed method makes significant contributions to the field, as outlined below. -A multi-context and multi-segmentation deep attentional architecture that retrieves lesion annotations from ADC and DWI, sequences. -An architecture generalization study by training the model with one reference radiologist but evaluating with respect to two radiologist annotations."
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,2,Proposed Approach,"This work introduces a multi-path encoder-decoder representation with the capability to recover stroke segmentations from ADC and DWI parameters. Independent branches are encoded for each modality and fused into an embedding representation. Then, independent decoder paths are evolved to recover stroke lesions, supported by cross-attention modules from the respective encoders. A multi-segmentation task is herein implemented to update the proposed deep representation. The general description of the proposed approach is illustrated in Fig.  Fig. "
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,2.1,Multimodal Attentional Mechanisms,"In this work, a dual path encoder-decoder is introduced to recover stroke shape findings, observed from ADC and DWI. From the decoder section, two independent convolutional paths build a deep representation of each considered parameter. Both paths project studies into a low-dimensional embedding space that code stroke associated findings (see Fig.  Then, a single and complementary latent representation is achieved by the concatenation of both encoders. We hypothesize that such late fusion from embedding space allows a more robust integration of salient features related to stroke findings in both modalities. Hence, independent decoder branches, for each modality, are implemented from the fused embedding representation. These decoder paths are updated at each level from skip connections, implemented as cross-attentional mechanisms. Particularly, these connections control the contribution of the encoder X i e into the decoder X i d representation, at each convolutional level i. The refined encoder features are computed as: where σ 1 and σ 2 are ReLU and Sigmoid activations, respectively (see in Fig. "
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,2.2,Multi-segmentation Minimization,"A multi-segmentation loss function is here introduced to capture complementary textural patterns from both MRI parameters and improve lesion segmentation. This function induces the learning of highly correlated features present in ADC and DWI images. Specifically, the loss function is defined as: Here, γ is a focusing parameter, α is a weight balancing factor, and ŶADC and ŶDWI are the annotations for ADC and DWI, respectively. It should be noted, that ischemic stroke segmentation is highly imbalanced, promoting background reconstruction. To overcome this issue, each modality loss is weighted by a set of class weight maps (C ∈ N H×W ) that increase the contribution of lesion voxels and counteract the negative impact of class imbalance. From this multi-segmentation loss function, the model can more effectively capture complementary information from both ADC and DWI, leading to improved lesion segmentation."
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,2.3,ADC-DWI Data and Experimental Setup,"A retrospective study was conducted to validate the capability of the proposed approach, estimating ischemic stroke lesion segmentation. The study collected 82 studies of patients with stroke symptoms at a clinical center between October 2021 and September 2022. Each study has between 20 to 26 slices with resolutions ranging in each axis as x, y = [0.83 -0.94] and z = [5.50 -7.20] mm. After reviewing imaging studies, the studies were categorized into control (n = 7) or ischemic stroke (n = 75) studies. Control patients with stroke symptoms were included to diversify tissue samples, potentially enhancing our models' ability to segment stroke lesion tissue. Each study included ADC and DWI sequences obtained using a Toshiba Vantage Titan MRI scanner. Two neuro-interventional radiologists, each with more than five years of experience, individually annotated each medical sequence based on clinical records using the MRIcroGL software  The dataset was stratified into two subsets for training and testing. The training set consisted of 51 studies with annotations from a single expert (R1), while the test set contained 31 studies with manual delineations from two experts. In addition, a validation set was created from 11 studies of the training set. This validation set was used to measure the model's capability to segment ischemic stroke lesions on unseen data during training and to save the best weights before predicting on the test set. The test set was specifically designed to evaluate the model's performance on quantifying differences between lesion annotations. It allowed for a comparison of annotations between modalities of the same radiologist and between radiologists with the same modality. Furthermore, the test set was used to assess the generalizability of the model when compared to annotations made by the two radiologists. Segmentation capability was measured from the standard Dice score (DSC), precision, and recall. Regarding the model, the proposed symmetric architecture consists of five convolutional levels (32, 64, 128, 256, and 512 filters for both networks and 1024 filters in the bottleneck) with residual convolutional blocks (3 × 3, batch normalization, and ReLU activation). For training, the architecture was updated over 300 epochs with a batch size of 16, and a Focal loss with an AdamW optimizer with γ = 2 and α = 0.25. A linear warm-up strategy was used in the first 90 epochs (learning rate from 5e-3 and decay 1e-5), followed by 210 epochs of cosine decay. The class weight map values were set to 1 for non-lesion voxels and 3 for lesion voxels. The whole studies were normalized and resized to 224 × 224 pixels. Registration and skull stripping were not considered. Furthermore, data augmentation such as random brightness and contrast, flips, rotations, random elastic transformations, and random grid and optical distortions were applied to the slices. As a baseline, the proposed architecture was trained with a single modality (ADC, DWI). The code for our work can be found in https://gitlab.com/bivl2ab/research/2023-cross-domain-stroke-segmentation."
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,3,Evaluation and Results,"From a retrospective study, we conducted an analysis to establish radiologist agreement in both ADC and DWI, following the Kappa coefficient from lesion differences. In Fig.  As expected, a substantial level of agreement among radiologists was found in both ADC and DWI sequences, with an average agreement of 65% and 70%, respectively. It should be noted however that in 25% of studies there exist between fair and moderate agreement. These discrepancies may be associated with acute strokes, where radiological findings are negligible and challenging even for expert radiologists. In such a sense, it should be noted that computational approaches may be biased for expert annotations, considering the substantial agreement. Also, among modalities is reported a kappa value of around 65% that evidences the importance to complement findings to achieve more precise stroke delineations. Hence, an ablation study was then carried out to measure the capabilities of the proposed approach to recover segmentation, regarding multi-segmentation segmentation and the contribution of cross-attention mechanisms. The capability of architecture was measured with respect to the two radiologist delineations and with respect to each modality. Figure  Table "
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,4,Conclusions and Perspectives,"The proposed approach demonstrates the potential of a multi-context and multisegmentation cross-attention encoder-decoder architecture for recovering stroke segmentations from DWI and ADC images. The proposed approach has the potential to improve the accuracy and reliability of stroke segmentation in clinical practice, nevertheless, it introduces potential challenges in scalability. Future research directions include the development of attention mechanisms that can fuse additional MRI modalities to improve the accuracy of stroke segmentation. Furthermore, the study could benefit from a more exhaustive preprocessing for the medical images. Finally, an extension of the agreement study to include a larger number of radiologists and studies could provide valuable insights into the reliability and consistency of lesion annotation in MRI studies."
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,,Fig. 2 .,
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,,Fig. 3 .,
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,,Fig. 4 .,
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,,Fig. 5 .,
Ischemic Stroke Segmentation from a Cross-Domain Representation in Multimodal Diffusion Studies,,Table 1 .,
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,1,Introduction,"Medical image segmentation (MIS) is a crucial component in medical image analysis, which aims to partition an image into distinct regions (or segments) that are semantically related and/or visually similar. This process is essential for clinicians to, among others, perform qualitative and quantitative assessments of various anatomical structures or pathological conditions and perform imageguided treatments or treatment planning  Given the inter-domain heterogeneity resulting from variations in imaging protocols, scanner manufacturers, etc.  To address ViTs' data-hunger, in this work, we propose MDViT, a novel fixedsize multi-domain ViT trained to adaptively aggregate valuable knowledge from multiple datasets (domains) for improved segmentation. In particular, we introduce a domain adapter that adapts the model to different domains to mitigate negative knowledge transfer caused by inter-domain heterogeneity. Besides, for better representation learning across domains, we propose a novel mutual knowledge distillation approach that transfers knowledge between a universal network (spanning all the domains) and additional domain-specific network branches. We summarize our contributions as follows: (1) To the best of our knowledge, we are the first to introduce multi-domain learning to alleviate ViTs' data-hunger when facing limited samples per dataset. "
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,2,Methodology,"Let X ∈ R H×W ×3 be an input RGB image and Y ∈ {0, 1} H×W be its groundtruth segmentation mask. Training samples {(X, Y )} come from M datasets, each representing a domain. We aim to build and train a single ViT that performs well on all domain data and addresses the insufficiency of samples in any of the datasets. We first introduce our baseline (BASE), a ViT with hierarchical transformer blocks (Fig.  is the number of patches and C i is the channel dimension. We use the same position embedding as "
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,2.1,MDViT,"MDViT consists of a universal network (spanning M domains) and M auxiliary network branches, i.e., peers, each associated with one of the M domains. The universal network is the same as BASE, except we insert a domain adapter (DA) in each factorized MHSA to tackle negative knowledge transfer. Further, we employ a mutual knowledge distillation (MKD) strategy to transfer domain-specific and shared knowledge between peers and the universal network to enhance representation learning. Next, we will introduce DA and MKD in detail."
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Domain Adapter (DA):,"In multi-domain adaptive training, some methods build domain-specific layers in parallel with the main network  Attention Generation generates attention for each head. We first pass a domain label vector m (we adopt one-hot encoding m ∈ R M but other encodings are possible) through one linear layer with a ReLU activation function to acquire a domain-aware vector d ∈ R K r . K is the channel dimension of features from the heads. We set the reduction ratio r to 2. After that, similar to  where ψ is a softmax operation across heads and Information Selection adaptively selects information from different heads. After getting the feature from the hth head, we utilize a h to calibrate the information along the channel dimension: ũh"
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Mutual Knowledge Distillation (MKD):,"Distilling knowledge from domainspecific networks has been found beneficial for universal networks to learn more robust representations  Each Auxiliary Peer is trained on a small, individual dataset specific to that peer (Fig. "
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,2.2,Objective Function,"Similar to Combo loss  We set both α and β to 0.5. L am seg does not optimize DA to avoid interfering with the domain adaptation learning. After training, we discard the auxiliary peers and only utilize the universal network for inference."
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,3,Experiments,
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Datasets and Evaluation Metrics:,"We study 4 skin lesion segmentation datasets collected from varied sources: ISIC 2018 (ISIC)  Implementation Details: We conduct 3 training paradigms: separate (ST), joint (JT), and multi-domain adaptive training (MAT), described in Sect. 1, to train all the models from scratch on the skin datasets. Images are resized to 256 × 256 and then augmented through random scaling, shifting, rotation, flipping, Gaussian noise, and brightness and contrast changes. The encoding transformer blocks' channel dimensions are [64, 128, 320, 512] (Fig. "
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Comparing Against BASE:,"In Table  We employ the two fixed-size (i.e., independent of M ) multi-domain algorithms proposed by Rundo et al. "
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Ablation Studies and Plug-in Capability of DA:,"We conduct ablation studies to demonstrate the efficacy of DA, MKD, and auxiliary peers. Table "
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,4,Conclusion,"We propose a new algorithm to alleviate vision transformers (ViTs)' datahunger in small datasets by aggregating valuable knowledge from multiple related domains. We constructed MDViT, a robust multi-domain ViT leveraging novel domain adapters (DAs) for negative knowledge transfer mitigation and mutual knowledge distillation (MKD) for better representation learning. MDViT is nonscalable, i.e., has a fixed model size at inference time even as more domains are added. The experiments on 4 skin lesion segmentation datasets show that MDViT outperformed SOTA data-efficient medical image segmentation ViTs and multi-domain learning methods. Our ablation studies and application of DA on other ViTs show the effectiveness of DA and MKD and DA's plug-in capability."
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Fig. 1 .,
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Fig. 2 .,
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Table 1 .,
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Table 2 .,
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Table 3 .,
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 43.
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,2,Methods,Problem Setting. Given limited labeled data Overview. The overview of the proposed method is shown in Fig. 
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,2.1,H-UNet for 3D Medical Image Segmentation,The backbone model H-UNet is a variant of H-DenseUNet 
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,2.2,Class Imbalance Alleviation via Prototype-Based Classifier,"Prototype-based classifier, leveraging prototypes instead of a parameterized predictor to make predictions, presents efficacy for semantic segmentation  as a set of prototypes associated with C classes for segmentation. Note that each class may have more than one prototype (i.e., K) due to the intra-class heterogeneity exhibited in medical images  (1) where s a,c ∈ [-1, 1] denotes the pixel-class similarity between pixel a and its closest prototype of class c. S(, ) is the similarity measure (i.e., cosine similarity). F i [a] denotes the extracted features of pixel a, and Y i [a] denotes the predicted probability. • 2 stands for the 2 normalization. Meanwhile, the features F are also fed into LC parameterized by The probability distribution of pixel a estimated by LC is defined as: These two classifiers complete each other during training. In the early training phase, LC dominates knowledge learning and provides PC with discriminative features to initialize and update the prototypes (See Sect. 2.4). With the addition of PC, the feature embedding space is further regularized, along with intraclass features being more compact and inter-class features being more separated, which in turn benefits the learning of LC."
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,2.3,Pseudo Label Refinement via Multi-level Tree Filters,"Rather than directly using the teacher's high-confidence prediction to generate pseudo labels  Graph Construction: First, we respectively represent the topology of the low-level original unlabeled image and the high-level hybrid features as two 4connected planar graphs: G * = {V * , E * } where V * is the vertex set associated with each pixel and E * is the edge set, and * ∈ {low, high}. The weight of the edge connecting two adjacent nodes a and b indicates their dissimilarity, which is defined by: Multi-level Filter Construction: Based on the two MSTs, we build the lowlevel TF F low and the high-level TF F high . The filter weight F * a,b of any two nodes is obtained by aggregating the MST edge weights along the path between the two nodes  Here, E * a,b is the edge set in the path from node a to node b. S G * mst (•) maps the distance of two vertices into a positive scalar which measures the pixel affinity. z a is a normalization factor, which is the summation of the similarity between node a and all other nodes in the MST. Cascaded Refinement: Lastly, we refine the teacher's prediction P with the two filters in a cascade manner to acquire high-quality pseudo labels Ŷ : where R(•, •) is the refinement process where each unlabeled pixel can contribute to the refinement for other pixels with a contribution proportional to their similarity. By exploiting the multi-level complementary features, i.e., the object boundary information encoded in F low and the semantic similarity encoded in F high , CRS shows superiority when a single TF fails in some cases. E.g., when two pixels of different classes have similar intensity values (pixel a and b in Fig. "
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,2.4,Network Training and Prototype Update,"The framework is trained by utilizing both the labeled data D L and the unlabeled data D U . For the labeled data, the supervised loss is defined as: where Y 2D is the prediction generated by the 2D UNet in H-UNet, and Y 2D is the transformation of the 3D ground truth Y by conducting T (See Sect. 2.1). l is the weighted sum of cross entropy loss and dice loss. For the unlabeled data, the student model is trained with the pseudo labels Ŷ refined by the proposed CRS. The unsupervised loss is defined as: (7) Following  Prototype Initialize and Update: The initialization of prototypes determines the PC performance. To get better prototypes, we first pretrain the network with the LC solely. Then, we collect pixelwise deep features and use K-means  where α is a momentum coefficient, i.e., 0.999. 3 Experiments and Results"
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,3.1,Datasets and Implementation Details,Datasets and Evaluation Metrics: The model is evaluated on two public cardiac MRI datasets: (1) The ACDC Implementation Details: We split the datasets into the training and validation set randomly at a ratio of 4:1 
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,3.2,Comparison with State-of-the-Arts,"Table  Self train  Among these, the proposed method outperforms competing methods across all datasets and label percentages, except for a slightly lower DSC than method Class-wise Sampling "
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,3.3,Ablation Studies,Please refer to Section B in the Supplementary material for more results in terms of average symmetric surface distance (ASSD) in voxel and the qualitative analyses of multi-level tree filters.
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,4,Conclusion,"The scarcity of pixel-level annotations affects the performance of deep neural networks for medical image segmentation. Moreover, the class imbalanced problem existing in the medical data can further exacerbate the model degradation. To address the problems, we propose a novel semi-supervised class imbalanced learning approach by additionally introducing the prototype-based classifier into the student model and constructing two multi-level tree filters to refine the pseudo labels for more robust learning. Experiments conducted on two public cardiac MRI datasets demonstrate the superiority of the proposed method."
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,,Fig. 1 .,
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,,Fig. 2 .,
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,,"F c,k 2",
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,,,
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,,Table 2 .,
Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_44.
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,1,Introduction,"Age-related macular degeneration (AMD) is a leading cause of blindness worldwide, primarily attributable to choroidal neovascularization (CNV)  Several methods have been proposed to segment CNV regions from OCTA images, including handcraft feature descriptors  Previously, Gal et al.  In this work, we propose a reliable boundary-guided network (RBGNet) to simultaneously segment both the CNV regions and vessels. Our proposed method is composed of a dual-branch encoder and a boundary uncertainty-guided multitask decoder. The dual-branch encoder is designed to capture both of the global long-range dependencies and the local context of CNVs with significant scale variations, while the proposed uncertainty-guided multi-task decoder is designed to strengthen the model to segment ambiguous boundaries. The uncertainty is achieved by approximating a Bayesian network through Monte Carlo dropout. The main contributions are summarized as follows: (a) We propose a multi-task joint optimization method to interactively learn shape patterns and boundary contours for more accurate segmentation of CNV regions and vessels. (b) We design a dual-stream encoder structure to take advantages of the CNN and transformer, which promote the network to effectively learn both local and global information. (c) We propose an uncertainty estimation-guided weight optimization strategy to provide reliable guidance for multi-task network training."
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,2,Proposed Method,"The proposed RBGNet comprises three primary components: a dual-stream encoder, a multi-task decoder, and an uncertainty-guided weight optimization strategy, as depicted in Fig. "
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,2.1,Dual-Stream Encoder,"The CNVs in OCTA images are of various shapes and a wide range of scales, which pose challenges to the accurate segmentation of CNVs. To extract the long-range dependencies of cross-scale CNV information, we employ VIT "
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,2.2,Multi-task Decoder,"The proposed multi-task decoder performs two main tasks: CNV region segmentation and vessel segmentation. The CNV region segmentation task contains two auxiliary subtasks including boundary prediction and shape regression. Each task is implemented at the end of the decoder using a 1 × 1 convolutional layer followed by a Sigmoid activation, as shown in Fig.  Region Segmentation: Region segmentation of CNV allows accurate assessment of lesion size. This task aims to accurately segment the entire CNV regions in OCTA images via boundary prediction and shape regression. Region segmentation is typically accomplished by categorizing individual pixels as either belonging to the CNV region or the background region. The purpose of region boundary prediction is to explicitly enhance the model's focus on ambiguous boundaries, allowing for more accurate region segmentation. The process of shape regression for region segmentation involves the transformation of boundary regression into a task of signed distance field regression. This is achieved by assigning a signed distance to each pixel, representing its distance from the boundary, with negative values inside the boundary, positive values outside the boundary, and zero values on the boundary. By converting the ground truth into a signed distance map (SDM), the network can learn CNV shape patterns from the rich shape pattern information contained in the SDMs. Vessel Segmentation: To improve the vessel segmentation performance, we propose to guide the model to focus on low-contrast vessel details by estimating their pixel uncertainty. Simultaneously, the complementary information from the CNV region segmentation task is further utilized to eliminate the interference of vessel pixels outside the region, thus better refining the vessel segmentation results. The proposed multi-task decoder improves the segmentation accuracy of regions and vessels by explicitly or implicitly using the information between individual tasks and optimizing each task itself."
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,2.3,Uncertainty-Guided Multi-Task Optimization,"Uncertainty Estimation: In contrast to traditional deep learning models that produce deterministic predictions, Bayesian neural networks  To learn the weight distribution of the network, we minimize the Kullback-Leibler (KL) scatter between the true posterior distribution and its approximation. The probability distribution of each pixel is obtained based on Dropout to sample the posterior weight distribution M times. Then, the mean P i of each pixel is used to generate the prediction, while the variance V i is used to quantify the uncertainty of the pixel. This process can be described as follows. Pm , and Uncertainty-Weighted Loss: In CNV region segmentation, the importance of each pixel may vary, especially for ambiguous boundaries, while assigning equal weights to all samples may not be optimal. To address this issue, uncertainty maps are utilized to assign increased weights to pixels with higher levels of uncertainty. This, in turn, results in a more substantial impact on the update of the model parameters. Moreover, the incorporation of multiple tasks can generate different uncertainty weights for a single image, enabling a more comprehensive exploration of CNV boundary features via joint optimization. We employ a combination of loss functions, including binary cross-entropy (BCE) loss, mean squared error (MSE) loss, and Dice loss, to optimize the model parameters across all tasks. However, for the region shape regression task, we restricted the loss functions to only BCE and MSE. We incorporate uncertainty weights into the BCE loss by weighting each pixel to guide uncertainty on model training, i.e., where y i and ŷi are respective ground truth and prediction for pixel i. The total loss function can be expressed as L = T t λ t L t , where L t denotes the loss function for t th task. λ t denotes the loss weight, obtained by averaging the uncertainty map of the corresponding task and normalizing them to sum to 1."
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,3,Experimental Results,
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,,Dataset:,"The proposed RBGNet was evaluated on a dataset consisting of 74 OCTA images obtained using the Heidelberg OCT2 system (Heidelberg, Germany). All images were from AMD patients with CNV progression, captured in a 3 × 3 mm 2 area centered at the fovea. The enface projected OCTA images of the avascular complex were used for our experiments. All the images were resized into a resolution of 384 × 384 for experiments. The CNV areas and vessels were manually annotated by one senior ophthalmologist, and then reviewed and refined by another senior ophthalmologist. All images were acquired with regulatory approvals and patient consents as appropriate, following the Declaration of Helsinki. Implementation Details: Our method is implemented based on the PyTorch framework with NVIDIA GeForce GTX 1080Ti. We train the model using an Adam optimizer with an initial learning rate of 0.0001 and a batch size of 4 for 300 epochs, without implementing a learning rate decay strategy. During training, the model inputs were subject to standard data augmentation pipelines, including random horizontal, vertical flips, random rotation, and random cropping. A 5-fold cross-validation approach is adopted to evaluate the performance."
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,,Comparison with State-of-the-Arts:,"To benchmark our model's performance, we compared it with several state-of-the-art methods in the medical image segmentation field, including U-Net "
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,3.1,Conclusion,"In summary, this study proposes a novel method to address the challenges of CNV segmentation in OCTA images. It incorporates a dual-branch encoder, multi-task optimization, and uncertainty-weighted loss to accurately segment CNV regions and vessels. The findings indicate that the utilization of crossscale information, multi-task optimization, and uncertainty maps improve CNV segmentations. The proposed method exhibits superior performance compared to state-of-the-art methods, which suggests potential clinical implications for the diagnosis of CNV-related diseases. Nevertheless, further research is needed to validate the effectiveness of the proposed approach in large-scale clinical studies."
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,,Fig. 1 .,
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,,Fig. 2 .,
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,,Fig. 3 .,
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,,Fig. 4 .,
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,,Table 1 .,
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,,9042 0.8328 0.0797 0.9464 0.9025 0.8229 0.0688 0.9455 proposed,
RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization,,Table 2 .,
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,2,Methodology,Overview. Figure 
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,2.1,Curvature-Based Feature Selection,"For a two-dimensional surface embedded in Euclidean space R 3 , two curvatures exist: Gaussian curvature and mean curvature. Compared with the Gaussian curvature, the mean curvature can better reflect the unevenness of the surface. Gong  where T , the values of α, β, and γ are -1/16, 5/16, -1. * denotes convolution, X represents the input image, and C is the mean curvature. Figure "
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,2.2,Information Entropy-Based Feature Selection,"As a statistical form of feature, information entropy  i n denotes the gray value of the center pixel in the n th 3 × 3 sliding window and j n denotes the average gray value of the remaining pixels in that window (teal blue box in Fig.  Each pixel on images corresponds to a gray or color value ranging from 0 to 255. However, each element in the feature map represents the activation level of the convolution filter at a particular position in the input image. Given an input feature F x , as shown in Fig. "
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,2.3,Instructive Feature Enhancement,"Although IFE can be applied to various deep neural networks, this study mainly focuses on widely used segmentation networks. Figure  While the input images are encoded into the feature space, the different channel features retain textures in various directions and frequencies. Notably, the information contained by the same channel may vary across different images, which can be seen in Fig.  Compute the histogram of (i, j). ext_k = kernel_size//2 P h i s t (i , j ) = f hist(i,j) / ((H + ext k ) × (W + ext k )) E = sum -P h i s t (i , j ) × log2 P h i s t (i , j ) their 2 nd channel features both focus on the edge details. By preserving the raw features, the channel features that contribute more to the segmentation of the current object can be enhanced by dynamically selecting from the input features. Naturally, it is possible to explicitly increase the sensitivity of the model to the channel information. Specifically, for input image X, the deep feature can be obtained by an encoder with the weights θ x : F x = Encoder (X, θ x ), our IFE can be expressed as: F x is the selected feature map and S is the quantification method (see Fig. "
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,3,Experimental Results,"Cosmos55k. To construct the large-scale Cosmos55k, 30 publicly available datasets  Implementation Details. Cosmos55k comprises 55,023 images, with 31,548 images used for training, 5,884 for validation, and 17,591 for testing. We conducted experiments using Pytorch for UNet  Ablation Studies. Choosing a suitable selection ratio r is crucial when applying IFE to different networks. Different networks' encoders are not equally capable of extracting features, and the ratio of channel features more favorable to the segmentation result varies. To analyze the effect of r, we conducted experiments using UNet "
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,4,Conclusion,"In order to benchmark the general DMIS, we build a large-scale dataset called Cosmos55k. To balance universality and accuracy, we proposed an approach (IFE) that can select instructive feature channels to further improve the segmentation over strong baselines against challenging tasks. Experiments showed that IFE can improve the performance of classic models with slight modifications in the network. It is simple, universal, and effective. Future research will focus on extending this approach to 3D tasks."
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,,Fig. 1 .,
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,,Fig. 2 .,
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,,Fig. 3 .,
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,,Fig. 4 .,
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,,Fig. 5 .,
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,,Fig. 6 .Fig. 7 .,
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,,Table 1 .,
Instructive Feature Enhancement for Dichotomous Medical Image Segmentation,,Table 2 .,
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,1,Introduction,"Deep learning methods have demonstrated their tremendous potential when it comes to medical image segmentation. However, the success of most existing architectures relies on the availability of pixel-level annotations, which are difficult to produce  Several generative models attempt to generalize to a target modality by performing unsupervised domain adaptation through image-to-image translation and image reconstruction. In  In this paper, we propose M-GenSeg, a novel training strategy for crossmodality domain adaptation, as illustrated in Fig. "
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,2,Methods,
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,2.1,M-GenSeg: Semi-supervised Segmentation,"In order to disentangle the information common to A and P, and the information specific to P, we split the latent representation of each image into a common code c and a unique code u. Essentially, the common code contains information inherent to both domains, which represents organs and other structures, while the unique code stores features like tumor shapes and location. In the two fol-lowing paragraphs, we explain P→A and A→P translations for source images. The same process is applied for target images by replacing S notation with T . Presence to Absence Translation. Given an image S P of modality S in the presence domain P, we use an encoder E S to compute the latent representation [c S P , u S P ]. A common decoder G S com takes as input the common code c S P and generates a healthy version S PA of that image by removing the apparent tumor region. Simultaneously, both common and unique codes are used by a residual decoder G S res to output a residual image Δ S PP , which corresponds to the additive change necessary to shift the generated healthy image back to the presence domain. In other words, the residual is the disentangled tumor that can be added to the generated healthy image to create a reconstruction S PP of the initial diseased image:  Like approaches in  Modality Translation. Our objective is to learn to segment tumor lesions in a target modality by reusing potentially scarce image annotations in a source modality. Note that for each modality m ∈ {S, T }, M-GenSeg holds a segmentation decoder G m seg that shares most of its weights with the residual decoder G m res , but has its own set of normalization parameters and a supplementary classifying layer. Thus, through the Absence and Presence translations, these segmenters have already learned how to disentangle the tumor from the background. However, supervised training on a few example annotations is still required to learn how to transform the resulting residual representation into appropriate segmentation maps. While this is a fairly straightforward task for the source modality using pixel-level annotations, achieving this for the target modality is more complex, justifying the second unsupervised translation objective between source and target modalities. Based on the CycleGan  for the T→S→T cycle. Note that to perform the domain adaptation, training the model to segment only the pseudo-target images generated by the S→T modality generator would suffice (in addition to the diseased/healthy target translation). However, training the segmentation on diseased source images also imposes additional constraints on encoder E S , ensuring the preservation of tumor structures. This constraint proves beneficial for the translation decoder G T as it generates pseudo-target tumoral samples that are more reliable. Segmentation is therefore trained on both diseased source images S P and their corresponding synthetic target images S T P , when provided with annotations y S . To such an extent, two segmentation masks are predicted ŷS = G S seg • E S (S P ) and ŷST = G T seg • E T (S T P )."
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,2.2,Loss Functions,"Segmentation Loss. For the segmentation objective, we compute a soft Dice loss  Reconstruction Losses. L mod cyc and L Gen rec respectively impose pixel-level image reconstruction constraints on modality translation and GenSeg tasks. Note that L 1 refers to the standard L1 norm: Moreover, like in  Adversarial Loss. For the healthy-diseased translation adversarial objective, we compute a hinge loss L Gen adv as in GenSeg, learning to discriminate between pairs of real/synthetic images of the same output domain and always in the same imaging modality, e.g. S A vs S PA . In the modality translation task, the L mod adv loss is computed between pairs of images of the same modality without distinction between domains A and P , e.g. {S A , S P } vs {T S A , T S P }. Overall Loss. The overall loss for M-GenSeg is a weighted sum of the aforementioned losses. These are tuned separately. All weights sum to 1. First, λ Gen adv , λ Gen rec , and λ Gen lat weights are tuned for successful translation between diseased and healthy images. Then, λ mod adv and λ mod cyc are tuned for successful modality translation. Finally, λ seg is tuned for segmentation performance."
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,2.3,Implementation Details,"Training and Hyper-Parameters. All models are implemented using PyTorch and are trained on one NVIDIA A100 GPU with 40 GB memory. We used a batch size of 15, an AMSGrad optimizer (β 1 = 0.5 and β 2 = 0.999) and a learning rate of 10 -4 . Our models were trained for 300 epochs and weights of the segmentation model with the highest validation Dice score were saved for evaluation. The same on-the-fly data augmentation as in  Architecture. One distinct encoder, common decoder, residual/segmentation decoder, and modality translation decoder are used for each modality. The architecture used for encoders, decoders and discriminators is the same as in "
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,3,Experimental Results,
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,3.1,Datasets,Experiments were performed on the BraTS 2020 challenge dataset 
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,3.2,Model Evaluation,"Domain Adaptation. We compared M-GenSeg with AccSegNet  Reaching Supervised Performance. We report that, when the target modality is completely unannotated, M-GenSeg reaches 90% of UAGAN's performance (vs 81% and 85% for AttENT and AccSegNet). Further experiments showed that with a fully annotated source modality, it is sufficient to annotate 25% of the target modality to reach 99% of the performance of fully-supervised UAGAN (e.g. M-GenSeg: 0.861 ± 0.004 vs UAGAN: 0.872 ± 0.003 for T1 → T2 experiment). Thus, the annotation burden could be reduced with M-GenSeg."
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,3.3,Ablation Experiments,We conducted ablation tests to validate our methodological choices. We report in Table 
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,4,Conclusion,"We propose M-GenSeg, a new framework for unpaired cross-modality tumor segmentation. We show that M-GenSeg is an annotation-efficient framework that greatly reduces the performance gap due to domain shift in cross-modality tumor segmentation. We claim that healthy tissues, if adequately incorporated to the training process of neural networks like in M-GenSeg, can help to better delineate tumor lesions in segmentation tasks. However, top performing methods on BraTS are 3D models. Thus, future work will explore the use of full 3D images rather than 2D slices, along with more optimal architectures. Our code is available: https://github.com/MaloADBA/MGenSeg_2D."
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,,Fig. 1 .,
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,,,
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,,Fig. 2 .,
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,,Fig. 3 .,
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,,Fig. 4 .,
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,,,
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,,Table 1 .,
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_14.
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,1,Introduction,"Simultaneous multi-index quantification (i.e., max diameter (MD), center point coordinates (X o , Y o ), and Area), segmentation, and uncertainty prediction of liver tumor have essential significance for the prognosis and treatment of patients  Recently, an increasing number of works have been attempted on liver tumor segmentation or quantification  To the best of our knowledge, although many works focus on the simultaneous quantization, segmentation, and uncertainty in medical images (i.e., heart  In this study, we propose an edge-aware multi-task network (EaMtNet) that integrates the multi-index quantification (i.e., center point, max-diameter (MD), and Area), segmentation, and uncertainty. Our basic assumption is that the model should capture the long-range dependency of features between multimodality and enhance the boundary information for quantification, segmentation, and uncertainty of liver tumors. The two parallel CNN encoders first extract local feature maps of multi-modality NCMRI. Meanwhile, to enhance the weight of tumor boundary information, the Sobel filters are employed to extract edge maps that are fed into edge-aware feature aggregation (EaFA) as prior knowledge. Then, the EaFA module is designed to select and fuse the information of multi-modality, making our EaMtNet edge-aware by capturing the long-range dependency of features maps and edge maps. Lastly, the proposed method estimates segmentation, uncertainty prediction, and multi-index quantification simultaneously by combining multi-task and cross-task joint loss. The contributions of this work mainly include: (1) For the first time, multiindex quantification, segmentation, and uncertainty of the liver tumor on multimodality NCMRI are achieved simultaneously, providing a time-saving, reliable, and stable clinical tool. (2) The edge information extracted by the Sobel filter enhances the weight of the tumor boundary by connecting the local feature as prior knowledge. (3) The novel EaFA module makes our EaMtNet edge-aware by capturing the long-range dependency of features maps and edge maps for feature fusion. The source code will be available on the author's website."
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,2,Method,"The EaMtNet employs an innovative approach for simultaneous tumor multiindex quantification, segmentation, and uncertainty prediction on multimodality NCMRI. As shown in Fig. "
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,2.1,CNN Encoder for Feature Extraction,In Step 1 of Fig. 
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,2.2,Edge-Aware Feature Aggregation(EaFA) for Multi-modality Feature Selection and Fusion,"In Step 2 of the proposed model, the feature maps (i.e., g i T 2 , g i DW I ) and the edge maps (i.e., edge i T 2 , edge i DW I ) are fed into EaFA for multi-modality feature fusion with edge-aware. In particular, the EaFA makes the EaMtNet edge-aware by using the Transformer to capture the long-range dependency of feature maps and edge maps. Specifically, the feature maps and edge maps are first flattened to the 1D sequence corresponding to X 1D ∈ R N ×P 2 and E 1D ∈ R 2×Q 2 , respectively. Where N = 2 × C means the channel number C of the last convolutional layer from the two parallel encoders. (P, P ) and (Q, Q) represent the resolution of each feature map and each edge map, respectively. On the basis of the 1D sequence, to make the feature fusion with edge awareness, the operation of position encoding is performed not only on feature maps but also on edge maps. The yielded embeddings Z ∈ R N ×P 2 +2×Q 2 can serve as the input sequence length for the multi-head attention layer in Transformer. The following operations in our EaFA are similar to the traditional Transformer  where query Q, key K, and value V are all vectors of the flattened 1D sequences of X 1D and E 1D . W O i is the projection matrix, and 1 is the scaling factor."
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,2.3,Multi-task Prediction,"In Step 3 of Fig.  where z i is the probability of pixel x belonging to category i. When a pixel has high entropy, it means that the network is uncertain about its classification. Therefore, pixels with high entropy are more likely to be misclassified. In other words, its entropy will decrease when the network is confident in a pixel's label. Under the constraints of uncertainty, the EaMtNet can effectively rectify the errors in tumor segmentation because the uncertainty estimation can avoid overconfidence and erroneous quantification  where ŷs represents the prediction, and y i represents the ground truth label. The sum is performed on S pixels, ŷi task represents the predicted multi-index value, and y i task represents the ground truth of multi-index value, task ∈ {MD, X, Y , Area}."
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,3,Experimental Results and Discussion,"For the first time, EaMtNet has achieved high performance with the dice similarity coefficient (DSC) up to 90.01 ± 1.23%, and the mean absolute error (MAE) of the MD, X o , Y o and Area are down to 2.72 ± 0.58 mm,1.87±0.76 mm, 2.14 ± 0.93 mm and 15.76 ± 8.02 cm 2 , respectively. Dataset and Configuration. An axial dataset includes 250 distinct subjects, each underwent initial standard clinical liver MRI protocol examinations with corresponding pre-contrast images (T2FS [4mm]) and DWI [4mm]) was collected. The ground truth was reviewed by two abdominal radiologists with 10 and 22 years of experience in liver imaging, respectively. If any interpretations demonstrated discrepancies between the reviewers, they would re-evaluate the examinations together and reach a consensus. To align the paired images of T2 and DWI produced at different times. We set the T2 as the target image and the DWI as the source image to perform the pre-processing of non-rigid registration between T2 and DWI by using the Demons non-rigid registration method. It has been widely used in the field of medical image registration since it was proposed by Thirion  Inspired by the work  Accurate Segmentation. The segmentation performance of EaMtNet has been validated and compared with three state-of-the-art (SOTA) segmentation methods (TransUNet  Ablation Study. To verify the contributions of edge-aware feature aggregation (EaFA) and uncertainty, we performed ablation study and compared and performance of different networks. First, we removed the EaFA and used concatenate, meaning we removed fusion multi-modality (No-EaFA). Then, we removed the uncertainty task (No-Uncertainty). The quantitative analysis results of these ablation studies are shown in Table  Performance Comparison with State-of-the-Art. The EaMtNet has been validated and compared with three SOTA segmentation methods and two SOTA quantification methods (i.e., ResNet-50 "
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,4,Conclusion,"In this paper, we have proposed an EaMtNet for the simultaneous segmentation and multi-index quantification of liver tumors on multi-modality NCMRI. The new EaFA enhances edge awareness by utilizing boundary information as prior knowledge while capturing the long-range dependency of features to improve feature selection and fusion. Additionally, multi-task leverages the prediction discrepancy to estimate uncertainty, thereby improving segmentation and quantification performance. Extensive experiments have demonstrated the proposed model outperforms the SOTA methods in terms of DSC and MAE, with great potential to be a diagnostic tool for doctors."
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,,Fig. 2 .,
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,,Fig. 3 .,
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,,,
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,,Table 1 .,
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,,Table 2 .,
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,,Table 3 .,
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,1,Introduction,"Automatic detection of brain tumors from Magnetic Resonance Imaging (MRI) is complex, tedious, and time-consuming because there are a lot of missed, misinterpreted, and misleading tumor-like lesions in the images of the brain tumors  With the rapid development of CNNs, the accuracies of different visual tasks are constantly improved. However, the increasingly complex network architecture in CNN-based models, such as ResNet  RepVGG "
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,2,Methods,The architecture of the proposed RCS-YOLO network is shown in Fig. 
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,2.1,RepVGG/RepConv ShuffleNet,"Inspired by ShuffleNet, we design a structural reparameterized convolution based on channel shuffle. Figure  Given that the feature dimensions of an input tensor are C × H × W , after the channel split operator, it is divided into two different channel-wise tensors with equal dimensions of C×H ×W . For one of the tensors, we use the identity branch, 1 × 1 convolution, and 3 × 3 convolution to construct the training-time RCS. At the inference stage, the identity branch, 1 × 1 convolution, and 3 × 3 convolution are transformed to 3 × 3 RepConv by using structural reparameterization. The multi-branch topology architecture can learn abundant information about features during the training time, simplified single-branch architecture can save memory consumption during the inference time to achieve fast inference. After the multi-branch training of one of the tensors, it is concatenated to the other tensor in a channel-wise manner. The channel shuffle operator is also applied to enhance information fusion between two tensors so that the depth measurement between different channel features of the input can be realized with low computational complexity. When there is no channel shuffle, the output feature of each group only relates to the input feature within a group of grouped convolutions, and outputs from a certain group only relate to the input within the group. This blocks information flow between channel groups and weakens the ability of feature extraction. When channel shuffle is used, input and output features are fully related where one convolution group takes data from other groups, enabling more efficient feature information communication between different groups. The channel shuffle operates on stacked grouped convolutions and allows more informative feature representation. Moreover, assuming that the number of groups is g, for the same input feature, the computational complexity of channel shuffle is 1 g times that of a generic convolution. Compared with the popular 3 × 3 convolution, during the inference stage, RCS uses the operators including channel split and channel shuffle to reduce the computational complexity by a factor of 2, while keeping the inter-channel information exchange. Moreover, using structural reparameterization enables deep representation learning from input features during the training stage, and reduction of inference-time memory consumption to achieve fast inference."
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,2.2,RCS-Based One-Shot Aggregation,"The One-Shot Aggregation (OSA) module has been proposed to overcome the inefficiency of dense connections in DenseNet, by representing diversified features with multi-receptive fields and aggregating all features only once in the last feature maps. VoVNet V1  We develop an RCS-OSA module by incorporating RCS developed in Sect. 2.1 for OSA, as shown in Fig.  Assuming n to be 4, FLOPs of the proposed RCS-OSA and Efficient Layer Aggregation Networks (ELAN) "
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,2.3,Detection Head,"To further reduce inference time, we decrease the number of detection heads comprised of RepVGG and IDetect from 3 to 2. The YOLOv5, YOLOv6, YOLOv7, and YOLOv8 have three detection heads. However, we use only two feature layers for prediction, reducing the number of original nine anchors with different scales to four and using the K-means unsupervised clustering method to regenerate anchors with different scales. The corresponding scales are (87, 90), (127, 139), (154, 171), "
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3,Experiments and Results,
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3.1,Dataset Details,"To evaluate the proposed RCS-YOLO model, we used the brain tumor detection 2020 dataset (Br35H) "
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3.2,Implementation Details,"For model training and inference, we used Ubuntu 18.04 LTS, Intel R Xeon R Gold 5218 CPU processor, CUDA 12.0, and cuDNN 8.2. GPU is GeForce RTX 3090 with 24G memory size. The networking development framework is Pytorch 1.9.1. The Integrated Development Environment (IDE) is PyCharm. We uniformly set epoch 150, the batch size as 8, image size as 640 × 640. Stochastic Gradient Descent (SGD) optimizer was used with an initial learning rate of 0.01 and weight decay of 0.0005."
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3.3,Evaluation Metrics,"In this paper, we choose precision, recall, AP 50 , AP 50:95 , FLOPs, and Frames Per Second (FPS) as comparative metrics of detection effect to determine the advantages and disadvantages of the model. Taking IoU = 0.5 as the standard, precision, and recall can be calculated by the following equations:"
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,,P recision = T P T P + F P,"(3) where T P represents the number of positive samples correctly identified as positive samples, F P represents the number of negative samples incorrectly identified as positive samples and F N represents the number of positive samples incorrectly identified as negative samples. AP 50 is the area under the precision-recall (PR) curve formed by precision and recall. For AP 50:95 , divide 10 IoU threshold of 0.5:0.05:0.95 to acquire the area under the PR curve, then average the results. FPS represents the number of images detected by the model per second."
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3.4,Results,"To highlight the accuracy and rapidity of the proposed model for the detection of brain tumor medical image data set, Table  It can be seen that RCS-YOLO with the advantages of incorporating the RCS-OSA module performs well. Compared with YOLOv7, the FLOPs of the object detectors of this paper decrease by 8.8G, and the inference speed improves by 43.4 FPS. In terms of detection rate, precision improves by 0.024; AP 50 increases by 0.01; AP 50:95 by 0.006. Also, RCS-YOLO is faster and more accurate than YOLOv6-L v3.0 and YOLOv8l. Although the AP 50:95 of RCS-YOLO equals that of YOLOv8l, it doesn't obscure the essential advantage of RCS-YOLO. The results clearly show the superior performance and efficiency of our method, compared to the state-of-the-art for brain tumor detection. As shown in supplementary material Fig. "
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3.5,Ablation Study,We demonstrate the effectiveness of the proposed RCS-OSA module in YOLObased object detectors. The results of RepVGG-CSP in Table 
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,4,Conclusion,"We developed an RCS-YOLO network for fast and accurate medical object detection, by leveraging the reparameterized convolution operator RCS based on channel shuffle in the YOLO architecture. We designed an efficient one-shot aggregation module RCS-OSA based on RCS, which serves as a computational unit in the backbone and neck of a new YOLO network. Evaluation of the brain MRI dataset shows superior performance for brain tumor detection in terms of both speed and precision, as compared to YOLOv6, YOLOv7, and YOLOv8 models."
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,,Fig. 1 .,
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,,Fig. 2 .,
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,,Fig. 3 .,
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,,,
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,,Table 1 .,
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,,Table 2 .,
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_57.
Certification of Deep Learning Models for Medical Image Segmentation,1,Introduction,"For the past decade, deep neural networks have dominated the computer vision community and provided near human performance on many different tasks, including classification  Nowadays, segmentation models are gaining widespread adoption in modern clinical practice and are being used with increasing frequency, making the results of these models critical for many patients. However, it is now commonly known that neural networks can be vulnerable to adversarial attacks  In this paper, we provide the first method for certified robustness in the context of segmentation for medical imaging. We leverage the randomized smoothing strategy "
Certification of Deep Learning Models for Medical Image Segmentation,2,Related Work,"Since the discovery of adversarial attacks  More specifically, in the context of classification, a previous work  Although a large body of work has focused on constructing defenses for classification and segmentation tasks in the context of medical imaging, certified defenses are under-studied by the medical community. In this paper, we propose to leverage randomized smoothing and diffusion models for certified segmentation on medical datasets, setting the first baseline for this challenging problem and certifying popular segmentation architectures."
Certification of Deep Learning Models for Medical Image Segmentation,3,Randomized Smoothing,"Randomized smoothing is a model agnostic technique, proposed by Cohen et al.  Randomized smoothing is a procedure to construct a new smooth classifier g given any base classifier f . Let N (0, σ 2 I) be a Gaussian distribution of mean 0 and variance σ, then, the smooth classifier g is defined as follows: Cohen et al.  However, since it is not possible to compute g at x exactly, they proposed using Monte Carlo algorithms as an alternative approach for estimating g(x) using random sampling. In order to obtain a reliable estimate of the probability g(x), they also suggested a method that involves generating n samples of η from a normal distribution N (0, σ 2 I) and evaluating f (x + η) for each sample. The resulting counts for each class in Y are then used to estimate probability p y and the radius R with confidence 1 -α (where α is a value between 0 and 1). If the confidence level cannot be achieved (for example, due to insufficient samples), the method will abstain from providing an estimate. More recently, Fischer et al.  To obtain a smooth classifier, it is necessary to add random noise to the input of the classifier. However, this creates a trade-off between accuracy and robustness. If low variance noise is added, accuracy won't be impacted significantly, but the certified radius will remain low. Conversely, adding high variance noise can improve certificates but at the expense of accuracy. To address this issue, Cohen et al. proposed a simple trick of training the network with noise injection during the training phase. While this method may reduce accuracy when evaluating the classifier with noise during the certification process, it can also help mitigate the trade-off between accuracy and robustness. One can note that during training, the network's objective is to learn to ignore the noise and classify at the same time. To improve the natural as well as the certified accuracy, Salman et al.  In this paper, we leverage randomized smoothing and diffusion probabilistic models to obtain state-of-the-art results on certified segmentation for medical imaging. To the best of our knowledge, we are the first to propose a comprehensive study on certified segmentation for medical imaging."
Certification of Deep Learning Models for Medical Image Segmentation,4,Diffusion Probabilistic Models for Certification,"The training of a Denoising Diffusion Probabilistic Model (DDPM) is an iterative process that involves adding a small amount of noise at every step of the diffusion process until random noise is reached. The reverse process then starts from random noise and generates a new image that conforms to the data distribution. Since DDPMs are inherently iterative denoising models, we can leverage this property for randomized smoothing. The idea would be to start the reverse process with a noisy image, rather than Gaussian noise, enabling the DDPM to output an image that resembles the original image. Similar to Carlini et al. "
Certification of Deep Learning Models for Medical Image Segmentation,5,Experiments and Results,"Datasets: We perform experiments on 5 different publicly available datasets. All datasets were divided to 70% for training, 10% for validation, and 20% for testing. The testing set is the one used to compute certified results. Chest X-rays Datasets: JSRT dataset  Skin Lesion: Skin images with their annotations provided by the ISIC 2018 boundary segmentation challenge  Implementation Details: We train three different segmentation models namely, a UNet  Results and Discussion: For all five datasets, we compute a certified Dice score and certified mean Intersection over Union (IoU). We also report the percentage of abstentions (% ) representing the mean number of pixels on which the model's prediction confidence was insufficient with respect to the radius R. The lower the percentage of abstentions the better the segmentation model is. In Table  Qualitative results are provided in Fig.  Table  Regarding the denoiser, we used a single-step denoising strategy, i.e., we perform a single call to the DDPM to compute the denoised image from t * to t = 0. Another strategy could be to iteratively denoise from t * , t * -1, ... until t = 0. However, this implies predicting a denoised image multiple times and in the end, may result in images with unwanted artifacts. We perform multi-step denoising experiments and report results in Table "
Certification of Deep Learning Models for Medical Image Segmentation,6,Conclusion,"In this paper, we present the first work on certified segmentation for medical imaging, and extensively evaluate it on five different datasets and three deep learning segmentation models. Our technique leverages off-the-shelf denoising and segmentation models and provides the highest certified Dice and mIoU on multi-class and binary segmentation of five different datasets. With that, we are able to remove the overhead of having to train and fine-tune models specifically for robustness. This paradigm shift alleviates the dilemma of having to choose between highly accurate segmentation models or models robust to attacks. We hope that this work serves as a baseline for the unexplored yet critical topic of certified segmentation in medical imaging. Future work will involve extending our approach to 3D medical imaging modalities as well as exploring the realm of certified classification."
Certification of Deep Learning Models for Medical Image Segmentation,,Fig. 1 .,
Certification of Deep Learning Models for Medical Image Segmentation,,,
Certification of Deep Learning Models for Medical Image Segmentation,,Table 1 .,
Certification of Deep Learning Models for Medical Image Segmentation,,Table 2 .,
Certification of Deep Learning Models for Medical Image Segmentation,,Table 3 .,
Certification of Deep Learning Models for Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_58.
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,Multi-site semi-supervised learning (MS-SSL),"The unlabeled image pool can be quickly enriched via the support from partner clinical centers with low barriers of entry (only unlabeled images are required) Data heterogeneity due to different scanners, scanning protocols and subject groups, which violate the typical SSL assumption of i.  "
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,1,Introduction,"Prostate segmentation from magnetic resonance imaging (MRI) is a crucial step for diagnosis and treatment planning of prostate cancer. Recently, deep learningbased approaches have greatly improved the accuracy and efficiency of automatic prostate MRI segmentation  Regarding quantity , the abundance of unlabeled data serves as a way to regularize the model and alleviate overfitting to the limited labeled data. Unfortunately, such ""abundance"" may be unobtainable in practice, i.e., the local unlabeled pool is also limited due to restricted image collection capabilities or scarce patient samples. As a specific case shown in Table  Here, we define this new SSL scenario as multi-site semi-supervised learning (MS-SSL), allowing to enrich the unlabeled pool with multi-site heterogeneous images. Being an under-explored scenario, few efforts have been made. To our best knowledge, the most relevant work is AHDC  In this work, we propose a more generalized framework called Categorylevel regularized Unlabeled-to-Labeled (CU2L) learning, as depicted in Fig. "
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,2,Methods,
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,2.1,Problem Formulation and Basic Architecture,"In our scenario of MS-SSL, we have access to a local target dataset D local (consisted of a labeled sub-set D l local and an unlabeled sub-set D u local ) and the external unlabeled support datasets D u e = m j=1 D u,j e , where m is the number of support centers. Specifically, with n u unlabeled scans. with n j unlabeled samples. Considering the large variance on slice thickness among different centers  where L l sup is the supervised guidance from local labeled data and L u denotes the additional guidance from the unlabeled data. λ is a trade-off weight scheduled by the time-dependent ramp-up Gaussian function  where w max and t max are the maximal weight and iteration, respectively. The key challenge of MS-SSL is the proper design of L u for robustly exploiting multi-site unlabeled data {D u local , D u e } to support the local center."
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,2.2,Pseudo Labeling for Local Distribution Fitting,"As mentioned above, supervised-like learning is advocated for local unlabeled data to help the model fit local distribution better. Owning the self-ensembling property, the teacher model provides relatively stable pseudo labels for the student model. Given the predicted probability map P u,t local of X u local from the teacher model, the pseudo label Ŷ u,t local corresponds to the class with the maximal posterior probability. Yet, with limited local labeled data for training, it is difficult to generate high-quality pseudo labels. Thus, for each pixel, if max c (p u,t local ) ≥ δ, where c denotes the c-th class and δ is a ramp-up threshold ranging from 0.75 to 0.9 as training goes, this pixel will be included in loss calculation. Considering that the cross-entropy loss has been found very sensitive to label noises  , where P u,s local denotes the prediction of X u local from the student model. The Dice loss is calculated for each of the K equally-sized regions of the image, and the final loss is obtained by taking their mean. Such a regional form "
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,2.3,Category-Level Regularized Unlabeled-to-Labeled Learning,"Unlabeled-to-Labeled Learning. Inherently, the challenge of MS-SSL stems from intra-class variation, which results from different imaging protocols, disease progress and patient demographics. Inspired by prototypical networks  . Likewise, the background prototype c u(bg) e can also be obtained. Considering the possible unbalanced sampling of prostate-containing slices, EMA strategy across training steps (with a decay rate of 0.9) is applied for prototype update. Then, as shown in Fig.  Category-Level Regularization. Being a challenging scheme itself, the above U2L learning can only handle minor intra-class variation. Thus, proper mechanisms are needed to alleviate the negative impact of significant shift and multiple distributions. Specifically, we introduce category-level regularization, which advocates class prototype alignment between local and external data, to regularize the distribution of intra-class features from arbitrary external data to be closer to the local one, thus reducing the difficulty of U2L learning. In U2L, we have obtained prototypes from local unlabeled data {c  where mean squared error is adopted as the distance function d(•, •). The weight of background prototype alignment is smaller due to less relevant contexts."
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,Stability Under Perturbations.,"Although originally designed for typical SSL, encouraging stability under perturbations  where mean squared error is also adopted as the distance function d(•, •). Overall, the final loss for the multi-site unlabeled data is summarized as:"
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,3,Experiments and Results,"Materials. We utilize prostate T2-weighted MR images from six different clinical centers (C1-6)  Implementation and Evaluation Metrics. The framework is implemented on PyTorch using an NVIDIA GeForce RTX 3090 GPU. Considering the large variance in slice thickness among different centers, we adopt the 2D architecture. Specifically, 2D U-Net  Comparison Study. Table  Ablation Study. To evaluate the effectiveness of each component, we conduct an ablation study under the setting with 6 local labeled scans, as shown in Fig. "
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,4,Conclusion,"In this work, we presented a novel Category-level regularized Unlabeled-to-Labeled (CU2L) learning framework for semi-supervised prostate segmentation with multi-site unlabeled MRI data. CU2L robustly exploits multi-site unlabeled data via three tailored schemes: local pseudo-label learning for better local distribution fitting, category-level regularized unlabeled-to-labeled learning for exploiting the external data in a distribution-insensitive manner and stability learning for further enhancing robustness to heterogeneity. We evaluated our method on prostate MRI data from six different clinical centers and demonstrated its superior performance compared to other semi-supervised methods."
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,Fig. 1 .,
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,Fig. 2 .,
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,,
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,,
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,Fig. 3 .,
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,Table 1 .,
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,Table 1,
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,Table 2 .,
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,,,
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,1,Introduction,"Radiology plays an important role in the diagnosis of some pulmonary infectious diseases, such as the COVID-19 pneumonia outbreak in late 2019  Ariadne's thread, the name comes from ancient Greek myth, tells of Theseus walking out of the labyrinth with the help of Ariadne's golden thread. Therefore, it is of importance to design automatic medical image segmentation algorithms to assist clinicians in developing accurate and fast treatment plans. Most of the biomedical segmentation methods  -We propose a language-driven segmentation method for segmenting infected areas from lung x-ray images. Source code of our method see: https://github.com/Junelin2333/LanGuideMedSeg-MICCAI2023 -The designed GuideDecoder in our method can adaptively propagate sufficient semantic information of the text prompts into pixel-level visual features, promoting consistency between two modalities. -We have cleaned the errors contained in the text annotations of QaTa-COV19 "
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,2,Method,"The overview of our proposed method is shown in Fig.  Visual Encoder and Text Encoder. The Visual Encoder used in the model is ConvNeXt-Tiny  GuideDecoder. Due to our modular design, visual features and textual features are encoded independently by different encoders. Therefore, the design of the decoder is particularly important, as we can only fuse multi-modal features from different encoders in post stage. The structure of GuideDecoder is shown in Fig.  The input textual features first go through a projection module (i.e. Project in the figure) that aligns the dimensionality of the text token with that of the image token and reduces the number of text tokens. The projection process is shown in Eq. 1. where W T is a learnable matrix, Conv(•) denotes a 1 × 1 convolution layer, and σ(•) denotes the ReLU activation function. Given an input feature T ∈ R L×D , the output projected features is , where M is the number of tokens after projection and C 1 is the dimension of the projected features, consistent with the dimension of the image token. For the input visual features I ∈ R H×W ×C1 , after adding the position encoding we use self-attention to enhance the visual information in them to obtain the evolved visual features. The process is shown in Eq. 2. where MHSA(•) denotes Multi-Head Self-Attention layer, LN (•) denotes Layer Normalization, and finally the evolved visual features f i ∈ R H×W ×C1 with residuals could be obtained. After those, the multi-head cross-attention layer is adopted to propagate fine-grained semantic information into the evolved image features. To obtain the multi-modal feature f c ∈ R H×W ×C1 , the output further computed by layer normalization and residual connection: where MHCA(•) denotes multi-head cross-attention and α is a learnable parameter to control the weight of the residual connection. Then, the multi-modal feature f c ∈ R (H×W )×C1 would be reshaped and upsampling to obtain f c ∈ R H ×W ×C1 . Finally the f c is concatenated with f s ∈ R H ×W ×C2 on the channel dimension, where f s is the low-level visual feature obtained from visual encoder via skip connection. The concatenated features are processed through a convolution layer and a ReLU activation function to obtain the final decoded output where [•, •] represents the concatenate operation on the channel dimension."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,3,Experiments,
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,3.1,Dataset,"The dataset used to evaluate our method performance is the QaTa-COV19 dataset  Besides, we found some obvious errors (e.g. misspelled words, grammatical errors and unclear referents) in the extended text annotations. We have fixed these identified errors and contacted the authors of LViT to release a new version of the dataset. Dataset see Github link: https://github.com/HUANGLIZI/LViT."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,3.2,Experiment Settings,"Following the file name of the subjects in the original train set, we split the training set and the validation set uniformly in the ratio of 80% and 20%. Therefore, the training set has a total of 5716 samples, the validation set has 1429 samples and the test set has 2113 samples. All images are cropped to 224 × 224 and the data is augmented using a random zoom with 10% probability. We used a number of open source libraries including but not limited to PyTorch, MONAI  We used three metrics to evaluate the segmentation results objectively: Accuracy, Dice coefficient and Jaccard coefficient. Both Dice and Jaccard coefficient calculate the intersection regions over the union regions of the given predicted mask and ground truth, where the Dice coefficient is more indicative of the segmentation performance of small targets."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,3.3,Comparison Experiments,We compared our method with common mono-modal medical image segmentation methods and with the LViT previously proposed by Li et al. The quantitative results of the experiment are shown in Table 
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,3.4,Ablation Study,Our proposed method introduces semantic information of text in the decoding process of image features and designs the GuideDecoder to let the semantic information in the text guide the generation of the final segmentation mask. We performed an ablation study on the number of GuideDecoder used in the model and the results are shown in the Table  Table 
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,,Method Acc,Dice Jaccard w/o text 0.9610 0.8414 0.7262 1 layer 0.9735 0.8920 0.8050 2 layers 0.9748 0.8963 0.8132 3 layers 0.9752 0.8978 0.8144 As can be seen from the Table 
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,3.5,Extended Study,"Considering the application of the algorithm in clinical scenarios, we conducted several interesting extension studies based on the QaTa-COV19 dataset with the text annotations. It is worth mentioning that the following extended studies were carried out on our proposed method."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,,Impact of Text Prompts at Different Granularity on Segmentation,"Performance. In Sect. 3.1 we mention that each sample is extended to a text annotation with three parts containing positional information at different granularity, as shown in the Fig. "
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,,Impact of the Size of Training Data on Segmentation Performance.,As shown in Table 
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,4,Conclusion,"In this paper, we propose a language-driven method for segmenting infected areas from lung x-ray images. The designed GuideDecoder in our method can adaptively propagate sufficient semantic information of the text prompts into pixel-level visual features, promoting consistency between two modalities. The experimental results on the QaTa-COV19 dataset indicate that the multi-modal segmentation method based on text-image could achieve better performance compared to the image-only segmentation methods. Besides, we have conducted several extended studies on the information granularity of the text prompts and the size of the training data, which reveals the flexibility of multi-modal methods in terms of the information granularity of text and demonstrates that multi-modal methods have a significant advantage over image-only methods in terms of the size of training data required."
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,,Fig. 1 .,
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,,Table 1 .,
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,,Table 3 .,
Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images,,Table 4 .,
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,1,Introduction,"Segmenting the prostate anatomy and detecting tumors is essential for both diagnostic and treatment planning purposes. Hence, the task of developing domain generalisable prostate MRI segmentation models is essential for the safe translation of these models into clinical practice. Deep learning models are susceptible to textural shifts and artefacts which is often seen in MRI due to variations in the complex acquisition protocols across multiple sites  The most common approach to tackle domain shifts is with data augmentation  The contributions of this paper are summarized as follows: 1. This work considers shape compositionality to enhance the generalisability of deep learning models to segment the prostate on MRI. 2. We use cellular sheaves to aid compositionality for segmentation as well as improve tumour localisation."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,2,Preliminaries,
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,2.1,Persistent Homology,Topological data analysis is a field which extracts topological features from complex data structures embedded in a topological space. One can describe a topological space through its connectivity which can be captured in many forms. One such form is the cubical complex. The cubical complex C is naturally equipped to deal with topological spaces represented as volumetric grid structured data such as images 
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,2.2,Cellular Sheaves,"The homology of segmentation maps provides a useful tool for analysing global topology but does not describe how local topology is related to construct global topological features. Sheaf theory provides a way of composing or 'gluing' local data together to build a global object (new data) that is consistent with the local information  We can consider a topological space, Y such as a segmentation output divided into a finite number of subspaces, {∅, Y 1 , Y 2 ...Y n } which are the base spaces for Y or equivalently the patches in a segmentation map. If we sequentially glue base spaces together in a certain order to form increasingly larger subspaces of Y starting with the ∅, one can construct a filtration of Y such that; We neatly formalise the subspaces and how subspaces are glued together with a poset. A poset (P ) is a partially ordered set defined by a relation, ≤ between elements in P which is reflexive, antisymmetric, and transitive  A cellular sheaf, F over a poset is constructed by mapping, each element, p ∈ P to a vector space F(p) over a fixed field which preserves the ordering in P by linear transformations, ρ .,. which are inclusion maps in this case "
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,3,Related Work,There have been various deep learning based architectures developed for prostate tumour segmentation  RandConv 
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,4,Methods,
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,4.1,Shape Equivariant Learning,"Given spatial, T s and textural, T i transformations of the input space, X , the goal is to learn an encoder, Φ e to map X to lower dimensional embedding features, E which are shape equivariant and texture invariant as shown in Eq. 2. We assume T2 and ADC MRI images share the same spatial information and only have textural differences. We exploit this idea in Fig. "
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,4.2,Shape Component Learning,"We posit that there is limited shape variation in the low dimensional embedding space across subjects which can be fully captured in N discrete shapes. N discrete shapes form a shape dictionary, D shown in Fig.  ie j 2 and m = 3048. Next, sampling D such that z 1 i is replaced by e k produces the spatially quantized embedding space ẑ. Straight-through gradient approximation is applied for backpropagation through the sampling process to update z 1 and D "
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,4.3,Cellular Sheaves for Shape Composition,Shapes in D sampled with a uniform prior can lead to anatomically implausible segmentations after composition which we tackle through the language of cellular sheaves to model the connectivity of patches in an image which provides a connectivity-based loss function.
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Composition:,"The quantised embedding space, ẑ, is split into c groups. The composition of each group in ẑ to form each class segmentation, Y c in the output, Y involves two steps. Initially, a decoder with grouped convolutions equal to the number of classes followed by the softmax function maps, ẑ ∈ R 128×16×16×12 to C ∈ R p×c×256×256×24 where p is the number of patches for each class c. The second step of the composition uses a cellular sheaf to model the composition of Y c by gluing the patches together in an ordered manner defined by a poset while tracking its topology using persistent homology. This in turn enforces D to be sampled in a topological preserving manner as input into the decoder/composer to improve both the local and global topological correctness of each class segmentation output, Y c after composition."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Illustration:,"We illustrate our methodology of using cellular sheaves with a simple example in Fig.  Each element in P is associated with a subspace in V such that the inclusion relationship is satisfied. Therefore, in Fig. "
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Implementation,"We construct cellular sheaves, F over P c and P c and minimise the distance between these cellular sheaves. We firstly plot persistence diagrams, D from the set of vectors (τ i , τ j ) in F(P c i ) and F( P c i ). Next, we minimise the total p th Wasserstein distance (topological loss) between the persistence diagrams D(F(P c i )) and D(F( P c i )) shown in Eq. 4 where η : D(F(P c i )) → D(F( P c i )) is a bijection between the persistence diagrams "
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,5.2,Results,"In the task of anatomical segmentation, the first two columns of Table  Here, we demonstrate that our method improves segmentation performance in all evaluation metrics compared to the baseline, nn-UNet and the other SDG methods. Similar findings are noted for the domain shift from the internal dataset to the RUNMC data in the ProstateX2 dataset (second two columns of Table  In Table "
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,6,Conclusion,"In conclusion, we propose shape compositionality as a way to improve the generalisability of segmentation models for prostate MRI. We devise a method to learn texture invariant and shape equivariant features used to create a dictionary of shape components. We use cellular sheaf theory to help model the composition of sampled shape components from this dictionary in order to produce more anatomically meaningful segmentations and improve tumour localisation."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Fig. 1 .,
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Fig. 2 .,
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,:,
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Table 1 .,
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Table 2 .,
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 24. Pre-processing: All images are resampled to 0.5 × 0.5 × 3 mm, centre cropped to 256 × 256 × 24 and normalised between 0 and 1."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Model:,"In order to address the anisotropic characteristics of Prostate MRI images, we have chosen a hybrid 2D/3D UNet as our baseline model. We use the same encoder and decoder architecture as the baseline model in our method. See supplementary material for further details."
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,,Comparison:,"We compare our method with the nnUNet  Training: In all our experiments, the models were trained using Adam optimization with a learning rate of 0.0001 and weight decay of 0.05. Training was run for up to 500 epochs on three NVIDIA RTX 2080 GPUs. The performance of the models was evaluated using the Dice score, Betti error  In our ablation studies, the minimum number of shape components required in D for the zonal and zonal + tumour segmentation experiments was 64 and 192 respectively before segmentation performance dropped. See supplementary material for ablation experiments analysing each component of our framework."
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,1,Introduction,"Transformers  The ConvNeXt architecture marries the scalability and long-range spatial representation learning capabilities of Vision  In this work, we maximize the potential of a ConvNeXt design while uniquely addressing challenges of limited datasets in medical image segmentation. We present the first fully ConvNeXt 3D segmentation network, MedNeXt, which is a scalable Encoder-Decoder network, and make the following contributions: MedNeXt achieves state-of-the-art performance against baselines consisting of Transformer-based, convolutional and large kernel networks. We show performance benefits on 4 tasks of varying modality (CT, MRI) and sizes (ranging from 30 to 1251 samples), encompassing segmentation of organs and tumors. We propose MedNeXt as a strong and modernized alternative to standard ConvNets for building deep networks for medical image segmentation."
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,2,Proposed Method,
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,2.1,Fully ConvNeXt 3D Segmentation Architecture,"In prior work, ConvNeXt  1. Depthwise Convolution Layer: This layer contains a Depthwise Convolution with kernel size k ×k ×k, followed by normalization, with C output channels. We use channel-wise GroupNorm "
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,2.,Expansion Layer:,"Corresponding to a similar design in Transformers, this layer contains an overcomplete Convolution Layer with CR output channels, where R is the expansion ratio, followed by a GELU  MedNeXt is convolutional and retains the inductive bias inherent to Conv-Nets that allows easier training on sparse medical datasets. Our fully ConvNeXt architecture also enables width (more channels) and receptive field (larger kernels) scaling at both standard and up/downsampling layers. Alongside depth scaling (more layers), we explore these 3 orthogonal types of scaling to design a compound scalable MedNeXt for effective medical image segmentation (Sect. 2.4).  "
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,2.2,Resampling with Residual Inverted Bottlenecks,"The original ConvNeXt design utilizes separate downsampling layers which consist of standard strided convolutions. An equivalent upsampling block would be standard strided transposed convolutions. However, this design does not implicitly take advantage of width or kernel-based ConvNeXt scaling while resampling. We improve upon this by extending the Inverted Bottleneck to resampling blocks in MedNeXt. This is done by inserting the strided convolution or transposed convolution in the first Depthwise Layer for Downsampling and Upsampling MedNeXt blocks respectively. The corresponding channel reduction or increase is inserted in the last compression layer of our MedNeXt 2× Up or Down block design as in Fig.  In doing so, MedNeXt fully leverages the benefits from Transformer-like inverted bottlenecks to preserve rich semantic information in lower spatial resolutions in all its components, which should benefit dense medical image segmentation tasks."
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,2.3,UpKern: Large Kernel Convolutions Without Saturation,"Large convolution kernels approximate the large attention windows in Transformers, but remain prone to performance saturation. ConvNeXt architectures in classification of natural images, despite the benefit of large datasets such as ImageNet-1k and ImageNet-21k, are seen to saturate at kernels of size 7 × 7 × 7 "
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,2.4,"Compound Scaling of Depth, Width and Receptive Field","Compound scaling  The largest 70-MedNext-block architecture uses high values of both R and B (MedNeXt-L) and is used to demonstrate the ability of MedNeXt to be significantly scaled depthwise (even at standard kernel sizes). We further explore large kernel sizes and experiment with k = {3, 5} for each configuration, to maximize performance via compound scaling of the MedNeXt architecture. 3 Experimental Design"
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,3.1,"Configurations, Implementation and Baselines",We use PyTorch 
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,3.2,Datasets,"We use 4 popular tasks, encompassing organ as well as tumor segmentation tasks, to comprehensively demonstrate the benefits of the MedNeXt architecture -1) Beyond-the-Cranial-Vault (BTCV) Abdominal CT Organ Segmentation "
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,4,Results and Discussion,
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,4.1,Performance Ablation of Architectural Improvements,"We ablate the MedNeXt-B configuration on AMOS22 and BTCV datasets to highlight the efficacy of our improvements and demonstrate that a vanilla ConvNeXt is unable to compete with existing segmentation baselines such as nnUNet. The following are observed in ablation tests in  3. The performance boost in large kernels is seen to be due to the combination of UpKern with a larger kernel and not merely a longer effective training schedule (Upkern vs Trained 2×), as a trained MedNeXt-B with kernel 3×3×3 retrained again is unable to match its large kernel counterpart. This highlights that the MedNeXt modifications successfully translate the ConvNeXt architecture to medical image segmentation. We further establish the performance of the MedNeXt architecture against our baselines -comprising of convolutional, transformer-based and large kernel baselines -on all 4 datasets. We discuss the effectiveness of the MedNeXt on multiple levels."
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,4.2,Performance Comparison to Baselines,"There are 2 levels at which MedNeXt successfully overcomes existing baselines -5 fold CV and public testset performance. In 5-fold CV scores in Table  Furthermore, in leaderboard scores on official testsets (Fig. "
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,5,Conclusion,"In comparison to natural image analysis, medical image segmentation lacks architectures that benefit from scaling networks due to inherent domain challenges such as limited training data. In this work, MedNeXt is presented as a scalable Transformer-inspired fully-ConvNeXt 3D segmentation architecture customized for high performance on limited medical image datasets. We demonstrate Med-NeXt's state-of-the-art performance across 4 challenging tasks against 7 strong baselines. Additionally, similar to ConvNeXt for natural images "
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,,Fig. 1 .,
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,,-,
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,,Table 1 .,
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,,Table 1 (,
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,,Table 2 .,
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,1,Introduction,"Light-sheet microscopy is a powerful imaging modality that allows for fast and high-resolution imaging of large samples, such as the whole brain of the Supported by NIH R01NS110791, NIH R01MH121433, NIH P50HD103573, and Foundation of Hope. mouse  Accurate 3D nuclei instance segmentation plays a crucial role in identifying and delineating individual nuclei within three-dimensional space, which is essential for understanding the complex structure and function of biological tissues in the brain. Previous  It is a common practice to use overlapped image stacks to stitch the intensity image (continuous values) by weighted averaging from multiple estimations  Stitching 2D nuclei instances in each image slice is considerably easier than stitching across image slices due to the finer image resolution in the X-Y plane. However, as shown in Fig.  In the experiments, we have comprehensively evaluated the segmentation and stitching accuracy (correspondence matching between 2D instances) and wholebrain NIS results (both visual inspection and quantitative counting results). Compared to no stitching, Our deep stitching model has shown a significant improvement in the whole-brain NIS results with different state-of-the-art models, indicating its potential for practical applications in the field of neuroscience."
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,2,Methods,"Current state-of-the-art NIS methods, such as Mask-RCNN "
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,2.1,Graph Contextual Model,"Problem Formulation. Our graph contextual model takes a set of partial nuclei instances, sliced by the inter-slice gap, as input. These nuclei instances can be obtained using a 2D instance segmentation method, such as Mask-RCNN. The output of our model is a collection of nuclei-to-nuclei correspondences, which enable us to stitch together the NIS results from different image slices (by running NIS separately). We formulate this correspondence matching problem as a knowledge graph learning task where the links between nodes in the graph contextual model represent the probability of them belonging to the same nuclei instance. In this regard, the key component of NIS stitching becomes seeking for a relationship function that estimates the likelihood of correspondence based on the node features, i.e., image appearance of to-be-stitched 2D nuclei instances. Machine Learning Components in Graph Contextual Model. First, we construct an initial contextual graph G = {V, E} for each 2D nucleus instance x (i.e., image appearance vector). The set of nodes V = {x i |D(x, x i ) > δ} includes all neighboring 2D nuclei instances, where the distance between the centers of two instances is denoted by D, and δ is a predefined threshold. The matrix E ∈ R N ×N represents the edges between nodes, where N is the number of neighboring instances. Specifically, we compute the Intersection over Union (IoU) between the two instances and set the edge weight as e ij = IoU (x i , x j ). Second, we train the model on a set of contextual graphs G to recursively (1) find the mapping function γ to describe the local image appearance on each graph node and (2) learn the triplet similarity function ψ. -Graph feature representation learning. For the k th iteration, we enable two connected nodes to exchange their feature representations constrained by the current relationship topology e k ij by the k th layer of the deep stitching model. In this context, we define the message-passing function as: Following the popular learning scheme in knowledge graphs  (2)"
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,2.2,Hierarchical Stitching Framework for Whole-Brain NIS,"Our graph contextual model is able to stitch the NIS results across the intraslice gap areas in X/Y-Z plane. As demonstrated in Fig.  1. Resolve intra-slice gap in X-Y plane. Suppose that each within-stack NIS result overlaps with its neighboring image stack in the X-Y plane. Then, we can resolve the intra-slice gap problem in X-Y plane in three steps: (i) identify the duplicated 2D nuclei instances from multiple overlapped image stacks, (ii) find the representative NIS result from the ""gap-free"" image stack, and (iii) unify multiple NIS estimations by using the ""gap-free"" NIS estimation as the appearance of the underlying 2D nuclei. The effect of this solution is shown in Fig. "
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,2.3,Implementation Details,"We empirically use 18.75 µm for the overlap size between two neighboring image stacks in X-Y plane. The conventional NIS method is trained using 128 × 128 patches. For the graph contextual model, the MLPs consist of 12 fully-connected layers. Annotated imaging data has been split into training, validation, and testing sets in a ratio of 6:1:1. Adam is used with lr = 5e -4 as the optimizer, Dropout = 0.5, and focal loss as the loss function. 3 Experiments"
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,3.1,Experimental Settings,"Stitching Methods Under Comparison. We mainly compare our stitching method with the conventional analytic approach by IoU (Intersection over Union) matching, which is widely used in object detection with no stitching. We perform the experiments using three popular NIS deep models, that are Mask-RCNN-R50 (with ResNet50 backbone), Mark-RCNN-R101 (with ResNet101 backbone) and CellPose, with two stitching methods, i.e., our hierarchical stitching framework and IoU-based matching scheme. Data and Computing Environment. "
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,3.2,Evaluation Metrics,"We use the common metrics precision, recall, and F1 score to evaluate the 3D NIS between annotated and predicted nuclei instances. Since the major challenge of NIS stitching is due to the large inter-slice gap, we also define the stitching accuracy for each 3D nuclei instance by counting the number of 2D NIS (in the X-Y plane) that both manual annotation and stitched nuclei share the same instance index."
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,3.3,Evaluating the Accuracy of NIS Stitching Results,Quantitative Evaluation. As shown in Fig.  Visual Inspection. We also show the visual improvement of whole-brain NIS results before and after stitching in Fig. 
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,3.4,Whole-Brain NIS in Neuroscience Applications,"One of the important steps in neuroscience studies is the measurement of regional variations in terms of nuclei counts. In light of this, we evaluate the counting accuracy. Since we only have 16 image stacks with manual annotations, we sim-ulate the stack-to-stack gap in the middle of each image stack and compare counting results between whole-brain NIS with or without hierarchical stitching. Compared to whole-brain NIS without stitching, our hierarchical stitching method has reduced the nuclei counting error from 48.8% down to 10.1%. The whole-brain NIS improvement has vividly appeared in Fig. "
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,4,Conclusion,"In this work, we introduce a learning-based stitching approach to achieve 3D instance segmentation of nuclei in whole-brain microscopy images. Our stitching framework is flexible enough to incorporate existing NIS methods, which are typically trained on small image stacks and may not be able to scale up to the whole-brain level. Our method shows great improvement by addressing interand intra-slice gap issues. The promising results in simulated whole-brain NIS, particularly in terms of counting accuracy, also demonstrate the potential of our approach for neuroscience research."
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,,Fig. 1 .,
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,,Fig. 2 .,
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,,Fig. 3 .,
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,,Fig. 4 .Fig. 5 .,
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,,,
A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_5.
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,1,Introduction,"Fluorodeoxyglucose Positron Emission Tomography (PET) is widely recognized as an essential tool in oncology  In the last decade, CNNs have demonstrated remarkable achievements in medical image segmentation tasks. This is primarily due to their ability to learn informative hierarchical features directly from data. However, as illustrated in  Beyond tumour delineation, another important use of functional images, such as PET images is their use for designing IMRT dose painting (DP). In particular, dose painting uses functional images to paint optimised dose prescriptions based on the spatially varying radiation sensitivities of tumours, thus enhancing the efficacy of tumour control  To address both tumour delineation and corresponding dose painting challenges, we propose to combine the expressiveness of deep CNNs with the versa-tility of KsPC in a unified framework, which we call KsPC-Net. In the proposed KsPC-Net, a CNN is employed to learn directly from the data to produce the pixel-wise bandwidth feature map and initial segmentation map, which are used to define the tuning parameters in the KsPC module. Our framework is completely automatic and differentiable. More specifically, we use the classic UNet "
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,2,Methods,
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,2.1,Kernel Smoothing Based Probability Contour,"Kernel-based method and follow up approach of modal clustering  where K is a kernel function and H is a symmetric, positive definite, d×d matrix of smoothing tuning parameters, called bandwidth which controls the orientation and amount of smoothing via the scaled kernel On the other hand, since x t is counted y i times at the same position, Eq. 1 can be further simplified as A scaled kernel is positioned so that its mode coincides with each data point x i which is expressed mathematically as K H (x-x i ). In this paper, we have used a Guassian kernel which is denoted as: which is a normal distribution with mean x i and variance-covariance matrix H. Therefore, we can interpret f in Eq. (  Result. The estimated probability contour level f ω can be computed as the ω-th quantile of fω of f (x 1 ; H), ..., f (x n ; H) (Proof in supplementary materials). The primary advantage of utilizing probability contours is their ability to assign a clear probabilistic interpretation on the defined contours, which are scale-invariant "
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,2.2,The KsPC-Net Architecture,"In the KsPC module, the model performance heavily depends on the bandwidth matrix H and it is often assumed that each kernel shares the same scalar bandwidth parameter. However, one may want to use different amounts of smoothing in the kernel at different grid positions. The commonly used approach for bandwidth selection is cross-validation "
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,2.3,Loss Function,"The Dice similarity coefficient is widely employed to evaluate segmentation models. We utilize the Dice loss function to optimize the model performance during training, which is defined as: , where y i is the label from experts and ŷi is the predicted label of i-th pixel. N is the total number of pixels and is a small constant in case of zero division. As shown in Fig.  where L f inal denotes the weighed dice loss while L CNN and L KsP C denotes the CNN loss and KsPC loss, respectively. In addition, α is a balancing parameter and is set to be 0.01 in this work."
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,3,Experiments,
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,3.1,Dataset,The dataset is from the HECKTOR challenge in MICCAI 2021 (HEad and neCK TumOR segmentation challenge). The HECKTOR training dataset consists of 224 patients diagnosed with oropharyngeal cancer 
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,3.2,Implementation Details,We used Python and a trained network on a NVIDIA Dual Quadro RTX machine with 64 GB RAM using the PyTorch package. We applied a batch size of 12 and the Adam algorithm 
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,4,Results,
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,4.1,Results on HECKTOR 2021 Dataset,"To evaluate the performance of our KsPC-Net, we compared it with the results of 5-fold cross-validation against three widely-used models namely, the standard 2D Unet, the 2D residual Unet and the 3D Unet. Additionally, we compare our performance against newly developed approaches MSA-Net "
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,4.2,Probability Contours,"One of the byproducts of using the kernel-smoothed densities to model the SUVs is the associated probability contours, which can be readily used to develop a comprehensive inferential framework and uncertainty quantification. For example, Fig. "
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,5,Conclusion,"In this paper, we present a novel network, KsPC-Net, for the segmentation in 2D PET images, which integrates KsPC into the UNet architecture in an end-toend differential manner. The KsPC-Net utilizes the benefits of KsPC to deliver both contour-based and grid-based segmentation outcomes, leading to improved precision in the segmentation of contours. Promising performance was achieved by our proposed KsPC-Net compared to the state-of-the-art approaches on the MICCAI 2021 challenge dataset (HECKTOR). It is worth mentioning that the architecture of our KsPC-Net is not limited to Head & Neck cancer type and can be broadcast to different cancer types. Additionally, a byproduct application of our KsPC-Net is to construct probability contours, which enables probabilistic interpretation of contours. The subregions created by probability contours allow for a strategy planning for the assigned dose boosts, which is a necessity for the treatment planning of radiation therapy for cancers."
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,,Fig. 1 .,
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,,Fig. 2 .,
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,,Fig. 3 .,
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,,Table 1 .,
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 51.
NISF: Neural Implicit Segmentation Functions,1,Introduction,"Image segmentation is a core task in domains where the area, volume or surface of an object is of interest. The principle of segmentation involves assigning a class to every presented point in the input space. Typically, the input is presented in the form of images: aligned pixel (or voxel) grids, with the intention to obtain a class label for each. In this context, the application of deep learning to the medical imaging domain has shown great promise in recent years. With the advent of the U-Net  Despite their efficacy, CNNs suffer from a range of limitations that lead to incompatibilities for some imaging domains. CNNs are restricted to data in the form of grids, and cannot easily handle sparse or partial inputs. Moreover, due to the CNN's segmentation output also being confined to a grid, obtaining smooth object surfaces requires post-processing heuristics. Predicting a high resolution segmentations also has implications on the memory and compute requirements in high-dimensional domains. Finally, the learning of long-distance spatial correlations requires deep stacks of layers, which may pose too taxing in low resource domains. We introduce a novel approach to image segmentation that circumvents these shortcomings: Neural Implicit Segmentation Functions (NISF). Inspired by ongoing research in the field of neural implicit functions (NIF), a neural network is taught to learn a mapping from a coordinate space to any arbitrary real-valued space, such as segmentation, distance function, or image intensity. While CNNs employ the image's pixel or voxel intensities as an input, NISF's input is a real-valued vector c ∈ R N for a single N-dimensional coordinate, alongside a subject-specific latent representation vector h ∈ R d . Given c and h, the network is taught to predict image intensity and segmentation value pairs. The space H over all possible latent vectors h serves as a learnable prior over all possible subject representations. In this paper, we describe an auto-decoder process by which a previously unseen subject's pairs of coordinate-image intensity values (c, i) may be used to approximate that subject's latent representation h. Given a latent code, the intensity and segmentation predictions from any arbitrary coordinates in the volume may be sampled. We evaluate the proposed framework's segmentation scores and investigate its generalization properties on the UK-Biobank cardiac magnetic resonance imaging (MRI) short-axis dataset. We make the source code publicly available"
NISF: Neural Implicit Segmentation Functions,2,Related Work,"Cardiac MRI. Cardiac magnetic resonance imaging (MRI) is often the preferred imaging modality for the assessment of function and structure of the cardiovascular system. This is in equal parts due to its non-invasive nature, and due to its high spatial and temporal resolution capabilities. The short-axis (SAX) view is a (3+t)-dimensional volume made up of stacked cross-sectional (2D+t) acquisitions which lay orthogonal to the ventricle's long axis (see Fig.  Image Segmentation. The capabilities of the CNN has caused it to become the predominant choice for image segmentation tasks  Additionally, segmentation performed by fully convolutional model is restricted to predicting in pixel (or voxel) grids. This requires post-processing heuristics to extract smooth object surfaces. Works such as  Neural Implicit Functions. In recent years, NIFs have achieved notable milestones in the field of shape representations  Image Priors. The typical application of a NIF involves the training of a multilayer perceptron (MLP) on a single scene. Although generalization still occurs in generating novel views of the target scene, the introduction of prior knowledge and conditioning of the MLP is subject to ongoing research "
NISF: Neural Implicit Segmentation Functions,3,Methods,"Shared Prior. In order to generalize to unseen subjects, we attempt to build a shared prior H over all subjects. This is done by conditioning the classifier with a latent vector h ∈ R d at the input level. Each individual subject j in a population X, can be thought of having a distinct h j that serves as a latent code of their unique features. Following  Model Architecture. The architecture is composed of a segmentation function f θ and a reconstruction function f φ . At each continuous-valued coordinate c ∈ R N , function f θ models the shape's segmentation probability s c for all M classes, and function f φ models the image intensity i c . The functions are conditioned by a latent vector h at the input level as follows: In order to improve local agreement between the segmentation and reconstruction functions, we jointly model f θ and f φ by a unique multi-layer perceptron (MLP) with two output heads (Fig.  Prior Training. Following the setup described in  The difference in image reconstruction from the ground-truth voxel intensities is supervised using binary cross-entropy (BCE). This is motivated by our data's voxel intensity distribution being heavily skewed towards the extremes. The segmentation loss is a sum of a BCE loss component and a Dice loss component. We found that adding a weighting factor of α = 10 to the image reconstruction loss component yielded inference-time improvements on both image reconstruction and segmentation metrics. Additionally, L2 regularization is applied to the latent vector h j and the MLP's parameters. The full loss is summarized as follows: Inference. Once the segmentation function f θ has learnt a mapping from the population prior H to the segmentation space S, inference becomes a task of finding a latent code h within H that correctly models the new subject's features. The ground-truth segmentation of a new subject is obviously not available at inference, and it is thus not possible to use f θ to optimize h. However, since both functions f φ (image reconstruction) and f θ (segmentation) have been jointly trained by consistently using the same latent vector h, we make the following assumption: A latent code h optimized for image reconstruction under f φ will also produce accurate segmentations under f θ . This assumption makes it possible to use the image reconstruction function f φ alone to find a latent code h for an unseen image in order to generalize segmentation predictions using f θ . For this task, a new h ∼ N 0, 10 -4 is initialized. The weights of the MLP are frozen, such that the only tuneable parameters are those of h. Optimization is performed exclusively on the image reconstruction loss (dashed green line in Fig.  Due to the loss being composed exclusively by the image reconstruction term, h is expected to eventually overfit to f φ . Special care should be taken to find a step-number hyperparameter that stops the optimization of h at the optimal segmentation performance. In our experiments, we chose this parameter based on the Dice score of the best validation run. "
NISF: Neural Implicit Segmentation Functions,4,Experiments and Results,"Data Overview. The dataset consists of a random subset of 1150 subjects from the UK Biobank's short-axis cardiac MRI acquisitions  Implementation Details. The architecture consists of 8 residual layers, each with 128 hidden units. The subject latent codes had 128 learnable parameters. The model was implemented using Pytorch and trained on an NVIDIA A40 GPU for 1000 epochs, lasting approximately 9 days. Inference optimization lasted 3-7 minutes per subject depending on volume dimensions. Losses are minimized using the ADAM optimizer "
NISF: Neural Implicit Segmentation Functions,,Results.,"As the latent code is optimized during inference, segmentation metrics follow an overfitting pattern (see Fig.  The benefits of training a prior over the population is investigated by tracking inference-time Dice scores obtained from spaced-out validation runs. Training of the prior is shown to significantly improve performance of segmentation and image reconstruction at inference-time as seen in Fig.  Validation results showed the average optimal number of latent code optmimization steps at inference to be 672. Thus, the test set per-class Dice scores (Table  Right ventricle segmentation in basal slices is notoriously challenging to manually annotate due to the delineation of the atrial and ventricular cavity combined with the sparsity of the resolution along the long axis  We go on to show NISF's ability to generate high-resolution segmentation for out-of-plane views. We optimize on a short-axis volume at inference and subsequently sample coordinates corresponding to long-axis views. Despite never presenting a ground-truth long-axis image, the model reconstructs an interpolated view and provides an accurate segmentation along its plane (Fig. "
NISF: Neural Implicit Segmentation Functions,5,Conclusion,"We present a novel family of image segmentation models that can model shapes at arbitrary resolutions. The approach is able to leverage priors to make predictions for regions not present in the original image data. Working directly on the coordinate space has the benefit of accepting high-dimensional sparse data, as well as not being affected by variations in image shapes and resolutions. We implement a simple version of this framework and evaluate it on a short-axis cardiac MRI segmentation task using the UK Biobank. Reported Dice scores on 100 unseen subjects average 0.87 ± 0.045. We also perform a qualitative analysis on the framework's ability to predict held-out sections of image volumes."
NISF: Neural Implicit Segmentation Functions,,Fig. 1 .,
NISF: Neural Implicit Segmentation Functions,,Fig. 2 .,
NISF: Neural Implicit Segmentation Functions,,Fig. 3 .,
NISF: Neural Implicit Segmentation Functions,,Fig. 4 .,
NISF: Neural Implicit Segmentation Functions,,Fig. 5 .,
NISF: Neural Implicit Segmentation Functions,,Fig. 6 .,
NISF: Neural Implicit Segmentation Functions,,Table 1 .,
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,1,Introduction,"Extracting brain tumors from medical image scans plays an important role in further analysis and clinical diagnosis. Typically, a brain tumor includes peritumoral edema, enhancing tumor, and non-enhancing tumor core. Since different modalities present different clarity of brain tumor components, we often use multi-modal image scans, such as T1, T1c, T2, and Flair, in the task of brain tumor segmentation  Current image segmentation methods for handling missing modalities can be divided into three categories, including: 1) brute-force methods: designing individual segmentation networks for each possible modality combination  To handle various numbers of modal inputs, HeMIS  Due to the complexity of current models, we tend to develop a simple model, which adopts a simple average fusion and attention mechanism. These two techniques are demonstrated to be effective in handling missing modalities and multimodal fusion  -We propose a simple multi-modal fusion network, A2FSeg, for brain tumor segmentation, which is general and can be extended to any number of modalities for incomplete image segmentation. -We conduct experiments on the BraTS 2020 dataset and achieve the SOTA segmentation performance, having a mean Dice core of 89.79% for the whole tumor, 82.72% for the tumor core, and 66.71% for the enhancing tumor."
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,2,Method,"Figure  Modality-Specific Feature Extraction (MSFE) Module. Before fusion, we first extract features for every single modality, using the nnUNet model  Here, the number of channels is C = 32; H f , W f , and D f are the height, width, and depth of feature maps F m , which share the same size as the input image. For every single modality, each MSFE module is supervised by the image segmentation mask to fasten its convergence and provide a good feature extraction for fusion later. All four MSFEs have the same architecture but with different weights. Average Fusion Module. To aggregate image features from different modalities and handle the possibility of missing one or more modalities, we use the average of the available features from different modalities as the first fusion result. That is, we obtain a fused average feature Here, N m is the number of available modalities. For example, as shown in Fig.  Adaptive Fusion Module. Since each modality contributes differently to the final tumor segmentation, similar to MAML  Here, F m is a convolutional layer for this specific modality m, and θ m represents the parameters of this layer, and σ is a Sigmoid function. That is, we have an individual convolution layer F m for each modality to generate different weights. Due to the possibility of missing modalities, we will have different numbers of feature maps for fusion. To address this issue, we normalize the different attention weights by using a Softmax function: That is, we only consider feature maps from those available modalities but normalize their contribution to the final fusion result, so that, the fused one has a consistent value range, no matter how many modalities are missing. Then, we perform voxel-wise multiplication of the attention weight with the corresponding modal feature maps. As a result, the adaptively fused feature maps F is calculated by the weighted sum of each modal feature: Here, ⊗ indicates the voxel-wise multiplication. Loss Function. We have multiple segmentation heads, which are distributed in each module of A2FSeg. For each segmentation head, we use the combination of the cross-entropy and the soft dice score as the basic loss function, which is defined as where ŷ and y represent the segmentation prediction and the ground truth, respectively. Based on this basic one, we have the overall loss function defined as where the first term is the basic segmentation loss for each modality m after feature extraction; the second term is the loss for the segmentation output of the average fusion module; and the last term is the segmentation loss for the final output from the adaptive fusion module. 3 Experiments"
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,3.1,Dataset,"Our experiments are conducted on BraTS2020, which contains 369 multicontrast MRI scans with four modalities: T1, T1c, T2, and Flair. These images went through a sequence of preprocessing steps, including co-registration to the same anatomical template, resampling to the same resolution (1 mm 3 ), and skullstripping. The segmentation masks have three labels, including the whole tumor (abbreviated as Complete), tumor core (abbreviated as Core), and enhancing tumor (abbreviated as Enhancing). These annotations are manually provided by one to four radiologists according to the same annotation protocol."
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,3.2,Experimental Settings and Implementation Details,We implement our model with PyTorch 
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,3.3,Experimental Results and Comparison to Baseline Methods,"To evaluate the performance of our model, we compare it with four recent models, HeMIS  Figure "
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,3.4,Ablation Study,"In this part, we investigate the effectiveness of the average fusion module and the adaptive fusion module, which are two important components of our method. Firstly, we set a baseline model without any modal interaction, that is, with the average fusion module only. Then, we add the adaptive fusion module to the baseline model. Table "
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,4,Discussion and Conclusion,"In this paper, we propose an average and adaptive fusion segmentation network (A2FSeg) for the incomplete multi-model brain tumor segmentation task. The essential components of our A2FSeg network are the two stages of feature fusion, including an average fusion and an adaptive fusion. Compare to existing complicated models, our model is much simpler and more effective, which is demonstrated by the best performance on the BraTS 2020 brain tumor segmentation task. The experimental results demonstrate the effectiveness of two techniques, i.e., the average fusion and the attention-based adaptive one, for incomplete modal segmentation tasks. Our study brings up the question of whether having complicated models is necessary. If there is no huge gap between different modalities, like in our case where all four modalities are images, the image feature maps are similar and a simple fusion like ours can work. Otherwise, we perhaps need an adaptor or an alignment strategy to fuse different types of features, such as images and audio. Also, we observe that a good feature extractor is essential for improving the segmentation results. In this paper, we only explore a reduced UNet for feature extraction. In future work, we will explore other feature extractors, such as Vision Transformer (ViT) or other pre-trained visual foundation models "
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,,Fig. 1 .,
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,,Fig. 2 .,
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,,Fig. 3 .,
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,,Table 1 .,
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,,Table 2 .,
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,1,Introduction,"Diffuse glioma is a common malignant tumor with highly variable prognosis across individuals. To improve survival outcomes, many pre-operative survival prediction methods have been proposed with success. Based on the prediction results, personalized treatment can be achieved. For instance, Isensee et al.  Despite the promising results of existing pre-operative survival prediction methods, they often overlook clinical knowledge that could aid in improving the prediction accuracy. Notably, tumor types have been found to be strongly correlated with the prognosis of diffuse glioma  Our method is evaluated using pre-operative multimodal MR brain images of 1726 diffuse glioma patients collected from cooperation hospitals and a public dataset BraTS2019 "
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2,Methods,"Diffuse glioma can be classified into three histological types: the oligodendroglioma, the astrocytoma, and the glioblastoma  The tumor subtyping network is trained independently before being integrated into the backbone. To solve the inherent issue of imbalanced tumor type in the training data collected in clinic, a novel ordinal manifold mixup based feature augmentation is applied in the training of the tumor subtyping network. It is worth noting that the ground truth of tumor types, which is determined after craniotomy, is available in the training data, while for the testing data, tumor types are not required, because tumor-type-related features can be learned from the pre-operative multimodal MR brain images."
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.1,The Survival Prediction Backbone,"The architecture of the survival prediction backbone, depicted in Fig.  from the tumor subtyping network (discussed later), and M is the vector dimension which is set to 128. As f type i has strong correlation with prognosis, the performance of the backbone can be improved. In addition, information of patient age and tumor position is also used. To encode the tumor position, the brain is divided into 3 × 3 × 3 blocks, and the tumor position is represented by 27 binary values (0 or 1) with each value for one block. If a block contains tumors, then the corresponding binary value is 1, otherwise is 0. The backbone is based on the deep Cox proportional hazard model, and the loss function is defined as: where h θ (x i ) represents the risk of the i-th patient predicted by the backbone, θ stands for the parameters of the backbone, x i is the input multimodal MR brain images of the i-th patient, R(t i ) is the risk group at time t i , which contains all patients who are still alive before time t i , t i is the observed time (time of death happened) of x i , and δ i = 0/1 for censored/non-censored patient."
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.2,The Tumor Subtyping Network,"The tumor subtyping network has almost the same structure as the backbone. It is responsible for learning tumor-type-related features from each input preoperative multimodal MR brain image x i and classifying the tumor into oligodendroglioma, astrocytoma, or glioblastoma. The cross entropy is adopted as the loss function of the tumor subtyping network, which is defined as: where y k i and p k i are the ground truth (0 or 1) and the prediction (probability) of the k-th tumor type (k = 1, 2, 3) of the i-th patient, respectively. The learned tumor-type-related feature f type i ∈ R M is fed to the survival prediction backbone and concatenated with f cox i learned in the backbone to predict the risk. In the in-house dataset, the proportions of the three tumor types are 20.9% (oligodendroglioma), 28.7% (astrocytoma), and 50.4% (glioblastoma), which is consistent with the statistical report in "
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.3,The Ordinal Manifold Mixup,"In the original manifold mixup  where y k i and y k j stand for the labels of the k-th tumor type of the i-th and j-th patients, respectively, and λ ∈ [0, 1] is a weighting factor. For binary classification, the original manifold mixup can effectively enhance the network performance, however, for the classification of more than two classes, e.g., tumor types, there exists a big issue. As shown in Fig. "
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,,right).,"Normally, the feature distribution of each tumor type is assumed to be independent normal distribution, so their joint distribution is given by: where F k , k = 1, 2, 3 represents the feature set of oligodendroglioma, astrocytoma, and glioblastoma, respectively, μ k and σ 2 k are mean and variance of F k . To impose the ordinal constraint, we define the desired feature distribution of each tumor type as N (μ 1 , σ2 1 ) for k = 1, and N (μ k-1 + Δ k , σ2 k ) for k = 2 and 3. In this way, the feature distribution of each tumor type depends on its predecessor, and the mean feature of each tumor type μk (except μ1 ) is equal to the mean feature of its predecessor μk-1 shifted by Δ k . Note that Δ k is set to be larger than 3 × σk to ensure the desired ordering  which can be represented as: where μ1 and σk , k = 1, 2, 3 can be learned by the tumor subtyping network. Finally, the ordinal loss, which is in the form of KL divergence, is defined as: In our method, μ k and σ 2 k are calculated by where Φ θ and G are the encoder and GAP of the tumor subtyping network, respectively, θ is the parameter set of the encoder, stands for the subset containing the pre-operative multimodal MR brain images of the patients with the k-th tumor type, N k is the patient number in D k . So we impose the ordinal loss L KL to the features after the GAP of the tumor subtyping network as shown in Fig.  The tumor subtyping network is first trained before being integrated into the survival prediction backbone. In the training stage of the tumor subtyping network, each input batch contains pre-operative multimodal MR brain images of N patients and can be divided into K = 3 subsets according to their corresponding tumor types, i.e., D k , k = 1, 2, 3. With the ordinal constrained feature distribution, high consistent features can be augmented between neighboring tumor types. Based on the original and augmented features, the performance of the tumor subtyping network can be enhanced. Once the tumor subtyping network has been trained, it is then integrated into the survival prediction backbone, which is trained under the constraint of the cox proportional hazard loss L Cox ."
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,3,Results,"In our experiment, both in-house and public datasets are used to evaluate our method. Specifically, the in-house dataset collected in cooperation hospitals contains pre-operative multimodal MR images, including T1, T1 contrast enhanced (T1c), T2, and FLAIR, of 1726 patients (age 49.7 ± 13.1) with confirmed diffuse glioma types. The patient number of each tumor type is 361 (oligodendroglioma), 495 (astrocytoma), and 870 (glioblastoma), respectively. In the 1726 patients, 743 have the corresponding overall survival time (dead, non-censored), and 983 patients have the last visiting time (alive, censored). Besides the inhouse dataset, a public dataset BraTS2019, including pre-operative multimodal MR images of 210 non-censored patients (age 61.4 ± 12.2), is adopted as the external independent testing dataset. All images of the in-house and BraTS2019 datasets go through the same pre-processing stage, including image normalization and affine transformation to MNI152  Besides our method, four state-of-the-art methods, including random forest based method (RF)  where D = {x 1 , ..., x N } is the dataset containing all patients, T i and T j are ground truth of survival times of the i-th and j-th patients, R i and R j are the days predicted by RF, MCSP, and PGSP or risks predicted by the deep Cox proportional hazard models (i.e., deepConvSurv and our method), 1 x<y = 1 if x < y, else 0, and δ i = 0 or 1 when the i-th patient is censored or non-censored. As RF, MCSP, and PGSP cannot use the censored data in the in-house dataset, 80% of the non-censored data (594 patients) are randomly selected as the training data, and the rest 20% non-censored data (149 patients) are for testing. While deepConvSurv and our method are deep Cox models, both censored and non-censored patients can be utilized. So besides the 80% non-censored patients, all censored data (983 patients) are also included in the training data. Table "
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,3.1,Ablation Study of Survival Prediction,"To show the effect of the tumor subtyping network and the ordinal manifold mixup in survival prediction, our method without the tumor subtyping network (Baseline-1) and our method with the tumor subtyping network (using original manifold mixup instead, Baseline-2) are evaluated. For the in-house dataset, the resulting C-indices are 0.744 (Baseline-1) and 0.735 (Baseline-2). So our method make the improvement of C-index more than 8% comparing with Baseline-2. For the external independent testing dataset BraTS2019, the resulting C-indices are 0.738 (Baseline-1) and 0.714 (Baseline-2), and our method still has more than 6% improvement comparing with Baseline-2. Figure "
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,4,Conclusions,"We proposed a new method for pre-operative survival prediction of diffuse glioma patients, where a tumor subtyping network is integrated into the prediction backbone. Based on the tumor subtyping network, tumor type information, which are only available after craniotomy, can be derived from the pre-operative multimodal MR images to boost the survival prediction performance. Moreover, a novel ordinal manifold mixup was presented, where ordinal constraint is imposed to make feature distribution of different tumor types in the order of risk grade, and feature augmentation only takes place between neighboring tumor types. In this way, inconsistency between the augmented features and corresponding labels can be effectively reduced. Both in-house and public datasets containing 1936 patients were used in the experiment. Our method outperformed the state-of-the-art methods in terms of the concordance-index."
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,,Fig. 1 .,
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,,Fig. 2 .,
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,,Fig. 3 .,
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,,Table 1 .,
Medical Boundary Diffusion Model for Skin Lesion Segmentation,1,Introduction,Segmentation of skin lesions from dermoscopy images is a critical task in disease diagnosis and treatment planning of skin cancers 
Medical Boundary Diffusion Model for Skin Lesion Segmentation,,Boundary Evolution Image,"Small Large Fig.  Two representative boundaries are visualized in Fig.  However, current models for skin lesion segmentation are still struggling with extremely challenging cases, which are often encountered in clinical practice. While some approaches aim to optimize the model architecture by incorporating local and global contexts and multi-task supervision, and others seek to improve performance by collecting more labeled data and building larger models, both strategies are costly and can be limited by the inherent complexity of skin lesion boundaries. Therefore, we propose a novel approach that shifts the focus from merely segmenting lesion boundaries to predicting their evolution. Our approach is inspired by recent advances in image synthesis achieved by diffusion probabilistic models  In this paper, we propose a Medical Boundary Diff usion model (MB-Diff ) to improve the skin lesion segmentation, particularly in cases where the lesion boundaries are ambiguous and have extremely large or small sizes. The MB-Diff model follows the basic design of the plain diffusion model, using a sequential denoising process to generate the lesion mask. However, it also includes two key innovations: Firstly, we have developed an efficient multi-scale image guidance module, which uses a pretrained transformer encoder to extract multi-scale features from prior images. These features are then fused with the evolution features to constrain the direction of evolution. Secondly, we have implemented an evolution uncertainty-based fusion strategy, which takes into account the uncertainty of different initializations to refine the evolution results and obtain more precise lesion boundaries. We evaluate our model on two popular skin lesion segmentation datasets, ISIC-2016 and PH 2 datasets, and find that it performs significantly better than existing models."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,2,Method,"The key objective of MB-Diff is to improve the representation of ambiguous boundaries by learning boundary evolution through a cascaded series of steps, rather than a single step. In this section, we present the details of our cascaded boundary evolution learning process and the parameterized architecture of the evolution process. We also introduce our evolution-based uncertainty estimation and boundary ensemble techniques, which have significant potential for enhancing the precision and reliability of the evolved boundaries."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,2.1,Boundary Evolution Process,"We adopt a step-by-step denoising process to model boundary evolution in MB-Diff, drawing inspiration from recent diffusion probabilistic models (DPMs). Specifically, given the image and boundary mask distributions as (X , Y), assuming that the evolution consists of T steps in total, the boundary at T -th step (y T ) is the randomly initialized noise and the boundary at 0-th (y 0 ) step denotes the accurate result. We formulate the boundary evolution process as follows: where p(y T ) = N (y T ; 0, I) is the initialized Gaussian distribution and p θ (y t-1 |y t ) is each learnable evolution step, formulated as the Gaussian transition, denoted as: Note that the prediction function takes the input image as a condition, enabling the evolving boundary to fit the corresponding lesion accurately. By modeling boundary evolution as a step-by-step denoising process, MB-Diff can effectively capture the complex structures of skin lesions with ambiguous boundaries, leading to superior performance in lesion segmentation. To optimize the model parameters θ, we use the evolution target as an approximation of the posterior at each evolution step. Given the segmentation label y as y 0 , the label is gradually added by a Gaussian noise as: where {β t } T t=1 is a set of constants ranging from 0 to 1. After that, we compute the posterior q(y t-1 |y t , y 0 ) using Bayes' rule. The MSE loss function is utilized to measure the distance between the predicted mean and covariance of the Gaussian transition distribution and the evolution target q(y t-1 |y t , y 0 )."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,2.2,Paramterized Architecture with Image Prior,"The proposed model is a parameterized chain process that predicts the μ * t-1 and * t-1 at each evolution step t under the prior conditions of the image x and the prior evolution y * t . To capture the deep semantics of these conditions and perform efficient fusion, we adopt a basic U-Net  The architecture consists of a multi-level convolutional encoder and a symmetric decoder with short connection layers between them. To incorporate the variable t into the model, we first embed it into the latent space. Then, the prior evolution y * t is added to the latent t before each convolution. At the bottleneck layer, we fuse the evolution features with the image guidance to constrain the evolution and ensure that the final boundary suits the conditional image. To achieve this, priors train a segmentation model concurrently with the evolution model and use an attention-based parser to translate the image features in the segmentation branch into the evolution branch "
Medical Boundary Diffusion Model for Skin Lesion Segmentation,2.3,Evolution Uncertainty,"Similar to typical evolutionary algorithms, the final results of boundary evolution are heavily influenced by the initialized population. As a stochastic chain process, the boundary evolution process may result in different endpoints due to the random Gaussian samples at each evolution step. This difference is particularly evident when dealing with larger ambiguity in boundary regions. The reason is that the image features in such ambiguous regions may not provide discriminative guidance for the evolution, resulting in significant variations in different evolution times. Instead of reducing the differences, we surprisingly find that these differences can represent segmentation uncertainty. Based on the evolution-based uncertainty estimation, the segmentation results become more accurate and trustworthy in practice  Uncertainty Estimation: To estimate uncertainty, the model parameters θ are fixed, and the evolution starts with a randomly sampled Gaussian noise y * T ∼ N (0, I). Let {y * ,i T } n i=1 denote a total of n initializations. Once the evolution is complete, the obtained {μ * ,i } n i=1 , { * ,i } n i=1 are used to the sample final lesion maps as: y * ,i = μ * ,i + exp( 1 2 * ,i )N (0, I). Unlike traditional segmentation models that typically scale the prediction into the range of 0 to 1, the evolved maps generated by MB-Diff have unfixed distributions due to random sampling. Since the final result is primarily determined by the mean value μ, and the predicted has a limited range  Evolution Ensemble: Instead of training multiple networks or parameters to make the ensemble, MB-Diff allows running the inference multiple times and fusing the obtained evolutions. However, simply averaging the predicted identities from multiple evolutions is not effective, as the used MSE loss without activation constrains the predicted identities to be around 0 or 1, unlike the Sigmoid function which would limit the identities to a range between 0 and 1. Therefore, we employ the max vote algorithm to obtain the final segmentation map. In this algorithm, each pixel is classified as a lesion only if its identity sum across all n evolutions is greater than a threshold value τ . Finally, the segmentation map is generated as 3 Experiment"
Medical Boundary Diffusion Model for Skin Lesion Segmentation,3.1,Datasets and Evaluation Metrics,"Datasets: We use two publicly available skin lesion segmentation datasets from different institutions in our experiments: the ISIC-2016 dataset and the PH 2 dataset. The ISIC-2016 dataset  Evaluation Metrics: To comprehensively compare the segmentation results, particularly the boundary delineations, we employ four commonly used metrics to quantitatively evaluate the performance of our segmentation methods. These metrics include the Dice score, the IoU score, Average Symmetric Surface Distance (ASSD), and Hausdorff Distance of boundaries (95-th percentile; HD95). To ensure fair comparison, all labels and predictions are resized to (512×512) before computing these scores, following the approach of a previous study "
Medical Boundary Diffusion Model for Skin Lesion Segmentation,3.2,Implementation Details,"For the diffusion model hyper-parameters, we use the default settings of the plain diffusion model, which can be found in the supplementary materials. Regarding dataset. We highlight the small lesions using dotted boxes in the third row. the training parameters, we resize all images to (256 × 256) for efficient memory utilization and computation. We use a set of random augmentations, including vertical flipping, horizontal flipping, and random scale change (limited to 0.9 ∼ 1.1), to augment the training data. We set the batch size to 4 and train our model for a total of 200,000 iterations. During training, we use the AdamW optimizer with an initial learning rate of 1e-4. For the inference, we set n = 4 and τ = 2 considering the speeds."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,3.3,Comparison with State-of-the-Arts,"We majorly compare our method to the latest skin lesion segmentation models, including the CNN-based and transformer-based models, i.e., U-Net++  Furthermore, we provide a visualization of evolution uncertainties in Fig. "
Medical Boundary Diffusion Model for Skin Lesion Segmentation,3.4,Detailed Analysis of the Evolution,"In this subsection, we make a comprehensive analysis to investigate the performance of each component in our method and compare it to the diffusion-based model, MedSegDiff. The results of our ablation study are presented in Fig. "
Medical Boundary Diffusion Model for Skin Lesion Segmentation,4,Conclusion,"In this paper, we introduced the medical boundary diffusion (MB-Diff) model, which is a novel approach to segment skin lesions. Our proposed method formulates lesion segmentation as a boundary evolution process with finite timesteps, which allows for efficient and accurate segmentation of skin lesions. To guide the boundary evolution towards the lesions, we introduce an efficient multi-scale image guidance module. Additionally, we propose an evolution uncertainty-based fusion strategy to yield more accurate segmentation. Our method is evaluated on two well-known skin lesion segmentation datasets, and the results demonstrate superior performance and generalization ability in unseen domains. Through a detailed analysis of our training program, we find that our model has faster convergence and better performance compared to other diffusion-based models. Overall, our proposed MB-Diff model offers a promising solution to accurately segment skin lesions, and has the potential to be applied in a clinical setting."
Medical Boundary Diffusion Model for Skin Lesion Segmentation,,Fig. 2 .,
Medical Boundary Diffusion Model for Skin Lesion Segmentation,,Fig. 3 .,
Medical Boundary Diffusion Model for Skin Lesion Segmentation,,Table 1 .,
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,1,Introduction,"Accurate tumor segmentation from medical images is essential for quantitative assessment of cancer progression and preoperative treatment planning  Recently, multimodal tumor segmentation has attracted the interest of many researchers. With the emergence of multimodal datasets (e.g., BRATS  In addition to the progress on the fusion of multimodal features, improving the model representation ability is also an effective way to boost segmentation performance. In the past few years, Transformer structure  Although remarkable performance has been accomplished with these efforts, there still exist several challenges to be resolved. Most existing methods are either limited to specific modality numbers due to the design of asymmetric connections or suffer from large computational complexity because of the huge amount of model parameters. Therefore, how to improve model ability while ensuring computational efficiency is the main focus of this paper. To this end, we propose an efficient multimodal tumor segmentation solution named Hybrid Densely Connected Network (H-DenseFormer). First, our method leverages Transformer to enhance the global contextual information of different modalities. Second, H-DenseFormer integrates a Transformer-based Multi-path Parallel Embedding (MPE) module, which can extract and fuse multimodal image features as a complement to naive input-level fusion structure. Specifically, MPE assigns an independent encoding path to each modality, then merges the semantic features of all paths and feeds them to the encoder of the segmentation network. This decouples the feature representations of different modalities while relaxing the input constraint on the specific number of modalities. Finally, we design a lightweight, Densely Connected Transformer (DCT) module to replace the standard Transformer to ensure performance and computational efficiency. Extensive experimental results on two publicly available datasets demonstrate the effectiveness of our proposed method. as the auxiliary extractor of multimodal fusion features, while the latter is used to generate predictions. Specifically, given a multimodal image input X 3D ∈ R C×H×W ×D or X 2D ∈ R C×H×W with a spatial resolution of H × W , the depth dimension of D (number of slices) and C channels (number of modalities), we first utilize MPE to extract and fuse multimodal image features. Then, the obtained features are progressively upsampled and delivered to the encoder of the segmentation network to enhance the semantic representation. Finally, the segmentation network generates multi-scale outputs, which are used to calculate deep supervision loss as the optimization target."
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,2,Method,
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,2.1,Overall Architecture of H-DenseFormer,
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,2.2,Multi-path Parallel Embedding,"Many methods  , where i ∈ [1, 2, ..., C], p = 16 and l = 128 denote the path size and embedding feature length respectively. First, we concatenate the features of all modalities and entangle them using a convolution operation. Then, interpolation upsampling is performed to obtain the multimodal fusion feature where k = 128 refers to the channel dimension. Finally, F out is progressively upsampled to multiple scales and delivered to different encoder stages to enhance the learned representation."
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,2.3,Densely Connected Transformer,"Standard Transformer structures  where z 0 represents the original input, cat(•) and p(•) denote the concatenation operator and the linear layer, respectively. The norm(•), att(•), f (•) are the regular layer normalization, multi-head self-attention mechanism, and feedforward layer. The output of DCT is z out = f (cat([z 0 ; z 1 ; ...; z 4 ])). Table "
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,2.4,Segmentation Backbone Network,"The H-DenseFormer adopts a U-shaped encoder-decoder structure as its backbone. As shown in Fig.  , where i ∈ [0, 1, 2, 3], and c = 2 (tumor and background) represents the number of segmentation classes. To mitigate the pixel imbalance problem, we use a combined loss of Focal loss  where N refers to the total number of pixels, p t and q t denote the predicted probability and ground truth of the t-th pixel, respectively, and r = 2 is the modulation factor. Thus, DS loss can be calculated as follows: where G i represents the ground truth after resizing and has the same size as O i . α is a weighting factor to control the proportion of loss corresponding to the output at different scales. This approach can improve the convergence speed and performance of the network."
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,3,Experiments,
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,3.1,Dataset and Metrics,"To validate the effectiveness of our proposed method, we performed extensive experiments on HECKTOR21 "
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,3.2,Implementation Details,"We use Pytorch to implement our proposed method and the baselines. For a fair comparison, all models are trained from scratch using two NVIDIA A100 GPUs and all comparison methods are implemented with open-source codes, following their original configurations. In particular, we evaluate the 3D and 2D H-DenseFormer on HECKTOR21 and PI-CAI22, respectively. During the training phase, the Adam optimizer is employed to minimize the loss with an initial learning rate of 10 -3 and a weight decay of 10 -4 . We use the PolyLR strategy "
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,3.3,Overall Performance,
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,3.4,Parameter Sensitivity and Ablation Study,Impact of DCT Depth. As illustrated in Table 
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,4,Conclusion,"In this paper, we proposed an efficient hybrid model (H-DenseFormer) that combines Transformer and CNN for multimodal tumor segmentation. Concretely, a Multi-path Parallel Embedding module and a Densely Connected Transformer block were developed and integrated to balance accuracy and computational complexity. Extensive experimental results demonstrated the effectiveness and superiority of our proposed H-DenseFormer. In future work, we will extend our method to more tasks and explore more efficient multimodal feature fusion methods to further improve computational efficiency and segmentation performance."
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,,Figure 1 Fig. 1 .,
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,,Fig. 2 .,
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,,Table 1 .,
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,,Table 2 .,
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,,Table 3 .,
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,,Table 4 .,
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,1,Introduction,"Accurate cancer diagnosis, grading, and treatment decisions from medical images heavily rely on the analysis of underlying complex nuclei structures  In the literature work, the sole-decoder design in these UNet variants (Fig.  Additionally, existing methods are CNN-based, and their intrinsic convolution operation fails to capture global spatial information or the correlation amongst nuclei "
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,2,Methodology,"Network Architecture Overview. Figure  Attention Sharing Scheme. To capture the strong correlation between nuclei segmentation and contour segmentation between multiple decoders  [•] writes for the concatenation, SA(•) denotes the self-attention head whose output dimension is D h , and U u MSA ∈ R (m+n)•D h ×D is a learnable matrix. The superscript s and u refer to the globally-shared and unshared weights across all decoders, respectively. Token MLP Bottleneck. To reduce the complexity of the model, we leverage a token MLP bottleneck as a light-weight alternative for the Swin Transformer bottleneck. Specifically, this approach involves shifting the latent features extracted by the encoder via two MLP blocks across the width and height channels, respectively  Consistency Self Distillation. To alleviate the inconsistency between the contour generated from the nuclei segmentation prediction and the predicted edge, we propose a novel consistency self distillation loss, denoted as L SD . Formally, this regularization is defined as the dice loss between the contour generated from the nuclei branch prediction (y n ) using the Sobel operation (sobel(y n )) and the predicted edges y e from the normal edge decoder. Specifically, the self distillation loss L D is formulated by L sd = Dice(sobel(y n ), y e ). Multi-task Learning Objective. We employ a multi-task learning paradigm to train the tri-decoder network, aiming to improve model performance by leveraging the additional supervision signal from edges. Particularly, the nuclei semantic segmentation is considered the primary task, while the normal edge and clustered edge semantic segmentation are viewed as auxiliary tasks. All decoder branches follow a uniform scheme that combines the cross-entropy loss and the dice loss, with the balancing coefficients set to 0.60 and 0.40 respectively, as previous work  , where coefficients γ n , γ e and γ c are set to 0.30, 0.35, 0.35 respectively, and γ sd is initially set to 1 with a 0.3 decrease for every 10 epochs until it reaches 0.4."
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,3,Experiments,"Dataset. We evaluated the applicability of our approach across multiple modalities by conducting evaluations on microscopy and histology datasets.   The private dataset contains 300 images sized at 512 × 512 tessellated from 50 WSIs scanned at 20×, and meticulously labeled by five pathologists according to the labeling guidelines of the MoNuSeg  Implementations. All experiments are performed on one NVIDIA RTX 3090 GPU with 24 GB memory. We use Adam optimizer with an initial learning rate of 1 × 10 -4 . We compare TransNuSeg with UNet  Results. Table "
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,4,Conclusion,"In this paper, we make the first attempt at an efficient but effective multi-task Transformer framework for modality-agnostic nuclei segmentation. Specifically, our tri-decoder framework TransNuSeg leverages an innovative self distillation regularization to impose consistency between the different branches. Experimental results on two datasets demonstrate the excellence of our TransNuSeg against state-of-the-art counterparts for potential real-world clinical deployment. Additionally, our work opens a new architecture to perform nuclei segmentation tasks with Swin Transformer, where further investigations can be performed to explore the generalizability to the top of our methods with different modalities."
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,,Fig. 1 .,
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,,,
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,,Fig. 3 .,
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,,Fig. 4 .,
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,,Fig. 5 .,
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,,,
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,,Table 1 .,
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,,Table 2 .,
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,,Table 3 .,
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,1,Introduction,Cortical surface reconstruction aims to extract 3D meshes of inner (white matter) and outer (pial) surfaces of the cerebral cortex from brain magnetic resonance images (MRI). These surfaces provide both 3D visualization and estimation of morphological features for the cortex  required to be closed manifolds and topologically homeomorphic to a sphere  With the recent advances of geometric deep leaning  Cortical surface reconstruction plays an essential role in modeling and quantifying the brain development in fetal and neonatal neuroimaging studies such as the Developing Human Connectome Project (dHCP) 
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,,Contribution.,"In this work, we present Conditional Temporal Attention Network (CoTAN). CoTAN adopts attention mechanism "
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,2,Method,"Diffeomorphic Surface Deformation. We define the diffeomorphic surface deformation φ t : R 3 × R → R 3 as a flow ordinary differential equation (ODE) following previous work  where v t is a time-varying velocity field (TVF) and Id is the identity mapping. Given an initial surface S 0 ⊂ R 3 with points x 0 ∈ S 0 , we define x t := φ t (x 0 ) as the trajectories of the points on the deformable surface S t = φ t (S 0 ) for t ∈ [0, T ]. Then the flow equation (1) can be rewritten as d dt x t = v t (x t ) with initial value x 0 . By the existence and uniqueness theorem for ODE solutions  Conditional Temporal Attention Network (CoTAN). An overview of the CoTAN architecture is shown in Fig.  CoTAN adopts a channel-wise attention mechanism  The CTVF is adaptive to the integration time and the age of subjects, which can handle the large deformation and variation for neonatal cortical surfaces. Such an attention mechanism encourages CoTAN to learn coarse-to-fine surface deformation by attending to SVFs at different resolutions. To deform the initial surface S 0 to the target surface, we integrate the flow ODE (1) with the CTVF through the forward Euler method. For k = 0, ..., K -1, the surface points are updated by x k+1 = x k + hv k (x k ; a), where K is the total integration steps and h = T /K is the step size with T = 1. For each step k, we only need to recompute the attention maps p(hk, a) and update the CTVF v k (x k ; a) accordingly by Eq. (  For white matter surface reconstruction, we consider loss functions that have been widely used in previous work  For pial surface reconstruction, we follow "
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,3,Experiments,"Implementation Details. We evaluate CoTAN on the third release of dHCP neonatal dataset  For the CoTAN model, we set the resolution R=3 and the number of SVFs M =4 for each resolution. For integration, we set the total number of steps to  K=50 with step size h=0.02. We re-mesh the initial mesh to 140k vertices, of which the coordinates are normalized to [-1, 1]. We use the Adam optimizer for training. For the white matter surface, we first pre-train CoTAN for 100 epochs using a learning rate of γ=10 -4 and weights λ lap =0.5, λ nc =5×10 -4 for the loss function. Then we fine-tune for 100 epochs using smaller weights λ lap =0.1 and λ nc =10 -4 with γ=2×10 -5 . For the pial surface, we set the maximum channel size of CoTAN as 32 to avoid overfitting and train for 200 epochs with γ=10 -4 . We only consider left brain hemisphere in the experiments. All experiments are conducted on a Nvidia RTX3080 GPU with 12GB memory. Comparative Results. We compare the performance of CoTAN with existing learning-based cortical surface extraction approaches including Cortex-ODE  -Geometric Accuracy: We measure the geometric accuracy by commonly used metrics  -Mesh Quality: We evaluate the mesh quality by the ratio of self-intersecting faces (SIF) as shown in Table  -Computational Efficiency: We report the runtime and GPU memory cost for both training and testing, as well as the number of learnable parameters for CoTAN and all baseline approaches in Table  Next, we examine the effectiveness of the CTVF v t (x; a) by fixing the input time t=0 or age a=0. We train CoTAN models to predict the TVF (a=0), CVF (t=0) and SVF (a=t=0), which are degraded from the CTVF. Table  Lastly, we train a U-Net to predict R×M SVFs and integrate them directly without attention. Since the gradients are backpropagated through all SVFs, it requires >140 h training time which is 2.4× slower than our attention-based CoTAN. The model is also sensitive to small updates, which can affect all SVFs. This results in exploding gradients which we have observed in the training, whereas CoTAN can be trained robustly by integrating a single CTVF. Attention Maps. We explore the attention maps p r,m (t, a) learned by CoTAN. We define p r = M m=1 p r,m to reflect the importance of the SVFs at each resolution level r=1, ..., R, where R = 3 and larger r means higher resolution. Figure  Discussion. One limitation of our experiments is that we only train and evaluate CoTAN based on the pseudo-GT generated by the dHCP structural pipeline "
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,4,Conclusion,"In this work, we propose CoTAN for diffeomorphic neonatal cortical surface reconstruction. CoTAN employs an attention mechanism to combine multiple SVFs to a CTVF, which outperforms existing baselines in geometric accuracy and mesh quality. CoTAN can also be extended and applied to extract adult cortical surfaces conditioned on the age, gender or pathological information of the subjects. Our future work will integrate CoTAN into a learning-based pipeline for universal cortical surface analysis across all age groups."
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,,Fig. 1 .,
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,,Fig. 2 .,
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,,Fig. 3 .,
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,,Fig. 4 .,
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,,Fig. 5 .,
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,,Fig. 6 .,
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,,Table 1 .,
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,,Table 2 .,
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,,Table 3 .,
Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_30.
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,1,Introduction,"Gliomas are the most commonly seen central nervous system malignancies with aggressive growth and low survival rates  Most previous studies of segmentation QC only provide subject-level quality assessment by either directly predicting segmentation-quality metrics or their surrogates. Specifically, Wang et al.  Multiple studies have also explored regression-based methods to directly predict segmentation-quality metrics, e.g., Dice Similarity Coefficient (DSC). For example, Kohlberger et al.  In summary, while numerous efforts have been devoted to segmentation QC, most works were in the context of cardiac MRI segmentation with few works tackling segmentation QC of brain tumors, which have more complex and heterogeneous appearances than the heart. Furthermore, most of the existing methods do not localize segmentation errors, which is meaningful for both auditing purposes and guiding manual refinement. To address these challenges, we propose a novel framework for joint subject-level and voxel-level prediction of segmentation quality from multimodal MRI. The contribution of this work is four-fold. First, we proposed a predictive model (QCResUNet) that simultaneously predicts DSC and localizes segmentation errors at the voxel level. Second, we devised a datageneration approach, called SegGen, that generates a wide range of segmentations of varying quality, ensuring unbiased model training and testing. Third, our end-to-end predictive model yields fast inference. Fourth, the proposed method achieved a good performance in predicting subject-level segmentation quality and identifying voxel-level segmentation failures."
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,2,Method,"Given four imaging modalities denoted as [X 1 , X 2 , X 3 , X 4 ] and a predicted multiclass brain tumor segmentation mask (S pred ), the goal of our approach is to automatically assess the tumor segmentation quality by simultaneously predicting DSC and identifying segmentation errors as a binary mask (S err ). Toward this end, we proposed a 3D encoder-decoder architecture termed QCResUNet (see Fig.  The ResNet-34 encoder enables the extraction of semantically rich features that are useful for characterizing the quality of the segmentation. We maintained the main structure of the vanilla 2D ResNet-34  The building block of the decoder consisted of an upsampling by a factor of two, which was implemented by a nearest neighbor interpolation in the feature map, followed by two convolutional blocks that halve the number of feature maps. Each convolutional block comprised a 3 × 3 × 3 convolutional layer followed by an instance normalization layer and a leaky ReLU activation  The objective function for training QCResUNet consists of two parts. The first part corresponds to the DSC regression task. It consists of a mean absolute error (MAE) loss (L MAE ) term that penalizes differences between ground truth (DSC gt ) and predicted DSC (DSC pred ): where N denotes the number of samples in a batch. The second part of the objective function corresponds to the segmentation error prediction. It consists of a dice loss  where S errgt , S err pred denote the binary ground-truth segmentation error map and the predicted error segmentation map from the sigmoid output of the decoder, respectively. The dice loss and cross-entropy loss were averaged across the number of pixels I in a batch. The two parts are combined using a weight parameter λ to balance the different loss components: 3 Experiments For this study, pre-operative multimodal MRI scans of varying grades of glioma were obtained from the 2021 Brain Tumor Segmentation (BraTS) challenge "
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,3.1,Data Generation,"The initial dataset was expanded by producing segmentation results at different levels of quality to provide an unbiased estimation of segmentation quality. To this end, we adopted a three-step approach. First, a nnUNet framework  The original BraTS 2021 training dataset was split into training (n = 800), validation (n = 200), and testing (n = 251) sets. After applying the three-step approach, it resulted in 48000, 12000, and 15060 samples for the three sets, respectively. However, this generated dataset suffered from imbalance (Fig.  In addition to the segmentations generated by the nnUNet framework and the SegGen method, we also generated out-of-distribution segmentation samples for the testing set to validate the generalizability of our proposed model. For this purpose, five models were trained on the training set using the DeepMedic framework "
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,3.2,Experimental Design,"Baseline Methods: In this study, we compared the performance of the proposed model to three baseline models: (i) a UNet model  Table "
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,,Statistical Analysis:,We assessed the performance of the subject-level segmentation quality prediction in terms of Pearson coefficient r and MAE between the predicted DSC and the ground-truth DSC. The performance of the segmentation error localization was assessed by the DSC err between the predicted segmenta- tion error map and the ground-truth segmentation error map. P-values were computed using a paired t-test between DSC predicted by QCResUNet versus ones predicted by corresponding baselines.
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,4,Results,"The proposed QCResUNet achieved good performance in predicting subjectlevel segmentation quality for in-sample (MAE = 0.0570, r = 0.964) and out-ofsample (MAE = 0.0606, r = 0.966) segmentations. The proposed method also showed statistically significant improvement against all three baselines (Table "
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,5,Conclusion,"In this work, we proposed a novel CNN architecture called QCResUNet to perform automatic brain tumor segmentation QC in multimodal MRI scans. QCRe-sUNet simultaneously provides subject-level segmentation-quality prediction and localizes segmentation failures at the voxel level. It achieved superior DSC prediction performance compared to all baselines. In addition, the ability to localize segmentation errors has the potential to guide the refinement of predicted segmentations in a clinical setting. This can significantly expedite clinical workflows, thus improving the overall clinical management of gliomas."
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,,Fig. 1 .,
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,,Fig. 2 .,
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,,Fig. 3 .,
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,,Fig. 4 .,
Boundary Difference over Union Loss for Medical Image Segmentation,1,Introduction,"Medical image segmentation is a vital branch of image segmentation  From CNN to Transformer, many different model architectures have been proposed, as well as a number of training loss functions. These losses can be mainly divided into three categories. The first class is represented by the Cross-Entropy Loss, which calculates the difference between the predicted probability distribution and the ground truth. Focal Loss  Our proposed Boundary DoU Loss improves the focus on regions close to the boundary through a region-like calculation similar to Dice Loss. The error region near the boundary is obtained by calculating the difference set of ground truth and prediction, which is then reduced by decreasing its ratio to the union of the difference set and a partial intersection set. To evaluate the performance of our proposed Boundary DoU loss, we conduct experiments on the ACDC "
Boundary Difference over Union Loss for Medical Image Segmentation,2,Method,This section first revisit the Boundary IoU metric 
Boundary Difference over Union Loss for Medical Image Segmentation,2.1,Boundary IoU Metric,"The Boundary IoU is a segmentation evaluation metric which mainly focused on boundary quality. Given the ground truth binary mask G, the G d denotes the inner boundary region within the pixel width of d. The P is the predicted binary mask, and P d denotes the corresponding inner boundary region, whose size is determined as a fixed fraction of 0.5% relative to the diagonal length of the image. Then, we can compute the Boundary IoU metric by using following equation, as shown in the left of Fig.  A large Boundary IoU value indicates that the G d and P d are perfectly matched, which means G and P are with a similar shape and their boundary are well aligned. In practice, the G d and P d is computed by the erode operation "
Boundary Difference over Union Loss for Medical Image Segmentation,2.2,Boundary DoU Loss,"As shown in the left of Fig.  where α is a hyper-parameter controlling the influence of the partial union area. Adaptive Adjusting α Based-on Target Size: On the other aspect, the proportion of the boundary area relative to the whole target varies for different sizes. When the target is large, the boundary area only accounts for a small proportion, and the internal regions can be easily segmented, so we are encouraged to focus more on the boundary area. In such a case, using a large α is preferred. However, when the target is small, neither the interior nor the boundary areas are easily distinguishable, so we need to focus simultaneously on the interior and boundary, and a small α is preferred. To achieve this goal, we future adaptively compute α based on the proportion, where C denotes the boundary length of the target, and S denotes its size. "
Boundary Difference over Union Loss for Medical Image Segmentation,2.3,Discussion,"In this part, we compare Boundary DoU Loss with Dice Loss. Firstly, we can re-write our Boundary DoU Loss as: where S D denotes the area of the difference set between ground truth and prediction, S I denotes the intersection area of them, and α = 1α. Meanwhile, the Dice Loss can be expressed by the following: where TP, FP and FN denote True Positive, False Positive, and False Negative, respectively. It can be seen that Boundary DoU Loss and Dice loss differ only in the proportion of the intersection area. Dice is concerned with the whole intersection area, while Boundary DoU Loss is concerned with the boundary since α < 1. Similar to the Dice loss function, minimizing the L DoU will encourage an increase of the intersection area (S I ↑) and a decrease of the different set (S D ↓). Meanwhile, the L DoU will penalize more over the ratio of S D /S I . To corroborate its effectiveness more clearly, we compare the values of L Dice and L DoU in different cases in Fig. "
Boundary Difference over Union Loss for Medical Image Segmentation,3.2,Implementation Details,"We conduct experiments on three advanced models to evaluate the performance of our proposed Boundary DoU Loss, i.e., UNet, TransUNet, and Swin-UNet. The models are implemented with the PyTorch toolbox and run on an NVIDIA GTX A4000 GPU. The input resolution is set as 224 × 224 for both datasets. For the Swin-UNet "
Boundary Difference over Union Loss for Medical Image Segmentation,3.3,Results,"Quantitative Results: Table  2) Compared with the Dice Loss, our Loss improves the DSC by 2.30%, 1.20%, and 1.89% on UNet, TransUNet, and Swin-UNet models, respectively. The Hausdorff Distance also shows a significant decrease. Meanwhile, we achieved the best performance on the Boundary IoU, which verified that our loss could improve the segmentation performance of the boundary regions. Table  Results of Target with Different Sizes: We further evaluate the influence of the proposed loss function for segmenting targets with different sizes. Based on the observation of C/S values for different targets, we consider a target to be a large one when C/S < 0.2 and otherwise as a small target. As shown in Table "
Boundary Difference over Union Loss for Medical Image Segmentation,4,Conclusion,"In this study, we propose a simple and effective loss (Boundary DoU) for medical image segmentation. It adaptively adjusts the penalty to regions close to the boundary based on the size of the different targets, thus allowing for better optimization of the targets. Experimental results on ACDC and Synapse datasets validate the effectiveness of our proposed loss function."
Boundary Difference over Union Loss for Medical Image Segmentation,,Fig. 1 .,
Boundary Difference over Union Loss for Medical Image Segmentation,,,
Boundary Difference over Union Loss for Medical Image Segmentation,,Fig. 2 .,
Boundary Difference over Union Loss for Medical Image Segmentation,,Fig. 3 .,
Boundary Difference over Union Loss for Medical Image Segmentation,,Fig. 4 .,
Boundary Difference over Union Loss for Medical Image Segmentation,,Table 1 .,
Boundary Difference over Union Loss for Medical Image Segmentation,,Table 2 .,
Boundary Difference over Union Loss for Medical Image Segmentation,,Table 3 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,1,Introduction,"Multi-modal learning has become a popular research area in computer vision and medical image analysis, with modalities spanning across various media types, including texts, audio, images, videos and multiple sensor data. This approach has been utilised in Robot Control  The missing modality issue is a significant challenge in the multi-modal domain, and it has motivated the community to develop approaches that attempt to address this problem. Havaei et al.  Aiming at this issue, we propose the non-dedicated training model -We propose the Learnable Cross-modal Knowledge Distillation (LCKD) model to address missing modality problem in multi-modal learning. It is a simple yet effective model designed from the viewpoint of distilling crossmodal knowledge to maximise the performance for all tasks; -The LCKD approach is designed to automatically identify the important modalities per task, which helps the cross-modal knowledge distillation process. It also can handle missing modality during both training and testing. The experiments are conducted on the Brain Tumour Segmentation benchmark BraTS2018 "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2,Methodology,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.1,Overall Architecture,"Let us represent the N -modality data with M l = {x ∈ X denotes the l th data sample and the superscript (i) indexes the modality. To simplify the notation, we omit the subscript l when that information is clear from the context. The label for each set M is represented by y ∈ Y, where Y represents the ground-truth annotation space. The framework of LCKD is shown in Fig.  Multi-modal segmentation is composed not only of multiple modalities, but also of multiple tasks, such as the three types of tumours in BraTS2018 dataset that represent the three tasks. Take one of the tasks for example. Our model undergoes an external Teacher Election Procedure prior to processing all modalities {x (i) } N i=1 ∈ M in order to select the modalities that exhibit promising performance as teachers. This is illustrated in Fig.  In the next sections, we explain each module of the proposed Learnable Crossmodal Knowledge Distillation model training and testing with full and missing modalities."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.2,Teacher Election Procedure,"Usually, one of the modalities is more useful than others for a certain task, e.g. for brain tumour segmentation, T1c scan clearly displays the enhanced tumour, but it does not clearly show edema  More specifically, in the teacher election procedure, a validation process is applied: for each task k (for k ∈ {1, ..., K}), the modality with the best performance is selected as the teacher t (k) . Formally, we have: where i indexes different modalities, F (•; Θ) is the LCKD segmentation model parameterised by Θ, including the encoder and decoder parameters {θ enc , θ dec } ∈ Θ, and d(•, •) is the function to calculate the Dice score. Based on the elected teachers for different tasks, a list of unique teachers (i.e., repetitions are not allowed in the list, so for BraTS, {T1c, T1c, Flair} would be reduced to {T1c, Flair}) are generated with: T = φ(t (1) , t (2) , ..., t (k) , ..., t (K) ), "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.3,Cross-Modal Knowledge Distillation,"As shown in Fig.  (3) The cross-modal knowledge distillation (CKD) is defined by a loss function that approximates all available modalities' features to the available teacher modalities in a pairwise manner for all tasks, as follows: where • p presents the p-norm operation, and here we expended the notation of missing modalities to make it more general by assuming a set of modalities m is missing. The minimisation of this loss pushes the model parameter values to a point in the parameter space that can maximise the performance of all tasks for all modalities."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.4,Missing Modality Feature Generation,"Because of the knowledge distillation between each pair of teachers and students, the features of modalities in the feature space ought to be close to the ""genuine"" features that can uniformly perform well for different tasks. Still assuming that modality set m is missing, the missing features f (n) can thus be generated from the available features: where |m| denotes the number of missing modalities."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.5,Training and Testing,"All features encoded from Eq. 3 or generated from Eq. 5 are then concatenated to be fed into the decoder parameterised by θ dec for predicting where ỹ ∈ Y is the prediction of the task. The training of the whole model is achieved by minimising the following objective function: tot (D, Θ) = task (D, θ enc , θ dec ) + α ckd (D; θ enc ),  Testing is based on taking all image modalities available in the input to produce the features from Eq. 3, and generating the features from the missing modalities with Eq. 5, which are then provided to the decoder to predict the segmentation with Eq. 6."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3,Experiments,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.1,Data and Implementation Details,"Our model and competing methods are evaluated on the BraTS2018 Segmentation Challenge dataset  3D UNet architecture (with 3D convolution and normalisation) is adopted as our backbone network, where the CKD process occurs at the bottom stage of the UNet structure. To optimise our model, we adopt a stochastic gradient descent optimiser with Nesterov momentum "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.2,Overall Performance,Table  The LCKD model significantly outperforms (as shown by the one-tailed paired t-test for each task between models in the last row of Table 
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.3,Analyses,"Single Teacher vs. Multi-teacher. To analyse the effectiveness of knowledge distillation from multiple teachers of all tasks in the proposed LCKD model, we perform a study to compare the model performance of adopting single teacher and multi-teachers for knowledge distillation. We enable multi-teachers for LCKD by default to encourage the model parameters to move to a point that can perform well for all tasks. However, for single teacher, we modify the  "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,4,Conclusion,"In this paper we introduced the Learnable Cross-modal Knowledge Distillation (LCKD), which is the first method that can handle missing modality during training and testing by distilling knowledge from automatically selected important modalities for all training tasks to train other modalities. Experiments on BraTS2018 "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 1 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Fig. 2 .Fig. 3 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Table 1 .,
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,,Table 2 .,
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,1,Introduction,"Deep neural networks (DNNs) have achieved phenomenal success in image analysis and comparable human performance in many semantic segmentation tasks. However, based on the assumption of DNNs, the training and testing data of the network should come from the same probability distribution  Ultrasound (US), as one of the most popular means of medical imaging, is widely used in daily medical practice to diagnose internal organs, such as vascular structures. Compared to other imaging methods, e.g., computed tomography (CT) and magnetic resonance imaging (MRI), US shows its advantages in terms of being radiation-free and portable. To accurately and robustly extract the vascular lumen for diagnosis, the Doppler signal  Data Augmentation. One of the most common ways of improving the generalization ability of DNNs is to increase the variability of the dataset  Image-Level Domain Adaptation. To make the network generalizable to target domains that are different from the source domain, the most intuitive way is to transfer the image style to the same domain. The work from Chen et al. achieved impressive segmentation results in MRI to CT adaptation by applying both image and feature level alignment  Feature Disentanglement. Instead of solving the domain adaptation problem directly at the image-level, many researchers focused on disentangling the features in latent space, forcing the network to learn the shared statistical shape model across different domains  To solve the performance drop caused by the domain shift in segmentation networks, the aforementioned methods require a known target domain, e.g., CT "
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,2,Method,"Our goal is to train a segmentation network that can generalize to unseen domains and serve as a good pre-trained model for downstream tasks, while the training dataset only contains images from a single domain. To this end, the training framework should be designed to focus on the shape of the segmentation target rather than the background or appearance of the images. Following this concept of design, we propose MI-SegNet. During the training phase, a parameterised data transformation procedure is undertaken for each training image (x). Two sets of parameters are generated for spatial (a 1 , a 2 ) and domain (d 1 , d 2 ) transformation respectively. For individual input, four transformed images (x a1d1 , x a2d2 , x a1d2 , x a2d1 ) are created according to the four possible combinations of the spatial and domain configuration parameters. Two encoders (E a , E d ) are applied to extract the anatomical features (f a1 , f a2 ) and domain features (f d1 , f d2 ) separately. The mutual information between the extracted anatomical features and the domain features from the same image is computed using mutual information neural estimator (MINE)  Notably, only two of the transformed images (x a1d1 , x a2d2 ) are fed into the network, while the other two (x a1d2 , x a2d1 ) are used as ground truth for reconstructions (Fig. "
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,2.1,Mutual Information,"In order to decouple the anatomical and domain features intuitively, a metric that can evaluate the dependencies between two variables is needed. Mutual information, by definition, is a metric that measures the amount of information obtained from a random variable by observing another random variable. The MI is defined as the Kullback-Leibler (KL) divergence between the joint distribution and the product of marginal distributions of random variables f a and f d : where p(f a , f d ) is the joint distribution and p(f a ) ⊗ p(f d ) is the product of the marginal distributions. Based on the Donsker-Varadhan representation  where T is any arbitrary given continuous function. By replacing T with a neural network T θMINE and applying Monte Carlo method  where (f a , f d ) are drawn from the joint distribution and (f a , f d ) are drawn from the product of marginal distributions. By updating the parameters θ MINE to maximize the lower bound expression in Eq. 3, a loose estimation of MI is achieved, also known as MINE  To force the anatomical and domain encoders to extract decoupled features, the MI is served as a loss to update the weights of these two encoder networks. The loss is defined as:"
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,2.2,Image Segmentation and Reconstruction,"To make the segmentation network independent of the domain information, the domain features are excluded when generating the segmentation mask. Here, the segmentation loss L seg is defined in the combined form of dice loss L dice and binary cross-entropy loss L bce . where l is the ground truth label, m represents the predicted mask, s is added to ensure the numerical stability, and N is the mini batch size. To ensure that the extracted anatomical and domain features can contain all the information of the input image, a generator network is used to reconstruct the image based on both features. The reconstruction loss is then defined as: where x n is the ground truth image, x n is the reconstructed image, w and h are the width and height of the image in pixel accordingly."
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,2.3,Data Transformation,"Since the training dataset only contains images from one single domain, it is necessary to enrich the diversity of the training data so that overfitting can be prevented and the generalization ability is increased. The transformation methods are divided into two categories, domain and spatial transformations. Each transformation (T ) is controlled by two parameters, probability (p) and magnitude (λ). Domain Transformations aim to transfer the single domain images to different domain styles. Five types of transformation methods are involved in this aspect, i.e., blurriness, sharpness, noise level, brightness, and contrast. The implementations are identical to  Spatial Transformations mainly consist of two parts, crop and flip. For cropping, a window with configurable sizes ([0.7, 0.9] of the original image size) is randomly masked on the original image. Then the cropped area is resized to the original size to introduce varying shapes of anatomy. Here λ controls the size and the position of the cropping window. Besides cropping, horizontal flipping is also involved. Unlike domain transformations, the labels are also transformed accordingly by the same spatial transformation. The probability (p) of flipping is 5%, while the p for cropping is 50% to introduce varying anatomy sizes.The images are then transformed in a stacked way: where n = 7 represents the seven different transformation methods involved in our work represents the magnitude parameter, and contains all the probability parameters for each transformations. In our setup, Λ and P can be further separated into a = [Λ a ; P a ] and d = [Λ d ; P d ] for spatial and domain transformations respectively."
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,2.4,Cross Reconstruction,"According to experimental findings, the MI loss indeed forces the two representations to have minimal shared information. However, the minimization of MI between the anatomical and domain features cannot necessarily make both features contain the respective information. The network goes into local optimums frequently, where the domain features are kept constant, and all the information is stored in the anatomical features. Because there is no information in the domain features, the MI between two representations is thus approaching zero. However, this is not our original intention. As a result, cross reconstruction strategy is introduced to tackle this problem. The cross reconstruction loss will punish the behavior of summarizing all the information into one representation. Thus, it can force each encoder to extract informative features accordingly and prevent the whole network from going into the local optimums."
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,3,Experiments,
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,3.1,Implementation Details,"The training dataset consists of 2107 carotid US images of one adult acquired using Siemens Juniper US Machine (ACUSON Juniper, SIEMENS AG, Germany) with a system-predefined ""Carotid"" acquisition parameter. The test dataset consists of (1) ValS: 200 carotid US images which are left out from the training dataset, (2) TS1: 538 carotid US images of 15 adults from Ultrasonix device, (3) TS2: 433 US images of 2 adults and one child from Toshiba device, and (4) TS3: 540 US images of 6 adults from Cephasonics device (Cephasonics, California, USA). TS1 and TS2 are from a public database of carotid artery  All the images are resized to 256 × 256 for training and testing. We use Adam optimizer with a learning rate of 1 × 10 -4 to optimize all the parameters. The training is carried out on a single GPU (Nvidia TITAN Xp) with 12 GB memory."
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,3.2,Performance Comparison on Unseen Datasets,"In this section, we compare the performance of the proposed MI-SegNet with other state-of-art segmentation networks. All the networks are trained on the same dataset described in Sect. 3.1 with 200 episodes. Without Adaptation: The trained models are then tested directly on 4 different datasets described in Sect. 3.1 without further training or adaptation on the unseen domains. The dice score (DSC) is applied as the evaluation metrics. The results are shown in Table "
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,4,Discussion and Conclusion,"In this paper, we discuss the particular importance of domain adaptation for US images. Due to the low speed of sound compared to light and X-ray, the complexity of US imaging and its dependency on many parameters are more remarkable than optical imaging, X-ray, and CT. Therefore, the performance decay caused by the domain shift is a prevalent issue when applying DNNs in US images. To address this problem, a MI-based disentanglement method is applied to increase the generalization ability of the segmentation networks for US image segmentation. The ultimate goal of increasing the generalizability of the segmentation network is to apply the network to different unseen domains directly without any adaptation process. However, from the authors' point of view, training a good pre-trained model that can be adapted to an unseen dataset with minimal annotated data is still meaningful. As demonstrated in Sect. 3.2, the proposed model also shows the best performance in the downstream adaptation tasks. Currently, only the conventional image transformation methods are involved. In the future work, more realistic and US specific image transformations could be implemented to strengthen the feature disentanglement."
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,,Fig. 1 .,
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,,Fig. 2 .,
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,,Table 1 .,
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,,Table 2 .,
MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_13.
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,1,Introduction,"Malignant melanoma is one of the most rapidly growing cancers in the world. As estimated by the American Cancer Society, there were approximately 100,350 new cases and over 6,500 deaths in 2020  TransUNet  Prior works have enhanced performance by introducing intricate modules, but neglected the constraint of computational resources in real medical settings. Hence, there is an urgent need to design a low-parameter and low-computational load model for segmentation tasks in mobile healthcare. Recently, UNeXt  To be specific, EGE-UNet leverages two key modules: the Group multi-axis Hadamard Product Attention module (GHPA) and Group Aggregation Bridge module (GAB). On the one hand, recent models based on ViT  In summary, our contributions are threefold: (1) GHPA and GAB are proposed, with the former efficiently acquiring and integrating multi-perspective information and the latter accepting features at different scales, along with an auxiliary mask for efficient multi-scale feature fusion. (2) We propose EGE-UNet, an extremely lightweight model designed for skin lesion segmentation. (3) We conduct extensive experiments, which demonstrate the effectiveness of our methods in achieving state-of-the-art performance with significantly lower resource requirements."
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,2,EGE-UNet,"The Overall Architecture. EGE-UNet is illustrated in Fig.  Group Aggregation Bridge Module. The acquisition of multi-scale information is deemed pivotal for dense prediction tasks, such as medical image segmentation. Hence, as shown in Fig.  Loss Function. In this study, since different GAB require different scales of mask information, deep supervision  (1) where Bce and Dice represent binary cross entropy and dice loss. λ i is the weight for different stage. In this paper, we set λ i to 1, 0.5, 0.4, 0.3, 0.2, 0.1 from i = 0 to i = 5 by default."
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,3,Experiments,"Datasets and Implementation Details. To assess the efficacy of our model, we select two public skin lesion segmentation datasets, namely ISIC2017 [1,3] and ISIC2018 [2,6], containing 2150 and 2694 dermoscopy images, respectively. Consistent with prior research "
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,,Comparative Results. The comparative experimental results presented in,Table  Figure  Ablation Results. We conduct extensive ablation experiments to demonstrate the effectiveness of our proposed modules. The baseline utilized in our work is referenced from MALUNet 
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,4,Conclusions and Future Works,"In this paper, we propose two advanced modules. Our GHPA uses a novel HPA mechanism to simplify the quadratic complexity of the self-attention to linear complexity. It also leverages grouping to fully capture information from different perspectives. Our GAB fuses low-level and high-level features and introduces a mask to integrate multi-scale information. Based on these modules, we propose EGE-UNet for skin lesion segmentation tasks. Experimental results demonstrate the effectiveness of our approach in achieving state-of-the-art performance with significantly lower resource requirements. We hope that our work can inspire further research on lightweight models for the medical image community. Regarding limitations and future works, on the one hand, we mainly focus on how to greatly reduce the parameter and computation complexity while improving performance in this paper. Thus, we plan to deploy EGE-UNet in a real-world environment in the future work. On the other hand, EGE-UNet is currently designed only for the skin lesion segmentation task. Therefore, we will extend our lightweight design to other tasks."
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,,Fig. 1 .,
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,,Fig. 2 .,
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,,Fig. 3 .,
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,,Fig. 4 .,
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,,Table 1 .,
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,,Table 2 .,
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,1,Introduction,"Quantitative analysis of structural MRI of the human brain is essential in various anatomical investigations in brain development, aging, and degradation. Accurate segmentation of brain structures is a prerequisite for quantitative and particularly morphometric analysis  During the past decade, MRI based whole brain segmentation approaches have been explored. Multi-atlas based methods  Recently, deep learning has demonstrated state-of-the-art (SOTA) performance on various medical image segmentation tasks  However, existing methods tend to ignore the ontology-based hierarchical structural relationship (OHSR) of the human brain's anatomy. Most of them assume all brain structures are disjoint and use multiple U-Nets to separately perform voxel-wise predictions for each of multiple structures of interest. It has been suggested that neuroanatomical experts recognize and delineate brain anatomy in a coarse-to-fine manner (Fig.  To mimic experts' hierarchical perception of the brain anatomy, we here propose a novel approach to learn brain's hierarchy based on ontology, for a purpose of whole brain segmentation. Specifically, we encode the multi-level ontology knowledge into a voxel-wise embedding space. Deep metric learning is conducted to cluster contextually similar voxels and separate contextually dissimilar ones using a triplet loss with dynamic violate margin. By formatting the brain hierarchy into a directed acyclic graph, the violate margin can be easily induced by the height of the tree rooted at triplet's least common subsumer. As a result, the network is able to exploit the hierarchical relationship across different brain structures. The feature prototypes in the latent space are hierarchically organized following the brain hierarchy. To the best of our knowledge, this is the first work to incorporate ontology-based brain hierarchy into deep learning segmentation models. We evaluate our method on two whole brain segmentation datasets with different granularity and successfully establish SOTA performance."
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,2,Methodology,"The proposed approach builds upon standard 3D U-Net and makes use of multilevel ontology knowledge from the brain hierarchy to enhance the whole brain segmentation performance. In subsequent subsections, we first revisit the standard triplet loss "
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,2.1,Triplet Loss,"The goal of the triplet loss is to learn a feature embedding space wherein distances between features correspond to semantic dissimilarities between objects. Given a triplet T i = {f i , f + i , f - i } comprising an anchor voxel-wise feature vector f i , a positive voxel-wise feature vector f + i which is semantically similar to the anchor vector, and a negative voxel-wise feature vector f - i which is semantically dissimilar to the anchor vector, the triplet loss is formulated as where •, • is a distance function to evaluate the semantic dissimilarity between two feature vectors. The violate margin M is a hyperparameter that defines the minimum distance between positive and negative samples. It forces the gap between f, f + and f, f -to be larger than M and ensures the model does not learn trivial solutions. [•] + = max{0, •} is the hinge loss, which prevents the model from being updated when the triplet is already fulfilled. During training, the overall objective of the voxel-wise triplet loss is to minimize the sum of the loss over all triplets in a mini-batch, namely where N is the total number of triplets in a mini-batch. Note that a triplet is allowed to consist of voxel-wise feature vectors from different subjects when the batch size is larger than 1. This strategy of sampling an anchor's neighbor enables the model to learn the global context in the brain instead of the local context in a subspace of the brain, since it is infeasible to train a 3D U-Net with whole brain MRI data. However, it is challenging to apply the standard triplet loss to learn brain hierarchy: postive or negative is ill-defined with a fixed violate margin. For instance, the violate margin between f hippo , f amyg and f hippo , f fimb is certainly different from that between f hippo , f amyg and f hippo , f IIIvent : the hippocampus, the amygdala, and the fimbria all belong to the limbic system while the third ventricle belongs to cerebrospinal fluid (CSF). As such, a distance function d G (•, •) is required to measure the semantic dissimilarity between two brain structures, and can be then used to determine the corresponding violate margin."
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,2.2,Measuring Semantic Dissimilarity,"Let G = (V, E) be a directed acyclic graph with vertices V and edges E ⊆ V 2 . It specifies the hyponymy relationship between structures at different ontology levels. An edge (u, v) ∈ E indicates u is an ancestor vertex of v. Specifically, v belongs to u at a higher ontology level. The structures of interest S = {s 1 , ..., s n } ⊆ V are of the lowest ontology level. An example is shown in Fig.  A common measure for the dissimilarity d G : S 2 → R between two structures is the height of the tree rooted at the least common subsumer (LCS) divided by the height of the whole brain hierarchy tree, namely where the height of a tree h(•) = max v∈V ψ(•, v) is defined as the length of the longest path from the root to a leaf. ψ(•, •) is defined as the number of edges in the shortest path between two vertices. lcs(•, •) refers to the ancestor shared by two vertices that do not have any child also being an ancestor of the same two vertices. With respect to the example hierarchy in Fig. "
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,2.3,Dynamic Violate Margin,"With d G (•, •), we can define positive and negative samples and their violate margin in the triplet loss. We sample triplet , where f, f + i , f - i are the feature vectors of the voxels respectively labeled as v, v + i , v - i . Then the violate margin M can be determined dynamically M = 0.5(M τ + M ), M τ ∈ (0, 1] is the hierarchy-induced margin required between negative pairs and positive pairs in terms of d G (•, •). M is the tolerance of the intra-class variance, which is computed as the average distance between samples in v. In this work, we adopt the cosine distance as our distance function in latent space: . The triplet loss can thus be reformulated as Collectively, the overall training objective to learn OHSR is where λ is a hyperparameter."
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,3,Experiment,
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,3.1,Datasets and Implementation,We evaluate our method on two public-accessible datasets with manually labeled fine-grained or coarse-grained brain structures. The first one is JHU Adult Atlas  Table 
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,3.2,Evaluation Results,"We now report quantitative and qualitative evaluation results. The Dice similarity coefficient (DSC) is used to quantitatively measure the segmentation accuracy. We compare our proposed method with SOTA methods and conduct several ablation studies to demonstrate the effectiveness of our method. Comparisons with SOTA. To fairly compare with SOTA methods, we use the identical 3D U-Net and data augmentation hyperparameters as the ""3d fullres nnUNetTrainerV2 noMirroring"" configuration in nnU-Net. As summarized in Table  Ablation Studies. We first evaluate the effectiveness of incorporating OHSR into U-Net. The experiments are conducted on JHU Adult Atlas. As shown in Table  Qualitative Results. Qualitative comparisons are demonstrated in Fig.  From the axial view, we can clearly see the external capsule is well-segmented by our proposed method, while other methods can hardly differentiate its boundary. From the coronal and sagittal views, we observe that our method can better capture and preserve the overall shape of the lateral frontal-orbital gyrus."
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,4,Conclusion,"In this paper, we propose a novel approach to learn brain hierarchy based on ontology for whole brain segmentation. By introducing graph-based dynamic violate margin into the triplet loss, we encode multi-level ontology knowledge into a voxel-wise embedding space and mimic experts' hierarchical perception of the brain anatomy. We successfully demonstrate that our proposed method outperforms SOTA methods both quantitatively and qualitatively. We consider introducing hierarchical information into the output space as part of our future efforts."
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,,Fig. 1 .,
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,,,
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,,Fig. 3 .,
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,,Fig. 4 .,
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,,.69 83.51 ± 13.13 85.52 ± 5.38 83.67 ± 11.26Table 2 .,
Learning Ontology-Based Hierarchical Structural Relationship for Whole Brain Segmentation,,Table 3 .,
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,1,Introduction,"Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) revealing tumor hemodynamics information is often applied to early diagnosis and treatment of breast cancer  Recently, denoising diffusion probabilistic model (DDPM)  Based on the above observations, we innovatively consider the underlying relation between hemodynamic response function (HRF) and denoising diffusion process (DDP). As shown in Fig.  Once the diffusion module is pretrained, the latent kinetic code can be easily generated with only pre-contrast images, which is fed into a segmentation module to annotate cancers. To verify the effectiveness of the latent kinetic code, the SM adopts a simple U-Net-like structure, with an encoder to simultaneously conduct semantic feature encoding and kinetic code fusion, along with a decoder to obtain voxel-level classification. In this manner, our latent kinetic code can be interpreted to provide TIC information and hemodynamic characteristics for accurate cancer segmentation. We verify the effectiveness of our proposed diffusion kinetic model (DKM) on DCE-MRI-based breast cancer segmentation using Breast-MRI-NACT-Pilot dataset  • We propose a diffusion kinetic model that implicitly exploits hemodynamic priors in DCE-MRI and effectively generates high-quality segmentation maps only requiring pre-contrast images. • We first consider the underlying relation between hemodynamic response function and denoising diffusion process and provide a DDPM-based solution to capture a latent kinetic code for hemodynamic knowledge. • Compared to the existing approaches with complete sequences, the proposed method yields higher cancer segmentation performance even with pre-contrast images."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,2,Methodology,The overall framework of the proposed diffusion kinetic model is illustrated in Fig. 
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,2.1,Diffusion Module,"The diffusion module is following the denoising diffusion probabilistic model  Particularly, a noisy image x t can be directly obtained from the data x 0 : where α t := 1β t and ᾱt := t s=1 α s . Afterwards, DDPM approximates the reverse diffusion process by the following parameterized Gaussian transitions: where μ θ (x t , t) is the learned posterior mean and θ (x t ; t) is a fixed set of scalar covariances. In particular, we employ a noise predictor network ( θ (x t , t)) to predict the noise component at the step t (As shown in Fig.  Inspired by the property of DDPM  where α t := 1-β t and ᾱt := t s=1 α s . Next, we employ the reverse diffusion process to transform the noisy sample x t to the post-contrast data x k . As thus, the DM gradually exploits the latent kinetic code by comparing the pre-contrast and post-contrast images, which contains hemodynamic knowledge for segmentation."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,2.2,Segmentation Module,"Once pretrained, the DM outputs multi-scale latent kinetic code f dm from intermediate layers, which is fed into the SM to guide cancer segmentation. As shown in Fig.  where * represents 1 × 1 based convolution operation, W is the weight matrix, BN represents batch normalization, φ represents ReLU activation function and C is concatenation operation. In this way, the hemodynamic knowledge can be incorporated into the SM to capture more expressive representations to improve segmentation performance."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,2.3,Model Training,"To maintain training stability, the proposed DKM adopts a two-step training procedure for cancer annotation. In the first step, the DM is trained to transform pre-contrast images into post-contrast images for a latent space where hemodynamic priors are exploited. In particular, the diffusion loss for the reverse diffusion process can be formulated as follows: where θ represents the denoising model that employs an U-Net structure, x 0 and x k are the pre-contrast and post-contrast images, respectively, is Gaussian distribution data ∼ N (0, I), and t is a timestep. For a second step, we train the SM that integrates the previously learned latent kinetic code to provide tumor hemodynamic information for voxel-level prediction. Considering the varying sizes, shapes and appearances of tumors that results from intratumor heterogeneity and results in difficulties of accurate cancer annotation, we design the segmentation loss as follows: where L SSIM is used to evaluate tumor structural characteristics, S and G represents segmentation map and ground truth, respectively; μ S is the mean of S and μ G is the mean of G; ϕ S represents the variance of S and ϕ G represents the variance of G; C 1 and C 2 denote the constant to hold training stable  where k 1 is set as 0.01, k 2 is set as 0.03 and L is set as the range of voxel values."
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,3,Experiments,"Dataset: To demonstrate the effectiveness of our proposed DKM, we evaluate our method on 4D DCE-MRI breast cancer segmentation using the Breast-MRI-NACT-Pilot dataset "
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,,Competing Methods and Evaluation Metrics:,"To comprehensively evaluate the proposed method, We compare it with 3D segmentation methods, including Dual Attention Net (DANet)  Implementation Details: We implement our proposed framework with PyTorch using two NVIDIA RTX 2080Ti GPUs to accelerate model training. Following DDPM  Once the DM is trained, we extract intermediate feature maps from four resolutions for further segmentation task. Similar to DM, the SM also consists of four resolution blocks. However, unlike channel settings of DM, we set 128, 256,  512, 1024 channels for each stage in the SM to capture expressive and sufficient semantic information. The SM is optimized by Adam with a learning rate 2 × 10 -5 and a weight decay 10 -6 . The model is trained for 500 epochs with the batch size to 1. No data augmentation techniques are used to ensure fairness. Comparison with SOTA Methods: The quantitative comparison of the proposed method to recent state-of-the-art methdos is reported in Table  2) The intermediate activations from  diffusion models effectively capture the semantic information and are excellent pixel-level representations for the segmentation problem  Ablation Study: To explore the effectiveness of the latent kinetic code, we first conduct ablation studies to select the optimal setting. We denote the intermediate features extracted from each stage in the DM as f 1 , f 2 , f 3 , and f 4 , respectively, where f i represents the feature map of i-th stage. Table "
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,4,Conclusion,We propose a diffusion kinetic model by exploiting hemodynamic priors in DCE-MRI to effectively generate high-quality segmentation results only requiring precontrast images. Our models learns the hemodynamic response function based on the denoising diffusion process and estimates the latent kinetic code to guide the segmentation task. Experiments demonstrate that our proposed framework has the potential to be a promising tool in clinical applications to annotate cancers.
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,,Fig. 1 .,
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,,Fig. 2 .,
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,,Fig. 3 .,
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,,Fig. 4 .,
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,,Table 1 .,
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,,Table 2 .,
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,1,Introduction,"Mitochondria segmentation from electron microscopy (EM) images is pivotal to mitochondria morphological analysis  The mainstream of previous works focus on aligning the distributions of the source and target domains with supervision directly on the output segmentation maps. One line of these works use the model pretrained on source domain to obtain pseudo labels for target domain  In this work, to take full advantage of the sufficient class related information in the feature space, we propose a class-aware domain alignment in the feature space for mitochondria segmentation, which relies on the prototype representation  Our contributions can be summarized as follows: 1) We propose a class-aware feature alignment method for domain adaptive mitochondria segmentation. To our best knowledge, it is the first attempt to align source and target domains on the feature level in UDA for EM mitochondria segmentation. 2) Our class-aware feature alignment relies on the source prototypes, which represent class knowledge from the feature space. With these prototypes, an innovative distance-based alignment and pseudo-labeling are incorporated to achieve class-aware feature alignment. 3) We propose an intra-domain consistency constraint in the target domain to tap into the potential target domain information. 4) We conduct thorough experiments on various EM dataset benchmarks and our proposed method achieves state-of-the-art performance for mitochondria segmentation."
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,2,Class-Aware Feature Alignment,"Problem Formulation. Unsupervised domain adaptation (UDA) aims to transfer the knowledge learned from the labeled source domain to the unlabeled target domain. In our work, we denote the source domain as with M samples, where y s i is the groundtruth binary segmentation map of the input image x s i . The unlabeled target domain is denoted as with N samples. The overall framework of our proposed method is shown in Fig. "
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,,Prototype Extraction.,"Considering there exists more plentiful class-aware information in the feature space than the predictions, we propose the class-aware alignment for better adaptation in the feature space. To achieve class-aware alignment, we first derive the class-aware source prototypes from the source features with the corresponding labels. The prototypes can be calculated as the centroid of each class in the feature space: where f s b,h,w ∈ R is the source feature vectors, B s is the batch size, and H s , W s is the height and width of the features. c is the index of class number C. The maximum of C is 1. The prototypes can represent the class knowledge in the source domain."
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,,Inter-and Intra-class Constraints.,"To make the source prototypes represent the class-discriminative source knowledge more accurately, we incorporate inter-and intra-class constraints on the prototypes, which can further help better class-aware alignment across domains. The inter-class loss L s inter intends to push the prototypes of different classes far away from each other, which can be implemented by minimizing the average cosine distance of different prototype pairs: In contrast, the intra-class loss L s intra is designed to pull the feature instance point closer to its corresponding prototype, i.e., making the feature distribution of the same class more concentrated/compact. The intra-class loss can be formulated as maximizing the average cosine distance between the prototype and the features belonging to the same class: It is not straightforward to align target domain to source domain in the class level, considering the lack of groundtruth labels in target domain. To achieve more reliable class-aware alignment for target samples, we only perform alignment on the instances with higher confidence. Specifically, we first calculate the cosine distance between each target feature and all the source prototypes, and only select instances { f t } the distance of which is closer than a preset threshold τ . The intra-class alignment loss enforces f t c to be closer to its corresponding prototype p t c : The class-aware alignment loss L align is the combination of these three losses, i.e., L align = L s intra + L s inter + L t intra . It is noteworthy that the alignment loss is optimized directly on the feature space instead of the final output predictions, considering there is more abundant information in the feature space. Pseudo Supervision. The above mentioned alignment loss L align only affects partial target features with higher confidence. To further force the alignment across domains in the feature space, we incorporate a pseudo supervision on the feature space. Specifically, based on the cosine distance between the feature of each location and the source prototypes, we can attain a distance map P g , which can be regarded as a segmentation prediction directly from feature space instead of the prediction head. We utilize a pseudo label map P t2 as groundtruth to supervise the learning of P g , leading to alignment directly on feature space. The formulation of P t2 will be discussed in the later section. The supervision is the standard cross entropy loss: Intra-domain Consistency. The alignment cross domains will borrow the knowledge from source domain to target domain. However, there exists abundant knowledge and information in the target domain itself  We incorporate two consistency losses on the feature level L cf and the final prediction level L cp , respectively: where MSE denotes the standard mean squared error loss. Training and Inference. During the training phase, the total training objective L total is formulated as : where L s seg denotes the supervised segmentation loss with the cross-entropy loss and λ {align,p,cf,cp} are the hyperparameters for balancing different terms. Note Table "
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,,Methods,"VNC III → Lucchi (Subset1) VNC III → Lucchi (Subset2) mAP(%) F1(%) MCC(%) IoU(%) mAP(%) F1(%) MCC(%) IoU(%) that the feature extractor and the segmentation head are shared weights in the training phase. Their detailed structures can be found in the supplementary material. During the inference phase, we only adopt the trained feature extractor and segmentation head to predict the target images."
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,,Oracle,
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,3,Experiments,"Datasets. Following the previous work  Implementation Details. Our network architecture is following  Comparisons with Baselines. The binary segmentation result comparisons of our proposed method with previous works on the Lucchi and MitoEM datasets are shown in Table  Ablation Study for Loss Functions. We conduct thorough ablation experiments to validate the contribution of each loss term in Eq. 7, where the results are shown in Table "
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,4,Conclusion,"In this paper, for the first time, we propose the class-aware alignment for domain adaptation on mitochondria segmentation in the feature space. Based on the extracted source prototypes representing class knowledge, we design intradomain and inter-domain alignment constraint for fine-grained alignment cross domains. Furthermore, we incorporate an intra-domain consistency loss to take full advantage of the potential information existed in target domain. Comprehensive experiments demonstrate the effectiveness of our proposed method."
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,,Fig. 1 .,
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,,Fig. 2 .,
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,,Table 2 .,
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,,Table 3 .,
Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 23.
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,1,Introduction,"Accurate gland segmentation from whole slide images (WSIs) plays a crucial role in the diagnosis and prognosis of cancer, as the morphological features of glands can provide valuable information regarding tumor aggressiveness  To reduce the annotation cost, developing annotation-efficient methods for semantic-level gland segmentation has attracted much attention  One potential solution is to adopt unsupervised semantic segmentation (USS) methods which have been successfully applied to medical image research and natural image research. On the one hand, existing USS methods have shown promising results in various medical modalities, e.g., magnetic resonance images  x-ray images  To tackle the above challenges, our solution is to incorporate an empirical cue about gland morphology as additional knowledge to guide gland segmentation. The cue can be described as: Each gland is comprised of a border region with high gray levels that surrounds the interior epithelial tissues. To this end, we propose a novel Morphology-inspired method via Selective Semantic Grouping, abbreviated as MSSG. To begin, we leverage the empirical cue to selectively mine out proposals for the two gland sub-regions with variant appearances. Then, considering that our segmentation target is the gland, we employ a Morphology-aware Semantic Grouping module to summarize the semantic information about glands by explicitly grouping the semantics of the sub-region proposals. In this way, we not only prioritize and dedicate extra attention to the target gland regions, thus avoiding under-segmentation; but also exploit the valuable morphology information hidden in the empirical cue, and force the segmentation network to recognize entire glands despite the excessive variance among the sub-regions, thus preventing over-segmentation. Ultimately, our method produces well-delineated and complete predictions; see Fig.  Our contributions are as follows: (1) We identify the major challenge encountered by prior unsupervised semantic segmentation (USS) methods when dealing with gland images, and propose a novel MSSG for unsupervised gland segmentation. "
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,2,Methodology,The overall pipeline of MSSG is illustrated in Fig. 
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,2.1,Selective Proposal Mining,"Instead of generating pseudo-labels for the gland region directly from all the pixels of the gland images as previous works typically do, which could lead to over-segmentation and under-segmentation results, we propose using the empirical cue as extra hints to guide the proposal generation process. Specifically, let the i th input image be denoted as X i ∈ R C×H×W , where H, W , and C refer to the height, width, and number of channels respectively. We first obtain a normalized feature map F i for X i from a shallow encoder f with 3 convolutional layers, which can be expressed as F i = f (X i ) 2 . We train the encoder in a self-supervised manner, and the loss function L consists of a typical self-supervised loss L SS , which is the cross-entropy loss between the feature map F i and the one-hot cluster label C i = arg max (F i ), and a spatial continuity loss L SC , which regularizes the vertical and horizontal variance among pixels within a certain area S to assure the continuity and completeness of the gland border regions (see Fig.  (1) (2) Then we employ K-means "
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,2.2,Morphology-Aware Semantic Grouping,"A direct merge of the two sub-region proposals to train a fully-supervised segmentation network may not be optimal for our case. Firstly, the two gland subregions exhibit significant variation in appearance, which can impede the segmentation network's ability to recognize them as integral parts of the same object. Secondly, the SPM module may produce proposals with inadequate highlighting of many gland regions, particularly the interior epithelial tissues, as shown in Fig.  Here, we first slice the gland image and its proposal map into patches as inputs. Let the input patch and its corresponding sliced proposal map be denoted as X ∈ R C× Ĥ× Ŵ and P ∈ R 3× Ĥ× Ŵ . We can obtain the feature embedding map F which is derived as F = f feat ( X) and the prediction map X as X = f cls ( F ), where f feat and f cls refers to the feature extractor and pixel-wise classifier of the segmentation network respectively. MSG for Variation is designed to mitigate the adverse impact of appearance variation between the gland sub-regions. It regulates the pixel-level feature embeddings of the two sub-regions by explicitly reducing the distance between them in the embedding space. Specifically, according to the proposal map P , we divide the pixel embeddings in F ∈ R D× Ĥ× Ŵ into gland border set G = g 0 , g 1 , ..., g kg , interior epithelial tissue set I = {i 0 , i 1 , ..., i ki } and non-glandular (i.e., background) set N = {n 0 , n 1 , ..., n kn }, where k g + k i + k n = Ĥ × Ŵ . Then, we use the average of the pixel embeddings in gland border set G as the alignment anchor and pull all pixels of I towards the anchor: ( MSG for Omission is designed to overcome the problem of partial omission in the proposals. It identifies and relabels the overlooked gland regions in the proposal map and groups them back into the gland semantic category. To achieve this, for each pixel n in the non-glandular (i.e., background) set N , two similarities are computed with the gland sub-regions G and I respectively: S G n (or S I n ) represents the similarity between the background pixel n and gland borders (or interior epithelial tissues). If either of them is higher than a preset threshold β (set to 0.7), we consider n as an overlooked pixel of gland borders (or interior epithelial tissues), and relabel n to G (or I). In this way, we could obtain a refined proposal map RP . Finally, we impose a pixel-level cross-entropy loss on the prediction and refined proposal RP to train the segmentation network: The total objective function L for training the segmentation network can be summarized as follows: "
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,3,Experiments,
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,3.1,Datasets,We evaluate our MSSG on The Gland Segmentation Challenge (GlaS) dataset 
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,3.2,Implementation Details,"The experiments are conducted on four RTX 3090 GPUs. For the SPM, a 3layer encoder is trained for each training sample. Each convolutional layer uses a 3 × 3 convolution with a stride of 1 and a padding size of 1. The encoder is trained for 50 iterations using an SGD optimizer with a polynomial decay policy and an initial learning rate of 1e-2. For the MSG, MMSegmentation "
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,3.3,Comparison with State-of-the-art Methods,We compare our MSSG with multiple approaches with different supervision settings in Table 
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,3.4,Ablation Study,"Table  With the incorporation of MSG for Variation, the latent distance between gland borders and interior epithelial tissues is becoming closer, while both of these two sub-regions are further away from the background. As a result, the model can highlight most of the gland borders and interior epithelial tissues. "
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,4,Conclusion,"This paper explores a DL method for unsupervised gland segmentation, which aims to address the issues of over/under segmentation commonly observed in previous USS methods. The proposed method, termed MSSG, takes advantage of an empirical cue to select gland sub-region proposals with varying appearances. Then, a Morphology-aware Semantic Grouping is deployed to integrate the gland information by explicitly grouping the semantics of the selected proposals. By doing so, the final network is able to obtain comprehensive knowledge about glands and produce well-delineated and complete predictions. Experimental results prove the superiority of our method qualitatively and quantitatively."
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,,Fig. 1 .,
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,,Fig. 2 .,
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,,Fig. 3 .,
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,,Fig. 4 .,
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,,,
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,,Table 1 .,
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,,Table 2 .,
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,1,Introduction,"Reliable and robust segmentation of medical images plays a significant role in clinical diagnosis  Combining both methods can potentially lead to improved performance, as pseudo labeling leverages unlabeled data and consistency regularization improves the model's robustness "
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,2,Method,"We present MLB-Seg, a novel consistency-guided meta-learning framework for semi-supervised medical image segmentation. Assume that we are given a training dataset consisting of clean data D c = {(x c i , y c i )} N i=1 , and unlabeled data , where the input image x c i , x u k are of size H × W with the corresponding clean ground-truth mask y c i . "
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,2.1,Meta-learning for Bootstrapping,"We first estimate labels for all unlabeled data using the baseline model which is trained on the clean data, denoted as follows where f c (:; θ c ) denotes the trained model parameterized by θ c and k = 1, 2, ..., K. We further denote them as D n = {(x ñ j , y ñ j )} K j=1 . We then develop a novel meta-learning model for medical image segmentation, which learns from the clean set D c to bootstrap itself up by leveraging the learner's own predictions (i.e., pseudo labels), called Meta-Learning for Bootstrapping (MLB). As shown in Fig.  are the batch size respectively. Our objective is: where y p j is the pseudo label generated by f (x ñ j ; θ), L(•) is the cross-entropy loss function, C is the number of classes (we set C = 2 throughout this paper), w n j , w p j ∈ R H×W are the weight maps used for adjusting the contribution between the initialized and the pseudo labels in two different loss terms. • denotes the Hadamard product. We aim to solve for Eq. 2 following 3 steps: -Step 1: Update θt+1 based on S n and current weight map set. Following  where α represents the step size. -Step 2: Generate the meta-learned weight maps w n * , w p * based on S c and θt+1 by minimizing the standard cross-entropy loss in the meta-objective function over the clean training data: Note that here we restrict every element in w n/p to be non-negative to prevent potentially unstable training  where β stands for the step size and w n/pr,s j,t indicates the value at r th row, s th column of w n/p j at time t. Equation 7 is used to enforce all weights to be strictly non-negative. Then Eq. 8 is introduced to normalize the weights in a single training batch so that they sum up to one. Here, we add a small number to keep the denominator greater than 0. -Step 3: The meta-learned weight maps are used to spatially modulate the pixel-wise loss to update θ t+1 :"
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,2.2,Consistency-Based Pseudo Label Enhancement,"To generate more reliable pseudo labels, we propose Pseudo Label Enhancement (PLE) scheme based on consistency, which enforces consistency across augmented versions of the same input. Specifically, we perform Q augmentations on the same input image and enhance the pseudo label by averaging the outputs of the Q augmented versions and the original input: where f (x ñq j ; θ) is the output of q-th augmented sample, f (x ñ0 N +j ; θ) is the output of the original input, and τ -1 q means the corresponding inverse transformation of the q-th augmented sample. Meanwhile, to further increase the output consistency among all the augmented samples and original input, we introduce an additional consistency loss L Aug c to the learning objective: where (r, s) denotes the pixel index. τ q is the corresponding transformation to generate the q-th augmented sample. (q, v) denotes the pairwise combination among all augmented samples and the original input. The final loss is the mean square distance among all (Q+1)Q 2 pairs of combinations."
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,2.3,Mean Teacher for Stabilizing Meta-learned Weights,"Using PLE alone can result in performance degradation with increasing numbers of augmentations due to increased noise in weight maps. This instability can compound during subsequent training iterations. To address this issue during meta-learning, we propose using a mean teacher model  where N (μ, σ) ∈ R H×W denotes the Gaussian distribution with μ as mean and σ as standard deviation. And γ is used to control the noise level. The consistency loss is implemented based on pixel-wise mean squared error (MSE) loss: 3 Experiments"
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,3.1,Experimental Setup,"Datasets. We evaluate our proposed method on two different datasets including 1) the left atrial (LA) dataset from the 2018 Atrial Segmentation Challenge  Implementation Details. All of our experiments are based on 2D images. We adopt UNet++ as our baseline. Network parameters are optimized by SGD setting the learning rate at 0.005, momentum to be 0.9 and weight decay as 0.0005. The exponential moving average (EMA) decay rate is set as 0.99 following  For the label generation process, we first train with all clean labeled data for 30 epochs with batch size set as 16. We then use the latest model to generate labels for unlabeled data. Next, we train our MLB-Seg for 100 epochs."
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,3.2,Results Under Semi-supervision,To illustrate the effectiveness of MLB-Seg under semi-supervision. We compare our method with the baseline (UNet++  We then select the best checkpoint based on the validation set and report the results on the test set. As shown in Table 
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,3.3,Ablation Study,"To explore how different components of our MLB-Seg contribute to the final result, we conduct the following experiments under semi-supervision on PROMISE12: 1) the bootstrapping method  To demonstrate how PLE combined with the mean teacher model help stabilize the meta-weight update, we compare the performance of MLB + PLE (w/mean teacher) with MLB + PLE + mean teacher under different augmentations (Q) on PROMISE12 dataset (See supplementary materials for details). We find out that for MLB + PLE (w/o mean teacher), performance improves from 74.34% to 74.99% when Q is increased from 1 to 2, but decreases significantly when Q ≥ 4. Specifically, when Q reaches 4 and 6, the performance significant drops from 74.99% to 72.07% (Q = 4) and from 74.99% to 70.91% (Q = 6) respectively. We hypothesize that this is due to increased noise from initialized labels in some training samples, which can lead to instability in weight updates. To address this issue, we introduce the mean-teacher model  Qualitative Analysis. To illustrate the benefits of MLB-Seg for medical image segmentation, we provide a set of qualitative examples in Fig. "
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,4,Conclusion,"In this paper, we propose a novel meta-learning based segmentation method for medical image segmentation under semi-supervision. With few expert-level labels as guidance, our model bootstraps itself up by dynamically reweighting the contributions from initialized labels and its own outputs, thereby alleviating the negative effects of the erroneous voxels. In addition, we address an instability issue arising from the use of data augmentation by introducing a mean teacher model to stabilize the weights. Extensive experiments demonstrate the effectiveness and robustness of our method under semi-supervision. Notably, our approach achieves state-of-the-art results on both the LA and PROMISE12 benchmarks."
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,,Fig. 1 .,
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,,Fig. 2 .,
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,,Table 1 .,
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,,Table 2 .,
Consistency-Guided Meta-learning for Bootstrapping Semi-supervised Medical Image Segmentation,,Table 3 .,
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,1,Introduction,"Robust and accurate elongated physiological structure segmentation is crucial for computer-aided diagnosis and quantification of clinical parameters  Many researchers have tried to use uncertainty information to concentrate on the ambiguous region, and to evaluate the reliability of model's prediction. According to the source of prediction errors  This paper proposes a spatial and scale uncertainty-aware network (SSU-Net) for elongated physiological structure segmentation, which fully uses both spatial and scale uncertainty to highlight ambiguous regions and integrate hierarchical structure contexts. First, we use a gated soft uncertainty-aware (GSUA) module to adaptively highlight ambiguous areas based on spatial uncertainty maps. Second, we extract the uncertainty under different scales and propose the multi-scale uncertainty-aware (MSUA) fusion module to integrate hierarchical predictions for enhancing the final segmentation. Experiment results on segmentation tasks of the cornea endothelium and retinal vessel show the effectiveness of SSU-Net.   . The Bayesian approximate network has two outputs: segmentation prediction ŷ and the estimation of aleatoric uncertainty v. We can calculate the epistemic and aleatoric uncertainty maps, u e and u a , after multiple inferences. Furthermore, we consider the sigmoid probabilities of predictions under different scales as the second uncertainty source, and fuse the predictions {ŷ 1 , ŷ2 , ŷ3 } from multiple scales using the multi-scale uncertainty-aware (MSUA) module. ŷF is the final target output."
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,2.1,Spatial Uncertainty and Gated Soft Uncertainty-Aware Module,"Spatial Uncertainty. Since the epistemic and aleatoric uncertainty maps are used to find the hard-to-classify spatial areas in this work, we regard them as spatial uncertainty. Referring to  Gated Soft Uncertainty-Aware Module. To endow the uncertainty-aware module with adaptive adjustment ability, we propose the gated soft uncertaintyaware (GSUA) module, as illustrated in Fig.  where x i , x o ∈ R N ×c×h×w are the input and output features respectively; u = [u a , u e ] ∈ R N ×2×H×W is a tensor of uncertainty maps; ψ avg and ψ max represent average and max pooling; σ is the sigmoid function; g s denotes a convolution operation with Gaussian kernel and resizes the attention maps to the size of input features; is element-wise multiplication."
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,2.2,Scale Uncertainty and Multi-scale Uncertainty-Aware Module,"Scale Uncertainty. To integrate the predictions from hierarchical layers during model training, we capture the uncertainty under multiple scales. The sigmoid function is a simple and effective way to estimate uncertainty for the binary classification task. We extract the multi-scale uncertainty by Equation (3), where u s is the uncertainty map of prediction ỹs under scale s ∈ {1, 2, 3}. Multi-scale Uncertainty-Aware Module. With the uncertainty maps from different scales, all the hierarchical predictions {ỹ 1 , ỹ2 , ỹ3 } are fused by the MSUA module to generate the enhanced prediction ỹF , as illustrate in Fig.  The uncertainty map u s provides the classification confidence for each pixel. Therefore, we use u s to highlight the confident region of ỹs and further extract the max value across the different scales. The process is formulated by: where ỹF (i, j) denotes the pixel value at location (i, j) of enhanced prediction; y s (i, j) is the value of prediction under scale s ∈ {1, 2, 3}; and σ denotes the sigmoid operation of Eq. (3)."
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,2.3,Objective Function,"As shown in Fig.  where α 1 , α 2 , α 3 , and α F are the weight parameters for sub loss L s1 , L s2 , L s3 , and L F . In this experiment, we set all the weight parameters as 1. For these sub-losses, we adopt binary cross-entropy loss, as shown in Eq. (  where y is the ground truth; the positive class value of each pixel is 1; the negative class value is 0; and ỹ ∈ (0, 1) is the predicted probability value. 3 Experiment"
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,3.1,Dataset,"Two cornea endothelium microscope image datasets, TM-EM3000 and Rodrep, and one retinal fundus image dataset, FIVES, are used in this work. The private dataset TM-EM3000 contains 183 images measured by EM3000 specular microscope (Tomey Corporation, Japan). Following Ruggeri et al. "
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,3.2,Evaluation Metrics and Implementation Details,"There is a class imbalance between foreground and background pixels. To better evaluate the segmentation performance, we choose Dice score  ỹi F and the epistemic uncertainty"
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,3.3,Ablation Study,"We investigated the influence of GSUA and MSUA modules on TM-EM3000, as shown in Table "
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,3.4,Comparison with State-of-the-Art Methods,"To study the effectiveness of the proposed SSU-Net, we compared it with a series of state-of-the-art methods. On TM-EM3000 and Rodrep, we implemented several popular networks for comparison: UNet  As shown in Table "
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,4,Conclusion,"This paper proposes a spatial and scale uncertainty-aware network (SSU-Net) for elongated physiological structure segmentation. The ablation study shows the effectiveness of core components: the soft gated uncertainty-aware (GSUA) and the multi-scale uncertainty-aware (MSUA) fusion modules. Compared with some SOTA methods on cornea endothelial cell and retinal vessel image segmentation tasks, the proposed SSU-Net achieved the best segmentation performance and is more robust than other uncertainty-based methods. It is noteworthy that the SSU-Net performed considerably better than specialized retinal vessel segmentation networks. In the future, we plan to conduct experiments on various challenging situations to further explore the characteristics of SSU-Net."
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,,Fig. 1 .,
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,,Figure 1 (,
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,,Fig. 2 .Fig. 3 .,
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,,Table 1 .,
Elongated Physiological Structure Segmentation via Spatial and Scale Uncertainty-Aware Network,,Table 2 .,
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,1,Introduction,"Left Ventricular Hypertrophy (LVH), one of the leading predictors of adverse cardiovascular outcomes, is the condition where heart's mass abnormally increases secondary to anatomical changes in the Left Ventricle (LV)  It is difficult for such ML models to achieve high accuracy due to the sparsity of positive training signals (four or six) pertaining to the correct pixel locations. In an attempt to address this, previous works use 2D Gaussian distributions to smooth the ground truth landmarks of the LV  In this work, to address the challenge brought by sparse annotations and label smoothing, we propose a hierarchical framework based on Graph Neural Networks (GNNs)  Our contributions are summarized below. • We propose a novel GNN framework for LV landmark detection, performing message passing over hierarchical graphs constructed from an input echo; • We introduce a hierarchical supervision that is automatically induced from sparse annotations to alleviate the issue of label smoothing; • We evaluate our model on two LV landmark datasets and show that it not only achieves state-of-the-art mean absolute errors (MAEs) (1.46 mm and 1.86 mm across three LV measurements) but also outperforms other methods in out-of-distribution (OOD) testing (achieving 4.3 mm). "
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,2,Related Work,Various convolution-based LV landmark detection works have been proposed. Sofka et al.  Other related works focus on the detection of cephalometric landmarks from X-ray images. These works are highly transferable to the task of LV landmark detection as they must also detect a sparse number of landmarks. McCouat et al.  Our approach is different from prior works in that it aims to avoid the issue shown in Fig. 
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,3,Method,
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,3.1,Problem Setup,"We consider the following supervised setting for LV wall landmark detection. We have a dataset D = {X, Y }, where |D| = n is the number of {x i , y i } pairs such that x i ∈ X, y i ∈ Y , and i ∈ "
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,3.2,Model Overview,As shown in Fig. 
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,3.3,Hierarchical Graph Construction,"To learn representations that better capture the dependencies among pixels and patches, we introduce a hierarchical grid graph along with multi-level prediction tasks. As an example, the simplest task consists of a grid graph with only four nodes, where each node corresponds to four equally-sized patches in the original echo image. In the main task (the one that is at the bottom in Fig.  More formally, let us denote a graph as G = (V, E), where V is the set of nodes, and E is the set of edges in the graph such that if v i , v j ∈ V and there is an edge from v i to v j , then e i,j ∈ E. To build hierarchical task representations, for each image x ∈ X and the ground truth y ∈ Y , K different auxiliary graphs G k (V k , E k ) are constructed using the following steps for each k ∈ [1, K]: that the larger values of k correspond to graphs of finer resolution, while the smaller values of k correspond to coarser graphs. 2. Grid-like, undirected edges are added such that e m-1,q , e m+1,q , e m,q-1 , e m,q+1 ∈ E k for each m, q ∈ [1 . . . 2 k ] if these neighbouring nodes exist in the graph (border nodes will not have four neighbouring nodes). 3. A patch feature embedding z k j , where j ∈ [1 . . . 4 k ] is generated and associated with that patch (node) v j ∈ V k . The patch feature construction technique is described in Sect. 3.4. 4. Binary node labels ŷk ∈ {0, 1} 4 k ×4 are generated such that ŷkj = 1 if at least one of the ground truth landmarks in y is contained in the patch associated with node v j ∈ V k . Note that for each auxiliary graph, four different one-hot labels are predicted, which correspond to each of the four landmarks required to characterize LV measurements. The main graph, G main , has a grid structure and contains H × W nodes regardless of the value of K, where each node corresponds to a pixel in the image. Additionally, to allow the model to propagate information across levels, we add inter-graph edges such that each node in a graph is connected to four nodes in the corresponding region in the next finer graph as depicted in Fig. "
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,3.4,Node Feature Construction,"The graph representation described in Sect. 3.3 is not complete without proper node features, denoted by z ∈ R |V |×d , characterizing patches or pixels of the image. To achieve this, the grey-scale image is initially expanded in the channel dimension using a CNN. The features are then fed into a U-Net where the decoder part is used to obtain node features such that deeper layer embeddings correspond to the node features for the finer graphs. This means that the main pixel-level graph would have the features of the last layer of the network. A figure clarifying node feature construction is provided in the supp. material (Fig. "
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,3.5,Hierarchical Message Passing,"We now introduce how we perform message passing on our constructed hierarchical graph using GNNs to learn node representations for predicting landmarks. The whole hierarchical graph created for each sample, i.e., the main graph, auxiliary graphs, and cross-level edges, are collectively denoted as G i , where i ∈  where σ is the Sigmoid function, h l nodes ∈ R |V G i |×d is the set of d-dimensional embeddings for all nodes in the graph at layer l, and h out ∈ [0, 1] |V G i |×4 is the four-channel prediction for each node with each channel corresponding to a heatmap for each of the pixel landmarks. The initial node features h 1 nodes are set to the features z described in Sects. 3.3 and 3.4. The coordinates (x p out , y p out ) for each landmark location p ∈  where similar operations are performed in the y direction for y p out . Here, we vectorize the 2D heatmap into a single vector and then feed it to the softmax. loc x and loc y return the x and y positions of a node in the image. It must be noted that unlike some prior works such as Duffy et al. "
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,3.6,Training and Objective Functions,"To train the network, we leverage two types of objective functions. 1) Weighted Binary Cross Entropy (BCE): Since the number of landmark locations is much smaller than non-landmark locations, we use a weighted BCE loss; 2) L2 regression of landmark coordinates: We add a regression objective which is the L2 loss between the predicted coordinates and the ground truth labels."
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,4,Experiments,
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,4.1,Datasets,"Internal Dataset: Our private dataset contains 29,867 PLAX echo frames, split in a patient-exclusive manner with 23824, 3004, and 3039 frames for training, validation, and testing, respectively. External Dataset: The public Unity Imaging Collaborative (UIC) "
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,4.2,Implementation Details,"Our model creates K = 7 auxiliary graphs. For the node features, the initial single-layer CNN uses a kernel size of 3 and zero-padding to output features with a dimension of 224 × 224 × 4 (C = 4). The U-Net's encoder contains 7 layers with 128 × 128, 64 × 64, 32 × 32, 16 × 16, 8 × 8, 4 × 4, and 2 × 2 spatial dimensions, and 8, 16, 32, 64, 128, 256, and 512 number of channels, respectively. Three Graph Convolutional Network (GCN) "
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,4.3,Results,"We evaluate models using Mean Absolute Error (MAE) in mm, and Mean Percent Error (MPE) in percents, which is formulated as MPE = 100× |L pred -Ltrue| Ltrue , where L pred and L true are the prediction and ground truth values for every measurement. We also report the Success Detection Rate (SDR) for LVID for 2 and 6 mm thresholds. This rate shows the percentage of samples where the absolute error between ground truth and LVID predictions is below the specific threshold. These thresholds are chosen based on the healthy ranges for IVS (0.6-1.1cm), LVID (2.0-5.6cm), and LVPW (0.6-0.1cm). Hence, the 2 mm threshold provides a stringent evaluation of the models, while the 6 mm threshold facilitates the assessment of out-of-distribution performance. In-Distribution (ID) Quantitative Results. In Table  Out-of-Distribution (OOD) Quantitative Results. To investigate the generalization ability of our model compared to previous works, we train all models on the private dataset (which consists of a larger number of samples compared to UIC), and test the trained models on the public UIC dataset as shown in Table  Ablation Studies. In Table  We provide a qualitative view of the ablation study in supp. material (Fig. "
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,5,Conclusion and Future Work,"In this work, we introduce a novel hierarchical GNN for LV landmark detection. The model performs better than the state-of-the-art on most measurements without relying on label smoothing. We attribute this gain in performance to two main contributions. First, our choice of representing each frame with a hierarchical graph has facilitated direct interaction between pixels at differing scales. This approach is effective in capturing the nuanced dependencies amongst the landmarks, bolstering the model's performance. Secondly, the implementation of a multi-scale objective function as a supervisory mechanism has enabled the model to construct a superior inductive bias. This approach allows the model to leverage simpler tasks to optimize its performance in the more challenging pixel-level landmark detection task. For future work, we believe that the scalability of the framework for higherresolution images must be studied. Additionally, extension of the model to video data can be considered since the concept of intra-scale and inter-scale edges connecting nodes could be extrapolated to include temporal edges linking similar spatial locations across frames. Such an approach could greatly enhance the model's performance in unlabeled frames, mainly through the enforcement of consistency in predictions from frame to frame."
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,,Fig. 1 .,
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,,Fig. 2 .,
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,,Table 1 .,
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,,Table 2 .,
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,,.8 4.3 18.4 23.8 34.6 35.8 74.9Table 3 .,
EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 22.
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,1,Introduction,"Accurate segmentation of the cardiac structure from echocardiogram videos is integral to several analysis tasks  MIMTP  To address this limitation, as shown in Fig.  In addition to capturing multi-view information, we propose a novel dense cycle loss designed to utilize unlabelled video data for improved representation learning. Our motivation is based on the idea that standard multi-view data is obtained from the same patient and under the same stable conditions, without abnormal behaviours such as suffocating or exercising, ensuring consistent cardiac cycles. Previous work  -To the best of our knowledge, this is the first study to examine multi-view echocardiogram video segmentation. -Our proposed GL-Fusion uses a multi-view local-global fusion module to combine information from different views and improve the representation of each view. -We further design a dense cycle loss that utilizes unlabelled data to enforce feature similarity based on temporal cyclicality.  -Extensive experiments demonstrate our method improved performance over existing methods, achieving an average dice score of 0.81. We plan to make our code publicly available upon paper acceptance. "
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,2,Methodology,
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,2.1,The Overall Framework,", where X i ∈ R C×H×W ×T is the i-th view video and V is the number of views, and, C, H, W and T indicate the channels, height, width, and length of input images. Each video consists of T frames, i.e., X i = {x i t } T t=1 , where T remain the same for different view and x i t ∈ R C×H×W indicate t-th frame of i-th view video. Since only sparse frames are provided segmentation annotation for training in a video, thus we denote the annotation frame pair as {x i tn , y i tn } N n=1 , where t n is the index of the annotation and N is the number of labelled frames that N << T . During the training, We feed the videos V into the view-based encoder to extract the corresponding feature maps {F i } V i=1 of each view, where F i ∈ R D×h×w×T , and, D, h and w indicate the channel number, height and width of feature maps. Then the multi-view global-local fusion module aims to obtain the multi-view fused features {F i } V i=1 , which extract global and local semantics information from other views to enhance the representation of each view (See Sect. 2.2). Following is the view-based decoder that generates the predicted segmentation result y i from fused features, and maps the results to corresponding segmentation annotation, i.e., ŷi tn to the segmentation masks y i tn . For the annotated frames, we use the segmentation loss to supervise them, formulated as follows: where L bce is the Binary Cross Entropy. The sparse annotations are only a few frames in the whole video; thus can not obtain a robust model. To leverage a large number of unlabelled frames, we design the dense cycle loss L cyc to enforce temporal feature similarity of videos based on cyclicality; See Sect. 2.3. The overall loss function of our model is as follows: where α is the hyper-parameter to control the weight between two losses. In the following, we will illustrate the multi-view global-local fusion module and the dense cycle loss in detail."
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,2.2,Multi-view Global-Local Fusion Module,"In this section, we describe the multi-view global-local fusion module that aggregates the information from different views to enhance their feature representation. To this end, we first concatenate extracted feature {F i } V i=1 from different views in a view-wise manner to obtain F = {f t } T t=1 , where f t is the t-th feature vector in F, and f t ∈ R D×V ×w×h . Then, we describe the multi-view global and local fusion with F global and F local , respectively. Multi-view Global Fusion. In order to enhance the representation of each view, we propose the global-based fusion module (MGFM) to interact with the global semantics between different views. To this end, we introduce a view-wise non-local block, which extracts the context information across views. Similarly to the previous research  Multi-view Local Fusion. Since each view represents different morphological information of the heart and may contain the same cardiac structure as others, for example, the view PLVLA and LVSA both contain left ventricle(LA) and right ventricle(RV). Hence, extracting the local feature that represents the cardiac structure can contribute to feature fusion more efficiently. In this module, the extracted feature F local will first pass to both the view-based decoder and a center block, where the decoder and center block has the same components with different output. The decoder provides the pseudo label {ŷ i } V i=1 of different cardiac structures. A center block is introduced to acquire the weight {w i } V i=1 of {ŷ i } V i=1 and compute the local feature masks {M i } V i=1 as Eq. 3, where weight w has the greatest volume in the central area of the segmented regions and attenuation with distance, σ denotes the sigmoid function and M ∈ R 1×H×W ×T . These masks highlight features with a stronger intensity that are closer to the object center, while discarding background information that is farther away from the center. This selection is based on the understanding that morphological information should remain consistent closer to the center. In the final, similar to the process of MLFM, the view-wise local feature will be conducted view-wise concatenation operation and multiplied with local feature mask {M i } V i=1 . Then sent to the view-wise attention module to acquire the local fused feature F local ."
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,2.3,Dense Cycle Loss,"In echocardiogram videos, since only sparse annotation is available for the supervised training, involving the unlabelled data for our training and enhancing the performance is a challenge. The previous research  Thus, we propose the dense cycle loss, which considers all the possible matching across all template and search regions in each view independently. For the multi-view fused feature F global of each video will be separated to template region P i and search region Q i with a ratio in 2:3 according to total frame length T . Then we densely sample all feature intervals {p i 1 , ..., p i n } from P i and {q i 1 , ..., q i m } from Q i , respectively, both sampling use the same chunk size s and in our experiment, n and m is 2 5 × T s and 3 5 × T s . Then we compute the similarity between candidate interval p i k and target intervals q i j of Q i . where W(•) is the computation of the similarity matrix. The similarity will be used as the weight to reconstruct the feature interval pi k . Then we back to template region P i and compute the similarity between pi k and all feature intervals{p i 1 , ..., p i n } in P i . Then we consider the index of p i k as one-hot label g of the most similar interval of pi k and compute view-wise cycle loss L cyc with label g as shown in the following equation:"
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,3,Experiment,"Datasets. We collect a large multi-view echocardiogram video dataset named MvEVD from one medical institution, with a total of 254 sparsely annotated videos and 10 fully annotated videos with 800 × 600 resolution across three cardiac views (PLVLA, LVSA and A4C view). Each video includes 5 annotated frames. The average length of each video is larger than 100 frames that are able to cover more than one cardiac cycle. Implementation Details. We use the model DeeplabV3 "
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,3.1,Comparison with the State-of-the-Art Methods,"To evaluate the performance of our method, we do the comparison with two types of methods: single-view methods and fusion-based methods in Table "
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,3.2,Ablation Study,"In this section, we analyze the contribution to the performance of the proposed modules Multi-view Global Fusion Module (MGFM) and Multi-view Global Fusion Module (MGFM) of our framework. All results are illustrated in Table "
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,4,Conclusion,"In this paper, we propose a novel fusion framework called GL-Fusion, which jointly uses global and local information to enhance the segmentation performance of echocardiogram videos. Additionally, to ensure fair evaluation of the multi-view segmentation results, we introduce a multi-view echocardiogram video dataset called MvEVD, which provides full annotation for validating and testing performance. Our results demonstrate that the proposed GL-Fusion framework significantly outperforms other methods. In the future, we aim to further improve our method and make it more efficient."
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,,Fig. 1 .,
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,,Fig. 2 .,
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,,Figure 2,
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,,Fig. 3 .,
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,,MGFM MLFM Global Fusion Module View * N,
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,,Table 1 .,
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,,,
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,,Table 2 .,
GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation,,Table 3 .,
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,1,Introduction,"Accurate segmentation of brain tumors from MRI images is of great significance as it enables more accurate assessment of tumor morphology, size, location, and distribution range, thereby providing clinicians with a reliable basis for diagnosis and treatment  CNN-based networks, such as UNet  Considerable advancement has been achieved in the field of natural image segmentation by focusing on the edge information  In this paper, we propose an Edge-oriented transFormer (EoFormer), for efficient and accurate brain tumor segmentation. We design a CNN-Transformer based encoder for more effective feature representation, called Efficient Hybrid Encoder (EHE). Specifically, the input image is first processed by the CNN blocks to extract low-level local features. Then, the extracted features are fed into the transformer blocks to create long-range dependencies, resulting in the formation of high-level semantic features. In addition, to provide more accurate edge predictions, we design two edge sharpening modules in the decoder, called Edgeoriented Sobel (EoS) and Laplacian (EoL) modules. By implicitly embedding Sobel and Laplacian filters into the convolution layers to extract 1st-order and 2nd-order differential features, the two modules could enhance the edge information contained in the feature maps. In order to reduce the computational and memory complexity of the model, we replace the vanilla attention module with our extended efficient attention module "
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,2,Method,
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,2.1,Efficient Hybrid Encoder,"The EHE, shown in Fig.  where the TokenMixer i (•) corresponds to DWConv (i ∈ {0, 1}) and MSA (i ∈ {2, 3}), Norm( • ) represents layer normalization, and MLP(•) denotes the Multilayer Perceptron. Our approach combines the strengths of CNN and transformer to create a more powerful encoder that can extract both local and global information from input data. We address the computational and memory complexity issues that arise from 3D input by replacing the vanilla attention with our extended 3D efficient attention. Assuming the size of the input feature is n and the dimensionality is d, the input feature X ∈ R n×d pass through three linear layers to generate the queries and the efficient attention E(•) are computed as follows: where ρ(•) is the softmax activation function, T represents the matrix transpose operator. The efficient attention reduces the memory complexity and computational complexity of vanilla attention from O(n 2 ) and O(dn 2 ) to O(dn + d 2 ) and O(nd 2 ), where"
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,2.2,Edge-Oriented Transformer Decoder,"We design the EoFormer block (see Fig.  Edge-Oriented Sobel Module. We use a dual-branch structure, where the input feature X is simultaneously processed by two different branches. The first branch contains a 3 × 3 × 3 convolution that extracts basic features from the input. The second branch, which is responsible for edge extraction, first uses a C × C × 1 × 1 × 1 convolution to enhance the interaction between channel features of X, then utilizes a learnable scaled Sobel filter to extract the 1storder differentiation edge information from X. This filter is capable of detecting edges in three directions (i.e. horizontal, vertical, and orthogonal directions), so it comprises three filters M x , M y , and M z , each of which is represented by a 3 × 3 × 3 array. Take M x as an example, which is described as: We then apply a learnable scaling matrix S ∈ R C×1×1×1 to M x , which allows for dynamic adjustment of the scaling factor in each channel. The resulting feature extracted from the scaled Sobel-x filter is denoted as: where the '•' denotes channel-wise multiplication; the DWConv S•Mx indicates that DWConv(•) applies a S • M x learnable scaled filter as its kernel weight. Similarly, F y and F z are processed in the same way. The final output of the EoS module, denoted as F sob , is given by: Edge-Oriented Laplacian Module. Different from the Sobel filter that only extracts edges in the horizontal, vertical, and orthogonal directions, the Laplacian filter can extract edges in all directions. After extracting the 1st-order differentiation edge information, the intermediate features are then fed into the EoL module for extracting the 2nd-order differentiation edge information. Similarly, the feature F , obtained from the learnable scaling Laplacian filter, and the final output of the EoL module, denoted as F lap , are defined as: Re-parameterization of the Edge-Oriented Modules. We introduce the re-parameterization  where ' * ' represents the convolution operation, W conv means the weights of the convolution and B conv denotes the bias, and up(•) is the spatial broadcasting operation ,which upgrades the bias B ∈ R 1×C×1×1×1 into up(B) ∈ R 1×C×3×3×3 . In the inference stage, the output feature F is produced by a normal 3 × 3 × 3 convolution as follows: "
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,3,Experiment,
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,3.1,Dataset and Evaluation Metric,"In order to validate the performance of EoFormer, we conduct extensive experiments on both the publicly available BraTS 2020 dataset and a private medulloblastoma segmentation dataset (MedSeg). The BraTS 2020 dataset  The MedSeg dataset includes MRI images of T1, T1ce, T2, and T2 FLAIR modalities from 255 patients with medulloblastoma. The dataset includes manual annotations of the WT and ET regions. These annotated masks are reviewed by two experienced physicians to ensure the accuracy of the annotated results. The images are registered to the size of 24 × 256 × 256. The training/validation/test split ratio is 3:1:1. Four-fold cross-validation is performed on this dataset. "
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,3.2,Implementation Details,We implement EoFormer in Pytorch 1.11. Our model is trained from scratch for 300 epochs using two NVIDIA GTX 3090 GPUs. We select a combination of soft dice loss and cross-entropy as the loss function and utilize the AdamW optimizer 
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,3.3,Results,"We compare EoFormer with seven methods, including CNN-based methods (3D-UNet  Table "
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,3.4,Ablation,We evaluate the effectiveness of our proposed EoFormer framework by conducting ablation experiments on the BraTS 2020. In 
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,4,Conclusion,"In this paper, we propose the EoFormer, a novel approach for brain tumor segmentation. Our method comprises the Efficient Hybrid Encoder and the Edgeoriented Transformer Decoder. The encoder effectively extracts features from images by striking a balance between CNN and Transformer architectures. The decoder integrates the Sobel and Laplacian edge detection filters into our edgeoriented modules that enhance the extraction capability of edge and texture information. Besides, we introduce the efficient attention mechanism and the re-parameterization technology to improve the model efficiency. Our EoFormer outperforms other state-of-the-art methods on both BraTS 2020 and MedSeg. Our model is computationally efficient and can be readily applied to other 3D medical image segmentation tasks."
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,,Fig. 1 .,
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,,Figure 1 (,
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,,Fig. 2 .,
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,,Table 1 .,
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,,Table 2 .,
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,,Table 3 .,
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,,Table 3,
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,,Encoder and Bottleneck Design.,
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,1,Introduction,"Medical Image Analysis has greatly benefited from advances in AI  One important diagnosis task is to segment Lyme lesion, particularly the EM pattern, from benign skins. Such DL-assisted segmentation not only helps clinicians in pre-screening patients but also improves downstream tasks such as lesion classification. However, while Lyme disease lesion segmentation is intuitively simple, it is challenging due to the following reasons. First, there lacks of a well-segmented dataset with manual labels on Lyme disease. On one hand, some datasets-such as HAM10000  Second, the segmentation of Lyme lesion is itself challenging due to the nature of EM pattern. Specifically, a typical Lyme lesion exhibits a bull's eye pattern with one central redness and one outer circle, which is different from darkness lesion in cancer-related skin disease like melanoma. Furthermore, clinical data collected for training is usually imbalanced in some properties, e.g., more samples with light skins compared with dark skins. Therefore, existing skin disease segmentation  In this paper, we present the first Lyme disease dataset that contains labeled segmentation and skin tones. Our Lyme disease dataset contains two parts: (i) a classification dataset, composed of more than 3,000 diseased skin images that are either obtained from public resources or clinicians with patient-informed consent, and (ii) a segmentation dataset containing 185 samples that are manually annotated for three regions-i.e., background, skin (light vs. dark), and lesionconducted under clinician supervision and Institutional Review Boards (IRB) approval. Our dataset with manual labels is available at this URL  Secondly, we design a simple yet novel data preprocessing and alternation method, called EdgeMixup, to improve Lyme disease segmentation and diagnosis fairness on samples with different skin-tones. The key insight is to alter a skin image with a linear combination of the source image and a detected lesion boundary so that the lesion structure is preserved while minimizing skin tone information. Such an improvement is an iterative process that gradually improves lesion edge detection and segmentation fairness until convergence. Then, the detected, converged edge in the first step also helps classification of Lyme diseases via mixup with improved fairness. Our source code is available at this URL  We evaluate EdgeMixup for skin disease segmentation and classification tasks. Our results show that EdgeMixup is able to increase segmentation utility and improve fairness. We also show that the improved segmentation further improves classification fairness as well as joint fairness-utility metrics compared to existing debiasing methods, e.g., AD "
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,2,Motivation,"In this section, we motivate the design of EdgeMixup by showing that added lesion boundary helps a DL model focus more on the lesion part instead of other features such as skin or background. Note that not all skin disease datasets are carefully processed either due to the large amount of work required or the scarcity of data samples collected, e.g., SD-198 "
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,3,Method,"In this section, we first give the definition for model fairness, and we then describe the design of EdgeMixup for the purpose of de-biasing in Fig. "
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,and,
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,Initial Edge Detection:,"The purpose of initial detection, which is documented in the Initial_edge_detection function of Algorithm 1, is to provide a starting point, i.e., a rough boundary, for the next step of iterative improvement. The high-level idea is that EdgeMixup detects several edge candidates using the color range of ground-truth lesions in both Red-Green-Blue (RGB) and Hue-Saturation-Value (HSV) color space and then selects the target edge using a learning model based on the output confidence score. First, EdgeMixup trains a classification model based on a mixup of the ground-truth segmentation under clinician supervision and the original image (Line 7). Second, EdgeMixup generates many edge candidates. For example, EdgeMixup collects the mean range of lesion color from the training set and use the range as threshold to filter out any given sample for a candidate mask (Line 9). Lastly, EdgeMixup selects an edge candidate with the highest confidence score output by the learning model (Line 11) and returns it as the edge for this given sample. Note that the initial edge detection is irrelevant to the sample size of a particular subpopulation, thus improving the fairness. That is, even if the original dataset is imbalanced, as long as one sample from a subpopulation exists, the color range of the sample's lesion is considered in the initial detection. "
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,Iterative Edge,
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,9:,"Get all edge candidates {edge1, edge2, .., edgen} for each sample x 10: Mixup each edge candidate with x 11: Query mclass using all mixed-up {xedge 1 , ...xedge n } and choose the optimal edge edge opt"
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,12:,"Generate edged sample xedge = Mixup(x, edge opt , α) "
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,21:,while current_Jaccard > best_Jaccard do
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,22:,best_Jaccard = current_Jaccard
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,23:,"Predict lesion masks using miter, convert them to lesion edge edge"
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,24:,"Generate new training set for next model Mixup(Dtrain, edge, α)"
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,25:,Train a model for next iteration miter+1
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,26:,Evaluate miter+1 using edged D test edge and get current_Jaccard
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,27:,"iter += 1  affected area, further detection will refine and constrain the detected boundary. Besides, EdgeMixup calculates a linear combination of original image and lesion boundary, i.e., by assigning the weight of original image as α and lesion boundary as 1α. Figure "
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,4,Datasets,"We present two datasets: (i) a dataset collected and annotated by us (called Skin), and (ii) a subset of SD-198  Table "
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,5,Evaluation,"We implement EdgeMixup using python 3.8 and Pytorch, and all experiments are performed using one GeForce RTX 3090 graphics card (NVIDIA). Segmentation Evaluation. Our segmentation evaluation adopts four baselines, (i) a U-Net trained to segment skin lesions, (ii) a polar training  Table "
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,6,Related Work,Skin Disease Classification and Segmentation: Previous researches mainly work on improving model utility for both medical image  Bias Mitigation: Researchers have addressed bias and heterogeneity in deep learning models 
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,7,Conclusion,"We present a simple yet novel approach to segment Lyme disease lesion, which can be further used for disease classification. The key insight is a novel data preprocessing method that utilizes edge detection and mixup to isolate and highlight skin lesions and reduce bias. EdgeMixup outperforms SOTAs in terms of Jaccord index for segmentation and CAI α and CAUCI α for disease classification."
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,Fig. 1 .,
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,Fig. 2 .,
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,2 : 3 : 6 : 7 : 8 :,
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,30 :Fig. 3 .,
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,Table 1 .,
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,Table 2 .,
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,,Table 3 .,
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,1,Introduction,"With the introduction of Vision Transformers (ViTs), CNNs have been greatly challenged as seen with the leading performance in multiple volumetric data benchmarks, especially for medical image segmentation  In this work, we first derive and extend the theoretical equivalency of the weight optimization in the CSLA block. We observe that the kernel weight of each branch can be optimized with variable convergence using branch-specific learning rates. Furthermore, the ERF with SR is visualized to be more widely distributed from the center element to the global surroundings  -We propose RepUX-Net with better adaptation in large kernel convolution than 3D UX-Net, achieving SOTA performance in 3D segmentation. To our best knowledge, this is the first network that effectively leverages large kernel convolution with plain design in the encoder for 3D segmentation. -We propose a novel theory-inspired re-parameterization strategy to scale the element-wise learning convergence in large kernels with Bayesian prior knowledge. To our best knowledge, this is the first re-parameterization strategy to adapt 3D large kernels in the medical domain. -We leverage six challenging public datasets to evaluate RepUX-Net in 1) direct training and 2) transfer learning scenarios with 3D multi-organ seg-mentation. RepUX-Net achieves significant improvement consistently in both scenarios across all SOTA networks."
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,2,Related Works,"Weights Re-parameterization: SR is a methodology of equivalently converting model structures via transforming the parameters in kernel weights. For example, RepVGG demonstrates to construct one extra ResNet-style shortcut as a 1 × 1 convolution, parallel to 3 × 3 convolution during training "
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,3,Methods,Instead of changing the gradient dynamics during training 
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,3.1,Variable Learning Convergence in Multi-Branch Design,"From Fig.  Here, we only showcase the conclusion with two convolutions and two constant scalars as the scaling factors for simplicity. The complete proof of equivalency is demonstrated in Supplementary 1.1. Let {α L , α S } and {W L , W S } be the two constant scalars and two convolution kernels (Large & Small) respectively. Let X and Y be the input and output features, the CSLA block is formulated as , where denotes as convolution. For SO blocks, we train the plain structure parameterized by W and Y SO = X W . Let i be the number of training iterations, we ensure that SO , ∀i ≥ 0 and derive the stochastic gradient descent of parallel branches as follows: where L is the objective function; λ L and λ S are the Learning Rate (LR) of each branch respectively. We observe that the optimization of each branch can be different, which is feasible to control by adjusting the branch-specific LR. The locality convergence in large kernels enhance with the quick convergence in small kernels. Additionally from our experiments, a significant improvement is demonstrated with different branch-wise LR using SGD (Table "
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,3.2,Bayesian Frequency Re-parameterization (BFR),"With the visualization of ERF in RepLKNet  where k and c are the element and central index of the kernel weight, α is the hyperparameter to control the shape of the generated frequency distribution. Instead of adjusting the LR in parallel branches, we propose to re-parameterize the convolution weights by multiplying the scaling factor δ to each kernel element and apply a static LR λ for stochastic gradient descent in single operator setting as follows: With the multiplication with δ, each element in the kernel weight is rescaled with respect to the frequency level and allow to converge differently with a static LR in stochastic gradient descent. Such design demonstrates to influence the weighted convergence diffused from local to global in theory, thus tackling the limitation of enhancing the local convergence only in branch-wise setting."
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,3.3,Model Architecture,The backbone of RepUX-Net is based on 3D UX-Net  where ẑl and ẑl+1 are the outputs from the DWC layer in each depth level; BN denotes as the batch normalization layer.
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,4,Experimental Setup,"Datasets. We perform experiments on six public datasets for volumetric segmentation, which comprise with 1) Medical Segmentation Decathlon (MSD) spleen dataset  Implementation. We evaluate RepUX-Net with three different scenarios: 1) internal validation with direct supervised learning, 2) external validation with the unseen datasets, and 3) transfer learning with pretrained weights. All preprocessing and training details including baselines, are followed with "
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,5,Results,"Different Scenarios Evaluations. Table  With our designed convolutional blocks as the encoder backbone, RepUX-Net demonstrates the best performance across all segmentation task with significant improvement in Dice score (FLARE: 0.934 to 0.944, AMOS: 0.891 to 0.902). Furthermore, RepUX-Net demonstrates the best generalizability consistently with a significant boost in performance across 4 different external datasets (MSD: 0.926 to 0.932, KiTS: 0.836 to 0.847, LiTS: 0.939 to 0.949, TCIA: 0.750 to 0.779). For transfer learning scenario, the performance of RepUX-Net significantly outper-   "
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,,Ablation Studies with Block Designs & Optimizers.,"With the plain convolution design, a mean dice score of 0.906 is demonstrated with AdamW optimizer and perform slightly better than that with SGD. With the additional design of a parallel small kernel branch, the segmentation performance significantly improved (SGD: 0.898 to 0.917, AdamW: 0.906 to 0.929) with the optimized parallel branch LR using SR. The performance is further enhanced (SGD: 0.917 to 0.930, AdamW: 0.929 to 0.937) without being saturated with the increase of the training steps. By adapting BFR, the segmentation performance outperforms the parallel branch design significantly with a Dice score of 0.944. Effectiveness on Different Frequency Distribution. From Fig.  Limitations. The shape of the generated Bayesian distribution is fixed across all kernel weights with an unlearnable distance function. Each channel in kernels is expected to extract variable features with different distributions. Exploring different families of distributions to rescale the element-wise convergence in kernels will be our potential future direction."
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,6,Conclusion,"We introduce RepUX-Net, the first 3D CNN adapting extreme large kernel convolution in encoder network for medical image segmentation. We propose to model the spatial frequency in the human visual system as a reciprocal function, which generates a Bayesian prior to rescale the learning convergence of each element in kernel weights. By introducing the frequency-guided importance during training, RepUX-Net outperforms current SOTA networks on six challenging public datasets via both direct training and transfer learning scenarios."
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,,Fig. 1 .Fig. 2 .,
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,,Fig. 3 .,
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,,Table 1 .,
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,,Table 2 .,
Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation,,Table 3 .,
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,1,Introduction,"Semantic segmentation, i.e., assigning a semantic label to each pixel in an image, is a common task in medical computer vision nowadays typically performed by fully convolutional encoder-decoder deep neural networks (DNNs). These DNNs usually incorporate some kind of normalization layers which are thought to reduce the impact of the internal covariate shift (ICS)  Neural architecture search (NAS) is a strategy to tweak a neural architecture as such to discover efficient combinations of architectural building blocks for optimal performance on given datasets and tasks  In this study, we propose a novel evolutionary NAS approach to increase semantic segmentation performance by optimizing the spatiotemporal usage of normalization methods in a baseline U-Net  We evaluated the performance of evoNMS on eleven biomedical segmentation datasets and compared it with a state-of-the-art semantic segmentation method (nnU-Net "
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,2,Related Works,"To gain optimal performance in semantic segmentation tasks, it is important to optimize data preprocessing and architectural design. Popat et al.  Various studies show that neural networks (NNs) benefit from normalization to enhance task performance, generalizability, and convergence behavior. Zhou et al. "
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,3,Methods,"We investigated the impact of normalization on semantic segmentation using eleven different medical image datasets. Eight datasets were derived from the Medical Segmentation Decathlon (MSD)  We implemented our evolutionary optimization algorithm and the NN architectural design in TensorFlow 2.9.1/Keras 2.9.0 and executed our code on NVIDIA A100 GPUs. Each individual U-Net variant was trained for 20 epochs using the Adam optimizer, a constant learning rate of 1 × 10 -3 , and a batch size of 64. All segmentation tasks were optimized using the Dice Loss (DL). To evaluate the performance of the trained network, we calculated the Dice Coefficient (DC), Intersection over Union (IoU) of the fitted bounding boxes (BBIoU), and Hausdorff Distance with a percentile of 95% (HD95) of the validation set after  20 epochs. In addition, we included an early stopping criterion that is activated when the validation loss changes less than 0.1% to avoid unnecessary training time without further information gain. To compare our approach to state-of-theart segmentation networks, we considered nnU-Net  Our proposed evoNMS approach is based on evolutionary optimization with leaderboard selection and is executed for 20 generations. Each generation's population consists of 20 individuals, i.e., U-Net variants, meaning that we train 400 variants for one evoNMS execution (duration 4 h (polyp) to 5 days (glottal area) on one A100 GPU). The first generation contains individuals with random sequences drawn from our gene pool containing either a BN, IN, FRN, GN2, GN4 layer or skips normalization (no normalization, NoN). Other U-Netspecific hyperparameters, such as initial filter size and activation functions were set across datasets to a fixed value (initial filter size of 16, and ReLU as activa-tion function). In general, we kept all other hyperparameters fixed to focus only on the influence of normalization and infer whether it exhibits decoder/encoder dependence or even dependence on the underlying modality. After training each architecture, the fitness F i (Eq. (  where we compute the mean validation DC, validation IoU of the Bounding Box, and reciprocal of the validation HD95. We use the reciprocal of HD95 to balance the influence of each metric on the fitness value by a value ranging from 0 to 1. After each generation, the top ten individuals with the highest fitness F were bred. To breed a new individual, we selected two random individuals from the top ten candidates and combined them with a randomly selected crossing point across the normalization layer arrays of the two parental gene pools. Next, we applied mutations at a rate of 10%, which basically changes the normalization method of a random position to any normalization technique available in the gene pool. "
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,4,Results,"We first evaluated the influence of different normalization methods on medicalimage segmentation. We report the performance of neural architectures defined by our proposed evoNMS, which followed an evolutionary approach to determine the potentially best normalization method for each bottleneck layer, at generations 1 and 20. For each dataset, we evaluated the DC across different baselines of our default U-Net implementation with a fixed normalization method across layers (BN, IN, or NoN) and a state-of-the-art semantic segmentation network (nnU-Net) against our proposed evoNMS approach. Table  We next were interested in the optimization behavior of evoNMS. We found that random initialization of normalization methods yielded poorly and highly variant converging behavior overall (Supplementary Fig.  As we initialize the first population randomly, we determined whether the evoNMS algorithm converges to the same set of normalization methods for each dataset. We found for three independent evoNMS runs, that evoNMS converges for four out of eleven datasets on very similar patterns (Supplementary Fig.  We next identified the final distribution of normalization methods across the encoder and decoder U-Net layers to determine dataset-specific normalization. In Fig.  Finally, we investigated if any set of normalization methods can be derived by the imaging modality, such as endoscopy, CT and MRI. In Fig. "
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,5,Discussion and Conclusion,"Our approach shows that using normalization methods wisely has a powerful impact on biomedical image segmentation across a variety of datasets acquired using different imaging modalities. Due to its inherent property of always finding an optimal solution, evoNMS is potentially capable of providing the bestperforming set of normalization patterns for any given data set. For feasibility reasons, we considered only 20 epochs for each training set and 20 generations. However, we show that evoNMS with these constrained settings provides competitive results and outperforms or performs on par compared to all baselines. Our results suggest the superior performance of IN and GN (Fig. "
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,,Fig. 1 .,
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,,Fig. 2 .,
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,,Fig. 3 .,
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,,Fig. 4 .,
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,,Table 1 .,
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,,Table 2 .,
Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 67.
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,1,Introduction,"Medical Hyperspectral Imaging (MHSI) is an emerging imaging modality which acquires two-dimensional medical images across a wide range of electromagnetic spectrum. It brings opportunities for disease diagnosis, and computational pathology  High spatiospectral dimensions make it difficult to perform a thorough analysis of MHSI. In MHSIs, there exist two types of correlation. One is a spectral correlation in adjacent pixels. As shown in Fig.  In this paper, we consider treating spatiospectral dimensions separately and propose an effective and efficient dual-stream strategy to ""factor"" the architecture, by exploiting the correlation information of MHSIs. Our dual-stream strategy is designed based on 2D CNNs with U-shaped "
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,2,Methodology,"Mathematically, let Z ∈ R S×H×W denote the 3D volume of a pathology MHSI, where H × W is the spatial resolution, and S is the number of spectral bands. The goal of MHSI segmentation is to predict the per-pixel annotation mask , where Y i denotes the per-pixel groundtruth for MHSI Z i . The overall architecture of our proposed method is shown in Fig. "
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,2.1,Dual-Stream Architecture with SpatioSpectral Representation,"As mentioned above, for the spatial stream, we first reshape MHSI Z ∈ R S×H×W into Z spa ∈ R G×(S/G)×H×W , which has G spectral agents. Each spectral agent is treated as one sample. One sample contains highly correlated spectral bands, so that the spatial stream can focus on spatial feature extraction. For the spectral stream, to deal with problems of spatiospectral redundancy and the inefficiency of global spectral feature representation, we propose a novel and concise hierarchical structure shown in Fig.  where Z in ∈ R S×Cspe×H×W indicates the input spectral token tensor, and C spe is the spectral feature dimension. We introduce Depth-wise Conv (DwConv) for dynamically integrating redundant spatial information into spectral features to reduce spatial redundant noises, achieved by setting different strides of the convolutional kernel. Then, we represent long-distance dependencies among spectral inter-bands as a low-rank completion problem. SM D(•) indicates the spectral matrix decomposition operation. Concretely, we flatten feature map X to spectral sequence tokens X spe ∈ R H•W ×S×Cspe , which has S spectral tokens. We map X spe to a feature space using a linear transform W l ∈ R Cspe×C spe . We then apply a matrix decomposition method NMF (Non-negative Matrix Factorization)  In the framework shown in Fig. "
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,2.2,Low-Rank Decomposition and Skip Connection Ensemble,"The MHSI has low-rank priority due to redundancy, so we propose a lowrank decomposition module using Canonical-Polyadic (CP) decomposition  r is the tensor rank and λ i is a scaling factor. Recent research  where G c (•), G h (•) and G w (•) are the channel, height and width generators. Finally, we aggregate all rank-1 tensors (from A 1 to A r ) into the attention map along the channel dimension, followed by a linear layer used to reduce the feature dimension to obtain the low-rank feature U low : where is the element-wise product, and U low ∈ R C ×H ×W . We employ a straightforward non-parametric ensemble approach for grouping spectral agents. This approach involves multiple agents combining their features by averaging the vote. The encoders in the spatial stream produce 2D feature maps with G spectral agents, defined as F i ∈ R G×Ci×H/2 i ×W/2 i for the ith encoder, where G, C i , H/2 i , and W/2 i represent the spectral, channel, and two spatial dimensions, respectively. The ensemble is computed by represents the 2D feature map of the Gth agent. The ensemble operation aggregates spectral agents to produce a 2D feature map Table "
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,,SA Spectral,The feature maps obtained from the ensemble can be decoded using lightweight 2D decoders to generate segmentation masks. 3 Experimental Results
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,3.1,Experimental Setup,"We conducted experiments on the public Multi-Dimensional Choledoch (MDC) Dataset  We use data augmentation techniques such as rotation and flipping, and train with an Adam optimizer using a combination of dice loss and cross-entropy loss for 8 batch size and 100 epochs. The segmentation performance is evaluated using Dice-Sørensen coefficient (DSC), Intersection of Union (IoU), and Hausdorff Distance (HD) metrics, and Throughput (images/s) is reported for comparison. Pytorch framework and four NVIDIA GeForce RTX 3090 are used for implementation."
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,3.2,Evaluation of the Proposed Dual-Stream Strategy,Ablation Study. Our dual-stream strategy is plug-and-play. We first conduct an ablation study to show the effectiveness of each component. We use a dualstream strategy with RegNetX40 and U-Net architecture. As shown in Table  It is known that high feature redundancy limits the generalization of neural networks 
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,4,Conclusion,"In this paper, we present to factor space and spectrum for accurate and fast medical hyperspectral image segmentation. Our dual-stream strategy, leveraging low-rank prior of MHSIs, is computationally efficient and plug-and-play, which can be easily plugged into any 2D architecture. We evaluate our approach on two MHSI datasets. Experiments show significant performance improvements on different evaluation metrics, e.g., with our proposed strategy, we can obtain over 7.7% improvement in DSC compared with its 2D counterpart. After plugging our strategy into ResNet-34 backbone, we can achieve state-of-the-art MHSI segmentation accuracy with 3-13 times faster in terms of inference speed than existing 3D networks."
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,,Fig. 1 .,
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,,Fig. 2 .,
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,,Fig. 3 .,
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,,Table 2 .,
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,,Table 3 .,
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_15.
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,1,Introduction,"Charting brain development during the first postnatal year is crucial for identifying typical and atypical changes of brain tissues, i.e., grey matter (GM), white matter (WM), and cerebrospinal fluid (CSF), which can be utilized for diagnosis of autism and other brain disorders  The acquisition of a mass of tissue maps relies on automatic segmentation techniques. However, the accuracy cannot be guaranteed, and the most difficult case is the segmentation of the isointense phase (6-9 months) data  Conventional methods can mainly be classified into two categories: 1) the registration-based methods and 2) the learning-based methods, as introduced below. The registration-based methods usually utilize a single previous-defined atlas, for cross-sectional data, or a sequence of atlases, for longitudinal dataguided methods, to indirectly obtain the tissue maps  In this work, we propose a novel Transformer-based framework for isointense tissue segmentation via T1-weighted MR images, which is composed of two stages: i.e., i ) the semantics-preserved GAN (SPGAN), and ii ) Transformerbased multi-scale segmentation network (TMSN). Specifically, SPGAN is a bidirectional synthesis model that enables both the isointense data synthesis using adult-like data and vice verse. The isointense structural MRI data is paired with the segmented tissue maps for extending the training dataset. Additionally, the synthesized adult-like data from isointense infant data are adopted to assist segmentation in TMSN. TMSN incorporates a Transformer-based cross-branch fusing (TCF) module which exploits supplementary tissue information from a patch with a larger receptive field to guide the local segmentation. Extensive experiments are conducted on the public dataset, National Database for Autism Research (NDAR) "
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,2,Methods,"We propose a Transformer-based framework for isointense tissue segmentation, which includes two main modules: i ) the semantic-preserved GAN (SPGAN), and ii ) the Transformer-based multi-scale segmentation network (TMSN). An overview of the framework is provided in Fig. "
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,2.1,Semantics-Preserved Multi-phase Synthesis,"Motivated by the effectiveness of classical unpaired image-to-image translation models  where μ c and σ c denote the mean and standard deviation, respectively. Additionally, we adopt PatchGAN  Adult-Like Phase Synthesis. The generator G I2A is designed to synthesize adult-like infant brain images from isointense infant brain images, which is employed to provide clear structural information for identifying the ambiguous tissue boundaries in isointense brain images. To ensure the synthesized adult-like infant brain images can provide tissue information as realistic and accurate as the real images, we utilize a pre-trained adult-like brain tissue segmentation network S A (3D UNet) to preserve the structural similarity between the synthesized adult-like data and the real adult-like data and promote more reasonable anatomical structures in the synthesized images. To achieve that goal, during the training of SPGAN, we freeze the parameters of S A and adopt the mean square error (MSE) to penalize the dissimilarity between the tissue probability maps of the real and synthesized brain images (extracted by the pre-trained segmentation model S A ). The MSE loss is formulated as where I RAB and I CAB denote the real adult-like brain images and synthesized adult-like brain images synthesized by G I2A . The overall loss function of SPGAN is defined as: where L cycle denotes the cycle consistency loss between two generators."
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,2.2,Transformer-Based Multi-scale Segmentation,"The overview of our proposed segmentation network (TMSN) is shown in Fig.  where Q, K, and V denote the query, key, and value in the Transformer. We take a hybrid Dice and focal loss to supervise the two segmentation branches as follows: where I s and GT s , respectively, denote the input and ground truth (GT) of the top branch, and I b and GT b represent the ones of the bottom branch. The final tissue segmentation results are obtained from the top branch."
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,3,Experiments and Results,
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,3.1,Dataset and Evaluation Metrics,We evaluated our proposed framework for isointense infant brain tissue segmentation on the public dataset NDAR 
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,3.2,Implementation Details,Our proposed framework is implemented based on PyTorch 1.7.1 
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,3.3,Evaluation and Discussion,We conduct extensive experiments to evaluate the effectiveness of our infant brain tissue segmentation framework. Table 
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,,Effectiveness of Multi-phase Synthesis.,"In Table  Effectiveness of Multi-scale Assistance. In order to demonstrate the effectiveness of the additional branch in TMSN, which contains a larger receptive field, we show the corresponding results as one part of the ablation study in Table  Comparison with State-of-the-Arts (SOTAs). We conduct a comparative evaluation of our framework against several SOTA learning-based tissue segmentation networks, including 1) the 3D UNet segmentation network  To further illustrate the advanced performance of our framework, we provide a visual comparison of two typical cases in Fig. "
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,4,Conclusion,"In this study, we have presented a novel Transformer-based framework to segment tissues from isointense T1-weighted MR images. We designed two modules, i.e., i ) semantic-preserved GAN (SPGAN), and ii ) Transformer-based multiscale segmentation network (TMSN). SPGAN is designed to synthesize both isointense and adult-like data, which augments the dataset and provides supplementary tissue constraint for assisting isointense tissue segmentation. TMSN is used to segment tissues from isointense data under the guidance of the synthesized adult-like data. Their advantages are distinctive. For example, SPGAN overcomes the lack of training samples and synthesizes adult-like infant brain images with clear structural information for enhancing ambiguous tissue boundaries. TMSN exploits the pair of isointense and adult-like phase images, as well as the multi-scale scheme, to provide more information for achieving accurate segmentation performance. Extensive experiments demonstrate that our proposed framework outperforms the SOTA approaches, which shows the potential in early brain development or abnormal brain development studies."
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,,Fig. 1 .,
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,,Fig. 2 .,
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,,Fig. 3 .,
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,,Table 1 .,
Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation,,Table 2 .,
