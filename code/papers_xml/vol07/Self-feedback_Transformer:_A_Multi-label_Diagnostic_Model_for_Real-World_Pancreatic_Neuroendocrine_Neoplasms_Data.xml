<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data</title>
				<funder ref="#_5ajw4MY">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Medical AI Lab</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep3">Medical School</orgName>
								<orgName type="institution" key="instit1">Shenzhen University</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Medical AI Lab</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep3">Medical School</orgName>
								<orgName type="institution" key="instit1">Shenzhen University</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chenglang</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Medical AI Lab</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep3">Medical School</orgName>
								<orgName type="institution" key="instit1">Shenzhen University</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yangdi</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution" key="instit1">The First Affiliated Hospital</orgName>
								<orgName type="institution" key="instit2">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanji</forename><surname>Luo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution" key="instit1">The First Affiliated Hospital</orgName>
								<orgName type="institution" key="instit2">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Bingsheng</forename><surname>Huang</surname></persName>
							<email>huangb@szu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Medical AI Lab</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep3">Medical School</orgName>
								<orgName type="institution" key="instit1">Shenzhen University</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Medical AI Lab</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep3">Medical School</orgName>
								<orgName type="institution" key="instit1">Shenzhen University</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="521" to="530"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">B1607771E83F8A8EF9183E49F47DD07E</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_49</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computer Aided Diagnosis</term>
					<term>Real-world</term>
					<term>Multi-label</term>
					<term>Self-feedback Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CAD is an emerging field, but most models are not equipped to handle missing and noisy data in real-world medical scenarios, particularly in the case of rare tumors like pancreatic neuroendocrine neoplasms (pNENs). Multi-label models meet the needs of real-world study, but current methods do not consider the issue of missing and noisy labels. This study introduces a multi-label model called Self-feedback Transformer (SFT) that utilizes a transformer to model the relationships between labels and images, and uses a ingenious self-feedback strategy to improve label utilization. We evaluated SFT on 11 clinical tasks using a real-world dataset of pNENs and achieved higher performance than other state-ofthe-art multi-label models with mAUCs of 0.68 and 0.76 on internal and external datasets, respectively. Our model has four inference modes that utilize self-feedback and expert assistance to further increase mAUCs to 0.72 and 0.82 on internal and external datasets, respectively, while maintaining good performance even with input label noise ratios up to 40% in expert-assisted mode.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computer-aided Diagnosis (CAD) systems have achieved success in many clinical tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17]</ref>. Most CAD studies were developed on regular and selected datasets in the laboratory environment, which avoided the problems (data noise, missing data, etc.) in the clinical scenarios <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref>. In a real clinical scenario, the clinicians generally synthesize all aspects of information, and conduct consultations with Multidisciplinary Team (MDT), to accurately diagnose and plan the treatment <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref>. Real-world studies have received increasing attention <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>, and it is challenging for the CAD in the real-world scenarios as: 1) Consistent with the clinical workflow, CAD needs to consider multidisciplinary information to obtain multidimensional diagnosis; 2) Due to information collection, storage and manual evaluation, there are missing and noisy medical data. This phenomenon is especially common in rare tumors like pancreatic neuroendocrine neoplasms (pNENs).</p><p>In order to overcome above challenges, some studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18]</ref> used multilabel method because of the following advantages: 1) The input of the model is only a single modality such as images, which is easy to apply clinically; 2) The model learns multi-label and multi-disciplinary knowledge, which is consistent with clinical logic; 3) Multi-label simultaneous prediction, which meets the need of clinical multi-dimensional description of patients. For the above advantages, multi-label technology is suitable for real-world CAD. The previous multi-label CAD studies were designed based on simple parameter sharing methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref> or Graph Neural Network (GNN) method <ref type="bibr" target="#b1">[2]</ref>. The former implicitly interacts with multi-label information, making it difficult to fully utilize the correlation among labels; And the latter requires the use of word embeddings pre-trained on public databases, which is not friendly to many medical domain proper nouns. The generalizability of previous multi-label CAD studies is poor due to these disadvantages. In addition, none of the current multi-label CAD studies have considered the problem of missing labels and noisy labels.</p><p>Considering these real-world challenges, we propose a multi-label model named Self-feedback Transformer (SFT), and validate our method on a realworld pNENs dataset. The main contributions of this work are listed: 1) A transformer multi-label model based on self-feedback mechanism was proposed, which provided a novel method for multi-label tasks in real-world medical application; 2) The structure is flexibility and interactivity to meet the needs of realworld clinical application by using four inference modes, such as expert-machine combination mode, etc.; 3) SFT has good noise resistance, and can maintain good performance under noisy label input in expert-assisted mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Transformer has achieved success in many fields <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>. Inspired by DETR <ref type="bibr" target="#b0">[1]</ref> and C-Tran <ref type="bibr" target="#b7">[8]</ref>, we propose a multi-label model based on transformer and selffeedback mechanism. As shown in Fig. <ref type="figure" target="#fig_0">1,</ref><ref type="figure" target="#fig_0">1</ref>) image is embedded by Convolutional Neural Network (CNN) firstly; 2) then all labels are embedded and combined with their state embeddings; 3) finally, all embeddings are fed into a transformer, and the output label tokens are fed into Fully Connection (FC) layers for final predictions. Based on this network, we further introduce a self-feedback strategy, which allows the label information (including the missing labels) to be reused iteratively for enhancing the utilization of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformer-Based Multi-label Model</head><p>Image Embeddings F . Given input image x ∈ R L×W ×H , the feature vector k ∈ R C is extracted by a CNN after Global Average Pooling (GAP), where the output channel C = 256. Then k is split along the channel dimension into N (N = 8) sub-feature vectors F = {f 1 , f 2 , . . . , f N }, f i ∈ R d , d = C/N for tokenization. We choose 3D VGG8, a simple CNN with 8 convolution layers.</p><p>Label Embeddings L. In order to realize the information interaction among labels, and between labels and image features, we embed labels by an embedding layer. Each image x has M labels, and all labels are embedded into a vector set L = {l 1 , l 2 , . . . , l M }, l i ∈ R d by the learnable embedding layer of size d × M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft State Embeddings S.</head><p>There is a correlation between labels, e.g. the lesions with indistinct borders tend to be malignant. Therefore, we hypothesize that the states (GT values) of some labels can be a context for helping predict the remaining labels. We use a soft state embedding method. Specifically, we first embed the positive and negative states into s p and s n , both ∈ R d , and then the final state embedding s i is the weighted sum of s p and s n as shown in Equation <ref type="bibr" target="#b0">(1)</ref>. The state weight w p i and w n i is the true label value (eg. w p i = 1.0 when label is positive), where w p i + w n i = 1. For labels with continuous values such as age, the value normalized to 0 ∼ 1 is w p i . The s i is set as a zero vector for unknown label. l i = s i + l i is the final label embedding.</p><formula xml:id="formula_0">s i = w p i s p + w n i s n , if l i is known 0, if l i is unknown . (1)</formula><p>Multi-label Inference with Transformer Encoder. In a transformer, each output token is the integration of all input tokens. Taking advantage of this structure, we use a transformer encoder to integrate image embeddings and label embeddings, and used the output label tokens to predict label value. Specifically, embedding set</p><formula xml:id="formula_1">E = {f 1 , f 2 , • • • , f N , l 1 , l 2 , • • • , l M }</formula><p>are the input tokens, the attention value α and output token e are computed as follows:</p><formula xml:id="formula_2">α ij = sof tmax((W q e i ) T (W k e j )/ √ d), (<label>2</label></formula><formula xml:id="formula_3">) ēi = M j=1 α ij W v e j , (<label>3</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">e i = ReLU (ē i W r + b 1 ) W o + b 2 , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where e i is from E, W q , W k and W v are weight matrices of query, key and value, respectively, W r and W o are transformation matrices, and b 1 and b 2 are bias vectors. This update procedure is repeated for L layers, where the e i are fed to the successive transformer layer. Finally, all e i which are label output tokens are fed into M independent FC layers for predicting value of each label. The states of unknown labels cannot provide context, thus, the information interaction between known labels and unknown labels may be weaken. To overcome this problem, we propose a Self-feedback Strategy (SFS) inspired by Recurrent Neural Networks (RNN) to enhance the interaction of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-feedback Strategy</head><p>Training Progress and Loss Function. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, at time point t = 0, the state embedding is initialized to s GT i by Ground Truth (GT) value, and the initial label embedding l initial i is computed by</p><formula xml:id="formula_7">l initial i = s GT i + l i . The l initial i</formula><p>is combined with f i as the initial input, and then the output predicted value is converted into state embedding s 0 i by Equation <ref type="bibr" target="#b0">(1)</ref>. When t&gt;0, the label embedding l t i is updated iteratively by l t i = s t-1 i + l i , and then fed into the transformer T times. For classification and regression labels, we use focal crossentropy loss and L2 loss respectively, and use the method in <ref type="bibr" target="#b6">[7]</ref> to auto-weight the loss value of each label. The backpropagation of gradients and parameter updates are performed immediately after calculating the loss at each time point t. In the regular inference phase, the state of all labels is initialized as unknown.</p><p>Label Mask Strategy. To avoid predicting with labels' own input state, we use a Label Mask Strategy (LMS) during training phase to randomly mask a certain proportion a of known labels, which causes the labels' states to be embedded as zero vectors. Meanwhile, only the loss of the masked known label is calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Evaluation</head><p>Real-World pNENs Dataset. We validated our method on a real-world pNENs dataset from two centers. All patients with arterial phase Computed Tomography (CT) images were included. The dataset contained 264 and 28 patients in center 1 and center 2, and a senior radiologist annotated the bounding boxes for all 408 and 28 lesions. We extracted 37 labels from clinical reports, including survival, immunohistochemical (IHC), CT findings, etc. Among them, 1)RECIST drug response (RS), 2)tumor shrink (TS), 3)durable clinical benefit (DCB), 4)progression-free survival (PFS), 5)overall survival (OS), 6)grade (GD), 7)somatostatin receptor subtype 2(SSTR2), 8)Vascular Endothelial Growth Factor Receptor 2 (VEFGR2), 9)O6-methylguanine methyltransferase (MGMT), 10)metastatic foci (MTF), and 11)surgical recurrence (RT) are main tasks, and the remaining are auxiliary tasks. 143 and 28 lesions were segmented by radiologists, and the radiomics features of them were extracted, of which 162 features were selected and binarized as auxiliary tasks because of its statistically significant correlation with the main labels. The label distribution and the overlap ratio (Jaccard index) of lesions between pairs of labels are shown in Fig. <ref type="figure" target="#fig_2">3</ref>. It is obvious that the real-world dataset has a large number of labels with randomly missing data, thus, we used an adjusted 5-fold cross-validation. Taking a patient as a sample, we chose the dataset from center 1 as the internal dataset, of which the samples with most of the main labels were used as Dataset 1 (219 lesions) and was split into 5 folds, and the remaining samples are randomly divided into the training set Dataset 2 (138 lesions) and the validation set Dataset 3 (51 lesions), the training set and the validation set of the corresponding folds were added during cross-validation, respectively. All samples in Center 2 left as external test set. Details of each dataset are in the Supplementary Material. Dataset Evaluation Metrics. We evaluate the performance of our method on the 10 main tasks for internal dataset, and due to missing labels and too few SSTR2 labels, only the performance of predicting RT, PFS, OS, GD, MTF are evaluated for external dataset. We employ accuracy (ACC), sensitivity (SEN), specificity (SPC), F1-score (F1) and area under the receiver operating characteristic (AUC) for each task, and compute the mean value of them (e.g. mAUC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>The CT window width and window level were adjusted to 310 and 130 HU refer to <ref type="bibr" target="#b16">[17]</ref>, and the image inside the bounding box was cropped and scaled to 128×128×64 pixels. The numbers of convolutional kernels of VGG8 are <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">32,</ref><ref type="bibr">32,</ref><ref type="bibr">64,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">128,</ref><ref type="bibr">256]</ref>. Transformer encoder contained 2 layers and 8 heads. Layer normalization was used in transformer. The LMS a was set as 0.5, and the training feedback times T was 4. We used Adam optimiser, and used cosine annealing to reduce the learning rate from 1e-4 to 1e-12 over all 200 epochs. Our method was implemented in Pytorch, using an NVIDIA RTX TITAN GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison and Ablation Experiments</head><p>We compared our method with Single-task(ST), Parameters Sharing (PS), ML-Decoder <ref type="bibr" target="#b13">[14]</ref> and C-tran <ref type="bibr" target="#b7">[8]</ref>. Specifically, a ST model is trained using single label, and PS model uses a FC layer followed by the CNN to predict all labels. It should be noted that the CNN backbone of each method was replaced as 3D VGG8 to ensure fair comparison. In the ablation experiment, we removed the LMS and SFS to analyze their impact. The AUC of each main label is shown in Fig. <ref type="figure" target="#fig_3">4</ref>, and the average performance is shown in Table <ref type="table" target="#tab_0">1</ref>. It can be seen that multi-label models is better than that of ST due to using the relationship among labels, and SFT outperform other methods on most tasks and the average performance. The ablation experiments results showed that removing the LMS and SFS components causes performance degradation, indicating the necessity of them.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Performance of Different Inference Modes</head><p>As shown in the Fig. <ref type="figure" target="#fig_4">5</ref>, we designed 4 inference modes: 1) Regular, only input images; 2) Expert-assisted (EA), certain information is provided by clinicians; 3) Self-feedback (SF), iteratively inference T times by using prediction; 4) Expertmachine Combination (EMC), expert-assisted and self-feedback inference are both performed. Only the auxiliary labels states were input in EA mode. The results is shown in Table <ref type="table" target="#tab_1">2</ref>. Both SF and EA perform better than regular mode, and EMC outperforms other modes with a mAUC of 0.72 (0.82 on external dataset). We tested the SFT with and without SFS training under different feedback times T in SF mode, and results (Fig. <ref type="figure" target="#fig_5">6</ref>.) showed that the performance of SFT with SFS training increases gradually with the increase of T , while the SFT without SFS training has a general trend of decreasing performance after continuous iteration.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Analysis of Noise Resistance</head><p>To explore the noise resistance of SFT in EA mode, we selected 20, 40, 60, 80, and 100 percent of the known labels respectively, and further negated 0, 20, 40, 60, 80, 100 percent of the selected labels to simulate noisy labels. The way to negate a label is to change the label value x to 1x. As shown in Fig. <ref type="figure" target="#fig_6">7</ref>, as the noise ratio increases, the performance shows a decreasing trend, and the performance decreases slightly when the noise ratio ≤ 40 %. Finally, it can be observed that the SFT using the SFS training strategy is relatively less affected by noise. When using 100 percent labels, the mAUC of the internal dataset decreased from 0.71 (noise ratio = 0.0) to 0.53 (noise ratio = 1.0), a decrease of 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We proposed a novel model SFT for multi-label prediction on real-world pNENs data. The model integrates label and image informations based on a transformer encoder, and iteratively uses its own prediction based on a self-feedback mechanism to improve the utilization of missing labels and correlation among labels. Experiment results demonstrated our proposed model outperformed other multilabel models, showed flexibility by multiple inference modes, and had a certain ability to maintain performance when the input context noise was less than 40%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. SFT architecture and illustration of self-feedback strategy.</figDesc><graphic coords="3,63,96,54,11,276,19,153,22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of the self-feedback strategy.</figDesc><graphic coords="4,58,80,404,00,271,33,73,72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Label value distribution and overlap ratio of lesions between pairs of labels.</figDesc><graphic coords="6,64,47,57,38,294,28,83,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Predictive performance of different methods on the main tasks.</figDesc><graphic coords="7,70,41,216,41,292,00,51,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Different inference modes of SFT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The relationship between mAUC and the iterations number T in SF mode.</figDesc><graphic coords="8,64,98,260,33,267,28,60,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Performance of SFT under different number of labels and different noise ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>19 (0.26 on external dataset); the corresponding internal mAUC of the model without SFS training decreased by 0.21 (0.38 on external dataset). So SFS training can improve a certain anti-noise ability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results of the comparison experiments and the ablation experiments (%). SFT** means SFT w/o SFS and LMS, and SFT* means SFT w/o SFS. .41 71.75 64.48 60.82 74.19 76.12 73.50 79.21 68.53 SFT 67.76 66.53 69.51 73.62 61.51 75.50 78.08 73.00 80.71 71.33</figDesc><table><row><cell>Method</cell><cell>Internal Dataset (219+51 lesions)</cell><cell>External Dataset (28 lesions)</cell></row><row><cell></cell><cell cols="2">mAUC mACC mSEN mSPC mF1 mAUC mACC mSEN mSPC mF1</cell></row><row><cell>ST</cell><cell cols="2">62.56 63.76 64.36 70.09 54.77 64.47 66.98 70.00 70.68 59.57</cell></row><row><cell>PS</cell><cell cols="2">62.04 64.27 68.45 65.97 60.71 69.85 71.00 86.50 64.09 68.40</cell></row><row><cell cols="3">ML-Decoder 59.36 59.13 73.81 60.23 57.26 67.66 65.02 79.50 61.94 60.86</cell></row><row><cell>C-tran</cell><cell cols="2">62.53 63.05 60.23 73.11 56.14 70.96 80.05 63.50 86.59 66.54</cell></row><row><cell>SFT**</cell><cell cols="2">61.01 64.90 60.98 72.77 54.46 62.27 74.35 67.50 76.83 65.46</cell></row><row><cell>SFT*</cell><cell>64.39 64</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance of different inference modes by using SFT (%). Regular 67.76 66.53 69.51 73.62 61.51 75.50 78.08 73.00 80.71 71.33 SF (T = 1) 69.94 67.24 72.24 72.24 62.29 77.33 81.95 84.00 74.13 78.35 EA 71.26 68.40 68.42 77.58 62.88 81.30 82.29 66.00 94.44 74.44 EMC 72.22 71.26 66.87 81.77 63.65 82.45 83.01 70.00 93.81 75.24</figDesc><table><row><cell>Mode</cell><cell>Internal Dataset (219+51 lesions)</cell><cell>External Dataset (28 lesions)</cell></row><row><cell></cell><cell cols="2">mAUC mACC mSEN mSPC mF1 mAUC mACC mSEN mSPC mF1</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">81971684</rs>), <rs type="person">Marshall Lab</rs> of Biomedical Engineering open fund: Medical-Engineering Project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5ajw4MY">
					<idno type="grant-number">81971684</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_49.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-8_13" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Label co-occurrence learning with graph convolutional networks for multi-label chest x-ray image classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2292" to="2302" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning only by normal brain pet identify unheralded brain anomalies</title>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D N</forename><surname>Initiative</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EBioMedicine</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="447" to="453" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning for classification of bone lesions on routine MRI</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Eweje</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EBioMedicine</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">103402</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Development and validation of a deep learning CT signature to predict survival and chemotherapy benefit in gastric cancer: a multicenter, retrospective study</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Surg</title>
		<imprint>
			<biblScope unit="volume">274</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1153" to="e1161" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">General multi-label image classification with transformers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lanchantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16478" to="16488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Application of comprehensive artificial intelligence retinal expert (care) system: a national real-world evidence study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Digit. Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="486" to="e495" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">European cancer organisation essential requirements for quality cancer care (erqcc): pancreatic cancer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Partelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Treat. Rev</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">102208</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An overview of real-world data sources for oncology and considerations for research</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Penberthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA: Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="287" to="300" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning-based artificial intelligence model to assist thyroid nodule diagnosis and management: a multicentre diagnostic study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Digit. Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="250" to="e259" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting the early risk of chronic kidney disease in patients with diabetes using real-world data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravizza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="59" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12933</idno>
		<title level="m">ML-Decoder: scalable and versatile classification head</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An interpretable deep hierarchical semantic convolutional neural network for lung nodule malignancy classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Aberle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="84" to="95" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Real-world evidence-what is it and what can it tell us</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Sherman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Predicting the recurrence risk of pancreatic neuroendocrine neoplasms after radical resection using deep learning radiomics with preoperative computed tomography images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Transl. Med</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">833</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Trustworthy and intelligent COVID-19 diagnostic IoMT through XR and deep-learning-based clinic data access</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="15965" to="15976" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A novel interpretable computer-aided diagnosis system of thyroid nodules on ultrasound based on clinical experience</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="53223" to="53231" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
