<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer</title>
				<funder ref="#_hGX4BBK">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fan</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>BNRist, Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="456" to="465"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F17ADE01CD50241BFE0CF7C961934CDA</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_43</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dental mesh segmentation</term>
					<term>Graph-Transformer</term>
					<term>Adaptive feature aggregation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate segmentation of digital 3D dental mesh plays a crucial role in various specialized applications within oral medicine. While certain deep learning-based methods have been explored for dental mesh segmentation, the current quality of segmentation fails to meet clinical requirements. This limitation can be attributed to the complexity of tooth morphology and the ambiguity of gingival line. Further more, the semantic information of mesh cells which can provide valuable insights into their categories and enhance local geometric attributes is usually disregarded. Therefore, the segmentation of dental mesh presents a significant challenge in digital oral medicine. To better handle the issue, we propose a novel semantics-based feature learning for dental mesh segmentation that can fully leverage the semantic information to grasp the local and non-local dependencies more accurately through a well-designed graph-transformer. Moreover, we perform adaptive feature aggregation of cross-domain features to obtain high-quality cell-wise 3D dental mesh segmentation results. We validate our method using real 3D dental mesh, and the results demonstrate that our method outperforms the state-ofthe-art one-stage methods on 3D dental mesh segmentation. Our Codes are available at https://github.com/df-boy/SGTNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The 3D dental mesh segmentation is aimed to accurately separate the dental mesh into distinct components, namely individual teeth and gums. Therefore stable and accurate 3D dental mesh segmentation plays an essential role in various areas of oral medicine, including orthodontics and denture design, where precise tooth segmentation is of great importance for subsequent procedures and treatments. However, this task is accompanied by notable challenges arising from the inherent limitations in scan accuracy and the presence of considerable noise within the reconstructed 3D dental mesh, consequently leading to the blurring of tooth boundary.</p><p>To solve these challenges, some methods have been widely explored. Some conventional methods usually utilize specific geometric properties <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> like coordinates and normal vectors to perform threshold segmentation. These methods mainly use the pre-defined attributes, making it hard to achieve high-quality results through automated segmentation.</p><p>In recent years, many deep learning-based methods have been proposed to perform more accurate automated dental mesh segmentation. Some methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref> pack the point-wise features into 2D image-like inputs which are fed into a multi-layer CNN-like network and some other methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref> extend the point cloud feature learning frameworks to perform the segmentation. These methods tend to ignore the inherent topology of mesh and thus the quality of the segmentation is not good enough. Some methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref> design a two-stage network consisting of tooth centroid extraction and tooth segmentation. But these methods are not equally effective for models with crowding or missing teeth which are common in real scenes. And they need centroid labels which will incur additional computational cost. Some recent methods like TSGCNet <ref type="bibr" target="#b13">[14]</ref> design a two-stream graph convolution-based feature extraction network to extract the features of the C-stream (coordinate) and the N-stream (normal vector) separately and predict the cell/vertex-wise segmentation results according to the concatenated features of two streams. TSGCNet pioneered a new dental mesh feature process paradigm of decoupling the initial feature into C-stream (coordinate) and N-stream (normal vector), and used the graph convolution <ref type="bibr" target="#b10">[11]</ref> to consider the topological continuity when updating the features. However, these methods still suffer from the following deficiencies.</p><p>First of all, these methods still have some difficulty on the tooth boundary especially for the crowded teeth. Although there always exist individual differences in the shape of teeth from person to person, the teeth at different positions do have distinctive shape priors that distinguish them from other teeth, such as canine teeth and molar teeth. If we can make full use of the shape priors attached to the semantic information of the teeth, these segmentation errors on the boundaries can be decreased greatly. Thus it provides a more promising way to perform the segmentation guided by the semantic information. Secondly, these methods are often confused at the molars since they only use graph convolution to model the dependencies. Therefore a better alternative would be utilizing the local and non-local semantic information at the same time to further enhance the features, that means the long distance dependencies also need to be considered. Lastly, the existing methods always directly perform concatenation on the features from different angles, resulting in the incorrect segmentation on the misaligned teeth. This is due to the fact that the importance of features from different perspectives can vary a lot in different regions. Hence regressing a specific weight and fusing the features with these weight parameters can effectively eliminate the feature imbalance and pay more emphasis on salient features.</p><p>To address these issues, we propose a novel semantics-based feature learning network to fully utilize the semantic information and grasp the local and non-local dependencies. We first follow the TSGCNet <ref type="bibr" target="#b13">[14]</ref> to decouple the fea-tures into coordinate domain (C-domain) and normal domain (N-domain) which indicate the spatial and geometric features respectively. And then we design a multi-scale encoder network and at each scale we utilize a coarse classifier which accepts the adaptively fused features from the C-domain and the embedded Ndomain to predict a semantic pseudo label. Then with the semantic label we use the graph-transformer module to model the long distance dependencies in the neighbourhood of cells with the minimal semantic distance and perform feature aggregation according to the dependencies. Last but not least, we use the global graph-transformer module on the cross-domain features to further learn the semantic information and fuse them by adaptive feature fusion module before fed into the decoder.</p><p>To conclude, our contributions are three-fold: <ref type="bibr" target="#b0">(1)</ref> We propose a novel semantics-based feature learning which can fully utilize semantic information to enhance the local and global mesh features. <ref type="bibr" target="#b1">(2)</ref> We design a new feature fusion module that obtains global dependencies in C-domain and N-domain to further utilize the semantic information and adaptively fuses cross-domain features. (3) We compare with several recent methods on the real 3D dental mesh collected by the hospital. The OA (Overall Accuracy) and mIoU (mean Intersection over Union) both indicates that we perform superior performance on the 3D dental mesh segmentation task. Extensive evaluations prove that our method significantly outperforms the state-of-the-art one-stage methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview</head><p>Our network mainly consists of a semantics-based graph-transformer module and an adaptive cross-domain feature fusion module, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We follow the TSGCNet <ref type="bibr" target="#b13">[14]</ref> to fetch the initial cross-domain features as two N × 12 matrices, but our N-domain features serve as the embedding domain instead. The semantics-based graph-transformer is a multi-scale encoder and at each scale we aggregate the features in the neighbourhood of cells with minimal semantic distance provided by the coarse semantics prediction module. The N-domain features are embedded through the adaptive feature fusion module to extract more accurate semantic information. And for the concatenated features from the different scales, we perform the global graph-transformer block respectively in the two domains and fuse them through the same adaptive fusion strategy for the subsequent cell-wise segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semantics-Based Graph-Transformer</head><p>The semantics-based graph-transformer module aims to generate the multi-scale cell-wise feature vectors in C-domain embedded with N-domain which can represent the geometric features of dental mesh at different positions accurately and discriminatively. We denote the initial feature vectors extracted from the dental mesh as a N × 24 matrix, N is the number of the cells and the 24-dimensional vector consists of 12-dimensional relative coordinates and 12-dimensional normal vectors. Then through a normal STN module <ref type="bibr" target="#b2">[3]</ref>, we make the C-domain and Ndomain space invariant due to the fact that the position and orientation of dental mesh can be various. Formed as two domains of N × 12 matrices which represent the spatial position and the geometric features respectively, we perform semantic prediction on the C-domain with the N-domain features embedded to generate a pseudo semantic label L = {l 1 , l 2 , ..., l n } for each cell. And then according to the semantic information, we use the graph-transformer for each cell in its local neighbourhood where the cells have the minimal semantic differences and update their features to make the difference between the cells with different labels greater. For N-domain, in a geometric sense, they can help classify the semantic label and enhance the local features, hence we mainly upsample it to adapt to the different scales of C-domain without any other modifications.</p><p>Semantics Prediction. The semantic prediction is mainly used to generate a pseudo cell-wise label for each cell which can effectively extract the semantic information. Denote the C-domain feature as C that has a shape of N × k, and the N-domain feature as N that has a shape of N ×k, and we regress a C-domain weight and a N-domain weight which indicates the weights of the domain fusion. So we have the cross-domain features F adaptively fused as:</p><formula xml:id="formula_0">F j = c j • C j ⊕ n j • N j (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where ⊕ is the channel-wise concatenation, and j indicates the layer of the semantics-based graph-transformer modules, and c j , n j is the adaptive weights of C-domain and N-domain in layer j, and C j , N j is the output from the previous layer. And after that we perform a simple MLP to generate the final pseudo semantic cell-wise label formed as:</p><formula xml:id="formula_2">L j = max(softmax(MLP(F j )))<label>(2)</label></formula><p>where softmax can get the probability that the cells belong to each class and max outputs the index of the maximum value. Thus we have the cell-wise pseudo semantic label which can be used to make the difference between the features of cells belonging to different categories greater.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-Transformer.</head><p>The graph-transformer is composed of a semantic KNN and a Transformer Encoder Block. For the input C-domain, we first construct a KNN graph based on the semantic biased Euclidean distance formed as:</p><formula xml:id="formula_3">Distance(cell i , cell j ) = Euclidean(cell i , cell j ) + Semantic Dist(cell i , cell j )<label>(3)</label></formula><p>where cell i and cell j indicate the ith cell and jth cell and the Semantic Dist function measures the difference of the two cells which is formulated as:</p><formula xml:id="formula_4">Semantic Dist(cell i , cell j ) = 0, l i = l j λ, l i = l j (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where λ is a positive parameter that can be set according to the specific task and l i indicates the pseudo label of the ith cell. Then we perform a transformer encoder block on each cell to get the local dependencies which can enhance the local features belong to each class. The attention we used is a standard multihead attention, and we set the query and value as the matrices of neighbour features of the cells and the key as the distance matrix between cells and their neighbours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Adaptive Cross-Domain Feature Fusion</head><p>The adaptive cross-domain feature fusion module aims to fuse the C-domain and N-domain features for the cell-wise segmentation. Through the above semanticsbased graph-transformer module we have obtained the accurate multi-scale Cdomain features embedded by the N-domain features. Then we need to fuse the features to integrate the spatial information and the geometric information of the cells. Therefore, we first perform the concatenation on the features of different scales and use MLP to fuse the multi-scale features together which can balance the features at different scales. This process is formulated as:</p><formula xml:id="formula_6">F c = MLP(F 1 c ⊕ F 2 c ⊕ F 3 c ) F n = MLP(F 1 n ⊕ F 2 n ⊕ F 3 n ) (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where ⊕ is the channel-wise concatenation while c and n represent C-domain and N-domain features. Then through a global graph-transformer block, we just use the standard multi-head attention on the C-domain and N-domain respectively, so as to capture long distance dependencies and have global knowledge of the semantic information. Since the learnable weights can fuse the cross-domain features adaptively, we use the same cross-domain feature fusion strategy similar to that we used in the semantics prediction module. Further more, we use a single MLP to generate a feature mask and perform the dot product on the feature and the mask which is formulated as:</p><formula xml:id="formula_8">F = MLP(F ) F (6)</formula><p>where is the element-wise multiplication and F is the fused cross-domain features for segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>Our network is implemented with PyTorch 1.11.0 on four NVIDIA GeForce RTX 3090 GPUs. The input meshes are all downsampled to 12000 cells. And in the training process, we optimize the network through minimizing the cross-entropy loss which is very commonly used in the segmentation task. The learning rate was empirically set as 10 -3 , and reduced by 0.2 decay every 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset</head><p>Our dataset consists of 200 3D dental meshes obtained by an intraoral scanner on real orthodontic patients from hospital and each raw mesh contains even more than 150,000 cells, so we down sample the raw mesh to 12000 cells while preserving the surface topology. We randomly split the whole dataset as a training set with 160 meshes and a testing set with 40 meshes. And we segment the raw mesh into 14 teeth and gums, following the FDI World Dental Federation notation <ref type="bibr" target="#b0">[1]</ref>. This means that each input mesh has 15 labels. For convenience, we do not distinguish between the maxillary teeth and mandibular teeth, and treat the teeth on the opposite side (from maxillary and mandibular teeth respectively) as the same class. And we augment the training set by the random translation between <ref type="bibr">[-10, 10]</ref>, the random rotation between [-π, π] and the random scaling between [0.8, 1.2].</p><p>For evaluation, overall accuracy (OA) and mean intersection over union (mIoU) are adopted for quantitative comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparing with SOTA Methods</head><p>We compare our network against five recent methods, including PointNet++ <ref type="bibr" target="#b6">[7]</ref>, DGCNN <ref type="bibr" target="#b10">[11]</ref>, PVCNN <ref type="bibr" target="#b5">[6]</ref>, MeshSegNet <ref type="bibr" target="#b4">[5]</ref> and TSGCNet <ref type="bibr" target="#b13">[14]</ref>. For a fair comparison, we utilize the public implementations of compared methods to fine-tune their network for generating their best segmentation results. All the methods are trained for 200 epochs.</p><p>The segmentation results are shown in Table <ref type="table" target="#tab_0">1</ref>. All the metrics show that our method outperforms the other methods a lot. Specifically, the TSGCNet <ref type="bibr" target="#b13">[14]</ref> is the state-of-the-art one-stage method of the 3D dental mesh segmentation task which pioneered a two-stream graph convolution network to extract the features more accurately and TSGCNet <ref type="bibr" target="#b13">[14]</ref> improve the segmentation performance on the teeth greatly compared to MeshSegNet <ref type="bibr" target="#b4">[5]</ref>. Compared with TSGCNet <ref type="bibr" target="#b13">[14]</ref>, we still increase the OA and m-IoU on the 3D dental mesh segmentation task by 1.34% and 3.1% respectively. We also perform the qualitative experiments to further evaluate the segmentation results in Fig. <ref type="figure" target="#fig_1">2</ref>. From the visualization results, we can find out that our method also outperforms other methods. In particular, we present some complicated cases where there may exist wisdom teeth, the crowded arrangement or the misplaced teeth. In the first row, the raw mesh has a total of 16 teeth which is different from our setting, and in this case, other methods tend to merge some of the small teeth which will cause confusion, while with semantic information, our method successfully labels all the teeth except the two wisdom teeth. In the second and third row where there exist misplaced teeth and worn teeth, we can see that the previous methods cannot perform segmentation correctly. But with adaptive feature fusion which balances the coordinate features and normal vector features, our method can segment the worn teeth accurately as well as the misplaced teeth guided by the semantic information. And the fourth row demonstrates that in terms of extracting the local geometric features, we can also achieve the superior performance. This qualitative experiment further suggests that our design is more effective and accurate on this segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>We evaluate the effectiveness of semantics prediction, graph-transformer and adaptive feature fusion as three critical components of our method. We perform the evaluation by excluding one of these critical components each time. Specifically, when we remove the semantics prediction module, we will use a naive KNN to construct the KNN graph. When we remove the graph-transformer module, we replace it with max-pooling layers. In the absence of the adaptive feature fusion module, we directly concatenate the features from C-domain and N-domain for cross-domain feature fusion. The results of the ablation study are presented in Table <ref type="table" target="#tab_1">2</ref>. It turns out that semantics prediction, graph-transformer and adaptive feature fusion all bring performance improvement on the 3D den- tal mesh segmentation task. And we can see that although semantics prediction module improves a little in metrics, it is primarily attributed to the fact that the segmentation errors focus on the boundary cells and the number of such errors is relatively small. But the impact of these boundary cells on the overall segmentation quality is significant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a novel semantics-based feature learning to make full use of local and unlocal semantic information to enhance the features extracted. This architecture can decouple the spatial and geometric features into C-domain and Ndomain and embed the N-domain into C-domain to further utilize the semantic pseudo label to perform the local graph-transformer module. Lastly we fuse the features from spatial and geometric domain adaptively by using the global graph-transformer module and adaptive feature fusion module. The effectiveness of our proposed method is evaluated on the real dental mesh from real orthodontic patients. In the future work, we will try to utilize the feature decoupling and fusing strategy in other segmentation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of the proposed framework.</figDesc><graphic coords="4,56,46,53,75,339,64,142,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of segmentation results comparing with four methods along with the raw dental meshes and ground-truth labels.</figDesc><graphic coords="8,56,46,53,90,339,52,149,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The segmentation results comparing with five methods on OA and mIoU. Gum-mIoU and teeth-mIoU indicates the mIoU computed on the gums and teeth respectively.</figDesc><table><row><cell>Method</cell><cell>OA</cell><cell cols="2">mIoU gum-mIoU teeth-mIoU</cell></row><row><cell cols="3">Point-based PointNet++ [7] 87.18 0.708 0.867</cell><cell>0.697</cell></row><row><cell>DGCNN [11]</cell><cell cols="2">91.56 0.793 0.927</cell><cell>0.784</cell></row><row><cell cols="3">MeshSegNet [5] 93.15 0.825 0.931</cell><cell>0.816</cell></row><row><cell>Voxel-based PVCNN [6]</cell><cell cols="2">94.54 0.849 0.934</cell><cell>0.843</cell></row><row><cell cols="3">Graph-based TSGCNet [14] 95.63 0.890 0.942</cell><cell>0.886</cell></row><row><cell>Ours</cell><cell cols="2">96.97 0.921 0.956</cell><cell>0.918</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of critical components of our method.</figDesc><table><row><cell>Method</cell><cell>OA</cell><cell>mIoU</cell></row><row><cell>Ours</cell><cell cols="2">96.97 0.921</cell></row><row><cell>Ours w/o semantics prediction</cell><cell cols="2">96.52 0.916</cell></row><row><cell>Ours w/o graph-transformer</cell><cell cols="2">96.18 0.911</cell></row><row><cell cols="3">Ours w/o adaptive feature fusion 96.05 0.909</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This research was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">61972221</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hGX4BBK">
					<idno type="grant-number">61972221</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Iso</surname></persName>
		</author>
		<title level="m">Dentistry-designation system for teeth and areas of the oral cavity</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">3950</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TSegNet: an efficient and accurate tooth segmentation network on 3D dental model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101949</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MeshSNet: deep multi-scale mesh feature learning for end-to-end tooth labeling on 3D dental surfaces</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_93</idno>
		<idno>978-3-030-32226-7 93</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="837" to="845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep multi-scale mesh feature learning for automated labeling of raw dental surfaces from 3D intraoral scanners</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2440" to="2450" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Point-voxel CNN for efficient 3D deep learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PointNet++: deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DArch: dental arch priorassisted 3D tooth instance segmentation with weak annotations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20752" to="20761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic tooth segmentation and dense correspondence of 3D dental model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_68</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59719-168" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="703" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic classification and segmentation of teeth on 3D dental model using hierarchical deep learning networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="84817" to="84828" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D tooth segmentation and labeling using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2336" to="2348" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning approach to semantic segmentation in 3D point cloud intra-oral scans of teeth</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Zanjani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="557" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TSGCNet: discriminative geometric feature learning with twostream graph convolutional network for 3D dental model segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6699" to="6708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Interactive tooth segmentation of dental models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Engineering in Medicine and Biology 27th Annual Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2006</date>
			<biblScope unit="page" from="654" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interactive tooth partition of dental mesh base on tooth-target harmonic field</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="132" to="144" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
