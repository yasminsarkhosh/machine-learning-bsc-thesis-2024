<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning</title>
				<funder>
					<orgName type="full">Chinese Scholarship Council scholarship</orgName>
					<orgName type="abbreviated">CSC</orgName>
				</funder>
				<funder ref="#_ETwAk2x">
					<orgName type="full">Science and Technology Development Fund of Macau SAR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<postCode>1066 CX</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">GROW School for Oncology and Development Biology</orgName>
								<orgName type="institution">Maastricht University</orgName>
								<address>
									<postCode>6200 MD</postCode>
									<settlement>Maastricht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Tao</forename><surname>Tan</surname></persName>
							<email>taotanjs@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<postCode>1066 CX</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Applied Sciences</orgName>
								<orgName type="institution">Macao Polytechnic University</orgName>
								<address>
									<postCode>999078</postCode>
									<settlement>Macao</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<postCode>1066 CX</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">GROW School for Oncology and Development Biology</orgName>
								<orgName type="institution">Maastricht University</orgName>
								<address>
									<postCode>6200 MD</postCode>
									<settlement>Maastricht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luyi</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<postCode>1066 CX</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Radiology and Nuclear Medicine</orgName>
								<orgName type="institution">Radboud University Medical Centre</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<postCode>1066 CX</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">GROW School for Oncology and Development Biology</orgName>
								<orgName type="institution">Maastricht University</orgName>
								<address>
									<postCode>6200 MD</postCode>
									<settlement>Maastricht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Radiology and Nuclear Medicine</orgName>
								<orgName type="institution">Radboud University Medical Centre</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunyao</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Regina</forename><surname>Beets-Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<postCode>1066 CX</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">GROW School for Oncology and Development Biology</orgName>
								<orgName type="institution">Maastricht University</orgName>
								<address>
									<postCode>6200 MD</postCode>
									<settlement>Maastricht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Radiology and Nuclear Medicine</orgName>
								<orgName type="institution">Radboud University Medical Centre</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruisheng</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<postCode>1066 CX</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Erasmus Medical Center</orgName>
								<orgName type="institution">Erasmus University</orgName>
								<address>
									<postCode>3015 GD</postCode>
									<settlement>Rotterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ritse</forename><surname>Mann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Netherlands Cancer Institute (NKI)</orgName>
								<address>
									<postCode>1066 CX</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Radiology and Nuclear Medicine</orgName>
								<orgName type="institution">Radboud University Medical Centre</orgName>
								<address>
									<settlement>Nijmegen</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="57" to="67"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">312BD14A580972A7F42B54146638591E</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bilateral mammogram</term>
					<term>Asymmetric transformer</term>
					<term>Disentanglement</term>
					<term>Self-adversarial learning</term>
					<term>Synthesis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Asymmetry is a crucial characteristic of bilateral mammograms (Bi-MG) when abnormalities are developing. It is widely utilized by radiologists for diagnosis. The question of "what the symmetrical Bi-MG would look like when the asymmetrical abnormalities have been removed ?" has not yet received strong attention in the development of algorithms on mammograms. Addressing this question could provide valuable insights into mammographic anatomy and aid in diagnostic interpretation. Hence, we propose a novel framework, DisAsym-Net, which utilizes asymmetrical abnormality transformer guided selfadversarial learning for disentangling abnormalities and symmetric Bi-MG. At the same time, our proposed method is partially guided by randomly synthesized abnormalities. We conduct experiments on three public and one in-house dataset, and demonstrate that our method outperforms existing methods in abnormality classification, segmentation, and localization tasks. Additionally, reconstructed normal mammograms can provide insights toward better interpretable visual cues for clinical diagnosis. The code will be accessible to the public.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Breast cancer (BC) is the most common cancer in women and incidence is increasing <ref type="bibr" target="#b14">[14]</ref>. With the wide adoption of population-based mammography screening programs for early detection of BC, millions of mammograms are conducted annually worldwide <ref type="bibr" target="#b23">[23]</ref>. Developing artificial intelligence (AI) for abnormality detection is of great significance for reducing the workload of radiologists and facilitating early diagnosis <ref type="bibr" target="#b21">[21]</ref>. Besides using the data-driven manner, to achieve accurate diagnosis and interpretation of the AI-assisted system output, it is essential to consider mammogram domain knowledge in a model-driven fashion.</p><p>Authenticated by the BI-RADS lexicon <ref type="bibr" target="#b12">[12]</ref>, the asymmetry of bilateral breasts is a crucial clinical factor for identifying abnormalities. In clinical practice, radiologists typically compare the bilateral craniocaudal (CC) and mediolateral oblique (MLO) projections and seek the asymmetry between the right and left views. Notably, the right and the left view would not have pixel-level symmetry differences in imaging positions for each breast and biological variations between the two views. Leveraging bilateral mammograms (Bi-MG) is one of the key steps to detect asymmetrical abnormalities, especially for subtle and non-typical abnormalities. To mimic the process of radiologists, previous studies only extracted simple features from the two breasts and used fusion techniques to perform the classification <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25]</ref>. Besides these simple feature-fusion methods, recent studies have demonstrated the powerful ability of transformerbased methods to fuse information in multi-view (MV) analysis (CC and MLO view of unilateral breasts) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">16,</ref><ref type="bibr" target="#b26">26]</ref>. However, most of these studies formulate the diagnosis as an MV analysis problem without dedicated comparisons between the two breasts.</p><p>The question of "what the Bi-MG would look like if they were symmetric?" is often considered when radiologists determine the symmetry of Bi-MG. It can provide valuable diagnostic information and guide the model in learning the diagnostic process akin to that of a human radiologist. Recently, two studies explored generating healthy latent features of target mammograms by referencing contralateral mammograms, achieving state-of-the-art (SOTA) classification performance <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref>. None of these studies is able to reconstruct a normal pixellevel symmetric breast in the model design. Image generation techniques for generating symmetric Bi-MG have not yet been investigated. Visually, the remaining parts after the elimination of asymmetrical abnormalities are the appearance of symmetric Bi-MG. A more interpretable and pristine strategy is disentanglement learning <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b17">17]</ref> which utilizes synthetic images to supervise the model in separating asymmetric anomalies from normal regions at the image level.</p><p>In this work, we present a novel end-to-end framework, DisAsymNet, which consists of an asymmetric transformer-based classification (AsyC) module and an asymmetric abnormality disentanglement (AsyD) module. The AsyC emulates the radiologist's analysis process of checking unilateral and comparing Bi-MG for abnormalities classifying. The AsyD simulates the process of disentangling the abnormalities and normal glands on pixel-level. Additionally, we leverage a self-adversarial learning scheme to reinforce two modules' capacity, where the feedback from the AsyC is used to guide the AsyD's disentangling, and the AsyD's output is used to refine the AsyC in detecting subtle abnormalities. To </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this study, the "asymmetric" refers to the visual differences on perception level that can arise between the left and right breasts due to any abnormality, including both benign and malignant lesions. Thus, a paired Bi-MG is considered symmetrical only if both sides are normal and the task is different from the malignancy classification study <ref type="bibr" target="#b13">[13]</ref>. The paired Bi-MG of the same projection is required, which can be formulated as I = {x r , x l , y asy , y r , y l }. Here, x ∈ R H×W represents a mammogram with the size of H × W , x r and x l correspond to the right and left view respectively. y r , y l , y asy ∈ {0, 1} are binary labels, indicating abnormality for each side, and the asymmetry of paired Bi-MG. The framework of our DisAsymNet is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Specifically, the AsyC module takes a pair of Bi-MG as input and predicts if it is asymmetric and if any side is abnormal. We employ an online Class Activation Mapping (CAM) module <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref> to generate heatmaps for segmentation and localization. Subsequently, the AsyD module disentangles the abnormality from the normal part of the Bi-MG through the self-adversarial learning and Synthesis method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Asymmetric Transformer-Based Classification Module</head><p>The AsyC module consists of shared encoders ψ e and asymmetric transformer layers ψ asyt to extract features and learn bilateral-view representations from the paired mammograms. In this part, we first extract the starting features f of each side (f r , f l represent the right and left features respectively) through ψ e in the latent space for left-right inspection and comparison, which can be denoted as f = ψ e (x). Then the features are fed into the ψ asyt .</p><p>Unlike other MV transformer methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">16]</ref> that use only cross-attention (CA), our asymmetric transformer employs self-attention (SA) and CA in parallel to aggregate information from both self and contralateral sides to enhance the side-by-side comparison. This is motivated by the fact that radiologists commonly combine unilateral (identifying focal suspicious regions according to texture, shape, and margin) and bilateral analyses (comparing them with symmetric regions in the contralateral breasts) to detect abnormalities in mammography <ref type="bibr" target="#b6">[6]</ref>. As shown in the right of Fig. <ref type="figure" target="#fig_0">1</ref>, starting features f are transformed into query (f Q ), key (f K ), and value (f V ) vectors through feed-forward network (FFN) layers. The SA and CA modules use multi-head attention (MHA),</p><formula xml:id="formula_0">ψ h=8 mha (f Q , f K , f V )</formula><p>with the number of heads h = 8, which is a standard component in transformers and has already gained popularity in medical image fields <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">16,</ref><ref type="bibr" target="#b26">26]</ref>. In the SA, the query, key, and value vectors are from the same features,</p><formula xml:id="formula_1">f SA = ψ h=8 mha (f Q , f K , f V ).</formula><p>While in the CA, we replace the key and value vectors with those from the contralateral features,</p><formula xml:id="formula_2">f l CA = ψ h=8 mha (f l Q , f r K , f r V ) or f r CA = ψ h=8 mha (f r Q , f l K , f l V ).</formula><p>Then, the starting feature f , and the attention features f SA and f CA are concatenated in the channel dimension and fed into the FFN layers to fuse the information and maintain the same size as f . The transformer block is repeated N = 12 times to iteratively integrate information from Bi-MG, resulting in the output feature f r out , f l out = ψ N =12 asyt (f r , f l ). To predict the abnormal probability ŷ of each side, the output features f out are fed into the abnormal classifier. For the asymmetry classification of paired mammograms, we compute the absolute difference of the output features between the right and left sides (f asy out = abs(f r out -f l out ), which for maximizing the difference between the two feature) and feed it into the asymmetry classifier. We calculate the classification loss using the binary cross entropy loss (BCE) L bce , denoted as L diag = L cls (y asy , y r , y l , x r , x l ) = L bce (y asy , ŷasy )+L bce (y, ŷ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Disentangling via Self-adversarial Learning</head><p>What would the Bi-MG look like when the asymmetrical abnormalities have been removed? Unlike previous studies <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref>, which only generated normal features in the latent space, our AsyD module use weights shared U-Net-like decoders ψ g , to generate both abnormal (x ab ) and normal (x n ) images for each side through a two-channel separation, as x n , x ab = ψ g (f out ). We constrain the model to reconstruct images realistically using L1 loss (L l1 ) with the guidance of CAMs (M ), as follows, L rec = L l1 ((1 -M )x, (1 -M )x n ) + L l1 (M x, x ab ). However, it is difficult to train the generator in a supervised manner due to the lack of annotations of the location for asymmetrical pairs. Inspired by previous self-adversarial learning work <ref type="bibr" target="#b10">[10]</ref>, we introduce a frozen discriminator ψ d to impose constraints on the generator to address this challenge. The frozen discriminator comprises the same components as AsyC. In each training step, we update the discriminator parameters by copying them from the AsyC for leading ψ g to generate the symmetrical Bi-MG. The ψ d enforces symmetry in the paired Bi-MG, which can be denoted as L dics = L cls (y asy = 0, y r = 0, y l = 0, x r n , x l n ). Furthermore, we use generated normal Bi-MG to reinforce the ability of AsyC to recognize subtle asymmetry and abnormal cues, as L ref ine = L cls (y asy , y r , y l , x r n , x l n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Asymmetric Synthesis for Supervised Reconstruction</head><p>To alleviate the lack of annotation pixel-wise asymmetry annotations, in this study, we propose a random synthesis method to supervise disentanglement.</p><p>Training with synthetic artifacts is a low-cost but efficient way to supervise the model to better reconstruct images <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b17">17]</ref>. In this study, we randomly select the number n ∈ [1, 2, 3] of tumors t from a tumor set T inserting into one or both sides of randomized selected symmetric Bi-MG (x r , x l |y asy = 0). For each tumor insertion, we randomly select a position within the breast region. The tumors and symmetrical mammograms are combined by an alpha blending-based method <ref type="bibr" target="#b17">[17]</ref>, which can be denoted by x|fake =</p><formula xml:id="formula_3">x n k=1 (1 -α k ) + n k=1 t k α k , t ∈ T .</formula><p>The alpha weights α k is a 2D Gaussian distribution map, in which the co-variance is determined by the size of k-th tumor t, representing the transparency of the pixels of the tumor. Some examples are shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The tumor set T is collected from real-world datasets. Specifically, to maintain the rule of weaklysupervised learning of segmentation and localization tasks, we collect the tumors from the DDSM dataset as T and train the model on the INBreast dataset.</p><p>When training the model on other datasets, we use the tumor set collected from the INBreast dataset. Thus, the supervised reconstruction loss is L syn = L l1 (x|real, x n |fake), where x|real is the real image before synthesis and x n |fake is the disentangled normal image from the synthesised image x|fake.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Loss Function</head><p>For each training step, there are two objectives, training AsyC and AsyD module, and then is the refinement of AsyC. For the first, the loss function can be denoted by</p><formula xml:id="formula_4">L = λ 1 L diag + λ 2 L rec + λ 3 L dics + λ 4 L syn .</formula><p>The values of weight terms λ 1 , λ 2 , λ 3 , and λ 4 are experimentally set to be 1, 0.1, 1, and 0.5, respectively. The loss of the second objective is L ref ine as aforementioned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>This study reports experiments on four mammography datasets. The INBreast dataset <ref type="bibr" target="#b7">[7]</ref> consists of 115 exams with BI-RADS labels and pixel-wise anno-tations, comprising a total of 87 normal (BI-RADS = 1) and 342 abnormal (BI-RADS = 1) images. The DDSM dataset <ref type="bibr" target="#b3">[3]</ref> consists of 2,620 cases, encompassing 6,406 normal and 4,042 (benign and malignant) images with outlines generated by an experienced mammographer. The VinDr-Mammo dataset <ref type="bibr" target="#b8">[8]</ref> includes 5,000 cases with BI-RADS assessments and bounding box annotations, consisting of 13,404 normal (BI-RADS = 1) and 6,580 abnormal (BI-RADS = 1) images. The In-house dataset comprises 43,258 mammography exams from 10,670 women between 2004-2020, collected from a hospital with IRB approvals. In this study, we randomly select 20% women of the full dataset, comprising 6,000 normal (BI-RADS = 1) and 28,732 abnormal (BI-RADS = 1) images. Due to a lack of annotations, the In-house dataset is only utilized for classification tasks. Each dataset is randomly split into training, validation, and testing sets at the patient level in an 8:1:1 ratio, respectively (except for that INBreast which is split with a ratio of 6:2:2, to keep enough normal samples for the test).</p><p>Table <ref type="table">1</ref>. Comparison of asymmetric and abnormal classification tasks on four mammogram datasets. We report the AUC results with 95% CI. Note that, when ablating the "AsyC ", we only drop the "asyt" and keep the encoders and classifiers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Settings</head><p>The mammogram pre-processing is conducted following the pipeline proposed by <ref type="bibr" target="#b5">[5]</ref>. Then we standardize the image size to 1024 × 512 pixels. For training models, we employ random zooming and random cropping for data augmentation. We employ the ResNet-18 <ref type="bibr" target="#b2">[2]</ref> with on ImageNet pre-trained weights as the common backbone for all methods. The Adam optimizer is utilized with an initial learning rate (LR) of 0.0001, and a batch size of 8. The training process on the INBreast dataset is conducted for 50 epochs with a LR decay of 0.1 every 20 epochs. For the other three datasets, the training is conducted separately on each one with 20 epochs and a LR decay of 0.1 per 10 epochs. All experiments are implemented in the Pytorch framework and an NVIDIA RTX A6000 GPU (48 GB). The training takes 3-24 h (related to the size of the dataset) on each dataset.</p><p>To assess the performance of different models in classification tasks, we calculate the area under the receiver operating characteristic curve (AUC) metric. The 95% confidence interval (CI) of AUC is estimated using bootstrapping (1,000 times) for each measure. For the segmentation task , we utilize Intersection over Union (IoU), Intersection over Reference (IoR), and Dice coefficients. For the localization task , we compute the mean accuracies of IoU or IoR values above a given threshold, following the approach <ref type="bibr" target="#b11">[11]</ref>. Specifically, we evaluated the mean accuracy with thresholds for IoU at 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, and 0.7, while the thresholds for IoR are 0.1, 0.25, 0.5, 0.75, and 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results</head><p>We compare our proposed DisAsymNet with single view-based baseline ResNet18, attention-driven method HAM <ref type="bibr" target="#b11">[11]</ref>, MV-based late-fusion method <ref type="bibr" target="#b4">[4]</ref>, current SOTA MV-based methods cross-view-transformer (CVT) [16], and attention-based MV methods proposed by Wang et al., <ref type="bibr" target="#b20">[20]</ref> on classification, segmentation, and localization tasks. We also conduct an ablation study to verify the effectiveness of "AsyC ", "AsyD", and "Synthesis". Note that, the asymmetric transformer (asyt) is a core component of our proposed "AsyC ". Thus, when ablating the "AsyC ", we only drop the asyt and keep the encoders and classifiers. The features from the Bi-MG are simply concatenated and passed to the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Performance in Different Tasks:</head><p>For the classification task, the AUC results of abnormal classification are shown in Table <ref type="table">1</ref>. Our method outperforms all the single-based and MV-based methods in these classification tasks across all datasets. Furthermore, the ablation studies demonstrate the effectiveness of each proposed model component. In particular, our "AsyC" only method already surpasses the CAT method, indicating the efficacy of the proposed combination of SA and CA blocks over using CA alone. Additionally, our "AsyD" only method improves the performance compared to the late-fusion method, demonstrating that our disentanglement-based self-adversarial learning strategy can refine classifiers and enhance the model's ability to classify anomalies and asymmetries. The proposed "Synthesis" method further enhances truth from the "Synthesis" method, our generator tends to excessively remove asymmetric abnormalities at the cost of leading to the formation of black holes or areas that are visibly darker than the surrounding tissue because of the limitation of our discriminator and lack of pixel-level supervision. The incorporation of proposing synthetic asymmetrical Bi-MG during model training can lead to more natural symmetric tissue generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We present, DisAsymNet, a novel asymmetrical abnormality disentangling-based self-adversarial learning framework based on the image-level class labels only. Our study highlights the importance of considering asymmetry in mammography diagnosis in addition to the general multi-view analysis. The incorporation of pixel-level normal symmetric breast view generation boosts the classification of Bi-MG and also provides the interpretation of the diagnosis. The extensive experiments on four datasets demonstrate the robustness of our DisAsymNet framework for improving performance in classification, segmentation, and localization tasks. The potential of leveraging asymmetry can be further investigated in other clinical tasks such as BC risk prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The schematic overview of the proposed DisAsymNet.</figDesc><graphic coords="3,71,85,69,17,306,94,148,30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Abnormality classification performance of DisAsymNet in terms of AUC trained on different sizes of training sets.</figDesc><graphic coords="6,67,32,477,38,284,38,63,01" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. <rs type="person">Xin Wang</rs> is funded by <rs type="funder">Chinese Scholarship Council scholarship (CSC)</rs> and this work was also funded by the <rs type="funder">Science and Technology Development Fund of Macau SAR</rs> (Grant number <rs type="grantNumber">0105/2022/A</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ETwAk2x">
					<idno type="grant-number">0105/2022/A</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the performance of our proposed method. Moreover, we investigate the ability of different methods to classify abnormalities under various percentages of DDSM, VinDr, and In-house datasets. The INBreast dataset was excluded from this experiment due to its small size. Figure <ref type="figure">2</ref> illustrates the robustness of our method's advantage and our approach consistently outperformed the other methods, regardless of the size of the training data used and data sources. For the weakly supervised segmentation and localization tasks, results are shown in Table <ref type="table">2</ref>. The results demonstrate that our proposed framework achieves superior segmentation and localization performance compared to other existing methods across all evaluation metrics. The results of the ablation experiment also reveal that all modules incorporated in our framework offer improvements for the tasks.</p><p>Visualization: Figure <ref type="figure">3</ref> displays multiple disentangled normal Bi-MG cases.</p><p>Our model achieves the efficient removal of asymmetrical abnormalities while retaining normal symmetric tissue. Without using pixel-level asymmetry ground</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-view local co-occurrence and global consistency learning improve mammogram classification generalisation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part III</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-81" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Current status of the digital database for screening mammography</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heath</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-94-011-5318-8_75</idno>
		<ptr target="https://doi.org/10.1007/978-94-011-5318-875" />
	</analytic>
	<monogr>
		<title level="m">Digital Mammography</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Thijssen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hendriks</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Van Erning</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="457" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view mammographic density classification by dilated and attention-guided residual learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinf</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1003" to="1013" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decoupling inherent risk and early cancer signs in image-based breast cancer risk models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-223" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020, Part VI</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="230" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From unilateral to bilateral learning: detecting mammogram masses with contrasted bilateral network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_53</idno>
		<idno>978-3-030-32226-7 53</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019, Part VI</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="477" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">INbreast: toward a full-field digital mammographic database</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Domingues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acad. Radiol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="248" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">VinDr-Mammo: a large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MedRxiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Asymmetry disentanglement network for interpretable acute ischemic stroke infarct segmentation in non-contrast CT scans</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ni</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_40</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-140" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part VIII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="416" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-adversarial learning for detection of clustered microcalcifications in mammograms</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87234-2_8</idno>
		<idno>978- 3-030-87234-2 8</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part VII</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12907</biblScope>
			<biblScope unit="page" from="78" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning hierarchical attention for weakly-supervised chest Xray abnormality localization and diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2698" to="2710" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BI-RADS R fifth edition: a summary of changes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Spak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plaxco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Santiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diagn. Interv. Imaging</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="179" to="190" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Stadnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.04800</idno>
		<title level="m">Meta-repository of screening mammography classifiers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Global cancer statistics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries. CA: A Canc</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clin</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="249" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Looking for abnormalities in mammograms with self-and weakly supervised reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-view analysis of unregistered medical images using cross-view transformers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Tulder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchiori</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-410" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part III</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Disentangling disease-related representation from obscure for disease prediction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="22652" to="22664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BR-GAN: bilateral residual generating adversarial network for mammogram classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59713-9_63</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59713-963" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020, Part II</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12262</biblScope>
			<biblScope unit="page" from="657" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bilateral asymmetry guided counterfactual generating network for mammogram classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="7980" to="7994" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Looking for abnormalities using asymmetrical information from bilateral mammograms</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beets-Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Artificial intelligence in breast imaging</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moriakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Mann</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-94918-1_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-94918-120" />
	</analytic>
	<monogr>
		<title level="m">Breast Imaging: Diagnosis and Intervention</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Fuchsjäger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Morris</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Helbich</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="435" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep neural networks improve radiologists&apos; performance in breast cancer screening</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1184" to="1194" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Toward robust mammography-based models for breast cancer risk</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Transl. Med</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">578</biblScope>
			<biblScope unit="page">4373</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MommiNet-v2: mammographic multi-view mass identification networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">102204</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-view attention network for breast cancer screening from multi-view mammograms</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1050" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Check and link: pairwise lesion correspondence guides mammogram mass detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19803-8_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19803-823" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022, Part XXI</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13681</biblScope>
			<biblScope unit="page" from="384" to="400" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
