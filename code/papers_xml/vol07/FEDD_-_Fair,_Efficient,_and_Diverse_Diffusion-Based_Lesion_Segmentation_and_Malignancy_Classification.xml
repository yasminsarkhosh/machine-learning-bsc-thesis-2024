<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Héctor</forename><surname>Carrión</surname></persName>
							<email>hcarrion@ucsc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Cruz, Santa Cruz</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Narges</forename><surname>Norouzi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley, Berkeley</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="270" to="279"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">84E3A70C664C6D2D29E77ACB034E08CB</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_26</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Lesion Segmentation</term>
					<term>Classification</term>
					<term>Fairness</term>
					<term>Diffusion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skin diseases affect millions of people worldwide, across all ethnicities. Increasing diagnosis accessibility requires fair and accurate segmentation and classification of dermatology images. However, the scarcity of annotated medical images, especially for rare diseases and underrepresented skin tones, poses a challenge to the development of fair and accurate models. In this study, we introduce a Fair, Efficient, and Diverse Diffusion-based framework for skin lesion segmentation and malignancy classification. FEDD leverages semantically meaningful feature embeddings learned through a denoising diffusion probabilistic backbone and processes them via linear probes to achieve state-of-the-art performance on Diverse Dermatology Images (DDI). We achieve an improvement in intersection over union of 0.18, 0.13, 0.06, and 0.07 while using only 5%, 10%, 15%, and 20% labeled samples, respectively. Additionally, FEDD trained on 10% of DDI demonstrates malignancy classification accuracy of 81%, 14% higher compared to the state-of-the-art. We showcase high efficiency in data-constrained scenarios while providing fair performance for diverse skin tones and rare malignancy conditions. Our newly annotated DDI segmentation masks and training code can be found on https://github.com/hectorcarrion/fedd.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Related Work</head><p>Skin diseases are a major public health concern that impacts millions of people worldwide. The first step towards diagnosis and treatment of skin diseases often involves visual inspection and analysis of the lesion by dermatologists or other medical experts. However, this process is often subjective, time-consuming, costly, and inaccessible for many people, especially in low-resource communities or remote areas. It is estimated that around 3 billion people lack adequate access to dermatological care <ref type="bibr" target="#b0">[1]</ref>. In the United States, only about one in three patients with skin disease are evaluated by a dermatologist, their average wait time exceeds 38 days while representing a cost of $75 billion on the healthcare system <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Therefore, there exists a growing need for automated methods that can assist dermatologists, especially those in low-resource environments, in attending to skin lesions accurately and efficiently.</p><p>Skin lesion semantic segmentation and malignancy classification are essential for providing accurate and explainable diagnosis information for patients with skin diseases, and recently Artificial Intelligent (AI) systems have led the stateof-the-art for these tasks. However, these systems are commonly based on data and training methods that are prone to racial biases <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Some of the main challenges facing AI systems that can lead to bias are:</p><p>-Data scarcity: Annotated medical images are often scarce and expensive to obtain due to privacy issues, cost, and expert availability. This limits the amount of data available for training Deep Learning (DL) models, which may result in overfitting, especially in a medical context, as shown in <ref type="bibr" target="#b6">[7]</ref>. -Class imbalance: The distribution of different types of skin lesions is often imbalanced in real-world datasets. For example, melanoma cases may be rare than basal cell carcinoma cases; this could then be exacerbated by datasets that are primarily sourced from light-skinned populations <ref type="bibr" target="#b7">[8]</ref>. This class imbalance can introduce biases in modeling. -Data diversity: The appearance and morphology of skin lesions can vary across different individuals due to factors such as age, gender, and ethnicity <ref type="bibr" target="#b8">[9]</ref>. A dataset can be large but not necessarily diverse. This lack of diversity in the data may lead to poor generalization <ref type="bibr" target="#b3">[4]</ref>. -Base models: Some recent works on dermatology images stem from transferlearning models designed for ImageNet <ref type="bibr" target="#b9">[10]</ref>, which may be overly large for smaller dermatologic datasets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Tuning these massive encoders could lead to overfitting. -Lack of diverse studies: A recent review of 70 dermatological AI studies between 2015 and 2020 found that only 17 studies included ethnicity descriptors, and only 7 included skin tone descriptors <ref type="bibr" target="#b12">[13]</ref>. This could lead to under-specification of model performance for different ethnicities.</p><p>Denoising Diffusion Probabilistic Models (DDPMs) have been introduced <ref type="bibr" target="#b13">[14]</ref> as a new form of generative modeling. DDPMs have achieved state-of-the-art performance in image synthesis <ref type="bibr" target="#b14">[15]</ref> and are effectively applied in colorization <ref type="bibr" target="#b15">[16]</ref>, super-resolution <ref type="bibr" target="#b16">[17]</ref>, segmentation <ref type="bibr" target="#b17">[18]</ref>, and other tasks. In the medical domain, recent work has presented results for DDPM-based anomaly detection <ref type="bibr" target="#b18">[19]</ref> and segmentation <ref type="bibr" target="#b19">[20]</ref>, but these are limited to MRI, CT, and ultrasonography not natural smartphone-captured images of dermatology conditions. To our knowledge, none have explored segmentation and malignancy classification in this context from DDPM-based embeddings without re-training and evaluated performance on diverse dermatology images.</p><p>We introduce the FEDD framework, a denoising diffusion-based approach trained on small, skin tone-balanced, Diverse Dermatology Images (DDI) <ref type="bibr" target="#b3">[4]</ref> subsets for skin lesion segmentation and malignancy classification that outperforms state-of-the-art across a diverse spectrum of skin tones and malignancy  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Description of Data</head><p>The most commonly used dataset to train and evaluate fairness in dermatology AI is Fitzpatrick17k <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> thanks to its large size of nearly 17,000 images. However it contains a significant skin tone imbalance (3.6 times more light than dark skin toned samples) and greater than 30% disease label noise <ref type="bibr" target="#b7">[8]</ref>. Further, the samples are not biopsied and visual inspection itself can be an unreliable way of diagnosis without the use of histopathological information <ref type="bibr" target="#b22">[23]</ref>. Recently, DDI <ref type="bibr" target="#b3">[4]</ref> was published as a dermatological image dataset with Fitzpatrick-scale <ref type="bibr" target="#b23">[24]</ref> scores for all images, classifying them as light (I-II), medium (III-IV), or dark (V-VI) in skin tone. While at a lower skin tone resolution (3-point instead of 6-point), these labels are reviewed by two board-certified dermatologists. It also includes a mix of rare and common benign and malignant skin conditions, all of which are confirmed via biopsy. The dataset contains visually ambiguous lesions that would be difficult to visually diagnose but represent the kind of lesions that are seen in clinical practice. DDI is somewhat balanced between skin tones, with about 16% more information for medium skin tones; however, it is not balanced between malignant and benign classes. In total, the dataset contains 656 samples. Details and distribution of diagnosis are shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>We draw 4 balanced subsets of DDI for training, each representing approximately 5% (10 samples per skin tone), 10% (20 samples per skin tone), 15% (30 samples per skin tone), and 20% (40 samples per skin tone) of DDI. The smaller training sets are subsets of the larger ones, this is to say 5% ⊆ 10% ⊆ 15% ⊆ 20% ⊆ DDI. For classification we draw validation and test sets, each containing 30 samples (10 samples per skin tone). Further, we test model checkpoints trained on each DDI subset on all remaining DDI images (476 samples), accuracy results from this larger test set are reported on the paper text and on Table <ref type="table">3</ref> of the supplementary materials. For segmentation, we test on 198 additionally annotated samples. This is due to DDI including disease labels suitable for malignancy classification. For segmentation however, samples need to be semantically labeled and some samples may be difficult to correctly annotate, leading to discarding; for example if the target lesion is ambiguous, blurry, partially visible or occluded. We annotated the dataset and all masks underwent a secondary quality review. We define 5 classes: lesion, skin, marker, ruler, and background. We opted to label these classes as many images include a ruler or markings to denote the lesion of focus. Visualizations of our ground truth labels are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Details on the annotation protocol, including skip criteria, can be found in the supplementary materials. We release our annotation work (a total of 378 annotated DDI images) as part of our contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>The UNet architecture was introduced for diffusion <ref type="bibr" target="#b13">[14]</ref> and found to improve generative performance <ref type="bibr" target="#b24">[25]</ref> over other denoising score-matching architectures.</p><p>Recent work <ref type="bibr" target="#b14">[15]</ref> has extensively ablated the diffusion UNet architecture by increasing depth while reducing the width, increasing the number of attention heads and applying it at different resolutions, applying BigGAN <ref type="bibr" target="#b25">[26]</ref> residual blocks, and introducing AdaGN for injecting timestep and class embedding onto residual blocks, obtaining state-of-the-art for image synthesis. From this work, we designate the unconditional image generation model as our backbone and freeze its weights. This network is an ImageNet-trained DDPM with 256 × 256 input and output resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Lesion, Marker, Ruler, Skin, and Background Segmentation</head><p>We obtain image encodings from blocks on the decoder side of the DDPM-UNet architecture, then apply bi-linear up-sampling up to some target output resolution (256 × 256 in this case) and concatenate them before feeding them to MLPs for per-pixel classification following <ref type="bibr" target="#b17">[18]</ref>. However, we use fewer blocks, a single-time step, and 5 MLPs. This is to avoid overfitting and because results from <ref type="bibr" target="#b17">[18]</ref> describe how different blocks at different timesteps perform differently depending on the target data. To understand which blocks are most promising for dermatology images, we obtain a sample of feature encodings at different blocks and perform K-Means clustering shown in the supplementary materials. We selected the blocks which clustered semantically meaningful areas (e.g. lesion, skin, and ruler). We identify block 6 and block 8 at timestep 100 as the most promising and use this setup for the rest of our segmentation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Malignancy Classification</head><p>For classification, we down-sample block encodings using a combination of 2D and 1D max pooling operations until the feature vectors are one-dimensional and of size 512. We note that the total number of pooling operations varies depending on the sampled block, as deeper blocks are smaller with more channels, while shallower blocks are larger with fewer channels. The vectors are then passed onto a 3-layer MLP of size 64, 32, and 1. We include batch normalization and dropout between each layer with 50% and 25%. This classification network is trained to predict the malignancy of the input image from the down-sampled feature vector. A summary of our approach is shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Lesion, Marker, Ruler, Skin, and Background Segmentation</head><p>Our segmentation results are evaluated by Intersection over Union (IoU) performance and are compared against other architectures pre-trained on ImageNet: DenseNet121 <ref type="bibr" target="#b26">[27]</ref>, VGG16 <ref type="bibr" target="#b27">[28]</ref>, ResNet50 <ref type="bibr" target="#b28">[29]</ref>, and two other smaller networks, EfficientNetB0 <ref type="bibr" target="#b29">[30]</ref> and MobileNetV2 <ref type="bibr" target="#b30">[31]</ref>. These architectures are configured as UNets and tasked with segmenting the input images. We observe FEDD outperforms all other architectures across all subsets of DDI on our validation and test sets. Importantly, other architectures (particularly the smaller networks) close the performance gap as the amount of training data increases, showcasing that FEDD's efficiency is most prevalent in very small data scenarios.</p><p>We further compare the FEDD's IoU performance on light and dark skin tone images to showcase algorithmic fairness. We note that all architectures show similar performance for both skin tones, suggesting that our balanced data subsets play a larger role in fairness than the choice of neural network. Finally, we plot test set performance when only considering the lesion class split between light and dark tones. We find that FEDD's performance is significantly better at segmenting the lesion class compared to other architectures. We believe this is due to the greater morphology variation of different skin lesions being harder to learn than other more consistent targets like the ruler. Since DDI contains a diversity of skin conditions, FEDD's efficiency becomes very useful for highquality lesion segmentation of this morphologically changing class. These results are shown in Fig. <ref type="figure" target="#fig_3">4</ref>.</p><p>In Fig. <ref type="figure" target="#fig_4">5</ref>, we visualize predicted segmentation masks between FEDD and the next best IoU-performing architecture, EfficientNetB0. We observe that FEDD achieves significantly better segmentation masks at lower fractions of labeled data across all skin tones. With a larger percentage of labeled data, EfficientNet begins to produce similar results to FEDD, but FEDD comparatively outputs higher-quality segmentations with fewer segmentation artifacts and false positives. The skin lesions themselves, which appear in different sizes, locations, and morphologies, are also most accurately segmented by FEDD. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Malignancy Classification</head><p>We ablate the performance of each individual block per timestep in the classification context as, to the best of our knowledge, it has not been done before. It is also not entirely intuitive which block depth and timestep combination would produce the best representations for classification, as well as how that performance varies as we introduce more data. We train FEDD's classifier on block embeddings produced between timestep 0 and 1000 of the backward diffusion process. We then record the accuracy of the classifier per block in increments of 50 timesteps. Figure <ref type="figure">6</ref> describes these results. While noisy, given the small amount of test data, the general pattern is that the earlier time-steps (later in the reverse diffusion de-noising process) allow for higher classification accuracy. This is likely due to the quality of the DDPM sample increasing as more noise is removed. Another finding is that as the classifier is shown more data, the shallower blocks begin to perform better. The best performing blocks are shown to be: block 4 at 5% DDI, block 6 at 10% DDI, block 12 at 15% DDI, and block 14 at 20% DDI. We attribute this to the fact that shallower blocks of the UNet decoder capture finer detail of the reconstructed image while deeper blocks capture lower-resolution detail. This coarse data is more generic and thus more generalizable than the finer features in later blocks. As we increase the amount of data, the classifier has enough information to learn from the finer details of later blocks, boosting performance.</p><p>We select the best-performing block and timestep combination at each fraction of data for the rest of our experiments. The previous classification state-of-Fig. <ref type="figure">6</ref>. Classification accuracy of each DDPM UNet decoder is shown. Later steps in the reverse diffusion process produce the highest quality embeddings. When less data is available (top row), earlier blocks of the UNet decoder perform best. When more data is available (bottom row), the later, shallower blocks of the decoder perform best. Fig. <ref type="figure">7</ref>. ROC-AUC scores (left) show FEDD outperforms other methods, including an ensemble of dermatologists. Accuracy per method is also higher (right).</p><p>the-art on the DDI dataset is reported on <ref type="bibr" target="#b3">[4]</ref> as the DeepDerm <ref type="bibr" target="#b10">[11]</ref> architecture pre-trained on HAM10000 <ref type="bibr" target="#b31">[32]</ref> and fine-tuned on DDI. We compare FEDD performance against this setup as well as other commonly used classifiers on each of our DDI subsets. We measure Receiver Operating Characteristic Area Under the Curve (ROC-AUC) at the best threshold for each method, F1 scores, and classification accuracy. We observe FEDD obtains a higher ROC-AUC than any other method at every level of data. It also surpasses the dermatologist ensemble performance reported by <ref type="bibr" target="#b3">[4]</ref>. FEDD also reports the best accuracy, however, it does not see improvement at 10% or 15% of DDI compared to 5% and 20%, as shown in Fig. <ref type="figure">7</ref>. When observing ROC curves for FEDD, we see it meets or exceeds the ensemble of dermatologists even at the smallest subset of DDI. We divide F1 scores between the light and dark skin tones finding that FEDD does not always obtain the best F1 performance at larger subsets of data, namely 15% and 20% of DDI. This result suggests that purpose-built classification networks could have a performance advantage over diffusion embeddings applied toward classification when allowed to train on larger amounts of data. Detailed F1 results are shown in table form on the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce the FEDD framework for skin lesion segmentation and malignancy classification that outperforms state-of-the-art methods and an ensemble of board-certified dermatologists across a diverse spectrum of skin tones and malignancy conditions under limited data scenarios. Our proposed methodology can improve the diagnosis and treatment of skin diseases while maintaining fair segmentation outcomes for under-represented skin tones and accurate malignancy predictions for rare malignancy conditions. We freely release our code and annotations to encourage further research around dermatological AI fairness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The disease count per skin tone (left) shows a smaller amount of malignancy data in DDI but otherwise mostly balanced between light and dark skin tones. The distribution of malignant illnesses (right) shows high diversity and thus the morphological variation of lesions in DDI.</figDesc><graphic coords="3,90,81,220,28,242,20,60,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Ground truth segmentation samples. We color code lesions as red, skin as green, markings as purple, and rulers as blue. Backgrounds are left to be transparent. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Image noise is added according to the diffusion noise schedule for the selected timestep. The DDPM processes the image, and feature embeddings are obtained from the desired block levels. Embeddings are concatenated and either up-sampled for segmentation or down-sampled for classification. Finally, Multi-Layer Perceptrons (MLPs) predict per-pixel semantic class or whole image malignancy.</figDesc><graphic coords="5,64,80,54,17,294,19,90,13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Top Row: The test IoU score for all segmentation classes for all, light, and dark skin tones (left-to-right). Bottom Row: The test IoU score for the lesion segment for all, light, and dark skin tones (left-to-right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Test set segmentation results for FEDD and EfficientNet.</figDesc><graphic coords="7,73,98,61,43,285,16,204,01" type="bitmap" /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 26.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clinical Applications -Fetal Imaging</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Use of teledermatology to improve dermatological access in rural areas</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coustasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Abodunde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Slater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Telemed. e-Health</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1022" to="1032" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<ptr target="http://www.aad.org" />
	</analytic>
	<monogr>
		<title level="j">Burden of skin disease</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Even patients with changing moles face long dermatology appointment wait-times: a study of simulated patient calls to dermatologists</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Resneck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Acad. Dermatol</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="54" to="58" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Disparities in dermatology AI performance on a diverse, curated clinical image set</title>
		<author>
			<persName><forename type="first">R</forename><surname>Daneshjou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Adv</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Those designing healthcare algorithms must become actively anti-racist</title>
		<author>
			<persName><forename type="first">K</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1327" to="1328" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ethical machine learning in healthcare</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ferryman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Rev. Biomed. Data Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="123" to="144" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning for medical image processing: overview, challenges and the future</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Razzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zaib</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-65981-7_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-65981-712" />
	</analytic>
	<monogr>
		<title level="m">Classification in BioApps</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Dey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ashour</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Borra</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="323" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Evaluating deep neural networks trained on clinical images in dermatology with the fitzpatrick 17k dataset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Groh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Skin color in dermatology textbooks: an updated evaluation and analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adelekun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Onyekaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Lipoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Acad. Dermatol</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Augmented intelligence dermatology: deep neural networks empower medical professionals in diagnosing skin cancer and predicting treatment options for 134 skin disorders</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Invest. Dermatol</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="1753" to="1761" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lack of transparency and potential bias in artificial intelligence data sets and algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Daneshjou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Dermatol</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image superresolution via iterative refinement</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Labelefficient semantic segmentation with diffusion models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Baranchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rubachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wolleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.04306</idno>
		<title level="m">Diffusion models for medical anomaly detection</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">MedSegDiff-V2: diffusion based medical image segmentation with transformer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.11798</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Meaningfully debugging model mistakes using conceptual counterfactual explanations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yuksekgonul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="66" to="88" />
		</imprint>
	</monogr>
	<note>In: proceedings.mlr.press</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FairDisCo: fairer AI in dermatology via disentanglement contrastive learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bayasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamarneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garbi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25069-9_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25069-913" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13804</biblScope>
			<biblScope unit="page" from="185" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Checklist for evaluation of image-based artificial intelligence reports in dermatology</title>
		<author>
			<persName><forename type="first">R</forename><surname>Daneshjou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Dermatol</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page">90</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The validity and practicality of sun-reactive skin types I through VI</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Fitzpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Dermatol</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="869" to="871" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adversarial score matching and improved sampling for image generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Piché-Taillefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T D</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05475</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<title level="m">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Efficientnet: rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<title level="m">Mobilenetv2: inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
