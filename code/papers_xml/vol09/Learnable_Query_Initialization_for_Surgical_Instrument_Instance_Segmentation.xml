<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learnable Query Initialization for Surgical Instrument Instance Segmentation</title>
				<funder>
					<orgName type="full">DBT, Govt of India</orgName>
				</funder>
				<funder ref="#_zJHES2a">
					<orgName type="full">ICMR, Govt of India</orgName>
				</funder>
				<funder ref="#_SQSXbtT">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Rohan</forename><surname>Raju Dhanakshirur</surname></persName>
							<email>rohanrd@sit.iitd.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">K</forename><forename type="middle">N Ajay</forename><surname>Shastry</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaustubh</forename><surname>Borgavi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ashish</forename><surname>Suri</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">AIIMS</orgName>
								<address>
									<settlement>New-Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prem</forename><forename type="middle">Kumar</forename><surname>Kalra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chetan</forename><surname>Arora</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Delhi</orgName>
								<address>
									<settlement>New Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learnable Query Initialization for Surgical Instrument Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F88AFDB31E1C00285C3D8CC2129BD3C5</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_70</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Surgical tool classification and instance segmentation are crucial for minimally invasive surgeries and related applications. Though most of the state-of-the-art for instance segmentation in natural images use transformer-based architectures, they have not been successful for medical instruments. In this paper, we investigate the reasons for the failure. Our analysis reveals that this is due to incorrect query initialization, which is unsuitable for fine-grained classification of highly occluded objects in a low data setting, typical for medical instruments. We propose a class-agnostic Query Proposal Network (QPN) to improve query initialization inputted to the decoder layers. Towards this, we propose a deformable-cross-attention-based learnable Query Proposal Decoder (QPD). The proposed QPN improves the recall rate of the query initialization by 44.89% at 0.9 IOU. This leads to an improvement in segmentation performance by 1.84% on Endovis17 and 2.09% on Endovis18 datasets, as measured by ISI-IOU. The source code can be accessed at https://aineurosurgery.github.io/learnableQPD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Background: Minimally invasive surgeries (MIS) such as laparoscopic and endoscopic surgeries have gained widespread popularity due to the significant reduction in the time of surgery and post-op recovery <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref>. Surgical instrument instance segmentation (SIIS) in these surgeries opens up the doors to increased precision and automation <ref type="bibr" target="#b17">[18]</ref>. However, the problem is challenging due to the lack of large-scale well-annotated datasets, occlusions of tool-tip (the distinguishing part of the surgical instrument), rapid changes in the appearance, reflections due to the light source of the endoscope, smoke, blood spatter etc. <ref type="bibr" target="#b4">[5]</ref>.</p><p>The Challenge: Most modern techniques for SIIS <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref> are multi-stage architectures, with the first stage generating region proposals (rectilinear boxes) and the second stage classifying each proposal independently. Unlike natural images, rectilinear bounding boxes are not an ideal choice for medical instruments, which are long, thin, and often visible diagonally in a bounding box. Thus, the ratio of the visible tool area to the area of the bounding box is highly skewed in medical scenarios. E.g., the ratio is 0.45 for the Endovis17 dataset and 0.47 for Endovis18, the two popular MIS datasets. In contrast, it is 0.60 for the MS-COCO dataset of natural images. The ratio is important because lower numbers imply more noise due to background and a more difficult classification problem.</p><p>Current Solution Strategy: Recently, S3Net <ref type="bibr" target="#b3">[4]</ref> adapted the MaskRCNN <ref type="bibr" target="#b13">[14]</ref> backbone to propose a 3-stage architecture. Their third stage implements hard attention based on the predicted masks from the second stage and re-classifies the proposals. The hard attention avoids distraction due to the presence of large background regions in the proposal boxes, allowing them to outperform all the previous state of the art for medical instruments or natural images.</p><p>Our Observation: In the last few years, attention-based transformer architectures have outperformed CNN architectures for many computer-vision-based tasks. Recent transformer-based object detection models implement deformable attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref> which predicts sampling points to focus attention on the fine-grained features in an image. One expects that this would allow transformer architectures to concentrate only on the tool instead of the background, leading to high accuracy for medical instrument instance segmentation. However, in our experiments, as well as the ones reported by <ref type="bibr" target="#b3">[4]</ref>, this is not observed. We investigate the reasons and report our findings on the probable causes. We also propose a solution strategy to ameliorate the problem. Our implementation of the strategy sets up a new state of the art for the SIIS problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions: (1)</head><p>We investigate the reason for the failure of transformerbased object detectors for the medical instrument instance segmentation tasks. Our analysis reveals that incorrect query initialization is to blame. We observe that recall of an instrument based on the initialized queries is a lowly 7.48% at 0.9 IOU, indicating that many of the relevant regions of interest do not even appear in the initialized queries, thus leading to lower accuracy at the last stage. (2) We observe that CNN-based object detectors employ a non-maximal suppression (NMS) at the proposal stage, which helps spread the proposal over the whole image. In contrast in transformer-based detection models, this has been replaced by taking the highest confidence boxes. In this paper, we propose to switch back to NMS-based proposal selection in transformers. (3) The NMS uses only bounding boxes and does not allow content interaction for proposal selection. We propose a Query Proposal Decoder block containing multiple layers of selfattention and deformable cross-attention to perform region-aware refinement of the proposals. The refined proposals are used by a transformer-based decoder backbone for the prediction of the class label, bounding box, and segmentation mask. (4) We show an improvement of 1.84% over the best-performing SOTA technique on the Endovis17 and 2.09% on the Endovis18 dataset as measured by ISI-IOU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Datasets: Surgical tool recognition and segmentation has been a well-explored research topic <ref type="bibr" target="#b10">[11]</ref>. Given the data-driven nature of recent computer vision techniques, researchers have also proposed multiple datasets for the problem. Twinanda et al. <ref type="bibr" target="#b27">[28]</ref> have proposed an 80-video dataset of cholecystectomy surgeries, with semantic segmentation annotation for 7 tools. Al Hajj et al. <ref type="bibr" target="#b0">[1]</ref> proposed a 50-video dataset of phacoemulsification cataract surgeries with 21 tools annotated for bounding boxes. Cadis <ref type="bibr" target="#b11">[12]</ref> complements this dataset with segmentation masks. Ross et al. <ref type="bibr" target="#b24">[25]</ref> in 2019 proposed a 30-video dataset corresponding to multiple surgeries with one instrument, annotated at image level for its presence. Endovis datasets, Endovis 2017 (EV17) <ref type="bibr" target="#b2">[3]</ref> and Endovis 2018 (EV18) <ref type="bibr" target="#b1">[2]</ref> have gained popularity in the recent past. Both of them have instance-level annotations for 7 tools. EV17 is a dataset of 10 videos of the Da Vinci robot, and EV18 is a dataset of 15 videos of abdominal porcine procedures.</p><p>Instance Segmentation Techniques for Medical Instruments: Multiple attempts have been made to perform instance segmentation using these datasets. <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b17">[18]</ref> use a MaskRCNN-based <ref type="bibr" target="#b13">[14]</ref> backbone pre-trained on natural images and perform cross-domain fine-tuning. Wang et al. <ref type="bibr" target="#b30">[31]</ref> assign categories to each pixel within an instance and convert the problem into a pixel classification. They then use the ResNet backbone to solve the problem. <ref type="bibr" target="#b28">[29]</ref> modified the MaskRCNN architecture and proposed a Sample Consistency Network to bring closer the distribution of the samples at training and test time. Ganea et al. <ref type="bibr" target="#b9">[10]</ref> use the concept of few-shot learning on top of MaskRCNN to improve the performance. Wentao et al. <ref type="bibr" target="#b7">[8]</ref> add a mask prediction head to YoLo V3 <ref type="bibr" target="#b22">[23]</ref> All these algorithms use CNN-based architectures, with ROI-Align, to crop the region of interest. Since the bounding boxes are not very tight in surgical cases due to the orientation of the tools, a lot of background information is passed along with the tool, and thereby the classification performance is compromised. Baby et al. <ref type="bibr" target="#b3">[4]</ref> use a third-stage classifier on top of MaskRCNN to correct the misclassified masks and improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer-based Instance Segmentation for Natural Images:</head><p>On the other hand, transformer-based instance segmentation architectures <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref> generate sampling points to extract the features and thereby learn more localised information. This gives extra leverage to transformer architectures to perform better classification. <ref type="bibr" target="#b16">[17]</ref> propose the first transformer-based end-to-end instance segmentation architecture. They predict low-level mask embeddings and combine them to generate the actual masks. <ref type="bibr" target="#b12">[13]</ref> learn the location-specific features by providing the information on position embeddings. <ref type="bibr" target="#b5">[6]</ref> uses a deformable-multihead-attention-based mechanism to enrich the segmentation task. Mask DINO <ref type="bibr" target="#b18">[19]</ref> utilizes better positional priors as originally proposed in <ref type="bibr" target="#b32">[33]</ref>. They also perform box refinement at multiple levels to obtain the tight instance mask. In these architectures, the query initialization is done using the top-k region proposals based on their corresponding classification score. Thus ambiguity in the classification results in poor query initialization, and thereby the entire mask corresponding to that instance is missed. This leads to a significant reduction in the recall rate of these models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methodology</head><p>Backbone Architecture: We utilize Mask DINO <ref type="bibr" target="#b18">[19]</ref> as the backbone architecture for our model. As illustrated in Fig. <ref type="figure">1</ref>, it uses ResNet <ref type="bibr" target="#b14">[15]</ref> as the feature extractor to generate a multi-scale feature map at varying resolutions. These feature maps are then run through an encoder to generate an enhanced feature map with the same resolution as the original feature map. The enhanced feature maps are used to generate a set of region proposals. Then, the region proposals are sorted based on their classification logit values. Mask DINO uses a d dimensional query vector to represent an object's information. The top n q generated region proposals are used to initialize a set of n q queries. These queries are then passed through a series of decoder layers to obtain a set of refined queries. These refined queries are used for the purpose of detection, classification, and segmentation. Problems with Mask DINO: The model predicts various outputs using the queries initialized with the top n q region proposals. Our analysis reveals that most false negative outputs are due to the queries initialized with significantly fewer to no region proposals corresponding to the missed objects. During the initialization procedure, Mask DINO sorts region proposals based on their classification logit values. The surgical instruments resemble one another and have few distinguishing features. In case of label confusion, which happens often in medical instruments, the encoder outputs a proposal with low confidence. When sorted, these low-confidence proposals get deleted. Hence, in the proposed architecture we argue for class-independent proposal selection, which does not rely on the classification label or its confidence at this stage.</p><p>Query Proposal Network: Spatial Diversification of the Proposals: The proposed Query Proposal Network (QPN) is shown in Fig. <ref type="figure">1</ref>. QPN takes the enhanced feature maps as input and performs a pixel-wise classification and regression to obtain the initial region proposals. These initial region proposals undergo Non-Maximal Suppression (NMS) in order to remove the duplicates, but more importantly, output the proposal boxes which are spread all through the image. This is important because, given the complexity of the classification in medical instruments, we do not wish to overly rely on the classification label and would rather explore more regions of interest in the decoder. Hence, we choose top k proposals based on the NMS instead of the label confidence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Proposal Network: Content-based Inter-proposal Interaction:</head><p>The top k region proposals from NMS are used to initialize the queries for the proposed Query Proposal Decoder (QPD). Note that the NMS works only on the basis of box coordinates and does not take care of content embeddings into account. We try to make up for the gap through the QPD module. The QPD module consists of self-attention, cross-attention and feed-forward layers. The self-attention layer of QPD allows the queries to interact with each other and the duplicates are avoided. We use the standard deformable cross-attention module as proposed in <ref type="bibr" target="#b34">[35]</ref>. Unlike traditional cross-attention-based mechanisms, this method attends only to a fixed number of learnable key points around the middlemost pixel of every region proposal (query) irrespective of the size of the feature maps. This allows us to achieve better convergence in larger feature maps. Thus, the cross-attention layer allows the interaction of queries with the enhanced feature maps from the encoder and feature representation for each query is obtained. The feed-forward layer refines the queries based on the feature representation obtained in the previous layer. We train the QPD layers using the mask and bounding box loss as is common in transformer architectures <ref type="bibr" target="#b18">[19]</ref>. Note that no classification loss is back-propagated. This allows the network to perform query refinement irrespective of the classification label and retains queries corresponding to the object instances, which were omitted due to ambiguity in classification. The queries outputted by the QPD module are used to initialize the standard decoder network of Mask DINO. Implementation Details: We train the proposed architecture using three kinds of losses, classification loss L cls , box regression loss L box , and the mask prediction loss L mask . We use focal loss <ref type="bibr" target="#b19">[20]</ref> as L cls . We use 1 and GIOU <ref type="bibr" target="#b23">[24]</ref> loss for L box .</p><p>For L mask , we use cross entropy and IOU (or dice) loss. The total loss is:</p><formula xml:id="formula_0">L total = λ 1 L cls + λ 2 1 + λ 3 L giou + λ 4 L ce + λ 5 L dice<label>(1)</label></formula><p>Through hyper-parameter tuning, we set λ = [0.19, 0.24, 0.1, 0.24, 0.24]. We use a batch size of 8. The initial learning rate is set to 0.0001, which drops by 0.1 after every 20 epochs. We set 0.9 as the Nesterov momentum coefficient. We train the network for 50 epochs on a server with 8 NVidia A100, 40 GB GPUs.</p><p>Besides QPN we use the exact same architecture as proposed in MaskDINO <ref type="bibr" target="#b18">[19]</ref>. However, we perform transfer learning using the MaskDINO pre-trained weights, and therefore, we do not use "GT+noise" as an input to the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Evaluation Methodology: We demonstrate the performance of the proposed methodology on two benchmark Robot-assisted endoscopic surgery datasets Endovis 2017 <ref type="bibr" target="#b2">[3]</ref> (denoted as EV17), and Endovis 2018 <ref type="bibr" target="#b1">[2]</ref> (denoted as EV18) as performed by <ref type="bibr" target="#b3">[4]</ref>. EV17 is a dataset of 10 videos (1800 images) obtained from the Da Vinci robotic system with 7 instruments. We adopt the same four-fold cross-validation strategy as shown by <ref type="bibr" target="#b25">[26]</ref> for the EV17 dataset. EV18 is a real surgery dataset of 15 videos containing 1639 training and 596 validation images, with 7 instruments. <ref type="bibr" target="#b3">[4]</ref> corrected the misclassified ground truth labels and added instance-level annotations to this dataset. We evaluate the performance of the proposed algorithm using challenge IOU as proposed by <ref type="bibr" target="#b2">[3]</ref>, as well as ISI IOU and Mean class IOU as reported in <ref type="bibr" target="#b10">[11]</ref>. We also report per instrument class IOU as suggested by <ref type="bibr" target="#b3">[4]</ref>.</p><p>Quantitative Results on EV17, EV18, and Cadis: The results of our technique for the EV17 dataset are shown in Table <ref type="table" target="#tab_1">1</ref> and for the EV18 dataset are shown in Table <ref type="table" target="#tab_2">2</ref>. It can be observed that the proposed technique outperforms the best-performing SOTA methods by 1.84% in terms of challenge IOU for EV17 and 1.96% for EV18 datasets. Due to the paucity of space, we show the performance of the proposed methodology on the Cadis <ref type="bibr" target="#b11">[12]</ref> dataset in the supplementary material. The qualitative analysis of instance segmentation by the proposed methodology against the other SOTA algorithms is shown in Fig. <ref type="figure" target="#fig_0">2</ref>. We demonstrate improved performance in the occluded and overlapping cases. We observe a testing speed of 40 FPS on a standard 40GB Nvidia A100 GPU cluster.  Evidence of Query Improvement: The improvement in the query initialization due to the proposed architecture is demonstrated in Fig. <ref type="figure" target="#fig_1">3</ref>. Here, we mark the centre of initialized query boxes and generate the scatter plot. The average recall rate for the EV18 dataset at 0.9 IOU for the top 300 queries in vanilla Mask DINO is 7.48%. After performing NMS, the recall rate decreases to 0.00%, but the queries get diversified. After passing the same through the proposed Query Proposal Decoder (QPD), the recall rate is observed to be 52.38%. The increase in recall rate to 52.38% indicates successful learning of QPD. It indicates that the proposed architecture is able to cover more objects in the initialized queries thereby improving the performance at the end. While the diversity of the queries is important, it is also important to ensure that the queries with higher confidence are near the ground truth to ensure better learning. Random initialization for NMS is observed with sub-optimal performance and is shown in Table <ref type="table" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study:</head><p>We perform an ablation on the EV18 dataset to show the importance of each block in the proposed methodology. The results of the same are summarised in Table <ref type="table" target="#tab_3">3</ref>. We also experiment with the number of layers in QPD. We have used the best-performing 2-layer QPD architecture for all other experiments. The reduction in performance with more QPD layers can be attributed to the under-learning of negative samples due to stricter query proposals</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a novel class-agnostic Query Proposal Network (QPN) to better initialize the queries for a transformer-based surgical instrument instance segmentation model. Towards this, we first diversified the queries using the non-maximal suppression and proposed a deformable-cross-attentionbased learnable Query Proposal Decoder (QPD). On average, the proposed QPN improved the recall rate of the query initialization by 52.38% at 0.9 IOU. The improvement translates to an improved ISI-IOU of 1.84% and 2.09% in the publicly available Endovis 2017 and Endovis 2018 datasets, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The qualitative analysis of instance segmentation by the proposed methodology against the other SOTA algorithms: Here, indicates that the instrument is classified and segmented correctly. O indicates missed instance, × indicates incorrect classification and A indicates ambiguous instance.</figDesc><graphic coords="7,73,80,272,57,276,64,272,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Proposed improvement in the Query Initialization. Left: two sample images and their corresponding query initialization by different architectures. Right: shows the recall rate at various IOU thresholds</figDesc><graphic coords="8,59,97,283,61,332,20,97,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,70,98,319,04,310,06,161,71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>NMS Enhanced Feature Maps Query Proposal Network Decoder Input Image Queries Initialisation Enhanced Feature Maps Query Proposal Decoder x k Queries Initialisation Pixel-wise Classification and Regression Self Attention Deformable Cross Attention Feed Forward Network Decoder Output Boxes Output Masks Self Attention Feed Forward Network Encoder Self Attention Deformable Cross Attention Feed Forward Network Backbone Feature Maps Fig</head><label></label><figDesc></figDesc><table /><note><p>. 1. Proposed Query Proposal Network</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The comparison of the proposed methodology against the other SOTA architectures for the Endovis 17<ref type="bibr" target="#b2">[3]</ref> dataset. Here rows 4-17 are obtained from<ref type="bibr" target="#b3">[4]</ref>. Note that<ref type="bibr" target="#b33">[34]</ref> is a video instance segmentation paper and uses video information to perform segmentation.</figDesc><table><row><cell>Method</cell><cell cols="5">Conference Instrument Classes IOU</cell><cell></cell><cell></cell><cell></cell><cell>Ch. lOU ISI. IOU MC. IOU</cell></row><row><cell></cell><cell></cell><cell>BF</cell><cell>PF</cell><cell cols="2">LND VS</cell><cell>GR</cell><cell cols="2">MCS UP</cell></row><row><cell>Dataset EV17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCaL [30]</cell><cell>ECCV20</cell><cell cols="5">39.44 38.01 46.74 16.52 1.90</cell><cell>1.98</cell><cell cols="2">13.11 49.56</cell><cell>45.71</cell><cell>23.78</cell></row><row><cell>Condlnst [27]</cell><cell>ECCV20</cell><cell cols="5">44.29 38.03 47.38 24.77 4.51</cell><cell cols="3">15.21 15.67 59.02</cell><cell>52.12</cell><cell>27.12</cell></row><row><cell>BMaskRCNN [7]</cell><cell>ECCV20</cell><cell cols="5">32.89 32.82 41.93 12.66 2.07</cell><cell>1.37</cell><cell cols="2">14.43 49.81</cell><cell>38.81</cell><cell>19.74</cell></row><row><cell>SOLO [31]</cell><cell cols="5">NeurIPS20 22.05 23.17 41.07 7.68</cell><cell>0.00</cell><cell cols="2">11.29 4.60</cell><cell>35.41</cell><cell>33.72</cell><cell>15.79</cell></row><row><cell>ISINET [11]</cell><cell cols="6">MICCAI20 38.70 38.50 50.09 27.43 2.01</cell><cell cols="3">28.72 12.56 55.62</cell><cell>52.20</cell><cell>28.96</cell></row><row><cell>SCNet [29]</cell><cell>AAAI21</cell><cell cols="5">43.96 29.54 48.75 22.89 1.19</cell><cell>4.90</cell><cell cols="2">14.47 48.17</cell><cell>46.92</cell><cell>25.98</cell></row><row><cell>MFTA [10]</cell><cell>CVPR21</cell><cell cols="3">31.16 35.07 39.9</cell><cell cols="2">12.05 2.28</cell><cell>6.08</cell><cell cols="2">11.61 46.16</cell><cell>41.77</cell><cell>20.27</cell></row><row><cell>Detectors [22]</cell><cell>CVPR21</cell><cell cols="5">48.54 34.36 49.72 20.33 2.04</cell><cell>8.92</cell><cell cols="2">10.58 50.93</cell><cell>47.38</cell><cell>24.93</cell></row><row><cell>Orienmask [8]</cell><cell>ICCV21</cell><cell cols="5">40.42 28.78 44.48 12.11 3.91</cell><cell cols="3">15.18 12.32 42.09</cell><cell>39.27</cell><cell>23.22</cell></row><row><cell>Querylnst [9]</cell><cell>ICCV21</cell><cell cols="5">20.87 12.37 46.75 10.48 0.52</cell><cell>0.39</cell><cell>4.58</cell><cell>33.59</cell><cell>33.06</cell><cell>15.32</cell></row><row><cell>FASA [32]</cell><cell>ICCV21</cell><cell cols="4">20.13 18.81 39.12 8.34</cell><cell>0.68</cell><cell>2.17</cell><cell>3.46</cell><cell>34.38</cell><cell>29.67</cell><cell>13.24</cell></row><row><cell>Mask2Former [6]</cell><cell>CVPR22</cell><cell cols="5">19.60 20.22 45.44 11.95 0.00</cell><cell>1.48</cell><cell cols="2">22.10 40.39</cell><cell>39.84</cell><cell>17.78</cell></row><row><cell>TraSeTR  *  [34]</cell><cell>ICRA22</cell><cell cols="8">45.20 56.70 55.80 38.90 11.40 31.30 18.20 60.40</cell><cell>65.20</cell><cell>36.79</cell></row><row><cell>S3Net [4]</cell><cell cols="9">WACV23 75.08 54.32 61.84 35.50 27.47 43.23 28.38 72.54</cell><cell>71.99</cell><cell>46.55</cell></row><row><cell>Mask DINO [19]</cell><cell cols="9">Archive 22 64.14 45.29 78.96 59.35 21.90 36.70 30.72 75.96</cell><cell>77.63</cell><cell>43.86</cell></row><row><cell>Proposed architecture</cell><cell></cell><cell cols="8">70.61 45.84 80.01 63.41 33.64 66.57 35.28 77.8</cell><cell>79.58</cell><cell>49.92</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The comparison of the proposed methodology against the other SOTA architectures for the Endovis 18<ref type="bibr" target="#b1">[2]</ref> dataset. Here rows 4-17 are obtained from<ref type="bibr" target="#b3">[4]</ref> Note that<ref type="bibr" target="#b33">[34]</ref> is a video instance segmentation paper and uses video information to perform segmentation.</figDesc><table><row><cell>Method</cell><cell cols="4">Conference Instrument Classes IOU</cell><cell></cell><cell></cell><cell>Ch. lOU ISI. IOU MC. IOU</cell></row><row><cell></cell><cell></cell><cell>BF</cell><cell>PF</cell><cell>LND SI</cell><cell>CA</cell><cell cols="2">MCS UP</cell></row><row><cell>Dataset EV18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCaL [30]</cell><cell>ECCV20</cell><cell cols="3">73.67 40.35 5.57 0.00</cell><cell>0.00</cell><cell cols="2">89.84 0.00 68.56</cell><cell>67.58</cell><cell>29.92</cell></row><row><cell>Condlnst [27]</cell><cell>ECCV20</cell><cell cols="4">77.42 37.43 7.77 43.62 0.00</cell><cell>87.8</cell><cell>0.00 72.27</cell><cell>71.55</cell><cell>36.29</cell></row><row><cell>BMaskRCNN [7]</cell><cell>ECCV20</cell><cell cols="4">70.04 28.91 9.97 45.01 4.28</cell><cell cols="2">86.73 3.31 68.94</cell><cell>67.23</cell><cell>35.46</cell></row><row><cell>SOLO [31]</cell><cell cols="5">NeurIPS20 69.46 23.92 2.61 36.19 0.00</cell><cell cols="2">87.97 0.00 65.59</cell><cell>64.88</cell><cell>31.45</cell></row><row><cell>ISINET [31]</cell><cell cols="5">MICCAI20 73.83 48.61 30.98 37.68 0.00</cell><cell cols="2">88.16 2.16 73.03</cell><cell>70.97</cell><cell>40.21</cell></row><row><cell>SCNet [29]</cell><cell>AAAI21</cell><cell cols="4">78.40 47.97 5.22 29.52 0.00</cell><cell cols="2">86.69 0.00 71.74</cell><cell>70.99</cell><cell>35.40</cell></row><row><cell>MFTA [10]</cell><cell>CVPR21</cell><cell cols="4">71.00 31.62 3.93 43.48 9.90</cell><cell cols="2">87.77 3.86 69.20</cell><cell>67.97</cell><cell>35.94</cell></row><row><cell>Detectors [22]</cell><cell>CVPR21</cell><cell cols="3">73.94 46.85 0.00 0.00</cell><cell>0.00</cell><cell cols="2">79.92 0.00 66.69</cell><cell>65.06</cell><cell>28.67</cell></row><row><cell>Orienmask [8]</cell><cell>ICCV21</cell><cell cols="4">68.95 38.66 0.00 31.25 0.00</cell><cell cols="2">91.21 0.00 67.69</cell><cell>66.77</cell><cell>32.87</cell></row><row><cell>Querylnst [9]</cell><cell>ICCV21</cell><cell cols="3">74.13 31.68 2.30 0.00</cell><cell>0.00</cell><cell cols="2">87.28 0.00 66.44</cell><cell>65.82</cell><cell>27.91</cell></row><row><cell>FASA [32]</cell><cell>ICCV21</cell><cell cols="3">72.82 37.64 5.62 0.00</cell><cell>0.00</cell><cell cols="2">89.02 1.03 68.31</cell><cell>66.84</cell><cell>29.45</cell></row><row><cell>Mask2Former [6]</cell><cell>CVPR22</cell><cell cols="3">69.35 24.13 0.00 0.00</cell><cell>0.00</cell><cell cols="2">89.96 10.29 65.47</cell><cell>64.69</cell><cell>27.67</cell></row><row><cell>TraSeTR  *  [34]</cell><cell>ICRA22</cell><cell cols="3">76.30 53.30 46.5 40.6</cell><cell cols="2">13.90 86.3</cell><cell>17.5 76.20</cell><cell>47.77</cell></row><row><cell>S3Net [4]</cell><cell cols="5">WACV23 77.22 50.87 19.83 50.59 0.00</cell><cell cols="2">92.12 7.44 75.81</cell><cell>74.02</cell><cell>42.58</cell></row><row><cell>Mask DINO [19]</cell><cell cols="5">Archive 22 82.35 57.67 0.83 60.46 0.00</cell><cell cols="2">90.73 0.00 75.63</cell><cell>76.39</cell><cell>41.73</cell></row><row><cell>Proposed architecture</cell><cell></cell><cell cols="4">82.8 60.94 19.96 49.70 0.00</cell><cell cols="2">93.93 0.00 77.77</cell><cell>78.43</cell><cell>43.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation analysis on EV18 dataset to demonstrate the importance of each block in the proposed architecture and to check for the most optimal number of Query Proposal Decoder (QPD) layers.</figDesc><table><row><cell>Configuration</cell><cell cols="3">Instrument Classes IOU</cell><cell>Ch. lOU ISI. IOU MC. IOU</cell></row><row><cell></cell><cell>BF</cell><cell>PF</cell><cell>LND SI</cell><cell>CA MCS UP</cell></row><row><cell>Vanilla MaskDINO</cell><cell cols="4">82.35 57.67 0.83 60.46 0.00 90.73 0.00 75.63</cell><cell>76.39</cell><cell>41.73</cell></row><row><cell>Mask DINO + 1 QPD</cell><cell cols="4">82.41 66.61 0.00 40.57 0.00 89.65 0.00 75.58</cell><cell>76.28</cell><cell>39.89</cell></row><row><cell>Mask DINO + 2 QPD</cell><cell cols="4">83.71 60.92 10.55 50.4 0.00 92.22 4.84 76.67</cell><cell>77.51</cell><cell>43.23</cell></row><row><cell>Mask DINO + 3 QPD</cell><cell cols="4">76.24 61.26 7.06 48.91 0.00 92.30 0.00 74.11</cell><cell>75.00</cell><cell>40.83</cell></row><row><cell>Mask DINO + 4 QPD</cell><cell cols="4">75.43 45.67 2.67 48.31 0.00 94.23 0.00 71.97</cell><cell>73.46</cell><cell>38.04</cell></row><row><cell>Mask DINO + 5 QPD</cell><cell cols="4">78.25 27.39 0.92 47.96 0.00 93.89 0.00 71.38</cell><cell>71.79</cell><cell>35.49</cell></row><row><cell>Mask DINO + Random Input to NMS</cell><cell cols="4">81.46 51.73 25.36 40.73 0.00 92.73 0.00 76.19</cell><cell>76.60</cell><cell>41.70</cell></row><row><cell>Mask DINO + Encoder Input to NMS</cell><cell cols="4">82.05 63.02 15.33 50.61 0.00 88.71 0.00 76.21</cell><cell>76.64</cell><cell>42.82</cell></row><row><cell cols="5">Mask DINO + NMS + 2 QPD (Proposed) 82.80 60.94 19.96 49.70 0.00 93.93 0.00 77.77</cell><cell>78.43</cell><cell>43.84</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We thank <rs type="person">Dr Britty Baby</rs> for her assistance. We also thank <rs type="funder">DBT, Govt of India</rs>, and <rs type="funder">ICMR, Govt of India</rs>, for the funding under the projects <rs type="grantNumber">BT/PR13455/CoE/34/24/2015</rs> and <rs type="grantNumber">5/3/8/1/2022/MDMS</rs>(CARE) respectively.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zJHES2a">
					<idno type="grant-number">BT/PR13455/CoE/34/24/2015</idno>
				</org>
				<org type="funding" xml:id="_SQSXbtT">
					<idno type="grant-number">5/3/8/1/2022/MDMS</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 70.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CATARACTS: challenge on automatic tool annotation for cataract surgery</title>
		<author>
			<persName><forename type="first">Al</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIA</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="24" to="41" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11190</idno>
		<title level="m">robotic scene segmentation challenge</title>
		<imprint>
			<date type="published" when="2018">2018. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06426</idno>
		<title level="m">robotic instrument segmentation challenge</title>
		<imprint>
			<date type="published" when="2017">2017. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">From forks to forceps: a new framework for instance segmentation of surgical instruments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Baby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>WACV</publisher>
			<biblScope unit="page" from="6191" to="6201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vision-based and marker-less surgical tool detection and tracking: a review of the literature</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bouget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIA</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="633" to="654" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Mask2former for video instance segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10764</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Boundary-preserving mask R-CNN</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58568-6_39</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58568-6" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12359</biblScope>
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Real-time instance segmentation with discriminative orientation maps</title>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7314" to="7323" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Instances as queries</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6910" to="6919" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Incremental few-shot instance segmentation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1185" to="1194" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ISINet: an instance-based approach for surgical instrument segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bravo-Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_57</idno>
		<idno>978-3-030-59716-0 57</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="595" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cadis: cataract dataset for image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grammatikopoulou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11586</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sotr: segmenting objects with transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7157" to="7166" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Mask R-CNN</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Minimally invasive (laparoscopic) surgery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Himal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surg. Endosc. Interv. Tech</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1647" to="1652" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">ISTR: end-to-end instance segmentation with transformers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.00637</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate instance segmentation of surgical instruments in robotic surgery: model refinement and cross-dataset evaluation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCARS</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1607" to="1614" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mask DINO: towards a unified transformer-based framework for object detection and segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.02777</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Haptics in minimally invasive surgery-a review</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Westebring-Van Der Putten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Goossens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Jakimowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dankelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minim. Invasive Therapy Allied Technol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Detectors: detecting objects with recursive feature pyramid and switchable atrous convolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10213" to="10224" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: an incremental improvement</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Generalized intersection over union: a metric and a loss for bounding box regression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="658" to="666" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Comparative validation of multi-instance instrument segmentation in endoscopy: results of the ROBUST-MIS 2019 challenge</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIA</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">101920</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic instrument segmentation in robot-assisted surgery using deep learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Iglovikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th ICMLA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="624" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-817" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="282" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">EndoNet: a deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SCNet: training inference sample consistency for instance segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2701" to="2709" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The devil is in classification: a simple framework for long-tail instance segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58568-6_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58568-643" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12359</biblScope>
			<biblScope unit="page" from="728" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SOLOv2: dynamic and fast instance segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ANIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17721" to="17732" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">FASA: feature augmentation and sampling adaptation for long-tailed instance segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3457" to="3466" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DINO: detr with improved denoising anchor boxes for end-to-end object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">11th ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Trasetr: track-to-segment transformer with contrastive query for instance-level instrument segmentation in robotic surgery</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="page" from="11186" to="11193" />
			<date type="published" when="2022">2022</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<title level="m">Deformable detr: deformable transformers for end-to-end object detection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
