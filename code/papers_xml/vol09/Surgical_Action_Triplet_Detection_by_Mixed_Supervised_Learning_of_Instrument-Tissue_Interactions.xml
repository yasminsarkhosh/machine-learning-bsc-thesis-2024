<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions</title>
				<funder>
					<orgName type="full">French</orgName>
				</funder>
				<funder ref="#_ME5eDDY #_3JdrfcU #_wDedesW">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_Y8jddHS #_HH9Dwfh">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Saurav</forename><surname>Sharma</surname></persName>
							<email>ssharma@unistra.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chinedu</forename><forename type="middle">Innocent</forename><surname>Nwoye</surname></persName>
							<email>nwoye@unistra.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Didier</forename><surname>Mutter</surname></persName>
							<email>didier.mutter@ihu-strasbourg.eu</email>
							<affiliation key="aff1">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University Hospital of Strasbourg</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
							<email>npadoy@unistra.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IHU Strasbourg</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1371F78A0643645ED22112ACD33DF294</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_48</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Activity recognition</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Surgical action triplets describe instrument-tissue interactions as 〈instrument, verb, target〉 combinations, thereby supporting a detailed analysis of surgical scene activities and workflow. This work focuses on surgical action triplet detection, which is challenging but more precise than the traditional triplet recognition task as it consists of joint (1) localization of surgical instruments and (2) recognition of the surgical action triplet associated with every localized instrument. Triplet detection is highly complex due to the lack of spatial triplet annotation. We analyze how the amount of instrument spatial annotations affects triplet detection and observe that accurate instrument localization does not guarantee a better triplet detection due to the risk of erroneous associations with the verbs and targets. To solve the two tasks, we propose MCIT-IG, a two-stage network, that stands for Multi-Class Instrument-aware Transformer -Interaction Graph. The MCIT stage of our network models per class embedding of the targets as additional features to reduce the risk of misassociating triplets. Furthermore, the IG stage constructs a bipartite dynamic graph to model the interaction between the instruments and targets, cast as the verbs. We utilize a mixed-supervised learning strategy that combines weak target presence labels for MCIT and pseudo triplet labels for IG to train our network. We observed that complementing minimal instrument spatial annotations with target embeddings results in better triplet detection. We evaluate our model on the CholecT50 dataset and show improved performance on both instrument localization and triplet detection, topping the leaderboard of the CholecTriplet challenge in MICCAI 2022.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Surgical workflow analysis in endoscopic procedures aims to process large streams of data <ref type="bibr" target="#b7">[8]</ref> from the operating room (OR) to build context awareness systems <ref type="bibr" target="#b18">[19]</ref>. These systems aim to provide assistance to the surgeon in decision making <ref type="bibr" target="#b8">[9]</ref> and planning <ref type="bibr" target="#b5">[6]</ref>. Most of these systems focus on coarse-grained recognition tasks such as phase recognition <ref type="bibr" target="#b14">[15]</ref>, instrument spatial localization and skill assessment <ref type="bibr" target="#b4">[5]</ref>. Surgical action triplets <ref type="bibr" target="#b10">[11]</ref>, defined as 〈instrument, verb, target〉, introduce the fine-grained modeling of elements present in an endoscopic scene. In cataract surgery, <ref type="bibr" target="#b6">[7]</ref> adopts similar triplet formulation and also provides bounding box details for both instrument and targets. Another related work <ref type="bibr" target="#b0">[1]</ref>, in prostatectomy, uses bounding box annotations for surgical activities defined as 〈verb, anatomy〉. On laparoscopic cholecystectomy surgical data, existing approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref> focus on the challenging triplet recognition task, where the objective is to predict the presence of triplets but ignores their spatial locations in a video frame.</p><p>A recently conducted endoscopic vision challenge <ref type="bibr" target="#b13">[14]</ref> introduced the triplet detection task that requires instrument localization and its association with the triplet. While most of the contributed methods employ weak supervision to learn the instrument locations by exploiting the model's class activation map (CAM), a few other methods exploit external surgical datasets offering complementary instrument spatial annotations. The significant number of triplet classes left the weakly supervised approaches at subpar performance compared to fully supervised methods. The results of the CholecTriplet2022 challenge <ref type="bibr" target="#b13">[14]</ref> have led to two major observations. First, imprecise localization from the CAMbased methods impairs the final triplet detection performance, where the best weakly-supervised method reaches only 1.47% detection mean average precision. Second, the correct association of triplet predictions and their spatial location is difficult to achieve with instrument position information alone. This is mainly due to the possible occurrence of multiple instances of the same instrument and many options of targets/verbs that can be associated with one instrument instance. We set two research questions following these observations: (1) since manual annotation of instrument spatial locations is expensive and tedious, how can we use learned target/verb features to supplement a minimal amount of instrument spatial annotations? (2) since instrument cues are insufficient for better triplet association, how can we generate valid representative features of the targets/verbs that do not require additional spatial labels?</p><p>To tackle these research questions, we propose a fully differentiable twostage pipeline, MCIT-IG, that stands for M ulti-Class Instrument-aware Transformer -Interaction Graph. The MCIT-IG relies on instrument spatial information that we generate with Deformable DETR <ref type="bibr" target="#b21">[22]</ref>, trained on a subset of Cholec80 <ref type="bibr" target="#b16">[17]</ref> annotated with instrument bounding boxes. In the first stage, MCIT, a lightweight transformer, learns class wise embeddings of the target influenced by instrument spatial semantics and high level image features. This allows the embeddings to capture global instrument association features useful in IG. We train MCIT with target binary presence label, providing weak supervision. In the second stage, IG creates an interaction graph that performs dynamic association between the detected instrument instances and the target embeddings, and learns the verb on the interacting edge features, thereby detecting triplets. To train IG, triplet labels for the detected instrument instances are needed, which is unavailable. To circumvent this situation, we generate pseudo triplet labels for the detected instrument instances using the available binary triplet presence labels. In this manner, we provide mixed supervision to train MCIT-IG.</p><p>We hypothesize that a precise instrument detector can reveal additional instrument-target associations as more instrument instances are detected. To test this hypothesis, we conduct a study to investigate how the accuracy of the instrument detector affects triplet detection. We train an instrument detector with limited spatial data and evaluate the impact on triplet detection. We find that enhancing instrument localization is strongly linked to improved triplet detection performance. Finally, we evaluate our model on the challenge split of CholecT50 <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. We report both improved instrument localization and triplet detection performance, thanks to the graph-based dynamic association of instrument instances with targets/verbs, that captures the triplet label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We design a novel deep-learning model that performs triplet detection in twostages. In the first stage, we use a transformer to learn the instrument-aware target class embeddings. In the second stage, we construct an interaction graph from instrument instances to the embeddings, learn verb on the interacting edges and finally associate a triplet label with instrument instances. Using a trained Deformable DETR <ref type="bibr" target="#b21">[22]</ref> based on the MMDetection <ref type="bibr" target="#b1">[2]</ref> framework, we obtain bounding boxes for the instruments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone:</head><p>To extract visual features, we utilize ResNet50 <ref type="bibr" target="#b3">[4]</ref> as the backbone, and apply a 1×1 convolution layer to reduce the feature dimension from R h×w×c to R h×w×d , where c and d are 2048 and 512 respectively. We flatten the features to R hw×d and input to the Base Encoder, a lightweight transformer with b l layers. The base encoder modulates the local scene features from ResNet50 and incorporates context from other regions to generate global features F b ∈ R hw×d . We then apply ROIAlign on F b to generate instrument instance features F r ∈ R O×d , where O denotes the number of detected instruments. We also apply a linear layer Φ b on the instrument box coordinates and concatenate with the embeddings of predicted instrument class category to get d-dimensional features, and finally fuse with F r using linear layer Φ f to produce final d-dimensional instrument features F i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Instrument-Aware Target Class Embeddings:</head><p>To learn target features, we introduce a M ulti-C lass I nstrument-aware T ransformer ( MCIT ) that generates embeddings for each target class. In the standard transformer <ref type="bibr" target="#b2">[3]</ref>, a single class-agnostic token models the class distribution, but dilutes crucial class specific details. Inspired from <ref type="bibr" target="#b20">[21]</ref>, MCIT utilizes N class tokens, N t ∈ R N ×d , to learn class-specific embeddings of the target. However, to make the class embeddings aware of the instruments in the scene, we use the instrument features F i along with class tokens and region features. Specifically, MCIT takes input (F b , F i ) and creates learnable queries of dimension R (hw+N )×d and keys, values of dimension R (hw+N +O)×d to compute attention. Then, MCIT applies t l layers of attention to generate the output sequence, P t ∈ R (hw+N )×d . The learned class embeddings are averaged across N and input to a linear layer Φ t to generate logits y t ∈ R N following Eq. 1:</p><formula xml:id="formula_0">y t = Φ t 1 N hw+N k=hw P[k, :] . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>MCIT learns meaningful class embeddings of the target enriched with visual and position semantics of the instruments. This instrument-awareness is useful to identify the interacting instrument-target pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Instrument-Target Interactions:</head><p>To learn the interaction of the instrument and the target, we introduce a graph based framework I nteraction-Graph ( IG) that relies on the discriminative features of instrument instances and target class embeddings. We create an unidirectional complete bipartite graph, G = (U, V, E), where |U| = O and |V| = N denotes the source and destination nodes respectively, and edges E = {e u v , u ∈ U ∧ v ∈ V}. The node features of U and V correspond to the detected instrument instance features F i and target class embeddings N t respectively. We further project the nodes features to a lower dimensional space d using a linear layer Φ p . This setup provides an intuitive way to model instrument-tissue interactions as a set of active edges. Next, we apply message passing using GAT <ref type="bibr" target="#b17">[18]</ref>, that aggregates instrument features in U and updates target class embeddings at V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Verbs:</head><p>We concatenate the source and destination node features of all the edges E in G to construct the edge feature E f = {e f , e f ∈ R 2d }. Then, we compute the edge confidence score E s = {e s , e s ∈ R} for all edges E in G by applying a linear layer Φ e on E f . As a result, shown in Fig. <ref type="figure" target="#fig_0">1</ref> Stage 2, only active edges (in blue) remain and inactive edges (in red) are dropped based on a threshold. The active edge indicates the presence of interaction between an instrument instance and the target class. To identify the verb, we apply a linear layer Φ v on E f and generate verb logits y v ∈ R V +1 , where V is the number of verb classes with an additional 1 to denote background class.</p><p>Triplet Detection: To perform target and verb association for each instrument instance i, first we select the active edge e i j that corresponds to the target class j = argmax(α(E i s )), where α denotes softmax function and E i s = {e u s , ∀e u s ∈ E s ∧ u = i}. For the selected edge e = e i j , we apply softmax on the verb logits to obtain the verb class id, k = argmax(α(y e v )). The final score for the triplet 〈i, k, j 〉 is given by p(e i j ) × p(y e v k ), where p denotes the probability score. Mixed Supervision: We train our model in two stages. In the first stage, we train MCIT to learn target classwise embeddings with target binary presence label with weighted binary cross entropy on target logits y t for multi-label classification task following Eq. 2:</p><formula xml:id="formula_2">L t = C c=1 -1 N (W c y c log (σ(ŷ c )) + (1 -y c )log ((1 -σ(ŷ c ))) ,<label>(2)</label></formula><p>where C refers to total number of target classes, y c and ŷc denotes correct and predicted labels respectively, σ is the sigmoid function and W c is the class balancing weight from <ref type="bibr" target="#b12">[13]</ref>. For the second stage IG, we generate pseudo triplet labels for each detected instrument instances, where we assign the triplet from the binary triplet presence label if the corresponding instrument class matches.</p><p>To train IG, we apply categorical cross entropy loss on edge set E i s and verb logits y e v for all instrument instances i to obtain losses L e G and L v G respectively following Eq. 3:</p><formula xml:id="formula_3">L = - M c=1 y c log(p c ),<label>(3)</label></formula><p>where M denotes the number of classes which is N for L e G and V + 1 for L v G . The final loss for training follows Eq. 4:</p><formula xml:id="formula_4">L = L t + α × L e G + β × L v G , (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where α and β denote the weights to balance the loss contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Evaluation Metrics</head><p>Our experiments are conducted on the publicly available CholecT50 <ref type="bibr" target="#b12">[13]</ref> dataset, which includes binary presence labels for 6 instruments, 10 verbs, 15 targets, and 100 triplet classes. We train and validate our models on the official challenge split of the dataset <ref type="bibr" target="#b11">[12]</ref>. The test set consists of 5 videos annotated with instrument bounding boxes and matching triplet labels. Since the test set is kept private to date, all our results are obtained by submitting our models to the challenge server for evaluation. The model performance is accessed using video-specific average precision and recall metrics at a threshold (θ = 0.5) using the ivtmetrics library <ref type="bibr" target="#b11">[12]</ref>. We also provide box association results in the supplementary material for comparison with other methods on the challenge leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We first train our instrument detector for 50 epochs using a spatially annotated 12 video subset of Cholec80 and generate instrument bounding boxes and pseudo triplet instance labels for CholecT50 training videos. In stage 1, we set b l = 2, t l = 4, and d to 512. We initialize target class embeddings with zero values. We use 2-layer MLP for Φ b , Φ f , and 1-layer MLP for Φ t . We resize the input frame to 256 × 448 resolution and apply flipping as data augmentation. For training, we set learning rate 1e -3 for (backbone, base encoder), and 1e -2 for MCIT. We use SGD optimizer with weight decay 1e -6 and train for 30 epochs. To learn the IG, we fine-tune stage 1 and train stage 2. We use learning rate 1e -4 for (MCIT, base encoder), and 1e -5 for the backbone. In IG, Φ p , Φ e , and Φ v are 1-layer MLP with learning rate set to 1e -3 , and d set to 128 in Φ p to project node features to lower dimensional space. We use Adam optimizer and train both stage 1 and stage 2 for 30 epochs, exponentially decaying the learning rate by 0.99. The loss weights α and β is set to 1 and 0.5 respectively. We set batch size to 32 for both stages. We implement our model in PyTorch and IG graph layers in DGL <ref type="bibr" target="#b19">[20]</ref> library. We train the model on Nvidia V100 and A40 GPUs and tune model hyperparameters using random search on 5 validation videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with the Baseline:</head><p>We obtain the code and weights of Rendezvous (RDV) <ref type="bibr" target="#b12">[13]</ref> model from the public github and generate the triplet predictions on the CholecT50-challenge test set. We then associate these predictions with the bounding box predictions from the Deformable DETR <ref type="bibr" target="#b21">[22]</ref> to generate baseline triplet detections as shown in Table <ref type="table" target="#tab_0">1</ref>. With a stable instrument localization performance (60.1 mAP), our MCIT model leverage the instrument-aware target features to captures better semantics of instrument-target interactions than the baseline. Adding IG further enforces the correct associations, thus improving the triplet detection performance by +0.89 mAP, which is 13.8% increase from the baseline performance. Moreover, the inference time in frame per seconds (FPS) for our MCIT-IG model on Nvidia V100 is ∼29.</p><p>Ablation Study on the Spatial Annotation Need: Here, we study the impact of an instrument localization quality on triplet detection and how the target features can supplement fewer spatial annotations of the instruments for better triplet detection. We compare with ResNet-CAM-YOLOv5 <ref type="bibr" target="#b13">[14]</ref> and Distilled-Swin-YOLO <ref type="bibr" target="#b13">[14]</ref> models which were also trained with bounding box labels. We observed that the triplet detection mAP increases with increasing instrument localization mAP for all the models as shown in Table <ref type="table" target="#tab_1">2</ref>. However, the scale study shows that with lesser bounding box instances, our MCIT-IG model stands tall: outperforming Distilled-Swin-YOLO by +0.86 mAP with ∼9K fewer frames and surpassing ResNet-CAM-YOLOv5 by +1.42 mAP with ∼7K frames to spare. Note that a frame can be annotated with one or more bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies on the Components of MCIT-IG:</head><p>We analyze the modules used in MCIT-IG and report our results in Table <ref type="table" target="#tab_2">3</ref>. Using both ROI and box features provides a complete representation of the instruments that benefits IG, whereas using just ROI or box features misses out on details about instruments hurting the triplet detection performance. We further test the quality of target class embeddings without instrument awareness in MCIT. Results in Table <ref type="table" target="#tab_2">3</ref> indicates that the lack of instrument context hampers the ability of the target class embeddings to capture full range of associations with the triplets. Also, message passing is key in the IG as it allows instrument semantics to propagate to target class embeddings, which helps distinguish interacting pairs from other non-interacting pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with the State-of-the-Art (SOTA) Methods:</head><p>Results in Table <ref type="table" target="#tab_3">4</ref> show that our proposed model outperforms all the existing methods in the CholecTriplet 2022 challenge <ref type="bibr" target="#b13">[14]</ref>, obtained the highest score that would have placed our model 1 st on the challenge leaderboard in all the accessed metrics. Leveraging our transformer modulated target embeddings and graph-based associations, our method shows superior performance in both instrument localization and triplet detection over methods weakly-supervised on binary presence labels and those fully supervised on external bounding box datasets like ours. More details on the challenge methods are provided in <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we propose a fully differentiable two-stage pipeline for triplet detection in laparoscopic cholecystectomy procedures. We introduce a transformerbased method for learning per class embeddings of target anatomical structures in the absence of target instance labels, and an interaction graph that dynamically associates the instrument and target embeddings to detect triplets. We also incorporate a mixed supervision strategy to help train MCIT and IG modules. We show that improving instrument localization has a direct correlation with triplet detection performance. We evaluate our method on the challenge split of the CholecT50 dataset and demonstrate improved performance over the leaderboard.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Model Overview: In Stage 1, MCIT learns instrument-aware target class embeddings. In Stage 2, IG enforces association between instrument instances and target class embeddings and learns verb on the pairwise edge features. Green edges denotes all interactions and blue edges denotes active instrument-target pair with verb v. Red edges denotes no interaction. (Color figure online)</figDesc><graphic coords="3,55,98,54,32,340,18,215,38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on Instrument Localization and Triplet Detection (mAP@0.5 in %).</figDesc><table><row><cell>Method</cell><cell>Instrument Detector</cell><cell cols="4">Instrument localization Triplet detection</cell></row><row><cell></cell><cell></cell><cell>API</cell><cell>ARI</cell><cell cols="2">APIV T ARIV T</cell></row><row><cell cols="3">RDV (Baseline) [13] Deformable DETR [22] 60.1</cell><cell>66.6</cell><cell>6.43</cell><cell>9.50</cell></row><row><cell>MCIT (Ours)</cell><cell cols="2">Deformable DETR [22] 60.1</cell><cell>66.6</cell><cell>6.94</cell><cell>9.80</cell></row><row><cell>MCIT+IG (Ours)</cell><cell cols="2">Deformable DETR [22] 60.1</cell><cell>66.6</cell><cell>7.32</cell><cell>10.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Instrument Localization and Triplet Detection (mAP@0.5 in %) vs number of videos/frames used in training the Instrument Detector.</figDesc><table><row><cell>Method</cell><cell cols="6">#Videos #Frames Instrument localization Triplet detection</cell></row><row><cell></cell><cell></cell><cell></cell><cell>API</cell><cell>ARI</cell><cell cols="2">APIV T ARIV T</cell></row><row><cell>ResNet-CAM-YOLOv5 [14]</cell><cell>51</cell><cell cols="2">∼22000 41.9</cell><cell>49.3</cell><cell>4.49</cell><cell>7.87</cell></row><row><cell>Distilled-Swin-YOLO [14]</cell><cell>33</cell><cell cols="2">∼13000 17.3</cell><cell>30.4</cell><cell>2.74</cell><cell>6.16</cell></row><row><cell>MCIT+IG (Ours)</cell><cell>1</cell><cell>4214</cell><cell>33.5</cell><cell>39.6</cell><cell>3.60</cell><cell>4.95</cell></row><row><cell>MCIT+IG (Ours)</cell><cell>5</cell><cell cols="2">15315 53.1</cell><cell>59.6</cell><cell>5.91</cell><cell>8.73</cell></row><row><cell>MCIT+IG (Ours)</cell><cell>12</cell><cell cols="2">24536 60.1</cell><cell>66.6</cell><cell>7.32</cell><cell>10.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Component-wise performance on Triplet Detection (mAP@0.5 in %).</figDesc><table><row><cell cols="2">ROI Box Graph T oolAwareness APIV T ARIV T</cell></row><row><cell>4.11</cell><cell>5.64</cell></row><row><cell>4.97</cell><cell>6.93</cell></row><row><cell>4.98</cell><cell>7.71</cell></row><row><cell>6.94</cell><cell>9.80</cell></row><row><cell>7.32</cell><cell>10.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison with top methods from CholecTriplet 2022 Challenge<ref type="bibr" target="#b13">[14]</ref>, leaderboard results on https://cholectriplet2022.grand-challenge.org/results.</figDesc><table><row><cell>Instrument</cell><cell>Triplet</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by <rs type="funder">French</rs> state funds managed by the <rs type="funder">ANR</rs> within the <rs type="programName">National AI Chair program</rs> under Grant <rs type="grantNumber">ANR-20-CHIA-0029-01 (Chair AI4ORSafety</rs>) and within the <rs type="programName">Investments for the future program</rs> under Grant <rs type="grantNumber">ANR-10-IAHU-02</rs> (<rs type="affiliation">IHU Strasbourg</rs>). It was also supported by <rs type="funder">BPI France</rs> under reference <rs type="grantNumber">DOS0180017/00</rs> (project <rs type="grantNumber">5G-OR</rs>). It was granted access to the <rs type="institution">HPC resources of Unistra Mesocentre</rs> and <rs type="institution">GENCI-IDRIS</rs> (Grant <rs type="grantNumber">AD011013710</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Y8jddHS">
					<idno type="grant-number">ANR-20-CHIA-0029-01 (Chair AI4ORSafety</idno>
					<orgName type="program" subtype="full">National AI Chair program</orgName>
				</org>
				<org type="funding" xml:id="_HH9Dwfh">
					<idno type="grant-number">ANR-10-IAHU-02</idno>
					<orgName type="program" subtype="full">Investments for the future program</orgName>
				</org>
				<org type="funding" xml:id="_ME5eDDY">
					<idno type="grant-number">DOS0180017/00</idno>
				</org>
				<org type="funding" xml:id="_3JdrfcU">
					<idno type="grant-number">5G-OR</idno>
				</org>
				<org type="funding" xml:id="_wDedesW">
					<idno type="grant-number">AD011013710</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 48.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Bawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03178</idno>
		<title level="m">The saras endoscopic surgeon action detection (ESAD) dataset: challenges and methods</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">MMDetection: open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Tool detection and operative skill assessment in surgical videos using region-based convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>WACV</publisher>
			<biblScope unit="page" from="691" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Surgical process modelling: a review</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lalys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCARS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="495" to="511" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instrument-tissue interaction quintuple detection in surgery videos</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-138" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="399" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Surgical data science for next-generation interventions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="691" to="696" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Artificial intelligence for surgical safety: automatic assessment of the critical view of safety in laparoscopic cholecystectomy using deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Surg</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="955" to="961" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cholectriplet 2021: a benchmark challenge for surgical action triplet recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">102803</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recognition of instrument-tissue interactions in endoscopic videos via action triplets</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_35</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-035" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="364" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Data splits and metrics for method benchmarking on surgical action triplet datasets</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05235</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rendezvous: attention mechanisms for the recognition of surgical action triplets in endoscopic videos</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">102433</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cholectriplet 2022: show me a tool and tell me the triplet -an endoscopic vision challenge for surgical action triplet detection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page">102888</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical modeling and recognition of surgical workflow</title>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feussner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="632" to="641" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rendezvous in time: an attentionbased temporal fusion approach for surgical triplet recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCARS</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1053" to="1059" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Endonet: a deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lió</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CAI4CAI: the rise of contextual artificial intelligence in computer-assisted interventions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="198" to="214" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep graph library: a graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multi-class token transformer for weakly supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4310" to="4319" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DD deformable transformers for end-to-end object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
