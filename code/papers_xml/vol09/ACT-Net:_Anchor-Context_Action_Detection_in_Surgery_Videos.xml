<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACT-Net: Anchor-Context Action Detection in Surgery Videos</title>
				<funder ref="#_GCFUB3e">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Central Research Fund</orgName>
					<orgName type="abbreviated">CRF</orgName>
				</funder>
				<funder ref="#_Rps5bJk #_qPhv4Qn">
					<orgName type="full">General Program of National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_mYdhjr8 #_R7t9yqp">
					<orgName type="full">Shenzhen Stable Support Plan Program</orgName>
				</funder>
				<funder ref="#_xTS5DAb">
					<orgName type="full">Agency for Science, Technology and Research (A*STAR) Advanced Manufacturing and Engineering</orgName>
					<orgName type="abbreviated">AME</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luoying</forename><surname>Hao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Birmingham</orgName>
								<address>
									<settlement>Birmingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Research Institute of Trustworthy Autonomous Systems</orgName>
								<orgName type="institution" key="instit2">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Research Institute of Trustworthy Autonomous Systems</orgName>
								<orgName type="institution" key="instit2">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenjun</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Research Institute of Trustworthy Autonomous Systems</orgName>
								<orgName type="institution" key="instit2">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qun</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Third Medical Center of Chinese PLAGH</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heng</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Research Institute of Trustworthy Autonomous Systems</orgName>
								<orgName type="institution" key="instit2">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Agency for Science, Technology and Research (A*STAR)</orgName>
								<orgName type="institution">Institute of High Performance Computing (IHPC)</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinming</forename><surname>Duan</surname></persName>
							<email>j.duan@bham.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Birmingham</orgName>
								<address>
									<settlement>Birmingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiang</forename><surname>Liu</surname></persName>
							<email>liuj@sustech.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Research Institute of Trustworthy Autonomous Systems</orgName>
								<orgName type="institution" key="instit2">Southern University of Science and Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ACT-Net: Anchor-Context Action Detection in Surgery Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8E96FED268BD58FDC019AC7CB5CEAA58</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_19</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action detection</term>
					<term>Anchor-context</term>
					<term>Conditional diffusion</term>
					<term>Surgical video</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognition and localization of surgical detailed actions is an essential component of developing a context-aware decision support system. However, most existing detection algorithms fail to provide highaccuracy action classes even having their locations, as they do not consider the surgery procedure's regularity in the whole video. This limitation hinders their application. Moreover, implementing the predictions in clinical applications seriously needs to convey model confidence to earn entrustment, which is unexplored in surgical action prediction. In this paper, to accurately detect fine-grained actions that happen at every moment, we propose an anchor-context action detection network (ACT-Net), including an anchor-context detection (ACD) module and a class conditional diffusion (CCD) module, to answer the following questions: 1) where the actions happen; 2) what actions are; 3) how confidence predictions are. Specifically, the proposed ACD module spatially and temporally highlights the regions interacting with the extracted anchor in surgery video, which outputs action location and its class distribution based on anchor-context interactions. Considering the full distribution of action classes in videos, the CCD module adopts a denoising diffusion-based generative model conditioned on our ACD estimator to further reconstruct accurately the action predictions. Moreover, we utilize the stochastic nature of the diffusion model outputs to access model confidence for each prediction. Our method reports the state-of-the-art performance, with improvements of 4.0% mAP against baseline on the surgical video dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Surgery is often an effective therapy that can alleviate disabilities and reduce the risk of death from common conditions <ref type="bibr" target="#b16">[17]</ref>. While surgical procedures are intended to save lives, errors within the surgery may bring great risks to the patient and even cause sequelae <ref type="bibr" target="#b17">[18]</ref>, which emphasizes the development of a computer-assisted system. A context-aware assistant system for surgery can not only decrease intraoperative adverse events, and enhance the quality of interventional healthcare <ref type="bibr" target="#b27">[28]</ref>, but also contribute to surgeon training, and assist procedure planning and retrospective analysis <ref type="bibr" target="#b8">[9]</ref>.</p><p>Designing intelligent assistance systems for operating rooms requires an understanding of surgical scenes and procedures <ref type="bibr" target="#b19">[20]</ref>. Most current works pay attention to phase and step recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref>, which is to get the major types of events that occurred during the surgery. They merely provided very coarse descriptions of scenes. As the granularity of action increases, the clinical utility becomes more valuable in providing an accurate depiction of detailed motion <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref>. Recent studies focus on fine-grained action recognition by modelling action as a group of the instrument, its role, and its target anatomy and capturing their associations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref>. Recognizing targets in different methods is dependent on different surgical scenarios and it also significantly increases the complexity and time consumption for anatomy annotation <ref type="bibr" target="#b30">[30]</ref>. In addition, although most existing methods can provide accurate action positions, the predicted action class is often inaccurate. Moreover, they do not provide any information about the reliability of their output, which is a key requirement for integrating into assistance systems of surgery <ref type="bibr" target="#b10">[11]</ref>. Thus, we propose a reliable surgical action detection method in this paper, with high-accuracy action predictions and their confidence.</p><p>Mistrust is a major barrier to deep-learning-based predictions applied to clinical implementation <ref type="bibr" target="#b13">[14]</ref>. Existing works measuring the model uncertainty <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> often need several-time re-evaluations, and store multiple sets of weights. It is hard for them to apply to surgery assistance applications to get confidence for each prediction directly <ref type="bibr" target="#b9">[10]</ref>, and they are limited to improving prediction performance. Conditional diffusion-based generative models have received significant attention due to their ability to accurately recover the full distribution of data guided by conditions from the perspective of diffusion probabilistic models <ref type="bibr" target="#b23">[24]</ref>. However, they focus on generating high-resolution photo-realistic images. Instead, after observing our surgical video dataset, our conditional diffusion model aims to reconstruct accurately class distribution. We also access the estimation of confidence with the stochastic nature of the diffusion model.</p><p>Here, to predict accurately micro-action (fine-grained action) categories happening every moment, we achieve it with two modules. Specifically, a novel anchor-context module for action detection is proposed to highlight the spatiotemporal regions that are interacted with the anchors (we extract instrument features as anchors), which includes surrounding tissues and movement information. Then, with the constraints of class distributions and the surgical videos, we propose a conditional diffusion model to cover the whole distribution of our data and to accurately reconstruct new predictions based on full learning. Furthermore, our class conditional diffusion model also accesses uncertainty for each prediction, through the stochasticity of outputs. We summarize our main contributions as follows: 1) We develop an anchorcontext action detection network (ACTNet), including an anchor-context detection (ACD) module and a class conditional diffusion (CCD) module, which combines three tasks: i) where actions locate; ii) what actions are; iii) how confident our model is about predictions. 2) For ACD module, we develop a spatiotemporal anchor interaction block (STAB) to spatially and temporally highlight the context related to the extracted anchor, which provides micro-action location and initial class. 3) By conditioning on the full distribution of action classes in the surgical videos, our proposed class conditional diffusion (CCD) model reconstructs better class prototypes in a stochastic fashion, to provide a more accurate estimations and push the assessment of the model confidence in its predictions. 4) We carry out comparison and ablation study experiments to demonstrate the effectiveness of our proposed algorithm based on cataract surgery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The overall framework of our proposed ACTNet for reliable action detection is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Based on a video frame sequence, the ACD module extracts anchor features and aggregates the spatio-temporal interactions with anchor features by proposed STAB, which generates action locations and initial action class distributions. Then considering the full distribution of action classes in surgical videos, we use the CCD module to refine the action class predictions and access confidence estimations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Our ACD Module</head><p>Anchor Extraction: Assuming a video X with T frames, denoted as X = {x t } T t=1 , where x t is the t-th frame of the video. The task of this work is to estimate all potential locations and classes P = {box n , c n } N n=1 for action instances contained in video X, where box n is the position of the n-th action happened, c n is the action class of n-th action, and N is the number of action instances. For video representation, this work tries to encode the original videos into features based on the backbone ResNet50 <ref type="bibr" target="#b4">[5]</ref> network to get each frame's feature F = {f t } T t=1 . In surgical videos, the instruments, as action subjects, are significant to recognize the action. For instrument detection, it is very important but not very complicated. Existing excellent object detection method like Faster R-CNN <ref type="bibr" target="#b21">[22]</ref> is enough to obtain results with high accuracy. After getting the detected instrument anchors, RoIAlign is applied to extract the instrument features from frame features. The instrument features are denoted as I = {i t } T t=1 . Since multiple instruments exist in surgeries, our action detection needs to solve the problem that related or disparate concurrent actions often lead to wrong predictions. Thus, in this paper, we propose to provide action location and class considering the spatio-temporal anchor interactions in the surgical videos, based on STAB.</p><p>Spatio-Temporal Action interaction Block (STAB): For several actions like pushing, pulling, and cutting, there is no difference just inferred from the local region in one frame. Thus we propose STAB to utilize spatial and temporal interactions with an anchor to improve the prediction accuracy of the action class, which finds actions with strong logical links to provide an accurate class. The structure of STAB is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We introduce spatial and temporal interactions respectively in the following.</p><p>For spatial interaction: The instrument feature i t acts as the anchor. In order to improve the anchor features, the module has the ability to select value features that are highly active with the anchor features and merge them. The formulation is defined as:</p><formula xml:id="formula_0">a t = 1 C(ft) j∈Sj h(f tj , i t )g(f tj</formula><p>), where j is the index that enumerates all possible positions of f t . A pairwise function h(•) computes the relationship such as affinity between i t and all f tj . In this work, dot-product is employed to compute the similarity. The unary function g(f tj ) computes a representation of the input signal at the position j. The response is normalized by a factor</p><formula xml:id="formula_1">C(f t ) = j∈Sj h(f tj , i t )</formula><p>. S j represents the set of all positions j. Through the formulation, the output a t obtains more information from the positions related to the instrument and catches interactions in space for the actions.</p><p>For temporal interaction: We build memory features consisting of features in consecutive frames:</p><formula xml:id="formula_2">m t = [f t-L , ..., f t-1 ].</formula><p>To effectively model temporal interactions of the anchor, the network offers a powerful tool for capturing the complex and dynamic dependencies that exist between elements in sequential data and anchors. Same with the spatial interaction, we take i t as an anchor and calculate the interactions between the memory features and the anchor. The formulation  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CCD Module for Reliable Action Detection</head><p>Since the surgical procedures follow regularity, we propose a CCD module to reconstruct the action class predictions considering the full distribution of action classes in videos. The diffusion conditioned on the action classes and surgical videos is adopted in our paper. Let y 0 ∈ R n be a sample from our data distribution. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, a diffusion model specified in continuous time is a generative model with latent y t , obeying a forward process q t (y t |y t-1 ) starting at data y 0 <ref type="bibr" target="#b5">[6]</ref>. y 0 indicates a one-hot encoded label vector. We treat each one-hot label as a class prototype, i.e., we assume a continuous data and state space, which enables us to keep the Gaussian diffusion model framework <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>The forward process and reverse process of unconditional diffusion are provided in the supplementary material.</p><p>Here, for the diffusion model optimization can be better guided by meaningful information, we integrate the ACD and our surgical video data as priors or constraints in the diffusion training process. We design a conditional diffusion model pθ (y t-1 |y t , x) that is conditioned on an additional latent variable x. Specifically, the model pθ (y t-1 |y t , x) is built to approximate the corresponding tractable ground-truth denoising transition step pt (y t-1 |y t , y 0 , x). We specify the reverse process with conditional distributions as <ref type="bibr" target="#b20">[21]</ref>: pt (y t-1 |y t , y 0 , x) = pt (y t-1 |y t , y 0 , f ϕ (x)) = N y t-1 ; μ (y t , y 0 , f ϕ (x)) , βt I where μ (y t , y 0 , f ϕ (x)) and βt are described in supplementary material. f ϕ (x) is the prior knowledge of the relation between x and y 0 , i.e., the ACD module pre-trained with our surgical video dataset. The x indicates the input surgical video frames. Since ground-truth step pt (y t-1 |y t , y 0 , x) cannot get directly, the model pθ (y t-1 |y t , x) are trained by following loss function for estimating θ to approximate the ground truth:</p><formula xml:id="formula_3">L(θ) = E t,y0, ,x -θ (x, √ ᾱt y 0 + √ 1 -ᾱt + (1 - √ ᾱt )f ϕ (x), f ϕ (x), t) 2</formula><p>where α t := 1β t , ᾱt := t s=1 (1β s ), ∼ N (0, 1) and θ (•) estimates using a time-conditional network parameterized by θ. β t is a constant hyperparameter.</p><p>To produce model confidence for each action instance, we mainly calculate the prediction interval width (IW). Specifically, we first sample N class prototype reconstruction with the trained diffusion model. Then calculate the IW between the 2.5 th and 97.5 th percentiles of the N reconstructed values for all test classes. Compared with traditional classifiers to get deterministic outputs, the denoising diffusion model is a preferable modelling choice due to its ability to produce stochastic outputs, which enables confidence generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>Cataract Surgical Video Dataset: To perform reliable action detection, we build a cataract surgical video dataset. Cataract surgery is a procedure to remove the lens of the eyes and, in most cases, replace it with an artificial lens. The dataset consists of 20 videos with a frame rate of 1 fps (a total of 17511 frames and 28426 action instances). Under the direction of ophthalmologists, each video is labelled frame by frame with the categories and locations of the actions. 49 types of action bounding boxes as well as class labels are included in our dataset. The surgical video dataset is randomly split into a training set with 15 videos (13583 frames) and a testing set with 5 videos (3928 frames).</p><p>Implementation Details: The proposed architecture is implemented using the publicly available Pytorch Library. A model with ResNet50 backbone from Faster R-CNN-benchmark <ref type="bibr" target="#b22">[23]</ref> is adopted for our instrument anchor detection. In STAB, we use ten adjacent frames. During inference, detected anchor boxes with a confidence score larger than 0.8 are used. More implementation details are listed in the supplementary material. The performances are evaluated with official metric frame level mean average precision (mAP) at IoU = 0.1, 0.3, and 0.5, respectively, obtaining figures in the following named mAP 10 , mAP 30 and mAP 50 with their mean mAP mean .</p><p>Method Comparison: In order to demonstrate the superiority of the proposed method for surgical action detection, we carry out a comprehensive comparison between the proposed method and the following state-of-the-art methods: 1) single-stage algorithms, including the Single Shot Detector (SSD) <ref type="bibr" target="#b15">[16]</ref>, SSDLite <ref type="bibr" target="#b24">[25]</ref> and RetinaNet <ref type="bibr" target="#b11">[12]</ref>. 2) two-stage algorithms, including Faster R-CNN <ref type="bibr" target="#b22">[23]</ref>, Mask R-CNN <ref type="bibr" target="#b3">[4]</ref>, Dynamic R-CNN <ref type="bibr" target="#b28">[29]</ref> and OA-MIL <ref type="bibr" target="#b14">[15]</ref>. The data presented in Table <ref type="table" target="#tab_0">1</ref> clearly demonstrate that our method outperforms other approaches, irrespective of the IoU threshold being set to 0.1, 0.3, 0.5, or the average values. Notably, the results obtained after incorporating diffusion even surpass Faster R-CNN by 2.5% and baseline by 4.0% in terms of average mAP. This finding provides compelling evidence for the efficacy of our method in integrating spatiotemporal interactive information under the guidance of anchors and leveraging diffusion to optimize the category distribution. The quantitative results further corroborate the effectiveness of our approach in Fig. <ref type="figure" target="#fig_4">3</ref>, which shows that our model does not only improve the performance of the baseline models but also localizes accurately the regions of interest of the actions. More results are listed in the material.</p><p>Ablation Study: To validate the effectiveness of our ACTNet, we have done some ablation studies. We train and test the model with spatial interaction, temporal interaction, spatio-temporal interaction (STAB), and finally together with our CCD model. The testing results are shown in Fig. <ref type="figure" target="#fig_4">3</ref> and Table <ref type="table" target="#tab_0">1</ref>. For our backbone, it is achieved by concatenating the anchor features through RoIAlign and the corresponding frame features to get the detected action classes.</p><p>The results reveal that the spatial and temporal interactions for instruments can provide useful information to detect the actions. What's more, spatial interaction has slightly better performance than temporal interaction. It may be led by the number of spatially related action categories being slightly more than that of temporally related action categories. It is worth noting that spatial interaction and temporal interaction can be enhanced by each other and achieve optimal performance. After being enhanced by the diffusion model conditioned on our obtained class distributions and video frames, we get optimal performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence Analysis:</head><p>To analyze the model confidence, we take the best prediction for each instance to calculate the instance accuracy. We can observe   from Table <ref type="table" target="#tab_2">2</ref> across the test set, the mean_IW of the class label among correctly classified instances by ACTNet is significantly narrower compared to that of incorrectly classified instances. This observation indicates that the model is more confident in its accurate predictions and is more likely to make errors when its predictions are vague. Furthermore, upon comparing the mean_IW at the true class level, we find that a more precise class tends to exhibit a larger disparity between the correct and incorrect predictions.  confidence estimations for some samples. We can see the correct prediction gets smaller IW values compared with the incorrect one (The rightmost figure in column (c)), which means it has more uncertainty for the incorrect prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we propose a conditional diffusion-based anchor-context spatiotemporal action detection network (ACTNet) to achieve recognition and localization of every occurring action in the surgical scenes. ACTNet improves the accuracy of the predicted action class from two considerations, including spatiotemporal interactions with anchors by the proposed STAB and full distribution of action classes by class conditional diffusion (CCD) module, which also provides uncertainty in surgical scenes. Experiments based on cataract surgery demonstrate the effectiveness of our method. Overall, the proposed ACTNet presents a promising avenue for improving the accuracy and reliability of action detection in surgical scenes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The pipeline of our ACTNet includes ACD and CCD modules.</figDesc><graphic coords="3,52,80,59,03,316,12,96,49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the class conditional diffusion (CCD) model.</figDesc><graphic coords="5,55,38,54,59,316,45,102,22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>is defined as: b t = 1 C(mt) j∈Tj h(m tj , i t )g(m tj ), where T j refers to the set of all possible positions along the time series in the range of L. In this way, temporal interactions with anchors are obtained. Then a global average pooling is carried out on the spatial and temporal outputs. Action localizations and initial action class distributions are produced based on the fully-connected classifier layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visualization on cataract dataset. We choose different actions to show the results of (a) Faster R-CNN, (b) OA_MIL, and (c) our ACTNet. For each example, we show the ground-truth (Blue), right predictions (Green) and wrong predictions (Red). The actions are labelled from left to right. IW values (multiplied by 100) mean prediction interval width to show the level of confidence. (Color figure online)</figDesc><graphic coords="8,56,46,54,20,339,43,203,50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 3 also shows the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Methods comparison and ablation study on cataract video dataset.</figDesc><table><row><cell>Methods</cell><cell cols="4">mAP10 mAP30 mAP50 mAPmean</cell></row><row><cell>Faster R-CNN [23]</cell><cell>0.388</cell><cell>0.384</cell><cell>0.371</cell><cell>0.381</cell></row><row><cell>SSD [16]</cell><cell>0.360</cell><cell>0.358</cell><cell>0.350</cell><cell>0.356</cell></row><row><cell>RetinaNet [12]</cell><cell>0.358</cell><cell>0.356</cell><cell>0.347</cell><cell>0.354</cell></row><row><cell>Mask R-CNN [4]</cell><cell>0.375</cell><cell>0.373</cell><cell>0.363</cell><cell>0.370</cell></row><row><cell>SSDlite [25]</cell><cell>0.305</cell><cell>0.304</cell><cell>0.298</cell><cell>0.302</cell></row><row><cell cols="2">Dynamic R-CNN [29] 0.315</cell><cell>0.310</cell><cell>0.296</cell><cell>0.307</cell></row><row><cell>OA-MIL [15]</cell><cell>0.395</cell><cell>0.394</cell><cell>0.378</cell><cell>0.389</cell></row><row><cell>backbone</cell><cell>0.373</cell><cell>0.365</cell><cell>0.360</cell><cell>0.366</cell></row><row><cell>+temporal</cell><cell>0.385</cell><cell>0.378</cell><cell>0.372</cell><cell>0.378</cell></row><row><cell>+spatial</cell><cell>0.394</cell><cell>0.385</cell><cell>0.377</cell><cell>0.385</cell></row><row><cell>+STAB</cell><cell>0.400</cell><cell>0.393</cell><cell>0.385</cell><cell>0.393</cell></row><row><cell>+CCD (ACTNet)</cell><cell cols="4">0.415 0.406 0.397 0.406</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The mean_IW (multiplied by 100) results from our CCD module.</figDesc><table><row><cell>Class</cell><cell cols="4">Instance Accuracy Mean_IW (Correct) Mean_IW (Incorrect)</cell></row><row><cell>grasp conjunctiva</cell><cell>487</cell><cell>0.702</cell><cell>0.91 (342)</cell><cell>9.78 (145)</cell></row><row><cell>aspirate lens cortex</cell><cell>168</cell><cell>0.613</cell><cell>1.37 (103)</cell><cell>15.82 (65)</cell></row><row><cell>chop lens nucleus</cell><cell>652</cell><cell>0.607</cell><cell>0.54 (396)</cell><cell>9.73 (256)</cell></row><row><cell cols="2">polish intraocular lens 222</cell><cell>0.572</cell><cell>0.90 (127)</cell><cell>8.48 (95)</cell></row><row><cell cols="2">aspirate lens nucleus 621</cell><cell>0.554</cell><cell>0.76 (344)</cell><cell>10.30 (277)</cell></row><row><cell>inject viscoelastic</cell><cell>112</cell><cell>0.536</cell><cell>2.17 (60)</cell><cell>9.14 (52)</cell></row><row><cell>Remove lens cortex</cell><cell>174</cell><cell>0.471</cell><cell>0.42 (82)</cell><cell>5.84 (92)</cell></row><row><cell>forceps null</cell><cell>280</cell><cell>0.464</cell><cell>2.67 (130)</cell><cell>8.38 (150)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>ACT-Net: Anchor-Context Action Detection in Surgery Videos</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported in part by <rs type="funder">General Program of National Natural Science Foundation of China</rs> (<rs type="grantNumber">82272086</rs> and <rs type="grantNumber">82102189</rs>), <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> (<rs type="grantNumber">2021A1515012195</rs>), <rs type="funder">Shenzhen Stable Support Plan Program</rs> (<rs type="grantNumber">20220815111736001</rs> and <rs type="grantNumber">20200925174052004</rs>), and <rs type="funder">Agency for Science, Technology and Research (A*STAR) Advanced Manufacturing and Engineering (AME) Programmatic Fund</rs> (<rs type="grantNumber">A20H4b0141</rs>) and <rs type="funder">Central Research Fund (CRF)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Rps5bJk">
					<idno type="grant-number">82272086</idno>
				</org>
				<org type="funding" xml:id="_qPhv4Qn">
					<idno type="grant-number">82102189</idno>
				</org>
				<org type="funding" xml:id="_GCFUB3e">
					<idno type="grant-number">2021A1515012195</idno>
				</org>
				<org type="funding" xml:id="_mYdhjr8">
					<idno type="grant-number">20220815111736001</idno>
				</org>
				<org type="funding" xml:id="_R7t9yqp">
					<idno type="grant-number">20200925174052004</idno>
				</org>
				<org type="funding" xml:id="_xTS5DAb">
					<idno type="grant-number">A20H4b0141</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">CARD: classification and regression diffusion models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07275</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computer vision analysis of intraoperative video: automated recognition of operative steps in laparoscopic sleeve gastrectomy</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Surg</title>
		<imprint>
			<biblScope unit="volume">270</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">414</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning and reasoning with the graph structure representation in robotic surgery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Seenivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_60</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-0_60" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020, Part III</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Surgical process modelling: a review</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lalys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11548-013-0940-5</idno>
		<ptr target="https://doi.org/10.1007/s11548-013-0940-5" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="495" to="511" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Localization uncertainty estimation for anchor-free object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25085-9_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25085-9_2" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022, Part VIII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13808</biblScope>
			<biblScope unit="page" from="27" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Leveraging uncertainty information from deep neural networks for disease detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Leibig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Allken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ayhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Instrument-tissue interaction quintuple detection in surgery videos</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1_38" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part VII</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="399" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human-automation collaboration in dynamic mission planning: a challenge requiring an ecological approach</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Linegang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Factors and Ergonomics Society Annual Meeting</title>
		<meeting>the Human Factors and Ergonomics Society Annual Meeting<address><addrLine>Los Angeles</addrLine></address></meeting>
		<imprint>
			<publisher>SAGE Publications Sage</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="2482" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust object detection with inaccurate bounding boxes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20080-9_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20080-9_4" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022, Part X</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13670</biblScope>
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46448-0_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46448-0_2" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016, Part I</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9905</biblScope>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A clinical perspective study on the compliance of surgical safety checklist in all surgical procedures done in operation theatres, in a teaching hospital, Ethiopia, 2021: a clinical perspective study</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Mersh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Melesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Chekol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Med. Surg</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">102702</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Global burden of postoperative death</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nepogodiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="page">401</biblScope>
			<date type="published" when="2019">10170. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rendezvous: attention mechanisms for the recognition of surgical action triplets in endoscopic videos</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">102433</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Machine and deep learning for workflow recognition during surgery</title>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minim. Invasive Ther. Allied Technol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="82" to="90" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">DiffuseVAE: efficient, controllable and high-fidelity generation from low-dimensional latents</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.00308</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10674" to="10685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MobileNetV2: inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Global-reasoned multi-task learning model for surgical scene understanding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seenivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitheran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1109/LRA.2022.3146544</idno>
		<ptr target="https://doi.org/10.1109/LRA.2022.3146544" />
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3858" to="3865" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">EndoNet: a deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CAI4CAI: the rise of contextual artificial intelligence in computer-assisted interventions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="198" to="214" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic R-CNN: towards high quality object detection via dynamic training</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2020, Part XV</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12360</biblScope>
			<biblScope unit="page" from="260" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58555-6_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58555-6_16" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic keyframe detection for critical actions from the experience of expert surgeons</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8049" to="8056" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
