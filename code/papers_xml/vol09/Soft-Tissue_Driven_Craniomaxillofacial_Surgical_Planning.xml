<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Soft-Tissue Driven Craniomaxillofacial Surgical Planning</title>
				<funder ref="#_bmjkNdf #_pYBpv9j #_DZrDFrU">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xi</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Daeseung</forename><surname>Kim</surname></persName>
							<email>dkim@houstonmethodist.org</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Oral and Maxillofacial Surgery</orgName>
								<orgName type="institution">Houston Methodist Research Institute</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuanang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianshu</forename><surname>Kuang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Oral and Maxillofacial Surgery</orgName>
								<orgName type="institution">Houston Methodist Research Institute</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nathan</forename><surname>Lampen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hannah</forename><forename type="middle">H</forename><surname>Deng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Oral and Maxillofacial Surgery</orgName>
								<orgName type="institution">Houston Methodist Research Institute</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jaime</forename><surname>Gateno</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Oral and Maxillofacial Surgery</orgName>
								<orgName type="institution">Houston Methodist Research Institute</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">A K</forename><surname>Liebschner</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Neurosurgery</orgName>
								<orgName type="institution">Baylor College of Medicine</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Xia</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Oral and Maxillofacial Surgery</orgName>
								<orgName type="institution">Houston Methodist Research Institute</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pingkun</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Engineering and Center for Biotechnology and Interdisciplinary Studies</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Soft-Tissue Driven Craniomaxillofacial Surgical Planning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AC20585D79B8DBB858B3F70ABC65BD44</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_18</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Learning</term>
					<term>Surgical Planning</term>
					<term>Bony Movement</term>
					<term>Bony Planner</term>
					<term>Facial Simulator</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In CMF surgery, the planning of bony movement to achieve a desired facial outcome is a challenging task. Current bone driven approaches focus on normalizing the bone with the expectation that the facial appearance will be corrected accordingly. However, due to the complex non-linear relationship between bony structure and facial softtissue, such bone-driven methods are insufficient to correct facial deformities. Despite efforts to simulate facial changes resulting from bony movement, surgical planning still relies on iterative revisions and educated guesses. To address these issues, we propose a soft-tissue driven framework that can automatically create and verify surgical plans. Our framework consists of a bony planner network that estimates the bony movements required to achieve the desired facial outcome and a facial simulator network that can simulate the possible facial changes resulting from the estimated bony movement plans. By combining these two models, we can verify and determine the final bony movement required for planning. The proposed framework was evaluated using a clinical dataset, and our experimental results demonstrate that the soft-tissue driven approach greatly improves the accuracy and efficacy of surgical planning when compared to the conventional bone-driven approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Craniomaxillofacial (CMF) deformities can affect the skull, jaws, and midface. When the primary cause of disfigurement lies in the skeleton, surgeons cut the bones into pieces and reposition them to restore normal alignment <ref type="bibr" target="#b12">[12]</ref>. In this context, the focus is on correcting bone deformities, as it is anticipated that the restoration of normal facial appearance will follow automatically. Consequently, it is customary to initiate the surgical planning process by estimating the positions of normal bones. The latter has given rise to bone-driven approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b14">14]</ref>. For example, methods based on sparse representation <ref type="bibr" target="#b13">[13]</ref> and deep learning <ref type="bibr" target="#b15">[15]</ref> have been proposed to estimate the bony shape that may lead to an acceptable facial appearance.</p><p>However, the current bone-driven methods have a major limitation, subjecting to the complex and nonlinear relationship between the bones and the draping soft-tissues. Surgeons estimate the required bony movement through trial and error, while computer-aided surgical simulation (CASS) software <ref type="bibr" target="#b14">[14]</ref> simulates the effect on facial tissues resulting from the proposed movements. Correcting the bone deformity may not completely address the soft-tissue disfigurement. The problem can be mitigated by iteratively revising the bony movement plan and simulating the corresponding soft-tissue changes. However, this iterative planning revision is inherently time-consuming, especially when the facial change simulation is performed using computationally expensive techniques such as the finite-element method (FEM) <ref type="bibr" target="#b4">[4]</ref>. Efforts have been made to accelerate facial change prediction using deep learning algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b8">8]</ref>, which, however, do not change the iterative nature of the bone-driven approaches.</p><p>To address the above challenge, this paper proposes a novel soft-tissue driven surgical planning framework. Instead of simulating facial tissue under the guessed movement, our approach directly aims at a desired facial appearance and then determines the optimal bony movements required to achieve such an appearance without the need for iterative revisions. Unlike the bone-driven methods, this soft-tissue driven framework eliminates the need for surgeons to make educated guesses about bony movement, significantly improving the efficiency of the surgical planning process. Our proposed framework consists of two main components, the Bony Planner (BP) and the Facial Simulator (FS). The BP estimates the possible bony movement plans (bony plans) required to achieve the desired facial appearance change, while the FS verifies the effectiveness of the estimated plans by simulating the facial appearance based on the bony plans. Without the intervention of clinicians, the BP automatically creates the most clinically feasible surgical plan that achieves the desired facial appearance.</p><p>The main contributions of our work are as follows. 1) This is the first softtissue driven approach for CMF surgical planning, which can substantially reduce the planning time by removing the need for repetitive guessing bony movement.</p><p>2) We develop a deep learning model as the bony planner, which can estimate the underlying bony movement needed for changing a facial appearance into a targeted one. 3) The developed FS module can qualitatively assess the effect of surgical plans on facial appearance, for virtual validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Figure <ref type="figure">1</ref> shows an overview of the proposed framework, which consists of two primary modules. The first module is a BP network that plans bony movement based on the desired facial outcome and the given preoperative facial and bony surfaces. The second module is the FS network that simulates corresponding postoperative facial appearances by applying the estimated bony plans. Instead of providing one single bony plan, the BP will estimate multiple plans because not all the generated plans may result in the desired clinical effect. The FS then simulates facial appearances by using those plans and chooses the plan leading to the facial appearance closest to the desired target. The two models are deployed together to determine and confirm the final bony plan. Below we first present the details of BP and FS modules, then we introduce how they are deployed together for inference. Fig. <ref type="figure">1</ref>. Overview of the proposed soft-tissue driven framework. The framework is composed of two main components: the creation of candidate bony plans using BP, and the simulation of facial outcomes following the plans using FS. Finally, the facial outcomes are compared with the desired face to select the final bony plan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bony Planner (BP)</head><p>Data Preprocessing: The bony plan is created by the BP network using preoperative facial F pre , bony surface B pre , and desired facial surface F des . The goal of the BP network is to convert the desired facial change from F pre to F des into rigid bony movements, denoted as T S for each bony segment S, as required for surgical planning. However, it is very challenging to directly estimate the rigid bone transformation from the facial difference. Therefore, we first estimate the non-rigid bony movement vector field and then convert that into the rigid transformations for each bone segment. Figure <ref type="figure" target="#fig_0">2</ref> illustrates the BP module and the details are provided as follows. For computational efficiency, pre-facial point set P Fpre , pre-bony point set P Bpre , desired point set P F des are subsampled from the pre-facial/bony and desired facial surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-rigid Bony Movement Vector Estimation:</head><p>We adopt the Attentive Correspondence assisted Movement Transformation network (ACMT-Net) <ref type="bibr" target="#b1">[2]</ref>, which was originally proposed for facial tissue simulation, to estimate the pointwise bony displacements to acquire F des . This method is capable of effectively computing the movement of individual bony points by capturing the relationship between facial points through learned affinity. In the network, point-wise facial features (F Fpre ) and bony features (F Bpre ) are extracted from P Fpre and P Bpre . We used a pair of modified PointNet++ modules where the classification layers were removed and a 1D convolution layer was added at the end to project F Fpre and F Bpre into the same lower dimensions <ref type="bibr" target="#b10">[10]</ref>. A correlation matrix R is established by computing the normalized dot product between F Fpre and F Bpre to evaluate the relationship between the facial and bony surfaces:</p><formula xml:id="formula_0">R = F Bpre T F Fpre N Fpre ,<label>(1)</label></formula><p>where N Fpre denotes the number of facial points P Fpre . On the other hand, desired facial movement V F is computed by subtracting P F des and P Fpre . V F is then concatenated with P Fpre and fed into a 1D convolution layer to encode the movement information. Then the movement feature of each bony point is estimtated by the normalized summary of facial features using R. Finally, the transformed bony movement features are decoded into movement vectors after being fed into one 1D convolution layer and normalization:</p><formula xml:id="formula_1">V B = ϕ(θ([P Fpre , V F ]R). (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>Rigid Bony Movement Regression: Upon obtaining the estimated pointwise bony movements, they are added to the corresponding bony points, resulting in a non-rigidly transformed bony point set denoted as P B pdt . The resulting points are grouped based on their respective bony segments, with point sets P Spre and P S pdt representing each bony segment S before and after movement, respectively. To fit the movement between P Spre and P S pdt , we estimate the rigid transformations [R S , T S ] by minimizing the mean square error as follows:</p><formula xml:id="formula_3">E(R S , T S ) = 1 N n i (R S P i Spre + T S -P i S pdt ) 2 , (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where i and N represent the i-th point and the total number of points in P Spre , respectively. First, we define the centroids of P Spre and P S pdt to be P Spre and P S pdt . The cross-covariance matrix H can be computed as</p><formula xml:id="formula_5">H = N i=1 (P Spre -P Spre )(P S pdt -P S pdt )<label>(4)</label></formula><p>Then we can use singular value decomposition (SVD) to decompose</p><formula xml:id="formula_6">H = USV T , (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>then the alignment minimizing E(R S , T S ) can be solved by</p><formula xml:id="formula_8">R S = V U T , T S = -R S P Spre + P S pdt .<label>(6)</label></formula><p>Finally, the rigid transformation matrices are applied to their corresponding bone segments for virtual planning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Facial Simulator (FS)</head><p>While the bony planner can estimate bony plans and we can compare them with ground truth. However, the complex relationship between bone and face makes it unknown whether adopting the bony plan will result in the desired facial outcome or not. To evaluate the effectiveness of the BP network in simulating facial soft tissue, an FS is developed to simulate the facial outcome following the estimated bony plans. For facial simulation, we employ the ACMT-Net, which takes P Fpre , P Bpre , and P B pdt as input and predicts the point-wise facial movement vector V F . The movement vector of all vertices in the facial surface is estimated by interpolating V F . The simulated facial surface F pdt is then reconstructed by adding the predicted movement vectors to the vertices of F pre .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self-verified Virtual Planning</head><p>To generate a set of potential bony plans, we randomly perturbed the surfaces by flipping and translating them up to 10mm in three directions during inference. We repeated this process 10 times in our work. After estimation, the bony surfaces were re-localized to their original position prior to perturbation. Sequentially, the FS module generated a simulated facial appearance for each bony plan estimated from the BP module, serving two purposes. Firstly, it verified the feasibility of the bony plan through facial appearance. Secondly, it allowed us to evaluate the facial outcomes of different bony plans. The simulated facial surfaces were compared with the desired facial surface, and the final plan was selected based on the similarity of the resulting facial outcome. This process verified the efficacy of the selected plan for achieving the desired facial outcome. 3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We employed a five-fold cross-validation technique to evaluate the performance of the proposed network using 34 sets of patient CT data. We partitioned the data into five groups with {7, 7, 7, 7, 6} sets of data, respectively. During each round of validation, four of these groups (folds) were used for training and the remaining group was used for testing. The CT scans were randomly selected from our digital archive of patients who had undergone double-jaw orthognathic surgery. To obtain the necessary data, we employed a semi-automatic method to segment the facial and bony surface from the CT images <ref type="bibr" target="#b6">[6]</ref>. Then we transformed the segmentation into surface meshes using the Marching Cube approach <ref type="bibr" target="#b7">[7]</ref>. To retrospectively recreate the surgical plan that could "achieve" the actual postoperative outcomes, we registered the postoperative facial and bony surfaces to their respective preoperative surfaces based on surgically unaltered bony volumes, i.e., cranium, and utilized them as a roadmap <ref type="bibr" target="#b8">[8]</ref>. To establish the surgical plan to achieve actual postoperative outcomes, virtual osteotomies were first reperformed on the preoperative bones to create bony segments, including LeFort 1 (LF), distal (DI), right proximal (RP), and left proximal (LP). The movement of the bony segments, which represents the surgical plan to achieve the actual postoperative outcomes, was retrospectively established by manually registering each bony segment to the postoperative roadmap, which served as the ground truth in the evaluation process. We rigidly registered the segmented bony and facial surfaces to their respective bony and facial templates to align different subjects. Additionally, we cropped the facial surfaces to retain only the regions of interest for CMF surgery. In this study, we assume the postoperative face is the desired face. For efficient training, we subsampled 4096 points from the facial and bony surfaces, respectively, with 1024 points for each bony segment. To augment the data, we randomly flipped the point sets symmetrically and translated them along three directions within a range of 10 mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation and Evaluation Methods</head><p>To compare our approach, we implemented the state-of-the-art bone-driven approach, i.e., deformation network (DefNet) <ref type="bibr" target="#b15">[15]</ref>, which takes the point coordinates and their normal vectors as input and generates the displacement vectors to deform the preoperative bones. Our method has two variations: BP and BP+FS. BP estimates only one plan, while BP+FS selects the final plan based on the simulated facial outcomes of our facial simulator. The PointNet++ networks utilized in both BP and FS are comprised of four feature-encoding blocks and four feature-decoding blocks. The output dimensions for each block are 128, 256, 512, 1024, 512, 256, 128, and 128, respectively, and the output point numbers of the modules are 1024, 512, 256, 64, 256, 512, 1024, and 4096, sequentially. We used the Adam optimizer with a learning rate of 0.001, Beta1 of 0.9, and Beta2 of 0.999 to train both the BP and FS networks. For data augmentation, both facial and bone data are subjected to the same random flipping and translation, ensuring that the relative position and scale of facial changes and bony movements remain unchanged. The models were trained for 500 epochs with a batch size of 4 and MSE loss was used, after which the models were used for evaluation. The models were trained on an NVIDIA DGX-1 deep learning server with eight V100 GPUs. The prediction accuracy of the soft-tissue driven approach was evaluated quantitatively and qualitatively by comparing it with DefNet. Then quantitative evaluation was to assess the accuracy using the mean absolute error (MAE) between the predicted bony surfaces and the ground truth. For detailed evaluation, MAE was also separately calculated for each bony segment, including LF, DI, RP, and LP. Statistical significance was determined using the Wilcoxon signed-rank test to compare the results obtained by different methods <ref type="bibr" target="#b11">[11]</ref>. Qualitative evaluation was carried out by directly comparing the bony surfaces and simulated facial outcomes generated by our approach and DefNet with the ground truth postoperative bony and facial surfaces. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The results of the quantitative evaluation are shown in Table <ref type="table" target="#tab_0">1</ref>. The results of the Wilcoxon signed-rank test showed that BP outperforms DefNet on LF (p &lt; 0.05), DI (p &lt; 0.001), LP segments (p &lt; 0.01), and the entire bone (p &lt; 0.001) by statistically significant margins. In addition, BP+FS significantly outperforms BP on DI segment (p &lt; 0.05). Two randomly selected patients are shown in Fig. <ref type="figure" target="#fig_1">3</ref>. To make a clear visual comparison, we superimposed the estimated bony surfaces (Blue) with their corresponding ground truth (Red) and set the surfaces to be transparent as shown in Fig. <ref type="figure" target="#fig_1">3(a)</ref>. Figure <ref type="figure" target="#fig_1">3</ref>(b) displays the surface distance error between the estimated bony surfaces and ground truth.</p><p>The evaluation of the bony surface showed that the proposed method successfully predicted bony surfaces that were similar to the real postoperative bones. To further assess the performance of the method, a facial simulator was used to qualitatively verify the simulated faces from the surgical plans. Figure <ref type="figure" target="#fig_2">4</ref> shows the comparison of the simulated facial outcomes of different methods using FS network and the desired facial appearance. The facial outcomes derived from bony plans of DefNet and our method are visualized, and the preoperative face is also superimposed with GT for reference. The results of the facial appearance simulation further validate the feasibility of the proposed learning-based framework for bony movement estimation and indicate that our method can achieve comparable simulation accuracy with the real bony plan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussions and Conclusions</head><p>As a result of our approach that considers both the bony and soft-tissue components of the deformity, the accuracy of the estimated bony plan, especially on the DI segment, has significantly improved. Nonetheless, the non-linear relationship between the facial and bony surfaces cannot be adequately learned using only facial and bony surface data, and additional information such as tissue properties can also affect the facial outcome. To account for uncertainty, we introduce random perturbations to generate different plans. In the future, we plan to incorporate additional uncertainty into the bony planner by using stronger perturbations or other strategies such as dropout <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b16">16]</ref> and adversarial attacks <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>, which could help create more diverse bony plans. Also, relying solely on a deep learning-based facial simulation to evaluate our method might not fully validate its effectiveness. We plan to utilize biomechanical models such as FEM to validate the efficacy of our approach in the future. Moreover, for our ultimate goal of translating the approach to clinical settings, we will validate the proposed method using a larger patient dataset and compare the predicted bony plans with the actual surgical plans.</p><p>In conclusion, we have developed a soft-tissue driven framework to directly predict the bony plans that achieve a desired facial outcome. Specifically, a bony planner and a facial simulator have been proposed for generating bony plans and verifying their effects on facial appearance. Evaluation results on a clinical dataset have shown that our method significantly outperforms the traditional bone-driven approach. By adopting this approach, we can create a virtual surgical plan that can be assessed and adjusted before the actual surgery, reducing the likelihood of complications and enhancing surgical outcomes. The proposed soft-tissue driven framework can potentially improve the accuracy and efficiency of CMF surgery planning by automating the process and incorporating a facial simulator to account for the complex non-linear relationship between bony structure and facial soft-tissue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Scheme of the proposed bony planner (BP). BP first estimates the non-rigidly deformed bony points based on the desired face, then regresses the rigid movements by fitting the non-rigid prediction.</figDesc><graphic coords="4,56,46,149,00,339,82,125,05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Examples of the results of bony plans. (a) Visual comparison of the estimated bony surface(red) with ground truth (blue), where deeper shades of blue and red indicate shading effects. (b) Color-coded error map to display the difference between the estimated bony surface and ground truth. (Color figure online)</figDesc><graphic coords="6,56,46,187,58,323,32,97,54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of the results of simulated facial outcomes. (a) Comparison of the simulated facial outcome(red) with ground truth (blue). (b) Color-coded error map of the simulated facial outcome compared with ground truth. (Color figure online)</figDesc><graphic coords="8,56,46,173,27,321,73,96,79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative evaluation results. Prediction accuracy comparison with the state-of-the-art bone-driven methods.</figDesc><table><row><cell>Method</cell><cell cols="5">Mean absolute distance (mean ± std) in millimeter</cell></row><row><cell></cell><cell>LF</cell><cell>DI</cell><cell>RP</cell><cell>LP</cell><cell>Entire Bone</cell></row><row><cell cols="6">DefNet [15] 2.69 ± 1.18 6.45 ± 1.79 3.08 ± 1.15 3.23 ± 1.29 3.86 ± 0.91</cell></row><row><cell>BP</cell><cell cols="5">2.42 ± 1.13 3.16 ± 1.38 2.58 ± 1.29 2.42 ± 1.16 2.64 ± 0.97</cell></row><row><cell>BP + FS</cell><cell cols="5">2.46 ± 1.13 2.89 ± 1.64 2.56 ± 1.30 2.41 ± 1.13 2.58 ± 0.93</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was partially supported by <rs type="funder">NIH</rs> under awards <rs type="grantNumber">R01 DE022676</rs>, <rs type="grantNumber">R01 DE027251</rs>, and <rs type="grantNumber">R01 DE021863</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bmjkNdf">
					<idno type="grant-number">R01 DE022676</idno>
				</org>
				<org type="funding" xml:id="_pYBpv9j">
					<idno type="grant-number">R01 DE027251</idno>
				</org>
				<org type="funding" xml:id="_DZrDFrU">
					<idno type="grant-number">R01 DE021863</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 18.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Virtual surgical planning for orthognathic surgery using digital data transfer and an intraoral fiducial marker: the charlotte method</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bobek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Oral Maxillofac. Surg</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1143" to="1158" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning-based facial appearance simulation driven by surgically planned craniomaxillofacial bony movement</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="565" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_54</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-154" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel incremental simulation of facial changes following orthognathic surgery using fem with realistic lip sliding effect</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">102095</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning for biomechanical modeling of facial tissue deformation in orthognathic surgical planning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lampen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="945" to="952" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SkullEngine: a multi-stage CNN framework for collaborative CBCT image segmentation and landmark detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87589-3_62</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87589-362" />
	</analytic>
	<monogr>
		<title level="m">MLMI 2021</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Rekik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12966</biblScope>
			<biblScope unit="page" from="606" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Marching cubes: a high resolution 3D surface construction algorithm</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Lorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Cline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Siggraph comput. graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="163" to="169" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep simulation of facial appearance changes following craniomaxillofacial bony movements in orthognathic surgical planning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_44</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-144" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Virtual model surgery for efficient planning and surgical performance</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Oral Maxillofac. Surg</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="638" to="644" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PointNet++: deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wilcoxon-signed-rank test</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neuhäuser</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-04898-2_616</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-04898-2616" />
	</analytic>
	<monogr>
		<title level="m">International Encyclopedia of Statistical Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Lovric</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1658" to="1659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The accuracy of three-dimensional prediction planning for the surgical correction of facial deformities using Maxilim</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khambay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Oral Maxillofac. Surg</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="801" to="806" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating patient-specific and anatomically correct reference model for craniomaxillofacial deformity via sparse representation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5809" to="5816" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithm for planning a double-jaw orthognathic surgery using a computer-aided surgical simulation (CASS) protocol. Part 1: planning sequence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Oral Maxillofac. Surg</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1431" to="1440" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimating reference bony shape models for orthognathic surgical planning using 3D point-cloud deep learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2958" to="2966" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shadow-consistent semi-supervised learning for prostate ultrasound segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Turkbey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1331" to="1345" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overlooked trustworthiness of saliency maps</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dasegowda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kalra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-843" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="451" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">When neural networks fail to generalize? A model sensitivity perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="11219" to="11227" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
