<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning</title>
				<funder ref="#_UEAg9JH #_zreSdyv">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">French</orgName>
				</funder>
				<funder ref="#_PhHQMNB">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Aditya</forename><surname>Murali</surname></persName>
							<email>murali@unistra.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deepak</forename><surname>Alapatt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pietro</forename><surname>Mascagni</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">IHU-Strasbourg</orgName>
								<orgName type="department" key="dep2">Institute of Image-Guided Surgery</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Fondazione Policlinico Universitario Agostino Gemelli IRCCS</orgName>
								<address>
									<settlement>Rome</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Armine</forename><surname>Vardazaryan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">IHU-Strasbourg</orgName>
								<orgName type="department" key="dep2">Institute of Image-Guided Surgery</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alain</forename><surname>Garcia</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">IHU-Strasbourg</orgName>
								<orgName type="department" key="dep2">Institute of Image-Guided Surgery</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nariaki</forename><surname>Okamoto</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">IHU-Strasbourg</orgName>
								<orgName type="department" key="dep2">Institute of Image-Guided Surgery</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Institute for Research Against Digestive Cancer (IRCAD)</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Padoy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ICube</orgName>
								<orgName type="institution" key="instit2">University of Strasbourg</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">IHU-Strasbourg</orgName>
								<orgName type="department" key="dep2">Institute of Image-Guided Surgery</orgName>
								<address>
									<settlement>Strasbourg</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">649AE754D2900B552C6B794FB5D9DAE0</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_62</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scene Graphs</term>
					<term>Surgical Scene Understanding</term>
					<term>Representation Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, spatiotemporal graphs have emerged as a concise and elegant manner of representing video clips in an object-centric fashion, and have shown to be useful for downstream tasks such as action recognition. In this work, we investigate the use of latent spatiotemporal graphs to represent a surgical video in terms of the constituent anatomical structures and tools and their evolving properties over time. To build the graphs, we first predict frame-wise graphs using a pre-trained model, then add temporal edges between nodes based on spatial coherence and visual and semantic similarity. Unlike previous approaches, we incorporate long-term temporal edges in our graphs to better model the evolution of the surgical scene and increase robustness to temporary occlusions. We also introduce a novel graph-editing module that incorporates prior knowledge and temporal coherence to correct errors in the graph, enabling improved downstream task performance. Using our graph representations, we evaluate two downstream tasks, critical view of safety prediction and surgical phase recognition, obtaining strong results that demonstrate the quality and flexibility of the learned representations.</p><p>Code is available at github.com/CAMMA-public/SurgLatentGraph.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Surgical video analysis is a rapidly growing field that aims to improve and gain insights into surgical practice by leveraging increasingly available surgical video footage <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b25">25]</ref>. Several key applications have been well explored, ranging from surgical skill assessment to workflow analysis to intraoperative safety enhancement <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b28">27</ref>]. Yet, effectively learning and reasoning based on surgical anatomy remains a challenging problem, as evidenced by lagging performance in fine-grained tasks such as surgical action triplet detection and critical view of safety prediction <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>. .. .</p><formula xml:id="formula_0">W = 1 W = 1</formula><p>Fig. <ref type="figure">1</ref>. Overview of our proposed approach. We begin by computing graphs for each frame using <ref type="bibr" target="#b15">[15]</ref>, then add temporal edges (shown with solid lines) between graphs at different horizons to obtain the video-level graph GV . We process GV with a GNN to yield spatiotemporally-aware node features, which we use for downstream prediction. Each node color corresponds to an object class, and each edge color to a relation class.</p><p>We retain spatial edges (shown with dotted lines) from the graph encoder in GV . (Color figure online)</p><p>Such anatomy-based reasoning can be accomplished through object-centric modeling, which is gaining popularity in general computer vision <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b29">28]</ref>. Object-centric models represent images or clips according to their constituent objects by running an object detector and then using the detections to factorize the visual feature space into per-object features. By retaining implicit visual features, these approaches maintain differentiability, allowing them to be fine-tuned for various downstream tasks. Meanwhile, they can also be extended to include object attributes such as class, location, and temporal order for tasks that rely heavily on scene semantics. Recent works have explored object-centric representations in the surgical domain, but they are characterized by one of several limitations: they often include only surgical tools <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b22">22]</ref>, preventing anatomydriven reasoning; they are limited to single-frames or short clips <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b22">22]</ref>, preventing video-level understanding; or they formulate the object-centric representation as a final output (e.g. scene graph prediction) and only include scene semantics, which limits their effectiveness for downstream tasks <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b23">23]</ref>.</p><p>In this work, we tackle these challenges by proposing to build latent spatiotemporal graph representations of entire surgical videos, with each node representing a surgical tool or anatomical structure and edges representing rela-tionships between nodes across space and time. To build our graphs, rather than use an off-the-shelf object detector, we employ the latent graph encoder of <ref type="bibr" target="#b15">[15]</ref> to generate per-frame graphs that additionally encode object semantics, segmentation details, and inter-object relations, all of which are important for downstream anatomy-driven reasoning. We then add edges between nodes in different graphs, resulting in a spatiotemporal graph representation of the entire video. We encounter two main challenges when building these graphs for surgical videos: (1) surgical scenes evolve slowly over time, calling for long-term modeling, and (2) object detection is often error-prone due to annotated data scarcity. To address the first challenge, we introduce a framework to add temporal edges at multiple horizons, enabling reasoning about the short-term and long-term evolution of the underlying video. Then, to address the error-prone object detection, we propose a Graph Editing Module that leverages the spatiotemporal graph structure and predicted object semantics to efficiently correct errors in object detection.</p><p>We evaluate our method on two downstream tasks: critical view of safety (CVS) clip classification and surgical phase recognition. CVS clip classification is a fine-grained task that requires accurate identification and reasoning about anatomy, and is thus an ideal target application for our object-centric approach. On the other hand, phase recognition is a coarse-grained task that requires holistic understanding of longer video segments, which can demonstrate the effectiveness of our temporal edge building framework. We achieve competitive performance in both of these tasks and show that our graph representations can be used with or without task-specific finetuning, thereby demonstrating their value as general-purpose video representations.</p><p>In summary, we contribute the following:</p><p>1. A method to encode surgical videos as latent spatiotemporal graphs that can then be used without modification for two diverse downstream tasks. 2. A framework for effectively modeling long-range relationships in surgical videos via multiple-horizon temporal edges. 3. A Graph Editing Module that can correct errors in the predicted graph based on temporal coherence cues and prior knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this section, we describe our approach to encode a T -frame video V = {I t | 1 ≤ t ≤ T } as a latent spatiotemporal graph G V (illustrated in Fig. <ref type="figure">1</ref>). Our method consists of a frame-wise object detection step followed by a temporal graph building step and a graph editing module to correct errors in the predicted graph representation. We also describe our graph neural network decoder to process the resulting representation G V for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Construction</head><p>Object Detection. To construct a latent spatiotemporal graph representation, we must first detect the objects in each frame, along with any additional prop-erties. We do so by employing the graph encoder φ SG proposed in <ref type="bibr" target="#b15">[15]</ref>, yielding a graph G t for each frame I t ∈ V . The resulting G t is composed of nodes N t and edges E t ; N t and E t are in turn composed of features h i , h i,j , bounding boxes b i , b i,j , and object/relation class r i , r i,j .</p><p>Spatiotemporal Graph Building. Once we have computed the graphs G t , we add temporal edges to construct a single graph G V that describes the entire video. G V retains the spatial edges from the various G t to describe geometric relations between objects in the same frame (i.e. to the left of, above), while also including temporal edges between spatially and visually similar nodes. It can then be processed with a graph neural network during downstream evaluation to efficiently propagate object-level information across space and time. We add temporal edges to G V based on object bounding box overlap and visual feature similarity, inspired by <ref type="bibr" target="#b26">[26]</ref>; however, we extend their approach to construct edges at multiple temporal horizons rather than between adjacent frames alone. Specifically, we design an operator φ TE that takes a pair of graphs G t , G t+w and outputs a list of edges, which are defined by their connectivity C t,t+w containing pairs of adjacent nodes, and their relation class R t,t+w containing relation class ids. To compute the edges, φ TE calculates pairwise similarities between nodes in G t and nodes in G t+w using two separate kernels: K B , which computes the generalized IoU between node bounding boxes, and K F , which computes the cosine similarity between node features. This yields similarity matrices M B and M F , each of size N t × N t+w . Using each matrix, we select the most similar node n j,t+w ∈ N t+w for each n i,t ∈ N t and vice-versa. Altogether, this yields 4 * (|N t | + |N t+w |) edges consisting of connectivity tuples c m,n = ((i, t), (j, t + w)) and relation classes r m,n , which we store in C t,t+w and R t,t+w respectively. We apply φ TE to all pairs of graphs G t , G t+w for various temporal horizons w ∈ W, then combine the resulting C t,t+w and R t,t+w to obtain temporal edge connectivities C ST and relation classes R ST . Finally, we augment each temporal edge with features h m,n and bounding boxes b m,n , yielding the video-level graph G V :</p><formula xml:id="formula_1">h m,n = h i,tx + h j,ty , b m,n = ∪(b i,tx , b j,ty ), where (i, t x ), (j, t y ) = c m,n E ST = {b m,n , r m,n , h m,n | (m, n) ∈ C ST }; E V = {E t | 1 ≤ t ≤ T } ∪ E ST G V = {N V , E V }, where N V = 1≤t≤T N t .</formula><p>(1) Edge Horizon Selection. While φ TE is designed to construct edges between arbitrarily distant graphs, effective selection of temporal horizons W is nontrivial. We could naively include every possible temporal horizon, setting W = {1, 2, ..., T -1} to maximize temporal information flow; however, making W too dense results in redundancies in the resulting graph, which can have an oversmoothing effect during downstream processing with a graph neural network (GNN) <ref type="bibr" target="#b30">[29]</ref>. To avoid this issue, we take inspiration from temporal convolutional networks (TCN) <ref type="bibr" target="#b10">[10]</ref>, which propagate information over long input sequences using a series of convolutions with exponentially increasing dilation. We similarly use exponentially increasing temporal horizons, setting W = {1, 2, 4, ..., 2 l } to enable efficient information flow at each GNN layer and long-horizon message passing via a stack of GNN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Editing Module</head><p>One limitation of object-centric approaches is a reliance on high quality object detection, which is particularly steep in surgical videos. These difficulties in object detection could be tackled by incorporating prior knowledge such as anatomical scene geometry, but incorporating these constraints into the learning process often requires complex constraint formulations and methodologies. We posit that our spatiotemporal graph structure represents a simpler framework to incorporate such constraints; to demonstrate this, we introduce a module to filter detections of anatomical structures, which are particularly difficult to detect, incorporating the constraint that there is only one of each structure in each frame. Specifically, after building the spatiotemporal graph, we compute a dropout probability p i,t = 1 deg(ni,t) for each node, where deg is the degree operator. Then, for each frame t, for each object class r j , we select the highest scoring node n t from {n i,t |r i,t }. During training, we apply graph editing with probability p edit , providing robustness to a wide range of input graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Downstream Task Decoder</head><p>For downstream prediction from G V , we first apply a GNN using the architecture proposed in <ref type="bibr" target="#b2">[3]</ref>, yielding spatiotemporally-aware node features. Then, we pool the node features within each frame and apply a linear layer to yield frame-wise predictions (see Fig. <ref type="figure">1</ref>). We process these predictions differently depending on the task: for clip classification, we output only the prediction for the last frame, while for temporal video segmentation, we output the frame-wise predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>We adopt a two-stage training approach, starting by training φ SG as proposed in <ref type="bibr" target="#b15">[15]</ref> and then extracting graphs for all images. Then, in the second stage, we process a sequence of graphs with our model to predict frame-wise outputs. We supervise each prediction with the corresponding frame label, propagating the clip label to each frame when per-frame labels are unavailable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>In this section, we describe our evaluation tasks and datasets, describe baseline methods and our model, then present results for each task. We conclude with an ablation study that illustrates the impact of our various model components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation Tasks and Datasets</head><p>Critical View of Safety (CVS) Prediction. The CVS consists of three independent criteria, and can be viewed as a multi-label classification problem <ref type="bibr" target="#b13">[13]</ref>.</p><p>For our experiments, we use the Endoscapes+ dataset introduced in <ref type="bibr" target="#b15">[15]</ref>, which contains 11090 images annotated with CVS evenly sampled from the dissection phase of 201 cholecystectomies at 0.2 fps; it also includes a subset of 1933 images with segmentation masks and bounding box annotations. We model CVS prediction as a clip classification problem, constructing clips of length 10 at 1 fps, and use the label of the last frame as the clip label. As in <ref type="bibr" target="#b15">[15]</ref>, we investigate CVS prediction performance in two experimental settings to study the label-efficiency of various methods: (1) using only the bounding box labels and CVS labels and (2) additionally using the segmentation labels. We report mean average precision (mAP) across the three criteria for all methods. Surgical Phase Recognition. For surgical phase recognition, we use the publically available Cholec80 dataset <ref type="bibr" target="#b24">[24]</ref>, which includes 80 videos with frame-wise phase annotations ({1, 2, ..., 7}). We use the first 40 videos for training, the next 8 for validation, and the remaining 32 for testing, as in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">20]</ref>. In addition, to enable object-centric approaches, we use the publically available CholecSeg8k dataset <ref type="bibr" target="#b7">[7]</ref> as it represents multiple surgical phases unlike Endoscapes+. As CholecSeg8k is a subset of Cholec80, we split the images into training, validation, and testing following the aforementioned video splits. We model phase recognition as a temporal video segmentation problem, and process the entire video at once. Again, we explore two experimental settings: (1) temporal phase recognition without single-frame finetuning to evaluate the surgical video representations learned by each method and (2) temporal phase recognition with single-frame finetuning, the classical setting <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">20]</ref>. We report mean F1 score across videos for all methods, as suggested in <ref type="bibr" target="#b20">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baseline Methods</head><p>Single-Frame Methods. As CVS clip classification is unexplored, we compare to two recent single-frame methods for reference: LG-CVS <ref type="bibr" target="#b15">[15]</ref>, a graph-based approach, and DeepCVS <ref type="bibr" target="#b13">[13]</ref>, a non-object-centric approach. We quote results from <ref type="bibr" target="#b15">[15]</ref>, which improves DeepCVS and enables training with bounding boxes.</p><p>DeepCVS-Temporal. We also extend DeepCVS for clip classification by replacing the last linear layer of the dilated ResNet-18 with a Transformer decoder, referring to this model as DeepCVS-Temporal.</p><p>STRG. Space-Time Region Graphs (STRG) <ref type="bibr" target="#b26">[26]</ref> is a spatiotemporal graphbased approach for action recognition that builds a latent graph by predicting region proposals and extracting per-region visual features using an I3D backbone; we repurpose STRG for CVS clip classification and phase recognition. Because STRG is trained end-to-end, it can only process clips of limited length; consequently, we train STRG on clips of 15 frames for phase recognition rather the entire video as in other methods. We also only consider phase recognition with finetuning. For CVS clip classification, we additionally pre-train the I3D feature extractor in STRG on bounding box/segmentation labels using a Faster-RCNN box head <ref type="bibr" target="#b21">[21]</ref> or DeepLabV3+ decoder <ref type="bibr" target="#b0">[1]</ref>.</p><p>TeCNO. TeCNO <ref type="bibr" target="#b1">[2]</ref> is a temporal model for surgical phase recognition consisting of frame-wise feature extraction followed by temporal decoding with a causal TCN <ref type="bibr" target="#b10">[10]</ref> to classify phases. For phase recognition without single-frame finetuning, we use a ResNet-50 pre-trained on CholecSeg8k using a DeepLabV3+ head to extract features, enabling fair comparisons with our method. For the other setting, we report performance from <ref type="bibr" target="#b20">[20]</ref>. Ours. We train our model in two stages, starting by training the graph encoder φ SG as described in <ref type="bibr" target="#b15">[15]</ref> on the subset of Endoscapes+ annotated with segmentation masks or bounding boxes for CVS clip classification, or on CholecSeg8k for phase recognition. We then extract frame-wise graphs for the entire dataset and apply our spatiotemporal graph approach to predict CVS or phase. In the second experimental setting for phase recognition, we additionally finetune φ SG on all training images with the frame-wise phase labels before extracting the graphs. Finally, we evaluate a version of our method that additionally applies a TCN to the un-factorized image features and adds the TCN-processed features to the pooled temporally-aware node features prior to linear classification. We set l = 3, p edit = 0.5, and use a 5-layer GNN for CVS prediction and an 8-layer GNN for phase recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Main Experiments</head><p>CVS. Our first takeaway from Table <ref type="table" target="#tab_0">1</ref> is that temporal models provide a methodagnostic boost of 3% mAP for CVS prediction. Furthermore, our approach outperforms both non-object-centric and object-centric temporal baselines, achieving a substantial performance boost in the label-efficient bounding box setting while remaining competitive when also trained with segmentation masks. In the box setting, we observe that the non-object-centric DeepCVS approaches perform rather poorly due to an over-reliance on predicted semantics rather than effective visual encodings <ref type="bibr" target="#b15">[15]</ref>. Object-centric modeling addresses some of these limitations, as evidenced by STRG outperforming DeepCVS-Temporal. Nevertheless, our method achieves a much stronger performance boost, owing to multiple factors: (1) our model is based on the underlying LG-CVS, which constructs its frame-wise object-centric representation by using the final bounding box predictions rather than just region proposals like STRG, and also encodes semantic information, and (2) our proposed improvements (multiple-horizon edges, graph editing) are critical to improving model performance. Meanwhile, in the segmentation setting, the object-centric STRG is ineffective, performing worse than DeepCVS-Temporal; this discrepancy arises because, as previously mentioned, STRG relies on region proposals rather than object-specific bounding boxes in its graph representation, and as a result, cannot fully take advantage of the additional information provided by the segmentation masks. Our approach translates the ideas of STRG but importantly builds on top of the already effective representations learnt by LG-CVS to achieve universal effectiveness for spatiotemporal modeling of CVS.</p><p>Phase. Table <ref type="table" target="#tab_1">2</ref> shows the phase recognition results for various methods with (bottom) and without (top) finetuning the underlying single-frame model. Our model is already highly effective for phase recognition without any finetuning, outperforming the corresponding TeCNO model by 6.1% F1 in its original form and by nearly 10% F1 when also using a TCN. This shows that the graph representations contain general-purpose information about the surgical scenes and their evolution. Finally, by finetuning the underlying single-frame graph encoder, we match the existing state-of-the-art, highlighting our method's flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Studies</head><p>Table <ref type="table" target="#tab_2">3</ref> illustrates the impact of each model component on CVS clip classification and phase recognition performance. The first two rows illustrate the importance of using exponential edge horizons. Without any long-term edges (as in STRG), we observe a staggering 7.20% drop in Phase F1; naively building edges between all the graphs improves performance but is still 4.14% worse than our proposed method. We observe similar trends for the CVS mAP but with lower magnitude, as CVS prediction is not as reliant on long-term video understanding. Meanwhile, we observe the opposite effect for the graph editing module, which is quite effective for CVS clip classification but does not considerably impact phase F1. This is again consistent with the nature of the tasks, as CVS requires fine-grained understanding of the surgical anatomy, and performance can suffer greatly from errors in object detection, while phase recognition is more coarse-grained and is thus less impacted by errors at this fine-grained level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We introduce a method to encode surgical videos in their entirety as latent spatiotemporal graph representations. Our graph representations enable finegrained anatomy-driven reasoning as well as coarse long-range video understanding due to the inclusion of edges at multiple-temporal horizons, robustness against errors in object detection provided by a graph editing module, and memory-and computational-efficiency afforded by a two-stage training pipeline. We believe that the resulting graphs are powerful general-purpose representations of surgical videos that can fuel numerous future downstream applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>CVS Clip Classification Performance. Standard deviation is across three runs of each method. Single frame methods from prior works are also reported for reference.</figDesc><table><row><cell>CVS mAP</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Surgical Phase Recognition Performance.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Phase F1</cell></row><row><cell></cell><cell>TeCNO [2]</cell><cell>64.3</cell></row><row><cell>No Single-Frame Finetuning</cell><cell>Ours</cell><cell>70.3</cell></row><row><cell></cell><cell>Ours + TCN</cell><cell>74.1</cell></row><row><cell></cell><cell>STRG [26]</cell><cell>77.1</cell></row><row><cell cols="2">TeCNO [2], reported from [20]</cell><cell>80.3</cell></row><row><cell>With Single-Frame Finetuning</cell><cell>Ours</cell><cell>79.9</cell></row><row><cell></cell><cell>Ours + TCN</cell><cell>81.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation Study of Model Components.</figDesc><table><row><cell>Ablated Feature</cell><cell cols="2">Performance Drop (↑ is worse) CVS mAP (Seg) Phase F1 (No FT)</cell></row><row><cell>No Long Term Edges (W = {1})</cell><cell>1 . 6</cell><cell>7 . 2</cell></row><row><cell>Naive Edge Horizon Selection (W = {1, 2, ..., T -1})</cell><cell>1.4</cell><cell>4.1</cell></row><row><cell>No Graph Editing Module</cell><cell>1.1</cell><cell>0.2</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by <rs type="funder">French</rs> state funds managed by the <rs type="funder">ANR</rs> within the <rs type="programName">National AI Chair program</rs> under Grant <rs type="grantNumber">ANR-20-CHIA-0029-01 (Chair AI4ORSafety</rs>) and within the <rs type="programName">Investments for the future program</rs> under Grants <rs type="grantNumber">ANR-10-IDEX-0002-02</rs> (<rs type="projectName">IdEx Unistra</rs>) and <rs type="grantNumber">ANR-10-IAHU-02</rs> (<rs type="affiliation">IHU Strasbourg</rs>). This work was granted access to the <rs type="institution">HPC resources of IDRIS</rs> under the allocation 2021-AD011011640R1 made by <rs type="institution">GENCI</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_PhHQMNB">
					<idno type="grant-number">ANR-20-CHIA-0029-01 (Chair AI4ORSafety</idno>
					<orgName type="program" subtype="full">National AI Chair program</orgName>
				</org>
				<org type="funded-project" xml:id="_UEAg9JH">
					<idno type="grant-number">ANR-10-IDEX-0002-02</idno>
					<orgName type="project" subtype="full">IdEx Unistra</orgName>
					<orgName type="program" subtype="full">Investments for the future program</orgName>
				</org>
				<org type="funding" xml:id="_zreSdyv">
					<idno type="grant-number">ANR-10-IAHU-02</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_62.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01234-2_49" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TeCNO: surgical phase recognition with multi-stage temporal convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-0_33" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="343" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Semantic image manipulation using scene graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dhamo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5213" to="5222" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using 3D convolutional neural networks to learn spatiotemporal features for automatic surgical gesture recognition in video</title>
		<author>
			<persName><forename type="first">I</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bodenstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Oehme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Von Bechtolsheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trans-SVNet: accurate phase recognition from surgical videos via hybrid embedding aggregation transformer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="593" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_57</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-1_57" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object-region video transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page" from="3148" to="3159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cholecseg8k: a semantic segmentation dataset for laparoscopic cholecystectomy based on cholec80</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Shih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12453</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning and reasoning with the graph structure representation in robotic surgery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Seenivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_60</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-0_60" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Spatiotemporal deformable scene graphs for complex activity detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cuzzolin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Artificial intelligence for intraoperative guidance: using semantic segmentation to identify surgical anatomy during laparoscopic cholecystectomy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals Surgery</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Surgical data science for next-generation interventions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="691" to="696" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Artificial intelligence for surgical safety: automatic assessment of the critical view of safety in laparoscopic cholecystectomy using deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mascagni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals Surgery</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Somethingelse: compositional action recognition with spatial-temporal interaction networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1049" to="1059" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Latent graph representations for critical view of safety assessment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Murali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.04155</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cholectriplet 2021: a benchmark challenge for surgical action triplet recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nwoye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04746</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">4d-or: semantic scene graphs for or domain modeling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Özsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Örnek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.11937</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rethinking feature extraction: gradient-based localized feature extraction for end-to-end surgical downstream tasks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitheran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Seenivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="12623" to="12630" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Differentiable scene graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Raboh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1488" to="1497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Dissecting self-supervised learning methods for surgical computer vision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.00449</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Towards generalizable surgical activity recognition using spatial temporal graph convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03728</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Global-reasoned multi-task learning model for surgical scene understanding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seenivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitheran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3858" to="3865" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Endonet: a deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cai4cai: the rise of contextual artificial intelligence in computer-assisted interventions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="198" to="214" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11209</biblScope>
			<biblScope unit="page" from="413" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01228-1_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01228-1_25" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-modal self-supervised representation learning for gesture and skill recognition in robotic surgery</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kazanzides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCARS</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="779" to="787" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">is an object-centric video representation beneficial for transfer?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2022-12">December 2022</date>
			<biblScope unit="page" from="1976" to="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph convolutional networks: a comprehensive review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Soc. Netw</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
