<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation</title>
				<funder ref="#_xa8TE4N">
					<orgName type="full">Hong Kong Research Grants Council</orgName>
					<orgName type="abbreviated">RGC</orgName>
				</funder>
				<funder ref="#_TdxhDTM">
					<orgName type="full">Shenzhen-Hong Kong-Macau Technology Research Programme</orgName>
					<orgName type="abbreviated">C</orgName>
				</funder>
				<funder ref="#_2GMYuYF">
					<orgName type="full">General Research Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Beilei</forename><surname>Cui</surname></persName>
							<email>beileicui@link.cuhk.edu.hk</email>
							<idno type="ORCID">0009-0009-7900-8032</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minqing</forename><surname>Zhang</surname></persName>
							<idno type="ORCID">0000-0002-7214-0569</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengya</forename><surname>Xu</surname></persName>
							<email>mengya@u.nus.edu</email>
							<idno type="ORCID">0000-0002-4338-7079</idno>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">An</forename><surname>Wang</surname></persName>
							<idno type="ORCID">0000-0001-5515-0653</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wu</forename><surname>Yuan</surname></persName>
							<email>wyuan@cuhk.edu.hk</email>
							<idno type="ORCID">0000-0001-5515-0653</idno>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongliang</forename><surname>Ren</surname></persName>
							<email>ren@nus.edu.sg</email>
							<idno type="ORCID">0000-0002-6488-1551</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9E843AE57F8F92F23AF5E24860DE1251</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Noisy label learning</term>
					<term>Feature affinity</term>
					<term>Semantic segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Noisy label problems are inevitably in existence within medical image segmentation causing severe performance degradation. Previous segmentation methods for noisy label problems only utilize a single image while the potential of leveraging the correlation between images has been overlooked. Especially for video segmentation, adjacent frames contain rich contextual information beneficial in cognizing noisy labels. Based on two insights, we propose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework to resolve noisy-labeled medical video segmentation issues. First, we argue the sequential prior of videos is an effective reference, i.e., pixel-level features from adjacent frames are close in distance for the same class and far in distance otherwise. Therefore, Temporal Feature Affinity Learning (TFAL) is devised to indicate possible noisy labels by evaluating the affinity between pixels in two adjacent frames. We also notice that the noise distribution exhibits considerable variations across video, image, and pixel levels. In this way, we introduce Multi-Scale Supervision (MSS) to supervise the network from three different perspectives by re-weighting and refining the samples. This design enables the network to concentrate on clean samples in a coarse-to-fine manner. Experiments with both synthetic and real-world label noise demonstrate that our method outperforms recent state-of-theart robust segmentation approaches. Code is available at https://github. com/BeileiCui/MS-TFAL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video segmentation, which refers to assigning pixel-wise annotation to each frame in a video, is one of the most vital tasks in medical image analysis. Thanks to the advance in deep learning algorithms based on Convolutional Neural Networks, medical video segmentation has achieved great progress over recent years <ref type="bibr" target="#b25">[9]</ref>. But a major problem of deep learning methods is that they are largely dependent on both the quantity and quality of training data <ref type="bibr" target="#b29">[13]</ref>. Datasets annotated by non-expert humans or automated systems with little supervision typically suffer from very high label noise and are extremely time-consuming. Even expert annotators could generate different labels based on their cognitive bias <ref type="bibr" target="#b22">[6]</ref>. Based on the above, noisy labels are inevitably in existence within medical video datasets causing misguidance to the network and resulting in severe performance degradation. Hence, it is of great importance to design medical video segmentation methods that are robust to noisy labels within training data <ref type="bibr" target="#b20">[4,</ref><ref type="bibr" target="#b34">18]</ref>.</p><p>Most of the previous noisy label methods mainly focus on classification tasks. Only in recent years, the problem of noise labels in segmentation tasks has been more explored, but still less involved in medical image analysis. Previous techniques for solving noisy label problems in medical segmentation tasks can be categorized in three directions. The first type of method aims at deriving and modeling the general distribution of noisy labels in the form of Noise Transition Matrix (NTM) <ref type="bibr" target="#b19">[3,</ref><ref type="bibr" target="#b24">8]</ref>. Secondly, some researchers develop special training strategies to re-weight or re-sample the data such that the model could focus on more dependable samples. Zhang et al. <ref type="bibr" target="#b35">[19]</ref> concurrently train three networks and each network is trained with pixels filtered by the other two networks. Shi et al. <ref type="bibr" target="#b30">[14]</ref> use stable characteristics of clean labels to estimate samples' uncertainty map which is used to further guide the network. Thirdly, label refinement is implemented to renovate noisy labels. Li et al. <ref type="bibr" target="#b23">[7]</ref> represent the image with superpixels to exploit more advanced information in an image and refine the labels accordingly. Liu et al. <ref type="bibr" target="#b26">[10]</ref> use two different networks to jointly determine the error sample, and use each other to refine the labels to prevent error accumulation. Xu et al. <ref type="bibr" target="#b31">[15]</ref> utilize the mean-teacher model and Confident learning to refine the low-quality annotated samples.</p><p>Despite the amazing performance in tackling noisy label issues for medical image segmentation, almost all existing techniques only make use of the information within a single image. To this end, we make the effort in exploring the feature affinity relation between pixels from consecutive frames. The motivation is that the embedding features of pixels from adjacent frames should be close if they belong to the same class, and should be far if they belong to different classes. Hence, if a pixel's feature is far from the pixels of the same class in the adjacent frame and close to the ones of different classes, its label is more likely to be incorrect. Meanwhile, the distribution of noisy labels may vary among different videos and frames, which also motivates us to supervise the network from multiple perspectives. We acquire the embedding feature maps of adjacent frames in the Backbone Section. Then, the temporal affinity is calculated for each pixel in current frame to obtain the positive and negative affinity map indicating possible noisy labels. The affinity maps are then utilized to supervise the network in a multi-scale manner.</p><p>Inspired by the motivation above and to better resolve noisy label problems with temporal consistency, we propose Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework. Our contributions can be summarized as the following points:</p><p>1. In this work, we first propose a novel Temporal Feature Affinity Learning (TFAL) method to evaluate the temporal feature affinity map of an image by calculating the similarity between the same and different classes' features of adjacent frames, therefore indicating possible noisy labels. 2. We further develop a Multi-Scale Supervision (MSS) framework based on TFAL by supervising the network through video, image, and pixel levels. Such a coarse-to-fine learning process enables the network to focus more on correct samples at each stage and rectify the noisy labels, thus improving the generalization ability. 3. Our method is validated on a publicly available dataset with synthetic noisy labels and a real-world label noise dataset and obtained superior performance over other state-of-the-art noisy label techniques. 4. To the best of our knowledge, we are the first to tackle noisy label problems using inter-frame information and discover the superior ability of sequential prior information to resolve noisy label issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The proposed Multi-Scale Temporal Feature Affinity Learning Framework is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. We aim to exploit the information from adjacent frames to identify the possible noisy labels, thereby learning a segmentation network robust to label noises by re-weighting and refining the samples. Formally, given an input training image x t ∈ R H×W ×3 , and its adjacent frame x t-1 , two feature maps f t , f t-1 ∈ R h×w×C f are first generated by a CNN backbone, where h,w and C f represent the height, width and channel number. Intuitively, for each pair of features from f t and f t-1 , their distance should be close if they belong to the same class and far otherwise. Therefore for each pixel in f t , we calculate two affinity relations with f t-1 . The first one is called positive affinity, computed by averaging the cosine similarity between one pixel f t (i) in the current frame and all the same class' pixels as f t (i) in previous frame. The second one is called negative affinity, computed by averaging the cosine similarity between one pixel f t (i) in current frame and all the different class' pixels as f t (i) in previous frame.</p><p>Then through up-sampling, the Positive Affinity Map a p and Negative Affinity Map a n can be obtained, where a p , a n ∈ R H×W , denote the affinity relation between x t and x t-1 . The positive affinity of clean labels should be high while the negative affinity of clean labels should be low. Therefore, the black areas in a p and the white areas in a n are more likely to be noisy labels.</p><p>Then we use two affinity maps a p , a n to conduct Multi-Scale Supervision training. Multi-scale refers to video, image, and pixel levels. Specifically, for pixel-level supervision, we first obtain thresholds t p and t n by calculating the average positive and negative affinity over the entire dataset. The thresholds are used to determine the possible noisy label sets based on positive and negative affinity separately. The intersection of two sets is selected as the final noisy set and relabeled with the model prediction p t . The affinity maps are also used to estimate the image-level weights λ I and video-level weights λ V . The weights enable the network to concentrate on videos and images with higher affinity confidence. Our method is a plug-in module that is not dependent on backbone type and can be applied to both image-based backbones and video-based backbones by modifying the shape of inputs and feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Temporal Feature Affinity Learning</head><p>The purpose of this section is to estimate the affinity between pixels in the current frame and previous frame, thus indicating possible noisy labels. Specifically, in addition to the aforementioned feature map f t , f t-1 ∈ R h×w×C f , we obtain the down-sampled labels with the same size of feature map ỹ t , ỹ t-1 ∈ R h×w×C , where C means the total class number. We derive the positive and negative label maps with binary variables: M p , M n ⊆ {0, 1}</p><p>hw×hw . The value corresponds to pixel (i, j) is determined by the label as:</p><formula xml:id="formula_0">M p (i, j) = 1 ỹ t (i) = ỹ t-1 (j) , M n (i, j) = 1 ỹ t (i) = ỹ t-1 (j)<label>(1)</label></formula><p>where 1 (•) is the indicator function. M p (i, j) = 1 when ith label in ỹ t and jth label in ỹ t-1 are the same class, while M p (i, j) = 0 otherwise; and M n vise versa. The value of cosine similarity map S ∈ R hw×hw corresponds to pixel (i, j) is determined by: S (i, j) =</p><formula xml:id="formula_1">ft(i)•ft-1(j) ft(i) × ft-1(j) .</formula><p>We then use the average cosine similarity of a pixel with all pixels in the previous frame belonging to the same or different class to represent its positive or negative affinity:</p><formula xml:id="formula_2">a p,f (i) = hw j=1 S (i, j) M p (i, j) hw j=1 M p (i, j) , a n,f (i) = hw j=1 S (i, j) M n (i, j) hw j=1 M n (i, j)<label>(2)</label></formula><p>where a p,f , a n,f ∈ R h×w means the positive and negative map with the same size as the feature map. With simple up-sampling, we could obtain the final affinity maps a p , a n ∈ R H×W , indicating the positive and negative affinity of pixels in the current frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-scale Supervision</head><p>The feature map is first connected with a segmentation head generating the prediction p. Besides the standard cross entropy loss L CE (p, ỹ) = -HW i ỹ (i) logp (i), we applied a label corrected cross entropy loss L CE LC (p, ŷ) = -HW i ŷ (i) logp (i) to train the network with pixel-level corrected labels. We further use two weight factors λ I and λ V to supervise the network in image and video levels. The specific descriptions are explained in the following sections.</p><p>Pixel-Level Supervision. Inspired by the principle in Confident Learning <ref type="bibr" target="#b28">[12]</ref>, we use affinity maps to denote the confidence of labels. if a pixel x (i) in an image has both small enough positive affinity a p (i) t p and large enough negative affinity a n (i) t n , then its label ỹ (i) can be suspected as noisy. The threshold t p , t n are obtained empirically by calculating the average positive and negative affinity, formulated as t p = 1 |Ap| ap∈Ap a p , t n = 1 |An| an∈An a n , where a p , a n means the average value of positive and negative affinity over an image. The noisy pixels set can therefore be defined by:</p><formula xml:id="formula_3">x := {x (i) ∈ x : a p (i) t p } {x (i) ∈ x : a n (i) t n } . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>Then we update the pixel-level label map ŷ as:</p><formula xml:id="formula_5">ŷ(i) = 1 (x(i) ∈ x) p(i) + 1 (x(i) / ∈ x) ỹ(i),<label>(4)</label></formula><p>where p(i) is the prediction of network. Through this process, we only replace those pixels with both low positive affinity and large negative affinity.</p><p>Image-Level Supervision. Even in the same video, different frames may contain different amounts of noisy labels. Hence, we first define the affinity confidence value as: q = a p + 1 -a n . The average affinity confidence value is therefore denoted as: q = t p + 1 -t n . Finally, we define the image-level weight as:</p><formula xml:id="formula_6">λ I = e 2(q-q) . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>λ I &gt; 1 if the sample has large affinity confidence and λ I &lt; 1 otherwise, therefore enabling the network to concentrate more on the clean samples.</p><p>Video-Level Supervision. We assign different weights to different videos such that the network can learn from more correct videos in the early stage. We first define the video affinity confidence as the average affinity confidence of all the frames:</p><formula xml:id="formula_8">q v = 1 |V |</formula><p>x∈V q x . Supposing there are N videos in total, we use k ∈ {1, 2, • • • , N} to represent the ranking of video affinity confidence from small to large, which means k = 1 and k = N denote the video with lowest and highest affinity confidence separately. Video-level weight is thus formulated as:</p><formula xml:id="formula_9">λ V = ⎧ ⎪ ⎨ ⎪ ⎩ θ l , if k &lt; N 3 θ l + 3k-N N (θ u -θ l ), if N 3 k 2N 3 θ u , if k &gt; 2N 3 (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where θ l and θ u are the preseted lower-bound and upper-bound of weight.</p><p>Combining the above-defined losses and weights, we obtain the final loss as:</p><formula xml:id="formula_11">L = λ V λ I L CE + L CE</formula><p>LC , which supervise the network in a multi-scale manner. These losses and weights are enrolled in training after initialization in an order of video, image, and pixel enabling the network to enhance the robustness and generalization ability by concentrating on clean samples from rough to subtle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Description and Experiment Settings</head><p>EndoVis 2018 Dataset and Noise Patterns. EndoVis 2018 Dataset is from the MICCAI robotic instrument segmentation dataset<ref type="foot" target="#foot_0">1</ref> of endoscopic vision challenge 2018 <ref type="bibr" target="#b17">[1]</ref>. It is officially divided into 15 videos with 2235 frames for training and 4 videos with 997 frames for testing separately. The dataset contains 12 classes including different anatomy and robotic instruments. Each image is resized into 256 × 320 in pre-processing. To better simulate manual noisy annotations within a video, we first randomly select a ratio of α of videos and in each selected video, we divide all frames into several groups in a group of 3-6 consecutive frames. Then for each group of frames, we randomly apply dilation, erosion, affine transformation, or polygon noise to each class <ref type="bibr" target="#b23">[7,</ref><ref type="bibr" target="#b32">16,</ref><ref type="bibr" target="#b34">18,</ref><ref type="bibr" target="#b35">19]</ref>. We investigated our algorithms in several noisy settings with α being {0.3, 0.5, 0.8}. Some examples of data and noisy labels are shown in supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rat Colon Dataset.</head><p>For real-world noisy dataset, we have collected rat colon OCT images using 800nm ultra-high resolution endoscopic spectral domain OCT. We refer readers to <ref type="bibr" target="#b33">[17]</ref> for more details. Each centimeter of rat colon imaged corresponds to 500 images with 6 class layers of interest. We select 8 sections with 2525 images for training and 3 sections with 1352 images for testing. The labels of test set were annotated by professional endoscopists as ground truth while the training set was annotated by non-experts. Each image is resized into 256 × 256 in pre-processing. Some dataset examples are shown in supplementary.</p><p>Implementation Details. We adopt Deeplabv3+ <ref type="bibr" target="#b18">[2]</ref> as our backbone network for fair comparison. The framework is implemented with PyTorch on two Nvidia 3090 GPUs. We adopt the Adam optimizer with an initial learning rate of 1e -4 and weight decay of 1e -4. Batch size is set to 4 with a maximum of 100 epochs for both Datasets. θ l and θ u are set to 0.4 and 1 separately. The video, image, and pixel level supervision are involved from the 16th, 24th, and 40th epoch respectively. The segmentation performance is assessed by mIOU and Dice scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment Results on EndoVis 2018 Dataset</head><p>Table <ref type="table">1</ref> presents the comparison results under different ratios of label noises. We evaluate the performance of backbone trained with clean labels, two state-ofthe-art instrument segmentation network <ref type="bibr" target="#b21">[5,</ref><ref type="bibr" target="#b27">11]</ref>, two noisy label learning techniques <ref type="bibr" target="#b20">[4,</ref><ref type="bibr" target="#b24">8]</ref>, backbone <ref type="bibr" target="#b18">[2]</ref> and the proposed MS-TFAL. We re-implement <ref type="bibr" target="#b20">[4,</ref><ref type="bibr" target="#b24">8]</ref> with the same backbone <ref type="bibr" target="#b18">[2]</ref> for a fair comparison. Compared with all other methods, MS-TFAL shows the minimum performance gap with the upper bound (Clean) for both mIOU and Dice scores under all ratios of noises demonstrating the robustness of our method. As noise increases, the performance of all baselines decreases significantly indicating the huge negative effect of noisy labels. It is noteworthy that when the noise ratio rises from 0.3 to 0.5 and from 0.5 to 0.8, our method only drops 2.57% mIOU with 2.41% Dice and 8.98% mIOU with 9.49% Dice, both are the minimal performance degradation, which further demonstrates the robustness of our method against label noise. In the extreme noise setting (α = 0.8), our method achieves 41.36% mIOU and 51.01% Dice and outperforms second best method 5.37% mIOU and 6.26% Dice. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, we provide partial qualitative results indicating the superiority of MS-TFAL over other methods in the qualitative aspect. More qualitative results are shown in supplementary.  Ablation Studies. We further conduct two ablation studies on our multi-scale components and choice of frame for feature affinity under noisy dataset with α = 0.5. With only video-level supervision (w/V), mIOU and Dice are increased by 4.93% and 4.43% compared with backbone only. Then we apply both video and image level supervision (w/V &amp; I) and gain an increase of 0.92% mIOU and 1.09% Dice. Pixel-level supervision is added at last forming the complete Multi-Scale Supervision results in another improvement of 1.62% mIOU and 1.96% Dice verifying the effectiveness in attenuating noisy label issues of individual components. For the ablation study of the choice of frame, we compared two different attempts with ours: conduct TFAL with the same frame and any frame in the dataset (Ours is adjacent frame). Results show that using adjacent frame has the best performance compared to the other two choices.</p><p>Visualization of Temporal Affinity. To prove the effectiveness of using affinity relation we defined to represent the confidence of label, we display comparisons between noise variance and selected noise map in Fig. <ref type="figure" target="#fig_2">3</ref>. Noise variance (Fourth column) represents the incorrect label map and the Selected noise map (Fifth column) denotes the noise map we select with Eq. ( <ref type="formula" target="#formula_3">3</ref>). We can observe that the noisy labels we affirm have a high overlap degree with the true noise labels, which demonstrates the validity of our TFAL module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Deeplabv3+ <ref type="bibr" target="#b18">[2]</ref> STswin <ref type="bibr" target="#b21">[5]</ref> RAUNet <ref type="bibr" target="#b27">[11]</ref> JCAS <ref type="bibr" target="#b20">[4]</ref> VolMin <ref type="bibr" target="#b24">[8]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiment Results on Rat Colon Dataset</head><p>The comparison results on real-world noisy Rat Colon Dataset are presented in Table <ref type="table" target="#tab_0">2</ref>. Our method outperforms other methods consistently on both mIOU and Dice scores, which verifies the superior robustness of our method on real-world label noise issues. Qualitative results are shown in supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>In this paper, we propose a robust MS-TFAL framework to resolve noisy label issues in medical video segmentation. Different from previous methods, we first introduce the novel TFAL module to use affinity between pixels from adjacent frames to represent the confidence of label. We further design MSS framework to supervise the network from multiple perspectives. Our method can not only identify noise in labels, but also correct them in pixel-wise with rich temporal consistency. Extensive experiments under both synthetic and real-world label noise data demonstrate the excellent noise resilience of MS-TFAL.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Illustration of proposed Multi-Scale Temporal Feature Affinity Learning framework. We acquire the embedding feature maps of adjacent frames in the Backbone Section. Then, the temporal affinity is calculated for each pixel in current frame to obtain the positive and negative affinity map indicating possible noisy labels. The affinity maps are then utilized to supervise the network in a multi-scale manner.</figDesc><graphic coords="3,43,29,54,02,337,36,131,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison of qualitative segmentation results on EndoVis18 Dataset.</figDesc><graphic coords="8,71,46,398,45,309,28,112,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of Noise variance and feature affinity. Selected noisy label (Fifth column) means the noise map selected with Eq. (3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>MS-TFAL (Ours) Comparison of other methods and our models on Rat Colon Dataset.</figDesc><table><row><cell cols="2">mIOU (%) 68.46</cell><cell>68.21</cell><cell>68.24</cell><cell>68.15</cell><cell>68.81</cell><cell>71.05</cell></row><row><cell>Dice (%)</cell><cell>75.25</cell><cell>77.70</cell><cell>77.39</cell><cell>77.50</cell><cell>77.89</cell><cell>80.17</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://endovissub2018-roboticscenesegmentation.grand-challenge.org/.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>. This work was supported by <rs type="funder">Hong Kong Research Grants Council (RGC) Collaborative Research Fund</rs> (<rs type="grantNumber">C4026-21G</rs>), <rs type="funder">General Research Fund</rs> (<rs type="grantNumber">GRF 14211420 &amp; 14203323</rs>), <rs type="funder">Shenzhen-Hong Kong-Macau Technology Research Programme (Type C) STIC</rs> Grant <rs type="grantNumber">SGDX20210823103535014 (202108233000303</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xa8TE4N">
					<idno type="grant-number">C4026-21G</idno>
				</org>
				<org type="funding" xml:id="_2GMYuYF">
					<idno type="grant-number">GRF 14211420 &amp; 14203323</idno>
				</org>
				<org type="funding" xml:id="_TdxhDTM">
					<idno type="grant-number">SGDX20210823103535014 (202108233000303</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Method mIOU (%) Sequence mIOU (%) Dice (%)</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 9.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<idno>50.42 49.90 48.50 67.18 36.10 60.60</idno>
	</analytic>
	<monogr>
		<title level="j">Deeplabv</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Stswin</surname></persName>
		</author>
		<idno>50.29 49.96 48.67 66.52 35.99 60.62</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Raunet</surname></persName>
		</author>
		<idno>11] 50.36 44.97 48.06 68.90 39.53 60.61 JCAS (22 ) [4] 48.65 48.77 46.60 64.83 34.39 58.97</idno>
		<imprint>
			<date>19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Volmin</surname></persName>
		</author>
		<idno>47.64 45.60 45.31 64.01 35.63 57.42</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ms-Tfal</surname></persName>
		</author>
		<idno>52.91 49.48 51.60 71.08 39.52 62.91</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<idno>42.87 41.72 42.96 59.54 27.27 53.02</idno>
	</analytic>
	<monogr>
		<title level="j">Deeplabv</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><surname>Stswin</surname></persName>
		</author>
		<idno>44.48 40.78 45.22 60.50 31.45 54.99</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><surname>Raunet</surname></persName>
		</author>
		<idno>11] 46.74 46.16 43.08 63.00 34.73 57.44 JCAS (22 ) [4] 45.24 41.90 44.06 61.13 33.90 55.22</idno>
		<imprint>
			<date>19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>Volmin</surname></persName>
		</author>
		<idno>44.02 42.68 46.26 59.67 27.47 53.59</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ms-Tfal</surname></persName>
		</author>
		<idno>50.34 49.15 50.17 67.37 34.67 60.50</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<idno>33.35 27.57 35.69 45.30 24.86 42.22</idno>
	</analytic>
	<monogr>
		<title level="j">Deeplabv</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><surname>Stswin</surname></persName>
		</author>
		<idno>32.27 28.92 34.48 42.97 22.72 42.61</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Raunet</surname></persName>
		</author>
		<idno>11] 33.25 30.23 34.95 44.99 22.88 43.67 JCAS (22 ) [4] 35.99 28.29 38.06 51.00 26.66 44.75</idno>
		<imprint>
			<date>19</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><surname>Volmin</surname></persName>
		</author>
		<idno>33.85 28.40 39.38 43.76 23.90 42.63</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ms-Tfal</surname></persName>
		</author>
		<idno>41.36 36.33 41.65 59.57 27.88 51.01</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ms-Tfal</surname></persName>
		</author>
		<idno>50.34 49.15 50.17 67.37 34.67 60.50</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Table 1. Comparison of other methods and our models on EndoVis 2018 Dataset under different ratios of noise. The best results are highlighted. References</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11190</idno>
		<title level="m">robotic scene segmentation challenge</title>
		<imprint>
			<date type="published" when="2018">2018. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MetaCorrection: domain-aware meta loss correction for unsupervised domain adaptation in semantic segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3927" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint class-affinity loss correction for robust medical image segmentation with noisy labels</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-856" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part IV</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="588" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring intra-and inter-video relation for surgical semantic scene segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2991" to="3002" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning with noisy labels: exploring techniques and remedies in medical image analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101759</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Superpixel-guided iterative learning from noisy labels for medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_50</idno>
		<idno>978-3-030-87193-2 50</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part I</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="525" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Provably end-to-end label-noise learning without anchor points</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v139/li21l.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="6403" to="6413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2017.07.005</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S1361841517301135" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">S-CUDA: self-cleansing unsupervised domain adaptation for medical image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">102214</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">RAUNet: residual attention U-net for semantic segmentation of cataract surgical instruments</title>
		<author>
			<persName><forename type="first">Z.-L</forename><surname>Ni</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-36711-4_13</idno>
		<idno>978-3-030-36711-4 13</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">ICONIP 2019</title>
		<editor>
			<persName><forename type="first">Tom</forename><surname>Gedeon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kok</forename><surname>Wong</surname></persName>
		</editor>
		<editor>
			<persName><surname>Wai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Minho</forename><surname>Lee</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11954</biblScope>
			<biblScope unit="page" from="139" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Confident learning: estimating uncertainty in dataset labels</title>
		<author>
			<persName><forename type="first">C</forename><surname>Northcutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1373" to="1411" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Suk</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-bioeng-071516-044442</idno>
		<ptr target="https://doi.org/10.1146/annurev-bioeng-071516-044442.pMID" />
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">28301734</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distilling effective supervision for robust medical image segmentation with noisy labels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_63</idno>
		<idno>978-3-030-87193-2 63</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021, Part I</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="668" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Anti-interference from noisy labels: mean-teacher-assisted confident learning for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3062" to="3073" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cascaded robust learning at imperfect labels for chest X-ray segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abolmaesumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Zuluaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Racoceanu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-256" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020, Part VI</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="579" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">In vivo assessment of inflammatory bowel disease in rats with ultrahigh-resolution colonoscopic oct</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1364/BOE.453396</idno>
		<ptr target="https://opg.optica.org/boe/abstract.cfm?URI=boe-13-4-2091" />
	</analytic>
	<monogr>
		<title level="j">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2091" to="2102" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Characterizing label errors: confident learning for noisy-labeled image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_70</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-870" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020, Part I</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="721" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robust medical image segmentation from non-expert annotations with tri-network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59719-125" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020, Part IV</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="249" to="258" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
