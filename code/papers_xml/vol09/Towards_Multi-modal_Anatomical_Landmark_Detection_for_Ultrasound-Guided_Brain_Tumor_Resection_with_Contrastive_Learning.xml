<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning</title>
				<funder>
					<orgName type="full">Natural Sciences and Engineering Research Council of Canada</orgName>
					<orgName type="abbreviated">NSERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Fonds de Recherche du Québec Nature et technologies (FRQNT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Soorena</forename><surname>Salari</surname></persName>
							<email>soorena.salari@concordia.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amirhossein</forename><surname>Rasoulian</surname></persName>
							<email>ah.rasoulian@concordia.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hassan</forename><surname>Rivaz</surname></persName>
							<email>hassan.rivaz@concordia.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiming</forename><surname>Xiao</surname></persName>
							<email>yiming.xiao@concordia.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Software Engineering</orgName>
								<orgName type="institution">Concordia University</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F5BADE03EDD0320AC487B0845F164C3C</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_64</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>Anatomical landmark</term>
					<term>Contrastive learning</term>
					<term>Inter-modality</term>
					<term>Neurosurgery</term>
					<term>Intraoperative ultrasound</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Homologous anatomical landmarks between medical scans are instrumental in quantitative assessment of image registration quality in various clinical applications, such as MRI-ultrasound registration for tissue shift correction in ultrasound-guided brain tumor resection. While manually identified landmark pairs between MRI and ultrasound (US) have greatly facilitated the validation of different registration algorithms for the task, the procedure requires significant expertise, labor, and time, and can be prone to inter-and intra-rater inconsistency. So far, many traditional and machine learning approaches have been presented for anatomical landmark detection, but they primarily focus on mono-modal applications. Unfortunately, despite the clinical needs, inter-modal/contrast landmark detection has very rarely been attempted. Therefore, we propose a novel contrastive learning framework to detect corresponding landmarks between MRI and intra-operative US scans in neurosurgery. Specifically, two convolutional neural networks were trained jointly to encode image features in MRI and US scans to help match the US image patch that contain the corresponding landmarks in the MRI. We developed and validated the technique using the public RESECT database. With a mean landmark detection accuracy of 5.88±4.79 mm against 18.78±4.77 mm with SIFT features, the proposed method offers promising results for MRI-US landmark detection in neurosurgical applications for the first time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gliomas are the most common central nervous system (CNS) tumors in adults, accounting for 80% of primary malignant brain tumors <ref type="bibr" target="#b0">[1]</ref>. Early surgical treatment to remove the maximum amount of cancerous tissues while preserving the eloquent brain regions can improve the patient's survival rate and functional outcomes of the procedure <ref type="bibr" target="#b1">[2]</ref>. Although the latest multi-modal medical imaging (e.g., PET, diffusion/functional MRI) allows more precise pre-surigcal planning, during surgery, brain tissues can deform under multiple factors, such as gravity, intracranial pressure change, and drug administration. The phenomenon is referred to as brain shift, and often invalidates the pre-surgical plan by displacing surgical targets and other vital anatomies. With high flexibility, portability, and cost-effectiveness, intra-operative ultrasound (US) is a popular choice to track and monitor brain shift. In conjunction with effective MRI-US registration algorithms, the tool can help update the pre-surgical plan during surgery to ensure the accuracy and safety of the intervention.</p><p>As the true underlying deformation from brain shift is impossible to obtain and the differences of image features between MRI and US are large, quantitative validation of automatic MRI-US registration algorithms often rely on homologous anatomical landmarks that are manually labeled between corresponding MRI and intra-operative US scans <ref type="bibr" target="#b2">[3]</ref>. However, manual landmark identification requires strong expertise in anatomy and is costly in labor and time. Moreover, inter-and intra-rater variability still exists. These factors make quality assessment of brain shift correction for US-guided brain tumor resection challenging. In addition, due to the time constraints, similar evaluation of inter-modal registration quality during surgery is nearly impossible, but still highly desirable. To address these needs, deep learning (DL) holds the promise to perform efficient and automatic inter-modal anatomical landmark detection.</p><p>Previously, many groups have proposed algorithms to label landmarks in anatomical scans <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. However, almost all earlier techniques were designed for mono-modal applications, and inter-modal landmark detection, such as for USguided brain tumor resection, has rarely been attempted. In addition, unlike other applications, where the full anatomy is visible in the scan and all landmarks have consistent spatial arrangements across subjects, intra-operative US of brain tumor resection only contains local regions of the pathology with noncanonical orientations. This results in anatomical landmarks with different spatial distributions across cases. To address these unique challenges, we proposed a new contrastive learning (CL) framework to detect matching landmarks in intra-operative US with those from MRI as references. Specifically, the technique leverages two convolutional neural networks (CNNs) to learn features between MRI and US that distinguish the inter-modal image patches which are centered at the matching landmarks from those that are not. Our approach has two major novel contributions to the field. First, we proposed a multi-modal landmark detection algorithm for US-guided brain tumor resection for the first time. Second, CL is employed for the first time in inter-modal anatomical landmark detection. We developed and validated the proposed technique with the public RESECT database <ref type="bibr" target="#b9">[10]</ref> and compared its landmark detection accuracy against the popular scale-invariant feature transformation (SIFT) algorithm in 3D <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Contrastive learning has recently shown great results in a wide range of medical image analysis tasks <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. In short, it seeks to boost the similarity of feature representations between counterpart samples and decrease those between mismatched pairs. Often, these similarities are calculated based on deep feature representations obtained from DL models in the feature embedding space. This self-supervised learning set-up allows robust feature learning and embedding without explicit guidance from fine-grained image annotations, and the encoded features can be adopted in various downstream tasks, such as segmentation. A few recent works <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> explored the potential of CL in anatomical landmark annotation in head X-ray images for 2D skull landmarks. Quan et al. <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> attempted to leverage CL for more efficient and robust learning. Yao et al. <ref type="bibr" target="#b20">[21]</ref> used multiscale pixel-wise contrastive proxy tasks for skull landmark detection in X-ray images. With a consistent protocol for landmark identification, they trained the network to learn signature features within local patches centered at the landmarks. These prior works with CL focus on single-modal 2D landmark identification with systematic landmark localization protocols and sharp image contrast (i.e., skull in X-ray). In contrast, our described application is more challenging due to the 3D nature, difficulty in inter-modal feature learning, weaker anatomical contrast (i.e., MRI vs US), and variable landmark locations. In CL, many works have employed the InfoNCE loss function <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref> in attaining good outcomes. Inspired by Yao et al. <ref type="bibr" target="#b20">[21]</ref>, we aimed to use InfoNCE as our loss function with a patch-based approach. To date, CL has not been explored in multi-modal landmark detection, a unique problem in clinical applications. In this paper, to bridge this knowledge gap, we proposed a novel CL-based framework for MRI-US anatomical landmark detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods and Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and Landmark Annotation</head><p>We employed the publicly available EASY-RESECT (REtroSpective Evaluation of Cerebral Tumors) dataset <ref type="bibr" target="#b9">[10]</ref> (https://archive.sigma2.no/pages/ public/dataset Detail.jsf?id=10.11582/2020.00025) to train and evaluate our proposed method. This dataset is a deep-learning-ready version of the original RESECT database, and was released as part of the 2020 Learn2Reg Challenge <ref type="bibr" target="#b24">[24]</ref>. Specifically, EASY-RESECT contains MRI and intra-operative US scans (before resection) of 22 subjects who have undergone low-grade glioma resection surgeries. All images were resampled to a unified dimension of 256 × 256 × 288 voxels, with an isotropic resolution of ∼0.5mm. Between MRI and the corresponding US images, matching anatomical landmarks were manually labeled by experts and 15∼16 landmarks were available per case. A sample illustration of corresponding inter-modal scans and landmarks is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. For the target application, we employed the T2FLAIR MRI to pair with intra-operative US since low-grade gliomas are usually more discernible in T2FLAIR than in T1-weighted MRI <ref type="bibr" target="#b9">[10]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contrastive Learning Framework</head><p>We used two CNNs with identical architectures in parallel to extract robust image features from MRI and US scans. Specifically, these CNNs are designed to acquire relevant features from MRI and US patches, and maximize the similarity between features of corresponding patches while minimizing those between mismatched patches. Each CNN network contains six successive blocks, and each block consists of one convolution layer and one group norm, with Leaky ReLU as the activation function. Also, the convolution layer of the first and last three blocks of the network has 64 and 32 convolutional filters, respectively, and a kernel size of 3 is used across all blocks. After the convolution layers, the proposed network has two multi-layer perceptron (MLP) layers with 64 and 32 neurons and Leaky ReLU as the activation function. These MLP layers compress the extracted features from convolutional layers and produce the final feature vectors. The resulting CNN network is depicted in Fig. <ref type="figure" target="#fig_1">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Landmark Matching with a 2.5D Approach</head><p>Working with 3D images is computationally expensive and can make the model training unstable and prone to overfitting, especially when the size of the database is limited. Therefore, instead of a full 3D processing, we decided to implement a 2.5D approach <ref type="bibr" target="#b25">[25]</ref> to leverage the efficiency of 2D CNN in the CL framework for the task. In this case, we extracted a series of three adjacent 2D image patches in one canonical direction (x-, y-, or z-direction), with the middle slice centred at the true or candidate landmarks in a 3D scan to provide slight spatial context for the middle slice of interest. To construct the full 2.5D formulation, we performed the same image patch series extraction in all x-, y-, and z-directions for a landmark, and this 2.5D patch forms the basis to compute the similarity between the queried US and reference MRI patches. Note that the setup of CL requires three types of samples, anchor, positive sample pairs, and negative sample pairs. Specifically, the anchor is defined as the 2.5D MRI patch centred at a predefined landmark, a positive pair is represented by an anchor and a 2.5D US patch at the corresponding landmark, and finally, a negative pair means an anchor and a mismatched 2.5D US patch. Note that during network training, instead of 2.5D patches, we compared the 2D image patch series in one canonical direction between MRI and US, and 2D patch series in all three directions were used. During the inference stage, the similarity between MRI and US 2.5D patches was obtained by summing the similarities of corresponding 2D image patch series in each direction, and a match was determined with the highest similarity from all queried US patches. With the assumption that the brain shift moves the anatomy within a limited range, during the inference time, we searched within a range of [-5,5] mm in each direction in the US around the reference MRI landmark location to find the best match. Note that this search range is an adjustable parameter by the user (e.g., surgeons/clinicians), and when no match is found in the search range, an extended search range can be used. The general overview of the utilized framework for 2D image patch extraction is shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Landmark Matching with 3D SIFT</head><p>The SIFT algorithm <ref type="bibr" target="#b10">[11]</ref> is a well-known tool for keypoint detection and image registration. It has been widely used in multi-modal medical registration, such as landmark matching for brain shift correction in image-guided neurosurgery <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">26]</ref>. To further validate the proposed CL-based method for multi-modal anatomical landmark detection in US scans, we replicated the procedure using the 3D SIFT algorithm as follows. First, we calculated the SIFT features at the reference landmark's location in MRI. Then, we acquired a set of candidate SIFT points in the corresponding US scan. Finally, we identified the matching US landmark by selecting the top ranking candidate based on SIFT feature similarity measured with cosine similarity. Note that, for SIFT-based landmark matching, we have attempted to impose a similar spatial constraint like in the CL-based approach. However, as the SIFT algorithm pre-selects keypoint candidates based on their feature strengths, with this in consideration, we saw no major benefits by imposing the spatial constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Preprocessing</head><p>For CL training, both positive and negative sample pairs need to be created. All 2D patch series were extracted according to Sect. 3.3 with a size of 42 × 42 × 3 voxels. These sample pairs were used to train two CNNs to extract relevant image features across MRI and US leveraging the InfoNCE loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Loss Function</head><p>We used the InfoNCE loss <ref type="bibr" target="#b23">[23]</ref> for our CL framework. The loss function has been widely used and demonstrated great performance in many vision tasks. Like other contrastive loss functions, InfoNCE requires a similarity function, and we chose commonly used cosine similarity. The formulas for InfoNCE (L Inf oNCE ) and cosine similarity (CosSim) are as follows:</p><formula xml:id="formula_0">L InfoNCE = -E log exp(α) exp(α) + exp (α ) ; α = s[F θ • X A i , G β • X P i ]; α = s[F θ • X A i , G β • X N j ],<label>(1)</label></formula><formula xml:id="formula_1">s [v, w] = Cos Sim (v, w) = v • w v • w (2)</formula><p>where F θ and G β are the CNN feature extractors for MR and US patches. X A i and X P i are the cropped image patches around the corresponding landmarks in MR and US scans, respectively, and X N i is a mismatched patch in the US image to that cropped around the MRI reference landmark. Here, F θ • X A i , G β • X P i , and G β • X N j give the extracted feature vectors for MR and US patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details and Evaluation</head><p>To train our DL model, we made subject-wise division of the entire dataset into 70%:15%:15% as the training, validation, and testing sets, respectively. Also, to improve the robustness of the network, we used data augmentation for the training data by random rotation, random horizontal flip, and random vertical flip. Furthermore, an AdamW optimizer with a learning rate of 0.00001 was used, and we trained our model for 50 epochs with a batch size of 256.</p><p>In order to evaluate the performance of our technique, we used the provided ground truth landmarks from the database and calculated the Euclidean distance between the ground truths and predictions. The utilized metric is as follows: where x i and x i , and N are the ground truth landmark location, model prediction, and the total number of landmarks per subject, respectively.</p><formula xml:id="formula_2">Mean landmark identification error = 1 N N i=1 x i -x i<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Table <ref type="table" target="#tab_0">1</ref> lists the mean and standard deviation of landmark identification errors (in mm) between the predicted position and the ground truth in intra-operative US for each patient of the RESECT dataset. In the table, we also provide the severity of brain shift for each patient. Here, tissue deformation measured as mean target registration errors (mTREs) with the ground truth anatomical landmarks is classified as small (mTRE below 3 mm), median (3-6 mm), or large (above 6 mm). The results show that our CL-based landmark selection technique can locate the corresponding US landmarks with a mean landmark identification error of 5.88±4.79 mm across all cases while the SIFT algorithm has an error 18.78±4.77 mm. With a two-sided paired-samples t-test, our method outperformed the SIFT approach with statistical significance (p &lt;1e-4). When reviewing the mean landmark identification error using our proposed technique, we also found that the magnitude is associated with the level of brain shift. However, no such trend is observed when using SIFT features for landmark identification. When inspecting landmark identification errors across all subjects between the CL and SIFT techniques, we also noticed that our CL framework has significantly lower standard deviations (p &lt;1e-4), implying that our technique has a better performance consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Inter-modal anatomical landmark localization is still a difficult task, especially for the described application, where landmarks have no consistent spatial arrangement across different cases and image features in US are rough. We tackled the challenge with the CL framework for the first time. As the first step towards more accurate inter-modal landmark localization, there are still aspects to be improved. First, while the 2.5D approach is memory efficient and quick, 3D approaches may better capture the full corresponding image features. This is partially reflected by the observation that the quality of landmark localization is associated with the level of tissue shift. However, due to limited clinical data, 3D approaches caused overfitting in our network training. Second, in the current setup, we employed landmarks in pre-operative MRIs as references since its contrast is easier to understand and it allows sufficient time for clinicians to annotate the landmarks before surgery. Future exploration will also seek techniques to automatically tag MRI reference landmarks. Finally, we only employed US scans before resection since tissue removal can further complicate feature matching between MRI and US, and requires more elaborate strategies, such as those involving segmentation of resected regions <ref type="bibr" target="#b27">[27]</ref>. We will explore suitable solutions to extend the application scenarios of our proposed framework as part of the future investigation. As a baseline comparison, we employed the SIFT algorithm, which has demonstrated excellent performance in a large variety of computer vision problems for keypoint matching. However, in the described inter-modal landmark identification for US-guided brain tumor resection, the SIFT algorithm didn't offer satisfactory results. This could be due to the coarse image features and textures of intra-operative US and the differences in the physical resolution between MRI and US. One major critique for using the SIFT algorithm is that it intends to find geometrically interesting keypoints, which may not have good anatomical significance. In the RESECT dataset, eligible anatomical landmarks were defined as deep grooves and corners of sulci, convex points of gyri, and vanishing points of sulci. The relevant local features may be hard to capture with the SIFT algorithm. In this sense, DL-based approaches may be a better choice for the task. With the CL framework, our method learns the common features between two different modalities via the training process.</p><p>Besides better landmark identification accuracy, the tighter standard deviations also imply that our DL approach serves a better role in grasping the local image features within the image patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this project, we proposed a CL framework for MRI-US landmark detection for neurosurgery for the first time by leveraging real clinical data, and achieved state-of-the-art results. The algorithm represents the first step towards efficient and accurate inter-modal landmark identification that has the potential to allow intra-operative assessment of registration quality. Future extension of the method in other inter-modal applications can further confirm its robustness and accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Sample corresponding landmarks on co-registered T2FLAIR MRI and US. The arrows point to the brain tumor region.</figDesc><graphic coords="4,65,46,54,29,321,61,82,42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The proposed CNN for feature encoding from MRI and US scans.</figDesc><graphic coords="4,63,96,388,55,324,97,100,57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: An overview of the framework for image feature learning.</figDesc><graphic coords="7,54,81,54,32,314,32,128,38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Landmark identification errors (mean±std) per case in mm. Our proposed CL-based algorithm achieved a mean landmark identification error of 5.88±4.79 mm across all cases while the SIFT algorithm obtained an error of 18.78±4.77 mm. The level of brain shift is listed beside the patient ID.</figDesc><table><row><cell>Patient ID</cell><cell cols="3">Proposed CL SIFT Algorithm Patient ID</cell><cell cols="2">Proposed CL SIFT Algorithm</cell></row><row><cell>1 (Small)</cell><cell>1.80 ± 0.78</cell><cell>12.16 ± 4.75</cell><cell cols="2">15 (Medium) 3.07 ± 1.39</cell><cell>19.33 ± 4.49</cell></row><row><cell cols="2">2 (Medium) 6.16 ± 1.40</cell><cell>17.84 ± 8.50</cell><cell cols="2">16 (Medium) 6.42 ± 0.92</cell><cell>12.48 ± 4.84</cell></row><row><cell>3 (Large)</cell><cell>7.79 ± 0.55</cell><cell>13.37 ± 6.08</cell><cell>17 (Large)</cell><cell>8.13 ± 0.72</cell><cell>18.57 ± 6.27</cell></row><row><cell>4 (Small)</cell><cell>3.59 ± 0.73</cell><cell>13.97 ± 5.50</cell><cell cols="2">18 (Medium) 4.19 ± 0.87</cell><cell>13.71 ± 5.815</cell></row><row><cell>5 (Large)</cell><cell cols="2">10.65 ± 1.13 20.71 ± 6.12</cell><cell cols="2">19 (Medium) 3.97 ± 0.93</cell><cell>27.70 ± 16.49</cell></row><row><cell cols="2">6 (Medium) 2.20 ± 0.93</cell><cell>28.11 ± 11.90</cell><cell cols="2">21 (Medium) 6.01 ± 0.77</cell><cell>24.40 ± 13.73</cell></row><row><cell>7 (Small)</cell><cell>1.96 ± 0.96</cell><cell>24.07 ± 8.02</cell><cell>23 (Large)</cell><cell>6.97 ± 1.03</cell><cell>22.50 ± 6.72</cell></row><row><cell>8 (Small)</cell><cell>2.56 ± 0.79</cell><cell>19.30 ± 6.09</cell><cell>24 (Small)</cell><cell>1.33 ± 0.49</cell><cell>14.91 ± 6.09</cell></row><row><cell>12 (Large)</cell><cell cols="2">23.77 ± 0.96 22.01 ± 6.64</cell><cell>25 (Large)</cell><cell>9.94 ± 2.43</cell><cell>15.37 ± 5.42</cell></row><row><cell cols="2">13 (Medium) 6.18 ± 1.43</cell><cell>13.86 ± 6.99</cell><cell>26 (Small)</cell><cell>2.95 ± 0.88</cell><cell>17.93 ± 10.15</cell></row><row><cell cols="2">14 (Medium) 3.39 ± 0.69</cell><cell>21.67 ± 6.46</cell><cell cols="2">27 (Medium) 6.42 ± 0.76</cell><cell>19.20 ± 8.65</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. We acknowledge the support of the <rs type="funder">Natural Sciences and Engineering Research Council of Canada (NSERC)</rs> and <rs type="funder">Fonds de Recherche du Québec Nature et technologies (FRQNT)</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Progenitor cells and glioma formation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Neurol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="683" to="688" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CBTRUS statistical report: primary brain and central nervous system tumors diagnosed in the united states in 2005-2009</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Dolecek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Propp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Stroup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kruchko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuro-oncology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">suppl_5</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluation of MRI to ultrasound registration methods for brain shift correction: the CuRIOUS2018 challenge</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="777" to="786" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Label-free segmentation of COVID-19 lesions in lung CT</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2808" to="2819" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An artificial agent for anatomical landmark detection in medical images</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Ghesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46726-9_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46726-9_27" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9902</biblScope>
			<biblScope unit="page" from="229" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">You only learn once: universal anatomical landmark detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_9" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="85" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised landmark detection and classification of lung infection using transporter neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page">106345</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient and robust model-to-image alignment using 3d scale-invariant features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Toews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="282" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uncertaintyaware transformer model for anatomical landmark detection in paraspinal muscle MRIs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Salari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rasoulian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Battie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fortin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rivaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging,: Image Processing</title>
		<imprint>
			<biblScope unit="volume">12464</biblScope>
			<biblScope unit="page" from="238" to="244" />
			<date type="published" when="2023">2023</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Re trospective evaluation of cerebral tumors (RESECT): a clinical database of pre-operative MRI and intra-operative ultrasound in low-grade glioma surgeries</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fortin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Unsgård</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rivaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reinertsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3875" to="3882" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Volumetric image registration from invariant keypoints</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4900" to="4910" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Intra-class contrastive learning improves computer aided diagnosis of breast cancer in mammography</title>
		<author>
			<persName><forename type="first">K</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_6" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI: 25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contrastive learning for echocardiographic view integration</title>
		<author>
			<persName><forename type="first">L.-H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Van Der Geest</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_33" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09">v September 2022. 2022</date>
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning to classify paranasal anomalies in the maxillary sinus</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bhattacharya</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_41</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_41" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="page" from="429" to="438" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tinc: temporally informed non-contrastive learning for disease progression modeling in retinal OCT volumes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Emre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rivail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bogunović</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_60</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-7_60" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="page" from="625" to="634" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint prediction of meningioma grade and brain invasion via task-aware contrastive learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_34" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="page" from="355" to="365" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vision-language contrastive learning approach to robust automatic placenta analysis using photographic images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Gernand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mithal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mwinyelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_68</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_68" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI: 25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="page" from="707" to="716" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reliability-aware contrastive self-ensembling for semi-supervised medical image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_71</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-6_71" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Which images to label for few-shot medical landmark detection?</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20606" to="20616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.07118</idno>
		<title level="m">Information-guided pixel augmentation for pixelwise contrastive learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">One-shot medical landmark detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="177" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-3_17" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: a new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learn2reg: comprehensive multi-task medical image registration challenge, dataset and evaluation in the era of deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust landmark-based brain shift correction with a Siamese neural network in ultrasound-guided brain tumor resection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pirhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rivaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inter. J. Comput. Assisted Radiol. Surgery</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A feature-driven active framework for ultrasound-based brain shift compensation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-3_4" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Segmentation-based registration of ultrasound volumes for glioma resection in image-guided neurosurgery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Canalini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1697" to="1713" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
