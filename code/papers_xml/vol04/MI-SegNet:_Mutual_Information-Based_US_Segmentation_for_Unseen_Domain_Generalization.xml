<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Bi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair for Computer-Aided Medical Procedures and Augmented Reality</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zhongliang</forename><surname>Jiang</surname></persName>
							<email>zl.jiang@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Chair for Computer-Aided Medical Procedures and Augmented Reality</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ricarda</forename><surname>Clarenbach</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Clinic for Vascular Surgery</orgName>
								<orgName type="institution">Helios Klinikum München West</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Ghotbi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Clinic for Vascular Surgery</orgName>
								<orgName type="institution">Helios Klinikum München West</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Angelos</forename><surname>Karlas</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department for Vascular and Endovascular Surgery</orgName>
								<orgName type="institution" key="instit1">Isar University Hospital</orgName>
								<orgName type="institution" key="instit2">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chair for Computer-Aided Medical Procedures and Augmented Reality</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="130" to="140"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">492E5D0F9346D29A2188B5A4C3F48204</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_13</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ultrasound segmentation</term>
					<term>feature disentanglement</term>
					<term>domain generalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generalization capabilities of learning-based medical image segmentation across domains are currently limited by the performance degradation caused by the domain shift, particularly for ultrasound (US) imaging. The quality of US images heavily relies on carefully tuned acoustic parameters, which vary across sonographers, machines, and settings. To improve the generalizability on US images across domains, we propose MI-SegNet, a novel mutual information (MI) based framework to explicitly disentangle the anatomical and domain feature representations; therefore, robust domain-independent segmentation can be expected. Two encoders are employed to extract the relevant features for the disentanglement. The segmentation only uses the anatomical feature map for its prediction. In order to force the encoders to learn meaningful feature representations a cross-reconstruction method is used during training. Transformations, specific to either domain or anatomy are applied to guide the encoders in their respective feature extraction task. Additionally, any MI present in both feature maps is punished to further promote separate feature spaces. We validate the generalizability of the proposed domain-independent segmentation approach on several datasets with varying parameters and machines. Furthermore, we demonstrate the effectiveness of the proposed MI-SegNet serving as a pre-trained model by comparing it with state-of-the-art networks (The code is available at: https://github.com/yuan-12138/MI-SegNet).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) have achieved phenomenal success in image analysis and comparable human performance in many semantic segmentation tasks.</p><p>However, based on the assumption of DNNs, the training and testing data of the network should come from the same probability distribution <ref type="bibr" target="#b23">[23]</ref>. The generalization ability of DNNs on unseen domains is limited. The lack of generalizability hinders the further implementation of DNNs in real-world scenarios.</p><p>Ultrasound (US), as one of the most popular means of medical imaging, is widely used in daily medical practice to diagnose internal organs, such as vascular structures. Compared to other imaging methods, e.g., computed tomography (CT) and magnetic resonance imaging (MRI), US shows its advantages in terms of being radiation-free and portable. To accurately and robustly extract the vascular lumen for diagnosis, the Doppler signal <ref type="bibr" target="#b8">[9]</ref> and artery pulsation signal <ref type="bibr" target="#b5">[6]</ref> were employed to facilitate vessel segmentation. However, the US image quality is operator-dependent and sensitive to inter-machine and inter-patient variations. Therefore, the performance of the US segmentation is often decayed due to the domain shift caused by the inconsistency between the training and test data <ref type="bibr" target="#b7">[8]</ref>.</p><p>Data Augmentation. One of the most common ways of improving the generalization ability of DNNs is to increase the variability of the dataset <ref type="bibr" target="#b27">[26]</ref>. However, in most clinical cases, the number of data is limited. Therefore, data augmentation is often used as a feasible method to increase diversity. <ref type="bibr">Zhang et al.</ref> proposed BigAug <ref type="bibr" target="#b28">[27]</ref>, a deep stacked transformation method for 3D medical image augmentation. By applying a wide variety of augmentation methods to the single source training data, they showed the trained network is able to increase its performance on unseen domains. In order to take the physics of US into consideration, Tirindelli et al. proposed a physics-inspired augmentation method to generate realistic US images <ref type="bibr" target="#b21">[21]</ref>.</p><p>Image-Level Domain Adaptation. To make the network generalizable to target domains that are different from the source domain, the most intuitive way is to transfer the image style to the same domain. The work from Chen et al. achieved impressive segmentation results in MRI to CT adaptation by applying both image and feature level alignment <ref type="bibr" target="#b3">[4]</ref>. To increase the robustness of segmentation networks for US images, Yang et al. utilized a rendering network to unify the image styles of training and test data so that the model is able to perform equally well on different domains <ref type="bibr" target="#b26">[25]</ref>. Velikova et al. extended this idea by defining a common anatomical CT-US space so that the labeled CT data can be exploited to train a segmentation network for US images <ref type="bibr" target="#b24">[24]</ref>.</p><p>Feature Disentanglement. Instead of solving the domain adaptation problem directly at the image-level, many researchers focused on disentangling the features in latent space, forcing the network to learn the shared statistical shape model across different domains <ref type="bibr" target="#b1">[2]</ref>. One way of realizing this is through adversarial learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b29">28]</ref>. However, adversarial learning optimization remains difficult and unstable in practice <ref type="bibr" target="#b12">[12]</ref>. A promising solution for decoupling latent representations is to minimize a metric that can explicitly measure the shared information between different features. Mutual information (MI), which measures the amount of shared information between two random variables <ref type="bibr" target="#b9">[10]</ref>, suits this demand. Previous researches have exploited its usage in increasing the generalizability for classification networks when solving the vision recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b16">16]</ref> and US image classification <ref type="bibr" target="#b14">[14]</ref> problems. In this study, we investigate the effective way to integrate MI into a segmentation network in order to improve the adaptiveness on unseen images.</p><p>To solve the performance drop caused by the domain shift in segmentation networks, the aforementioned methods require a known target domain, e.g., CT <ref type="bibr" target="#b3">[4]</ref>, MRI <ref type="bibr" target="#b15">[15]</ref>, contrast enhanced US <ref type="bibr" target="#b29">[28]</ref>. However, compared to MRI and CT, the image quality of US is more unstable and unpredictable. It is frequently observed that the performance of a segmentation network decreases dramatically for the US images acquired from a different machine or even with a different set of acquisition parameters. In such cases, it is impractical to define a so-called target US domain. Here we introduce MI-SegNet, an MI-based segmentation network, to address the domain shift problem in US image segmentation. Specifically, the proposed network extracts the disentangled domain (image style) and anatomical (shape) features from US images. The segmentation mask is generated based on the anatomical features, while the domain features are explicitly excluded. Thereby, the segmentation network is able to understand the statistical shape model of the target anatomy and generalize to different unseen scenarios. The ablation study shows that the proposed MI-SegNet is able to increase the generalization ability of the segmentation network in unseen domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our goal is to train a segmentation network that can generalize to unseen domains and serve as a good pre-trained model for downstream tasks, while the training dataset only contains images from a single domain. To this end, the training framework should be designed to focus on the shape of the segmentation target rather than the background or appearance of the images. Following this concept of design, we propose MI-SegNet. During the training phase, a parameterised data transformation procedure is undertaken for each training image (x). Two sets of parameters are generated for spatial (a 1 , a 2 ) and domain (d 1 , d 2 ) transformation respectively. For individual input, four transformed images (x a1d1 , x a2d2 , x a1d2 , x a2d1 ) are created according to the four possible combinations of the spatial and domain configuration parameters. Two encoders (E a , E d ) are applied to extract the anatomical features (f a1 , f a2 ) and domain features (f d1 , f d2 ) separately. The mutual information between the extracted anatomical features and the domain features from the same image is computed using mutual information neural estimator (MINE) <ref type="bibr" target="#b0">[1]</ref> and minimized during training. Only the anatomical features are used to compute segmentation masks (m 1 , m 2 ). The extracted anatomical and domain features are then combined and fed into the generator network (G) to reconstruct the images ( x a1d1 , x a1d2 , x a2d1 , x a2d2 ) accordingly. Since the images are transformed explicitly, it is possible to provide direct supervision to the reconstructed images.</p><p>Notably, only two of the transformed images (x a1d1 , x a2d2 ) are fed into the network, while the other two (x a1d2 , x a2d1 ) are used as ground truth for reconstructions (Fig. <ref type="figure" target="#fig_0">1</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mutual Information</head><p>In order to decouple the anatomical and domain features intuitively, a metric that can evaluate the dependencies between two variables is needed. Mutual information, by definition, is a metric that measures the amount of information obtained from a random variable by observing another random variable. The MI is defined as the Kullback-Leibler (KL) divergence between the joint distribution and the product of marginal distributions of random variables f a and f d :</p><formula xml:id="formula_0">MI(f a ; f d ) = KL(p(f a , f d ) p(f a ) ⊗ p(f d ))<label>(1)</label></formula><p>where p(f a , f d ) is the joint distribution and p(f a ) ⊗ p(f d ) is the product of the marginal distributions. Based on the Donsker-Varadhan representation <ref type="bibr" target="#b4">[5]</ref>, the lower bound of MI can be represented as:</p><formula xml:id="formula_1">MI(f a ; f d ) ≥ E p(fa,f d ) [T (f a , f d )] -log(E p(fa)⊗p(f d ) [e T (fa,f d ) ])<label>(2)</label></formula><p>where T is any arbitrary given continuous function. By replacing T with a neural network T θMINE and applying Monte Carlo method <ref type="bibr" target="#b16">[16]</ref>, the lower bound can be calculated as:</p><formula xml:id="formula_2">MI(f a ; f d ) = 1 N N i=1 T θMINE (f a , f d ) -log 1 N N i=1 e T θ MINE (f a ,f d )<label>(3)</label></formula><p>where (f a , f d ) are drawn from the joint distribution and (f a , f d ) are drawn from the product of marginal distributions. By updating the parameters θ MINE to maximize the lower bound expression in Eq. 3, a loose estimation of MI is achieved, also known as MINE <ref type="bibr" target="#b0">[1]</ref>.</p><p>To force the anatomical and domain encoders to extract decoupled features, the MI is served as a loss to update the weights of these two encoder networks. The loss is defined as:</p><formula xml:id="formula_3">L MI = MI(f a ; f d )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Segmentation and Reconstruction</head><p>To make the segmentation network independent of the domain information, the domain features are excluded when generating the segmentation mask. Here, the segmentation loss L seg is defined in the combined form of dice loss L dice and binary cross-entropy loss L bce .</p><formula xml:id="formula_4">L seg = L dice + L bce = 1 - 1 N N n=1 2l n m n + s l n + m n + s - 1 N N n=1 (l n log m n + (1 -l n ) log(1 -m n )) (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where l is the ground truth label, m represents the predicted mask, s is added to ensure the numerical stability, and N is the mini batch size.</p><p>To ensure that the extracted anatomical and domain features can contain all the information of the input image, a generator network is used to reconstruct the image based on both features. The reconstruction loss is then defined as:</p><formula xml:id="formula_6">L rec = 1 N N n=1 1 wh (x n -x n ) 2<label>(6)</label></formula><p>where x n is the ground truth image, x n is the reconstructed image, w and h are the width and height of the image in pixel accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data Transformation</head><p>Since the training dataset only contains images from one single domain, it is necessary to enrich the diversity of the training data so that overfitting can be prevented and the generalization ability is increased. The transformation methods are divided into two categories, domain and spatial transformations. Each transformation (T ) is controlled by two parameters, probability (p) and magnitude (λ).</p><p>Domain Transformations aim to transfer the single domain images to different domain styles. Five types of transformation methods are involved in this aspect, i.e., blurriness, sharpness, noise level, brightness, and contrast. The implementations are identical to <ref type="bibr" target="#b28">[27]</ref>, except the Gaussian noise is replaced by Rayleigh noise. The possibility of all the domain transformations are empirically set to 10%.</p><p>Spatial Transformations mainly consist of two parts, crop and flip. For cropping, a window with configurable sizes ([0.7, 0.9] of the original image size) is randomly masked on the original image. Then the cropped area is resized to the original size to introduce varying shapes of anatomy. Here λ controls the size and the position of the cropping window. Besides cropping, horizontal flipping is also involved. Unlike domain transformations, the labels are also transformed accordingly by the same spatial transformation. The probability (p) of flipping is 5%, while the p for cropping is 50% to introduce varying anatomy sizes.The images are then transformed in a stacked way:</p><formula xml:id="formula_7">x aug = T (P [n],Λ[n]) (T (P [n-1],Λ[n-1]) • • • (T (P [1],Λ[1]) (x)))<label>(7)</label></formula><p>where n = 7 represents the seven different transformation methods involved in our work</p><formula xml:id="formula_8">, Λ = [λ n , λ n-1 , • • • , λ 1 ]</formula><p>represents the magnitude parameter, and</p><formula xml:id="formula_9">P = [p n , p n-1 , • • • , p 1 ]</formula><p>contains all the probability parameters for each transformations. In our setup, Λ and P can be further separated into a = [Λ a ; P a ] and d = [Λ d ; P d ] for spatial and domain transformations respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Cross Reconstruction</head><p>According to experimental findings, the MI loss indeed forces the two representations to have minimal shared information. However, the minimization of MI between the anatomical and domain features cannot necessarily make both features contain the respective information. The network goes into local optimums frequently, where the domain features are kept constant, and all the information is stored in the anatomical features. Because there is no information in the domain features, the MI between two representations is thus approaching zero. However, this is not our original intention. As a result, cross reconstruction strategy is introduced to tackle this problem. The cross reconstruction loss will punish the behavior of summarizing all the information into one representation. Thus, it can force each encoder to extract informative features accordingly and prevent the whole network from going into the local optimums.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>The training dataset consists of 2107 carotid US images of one adult acquired using Siemens Juniper US Machine (ACUSON Juniper, SIEMENS AG, Germany) with a system-predefined "Carotid" acquisition parameter. The test dataset consists of (1) ValS: 200 carotid US images which are left out from the training dataset, (2) TS1: 538 carotid US images of 15 adults from Ultrasonix device, (3) TS2: 433 US images of 2 adults and one child from Toshiba device, and (4) TS3: 540 US images of 6 adults from Cephasonics device (Cephasonics, California, USA). TS1 and TS2 are from a public database of carotid artery <ref type="bibr" target="#b17">[17]</ref>. Notably, due to the absence of annotations, the publicly accessed images were also annotated by ourselves under the supervision of US experts. The acquisition was performed within the Institutional Review Board Approval by the Ethical Commission of the Technical University of Munich (reference number 244/19 S).</p><p>All the images are resized to 256 × 256 for training and testing. We use Adam optimizer with a learning rate of 1 × 10 -4 to optimize all the parameters. The training is carried out on a single GPU (Nvidia TITAN Xp) with 12 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Comparison on Unseen Datasets</head><p>In this section, we compare the performance of the proposed MI-SegNet with other state-of-art segmentation networks. All the networks are trained on the same dataset described in Sect. 3.1 with 200 episodes.</p><p>Without Adaptation: The trained models are then tested directly on 4 different datasets described in Sect. 3.1 without further training or adaptation on the unseen domains. The dice score (DSC) is applied as the evaluation metrics. The results are shown in Table <ref type="table" target="#tab_0">1</ref>. Compared to the performance on ValS, all networks demonstrate a performance degradation on unseen datasets (TS1, TS2, and TS3). In order to validate the effectiveness of the MI loss as well as the cross reconstruction design, two ablation networks (MI-SegNet w/o L MI and MI-SegNet w/o cross rec.) are introduced here for comparison. The visual comparisons are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The results on TS1 are the best among all three unseen datasets while the scores on TS3 are the worst for most networks, which indicates that the domain similarity between the source and target domain decreases accordingly from TS1 to TS3. The MI-SegNet performs the best among others on all three unseen datasets, which showcases the high generalization ability of the proposed framework. After Adaptation: Although the proposed network achieves the best scores when applied directly to unseen domains, performance decay still occurs. Using it directly to unseen dataset with degraded performance is not practical. As a result, adaptation on the target domain is needed. The trained models in Sect. 3.2 are further trained with 5% data of each unseen test dataset. The adapted models are then tested on the rest 95% of each dataset. Notably, for the MI-SegNet only the anatomical encoder and segmentor are involved in this adaptation process, which means the network is updated solely based on L seg . The intention of this experiment is to validate whether the proposed network can serve as a good pre-trained model for the downstream task. A well-trained pre-trained model, which can achieve good results when only a limited amount of annotations is provided, has the potential to release the burden of manual labeling and adapts to different domains with few annotations. Table <ref type="table" target="#tab_1">2</ref> shows that the MI-SegNet performs the best on all test datasets. However, the difference is not that significant as in Table <ref type="table" target="#tab_0">1</ref> when no data is provided for the target domain. This is partially due to the fact that carotid artery is a relatively easy anatomy for segmentation. It is observed that when more data (10%) is involved in the adaptation process GLFR and Att-UNet tend to outperform the others and it can be therefore expected when the data size further increases all the networks will perform equally well on each test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>In this paper, we discuss the particular importance of domain adaptation for US images. Due to the low speed of sound compared to light and X-ray, the complexity of US imaging and its dependency on many parameters are more remarkable than optical imaging, X-ray, and CT. Therefore, the performance decay caused by the domain shift is a prevalent issue when applying DNNs in US images. To address this problem, a MI-based disentanglement method is applied to increase the generalization ability of the segmentation networks for US image segmentation. The ultimate goal of increasing the generalizability of the segmentation network is to apply the network to different unseen domains directly without any adaptation process. However, from the authors' point of view, training a good pre-trained model that can be adapted to an unseen dataset with minimal annotated data is still meaningful. As demonstrated in Sect. 3.2, the proposed model also shows the best performance in the downstream adaptation tasks. Currently, only the conventional image transformation methods are involved. In the future work, more realistic and US specific image transformations could be implemented to strengthen the feature disentanglement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Network structure of MI-SegNet. The green and blue arrows represent the data flow of the first (x a 1 d 1 ) and the second input image (x a 2 d 2 ), respectively. (Color figure online)</figDesc><graphic coords="4,59,46,109,94,333,61,162,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual comparison between MI-SegNet and other segmentation networks on US carotid artery datasets without adaptation. For each row, we show the input US image, the ground truth (GT), and the output of each network. Red, pink and green regions represent the false negative, false positive and true positive, respectively. (Color figure online)</figDesc><graphic coords="8,55,98,54,41,340,18,148,30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison of the proposed MI-SegNet with different segmentation networks on the US carotid artery datasets without adaptation.</figDesc><table><row><cell>Method</cell><cell>DSC</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ValS</cell><cell>TS1</cell><cell>TS2</cell><cell>TS3</cell></row><row><cell>UNet [18]</cell><cell cols="4">0.920 ± 0.080 0.742 ± 0.283 0.572 ± 0.388 0.529 ± 0.378</cell></row><row><cell>GLFR [20]</cell><cell cols="4">0.927 ± 0.045 0.790 ± 0.175 0.676 ± 0.272 0.536 ± 0.347</cell></row><row><cell>Att-UNet [19]</cell><cell cols="4">0.932 ± 0.046 0.687 ± 0.254 0.602 ± 0.309 0.438 ± 0.359</cell></row><row><cell>MedT [22]</cell><cell cols="4">0.875 ± 0.056 0.674 ± 0.178 0.583 ± 0.303 0.285 ± 0.291</cell></row><row><cell>MI-SegNet w/o LMI</cell><cell cols="4">0.928 ± 0.057 0.768 ± 0.217 0.627 ± 0.346 0.620 ± 0.344</cell></row><row><cell cols="5">MI-SegNet w/o cross rec. 0.921 ± 0.050 0.790 ± 0.227 0.662 ± 0.309 0.599 ± 0.344</cell></row><row><cell>MI-SegNet</cell><cell cols="4">0.928 ± 0.046 0.821 ± 0.146 0.725 ± 0.215 0.744 ± 0.251</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison of the proposed MI-SegNet with different segmentation networks after adaptation when 5% data of each test dataset is used for adaptation.</figDesc><table><row><cell>Method</cell><cell>DSC</cell><cell></cell></row><row><cell></cell><cell>TS1</cell><cell>TS2</cell><cell>TS3</cell></row><row><cell>UNet [18]</cell><cell cols="3">0.890 ± 0.183 0.707 ± 0.328 0.862 ± 0.139</cell></row><row><cell>GLFR [20]</cell><cell cols="3">0.915 ± 0.154 0.875 ± 0.099 0.907 ± 0.049</cell></row><row><cell cols="4">Att-UNet [19] 0.916 ± 0.117 0.876 ± 0.145 0.893 ± 0.087</cell></row><row><cell>MedT [22]</cell><cell cols="3">0.870 ± 0.118 0.837 ± 0.137 0.795 ± 0.170</cell></row><row><cell>MI-SegNet</cell><cell cols="3">0.919 ± 0.095 0.881 ± 0.111 0.916 ± 0.061</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_13.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: a review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain generalization by mutual-information regularization with pre-trained models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20050-2_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20050-2_26" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13683</biblScope>
			<biblScope unit="page" from="440" to="457" />
		</imprint>
	</monogr>
	<note>ECCV 2022</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised bidirectional crossmodality adaptation via deeply synergistic image and feature alignment for medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2494" to="2505" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Asymptotic evaluation of certain Markov process expectations for large time</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Donsker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R S</forename><surname>Varadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IV. Commun. Pure Appl. Math</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="212" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Motion magnification in robotic sonography: enabling pulsation-aware artery segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.03698</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-toimage translation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01219-9_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01219-9_11" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11207</biblScope>
			<biblScope unit="page" from="172" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online Reflective learning for robust medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_62</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-1_62" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="652" to="662" />
		</imprint>
	</monogr>
	<note>MICCAI 2022</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DopUS-Net: quality-aware robotic ultrasound imaging based on doppler signal</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Duelmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Sci. Eng</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimating mutual information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stögbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66138</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diverse image-toimage translation via disentangled representations</title>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11205</biblScope>
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01246-5_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01246-5_3" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overcoming the disentanglement vs reconstruction trade-off via Jacobian supervision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mutual information regularized feature-level Frankenstein for discriminative recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5243" to="5260" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mutual information-based disentangled neural networks for classifying unseen categories in different domains: application to fetal ultrasound imaging</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="722" to="734" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new bidirectional unsupervised domain adaptation segmentation framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ning</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-78191-0_38</idno>
		<idno>978-3-030-78191-0_38</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Feragen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sommer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12729</biblScope>
			<biblScope unit="page" from="492" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain agnostic learning with disentangled representations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5102" to="5112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Novel method for localization of common carotid artery transverse section in ultrasound images using modified Viola-Jones detector</title>
		<author>
			<persName><forename type="first">K</forename><surname>Říha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mašek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beneš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Závodná</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ultrasound Med. Biol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1887" to="1902" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention gated networks: learning to leverage salient regions in medical images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schlemper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Global and local feature reconstruction for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2273" to="2284" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking ultrasound augmentation: a physics-inspired approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tirindelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eilers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Simson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Azampour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_66</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-3_66" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="690" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Medical transformer: gated axial-attention for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_4" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A theory of the learnable</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1134" to="1142" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CACTUSS: common anatomical CT-US space for US examinations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Velikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Simson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Azampour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paprottka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention, MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="492" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_47</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_47" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generalizing deep models for ultrasound image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_57</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-3_57" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="497" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding deep learning (still) requires rethinking generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="107" to="115" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalizing deep learning for medical image segmentation to unseen domains via deep stacked transformation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2531" to="2540" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A multi-modality ovarian tumor ultrasound image dataset for unsupervised cross-domain semantic segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.06799</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
