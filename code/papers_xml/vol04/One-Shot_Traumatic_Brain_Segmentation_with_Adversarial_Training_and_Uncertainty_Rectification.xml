<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification</title>
				<funder ref="#_ZNC56kB">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenrong</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zixu</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Lichi</forename><surname>Zhang</surname></persName>
							<email>lichizhang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="120" to="129"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">1B5934734B4E5CE422B4E82CC729682A</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_12</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>One-Shot Segmentation</term>
					<term>Adversarial Training</term>
					<term>Traumatic Brain Injury</term>
					<term>Uncertainty Rectification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Brain segmentation of patients with severe traumatic brain injuries (sTBI) is essential for clinical treatment, but fully-supervised segmentation is limited by the lack of annotated data. One-shot segmentation based on learned transformations (OSSLT) has emerged as a powerful tool to overcome the limitations of insufficient training samples, which involves learning spatial and appearance transformations to perform data augmentation, and learning segmentation with augmented images. However, current practices face challenges in the limited diversity of augmented samples and the potential label error introduced by learned transformations. In this paper, we propose a novel one-shot traumatic brain segmentation method that surpasses these limitations by adversarial training and uncertainty rectification. The proposed method challenges the segmentation by adversarial disturbance of augmented samples to improve both the diversity of augmented data and the robustness of segmentation. Furthermore, potential label error introduced by learned transformations is rectified according to the uncertainty in segmentation. We validate the proposed method by the one-shot segmentation of consciousness-related brain regions in traumatic brain MR scans. Experimental results demonstrate that our proposed method has surpassed state-of-the-art alternatives. Code is available at https://github.com/ hsiangyuzhao/TBIOneShot.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic brain ROI segmentation for magnetic resonance images (MRI) of severe traumatic brain injuries (sTBI) patients is crucial in brain damage assessment and brain network analysis <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref>, since manual labeling is time-consuming and labor-intensive. However, conventional brain segmentation pipelines, such as FSL <ref type="bibr" target="#b13">[14]</ref> and FreeSurfer <ref type="bibr" target="#b3">[4]</ref>, suffer significant performance deteriorations due to skull deformation and lesion erosions in traumatic brains. Although automatic segmentation based on deep learning has shown promises in accurate segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>, these methods are still constrained by the scarcity of annotated sTBI scans. Thus, researches on traumatic brain segmentation under insufficient annotations needs further exploration.</p><p>Recently, one-shot medical image segmentation based on learned transformations (OSSLT) has shown great potential <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref> to deal with label scarcity. These methods typically utilize deformable image registration to learn spatial and appearance transformations and perform data augmentation on the single labeled image to train the segmentation, which is shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. Given a labeled image as the atlas, two unlabeled images are provided as spatial and appearance references. Appearance transform and spatial transform learned by deformable registration are applied to the atlas image to generate a pseudolabeled image to train the segmentation, and the label warped by spatial transform serves as the ground-truth. In this way, the data diversity is ensured by a large amount of unlabeled data, and the segmentation is learned by abundant pseudo images.</p><p>However, despite the previous success, the generalization ability of these methods is challenged by two issues in traumatic brain segmentation: 1) Limited diversity of generated data due to the amount of available unlabeled images. Although several studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> have proposed transformation sampling to introduce extra diversity for alleviating this issue, their strategies rely on a manualdesigned distribution, which is not learnable and limits the capacity of data augmentation. 2) The assumption that appearance transforms in atlas augmentation do not affect semantic labels in the images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>. However, this assumption neglects the presence of abnormalities in traumatic brains, such as brain edema, herniation, and erosions, which affect the appearance of brain tissues and introduce label errors.</p><p>To address the aforementioned issues, we propose a novel one-shot traumatic brain segmentation method that leverages adversarial training and uncertainty rectification. We introduce an adversarial training strategy that improves both the diversity of generated data and the robustness of segmentation, and incorporate an uncertainty rectification strategy that mitigates potential label errors in generated samples. We also quantify the segmentation difference of the same image with and without the appearance transform, which is used to estimate the uncertainty of segmentation and rectify the segmentation results accordingly. The main contributions of our method are summarized as follows: First, we develop an adversarial training strategy to enhance the capacity of data augmentation, which brings better data diversity and segmentation robustness. Second, we notice the potential label error introduced by appearance transform in current one-shot segmentation attempts, and introduce uncertainty rectification for compensation. Finally, we evaluate the proposed method on brain segmentation of sTBI patients, where our method outperforms current state-of-the-art methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of One-Shot Medical Image Segmentation</head><p>After training the unsupervised deformable registration, one-shot medical image segmentation based on learned transformations typically consists of two steps: 1) Data augmentation on the atlas image by learned transformations; 2) Training the segmentation using the augmented images. The basic workflow is shown in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. Specifically, given a labeled image x A as the atlas and its semantic labels y A , two reference images, including the spatial reference x s and appearance reference x a , are provided to augment the atlas by spatial transform φ and appearance transform ψ that are calculated by the same pretrained registration network.</p><p>For spatial transform, given an atlas image x A and a spatial reference x s , the registration network performs the deformable registration between them and predicts a deformation field φ, which is used as the spatial transform to augment the atlas image spatially. For appearance transform, given an atlas image x A and an appearance reference x a , we warp x a to x A via an inverse registration φ -1  xa and generates a inverse-warped xa = x a •φ -1  xa , and appearance transform ψ = xa -x A is calculated by the residual of inverse-warped appearance reference xa and the atlas image x A . It should be noted that the registration here is diffeomorphic to allow for inverse registration.</p><p>After acquiring both the spatial and appearance transform, the augmented atlas x g = (x A + ψ) • φ is generated by applying both transformations. The corresponding ground-truth y g = y A • φ is the atlas label warped by φ, as it is hypothesized that appearance transform does not alter the semantic labels in the atlas image. During segmentation training, a large amount of spatial and appearance reference images are sampled to ensure the diversity of x g , and the segmentation is trained with generated pairs of x g and y g . In this work, we focus on both of the two steps in OSSLT by adversarial training and uncertainty rectification, which is shown in Fig. <ref type="figure" target="#fig_0">1(b</ref>). Specifically, we generate an adversarial image x ag along with x g by adversarial training to learn better data augmentation for the atlas image, and uncertainty rectification is utilized during segmentation learning to bypass the potential label errors introduced by appearance transforms. We discuss our proposed adversarial training and uncertainty rectification in Sect. 2.2 and Sect. 2.3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adversarial Training</head><p>Although the diversity of generated pairs of x g and y g is ensured by the increased number of unlabeled images as references, such a setting requires a large amount of unlabeled data. Inspired by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>, we adopt the adversarial training strategy to increase both the data diversity and the segmentation robustness, which is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Given a learned spatial transform φ, appearance transform ψ, and a generated image x g augmented by φ and ψ, our adversarial training is decomposed into the following 3 steps: First, we feed x g into the adversarial network and generate two sampling layers α and β activated by Sigmoid function. The sampling layers α and β have the same spatial shape with φ and ψ respectively, and each location in the sampling layers represents the sampling amplitude of the original transform, ranging from 0 to 1. In this way, the diversity of spatial and appearance transforms is significantly improved by the infinite possibilities of sampling layers:</p><formula xml:id="formula_0">φ a = φ × α, ψ a = ψ × β (1)</formula><p>Second, by applying the sampled transformations φ a and ψ a to the atlas x A , we acquire an adversarial generated image x ag . We expect x ag to add extra diversity of data augmentation and maintain realistic as well:</p><formula xml:id="formula_1">x ag = (x A + ψ a ) • φ a (2)</formula><p>Finally, both the original generated image x g and the adversarial generated image x ag are fed to the segmentation network, and the training objective is the min-max game of the adversarial network and the segmentation network. Thus, we ensure the diversity of generation and robustness of segmentation simultaneously by adversarial training:</p><formula xml:id="formula_2">min g(•;θ h ) max f (•;θg) L adv (ŷ g , ŷag )<label>(3)</label></formula><formula xml:id="formula_3">L adv (ŷ g , ŷag ) = ŷg • ŷag ŷg 2 • ŷag 2<label>(4)</label></formula><p>where f (•; θ g ) and g(•; θ h ) denote the adversarial network and the segmentation network, respectively. ŷg and ŷag are the segmentation predictions of x g and x ag .</p><p>It should be noted that since the spatial transformation applied to x g and x ag is different, the loss calculation is performed in atlas space by inverse registration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Uncertainty Rectification</head><p>Most of the current methods hypothesize that appearance transformation does not alter the label of the atlas. However, in brain scans with abnormalities such as sTBI, the appearance transformation may include edema, lesions, and etc, which may affect the actual semantic labels of the atlas and weaken the accuracy of segmentation. Inspired by <ref type="bibr" target="#b17">[18]</ref>, we introduce uncertainty rectification to bypass the potential label errors. Specifically, given a segmentation network, fully augmented image x g = (x A + ψ) • φ and spatial-augmented image x As = x A • φ are fed to the network. The only difference between x g and x As is that the latter lacks the transformation on appearance. Thus, the two inputs x g and x As serve different purposes. Fully augmented image x g is equipped with more diversity compared with x As , as appearance transform has been applied to it, while the spatial augmented image x As has more label authenticity and could guide a more accurate segmentation.</p><p>The overall supervised loss consists of two items. First, the segmentation loss L seg = L ce (ŷ As , y g ) of spatial-augmented image x As guides the network to learn spatial variance only, where ŷAs is the prediction of x As , and L ce denotes cross-entropy loss. Second, the rectified segmentation loss L rseg of x g guides the network to learn segmentation under both spatial and appearance transformations. We adopt the KL-divergence D KL of the segmentation results ŷAs and ŷg as the uncertainty in prediction <ref type="bibr" target="#b17">[18]</ref>. Compared with Monte-Carlo dropout <ref type="bibr" target="#b4">[5]</ref>, KL-divergence for uncertainty estimation does not require multiple forward runs. A voxel with a greater uncertainty indicates a higher possibility of label error in the corresponding location of x g , thus, the supervision signal of this location should be weakened to reduce the effect of label errors:</p><formula xml:id="formula_4">L rseg = exp[-D KL (ŷ g , ŷAs )]L ce (ŷ g , y g ) + D KL (ŷ g , ŷAs )<label>(5)</label></formula><p>Thus, the overall supervised loss L sup = L seg +L rseg is the segmentation loss L seg of x As and the rectified segmentation loss L rseg of x g . We apply the overall supervised loss L sup on both x g and x ag in practice. During segmentation training, the linear summation of supervised segmentation loss L sup and adversarial loss L adv is minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>We have collected 165 MR T1-weighted scans with sTBI from 2014-2017, acquired on a 3T Siemens MR scanner from Huashan hospital. Among the 165 MR scans, 42 scans are labeled with the 17 consciousness-related brain regions (see appendix for details) while the remaining are left unlabeled, since the manual labeling requires senior-level expertise. Informed consent was obtained from all patients for the use of their information, medical records, and MRI data. All MR scans are linearly aligned to the MNI152 template using FSL <ref type="bibr" target="#b13">[14]</ref>. For the atlas image, we randomly collect a normal brain scan at the same institute and label its 17 consciousness-related brain regions as well. During training, the labeled normal brain scan serves as the atlas image, and the 123 unlabeled sTBI scans are used as spatial or appearance references. For one-shot setting, the labeled sTBI scans are used for evaluation only and completely hidden during training. In order to validate the effectiveness of the proposed one-shot segmentation method, a U-Net <ref type="bibr" target="#b12">[13]</ref> trained on the labeled scans by 5-fold cross validation is used as the reference of fully supervised segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>The framework is implemented with PyTorch 1.12.1 on a Debian Linux server with an NVIDIA RTX 3090 GPU. In practice, the registration network is based on VoxelMorph <ref type="bibr" target="#b0">[1]</ref> and pretrained on the unlabeled sTBI scans. During adversarial training, the registration is fixed, while the adversarial network and segmentation network are trained alternately. Both the adversarial network and the segmentation network is based on U-Net <ref type="bibr" target="#b12">[13]</ref> architectures and optimized by SGD optimizer with a momentum of 0.9 and weight decay of 1 × 10 -4 . The initial learning rate is set to 1×10 -2 and is slowly reduced with polynomial strategy. We have pretrained the registration network for 100 epochs, and trained the adversarial network and segmentation network for 100 epochs as well. The batch size is set to 1 during training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>We have evaluated our one-shot segmentation framework on the labeled sTBI MR scans in the one-shot setting, which means that only one labeled image is available to learn the segmentation. We explore the effectiveness of the proposed adversarial training and uncertainty rectification, and make the comparison with state-of-the-art alternatives, by reporting the Dice coefficients.</p><p>First, we conduct an ablation study to evaluate the impact of adversarial training and uncertainty rectification, which is shown in Table <ref type="table" target="#tab_0">1</ref>   <ref type="formula">1</ref>). This is because the rectified segmentation loss L rseg does not provide enough supervision signal in regions where the segmentation uncertainty is too high, and thus we need segmentation loss L seg to compensate it. Finally, by applying both adversarial training and uncertainty rectification, the proposed method yields the best results with an improved Dice coefficient of approximately 5% and a lower standard deviation of segmentation performance, which is shown in No. <ref type="bibr" target="#b4">(5)</ref>. Experimental results demonstrate that the proposed adversarial training and uncertainty rectification can both contribute to the segmentation performance, compared with the baseline setting.</p><p>Then, we compare the proposed method with three cutting-edge alternatives in one-shot medical image segmentation, including BrainStorm <ref type="bibr" target="#b16">[17]</ref>, LT-Net <ref type="bibr" target="#b14">[15]</ref>, and DeepAtlas <ref type="bibr" target="#b15">[16]</ref>, which is shown in Table <ref type="table" target="#tab_1">2</ref>. The proposed method outperforms other segmentation methods with an average Dice score of 56.3%, higher than all of the previous state-of-the-art methods, and achieves the highest and second highest segmentation performance in all of the 17 brain regions. Also, it should be noted that the proposed method has a lower standard deviation in terms of segmentation performance, which also demonstrates the robustness of our method. However, despite the promising results of the proposed method, we have observed that the performance gain of proposed method in certain brain regions that are usually very small is not significant. The plausible reason is that the uncertainty of these small brain regions is too high and affects the segmentation.</p><p>For qualitative evaluation, we have visualized the segmentation results of the proposed method and the above-mentioned alternatives, which are shown in Fig. <ref type="figure" target="#fig_3">3</ref>. The red bounding boxes indicate the regions where our method achieves better segmentation results compared with the alternatives. Overall, our method achieves more accurate segmentation, especially in the brain regions affected by ventriculomegaly, compared with BrainStorm, LT-Net, and DeepAtlas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we present a novel one-shot segmentation method for severe traumatic brain segmentation, a difficult clinical scenario where limited annotated data is available. Our method addresses the critical issues in sTBI brain segmentation, namely, the need for diverse training data and mitigation of potential label errors introduced by appearance transforms. The introduction of adversarial training enhances both the data diversity and segmentation robustness, while uncertainty rectification is designed to compensate for the potential label errors. The experimental results on sTBI brains demonstrate the efficacy of our proposed method and its advantages over state-of-the-art alternatives, highlighting the potential of our method in enabling more accurate segmentation in severe traumatic brains, which may aid clinical pipelines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of one-shot segmentation based on learned transformations (OSSLT). Compared with previous methods, we aim to use adversarial training and uncertainty rectification to address the current challenges in OSSLT.</figDesc><graphic coords="3,44,43,54,14,335,59,160,84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Framework of adversarial training, which includes adversarial transform sampling, adversarial image generation and segmentation difference calculation.</figDesc><graphic coords="4,58,05,57,74,338,38,159,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. In No. (1), the baseline OSSLT method yields an average Dice of 51.42%. In No. (3), adversarial training adds a performance gain of approximately 3% compared with No. (1), and is also superior to predefined uniform transform sampling in No. (2). The results indicate that adversarial training brings both extra diversity of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual comparison of different segmentation methods. (Color figure online)</figDesc><graphic coords="8,55,98,54,11,340,30,134,59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation studies on different components. Rand. denotes uniform transform sampling following<ref type="bibr" target="#b6">[7]</ref>, Adv. denotes adversarial training, and UR denotes uncertainty rectification.</figDesc><table><row><cell>Experiments</cell><cell>Dice Coefficient (%)</cell></row><row><cell>(1) Baseline</cell><cell>51.4 ± 20.2</cell></row><row><cell>(2) Baseline + Rand</cell><cell>53.1 ± 20.6</cell></row><row><cell>(3) Baseline + Adv</cell><cell>54.3 ± 19.7</cell></row><row><cell>(4) Baseline + UR (wo/ Lseg)</cell><cell>48.8 ± 23.5</cell></row><row><cell>(5) Baseline + UR (w/ Lseg)</cell><cell>53.3 ± 19.0</cell></row><row><cell>(6) Baseline + Adv. + UR (w/ Lseg)</cell><cell>56.3 ± 18.8</cell></row><row><cell cols="2">(Upper Bound) Fully-Supervised U-Net 61.5 ± 16.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with alternative segmentation methods (%).</figDesc><table><row><cell></cell><cell>IR</cell><cell>IL</cell><cell>TR</cell><cell>TL</cell><cell>ICRA</cell><cell>ICRP</cell></row><row><cell cols="7">BrainStorm 46.2±21.9 43.2±21.3 66.7±18.1 57.1±7.3 46.8±11.7 34.5±21.0</cell></row><row><cell>LT-Net</cell><cell cols="6">45.4±24.6 52.4±21.0 59.5±17.9 58.3±10.4 47.0±19.7 42.6±17.3</cell></row><row><cell cols="7">DeepAtlas 50.3±25.4 44.2±27.0 56.4±16.1 57.5±8.7 50.4±13.8 38.8±18.7</cell></row><row><cell cols="2">Proposed 52.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>6±24.8 54.4±22.9 62.0±16.2 58.4±10.2 51.1±16.6 44.2±17.5</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>ICLA</cell><cell>ICLP</cell><cell>CRA</cell><cell>CRP</cell><cell>CLA</cell><cell>CLP</cell></row><row><cell cols="7">BrainStorm 46.1±14.3 38.8±14.6 50.1±16.6 48.0±13.2 50.9±11.1 54.2±10.8</cell></row><row><cell>LT-Net</cell><cell cols="6">43.5±19.2 42.6±17.2 52.2±18.4 48.4±12.5 48.5±13.0 56.5±11.9</cell></row><row><cell cols="7">DeepAtlas 53.8±15.6 46.2±16.9 46.4±19.3 42.5±13.9 43.4±15.8 52.1±13.0</cell></row><row><cell cols="2">Proposed 53.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>0±16.7 44.7±18.1 56.1±15.0 53.2±12.5 51.7±12.9 60.8±11.1</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>MCR</cell><cell>MCL</cell><cell>IPL</cell><cell>IPR</cell><cell>B</cell><cell>Average</cell></row><row><cell cols="7">BrainStorm 52.5±18.7 57.9±11.9 50.1±17.1 52.2±16.5 86.4±4.3 51.9±19.0</cell></row><row><cell>LT-Net</cell><cell cols="6">56.1±20.1 59.2±12.3 43.0±15.4 53.5±18.2 87.0±6.2 52.7±19.6</cell></row><row><cell cols="7">DeepAtlas 55.9±17.9 54.6±13.2 44.0±17.5 50.8±15.3 88.9±4.0 51.5±19.8</cell></row><row><cell>Proposed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>61.0±16.9 61.1±11.1 48.1±18.9 53.7±17.0 90.1±4.2 56.3±18.8</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62001292</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZNC56kB">
					<idno type="grant-number">62001292</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 12.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Voxelmorph: a learning framework for deformable medical image registration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1788" to="1800" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhancing MR image segmentation with realistic adversarial data augmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102597</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling the probabilistic distribution of unlabeled data for one-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1246" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freesurfer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="774" to="781" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning better registration to learn better few-shot medical image segmentation: authenticity, diversity, and robustness</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep complementary joint model for complex scene registration and few-shot segmentation on medical images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58523-5_45</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58523-545" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12363</biblScope>
			<biblScope unit="page" from="770" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The self and its resting state in consciousness: an investigation of the vegetative state</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hum. Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1997" to="2008" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial data augmentation via deformation statistics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Olut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58526-6_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58526-638" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12374</biblScope>
			<biblScope unit="page" from="643" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust hydrocephalus brain segmentation via globally and locally spatial guidance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87586-2_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87586-210" />
	</analytic>
	<monogr>
		<title level="m">MLCN 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13001</biblScope>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How are different neural networks related to consciousness?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Neurol</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="605" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust brain magnetic resonance image segmentation for hydrocephalus patients: hard and soft attention</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="385" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Advances in functional and structural MR image analysis and implementation as FSL</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="208" to="S219" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LT-Net: label transfer by learning reversible voxel-wise correspondence for one-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9162" to="9171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DeepAtlas: joint semi-supervised learning of image registration and segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_47</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data augmentation using learned transformations for one-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8543" to="8553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1106" to="1120" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
