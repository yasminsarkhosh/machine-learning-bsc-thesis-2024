<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees</title>
				<funder ref="#_X4AhCvz #_3dngqZX">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_SWz4cJ2">
					<orgName type="full">Interdisciplinary Program of Shanghai Jiao Tong University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zixu</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenrong</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengjun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhong</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lichi</forename><surname>Zhang</surname></persName>
							<email>lichizhang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
							<email>qianwang@shanghaitech.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="110" to="119"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">027C8A189E58FC3715A506A5A8AECC66</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_11</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knee Segmentation</term>
					<term>Multi-view MRI</term>
					<term>Super Resolution</term>
					<term>Graph Representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Magnetic Resonance Imaging (MRI) has become an essential tool for clinical knee examinations. In clinical practice, knee scans are acquired from multiple views with stacked 2D slices, ensuring diagnosis accuracy while saving scanning time. However, obtaining fine 3D knee segmentation from multi-view 2D scans is challenging, which is yet necessary for morphological analysis. Moreover, radiologists need to annotate the knee segmentation in multiple 2D scans for medical studies, bringing additional labor. In this paper, we propose the Cross-view Aligned Segmentation Network (CAS-Net) to produce 3D knee segmentation from multi-view 2D MRI scans and annotations of sagittal views only. Specifically, a knee graph representation is firstly built in a 3D isotropic space after the super-resolution of multi-view 2D scans. Then, we utilize a graph-based network to segment individual multi-view patches along the knee surface, and piece together these patch segmentations into a complete knee segmentation with help of the knee graph. Experiments conducted on the Osteoarthritis Initiative (OAI) dataset demonstrate the validity of the CAS-Net to generate accurate 3D segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Magnetic resonance imaging (MRI) plays a pivotal role in knee clinical examinations. It provides a non-invasive and accurate way to visualize various internal knee structures and soft tissues <ref type="bibr" target="#b1">[2]</ref>. With its excellent contrast and resolution, MRI can help radiologists detect and diagnose knee injuries or diseases. The choice of MRI sequences and acquisition techniques depends on the specific clinical scenario. While the acquisition of a 3D sequence can render the entire knee as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a), it is not always feasible in clinical practice where scanner resource is limited <ref type="bibr" target="#b12">[13]</ref>. In contrast, multi-view 2D scans can offer higher intra-slice resolution, better tissue contrast, and shorter scanning time <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, as displayed in Fig. <ref type="figure" target="#fig_0">1(b)-(d</ref>). Therefore, multi-view 2D scans are widely used for diagnostics in clinical practice.</p><p>Although multi-view 2D MRI scanning can provide sufficient image quality for clinical diagnosis in most cases <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref>, it is challenging for estimating the corresponding 3D knee segmentation, which is essential for the functional and morphological analysis <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>. Previous studies mostly rely on 3D sequences to conduct image segmentation and morphology-based knee analysis <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>, which is impractical for clinical data of only 2D scans.</p><p>The cross-view consistency of multi-view knee MR scans provides the basis for generating 3D segmentation from 2D scans, as shown in Fig. <ref type="figure" target="#fig_0">1(e)</ref>. Some studies have already utilized the cross-view consistency for medical image applications. For example, Liu et al. <ref type="bibr" target="#b9">[10]</ref> proposed a Transformer-based method to fuse multi-view 2D scans for ventricular segmentation, by learning the crossview consistency with the self-attention mechanism. Perslev et al. proposed to extract multi-view 2D slices from 3D images for 3D segmentation on knees <ref type="bibr" target="#b11">[12]</ref> and brains <ref type="bibr" target="#b10">[11]</ref>. Li et al. <ref type="bibr" target="#b8">[9]</ref> transferred widely used sagittal segmentation to coronal and axial views based on cross-view consistency. Zhuang et al. <ref type="bibr" target="#b21">[22]</ref> introduced a unified knee graph architecture to fuse the multi-view MRIs in a local manner for knee osteoarthritis diagnosis. These studies have demonstrated the value of cross-view consistency in medical analysis.</p><p>In this paper, we propose a novel framework, named Cross-view Aligned Segmentation Network (CAS-Net), to generate 3D knee segmentation from clinical multi-view 2D scans. Moreover, following the convention of radiologists, we require the supervising annotation for the sagittal segmentation only. We first align the multi-view knee MRI scans into an isotropic 3D volume by superresolution. Then we sample multi-view patches and construct a knee graph to cover the whole knee joint. Next, we utilize a graph-based network to derive a fine 3D segmentation. We evaluate our proposed CAS-Net on the Osteoarthritis Initiative (OAI) dataset, demonstrating its effectiveness in cross-view 3D segmentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The goal of CAS-Net is to generate 3D knee segmentation with multi-view 2D scans and sagittal segmentation annotation. The overall architecture is shown in Fig. <ref type="figure" target="#fig_1">2</ref>, which consists of two major parts.</p><p>1 Knee Graph Construction. We conduct super-resolution on multi-view 2D scans and align them in a 3D volume. Then, we sample multi-view patches along bone-cartilage interfaces and construct a knee graph. 2 Graph-based Segmentation Network. We deal with the 3D multi-view patches on the knee graph by a 3D UNet <ref type="bibr" target="#b13">[14]</ref> combined with Graph Transformer Network (GTN) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref>. We also randomly mask out views of patches to enforce cross-view consistency during network learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knee Graph Construction</head><p>The 2D clinical multi-view scans are acquired from non-orthogonal angles and have large differences in inter-and intra-slice spacing (e.g. 3.5 mm v.s. 0.3 mm in clinical practice). Therefore, we use super-resolution to unify them into one 3D volume with a pseudo segmentation (with tolerable errors), and sample the volume into graph representation.</p><p>Isotropic Super-Resolution. We first apply super-resolution <ref type="bibr" target="#b18">[19]</ref> to the multiview 2D scans, which include sagittal, coronal and axial scans, for representing them in the same space and reducing the large inter-slice spacing. The process is shown in the left part of Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Specifically, we follow <ref type="bibr" target="#b18">[19]</ref> by projecting multi-view 2D scans into the world coordinate system (WCS) and treating each scan as a continuous function of coordinates. Then, we take advantage of the continuity of coordinates to synthesize an isotropic 3D volume, by querying from the learned function for the coordinates at a equidistantly distributed interval. Since each 2D scan is reconstructed from the same set of WCS coordinates, the multi-view volumes after super-resolution are naturally aligned in the WCS space. Therefore, we concatenate these multi-view volumes as a multi-channel 3D image.</p><p>Moreover, we train a 2D nnUNet <ref type="bibr" target="#b5">[6]</ref> on the sagittal 2D scan and apply it to the sagittal 3D volume slice-by-slice, to acquire a pseudo-3D segmentation. The pseudo-3D segmentation is the basis of knee graph construction. It also provides supervision for the graph-based segmentation network; yet it contains many errors caused by 2D segmentation, e.g., unsmooth segmentation boundaries of bones and cartilages. Knee Graph Sampling. Errors in the pseudo-3D segmentation are distributed around bone surfaces. We sample multi-view patches from these areas and refine their segmentations in 3D. Previous studies have demonstrated the usefulness of organizing knee patches with graph-based representation <ref type="bibr" target="#b21">[22]</ref>. Therefore, we follow this approach and construct the knee graph G.</p><p>We start extracting points on the bone surface by calculating the bone boundaries on the pseudo-3D segmentation. Then, we uniformly sample from these points until the average distance between them is about 0.8 times the patch size. These sampled points are collected as the vertices V in the graph. V stores the center-cropped multi-view patches P and their WCS positional coordinates C. The distribution of the sampled points ensures that multi-view patches centercropped on V can fully cover the bone surfaces and bone-cartilage interfaces. Finally, we connect the vertices in V using edges E, which enables the vertex features to be propagated on the knee graph. The edges are established by connecting each vertex to its k (k = 10, following <ref type="bibr" target="#b21">[22]</ref>) nearest neighbors in V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph-Based Segmentation Network</head><p>Graph-Based UNet. The knee graph, which is constructed early, is input to a UNet combined with a 3-layer Graph Transformer Network (GTN) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref> to generate patch segmentation, as shown on the right part of Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>To encode the multi-view patches P as vertex features H in the bottleneck of the UNet, we first apply average pooling to H and then add the linearlyprojected WCS coordinate C to enable convolution of H along the graph. Specifically, we compute H (0) = AvgPooling(H) + Linear(C). Next, we pass the resulting vertex features H (0) through a graph convolutional network. Note that the knee graphs have varying numbers of vertices and edges, we utilize the Graph Transformer Networks (GTN) to adapt it.</p><p>Similar to Transformers <ref type="bibr" target="#b17">[18]</ref>, the GTN generates three attention embeddings Q, K, and V from H (i) by linear projection in the i-th layer. The difference is that GTN uses the adjacency matrix A to restrict the self-attention computation to between adjacent vertices instead of all of them:</p><formula xml:id="formula_0">α = Softmax(A QK T √ d ),</formula><p>where α is the self-attention, denotes dot product, and d is the length of attention embeddings. Then, the local feature H (i) in the i-th layer can be computed as H (i) = σ(αV ), where σ is feedforward operation.</p><p>After the attention-based graph representation, H (3) is repeated to match the shape of H and then summed up, serving as the input to the UNet decoder. And the UNet decoder derives the 3D patch segmentation according to H (3)  and skip connection. We further project the patch segmentations back into the pseudo-3D segmentation, thereby obtaining a rectified 3D knee segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random View Masking.</head><p>To reduce impacts of the errors in the initial pseudo-3D segmentation, we randomly masked out views (channels) in the multi-view (multi-channel) patches during training. The 3D volume, obtained from 2D scans by super-resolution, has a clearer tissue texture in the plane parallel to its original scanning plane. Thus, the random view masking helps to produce more accurate segmentation boundaries by forcing the network to learn from different combinations of patch views (channels). We set the probability of each view to be masked as 0.25. During inference, complete multi-view patches are used, reducing errors caused by the pseudo-3D segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and Experimental Settings</head><p>We have tested the CAS-Net on the Osteoarthritis Initiative (OAI) dataset <ref type="bibr" target="#b4">[5]</ref>, which contains multi-view 2D scans and 3D volumes aligned in the same WCS space. The 3D DESS modality in the OAI dataset has segmentation annotation <ref type="bibr" target="#b0">[1]</ref>, which is used as the ground truth to evaluate the 3D segmentation performance in our work. The SAG IW TSE LEFT, COR MPR LEFT, and AX MPR LEFT modalities in the OAI dataset are selected as multi-view 2D scans. For the 3D DESS is unsmooth when it is resampled to 2D scans, we recruit a radiologist to manually annotate the femur, tibia, their attached cartilages, and meniscus in 2D scans. The sagittal annotation is used in our method, and the coronal or axial annotation is set as the ground truth for evaluation. We extract 350 subjects for our experiments, with 210 for training, 70 for validation, and 70 for testing. We report the results of the testing set in the tables below. The 2D annotations are released in https://github.com/zixuzhuang/OAI seg.</p><p>In the following, we refer to the femur bone and tibia bone as FB and TB for short, their attached cartilages as FC and TC, and meniscus as M. We use GTN and RM to denote the Graph Transformer Network and random masking strategies, respectively. SR and NN stand for super-resolution and nearest neighbor interpolation.</p><p>To ensure the fairness of the comparison, we use the same training and testing settings for all experiments: PyTorch1.9.0, RTX 3090, learning rate is 3e-4, 100 epochs, cosine learning rate decay, Adam optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>To find the optimal setting for the CAS-Net, we apply different hyper-parameters on it and test them on cross-view 2D segmentation. We evaluate the performance of the CAS-Net with different patch size, and then remove the GTN, random masking, and super-resolution to see the effect of each component. To ensure the multi-view slices can be aligned in the same space without super-resolution, we use trilinear interpolation as the alternative. The results are shown in Table <ref type="table" target="#tab_1">1</ref>. As shown in Table <ref type="table" target="#tab_1">1</ref>, the best performance appears when patch size is 128 (86.8% mean Dice ratio in coronal view), and the removing of GTN (86.2%), random masking (85.7%) or super-resolution (82.4%) always lead to performance drop. The patch size of 64 is too small to capture local appearance of the knee, and thus causes performance degradation. GTN enables the network to capture knee features globally and therefore brings performance improvement. Random masking encourages CAS-Net to learn knee segmentation with aligned views across different planes, avoiding errors caused by super-resolution and 3D pseudo segmentation, thus resulting in better performance. Super-resolution reduces the aliasing effect during resampling, resulting in better segmentation. Therefore, we select the patch size as 128, and keep GTN, RM and SR enabled in the next experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation of 3D Segmentation Performance</head><p>We report the performance of the CAS-Net for 3D knee segmentation. To the best of our knowledge, CAS-Net is the first method that can perform knee segmentation from multi-view 2D scans and sagittal segmentation annotation. Therefore, we compare CAS-Net with the following settings: (1) the sagittal annotation that is resampled to 3D DESS space by nearest neighbor interpolation; (2) the pseudo-3D segmentation generated in Sect. 2.1; (3) the 3D prediction from 3D nnUNet based on 3D DESS images and annotations, which is considered as the upper bound for this task.</p><p>As can be seen from Table <ref type="table" target="#tab_2">2</ref>, the CAS-Net is capable of producing excellent 3D segmentation results with only sagittal annotation, as demonstrated in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation of Cross-View Projection of 3D Segmentation</head><p>The 3D segmentation results inferred by CAS-Net can be resampled onto unannotated 2D scans to achieve cross-view segmentation. We evaluate the cross-view segmentation performance on the coronal view, which is shown in Table <ref type="table" target="#tab_3">3</ref>. We compare with the three alternative methods: (1) the sagittal annotation that is projected to the coronal view by nearest neighbor interpolation, named NN in Table <ref type="table" target="#tab_3">3</ref>, which is considered as the lower bound for the cross-view segmentation task;</p><p>(2) the pseudo-segmentation generated in Sect. 2.1 projected to the coronal view, named as Pseudo Seg. in Table <ref type="table" target="#tab_3">3;</ref><ref type="table"></ref>   CAS-Net not only eliminates some segmentation errors but also provides smooth segmentation boundaries in cross-view segmentation tasks. As seen in Table <ref type="table" target="#tab_3">3</ref> (Row 1/2) and Fig. <ref type="figure" target="#fig_4">3(b</ref>) and (c), while the super-resolution method has improved the mean Dice ratio by only 0.7%, it has resulted in noticeably smoother segmentation boundary of the knee tissue. The CAS-Net then further eliminates errors in Pseudo Segmentation, resulting in better segmentation outcomes and leading to additional improvements (86.8% mean coronal Dice).</p><p>Compared with nnUNet, there is still improvement room for CAS-Net in the case of small tissues, such as meniscus (79.9% vs. 92.9% Dice ratio). This is likely caused by subtle movements of subjects during the acquisition of multi-view 2D scans. As shown in Fig. <ref type="figure" target="#fig_5">4</ref>, compared to sagittal MRI, FC is slightly shifted upwards in coronal and 3D MRIs. Thus the segmentation is aligned in sagittal view but not in coronal and 3D. We have examined subjects in the dataset that have lower performance (around 50% mean Dice ratio) and confirmed this observation. On the contrary, given correctly aligned multi-view 2D scans, CAS-Net can achieve a mean Dice ratio of 95%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have proposed a novel framework named CAS-Net for generating cross-view consistent 3D knee segmentation via super-resolution and graph representation with clinical 2D multi-view scans and sagittal annotations. By creating a detailed 3D knee segmentation from clinical 2D multi-view MRI, our framework provides significant benefits to morphology-based knee analysis, with promising applications in knee disease analysis. We believe that this framework can be extended to other 3D segmentation tasks, and we intend to explore those possibilities. However, it should be noted that the performance of the CAS-Net is affected by misaligned multi-view images, as shown in Fig. <ref type="figure" target="#fig_5">4</ref>. After removing the misaligned cases, the femur cartilage dice can be increased from 81.7% to 87.3% in 3D segmentation. We plan to address this issue by incorporating multimodal alignment in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of knee MRI scans in OAI dataset. (a) is a slice of 3D knee volume, which takes much longer scanning time; (b-d) are slices of multi-view 2D knee scans from sagittal, coronal, and axial views; (e) is the multi-view knee MRI scans aligned in the world coordinate system (WCS).</figDesc><graphic coords="2,72,48,54,20,307,27,59,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The overall architecture of the proposed Cross-view Aligned Segmentation Network (CAS-Net). It is composed of two major parts: (1) Knee Graph Construction, which involves super-resolution of the multi-view 2D scans into a 3D volume and representing with a knee graph; (2) Graph-based Segmentation Network, which segments patches on the knee graph, resulting in 3D segmentation.</figDesc><graphic coords="3,42,06,53,99,340,30,130,72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 (</head><label>3</label><figDesc>Fig.3(f). In comparison to 3D nnUNet that utilizes 3D images and annotations, CAS-Net provides better performance on FB (96.7% vs 92.2% mean 3D Dice) and TB (95.8% vs 90.2%), since the 3D DESS sequence has a lower contrast of bone and muscle, resulting in more errors. Additionally, although there is roughly a 16% performance gap of cartilages compared to 3D nnUNet, CAS-Net demonstrates satisfactory outcomes in FC (81.7%) and M (81.0%) due to multi-view image localization errors like slight knee movements during scanning, resulting in misaligned segmentation generation from the multi-view scans and 3D annotation, as shown in Fig.4. Furthermore, our method significantly improves 3D mean segmentation performance (86.0%) in comparison to the two techniques of using NN interpolation directly (83.9%) and employing super-resolution to generate pseudo sagittal segmentation (84.6%). These experimental findings indicate that our approach effectively leverages multi-view 2D scans and sagittal segment annotations to produce superior 3D segmentation.</figDesc><graphic coords="7,41,13,366,59,342,61,125,05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. We test our model on OAI data and in-the-wild clinical data. Here are two example cases with comparison of different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 3 )</head><label>3</label><figDesc>the 2D prediction from 2D nnUNet based on coronal 2D scans and annotations, which is considered as the upper bound for the cross-view segmentation task. The visualized results are shown in Fig. 3(b)-(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The multi-view scans and 3D image are misaligned due to knee motion during scanning, though the segmentation have the correct shape as in sagittal view. The white cross indicates the same position in WCS space but cartilage is moved.</figDesc><graphic coords="8,70,47,490,85,311,41,58,84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Resample Projection Knee Graph Construction Graph-Based Segmentation Network</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Graph-Based UNet</cell></row><row><cell></cell><cell>Super</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Resolution</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Axial Slices</cell><cell>Multi-view Volume</cell><cell></cell><cell></cell><cell></cell><cell>GTN</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Multi-view</cell><cell></cell><cell>Patch</cell><cell>Axial</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Patch</cell><cell></cell><cell>Segmentation</cell><cell>Segmentation</cell></row><row><cell></cell><cell>Pseudo 3D</cell><cell></cell><cell>( )</cell><cell>Layer</cell><cell>Layer</cell><cell>( + )</cell></row><row><cell>Coronal Slices</cell><cell>Segmentation</cell><cell></cell><cell></cell><cell>Norm &amp;</cell><cell>Norm &amp;</cell></row><row><cell></cell><cell>2D nnUNet</cell><cell>Knee Graph Representation</cell><cell>( )</cell><cell>Linear Layer</cell><cell cols="2">Feed Forward</cell><cell>( + )</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Matrix Multiplication</cell><cell>Dot Product</cell><cell>Softmax</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Vertex Feature</cell><cell cols="2">Adjacency Matrix</cell><cell>Refined 3D</cell><cell>Coronal</cell></row><row><cell>Sagittal Slices</cell><cell cols="2">Sagittal Segmentation</cell><cell cols="4">Graph Transformer Network (GTN)</cell><cell>Segmentation</cell><cell>Segmentation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Results from a controlled ablation study of the proposed CAS-Net, examining the effects of patch size, GTN, randomly masking (RM), and super-resolution (SR).</figDesc><table><row><cell cols="2">Patch Size Method</cell><cell>Coronal Dice Ratio (%)</cell><cell>Axial Dice Ratio (%)</cell></row><row><cell></cell><cell></cell><cell>FB FC TB TC M</cell><cell>Mean FB FC TB TC M</cell><cell>Mean</cell></row><row><cell>64</cell><cell></cell><cell cols="2">96.1 79.3 95.3 80.7 79.1 86.1 96.9 79.9 96.9 78.6 76.9 85.8</cell></row><row><cell>128</cell><cell></cell><cell cols="2">96.5 79.7 96.0 81.9 79.9 86.8 97.1 80.1 97.1 78.8 77.8 86.2</cell></row><row><cell>128</cell><cell cols="3">w/o GTN 96.1 79.0 95.5 81.0 79.3 86.2 97.1 79.7 97.1 78.4 77.8 86.0</cell></row><row><cell>128</cell><cell cols="3">w/o RM 95.4 78.4 95.0 80.7 79.0 85.7 97.0 80.1 97.1 78.0 77.0 85.8</cell></row><row><cell>128</cell><cell>w/o SR</cell><cell cols="2">96.0 74.1 95.2 74.7 72.0 82.4 96.5 74.2 96.3 73.6 70.7 82.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results of the comparison of different methods in 3D segmentation.</figDesc><table><row><cell>Methods</cell><cell cols="3">Image View Annotation View 3D Dice Ratio (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>FB FC TB TC M</cell><cell>Mean</cell></row><row><cell>NN</cell><cell>-</cell><cell>Sagittal</cell><cell cols="2">96.1 76.4 95.2 72.5 79.7 83.9</cell></row><row><cell cols="2">Pseudo Seg Sagittal</cell><cell>Sagittal</cell><cell cols="2">96.3 79.0 95.2 72.6 79.7 84.6</cell></row><row><cell cols="2">3D nnUNet 3D DESS</cell><cell>3D DESS</cell><cell cols="2">92.2 97.8 90.2 90.2 89.8 92.0</cell></row><row><cell>Ours</cell><cell cols="2">Multi-View Sagittal</cell><cell cols="2">96.7 81.7 95.8 74.9 81.0 86.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Results of the comparison of different methods in 2D segmentation.</figDesc><table><row><cell>Methods</cell><cell cols="3">Image View Annotation View Coronal Dice Ratio (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>FB FC TB TC M</cell><cell>Mean</cell></row><row><cell>NN</cell><cell>-</cell><cell>Sagittal</cell><cell cols="2">96.1 76.4 95.2 71.5 79.7 83.8</cell></row><row><cell cols="2">Pseudo Seg Sagittal</cell><cell>Sagittal</cell><cell cols="2">96.3 79.1 95.3 72.1 79.7 84.5</cell></row><row><cell>nnUNet</cell><cell>3D DESS</cell><cell>Coronal</cell><cell cols="2">98.9 92.2 98.8 92.8 92.9 95.1</cell></row><row><cell>Ours</cell><cell>Sagittal</cell><cell>Sagittal</cell><cell cols="2">96.5 79.7 96.0 81.9 79.9 86.8</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62001292</rs>), and partially supported by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">U22A20283</rs>), and the <rs type="funder">Interdisciplinary Program of Shanghai Jiao Tong University</rs> (No. <rs type="grantNumber">YG2023LC07</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_X4AhCvz">
					<idno type="grant-number">62001292</idno>
				</org>
				<org type="funding" xml:id="_3dngqZX">
					<idno type="grant-number">U22A20283</idno>
				</org>
				<org type="funding" xml:id="_SWz4cJ2">
					<idno type="grant-number">YG2023LC07</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated segmentation of knee bone and cartilage combining statistical shape knowledge and convolutional neural networks: data from the osteoarthritis initiative</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ambellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ehlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zachow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="109" to="118" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Studying osteoarthritis with artificial intelligence applied to magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">F</forename><surname>Calivà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Namiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubreuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pedoia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ozhinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Rheumatol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inter-subject comparison of MRI knee cartilage thickness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carballido-Gamio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="120" to="135" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09699</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recent advances in osteoarthritis imagingthe osteoarthritis initiative</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Nevitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Rheumatol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="622" to="630" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diagnostic advantage of thin slice 2D MRI and multiplanar reconstruction of the knee joint using deep learning based denoising approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kakigi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluation of the menisci of the knee joint using three-dimensional isotropic resolution fast spin-echo imaging: diagnostic performance in 250 patients with surgical correlation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kijowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Blankenbaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Del Rio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>De Smet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Skeletal Radiol</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="169" to="178" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cross-view label transfer in knee MR segmentation using iterative context learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-60548-3_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-60548-310" />
	</analytic>
	<monogr>
		<title level="m">DART/DCL -2020</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12444</biblScope>
			<biblScope unit="page" from="96" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transfusion: multi-view divergent fusion for medical image segmentation with transformers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_47</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-947" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="485" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One network to segment them all: a general, lightweight system for accurate 3D medical image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Perslev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Dam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-84" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-cohort automatic knee MRI segmentation with multi-planar U-nets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Perslev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Runhaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Dam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1650" to="1663" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using deep learning to accelerate knee MRI at 3 T: results of an interchangeability study</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AJR Am. J. Roentgenol</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1421</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalizability of deep learning segmentation algorithms for automated assessment of cartilage morphology and MRI relaxometry</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1029" to="1039" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diagnosis of knee meniscal injuries by using three-dimensional MRI: a systematic review and meta-analysis of diagnostic performance</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shakoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="435" to="445" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The relationship between knee adduction moment and cartilage and meniscus morphology in women with osteoarthritis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Vanwanseele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Osteoarthritis Cartilage</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="894" to="901" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Arbitrary reduction of MRI slice spacing based on local-aware implicit representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11346</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relationship of 3D meniscal morphology and position with knee pain in subjects with knee osteoarthritis: a pilot study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wenger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="211" to="220" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knee cartilage defect assessment by graph representation and surface convolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="368" to="379" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local graph fusion of multi-view MR images for knee osteoarthritis diagnosis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_53</idno>
		<idno>978-3-031-16437-8 53</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="554" to="563" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
