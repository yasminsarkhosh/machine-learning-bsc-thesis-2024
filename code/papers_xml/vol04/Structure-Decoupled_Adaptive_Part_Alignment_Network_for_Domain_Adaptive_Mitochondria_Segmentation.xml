<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation</title>
				<funder ref="#_NKDn2aD #_DacuGKE">
					<orgName type="full">National Nature Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rui</forename><surname>Sun</surname></persName>
							<email>issunrui@mail.ustc.edu.cn</email>
							<idno type="ORCID">0000-0002-8009-4240</idno>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huayu</forename><surname>Mai</surname></persName>
							<idno type="ORCID">0000-0001-8131-6944</idno>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Naisong</forename><surname>Luo</surname></persName>
							<idno type="ORCID">0000-0002-1488-3081</idno>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
							<email>tzzhang@ustc.edu.cn</email>
							<idno type="ORCID">0000-0003-1856-9564</idno>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Deep Space Exploration Lab</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
							<email>zwxiong@ustc.edu.cn</email>
							<idno type="ORCID">0000-0002-9787-7460</idno>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Wu</surname></persName>
							<email>fengwu@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="523" to="533"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">CBB02734AC93B267B74875286B680D14</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_50</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Mitochondria segmentation</term>
					<term>Unsupervised domain adaptation</term>
					<term>Electron microscopy images</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing methods for unsupervised domain adaptive mitochondria segmentation perform feature alignment via adversarial learning, and achieve promising performance. However, these methods neglect the differences in structure of long-range sections. Besides, they fail to utilize the context information to merge the appropriate pixels to construct a part-level discriminator. To mitigate these limitations, we propose a Structure-decoupled Adaptive Part Alignment Network (SAPAN) including a structure decoupler and a part miner for robust mitochondria segmentation. The proposed SAPAN model enjoys several merits. First, the structure decoupler is responsible for modeling long-range section variation in structure, and decouple it from features in pursuit of domain invariance. Second, the part miner aims at absorbing the suitable pixels to aggregate diverse parts in an adaptive manner to construct partlevel discriminator. Extensive experimental results on four challenging benchmarks demonstrate that our method performs favorably against state-of-the-art UDA methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic mitochondria segmentation from electron microscopy (EM) volume is a fundamental task, which has been widely applied to basic scientific research and R. Sun and H. Mai-Equal Contribution.</p><p>clinical diagnosis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>. Recent works like <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> have achieved conspicuous achievements attributed to the development of deep neural networks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. However, these approaches tend to suffer from severe performance degradation when evaluated on target volume (i.e., target domain) that are sampled from a different distribution (caused by different devices used to image different organisms and tissues) compared to that of training volume (i.e., source volume/domain). Therefore, how to alleviate this gap to empower the learned model generalization capability is very challenging.</p><p>In this work, we tackle the unsupervised domain adaptive (UDA) problem, where there are no accessible labels in the target volume. To alleviate this issue, representative and competitive methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> attempt to align feature distribution from source and target domains via adversarial training in a pixel-wise manner, that is, learning domain-invariant features against the substantial variances in data distribution. However, after an in-depth analysis of adversarial training, we find two key ingredients lacking in previous works. <ref type="bibr" target="#b0">(1)</ref> Structure-entangled feature. In fact, there exists a large variation in the complex mitochondrial structure from different domains, as proven in <ref type="bibr" target="#b27">[28]</ref>. However, previous methods only focus on domain gap either in each individual section or in adjacent sections, and neglect the differences in morphology and distribution (i.e., structure) of long-range sections, leading to sub-optimal results in the form of hard-to-align features. (2) Noisy discrimination. Intuitively, humans can quickly distinguish domain of images with the same categories from cluttered backgrounds by automatically decomposing the foreground into multiple local parts, and then discriminate them in a fine-grained manner. Inspired by this, we believe that during adversarial learning, relying solely on context-limited pixel-level features to discriminate domains will inevitably introduce considerable noise, considering that the segmentation differences between the source and target domain are usually in local parts (e.g., boundary). Thus, it is highly desirable to make full use of the context information to merge the appropriate neighboring pixel features to construct a part-level discriminator.</p><p>In this paper, we propose a Structure-decoupled Adaptive Part Alignment Network (SAPAN) including a structure decoupler and a part miner for robust mitochondria segmentation. <ref type="bibr" target="#b0">(1)</ref> In the structure decoupler, we draw inspiration from <ref type="bibr" target="#b24">[25]</ref> to model long-range section variation in distribution and morphology (i.e., structural information), and decouple it from features in pursuit of domain invariance. In specific, we first prepend a spatial smoothing mechanism for each pixel in the current section to seek the corresponding location of other sections to attain the smoothed features, which are subsequently modulated to obtain decoupled features with easy-to-align properties. (2) Then, we devise a part miner as discriminator, which can dynamically absorb the suitable pixels to aggregate diverse parts against noise in an adaptive manner, thus the detailed local differences between the source and target domain can be accurately discriminated. Extensive experimental results on four challenging benchmarks demonstrate that our method performs favorably against SOTA UDA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Domain adaptation in EM volume <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> has attracted the attention of more and more researchers due to the difficulty in accessing manual annotation. In <ref type="bibr" target="#b28">[29]</ref>, they employ self-training paradigm while in <ref type="bibr" target="#b17">[18]</ref> adversarial training is adopted and performs better. To further improve the domain generalization, <ref type="bibr" target="#b4">[5]</ref> considering the inter-section gap. However, those methods neglect the differences in morphology and distribution (i.e., structure) of long-range sections. In addition, existing adversarial training <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18]</ref> adopt context-limited pixel-wise discriminator leading to sub-optimal results. Fig. <ref type="figure">1</ref>. Overview of our method. There are two main modules in SAPAN, i.e., the structure decoupler for decoupling the feature from the domain-specific structure information and the part miner for adaptively discovering different parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The domain adaptive mitochondria segmentation task aims at learning an accurate segmentation model in the target domain based on a labeled source domain EM volume</p><formula xml:id="formula_0">V S = {X S i , Y S i } N i=1 and an unlabeled target domain EM volume V T = {X T j } M j=1 .</formula><p>As shown in Fig. <ref type="figure">1</ref>, given an EM section X i ∈ R H×W (omit the superscript S/T for convenience) from either domain, the encoder of U-Net <ref type="bibr" target="#b19">[20]</ref> first extracts its feature map F i ∈ R h×w×C , where h, w and C denote the height, width and channel number of the feature map respectively. A structure decoupler is applied to decouple the extracted feature map from domain-specific structure knowledge, which involves a channel-wise modulation mechanism. Subsequently, the decoupled feature map F i is fed into the decoder of U-Net which outputs prediction of the original size. In the end, we design a part miner to dynamically divide foreground and background into diverse parts in an adaptive manner, which are utilized to facilitate adversarial training. The details are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structure Decoupler</head><p>In order to bridge the gap in mitochondrial structure between domains, a structure decoupler is designed to decouple the extracted feature map from domainspecific structure information, realized by a channel-wise modulation mechanism.</p><p>To better model the structure information, we first apply attention-based spatial smoothing for adjacent sections. Concretely, given feature maps of adjacent sections F i and F i+1 , we define the spatial smoothing S i+1 (•) w.r.t F i+1 as: each pixel f i,j ∈ R 1×C (j = 1, 2, ..., hw) in feature map F i as query, the feature map of adjacent section F i+1 ∈ R h×w×C as keys and values. Formally,</p><formula xml:id="formula_1">S i+1 (f i,j ) = softmax( f i,j F T i+1 √ C )F i+1 , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where the T refers to the matrix transpose operation and the √ C is a scaling factor to stabilize training. We compute the structure difference D i ∈ R h×w×C between F i and its adjacent F i+1 and F i-1 by:</p><formula xml:id="formula_3">D i = [F i -S i (F i+1 )] + [F i -S i (F i-1 )].</formula><p>(</p><formula xml:id="formula_4">)<label>2</label></formula><p>The final structure embedding e ∈ R 1×C for each domain is calculated by exponential momentum averaging batch by batch:</p><formula xml:id="formula_5">e b = 1 B B i=1 GAP(D i ), e = θe + (1 -θ)e b ,<label>(3)</label></formula><p>where GAP(•) denotes the global average pooling, B denotes the batch size and θ ∈ [0, 1] is a momentum coefficient. In this way, e S and e T condense the structure information for the whole volume of the corresponding domain. To effectively mitigate the discrepancy between different domains, we employ channel-wise modulation to decouple the feature from the domain-specific structure information. Taking source domain data F S i as example, we first produce the channel-wise modulation factor γ S ∈ R 1×C and β S ∈ R 1×C conditioned on e S :</p><formula xml:id="formula_6">γ S = G γ (e S ), β S = G β (e S ),<label>(4)</label></formula><p>where G γ (•) and G β (•) shared by the source domain and target domain, are implemented by two linear layers and an activation layer. Then the decoupled source feature can be obtained by:</p><formula xml:id="formula_7">F S i = γ S F S i + β S , (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where denotes element-wise multiplication. The decoupled target feature F T i can be acquired in a similar way by Eq. ( <ref type="formula" target="#formula_6">4</ref>) and Eq. ( <ref type="formula" target="#formula_7">5</ref>). Subsequently, the structure decoupled feature is fed into the decoder of U-Net, which outputs the foreground-background probability map Y i ∈ R 2×H×W . The finally prediction Y i ∈ R H×W can be obtained through simply apply argmax(•) operation on Y i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Part Miner</head><p>As far as we know, a good generator is inseparable from a powerful discriminator.</p><p>To inspire the discriminator to focus on discriminative regions, we design a part miner to dynamically divide foreground and background into diverse parts in an adaptive manner, which are classified by the discriminator D part subsequently.</p><p>To mine different parts, we first learn a set of part filters P = {p k } 2K k=1 , each filter p k is represented as a C-dimension vector to interact with the feature map F (omit the subscript i for convenience). The first half {p k } K k=1 are responsible for dividing the foreground pixels into K groups and vice versa. Take the foreground filters for example. Before the interaction between p k and F, we first filter out the pixels belonging to the background using downsampled prediction Y . Then we get K activation map A i ∈ R h×w by multiplying p k with the masked feature map:</p><formula xml:id="formula_9">A i = sigmoid(p k ( Y F) T ).<label>(6)</label></formula><p>In this way, the pixels with a similar pattern will be highlighted in the same activation map. And then, the foreground part-aware prototypes P = { p k } K k=1 can be got by:</p><formula xml:id="formula_10">p k = GAP(A i F).<label>(7)</label></formula><p>Substituting Y with (1 -Y ), we can get the background part-aware prototypes P = { p k } 2K k=K+1 in the same manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Objectives</head><p>During training, we calculate the supervise loss with the provided label Y S i of the source domain by:</p><formula xml:id="formula_11">L sup = 1 N N i=1 CE( Y S i , Y S i ),<label>(8)</label></formula><p>where the CE(•, •) refers to the standard cross entropy loss.</p><p>Considering the part-aware prototypes may focus on the same, making the part miner degeneration, we impose a diversity loss to expand the discrepancy among part-aware prototypes. Formally,</p><formula xml:id="formula_12">L div = 2K i=1 2K j=1,i =j cos( p i , p j ),<label>(9)</label></formula><p>where the cos(•, •) denotes cosine similarity between two vectors. The discriminator D part takes p k as input and outputs a scalar representing the probability that it belongs to the target domain. The loss function of D part can be formulated as: As a result, the overall objective of our SAPAN is as follows:</p><formula xml:id="formula_13">L part = 1 2K 2K k=1 [CE(D part ( p S k ), 0) + CE(D part ( p T k ), 1)]. (<label>10</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">L = L sup + λ div × L div -λ part × L part , (<label>11</label></formula><formula xml:id="formula_16">)</formula><p>where the λ div and λ part are the trade-off weights. The segmentation network and the D part are trained alternately by minimizing the L and the L part , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metric</head><p>Dataset. We evaluate our approach on three widely used EM datasets: the VNC III <ref type="bibr" target="#b2">[3]</ref> dataset, the Lucchi dataset <ref type="bibr" target="#b8">[9]</ref> and the MitoEM dataset <ref type="bibr" target="#b27">[28]</ref>. These datasets exhibit significant diversity making the domain adaptation task challenging. The VNC III <ref type="bibr" target="#b2">[3]</ref>  Metrics. Following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref>, four widely used metrics are used for evaluation, i.e., mean Average Precision (mAP), F1 score, Mattews Correlation Coefficient (MCC) <ref type="bibr" target="#b13">[14]</ref> and Intersection over Union (IoU).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We adopt a five-stage U-Net with feature channel number of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">32,</ref><ref type="bibr">48,</ref><ref type="bibr">64,</ref><ref type="bibr">80]</ref>.</p><p>During training, we randomly crop the original EM section into 512 × 512 with random augmentation including flip, transpose, rotate, resize and elastic transformation. All models are trained 20,000 iterations using Adam optimizer with batch size of 12, learning rate of 0.001 and β of (0.9, 0.99). The λ div , λ part and K are set as 0.01, 0.001 and 4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with SOTA Methods</head><p>Two groups of experiments are conducted to verify the effectiveness of our method. Note that "Oracle" means directly training the model on the target domain dataset with the ground truth and "NoAdapt" means training the model only using the source domain dataset without the target domain dataset. Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref> present the performance comparison between our method and other competitive methods. Due to the similarity between time series (video) and space series (EM volume), the SOTA domain adaptive video segmentation method <ref type="bibr" target="#b3">[4]</ref> is also compared. We consistently observe that our method outperforms all other models on two groups of experiments, which strongly proves the effectiveness of our method. For example, our SAPAN enhances the IoU of VNC III → Lucchi-Test and Lucchi-Train to 72.8% and 77.1%, outperforming DA-ISC by significant margins of 4.1% and 2.8%. Compared with the pixel-wise alignment, the part-aware alignment can make the model fully utilize the global context information so as to perceive the mitochondria comprehensively.</p><p>On the MitoEM dataset with a larger structure discrepancy, our SAPAN can achieve 75.6% and 80.6% IoU on the two subsets respectively, outperforming DA-ISC by 0.8% and 1.2%. It demonstrates the remarkable generalization capacity of SAPAN, credited to the structure decoupler, which can effectively alleviate the domain gap caused by huge structural difference.</p><p>Figure <ref type="figure">2</ref> shows the qualitative comparison between our SAPAN and other competitive methods including UALR <ref type="bibr" target="#b28">[29]</ref>, DAMT-Net <ref type="bibr" target="#b17">[18]</ref>, DA-VSN <ref type="bibr" target="#b3">[4]</ref> and ISC <ref type="bibr" target="#b4">[5]</ref>. We can observe that other methods tend to incorrectly segment the background region or fail to activate all the mitochondria. We deem the main reason is that the large domain gap severely confuses the segmentation model. With the assistance of the structure decoupler and the part miner, SAPAN is more robust in the face of the large domain gap and generates a more accurate prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study and Analysis</head><p>To look deeper into our method, we perform a series of ablation studies on VNC III → Lucchi-Test to analyze each component of our SAPAN, including the Structure Decoupler (SD) and the Part Miner (PM). Note that we remove all modules except the U-Net and the pixel-wise discriminator as our baseline. Hyperparameters are discussed in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Components.</head><p>As shown in Table <ref type="table" target="#tab_2">3</ref>, both structure decoupler and part miner bring a certain performance lift compared with the baseline. ( <ref type="formula" target="#formula_1">1</ref>) With the utilization of SD, a 3.0% improvement of IoU can be observed, indicating that decoupling the feature from domain-specific information can benefit the domain adaptation task. In Fig. <ref type="figure">3</ref>, we visualize the feature distribution with/without SD using t-SNE <ref type="bibr" target="#b10">[11]</ref>. We can see that SD makes the feature distribution more compact and effectively alleviates the domain gap. (2) The introduction of PM achieves further accuracy gains, mainly ascribed to the adaptive part alignment mechanism. As shown in Fig. <ref type="figure" target="#fig_2">4</ref>, the different prototypes focus on significant distinct areas. The discriminator benefits from the diverse parts-aware prototypes, which in turn promotes the segmentation network.</p><p>Effectiveness of the Attention-Based Smoothing. As shown in Table <ref type="table" target="#tab_3">4</ref>, abandoning the spatial smoothing operation makes the performance decrease. Compared with simply employing a convolution layer for smoothing, attentionbased smoothing contributes to a remarkable performance (72.8% vs. 70.6% IoU), thanks to the long-range modeling capabilities of the attention mechanism.</p><p>Effectiveness of the Ways Modeling Part-Aware Prototypes. In Table <ref type="table" target="#tab_4">5</ref>, fg. means only focusing on foreground and vice versa. Neglecting L div leads to severe performance degradation, that is because L div is able to prevent the prototypes from focusing on similar local semantic clues. And simultaneously modeling foreground/background prototypes brings further improvement, demonstrating there is a lot of discriminative information hidden in the background region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a structure decoupler to decouple the distribution and morphology, and a part miner to aggregate diverse parts for UDA mitochondria segmentation. Experiments show the effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>dataset contains 20 sections of size 1024 × 1024. The training and testing set of Lucchi [9] dataset are both 165 × 1024 × 768. The MitoEM dataset consists of two subsets of size 1000 × 4096 × 4096, dubbed MitoEM-R and MitoEM-H respectively. The ground-truth of their training set (400) and validation set (100) are publicly available.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Qualitative comparison with different methods. Note that the red and green contours denote the ground-truth and prediction. And we mark significant improvements using blue boxes. (Color figure online)</figDesc><graphic coords="7,136,29,417,80,252,88,127,78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visualization of activation maps A for different prototypes in part miner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with other SOTA methods on the Lucchi dataset. Note that "VNC III → Lucchi-Test" means training the model with VNC III as source domain and Lucchi training set as target domain and testing it on Lucchi testing set, and vice versa.</figDesc><table><row><cell>Methods</cell><cell cols="6">VNC III → Lucchi-Test VNC III → Lucchi-Train</cell></row><row><cell></cell><cell cols="2">mAP F1</cell><cell cols="3">MCC IoU mAP F1</cell><cell>MCC IoU</cell></row><row><cell>Oracle</cell><cell cols="6">97.5 92.9 92.3 86.8 99.1 94.2 93.7 88.8</cell></row><row><cell>NoAdapt</cell><cell cols="6">74.1 57.6 58.6 40.5 78.5 61.4 62.0 44.7</cell></row><row><cell>Y-Net [19]</cell><cell>-</cell><cell cols="2">68.2 -</cell><cell>52.1 -</cell><cell cols="2">71.8 -</cell><cell>56.4</cell></row><row><cell>DANN [2]</cell><cell>-</cell><cell cols="2">68.2 -</cell><cell>51.9 -</cell><cell cols="2">74.9 -</cell><cell>60.1</cell></row><row><cell cols="2">AdaptSegNet [24] -</cell><cell cols="2">69.9 -</cell><cell>54.0 -</cell><cell cols="2">79.0 -</cell><cell>65.5</cell></row><row><cell>UALR [29]</cell><cell cols="6">80.2 72.5 71.2 57.0 87.2 78.8 77.7 65.2</cell></row><row><cell>DAMT-Net [18]</cell><cell>-</cell><cell cols="2">74.7 -</cell><cell>60.0 -</cell><cell cols="2">81.3 -</cell><cell>68.7</cell></row><row><cell>DA-VSN [4]</cell><cell cols="6">82.8 75.2 73.9 60.3 91.3 83.1 82.2 71.1</cell></row><row><cell>DA-ISC [5]</cell><cell cols="6">89.5 81.3 80.5 68.7 92.4 85.2 84.5 74.3</cell></row><row><cell>Ours</cell><cell cols="6">91.1 84.1 83.5 72.8 94.4 86.7 86.1 77.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with other SOTA methods on the MitoEM dataset. Note that "MitoEM-R → MitoEM-H" means training the model with MitoEM-R training set as the source domain and MitoEM-H training set as the target domain and testing it on MitoEM-H validation set, and vice versa.</figDesc><table><row><cell>Method</cell><cell cols="4">MitoEM-R → MitoEM-H MitoEM-H → MitoEM-R</cell></row><row><cell></cell><cell>mAP F1</cell><cell>MCC IoU</cell><cell>mAP F1</cell><cell>MCC IoU</cell></row><row><cell>Oracle</cell><cell cols="4">97.0 91.6 91.2 84.5 98.2 93.2 92.9 87.3</cell></row><row><cell>NoAdapt</cell><cell cols="4">74.6 56.8 59.2 39.6 88.5 76.5 76.8 61.9</cell></row><row><cell>UALR [29]</cell><cell cols="4">90.7 83.8 83.2 72.2 92.6 86.3 85.5 75.9</cell></row><row><cell cols="5">DAMT-Net [18] 92.1 84.4 83.7 73.0 94.8 86.0 85.7 75.4</cell></row><row><cell>DA-VSN [4]</cell><cell cols="4">91.6 83.3 82.6 71.4 94.5 86.7 86.3 76.5</cell></row><row><cell>DA-ISC [5]</cell><cell cols="4">92.6 85.6 84.9 74.8 96.8 88.5 88.3 79.4</cell></row><row><cell>Ours</cell><cell cols="4">93.9 86.1 85.5 75.6 97.0 89.2 88.8 80.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation study on components.</figDesc><table><row><cell>SD PM mAP F1</cell><cell cols="2">MCC IoU</cell></row><row><cell cols="2">85.3 80.3 79.4</cell><cell>67.5</cell></row><row><cell cols="2">88.8 82.6 81.9</cell><cell>70.5</cell></row><row><cell cols="2">87.9 81.6 80.9</cell><cell>69.1</cell></row><row><cell cols="3">91.1 84.1 83.5 72.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on smoothing operations (SMT).</figDesc><table><row><cell></cell><cell>mAP F1</cell><cell cols="2">MCC IoU</cell></row><row><cell>w/o SMT</cell><cell cols="2">84.6 81.5 80.7</cell><cell>68.9</cell></row><row><cell cols="3">Conv. SMT 85.9 82.6 82.0</cell><cell>70.6</cell></row><row><cell cols="4">Atten. SMT 91.1 84.1 83.5 72.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation on different ways for modeling part prototypes.</figDesc><table><row><cell>fg. bg. L div mAP F1</cell><cell cols="2">MCC IoU</cell></row><row><cell cols="2">75.3 75.5 74.7</cell><cell>61.2</cell></row><row><cell cols="2">88.4 82.2 81.4</cell><cell>69.9</cell></row><row><cell cols="3">91.1 84.1 83.5 72.8</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work was partially supported by the <rs type="funder">National Nature Science Foundation of China</rs> (Grant <rs type="grantNumber">62022078</rs>, <rs type="grantNumber">62021001</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NKDn2aD">
					<idno type="grant-number">62022078</idno>
				</org>
				<org type="funding" xml:id="_DacuGKE">
					<idno type="grant-number">62021001</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 50.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mitochondria and Ca2+ in cell physiology and pathophysiology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Duchen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell Calcium</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="339" to="348" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Segmented anisotropic sstem dataset of neural tissue</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gerhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fetter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Figshare</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptive video segmentation via temporal consistency regularization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8053" to="8064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptive mitochondria segmentation via enforcing inter-section consistency</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-89" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Advanced deep networks for 3D mitochondria instance segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contrastive learning for mitochondria segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3496" to="3500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">PDAM: a panoptic-level feature alignment framework for unsupervised domain adaptive instance segmentation in microscopy images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="154" to="165" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning for structured prediction using approximate subgradient descent with working sets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1987" to="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Camouflaged instance segmentation via explicit de-camouflaging</title>
		<author>
			<persName><forename type="first">N</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="17918" to="17927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DualRel: semi-supervised mitochondria segmentation from a prototype perspective</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="19617" to="19626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Biology of mitochondria in neurodegenerative diseases</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prog. Mol. Biol. Transl. Sci</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="355" to="415" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Comparison of the predicted and observed secondary structure of T4 phage lysozyme</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biochimica et Biophysica Acta (BBA)-Protein Structure</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="442" to="451" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mitochondria and diabetes. An intriguing pathogenetic role</title>
		<author>
			<persName><forename type="first">P</forename><surname>Newsholme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gaudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Mitochondrial Med</title>
		<imprint>
			<biblScope unit="page" from="235" to="247" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic instance segmentation of mitochondria in electron microscopy data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De Folter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Spiers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Strange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Collinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioRxiv</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2025" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive template transformer for mitochondria segmentation in electron microscopy images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised mitochondria segmentation in EM images via domain adaptive multi-task learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Sig. Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1199" to="1209" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain adaptive segmentation in volume electron microscopy imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Roels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hennies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Saeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kreshuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1519" to="1522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lesion-aware transformers for diabetic retinopathy grading</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10938" to="10947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Appearance prompt vision transformer for connectome reconstruction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Alignment before aggregation: trajectory memory retrieval network for video object segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7472" to="7481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TDN: temporal difference networks for efficient action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1895" to="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking the correlation in few-shot segmentation: a buoys view</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7183" to="7192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive agent transformer for few-shot segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19818-2_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19818-23" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MitoEM dataset: large-scale 3D mitochondria instance segmentation from EM images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59722-17" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page" from="66" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Uncertainty-aware label rectification for domain adaptive mitochondria segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-418" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial-prediction guided multi-task adaptation for semantic segmentation of electron microscopy images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1205" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
