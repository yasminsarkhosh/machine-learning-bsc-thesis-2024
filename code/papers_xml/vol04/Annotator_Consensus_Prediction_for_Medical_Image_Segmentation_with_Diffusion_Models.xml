<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models</title>
				<funder ref="#_s5qxpru">
					<orgName type="full">ISRAEL SCIENCE FOUNDATION</orgName>
				</funder>
				<funder>
					<orgName type="full">Tel Aviv University Center for AI and Data Science</orgName>
					<orgName type="abbreviated">TAD</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tomer</forename><surname>Amit</surname></persName>
							<email>tomeramit1@mail.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shmuel</forename><surname>Shichrur</surname></persName>
							<email>shmuels1@mail.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tal</forename><surname>Shaharabany</surname></persName>
							<email>shaharabany@mail.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lior</forename><surname>Wolf</surname></persName>
							<email>wolf@mail.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
								<address>
									<settlement>Tel Aviv</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="544" to="554"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">48C09447CF07FB9D820D084A401E2D39</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_52</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi annotator</term>
					<term>Image segmentation</term>
					<term>Diffusion Model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A major challenge in the segmentation of medical images is the large inter-and intra-observer variability in annotations provided by multiple experts. To address this challenge, we propose a novel method for multi-expert prediction using diffusion models. Our method leverages the diffusion-based approach to incorporate information from multiple annotations and fuse it into a unified segmentation map that reflects the consensus of multiple experts. We evaluate the performance of our method on several datasets of medical segmentation annotated by multiple experts and compare it with the state-of-the-art methods. Our results demonstrate the effectiveness and robustness of the proposed method. Our code is publicly available at https://github.com/ tomeramit/Annotator-Consensus-Prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical image segmentation is a challenging task that requires accurate delineation of structures and regions of interest in complex and noisy images. Multiple expert annotators are often employed to address this challenge, to provide binary segmentation annotations for the same image. However, due to differences in experience, expertise, and subjective judgments, annotations can vary significantly, leading to inter-and intra-observer variability. In addition, manual annotation is a time-consuming and costly process, which limits the scalability and applicability of segmentation methods.</p><p>To overcome these limitations, automated methods for multi-annotator prediction have been proposed, which aim to fuse the annotations from multiple annotators and generate an accurate and consistent segmentation result. Existing approaches for multi-annotator prediction include majority voting <ref type="bibr" target="#b6">[7]</ref>, label fusion <ref type="bibr" target="#b2">[3]</ref>, and label sampling <ref type="bibr" target="#b11">[12]</ref>.</p><p>In recent years, diffusion models have emerged as a promising approach for image segmentation, for example by using learned semantic features <ref type="bibr" target="#b1">[2]</ref>. By modeling the diffusion of image intensity values over the iterations, diffusion models capture the underlying structure and texture of the images and can separate regions of interest from the background. Moreover, diffusion models can handle noise and image artifacts, and adapt to different image modalities.</p><p>In this work, we propose a novel method for multi-annotator prediction, using diffusion models for medical segmentation. The goal is to fuse multiple annotations of the same image from different annotators and obtain a more accurate and reliable segmentation result. In practice, we leverage the diffusionbased approach to create one map for each level of consensus. To obtain the final prediction, we average the obtained maps and obtain one soft map.</p><p>We evaluate the performance of the proposed method on a dataset of medical images annotated by multiple annotators. Our results demonstrate the effectiveness and robustness of the proposed method in handling inter-and intra-observer variability and achieving higher segmentation accuracy than the state-of-the-art methods. The proposed method could improve the efficiency and quality of medical image segmentation and facilitate the clinical decision-making process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-annotator Strategies. Research attention has recently been directed towards the issues of multi-annotator labels <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>. During training, Jensen et al. <ref type="bibr" target="#b11">[12]</ref> randomly sampled different labels per image. This method produced a more calibrated model. Guan et al. <ref type="bibr" target="#b6">[7]</ref> predicted the gradings of each annotator individually and acquired the corresponding weights for the final prediction. Kohl et al. <ref type="bibr" target="#b14">[15]</ref> used the same sampling strategy to train a probabilistic model, based on a U-Net combined with a conditional variational autoencoder. Another recent probabilistic approach <ref type="bibr" target="#b19">[20]</ref> combines a diffusion model with KL divergence to capture the variability between the different annotators. In our work, we use consensus maps as the ground truth and compare them to other strategies.</p><p>Diffusion Probabilistic Models (DPM). <ref type="bibr" target="#b22">[23]</ref> are a class of generative models based on a Markov chain, which can transform a simple distribution (e.g. Gaussian) to data sampled from a complex distribution. Diffusion models are capable of generating high-quality images that can compete with and even outperform the latest GAN methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>. A variational framework for the likelihood estimation of diffusion models was introduced by Huang et al. <ref type="bibr" target="#b10">[11]</ref>. Subsequently, Kingma et al. <ref type="bibr" target="#b13">[14]</ref> proposed a Variational Diffusion Model that produces state-of-the-art results in likelihood estimation for image density.</p><p>Conditional Diffusion Probabilistic Models. In our work, we use diffusion models to solve the image segmentation problem as conditional generation, given the image. Conditional generation with diffusion models includes methods for class-conditioned generation, which is obtained by adding a class embedding to the timestep embedding <ref type="bibr" target="#b18">[19]</ref>. In <ref type="bibr" target="#b3">[4]</ref>, a method for guiding the generative process in DDPM is present. This method allows the generation of images based on a given reference image without any additional learning. In the domain of super-resolution, the lower-resolution image is upsampled and then concatenated, channelwise, to the generated image at each iteration <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21]</ref>. A similar approach passes the low-resolution images through a convolutional block <ref type="bibr" target="#b15">[16]</ref> prior to the concatenation.</p><p>A previous study directly applied a diffusion model to generate a segmentation mask based on a conditioned input image <ref type="bibr" target="#b0">[1]</ref>. Baranchuk et al. <ref type="bibr" target="#b1">[2]</ref> extract features from a pretrained diffusion model for training a segmentation network, while our diffusion model generates the output mask. Compared to the diffusionbased image segmentation method of Wolleb et al. <ref type="bibr" target="#b25">[26]</ref>, our architecture differs in two main aspects: (i) the concatenation method of the condition signal, and (ii) an encoder that processes the conditioning signal. We also use a lower value of T, which reduces the running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our approach for binary segmentation with multi-annotators employs a diffusion model that is conditioned on the input image I ∈ R W ×H , the step estimation t, and the consensus index c. The diffusion model updates its current estimate x t iteratively, using the step estimation function θ . See Fig. <ref type="figure" target="#fig_0">1</ref> for an illustration.</p><p>Given a set of C annotations {A i k } C i=1 associated with input sample I k , we define the ground truth consensus map at level c to be</p><formula xml:id="formula_0">M c k [x, y] = 1 C i=1 A i k [x, y] ≥ c, 0 otherwise,<label>(1)</label></formula><p>During training, our algorithm iteratively samples a random level of the consensus c ∼ U [1, 2, ..., C] and an input-output pair (I k , M c k ). The iteration number 1 ≤ t ≤ T is sampled from a uniform distribution and X T is sampled from a normal distribution.</p><p>We then compute x t from X T , M c k and t according to:</p><formula xml:id="formula_1">x t = √ ᾱt M c k + (1 -ᾱt )X T , X T ∼ N (0, I n×n ). (<label>2</label></formula><formula xml:id="formula_2">) Algorithm 1. Training Algorithm Input T , D = {(I k , M 1 k , ..., M C k )} K k repeat sample c ∼ {1, ..., C} sample (I k , M c k ) ∼ D sample ∼ N (0, In×n) sample t ∼ ({1,...,T}) zc = LU Tc(c) zt = LU Tt(t) βt = 10 -4 (T -t)+2 * 10 -2 (t-1) T -1 αt = 1 -βt ᾱt = t s=0 αs xt = √ ᾱtM c k + √ 1 -ᾱt ∇ θ || -θ (xt, I k , zt, zc)|| until convergence Algorithm 2. Inference Algorithm Input T , I for c = 1, ..., C do sample xT c ∼ N (0, In×n) for t = T, T -1, ..., 1 do sample z ∼ N (0, In×n) zc = LU Tc(c), zt = LU Tt(t) βt = 10 -4 (T -t)+2 * 10 -2 (t-1) T -1 αt = 1 -βt. ᾱt = t s=0 αs βt = 1-ᾱt-1 1-ᾱt βt t = 1-α t √ 1-ᾱt θ (xt, I, zt, zc) xt-1 c = αt -1 2 (xt -t ) xt-1 c = xt-1 c + 1 [t&gt;1] β 1 2 t z return ( C i=1 x0 i )/C</formula><p>where ᾱ is a constant that defines the schedule of added noise. The current step index t, and the consensus index c are integers that are translated to z t ∈ R d and z c ∈ R d , respectively with a pair of lookup tables. The embeddings are passed to the different networks F , D and E.</p><p>In the next step, our algorithm encodes the input signal x t with network F and encodes the condition image I k with network G. We compute the conditioned signal u t = F (x t , z c , z t ) + G(I k ), and apply it to the networks E and D, where the output is the estimation of x t-1 .</p><formula xml:id="formula_3">θ (x t , I k , z t , z c ) = D(E(F (x t , z t , z c ) + G(I k ), z t , z c ), z t , z c ) . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>The loss function being minimized is:</p><formula xml:id="formula_5">E x0, ,xe,t,c [|| -θ (x t , I k , z t , z c )|| 2 ].<label>(4)</label></formula><p>The training procedure is depicted in Algorithm 1. The total number of diffusion steps T is set by the user, and C is the number of different annotators in the dataset. Our model is trained using binary consensus maps (M c k ) as the ground truth, where k is the sample id, and c is the consensus index.</p><p>The inference process is described in Algorithm 2. We sample our model for each consensus index, and then calculate the mean of all results to obtain our target, which is a soft-label map representing the annotator agreement. Mathematically, if the consensus maps are perfect, this is equivalent to assigning each image location with the fraction of annotations that consider this location to be part of the mask (if c annotators mark a pixel, it would appear in levels 1..c). In Sect. 4, we compare our method with other variants and show that estimating the fraction map directly, using an identical diffusion model, is far inferior to estimating each consensus level separately and then averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Employing Multiple Generations. Since calculating x t-1 during inference includes the addition of 1 [t&gt;1]</head><p>β 1 is significant variability between different runs of the inference method on the same inputs, see Fig. <ref type="figure" target="#fig_1">2(b)</ref>.</p><p>In order to exploit this phenomenon, we run the inference algorithm multiple times, then average the results. This way, we stabilize the results of segmentation and improve performance, as demonstrated in Fig. <ref type="figure" target="#fig_1">2(c</ref>). We use twenty-five generated instances in all experiments. In the ablation study, we quantify the gain of this averaging procedure.</p><p>Architecture. In this architecture, the U-Net's decoder D is conventional and its encoder is broken down into three networks: E, F , and G. The last encodes the input image, while F encodes the segmentation map of the current step x t . The two processed inputs have the same spatial dimensionality and number of channels. Based on the success of residual connections <ref type="bibr" target="#b7">[8]</ref>, we sum these signals F (x t , z t , z c ) + G(I). This sum then passes to the rest of the U-Net encoder E.</p><p>The input image encoder G is built from Residual in Residual Dense Blocks <ref type="bibr" target="#b23">[24]</ref> (RRDBs), which combine multi-level residual connections without batch normalization layers. G has an input 2D-convolutional layer, an RRDB with a residual connection around it, followed by another 2D-convolutional layer, leaky RELU activation and a final 2D-convolutional output layer. F is a 2Dconvolutional layer with a single-channel input and an output of L channels.</p><p>The encoder-decoder part of θ , i.e., D and E, is based on U-Net, similarly to <ref type="bibr" target="#b18">[19]</ref>. Each level is composed of residual blocks, and at resolution 16 × 16 and 8 × 8 each residual block is followed by an attention layer. The bottleneck contains two residual blocks with an attention layer in between. Each attention layer contains multiple attention heads.</p><p>The residual block is composed of two convolutional blocks, where each convolutional block contains group-norm, SiLU activation, and a 2D-convolutional layer. The residual block receives the time embedding through a linear layer, SiLU activation, and another linear layer. The result is then added to the output of the first 2D-convolutional block. Additionally, the residual block has a residual connection that passes all its content.</p><p>On the encoder side (network E), there is a downsample block after the residual blocks of the same depth, which is a 2D-convolutional layer with a stride of two. On the decoder side (network D), there is an upsample block after the residual blocks of the same depth, which is composed of the nearest interpolation that doubles the spatial size, followed by a 2D-convolutional layer. Each layer in the encoder has a skip connection to the decoder side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted a series of experiments to evaluate the performance of our proposed method for multi-annotator prediction. Our experiments were carried out on datasets of the QUBIQ benchmark<ref type="foot" target="#foot_1">1</ref> . We compared the performance of our proposed method with several state-of-the-art methods. Datasets. The Quantification of Uncertainties in Biomedical Image Quantification Challenge (QUBIQ), is a recently available challenge dataset specifically for the evaluation of inter-rater variability. QUBIQ comprises four different segmentation datasets with CT and MRI modalities, including brain growth (one task, MRI, seven raters, 34 cases for training and 5 cases for testing), brain tumor (one task, MRI, three raters, 28 cases for training and 4 cases for testing), prostate (two subtasks, MRI, six raters, 33 cases for training and 15 cases for testing), and kidney (one task, CT, three raters, 20 cases for training and 4 cases for testing). Following <ref type="bibr" target="#b12">[13]</ref>, the evaluation is performed using the soft Dice coefficient with five threshold levels, set as (0.1, 0.3, 0.5, 0.7, 0.9). Implementation Details. The number of diffusion steps in previous works was 1000 <ref type="bibr" target="#b8">[9]</ref> and even 4000 <ref type="bibr" target="#b18">[19]</ref>. The literature suggests that more is better <ref type="bibr" target="#b21">[22]</ref>. In our experiments, we employ 100 diffusion steps, to reduce inference time.</p><p>The AdamW <ref type="bibr" target="#b17">[18]</ref> optimizer is used in all our experiments. Based on the intuition that the more RRDB blocks, the better the results, we used as many blocks as we could fit on the GPU without overly reducing batch size.</p><p>Following <ref type="bibr" target="#b12">[13]</ref>, for all datasets of the QUBIQ benchmark the input image resolution, as well as the test image resolution, was 256 × 256. The experiments were performed with a batch size of four images and eight RRDB blocks. The network depth was seven, and the number of channels in each depth was [L, L, L, 2L, 2L, 4L, 4L], with L = 128. The augmentations used were: random  scaling by a factor sampled uniformly in the range [0.9, 1.1], a rotation between 0 and 15 • , translation between [0, 0.1] in both axes, and horizontal and vertical flips, each applied with a probability of 0.5.</p><p>Results. We compare our method with FCN <ref type="bibr" target="#b16">[17]</ref>, MCD <ref type="bibr" target="#b5">[6]</ref>, FPM <ref type="bibr" target="#b26">[27]</ref>, DAF <ref type="bibr" target="#b24">[25]</ref>, MV-UNet <ref type="bibr" target="#b12">[13]</ref>, LS-UNet <ref type="bibr" target="#b11">[12]</ref>, MH-UNet <ref type="bibr" target="#b6">[7]</ref>, and MRNet <ref type="bibr" target="#b12">[13]</ref>.</p><p>We also compare with models that we train ourselves, using public code AMIS <ref type="bibr" target="#b19">[20]</ref>, and DMISE <ref type="bibr" target="#b25">[26]</ref>. The first is trained in a scenario where each annotator is a different sample ("No annotator" variant of our ablation results below), and the second is trained on the consensus setting, similar to our method. As can be seen in Table <ref type="table" target="#tab_0">1</ref>, our method outperforms all other methods across all datasets of QUBIQ benchmark.</p><p>Ablation Study. We evaluate alternative training variants as an ablation study in Table <ref type="table" target="#tab_1">2</ref>. The "Annotator" variant, in which our model learns to produce each annotator binary segmentation map and then averages all the results to obtain the required soft-label map, achieves lower scores compared to the "Consensus" variant, which is our full method. The "No annotator" variant, where images were paired with random annotators without utilizing the annotator IDs, achieves a slightly lower average score compared to the "Annotator" variant. We also note that our "No annotator" variant outperforms the analog AMIS model in four out of five datasets, indicating that our architecture is somewhat preferable. In a third variant, our model learns to predict the soft-label map that denotes the fraction of annotators that mark each image location directly. Since this results in fewer generated images, we generate C times as many images per test sample. The score of this variant is also much lower than that of our method.</p><p>Next, we study the effect of the number of generated images on performance. The results can be seen in Fig. <ref type="figure" target="#fig_2">3</ref>. In general, increasing the number of generated instances tends to improve performance. However, the number of runs required to reach optimal performance varies between classes. For example, for the Brain and the Prostate 1 datasets, optimal performance is achieved using 5 generated images, while on Prostate 2 the optimal performance is achieved using 25 gen-  erated images. Figure <ref type="figure" target="#fig_3">4</ref> depicts samples from multiple datasets and presents the progression as the number of generated images increases. As can be seen, as the number of generated images increases, the outcome becomes more and more similar to the target segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In order to investigate the relationship between the annotator agreement and the performance of our model, we conducted an analysis by calculating the average Dice score between each pair of annotators across the entire dataset. The results of this pairwise Dice analysis can be found in Table <ref type="table" target="#tab_2">3</ref>, where higher mean-scores indicate a greater consensus among the annotators. We observed that our proposed method demonstrated improved performance on datasets with higher agreement among annotators, specifically the kidney and prostate 1 datasets. Conversely, the performance of the other methods significantly deteriorated on the kidney dataset, leading to a lower correlation between the Dice score and the overall performance. Additionally, we examined the relationship between the number of annotators and the performance of our model. Surprisingly, we found no significant correlation between the number of annotators and the performance of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Shifting the level of consensus required to mark a region from very high to as low as one annotator, can be seen as creating a dynamic shift from a very conservative segmentation mask to a very liberal one. As it turns out, this dynamic is wellcaptured by diffusion models, which can be readily conditioned on the level of consensus. Another interesting observation that we make is that the mean (over the consensus level) of the obtained consensus masks is an effective soft mask. Applying these two elements together, we obtain state-of-the-art results on multiple binary segmentation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The figure below illustrates our proposed method for multi-annotator segmentation. The input I k image with the noisy segmentation map xt is passed through our network iteratively T times in order to obtain an output segmentation map x0. Each network receives the consensus level c as an embedding zc as well as the time step data.</figDesc><graphic coords="3,56,79,59,78,310,57,66,82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Multiple segmentation results on all datasets of the QUBIQ benchmark. (a) dataset, (b) input image, (c) a subset of the obtained consensus maps for multiple runs with different consensus index on the same input, (d) average result, visualized by the 'bwr' color scale between 0 (blue) and 1 (red), and (e) ground truth. (Color figure online)</figDesc><graphic coords="6,128,40,53,96,209,44,174,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Soft Dice vs. #generated images.</figDesc><graphic coords="7,40,14,53,90,157,57,116,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Multiple segmentation results per number of generated images. (a) dataset, (b) input image, (c) results for 1, 5, 10, 25 generated images, and (d) ground truth.</figDesc><graphic coords="8,126,84,168,95,209,41,174,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>QUBIQ soft Dice results.</figDesc><table><row><cell>Method</cell><cell cols="5">Kidney Brain Tumor Prost1 Prost2</cell></row><row><cell>FCN</cell><cell cols="5">70.03 80.99 83.12 84.55 67.81</cell></row><row><cell>MCD</cell><cell cols="5">72.93 82.91 86.17 86.40 70.95</cell></row><row><cell>FPM</cell><cell cols="2">72.17 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DAF</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">85.98 72.87</cell></row><row><cell cols="6">MV-UNet 70.65 81.77 84.03 85.18 68.39</cell></row><row><cell cols="6">LS-UNet 72.31 82.79 85.85 86.23 69.05</cell></row><row><cell cols="6">MH-UNet 73.44 83.54 86.74 87.03 75.61</cell></row><row><cell>MRNet</cell><cell cols="5">74.97 84.31 88.40 87.27 76.01</cell></row><row><cell>AMIS</cell><cell cols="5">68.53 74.09 92.95 91.64 21.91</cell></row><row><cell>DMISE</cell><cell cols="5">74.50 92.80 87.80 94.70 80.20</cell></row><row><cell>Ours</cell><cell cols="5">96.58 93.81 93.16 95.21 84.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study showing soft Dice results for various alternative methods of training similar diffusion models.</figDesc><table><row><cell>Method</cell><cell cols="3">Kidney Brain Tumor Prostate 1 Prostate 2</cell></row><row><cell>Annotator</cell><cell>96.13</cell><cell>89.88 92.51 93.89</cell><cell>76.89</cell></row><row><cell>No annotator</cell><cell>94.46</cell><cell>89.78 91.78 92.58</cell><cell>78.61</cell></row><row><cell>Soft-label</cell><cell>65.41</cell><cell>79.56 75.60 73.23</cell><cell>65.24</cell></row><row><cell cols="3">Consensus (our method) 96.58 93.81 93.16 95.21</cell><cell>84.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Pairwise Dice scores per dataset.</figDesc><table><row><cell>Dataset</cell><cell>Mean score between pairs</cell></row><row><cell>Kidney</cell><cell>94.95</cell></row><row><cell>Brain</cell><cell>85.74</cell></row><row><cell>Tumor</cell><cell>90.65</cell></row><row><cell cols="2">Prostate 1 94.64</cell></row><row><cell cols="2">Prostate 2 89.91</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>t z where z is from a standard distribution, there</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Quantification of Uncertainties in Biomedical Image Quantification Challenge in MICCAI20'-link.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This project has received funding from the <rs type="funder">ISRAEL SCIENCE FOUNDATION</rs> (grant No. <rs type="grantNumber">2923/20</rs>) within the <rs type="programName">Israel Precision Medicine Partnership program</rs>. It was also supported by a grant from the <rs type="funder">Tel Aviv University Center for AI and Data Science (TAD)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_s5qxpru">
					<idno type="grant-number">2923/20</idno>
					<orgName type="program" subtype="full">Israel Precision Medicine Partnership program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">SegDiff: image segmentation with diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaharbany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00390</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Label-efficient semantic segmentation with diffusion models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Baranchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rubachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.03126</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic pathological lung segmentation in low-dose CT image using eigenspace sparse shape composition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1736" to="1749" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02938</idno>
		<title level="m">ILVR: conditioning method for denoising diffusion probabilistic models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Who said what: modeling individual labelers improves classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cascaded diffusion models for high fidelity image generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">47</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A variational perspective on diffusionbased generative models and score matching</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving uncertainty estimation in convolutional neural networks using inter-rater agreement</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Jørgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jalaboi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_59</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32251-9_59" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019, Part IV</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="540" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning calibrated medical image segmentation via multi-rater agreement modeling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12341" to="12351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00630</idno>
		<title level="m">Variational diffusion models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A probabilistic U-Net for segmentation of ambiguous images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SRDIFF: single image super-resolution with diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">479</biblScope>
			<biblScope unit="page" from="47" to="59" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8162" to="8171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ambiguous medical image segmentation using diffusion models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="11536" to="11546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07636</idno>
		<title level="m">Image superresolution via iterative refinement</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Noise estimation for generative diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>San-Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02600</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ESRGAN: enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep attentional features for prostate segmentation in ultrasound</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_60</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-3_60" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018, Part IV</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wolleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valmaggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
		<title level="m">Diffusion models for implicit image segmentation ensembles</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A fixedpoint model for pancreas segmentation in abdominal CT scans</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Fishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66182-7_79</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66182-7_79" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017, Part I</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Franz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Duchesne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10433</biblScope>
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
