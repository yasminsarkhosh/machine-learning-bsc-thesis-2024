<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI</title>
				<funder ref="#_YTh7s2J">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_nzSDaTF">
					<orgName type="full">Science and Technology Development Fund, Macau SAR</orgName>
				</funder>
				<funder ref="#_ZhcY6FE #_j45n5mA #_ZgyGwrk #_AqPRA4c">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_8C72JRJ">
					<orgName type="full">Zhejiang Provincial Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_8aNxepr #_hbcJaQs">
					<orgName type="full">Wuxi Health Commission Precision Medicine Project</orgName>
				</funder>
				<funder ref="#_DN5cuHc #_Z4CgBHX">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_utGWy2h">
					<orgName type="full">Postgraduate Research &amp; Practice Innovation Program of Jiangsu Province</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianxu</forename><surname>Lv</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Computer Science</orgName>
								<orgName type="institution">Jiangnan University</orgName>
								<address>
									<postCode>214122</postCode>
									<settlement>Wuxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Computer Science</orgName>
								<orgName type="institution">Jiangnan University</orgName>
								<address>
									<postCode>214122</postCode>
									<settlement>Wuxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Miao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Cancer Center</orgName>
								<orgName type="department" key="dep2">Faculty of Health Sciences</orgName>
								<orgName type="institution">University of Macau</orgName>
								<address>
									<settlement>Macau SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lihua</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Biomedical Engineering and Instrumentation</orgName>
								<orgName type="institution">Hangzhou Dianzi University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiang</forename><surname>Pan</surname></persName>
							<email>xiangpan@jiangnan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence and Computer Science</orgName>
								<orgName type="institution">Jiangnan University</orgName>
								<address>
									<postCode>214122</postCode>
									<settlement>Wuxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Cancer Center</orgName>
								<orgName type="department" key="dep2">Faculty of Health Sciences</orgName>
								<orgName type="institution">University of Macau</orgName>
								<address>
									<settlement>Macau SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="100" to="109"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">9DA5E345B364FF31330CB07D3E6F9995</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_10</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>Kinetic representation</term>
					<term>DCE-MRI</term>
					<term>Cancer segmentation</term>
					<term>Denoising Diffusion model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent researches on cancer segmentation in dynamic contrast enhanced magnetic resonance imaging (DCE-MRI) usually resort to the combination of temporal kinetic characteristics and deep learning to improve segmentation performance. However, the difficulty in accessing complete temporal sequences, especially post-contrast images, hinders segmentation performance, generalization ability and clinical application of existing methods. In this work, we propose a diffusion kinetic model (DKM) that implicitly exploits hemodynamic priors in DCE-MRI and effectively generates high-quality segmentation maps only requiring precontrast images. We specifically consider the underlying relation between hemodynamic response function (HRF) and denoising diffusion process (DDP), which displays remarkable results for realistic image generation. Our proposed DKM consists of a diffusion module (DM) and segmentation module (SM) so that DKM is able to learn cancer hemodynamic information and provide a latent kinetic code to facilitate segmentation performance. Once the DM is pretrained, the latent code estimated from the DM is simply incorporated into the SM, which enables DKM to automatically and accurately annotate cancers with pre-contrast images. To our best knowledge, this is the first work exploring the relationship between HRF and DDP for dynamic MRI segmentation. We evaluate the proposed method for tumor segmentation on public breast cancer DCE-MRI dataset. Compared to the existing state-of-the-art approaches with complete sequences, our method yields higher segmentation performance even with pre-contrast images. The source code will be available on https://github.com/Medical-AI-Lab-of-JNU/DKM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) revealing tumor hemodynamics information is often applied to early diagnosis and treatment of breast cancer <ref type="bibr" target="#b0">[1]</ref>. In particular, automatically and accurately segmenting tumor regions in DCE-MRI is vital for computer-aided diagnosis (CAD) and various clinical tasks such as surgical planning. For the sake of promoting segmentation performance, recent methods utilize the dynamic MR sequence and exploit its temporal correlations to acquire powerful representations <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. More recently, a handful of approaches take advantage of hemodynamic knowledge and time intensity curve (TIC) to improve segmentation accuracy <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. However, the aforementioned methods require the complete DCE-MRI sequences and overlook the difficulty in assessing complete temporal sequences and the missing time point problem, especially post-contrast phase, due to the privacy protection and patient conditions. Hence, these breast cancer segmentation models cannot be deployed directly in clinical practice.</p><p>Recently, denoising diffusion probabilistic model (DDPM) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> has produced a tremendous impact on image generation field due to its impressive performance. Diffusion model is composed of a forward diffusion process that add noise to images, along with a reverse generation process that generates realistic images from the noisy input <ref type="bibr" target="#b7">[8]</ref>. Based on this, several methods investigate the potential of DDPM for natural image segmentation <ref type="bibr" target="#b8">[9]</ref> and medical image segmentation <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. Specifically, Baranchuk et al. <ref type="bibr" target="#b8">[9]</ref> explores the intermediate activations from the networks that perform the markov step of the reverse diffusion process and find these activations can capture semantic information for segmentation. However, the applicability of DDPM to medical image segmentation are still limited. In addition, existing DDPM-based segmentation networks are generic and are not optimized for specific applications. In particular, a core question for DCE-MRI segmentation is how to optimally exploit hemodynamic priors.</p><p>Based on the above observations, we innovatively consider the underlying relation between hemodynamic response function (HRF) and denoising diffusion process (DDP). As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, during HRF process, only tumor lesions are enhanced and other non-tumor regions remain unchanged. By designing a network architecture to effectively transmute pre-contrast images into post-contrast images, the network should acquire hemodynamic inherent in HRF that can be used to improve segmentation performance. Inspired by the fact that DDPM generates images from noisy input provided by the parameterized Gaussian process, this work aims to exploit implicit hemodynamic information by a diffusion process that predict post-contrast images from noisy pre-contrast images. Specifically, given the pre-contrast and post-contrast images, the latent kinetic code is learned using a score function of DDPM, which contains sufficient hemodynamic characteristics to facilitate segmentation performance.</p><p>Once the diffusion module is pretrained, the latent kinetic code can be easily generated with only pre-contrast images, which is fed into a segmentation module to annotate cancers. To verify the effectiveness of the latent kinetic code, the SM adopts a simple U-Net-like structure, with an encoder to simultaneously conduct semantic feature encoding and kinetic code fusion, along with a decoder to obtain voxel-level classification. In this manner, our latent kinetic code can be interpreted to provide TIC information and hemodynamic characteristics for accurate cancer segmentation.</p><p>We verify the effectiveness of our proposed diffusion kinetic model (DKM) on DCE-MRI-based breast cancer segmentation using Breast-MRI-NACT-Pilot dataset <ref type="bibr" target="#b12">[13]</ref>. Compared to the existing state-of-the-art approaches with complete sequences, our method yields higher segmentation performance even with precontrast images. In summary, the main contributions of this work are listed as follows:</p><p>• We propose a diffusion kinetic model that implicitly exploits hemodynamic priors in DCE-MRI and effectively generates high-quality segmentation maps only requiring pre-contrast images. • We first consider the underlying relation between hemodynamic response function and denoising diffusion process and provide a DDPM-based solution to capture a latent kinetic code for hemodynamic knowledge. • Compared to the existing approaches with complete sequences, the proposed method yields higher cancer segmentation performance even with pre-contrast images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The overall framework of the proposed diffusion kinetic model is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. It can be observed that the devised model consists of a diffusion module (DM) and a segmentation module (SM). Let {x K , K = 0, 1, ..., k} be a sequence of images representing the DCE-MRI protocol, in which x 0 represents the precontrast image and x k represents the late post-contrast image. The DM takes a noisy pre-contrast image x t as input and generates post-contrast images to estimate the latent kinetic code. Once the DM is trained, the learned kinetic code is incorporated into the SM as hemodynamic priors to guide the segmentation process. Model details are shown as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Diffusion Module</head><p>The diffusion module is following the denoising diffusion probabilistic model <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>. Based on the consideration from nonequilibrium thermodynamics, DDPM approximates the data distribution by learning a Markov chain process which originates from the Gaussian distribution. The forward diffusion process gradually adds Gaussian noise to the data x 0 according to a variance schedule β 1 , ..., β T <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_0">q(x t |x t-1 ) := N (x t ; 1 -β t x t-1 , β t I)<label>(1)</label></formula><p>Particularly, a noisy image x t can be directly obtained from the data x 0 :</p><formula xml:id="formula_1">q(x t |x 0 ) := N (x t ; √ ᾱt x 0 , (1 -ᾱt )I)<label>(2)</label></formula><p>where α t := 1β t and ᾱt := t s=1 α s . Afterwards, DDPM approximates the reverse diffusion process by the following parameterized Gaussian transitions:</p><formula xml:id="formula_2">p θ (x t-1 |x t ) := N (x t-1 ; μ θ (x t , t), θ (x t , t)) (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where μ θ (x t , t) is the learned posterior mean and θ (x t ; t) is a fixed set of scalar covariances. In particular, we employ a noise predictor network ( θ (x t , t)) to predict the noise component at the step t (As shown in Fig. <ref type="figure" target="#fig_1">2(a)</ref>).</p><p>Inspired by the property of DDPM <ref type="bibr" target="#b7">[8]</ref>, we devise the diffusion module by considering the pre-contrast images x 0 as source and regarding the post-contrast images x k as target. Formally, a noisy sample can be acquired by:</p><formula xml:id="formula_4">x t = √ ᾱt x 0 + √ 1 -ᾱt , ∼ N (0, I)<label>(4)</label></formula><p>where α t := 1-β t and ᾱt := t s=1 α s . Next, we employ the reverse diffusion process to transform the noisy sample x t to the post-contrast data x k . As thus, the DM gradually exploits the latent kinetic code by comparing the pre-contrast and post-contrast images, which contains hemodynamic knowledge for segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Segmentation Module</head><p>Once pretrained, the DM outputs multi-scale latent kinetic code f dm from intermediate layers, which is fed into the SM to guide cancer segmentation. As shown in Fig. <ref type="figure" target="#fig_1">2(b)</ref>, the SM consists of four kinetic blocks and four up blocks. Each kinetic block is composed of a fusion layer, two convolutional layers, two batch normalization layers, two ReLU activation functions, a max pooling layer and a residual addition. Specifically, to obtain sufficient expressive power to transform the learned kinetic code into higher-level features, at least one learnable linear transformation is required. To this end, a linear transformation, parametrized by a weight matrix W , is applied to the latent code f dm , followed by a batch normalization, ReLU activation layer and concatenation, which can be represented as follows:</p><formula xml:id="formula_5">f = C(φ(BN(W * f dm ); f sm )<label>(5)</label></formula><p>where * represents 1 × 1 based convolution operation, W is the weight matrix, BN represents batch normalization, φ represents ReLU activation function and C is concatenation operation. In this way, the hemodynamic knowledge can be incorporated into the SM to capture more expressive representations to improve segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Training</head><p>To maintain training stability, the proposed DKM adopts a two-step training procedure for cancer annotation. In the first step, the DM is trained to transform pre-contrast images into post-contrast images for a latent space where hemodynamic priors are exploited. In particular, the diffusion loss for the reverse diffusion process can be formulated as follows:</p><formula xml:id="formula_6">L DM = E t, ,x || θ (x t , t; x 0 , x k ) -|| 2<label>(6)</label></formula><p>where θ represents the denoising model that employs an U-Net structure, x 0 and x k are the pre-contrast and post-contrast images, respectively, is Gaussian distribution data ∼ N (0, I), and t is a timestep. For a second step, we train the SM that integrates the previously learned latent kinetic code to provide tumor hemodynamic information for voxel-level prediction. Considering the varying sizes, shapes and appearances of tumors that results from intratumor heterogeneity and results in difficulties of accurate cancer annotation, we design the segmentation loss as follows:</p><formula xml:id="formula_7">L SM = L seg + λL SSIM = L CE (S, G) + L Dice (S, G) + λ(1 - (2μ S μ G + C 1 )(2ϕ SG + C 2 ) (μ 2 S + μ 2 G + C 1 )(ϕ 2 S + ϕ 2 G + C 2 ) )<label>(7)</label></formula><p>where L SSIM is used to evaluate tumor structural characteristics, S and G represents segmentation map and ground truth, respectively; μ S is the mean of S and μ G is the mean of G; ϕ S represents the variance of S and ϕ G represents the variance of G; C 1 and C 2 denote the constant to hold training stable <ref type="bibr" target="#b14">[15]</ref>, and ϕ SG is the covariance between S and G. The λ is set as 0.5 empirically. Following <ref type="bibr" target="#b15">[16]</ref>,</p><formula xml:id="formula_8">C 1 = (k 1 L) 2 and C 2 = (k 2 L) 2 ,</formula><p>where k 1 is set as 0.01, k 2 is set as 0.03 and L is set as the range of voxel values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Dataset: To demonstrate the effectiveness of our proposed DKM, we evaluate our method on 4D DCE-MRI breast cancer segmentation using the Breast-MRI-NACT-Pilot dataset <ref type="bibr" target="#b12">[13]</ref>, which contains a total of 64 patients with the contrastenhanced MRI protocol: a pre-contrast scan, followed by 2 consecutive postcontrast time points (As shown in Fig. <ref type="figure" target="#fig_2">3</ref>). Each MR volume consists of 60 slices and the size of each slice is 256 × 256. Regarding preprocessing, we conduct zeromean unit-variance intensity normalization for the whole volume. We divided the original dataset into training (70%) and test set (30%) based on the scans. Ground truth segmentations of the data are provided in the dataset for tumor annotation. No data augmentation techniques are used to ensure fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing Methods and Evaluation Metrics:</head><p>To comprehensively evaluate the proposed method, We compare it with 3D segmentation methods, including Dual Attention Net (DANet) <ref type="bibr" target="#b16">[17]</ref>, MultiResUNet <ref type="bibr" target="#b17">[18]</ref> and multi-task learning network (MTLN) <ref type="bibr" target="#b18">[19]</ref>, and 4D segmentation methods, including LNet <ref type="bibr" target="#b19">[20]</ref>, 3D patch U-Net <ref type="bibr" target="#b20">[21]</ref>, and HybridNet <ref type="bibr" target="#b4">[5]</ref>. All approaches are evaluated using 1) Dice Similarity Coefficient (DSC) and 2) Jaccard Index (JI).</p><p>Implementation Details: We implement our proposed framework with PyTorch using two NVIDIA RTX 2080Ti GPUs to accelerate model training.</p><p>Following DDPM <ref type="bibr" target="#b7">[8]</ref>, we set 128, 256, 256, 256 channels for each stage in the DM and set the noise level from 10 -4 to 10 -2 using a linear schedule with T = 1000.</p><p>Once the DM is trained, we extract intermediate feature maps from four resolutions for further segmentation task. Similar to DM, the SM also consists of four resolution blocks. However, unlike channel settings of DM, we set 128, 256,  512, 1024 channels for each stage in the SM to capture expressive and sufficient semantic information. The SM is optimized by Adam with a learning rate 2 × 10 -5 and a weight decay 10 -6 . The model is trained for 500 epochs with the batch size to 1. No data augmentation techniques are used to ensure fairness.</p><p>Comparison with SOTA Methods: The quantitative comparison of the proposed method to recent state-of-the-art methdos is reported in Table <ref type="table" target="#tab_0">1</ref>. Experimental results demonstrate that the proposed method comprehensively other models with less scans (i.e., pre-contrast) in testing. We attribute it to the ability of diffusion module to exploit hemodynamic priors to guide the segmentation task. Specifically, in comparison with 3D segmentation models (e.g. MTLN), our method yields higher segmentation scores. The possible reason is that our method is able to exploit the time intensity curve, which contains richer information compared to post-contrast scan. Besides, we can observe that our method achieves improvements when compared to 4D segmentation models using complete sequence. Our method outperform the HybridNet by 7.1% and 7.0% in DSC and JI, respectively. It probably due to two aspects: 1) The hemodynamic knowledge is implicitly exploited by diffusion module from pre-contrast images, which is useful for cancer segmentation.</p><p>2) The intermediate activations from  diffusion models effectively capture the semantic information and are excellent pixel-level representations for the segmentation problem <ref type="bibr" target="#b8">[9]</ref>. Thus, combining the intermediate features can further promote the segmentation performance. In a word, the proposed framework can produce accurate prediction masks only requiring pre-contrast images. This is useful when post-contrast data is limited.</p><p>Ablation Study: To explore the effectiveness of the latent kinetic code, we first conduct ablation studies to select the optimal setting. We denote the intermediate features extracted from each stage in the DM as f 1 , f 2 , f 3 , and f 4 , respectively, where f i represents the feature map of i-th stage. Table <ref type="table" target="#tab_1">2</ref> reports the segmentation performance with different incorporations of intermediate kinetic codes. It can be observed that the latent kinetic code is able to guide the network training for better segmentation results. Specifically, we note that the incorporation of f 3 and f 4 achieves the highest scores among these combinations, and outperforms the integration of all features by 2.0% and 2.6% in DSC and JI, respectively. We attribute it to the denoising diffusion model that receives the noisy input, leading to the noise of shallow features. In contrast, the deep features capture essential characteristics to reveal the structural information and hemodynamic changes of tumors. Figure <ref type="figure" target="#fig_3">4</ref> shows visual comparison of segmentation performance. The above results reveal that incorporation of kinetic code comfortably outperform the baseline without hemodynamic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a diffusion kinetic model by exploiting hemodynamic priors in DCE-MRI to effectively generate high-quality segmentation results only requiring precontrast images. Our models learns the hemodynamic response function based on the denoising diffusion process and estimates the latent kinetic code to guide the segmentation task. Experiments demonstrate that our proposed framework has the potential to be a promising tool in clinical applications to annotate cancers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of hemodynamic response function and denoising diffusion process as well as their underlying relation. Right is the time intensity curve (TIC). x0 and x k represent pre-contrast images and post-contrast images in DCE-MRI, respectively.</figDesc><graphic coords="2,55,98,54,44,340,15,153,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Illustration of our method for implicitly exploiting hemodynamic information from pre-contrast images. The combination of learned kinetic code is an example.</figDesc><graphic coords="4,55,98,54,35,340,93,216,37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Examples of breast DCE-MRI sequences. The bottom is heatmaps to observe the intensity change of cancers. EPO: early post-contrast, and LPO: late post-contrast.</figDesc><graphic coords="7,41,79,53,78,340,21,93,94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visual comparison of segmentation performance. The baseline is implemented without the incorporation of kinetic code.</figDesc><graphic coords="8,55,98,54,11,340,15,145,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Cancer segmentation comparison between our method and previous models (Mean ± Std). The scans are employed for testing.</figDesc><table><row><cell>Method</cell><cell>Scans</cell><cell>Dice (%) ↑ JI (%) ↑</cell></row><row><cell>DANet [17]</cell><cell>post-contrast</cell><cell>52.3 ± 3.1 40.5 ± 3.1</cell></row><row><cell cols="2">MultiResUNet [18] post-contrast</cell><cell>55.6 ± 2.8 43.2 ± 3.0</cell></row><row><cell>MTLN [19]</cell><cell>post-contrast</cell><cell>54.2 ± 2.5 41.6 ± 2.8</cell></row><row><cell>LNet [20]</cell><cell cols="2">complete sequence 52.3 ± 2.9 40.4 ± 3.2</cell></row><row><cell cols="3">3D patch U-Net [21] complete sequence 53.8 ± 2.8 41.1 ± 3.0</cell></row><row><cell>HybridNet [5]</cell><cell cols="2">complete sequence 64.4 ± 2.4 51.5 ± 2.6</cell></row><row><cell>DKM (ours)</cell><cell>pre-contrast</cell><cell>71.5 ± 2.5 58.5 ± 2.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study for the incorporation of intermediate kinetic code.</figDesc><table><row><cell>67.6 ± 2.3 53.2 ± 2.4</cell></row><row><cell>70.1 ± 2.1 56.2 ± 2.3</cell></row><row><cell>71.5 ± 2.5 58.5 ± 2.6</cell></row><row><cell>70.2 ± 2.3 56.4 ± 2.3</cell></row><row><cell>69.5 ± 2.1 55.9 ± 2.4</cell></row></table><note><p>f1 f2 f3 f4 Dice (%) ↑ JI (%) ↑ 67.9 ± 2.4 54.4 ± 2.4 68.6 ± 2.3 55.0 ± 2.2 68.3 ± 2.4 54.8 ± 2.5 69.3 ± 2.0 55.5 ± 2.1</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported in part by the <rs type="funder">National Key R&amp;D Program of China</rs> under Grants <rs type="grantNumber">2021YFE0203700</rs> and <rs type="grantNumber">2018YFA0701700</rs>, the <rs type="funder">Postgraduate Research &amp; Practice Innovation Program of Jiangsu Province</rs> <rs type="grantNumber">KYCX23_2524</rs>, and is supported by <rs type="funder">National Natural Science Foundation of China</rs> grants <rs type="grantNumber">61602007</rs>, <rs type="grantNumber">U21A20521</rs> and <rs type="grantNumber">61731008</rs>, <rs type="funder">Zhejiang Provincial Natural Science Foundation of China</rs> (<rs type="grantNumber">LZ15F010001</rs>), Jiangsu Provincial Maternal and Child Health Research Project (<rs type="grantNumber">F202034</rs>), <rs type="funder">Wuxi Health Commission Precision Medicine Project</rs> (<rs type="grantNumber">J202106</rs>), <rs type="programName">Jiangsu Provincial Six Talent Peaks Project</rs> (<rs type="grantNumber">YY-124</rs>), and the <rs type="funder">Science and Technology Development Fund, Macau SAR</rs> (File no. <rs type="grantNumber">0004/2019/AFJ</rs> and <rs type="grantNumber">0011/2019/AKP</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_DN5cuHc">
					<idno type="grant-number">2021YFE0203700</idno>
				</org>
				<org type="funding" xml:id="_Z4CgBHX">
					<idno type="grant-number">2018YFA0701700</idno>
				</org>
				<org type="funding" xml:id="_utGWy2h">
					<idno type="grant-number">KYCX23_2524</idno>
				</org>
				<org type="funding" xml:id="_YTh7s2J">
					<idno type="grant-number">61602007</idno>
				</org>
				<org type="funding" xml:id="_ZhcY6FE">
					<idno type="grant-number">U21A20521</idno>
				</org>
				<org type="funding" xml:id="_8C72JRJ">
					<idno type="grant-number">61731008</idno>
				</org>
				<org type="funding" xml:id="_j45n5mA">
					<idno type="grant-number">LZ15F010001</idno>
				</org>
				<org type="funding" xml:id="_8aNxepr">
					<idno type="grant-number">F202034</idno>
				</org>
				<org type="funding" xml:id="_hbcJaQs">
					<idno type="grant-number">J202106</idno>
					<orgName type="program" subtype="full">Jiangsu Provincial Six Talent Peaks Project</orgName>
				</org>
				<org type="funding" xml:id="_nzSDaTF">
					<idno type="grant-number">YY-124</idno>
				</org>
				<org type="funding" xml:id="_ZgyGwrk">
					<idno type="grant-number">0004/2019/AFJ</idno>
				</org>
				<org type="funding" xml:id="_AqPRA4c">
					<idno type="grant-number">0011/2019/AKP</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Radiogenomic signatures reveal multiscale intratumour heterogeneity associated with biological functions and survival in breast cancer</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4861</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A U-net ensemble for breast lesion segmentation in DCE MRI</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Vilanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martí</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">105093</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Three-dimensional breast tumor segmentation on DCE-MRI with a multilabel attention-guided joint-phase-learning network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">101909</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully-automated deep learning-powered system for DCE-MRI analysis of brain tumors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nalepa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">101769</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hybrid hemodynamic knowledge-powered and feature reconstruction-guided scheme for breast cancer segmentation based on dce-mri</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102572</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Breast tumor segmentation in DCE-MRI with tumor sensitive synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Labelefficient semantic segmentation with diffusion models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Baranchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rubachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation (ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Can segmentation models be trained with fully synthetically generated data?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Fernandez</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16980-9_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16980-9_8" />
	</analytic>
	<monogr>
		<title level="m">SASHIMI 2022</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Svoboda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolterink</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Escobar</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13570</biblScope>
			<biblScope unit="page" from="79" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diffusion models for implicit image segmentation ensembles</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wolleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valmaggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1336" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Junde</surname></persName>
		</author>
		<title level="m">Medsegdiff: medical image segmentation with diffusion probabilistic model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Single site breast DCE-MRI data and segmentations from patients undergoing neoadjuvant chemotherapy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Newitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hylton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Imaging Arch</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diffusion deformable model for 4D temporal medical image generation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_51</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-6_51" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SSIM-variation-based complexity optimization for versatile video coding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yiwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2617" to="2621" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiresunet: rethinking the U-net architecture for multimodal biomedical image segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ibtehaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-task learning for segmentation and classification of tumors in 3D automated breast ultrasound images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">101918</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatio-temporal learning from longitudinal data for multiple sclerosis lesion segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Denner</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-72084-1_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-72084-1_11" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12658</biblScope>
			<biblScope unit="page" from="111" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning based segmentation of breast lesions in DCE-MRI</title>
		<author>
			<persName><forename type="first">R</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martí</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-68763-2_32</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-68763-2_32" />
	</analytic>
	<monogr>
		<title level="m">ICPR 2021</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12661</biblScope>
			<biblScope unit="page" from="417" to="430" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
