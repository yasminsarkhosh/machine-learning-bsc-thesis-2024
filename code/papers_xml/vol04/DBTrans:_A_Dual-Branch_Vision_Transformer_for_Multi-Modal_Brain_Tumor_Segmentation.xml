<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation</title>
				<funder ref="#_K432qwt">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Fgpmwjv">
					<orgName type="full">Opening Foundation of Agile and Intelligent Computing Key Laboratory of Sichuan Province</orgName>
				</funder>
				<funder ref="#_aTrs84A #_3KujUMw #_jSY2jwz">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xinyi</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pinxian</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Binyu</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<email>wangyanscu@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Sichuan University</orgName>
								<address>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="502" to="512"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">C0067E792CD3F9B59759604A0E2C4A24</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_48</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spatially aligned multi-modal MRI (SAMM)</term>
					<term>Brain tumor segmentation (BTS)</term>
					<term>Transformer</term>
					<term>Cross-Attention</term>
					<term>Channel-Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D Spatially Aligned Multi-modal MRI Brain Tumor Segmentation (SAMM-BTS) is a crucial task for clinical diagnosis. While Transformer-based models have shown outstanding success in this field due to their ability to model global features using the self-attention mechanism, they still face two challenges. First, due to the high computational complexity and deficiencies in modeling local features, the traditional self-attention mechanism is ill-suited for SAMM-BTS tasks that require modeling both global and local volumetric features within an acceptable computation overhead. Second, existing models only stack spatially aligned multi-modal data on the channel dimension, without any processing for such multi-channel data in the model's internal design. To address these challenges, we propose a Transformer-based model for the SAMM-BTS task, namely DBTrans, with dual-branch architectures for both the encoder and decoder. Specifically, the encoder implements two parallel feature extraction branches, including a local branch based on Shifted Window Self-attention and a global branch based on Shuffle Window Cross-attention to capture both local and global information with linear computational complexity. Besides, we add an extra global branch based on Shifted Window Cross-attention to the decoder, introducing the key and value matrices from the corresponding encoder block, allowing the segmented target to access a more complete context during up-sampling. Furthermore, the above dual-branch designs in the encoder and decoder are both integrated with improved channel attention mechanisms to fully explore the contribution of features at different channels. Experimental results demonstrate the superiority of our DBTrans model in both qualitative and quantitative measures. Codes will be released at https://github.com/Aru321/DBTrans.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Glioma is one of the most common malignant brain tumors with varying degrees of invasiveness <ref type="bibr" target="#b0">[1]</ref>. Brain Tumor Semantic segmentation of gliomas based on 3D spatially aligned Magnetic Resonance Imaging (SAMM-BTS) is crucial for accurate diagnosis and treatment planning. Unfortunately, radiologists suffer from spending several hours manually performing the SAMM-BTS task in clinical practice, resulting in low diagnostic efficiency. In addition, manual delineation requires doctors to have high professionalism. Therefore, it is necessary to design an efficient and accurate glioma lesion segmentation algorithm to effectively alleviate this problem and relieve doctors' workload and improve radiotherapy quality.</p><p>With the rise of deep learning, researchers have begun to study deep learning-based image analysis methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">37]</ref>. Specifically, many convolutional neural network-based (CNN-based) models have achieved promising results <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. Compared with natural images, medical image segmentation often requires higher accuracy to make subsequent treatment plans for patients. U-Net reaches an outstanding performance on medical image segmentation by combining the features from shallow and deep layers using skipconnection <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. Based on U-Net, Brugger. et al. <ref type="bibr" target="#b11">[12]</ref> proposed a partially reversible U-Net to reduce memory consumption while maintaining acceptable segmentation results. Pei et al. <ref type="bibr" target="#b12">[13]</ref> explored the efficiency of residual learning and designed a 3D ResUNet for multi-modal brain tumor segmentation. However, due to the lack of global understanding of images for convolution operation, CNN-based methods struggle to model the dependencies between distant features and make full use of the contextual information <ref type="bibr" target="#b13">[14]</ref>. But for semantic segmentation tasks whose results need to be predicted at pixel-level or voxel-level, both local spatial details and global dependencies are extremely important.</p><p>In recent years, models based on the self-attention mechanism, such as Transformer, have received widespread attention due to their excellent performance in Natural Language Processing (NLP) <ref type="bibr" target="#b14">[15]</ref>. Compared with convolution operation, the self-attention mechanism is not restricted by local receptive fields and can capture long-range dependencies. Many works <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> have applied Transformers to computer vision tasks and achieved favorable results. For classification tasks, Vision Transformer (ViT) <ref type="bibr" target="#b18">[19]</ref> was a groundbreaking innovation that first introduced pure Transformer layers directly across domains. And for semantic segmentation tasks, many methods, such as SETR <ref type="bibr" target="#b19">[20]</ref> and Segformer <ref type="bibr" target="#b20">[21]</ref>, use ViT as the direct backbone network and combine it with a taskspecific segmentation head for prediction results, reaching excellent performance on some 2D natural image datasets. For 3D medical image segmentation, Vision Transformer has also been preferred by researchers. A lot of robust variants based on Transformer have been designed to endow U-Net with the ability to capture contextual information in long-distance dependencies, further improving the semantic segmentation results of medical images <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b27">[27]</ref>. Wang et al. <ref type="bibr" target="#b24">[25]</ref> proposed a novel framework named TransBTS that embeds the Transformer in the bottleneck part of a 3D U-Net structure. Peiris et al. <ref type="bibr" target="#b25">[26]</ref> introduced a 3D Swin-Transformer <ref type="bibr" target="#b29">[28]</ref> to segmentation tasks and first incorporated the attention mechanism into skip-connection.</p><p>While Transformer-based models have shown effectiveness in capturing long-range dependencies, designing a Transformer architecture that performs well on the SAMM-BTS task remains challenging. First, modeling relationships between 3D voxel sequences is much more difficult than 2D pixel sequences. When applying 2D models, 3D images need to be sliced along one dimension. However, the data in each slice is related to three views, discarding any of them may lead to the loss of local information, which may cause the degradation of performance <ref type="bibr" target="#b30">[29]</ref>. Second, most existing MRI segmentation methods still have difficulty capturing global interaction information while effectively encoding local information. Moreover, current methods just stack modalities and pass them through a network, which treats each modality equally along the channel dimension and may ignore the contribution of different modalities. To address the above limitations, we propose a novel encoder-decoder model, namely DBTrans, for multi-modal medical image segmentation. In the encoder, two types of window-based attention mechanisms, i.e., Shifted Window-based Multi-head Self Attention (Shifted-W-MSA) and Shuffle Window-based Multi-head Cross Attention (Shuffle-W-MCA), are introduced and applied in parallel to dual-branch encoder layers, while in the decoder, in addition to Shifted-W-MSA mechanism, Shifted Window-based Multi-head Cross Attention (Shifted-W-MCA) is designed for the dual-branch decoder layers. These mechanisms in the dual-branch architecture greatly enhance the ability of both local and global feature extraction. Notably, DBTrans is designed for 3D medical images, avoiding the information loss caused by data slicing.</p><p>The contributions of our proposed method can be described as follows: 1) Based on Transformer, we construct dual-branch encoder and decoder layers that assemble two attention mechanisms, being able to model close-window and distant-window dependencies without any extra computational cost. 2) In addition to the traditional skipconnection structure, in the dual-branch decoder, we also establish an extra path to facilitate the decoding process. We design a Shifted-W-MCA-based global branch to build a bridge between the decoder and encoder, maintaining affluent information of the segmentation target during the decoding process. 3) For the multi-modal data adopt in the task of SAMM-BTS, we improve the channel attention mechanism in SE-Net by applying SE-weights to features from both branches in the encoder and decoder layers. By this means, we implicitly consider the importance of multiple MRI modalities and two window-based attention branches, thereby strengthening the fusion effect of the multi-modal information from a global perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the overall structure of the proposed DBTrans. It is an end-to-end framework that has a 3D patch embedding along with a U-shaped model containing an encoder and a decoder. The model takes MRI data of D×H ×W ×C with four modalities stacked along channel dimensions as the input. The 3D patch embedding converts the input data to feature embedding e 1 ∈ R D 1 ×H 1 ×W 1 ×C 1 which will be further processed by encoder layers. At the tail of the decoder, the segmentation head takes the output of the last layer and generates the final segmentation result of D × H × W × K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dual-Branch in Encoder</head><p>As shown in Fig. <ref type="figure" target="#fig_2">1(b)</ref>, the encoder consists of four dual-branch encoder layers (one bottleneck included). Each encoder layer contains two consecutive encoder blocks and a 3D patch merging to down-sample the feature embedding. Note that there is only one encoder block in the bottleneck. The encoder block includes a dual-branch architecture with the local feature extraction branch, the global feature extraction branch, and the channel-attention-based dual-branch fusion module. After acquiring the embedding 2] which are then separately fed into two branches.</p><formula xml:id="formula_0">e i ∈ R D i ×H i ×W i ×C i in the i-th encoder layer (i ∈ [1, 4]), we split it along the channel dimension, obtaining e i 1 , e i 2 ∈ R D i ×H i ×W i ×[C i /</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shifted-W-MSA-Based Local Branch. The image embedding e</head><formula xml:id="formula_1">i 1 ∈ R D i ×H i ×W i ×[C i /2</formula><p>] is fed into the local branch in the encoder block. In the process of window partition (denoted as WP), e i 1 is split into non-overlapping windows after a layer normalization (LN) operation to obtain the window matrix m i 1 . Since we set M as 2, the input of 4×4×4 size is uniformly divided into 8 windows of 2 × 2 × 2. Following 3D Swin-Transformer <ref type="bibr" target="#b27">[27]</ref>, we introduce MSA based on the shifted-window partition to the second block in an encoder layer. During window partition of Shifted-W-MSA, the whole feature map is shifted by half of the window size, i.e., M  2 , M 2 , M 2 . After the window partition, W -MSA is applied to calculate multi-head self-attention within each window. Specifically, for window matrix m i 1 , we first apply projection matrices W i Q ,W i K , and <ref type="figure"></ref>and<ref type="figure">V i</ref> 1 matrices (the process is denoted as Proj). After projection, multi-head selfattention calculation is performed on Q i 1 ,K i 1 , and V i 1 to get the attention score of every window. Finally, we rebuild the feature map from the windows, which serves as the inverse process of the window partition. After calculating the attention score, other basic components in Transformer are also employed, that is, layer normalization (LN), as well as a multi-layer perceptron (MLP) with two fully connected layers and a Gaussian Error Linear Unit (GELU). Residual connection is applied after each module. The whole process can be expressed as follows:</p><formula xml:id="formula_2">W i V to obtain Q i 1 ,K i 1 ,</formula><formula xml:id="formula_3">m i 1 = Shifted-WP LN (e i 1 ) , Q i 1 , K i 1 , V i 1 ∈ R D i H i W i M 3 ×M 3 ×[C i /2] = Proj i m i 1 = W i Q • m i 1 , W i K • m i 1 , W i V • m i 1 , ẑi 1 = W -MSA Q i 1 , K i 1 , V i 1 , o i 1 = MLP i LN ẑi 1 + e i 1 + ẑi 1 + e i 1 ,<label>(1)</label></formula><p>where ẑi 1 represents the attention score after W -MSA calculation, " Shifted-" represents that we use the shifted-window partition and restoration in the second block of every encoder layer. o i 1 is the final output of the local branch. Shuffle-W-MCA-Based Global Branch. Through the local branch, the network still cannot model the long-distance dependencies between non-adjacent windows in the same layer. Thus, Shuffle-W-MCA is designed to complete the complementary task. After the window-partition process that converts the embedding</p><formula xml:id="formula_4">e i 2 ∈ R D i ×H i ×W i ×[C i /2] to m i 2 ∈ R D i H i W i M 3 ×M 3 ×[C i /2]</formula><p>, inspired by ShuffleNet <ref type="bibr" target="#b37">[35]</ref>, instead of moving the channels, we propose to conduct shuffle operations on the patches in different windows. The patches at the same relative position in different windows are rearranged together in a window, and their position is decided by the position of the window they originally belong to. Then, this branch takes the query from the local branch, while generating keys and values from m i 2 to compute cross-attention scores. Such a design aims to enable the network to model the relationship between a window and other distant windows. Note that we adopt the projection function Proj from the local branch, indicating that the weights of the two branches are shared. Through the shuffle operation, the network can generate both local and global feature representations without setting additional weight parameters. The whole process can be formulated as follows:</p><formula xml:id="formula_5">Q i 2 , K i 2 , V i 2 ∈ R D i H i W i M 3 ×M 3 ×[C i /2] = Proj i Shuffle(m i 2 ) , ẑi 2 = W -MCA Q = Q i 1 , K = K i 2 , V = V i 2 , o i 2 = MLP i LN ẑi 2 + e i 2 + ẑi 1 + e i 1 .</formula><p>(</p><p>In the second block, we get the final output o i 2 of the layer through the same process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dual-Branch in Decoder</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(c), the decoder takes the output of the encoder as the input and generates the final prediction through three dual-branch decoder layers as well as a segmentation head. Each decoder layer consists of two consecutive decoder blocks and a 3D patch expanding layer. As for the j-th decoder layer (j ∈ [1, 3]), the embedding d j ∈ R D j ×H j ×W j ×C j is also divided into the feature maps 2] . We further process d j 1 , d j 2 using a dual-branch architecture similar to that of the encoder, but with an additional global branch based on Shifted-W-MCA mechanism. Finally, the segmentation head generates the final segmentation result of D × H × W × K, where K represents the number of classes. The local branch based on Shifted-W-MSA is the same as that in the encoder and will not be introduced in this section.</p><formula xml:id="formula_7">d j 1 , d j 2 ∈ R D j ×H j ×W j ×[C j /</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shifted-W-MCA-Based Global Branch.</head><p>Apart from employing Shifted-W-MSA to form the local branch of the decoder layer, we design a novel Shifted-W-MCA mechanism for the global branch to ease the information loss during the decoding process and take full advantage of the features from the encoder layers. The global branch receives the query matrix from the split feature map d j 2 ∈ R D j ×H j ×W j ×[C j /2] , while receiving key and value matrices from the encoder block in the corresponding stage, denoted as Q j 2 , K e i , and V e i . The process of Shifted-W-MCA can be formulated as follows:</p><formula xml:id="formula_8">Q j 2 , K j 2 , V j 2 = Proj j Shifted-WP LN (d i 2 ) , ẑj 2 = W -MCA Q = Q j 2 , K = K e 4-j 1 , V = V e 4-j 1 , o j 2 = MLP i LN ẑj 2 + d j 2 + ẑj 1 + d j 1 ,<label>(3)</label></formula><p>where ẑj 2 denotes the attention score after MCA calculation, o j 2 denotes the final output of the global branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Channel-Attention-Based Dual-Branch Fusion</head><p>As shown in Fig. <ref type="figure" target="#fig_2">1(b</ref>) and Fig. <ref type="figure" target="#fig_2">1(c</ref>), the dual-branch fusion module is based on the channel attention. For the block of the m -th (m ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>) encoder or decoder layer, the dual-branch fusion module combines the features 2]  from the two extraction branches, obtaining a feature map filled with abundant multiscale information among different modalities. Subsequently, the dependencies between the feature channels within the individual branches are implicitly modeled with the SE-Weight assignment first proposed in SE-Net <ref type="bibr" target="#b31">[30]</ref>. Different from SE-Net, we dynamically assign weights for both dual-branch fusion and multi-modal fusion. The process of obtaining the attention weights can be represented as the formula (5) below:</p><formula xml:id="formula_9">o m 1 , o m 2 ∈ R D m ×H m ×W m ×[C m /</formula><formula xml:id="formula_10">Z p = SE_Weight o m p , p = 1, 2,<label>(4)</label></formula><p>where</p><formula xml:id="formula_11">Z p ∈ R [C/2]×1×1×1</formula><p>is the attention weight of a single branch. Then, the weight vectors of the two branches are re-calibrated using a Softmax function. Finally, the weighted channel attention is multiplied with the corresponding scale feature map to obtain the refined output feature map with richer multi-scale feature information:</p><formula xml:id="formula_12">attn p = Softmax Z p = exp(Z p) 2 p=1 exp(Z p) , Y p = o m p attn p , p = 1, 2, O = Cat([Y 1 , Y 2 ]),<label>(5)</label></formula><p>where " " represents the operation of element-wise multiplication and "Cat" represents the concatenation. The concatenated output O, serving as the dual-branch output of a block in the encoder or decoder, implicitly integrates attention interaction information within individual branches across different channels/modalities, as well as across different branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training Details</head><p>For the loss function, the widely used cross entropy (CE) loss and Dice loss <ref type="bibr" target="#b34">[32]</ref> are introduced to train our DBTrans. Here we use parameter γ to balance the two loss parts. Our network is implemented based on PyTorch, and trained for 300 epochs using a single RTX 3090 with 24G memory. The weights of the network were updated using the Adam optimizer, the batch size was set to 1, and the initial learning rate was set to 1 × 10 -4 . A cosine decay scheduler was used as the adjustment strategy for the learning rate during training. We set the embedding dimensions C 0 as 144. Following previous segmentation methods, the parameter γ is set to 0.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Datasets. We use the Multimodal Brain Tumor Segmentation Challenge (BraTS 2021 <ref type="bibr" target="#b35">[33,</ref><ref type="bibr" target="#b36">34,</ref><ref type="bibr" target="#b41">38]</ref>) as the benchmark training set, validation set, and testing set. We divide the 1251 scans provided into 834, 208, and 209 (in a ratio of 2:1:1), respectively for training and testing. The ground truth labels of GBM segmentation necrotic/active tumor and edema are used to train the model. The BraTS 2021 dataset reflects real clinical diagnostic species and has four spatially aligned MRI modality data, namely T1, T1CE, T2, and Flair, which are obtained from different devices or according to different imaging protocols. The dataset contains three distinct sub-regions of brain tumors, namely peritumoral edema, enhancing tumor, and tumor core. The data augmentation includes random flipping, intensity scaling and intensity shifting on each axis with probabilities set to 0.5, 0.1 and 0.1, respectively.</p><p>Comparative Experiments. To evaluate the effectiveness of the proposed DBTrans, we compare it with the state-of-the-art brain tumor segmentation methods including six Transformer-based networks Swin-Unet <ref type="bibr" target="#b27">[27]</ref>, TransBTS <ref type="bibr" target="#b24">[25]</ref>, UNETR <ref type="bibr" target="#b21">[22]</ref>, nnFormer <ref type="bibr" target="#b22">[23]</ref>, VT-Unet-B <ref type="bibr" target="#b25">[26]</ref>, NestedFormer <ref type="bibr" target="#b38">[36]</ref> as well as the most basic CNN network 3D U-Net <ref type="bibr" target="#b32">[31]</ref> as the baseline. During inference, for any size of 3D images, we utilize the overlapping sliding windows technique to generate multi-class prediction results and take average values for the voxels in the overlapping region. The evaluation strategy adopted in this work is consistent with that of VT-Unet <ref type="bibr" target="#b25">[26]</ref>. For other methods, we used the corresponding hyperparameter configuration mentioned in the original papers and reported the average metrics over 3 runs.  Ablation Study. To further verify the contribution of each module, we establish the ablation models based on the modules introduced above. Note that, DP-E represents the dual-branch encoder layer, while DB-D represents the dual-branch decoder layer. When the dual-branch fusion is not included, we do not split the input, and simply fuse the features from the two branches using a convolution layer. In all, there are 5 models included in this ablation study:  <ref type="table" target="#tab_1">2</ref>, After applying our proposed dual-branch encoder and decoder layers to the baseline model, the average Dice score notably increased by 2.13. Subsequently, applying the dual-branch fusion module also prominently contributes to the performance of the model by an improvement of 0.83 on the Dice score. Notably, our dual-branch designs achieve higher performance while also reducing the number of parameters required. This is because we split the original feature embedding into two parts, thus the channel dimensions of features in two branches are halved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we innovatively proposed an end-to-end model named DBTrans for multimodal medical image segmentation. In DBTrans, first, we well designed the dual-branch structures in encoder and decoder layers with Shifted-W-MSA, Shuffle-W-MCA, and Shifted-W-MCA mechanisms, facilitating feature extraction from both local and global views. Moreover, in the decoder, we establish a bridge between the query of the decoder and the key/value of the encoder to maintain the global context during the decoding process for the segmentation target. Finally, for the multi-modal superimposed data, we modify the channel attention mechanism in SE-Net, focusing on exploring the contribution of different modalities and branches to the effective information of feature maps. Experimental results demonstrate the superiority of DBTrans compared with the state-of-the-art medical image segmentation methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overall framework of the proposed DBTrans. The encoder contains dual-branch encoder layers (including a bottleneck), and the decoder contains dual-branch encoder layers. Skip connections based on cross attention are built between encoder and decoder.</figDesc><graphic coords="4,89,97,56,48,272,53,138,91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative segmentation results on the test samples. The bottom row zooms-in the segmentation regions. Green, yellow, and red represent the peritumoral edema (ED), enhancing tumor (ET) and non-enhancing tumor/necrotic tumor (NET/NCR).</figDesc><graphic coords="7,59,79,136,58,304,69,96,70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 1 )</head><label>1</label><figDesc>SwinUnet-1 (baseline): We use Swin-Transformer layers without any dual-branch module. (2) SwinUnet-2: Based on (1), we add dual-branch encoder layers to the model. (3) SwinUnet-3: Based on (1), we add dual-branch decoder layers to the model. (4) SwinUnet-4: Based on (1), add both the encoder and decoder without the dual-branch fusion module. (5) Our proposed DBTrans model. As shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Table 1 presents the Dice scores and 95% HDs of different methods for segmentation results on three different tumor regions (i.e., Quantitative comparison with other state-of-the-arts methods in terms of dice score and 95% Hausdorff distance. Best results are bold, and second best are underlined.ET, TC and WT) respectively, where a higher Dice score indicates better results and a lower 95% HD indicates better performance. By observation, our approach achieved the best performance on average among these methods in all three tumor regions. Compared with the state-of-the-art method VT-Unet, our method increased the average Dice score by 1.62 percentage points and achieved the lowest average 95% HD. Moreover, to verify the significance of our improvements, we calculate the variances of all results and conduct statistical tests (i.e., paired t-test). The results show that p-values on Dice and 95%HD are less than 0.05 in most comparison cases, indicating that the improvements are statistically significant. Compared with other methods, our DBTrans can not only capture more precise long-term dependencies between non-adjacent slices through Shuffle-W-MCA, but also accomplish dual-branch fusion and multi-modal fusion through dynamic SE-Weight assignment, obtaining better feature representations for segmentation.</figDesc><table><row><cell>Method</cell><cell cols="4">#param FLOPS Dice Score</cell><cell>95% Hausdorff Distance</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ET</cell><cell>TC</cell><cell>WT AVG ET TC</cell><cell>WT AVG</cell></row><row><cell>3D U-Net[31]</cell><cell cols="4">11.9M 557.9G 83.39 86.28 89.59 86.42 6.15 6.18 11.49 7.94</cell></row><row><cell>Swin-Unet[27]</cell><cell cols="4">52.5M 93.17G 83.34 87.62 89.81 89.61 6.19 6.35 11.53 8.03</cell></row><row><cell>TransBTS[25]</cell><cell>33M</cell><cell>333G</cell><cell cols="2">80.35 85.35 89.25 84.99 7.83 8.21 15.12 10.41</cell></row><row><cell>UNETR[22]</cell><cell cols="4">102.5M 193.5G 79.78 83.66 90.10 84.51 9.72 10.01 15.99 11.90</cell></row><row><cell>nnFormer[23]</cell><cell cols="4">39.7M 110.7G 82.83 86.48 90.37 86.56 8.00 7.89 11.66 9.18</cell></row><row><cell>VT-Unet-B[26]</cell><cell cols="4">20.8M 165.0G 85.59 87.41 91.02 88.07 6.23 6.29 10.03 7.52</cell></row><row><cell cols="5">NestedFormer[36] 10.48M 71.77G 85.62 88.18 90.12 87.88 6.08 6.43 10.23 7.63</cell></row><row><cell>DBTrans(Ours)</cell><cell cols="4">24.6M 146.2G 86.70 90.26 92.41 89.69 6.13 6.24 9.84 7.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison of ablation models in terms of average dice score.Figure 2 also shows the qualitative segmentation results on the test samples of test set patients, which further proves the feasibility and superiority of our DBTrans model. From the zoom-in area, we can observe that our model can more accurately segment tumor structures and delineate tumor boundaries compared with other methods.</figDesc><table><row><cell>Name</cell><cell>Index</cell><cell>DB-E</cell><cell>DB-D</cell><cell>Dual-branch fusion</cell><cell>Avg-Dice</cell><cell>Param</cell></row><row><cell>SwinUnet-1 SwinUnet-2 SwinUnet-3 SwinUnet-4 proposed</cell><cell>(1) (2) (3) (4) (5)</cell><cell>× √ × √ √</cell><cell>× × √ √ √</cell><cell>× × × × √</cell><cell>86.73 87.52 88.26 88.86 89.69</cell><cell>52.5M 43.2M 46.1M 27.7M 24.6M</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work is supported by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">NSFC 62371325</rs>, <rs type="grantNumber">62071314</rs>), <rs type="programName">Sichuan Science and Technology Program</rs> <rs type="grantNumber">2023YFG0263</rs>, <rs type="grantNumber">2023YFG0025</rs>, <rs type="grantNumber">2023NSFSC0497</rs>, and <rs type="funder">Opening Foundation of Agile and Intelligent Computing Key Laboratory of Sichuan Province</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_K432qwt">
					<idno type="grant-number">NSFC 62371325</idno>
				</org>
				<org type="funding" xml:id="_aTrs84A">
					<idno type="grant-number">62071314</idno>
					<orgName type="program" subtype="full">Sichuan Science and Technology Program</orgName>
				</org>
				<org type="funding" xml:id="_3KujUMw">
					<idno type="grant-number">2023YFG0263</idno>
				</org>
				<org type="funding" xml:id="_jSY2jwz">
					<idno type="grant-number">2023YFG0025</idno>
				</org>
				<org type="funding" xml:id="_Fgpmwjv">
					<idno type="grant-number">2023NSFSC0497</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">State of the art survey on MRI brain tumor segmentation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gordillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Montseny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1426" to="1438" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive rectification based adversarial network with spectrum constraint for high-quality PET image synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">102335</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised medical image segmentation via a tripled-uncertainty guided mean teacher model with contrastive learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">102447</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coarse-To-fine segmentation of organs at risk in nasopharyngeal carcinoma radiotherapy</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_34" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="358" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unified medical image segmentation by learning from uncertainty in an end-to-end manner</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">241</biblScope>
			<biblScope unit="page">108215</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Kernel-based feature aggregation framework in point cloud networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Uncertainty-weighted and relation-driven consistency training for semi-supervised head-and-neck tumor segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">272</biblScope>
			<biblScope unit="page">110598</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An efficient semi-supervised framework with multi-task and curriculum learning for medical image</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">09</biblScope>
			<biblScope unit="page">2250043</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-level feature aggregation network for polyp segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">109555</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Medical image segmentation based on u-net: a review</title>
		<author>
			<persName><forename type="first">G</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Imaging Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="20508" to="20509" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A partially reversible U-Net for memoryefficient volumetric image segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Brügger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32248-9_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32248-9_48" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11766</biblScope>
			<biblScope unit="page" from="429" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BrainLes 2021, Held in Conjunction with MICCAI 2021, Virtual Event, Revised Selected Papers, Part I</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-08999-2_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-08999-2_26" />
	</analytic>
	<monogr>
		<title level="m">th International Workshop</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
	<note>Multimodal brain tumor segmentation using a 3D ResUNet in BraTS 2021</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3D CVT-GAN: a 3D convolutional vision transformer-GAN for PET reconstruction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_49" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Proceedings</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="516" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A"</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3D Transformer-GAN for high-quality PET reconstruction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-1_27" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="276" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A transformer-embedded multi-task model for dose distribution prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2350043" to="2350043" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-tosequence perspective with transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SegFormer: simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12077" to="12090" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unetr: transformers for 3d medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">nnformer: Interleaved transformer for volumetric segmentation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03201</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ds-transunet: dual swin transformer u-net for medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transbts: multimodal brain tumor segmentation using transformer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_11" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="109" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A robust volumetric transformer for accurate 3d tumor segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Proceedings, Part V</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="162" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_16" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Swin-unet: unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022 Workshops. Proceedings, Part III</title>
		<imprint>
			<biblScope unit="page" from="205" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25066-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25066-8_9" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video swin transformer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3202" to="3211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A novel domain adaptation framework for medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gholami</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11726-9_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-11726-9_26" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2018</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Kuijf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Keyvan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Van Walsum</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11384</biblScope>
			<biblScope unit="page" from="289" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Çiçek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8_49" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey of loss functions for semantic segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jadon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)</title>
		<meeting>IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The RSNA-ASNR-MICCAI BraTS 2021 benchmark on brain tumor segmentation and radiogenomic classification</title>
		<author>
			<persName><forename type="first">U</forename><surname>Baid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02314</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma mri collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kirby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Sci. Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">170117</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shufflenet: an extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">NestedFormer: nested modality-aware transformer for brain tumor segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Proceedings, Part V</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="140" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_14" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">auto-context-based locality adaptive multimodality GANs for PET synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3D</biblScope>
			<biblScope unit="page" from="1328" to="1339" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (BRATS)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
