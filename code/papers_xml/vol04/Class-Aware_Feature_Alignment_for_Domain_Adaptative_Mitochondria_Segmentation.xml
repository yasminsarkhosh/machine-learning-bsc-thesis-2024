<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation</title>
				<funder ref="#_pw2nUb6">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dan</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Brain-inspired Intelligence Technology and Application</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<postCode>230088</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Brain-inspired Intelligence Technology and Application</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<postCode>230088</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiwei</forename><surname>Xiong</surname></persName>
							<email>zwxiong@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Brain-inspired Intelligence Technology and Application</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<postCode>230088</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuejin</forename><surname>Chen</surname></persName>
							<email>xjchen99@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Laboratory for Brain-inspired Intelligence Technology and Application</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<postCode>230088</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="238" to="248"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">33F51718D05EDA9969AE1A11E20B88B1</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_23</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised domain adaptation</term>
					<term>Class-aware alignment</term>
					<term>Mitochondria segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation (UDA) has gained great popularity in mitochondria segmentation, aiming to improve the adaptability of models from the labeled source domain to the unlabeled target domain via domain alignment. However, existing UDA methods only focus on aligning domains on the prediction level, while ignoring the feature space containing more adequate information than the predictions. In this paper, we propose a class-aware domain adaptation method for mitochondria segmentation on the feature level, which relies on the prototype representation to achieve more fine-grained alignment. In particular, we first extract the feature centroids of classes from the source domain as prototypes. Leveraging the extracted prototypes as a bridge, we constrain that features belonging to the same class but from different domains are pulled closer to each other, achieving the class-aware alignment. Meanwhile, we derive a segmentation prediction directly from feature space based on the distance between target features and source prototypes. By incorporating a pseudo label to supervise the learning of this prediction, the feature distribution gap across domains is further reduced. Furthermore, to take full advantage of the potential of target domain, we propose an intra-domain consistency constraint to maintain consistent predictions of samples perturbed differently from the target image. Extensive experiments on different datasets demonstrate the superiority of our proposed method over existing UDA methods. Code is available at https://github.com/Danyin813/CAFA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Mitochondria segmentation from electron microscopy (EM) images is pivotal to mitochondria morphological analysis <ref type="bibr" target="#b14">[15]</ref>. With pixel-wise annotations, existing supervised mitochondria segmentation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref> have achieved extraordinary advances in the test data, when the training data and test data come from the same distribution. However, the data distribution of EM images varies in real scenarios due to the diversity in imaging devices, collected organisms and tissues. The different distributions between training and test data, i.e., domain shift <ref type="bibr" target="#b12">[13]</ref>, lead to drastic performance drops on the test data. Manual annotations and model finetuning can ameliorate this problem but at a huge cost. Instead, unsupervised domain adaptive (UDA) mitochondria segmentation methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21]</ref>, aiming to transfer the knowledge learned in the labeled dataset (source domain) to unlabeled data (target domain) without any annotations, have gained great popularity in the community.</p><p>The mainstream of previous works focus on aligning the distributions of the source and target domains with supervision directly on the output segmentation maps. One line of these works use the model pretrained on source domain to obtain pseudo labels for target domain <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>, the performance of which highly relies on the quality of the pseudo labels. Other methods are mainly based on GAN <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref>, where an additional domain adversarial learning task, designed to reduce the domain gap between source and target domains, is jointly optimized with segmentation task. The feature space, having higher dimension than the predictions, can express more adequate class-aware knowledge. However, these methods perform domain alignment on the output space, which has insufficient information compared with the feature space, hindering effective alignment.</p><p>In this work, to take full advantage of the sufficient class related information in the feature space, we propose a class-aware domain alignment in the feature space for mitochondria segmentation, which relies on the prototype representation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref> to achieve fine-grained feature alignment. Specifically, 1) we first extract the source feature centroid of each class as prototype. To make the prototypes represent source class knowledge better, we minimize the distance between prototype and its within-class source features, as well as push different prototypes away from each other. Also, we select partial target features close enough to source prototypes and minimize their distance to align domains at class level. 2) To further supervise all target features, we derive the closest prototype as predicted result for each target feature vector based on its distance to prototypes, resulting in a segmentation result directly from the feature space without the segmentation head. A pseudo label is utilized to supervise these predictions for further cross-domain alignment with class knowledge. 3) Though cross-domain alignment can introduce knowledge from source domain to the target domain, there still exists additional potential information useful to segmentation in the target domain <ref type="bibr" target="#b17">[18]</ref>. Taking this into consideration, we further propose an intra-domain consistency constraint for target samples, where two input images perturbed differently from the same image are enforced to generate the same features and predictions.</p><p>Our contributions can be summarized as follows: 1) We propose a class-aware feature alignment method for domain adaptive mitochondria segmentation. To our best knowledge, it is the first attempt to align source and target domains on the feature level in UDA for EM mitochondria segmentation. 2) Our class-aware feature alignment relies on the source prototypes, which represent class knowledge from the feature space. With these prototypes, an innovative distance-based alignment and pseudo-labeling are incorporated to achieve class-aware feature alignment. 3) We propose an intra-domain consistency constraint in the target domain to tap into the potential target domain information. 4) We conduct thorough experiments on various EM dataset benchmarks and our proposed method achieves state-of-the-art performance for mitochondria segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Class-Aware Feature Alignment</head><p>Problem Formulation. Unsupervised domain adaptation (UDA) aims to transfer the knowledge learned from the labeled source domain to the unlabeled target domain. In our work, we denote the source domain as</p><formula xml:id="formula_0">D S = {X S , Y S } = {(x s i , y s i )} M i=1</formula><p>with M samples, where y s i is the groundtruth binary segmentation map of the input image x s i . The unlabeled target domain is denoted as</p><formula xml:id="formula_1">D T = {X t } = {x t i } N i=1</formula><p>with N samples. The overall framework of our proposed method is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prototype Extraction.</head><p>Considering there exists more plentiful class-aware information in the feature space than the predictions, we propose the class-aware alignment for better adaptation in the feature space. To achieve class-aware alignment, we first derive the class-aware source prototypes from the source features with the corresponding labels. The prototypes can be calculated as the centroid of each class in the feature space:</p><formula xml:id="formula_2">p s c = Bs b=1 Hs h=1 Ws w=1 f s b,h,w 1[Y s b,h,w = c] Bs b=1 Hs h=1 Ws w=1 1[Y s b,h,w = c] ,<label>(1)</label></formula><p>where f s b,h,w ∈ R is the source feature vectors, B s is the batch size, and H s , W s is the height and width of the features. c is the index of class number C. The maximum of C is 1. The prototypes can represent the class knowledge in the source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-and Intra-class Constraints.</head><p>To make the source prototypes represent the class-discriminative source knowledge more accurately, we incorporate inter-and intra-class constraints on the prototypes, which can further help better class-aware alignment across domains. The inter-class loss L s inter intends to push the prototypes of different classes far away from each other, which can be implemented by minimizing the average cosine distance of different prototype pairs:</p><formula xml:id="formula_3">L s inter = C i=0 C j&gt;i p s i p s j |p s i ||p s j | . (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>In contrast, the intra-class loss L s intra is designed to pull the feature instance point closer to its corresponding prototype, i.e., making the feature distribution of the same class more concentrated/compact. The intra-class loss can be formulated as maximizing the average cosine distance between the prototype and the features belonging to the same class:</p><formula xml:id="formula_5">L s intra = 1 - C c=0 f s c p s c |f s c ||p s c | , (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>It is not straightforward to align target domain to source domain in the class level, considering the lack of groundtruth labels in target domain. To achieve more reliable class-aware alignment for target samples, we only perform alignment on the instances with higher confidence. Specifically, we first calculate the cosine distance between each target feature and all the source prototypes, and only select instances { f t } the distance of which is closer than a preset threshold τ . The intra-class alignment loss enforces f t c to be closer to its corresponding prototype p t c :</p><formula xml:id="formula_7">L t intra = 1 - C c=0 f t c p s c | f t c ||p s c | , (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>The class-aware alignment loss L align is the combination of these three losses, i.e., L align = L s intra + L s inter + L t intra . It is noteworthy that the alignment loss is optimized directly on the feature space instead of the final output predictions, considering there is more abundant information in the feature space.</p><p>Pseudo Supervision. The above mentioned alignment loss L align only affects partial target features with higher confidence. To further force the alignment across domains in the feature space, we incorporate a pseudo supervision on the feature space. Specifically, based on the cosine distance between the feature of each location and the source prototypes, we can attain a distance map P g , which can be regarded as a segmentation prediction directly from feature space instead of the prediction head. We utilize a pseudo label map P t2 as groundtruth to supervise the learning of P g , leading to alignment directly on feature space. The formulation of P t2 will be discussed in the later section. The supervision is the standard cross entropy loss:</p><formula xml:id="formula_9">L p = CE(P g , P t2 ).<label>(5)</label></formula><p>Intra-domain Consistency. The alignment cross domains will borrow the knowledge from source domain to target domain. However, there exists abundant knowledge and information in the target domain itself <ref type="bibr" target="#b17">[18]</ref>. To further exploit the sufficient knowledge existed in target domain, we propose an intra-domain consistency constraint in target domain. Specifically, for each target input image I t , we first augment it with two different random augmentation strategies, resulting in I t1 and I t2 , which are then fed into the network for segmentation prediction.</p><p>We incorporate two consistency losses on the feature level L cf and the final prediction level L cp , respectively:</p><formula xml:id="formula_10">L cf = MSE(f t1 , f t2 ), L cp = CE(P t1 , P t2 ),<label>(6)</label></formula><p>where MSE denotes the standard mean squared error loss.</p><p>Training and Inference. During the training phase, the total training objective L total is formulated as :</p><formula xml:id="formula_11">L total = L s seg + λ align L align + λ p L p + λ cf L cf + λ cp L cp ,<label>(7)</label></formula><p>where L s seg denotes the supervised segmentation loss with the cross-entropy loss and λ {align,p,cf,cp} are the hyperparameters for balancing different terms. Note Table <ref type="table">1</ref>. Quantitative comparisons on the Lucchi and MitoEM datasets. Oracle denotes the model is trained on target with groundtruth labels, while NoAdapt represents the model pretrained on source is directly applied in target for inference without any adaptation strategy. The results of Oracle, NoAdapt, UALR, DAMT-Net, DA-VSN and DA-ISC are adopted from <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>VNC III → Lucchi (Subset1) VNC III → Lucchi (Subset2) mAP(%) F1(%) MCC(%) IoU(%) mAP(%) F1(%) MCC(%) IoU(%) that the feature extractor and the segmentation head are shared weights in the training phase. Their detailed structures can be found in the supplementary material. During the inference phase, we only adopt the trained feature extractor and segmentation head to predict the target images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Oracle</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets. Following the previous work <ref type="bibr" target="#b6">[7]</ref>, our experiments involve four challenging EM datasets for domain adaptive mitochondria segmentation tasks, i.e., VNC III <ref type="bibr" target="#b4">[5]</ref>→ Lucchi (Subset1) <ref type="bibr" target="#b11">[12]</ref>, VNC III→ Lucchi (Subset2) <ref type="bibr" target="#b11">[12]</ref>, MitoEM-H <ref type="bibr" target="#b19">[20]</ref> → MitoEM-R <ref type="bibr" target="#b19">[20]</ref> and MitoEM-R → MitoEM-H. VNC III <ref type="bibr" target="#b4">[5]</ref> is imaged from the Drosophila ventral nerve cord by ssTEM. The physical resolution of the pixel is 50 × 5× 5 nm 3 . The dataset consists of 20 images, and their resolution is 1024 × 1024. Lucchi <ref type="bibr" target="#b11">[12]</ref> is imaged from the hippocampus of mice collected by FIB-SEM. The physical resolution of the pixel is 5×5×5 nm 3 , the training (Sub-set1) and test (Subset2) sets both have 165 images with 1024 × 768 resolution. MitoEM <ref type="bibr" target="#b19">[20]</ref> contains two image volumes imaged by mbSEM, one is from the  Evaluation Metrics. To thoroughly evaluate the performance of models, we conduct comparisons both on semantic-level and instance-level predictions. 1) Following <ref type="bibr" target="#b6">[7]</ref>, we compare different methods with mAP, F1, MCC and IoU scores on the 2D binary segmentation. 2) Considering that the quantity, size and morphology of mitochondria are pivotal to related studies, we further evaluate on the 3D instance segmentation task. Following <ref type="bibr" target="#b19">[20]</ref>, we take AP 50 and AP 75 as the metrics to quantitatively compare the performance of different methods.</p><p>Implementation Details. Our network architecture is following <ref type="bibr" target="#b6">[7]</ref>. We crop each image into 512 × 512 as patch to input feature extractor. All models are trained using the Adam optimizer with β 1 = 0.9 and β 2 = 0.999. The learning rate is set at 1e -4 and is polynomially decayed with a power of 0.9. We train models for 200k iterations in total. The balancing weights λ align , λ proto , λ cf , and λ cp in Eq. 7 are set as 0.1, 0.1, 0.1, and 0.1, respectively. The preset threshold τ is set as 0.7. To obtain 3D instance segmentation results, we adopt the markercontrolled watershed algorithm <ref type="bibr" target="#b19">[20]</ref> on the predicted binary predictions.</p><p>Comparisons with Baselines. The binary segmentation result comparisons of our proposed method with previous works on the Lucchi and MitoEM datasets are shown in Table <ref type="table">1</ref>. The competitors include UALR <ref type="bibr" target="#b20">[21]</ref>, DAMT-Net <ref type="bibr" target="#b15">[16]</ref>, Advent <ref type="bibr" target="#b18">[19]</ref>, DA-VSN <ref type="bibr" target="#b5">[6]</ref>, and DA-ISC <ref type="bibr" target="#b6">[7]</ref>. Our method achieves the new stateof-the-art results in all cases, which corroborates the superiority of the proposed class-aware alignment in the feature space. Especially, compared with the previous state-of-the-art DA-ISC <ref type="bibr" target="#b6">[7]</ref>, our method surpasses it by a large margin on the benchmarks of VNC III→ Lucchi (Subset1) (3.1% IoU). The mitochondria in MitoEM-H distribute more densely and are more complex than those in MitoEM-R, leading to the result of MitoEM-R → MitoEM-H is lower than that of MitoEM-H → MitoEM-R. However, our result on MitoEM-R → MitoEM-H has remarkable improvement, owing to that our method not only aligns domain in a fine-grained way but also explore the full potential of target. As shown in Table <ref type="table" target="#tab_1">2</ref>, we also evaluate the effectiveness of our proposed method on 3D instance segmentation results. We only conduct experiments on the MitoEM dataset due to the lack of groundtruth for 3D instance segmentation in Lucchi. Our method not only deals with the domain adaptation for binary segmentation but also behaves well for the harder 3D instance segmentation, where the latter has rarely been studied in the literature. Furthermore, to further evaluate the effectiveness of our method, we visualize the predicted segmentation of our method and baselines in Fig. <ref type="figure" target="#fig_1">2</ref>. Credited to the proposed class-aware feature alignment, our method estimates more fine-grained results on the target domain, substantiating that our method can alleviate the domain shift between source and target domains. In Fig. <ref type="figure" target="#fig_1">2</ref>, the yellow and orange boxes represent mitochondria and background, respectively. In R2H, only our method segments the mitochondria in the yellow box. This is because our method is able to extract more representative features of mitochondria and background, and can separate these two types of features more effectively.</p><p>Ablation Study for Loss Functions. We conduct thorough ablation experiments to validate the contribution of each loss term in Eq. 7, where the results are shown in Table <ref type="table" target="#tab_2">3</ref>. The experiment ① with only supervised segmentation loss L s seg is the Source-Only method. All other variants with our proposed losses have superior performance than ①. Specifically, with the proposed class-aware feature alignment (L align and L p ), ② improves ① by 6.6% IoU and ③ further enlarges the gain to 7.8%. To take advantage of the potential information in target domain with the intra-domain losses L cf and L cp , the final ⑤ improves by 3.6% IoU, leading to the 11.4% IoU improvement in total. To further explore the impact of different components in L align , i.e., L s intra , L s inter and L t intra , we conduct experiments and the results are shown in the supplementary material. We find that the cross-domain alignment term L t intra plays the core role. It is noteworthy that L s intra and L s inter help to learn better class-aware prototypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, for the first time, we propose the class-aware alignment for domain adaptation on mitochondria segmentation in the feature space. Based on the extracted source prototypes representing class knowledge, we design intradomain and inter-domain alignment constraint for fine-grained alignment cross domains. Furthermore, we incorporate an intra-domain consistency loss to take full advantage of the potential information existed in target domain. Comprehensive experiments demonstrate the effectiveness of our proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overall framework of our proposed method. (a) Images are first fed into the feature extractor to extract image features, which are then used to realize feature alignment by the class-aware alignment module and obtain the segmentation predictions by the segmentation (Seg.) head. We randomly perturb the target image Xt to obtain two augmented counterparts Xt 1 and Xt 2 with different augmentations. An intra-domain consistency constraint is incorporated in the feature level (L cf ) and the prediction level (Lcp). (b) In the Class-Aware Alignment Module, the source feature fs and its corresponding groundtruth label are used to extract the centroids/prototypes (i.e., p f and p b ) of each class. Based on the distance between target features ft 1 and the prototypes, we can obtain the distance map Pg, representing the segmentation prediction directly from the features, which is further supervised by the pseudo label Pt 2 (Lp). (c) The illustration of the class-aware alignment loss L align , where the features within the same class are pulled together and the features belonging to different classes are pushed away from each other.</figDesc><graphic coords="3,42,81,54,05,338,05,211,66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual comparisons of different domain adaptive mitochondria segmentation methods, from VNC III to Lucchi (Subset1), i.e., V2L1, and MitoEM-R to MitoEM-H, i.e., R2H. The pixels in green, red and blue denote the true-positive, false-negative and false-positive segmentation results respectively. More 3D instance segmentation visualizations can be found in the supplementary material. (Color figure online)</figDesc><graphic coords="7,50,31,207,95,323,80,96,94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparisons for the 3D instance segmentation results on the MitoEM dataset.</figDesc><table><row><cell>Methods</cell><cell cols="4">MitoEM-R → MitoEM-H MitoEM-H → MitoEM-R</cell></row><row><cell></cell><cell cols="2">AP 50 (%) AP 75 (%)</cell><cell cols="2">AP 50 (%) AP 75 (%)</cell></row><row><cell>Advent [19]</cell><cell>43.6</cell><cell>17.3</cell><cell>56.4</cell><cell>27.0</cell></row><row><cell>UALR [21]</cell><cell>56.4</cell><cell>29.1</cell><cell>55.4</cell><cell>33.6</cell></row><row><cell cols="2">DAMT-Net [16] 55.2</cell><cell>29.5</cell><cell>54.6</cell><cell>25.5</cell></row><row><cell>DA-VSN [6]</cell><cell>53.1</cell><cell>24.6</cell><cell>60.2</cell><cell>29.3</cell></row><row><cell>DA-ISC [7]</cell><cell>60.0</cell><cell>37.4</cell><cell>63.0</cell><cell>34.7</cell></row><row><cell>Ours</cell><cell>65.6</cell><cell>46.3</cell><cell>68.5</cell><cell>42.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation results for the effectiveness of each loss term.</figDesc><table><row><cell>MitoEM-R → MitoEM-H</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Settings L s seg L align Lp L cf Lcp mAP(%) F1(%) MCC(%) IoU(%)</cell></row><row><cell>①</cell><cell>87.2</cell><cell>78.7</cell><cell>78.1</cell><cell>64.9</cell></row><row><cell>②</cell><cell>88.8</cell><cell>83.3</cell><cell>82.8</cell><cell>71.5</cell></row><row><cell>③</cell><cell>91.1</cell><cell>84.2</cell><cell>83.6</cell><cell>72.7</cell></row><row><cell>④</cell><cell>91.5</cell><cell>84.2</cell><cell>85.4</cell><cell>75.4</cell></row><row><cell>⑤</cell><cell>92.8</cell><cell cols="2">86.6 86.0</cell><cell>76.3</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62076230</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pw2nUb6">
					<idno type="grant-number">62076230</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 23.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual correspondences for unsupervised domain adaptation on electron microscopy images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bermúdez-Chacón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Altingövde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1256" to="1267" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A domainadaptive two-stream u-net for electron microscopy image segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bermúdez-Chacón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Márquez-Neila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISBI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stable deep neural network architectures for mitochondria segmentation on electron microscopy volumes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Franco-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muñoz-Barrutia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Arganda-Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="437" to="450" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning based domain adaptation for mitochondria segmentation on EM volumes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Franco-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pastor-Tronch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>González-Marfil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muñoz-Barrutia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Arganda-Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">222</biblScope>
			<biblScope unit="page">106949</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Segmented anisotropic ssTEM dataset of neural tissue</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gerhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Funke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fetter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Figshare</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptive video segmentation via temporal consistency regularization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain adaptive mitochondria segmentation via enforcing inter-section consistency</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Prototypical contrast adaptation for domain adaptive semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19830-4_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19830-43" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13694</biblScope>
			<biblScope unit="page" from="36" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Advanced deep networks for 3d mitochondria instance segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Contrastive learning for mitochondria segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>EMBC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PDAM: a panoptic-level feature alignment framework for unsupervised domain adaptive instance segmentation in microscopy images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="154" to="165" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning for structured prediction using approximate subgradient descent with working sets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Taking a closer look at domain shift: category-level adversaries for semantics consistent domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2507" to="2516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic segmentation of mitochondria and endolysosomes in volumetric electron microscopy data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Ž</forename><surname>Mekuč</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bohak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hudoklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">103693</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computerized detection and segmentation of mitochondria on electron microscope images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mumcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hassanpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Perkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gurcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Microsc</title>
		<imprint>
			<biblScope unit="volume">246</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="248" to="265" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised mitochondria segmentation in EM images via domain adaptive multi-task learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Signal Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1199" to="1209" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain adaptive segmentation in volume electron microscopy imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Roels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hennies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Saeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kreshuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISBI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fixmatch: simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="596" to="608" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Advent: adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MitoEM dataset: large-scale 3D mitochondria instance segmentation from EM images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59722-1_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59722-17" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12265</biblScope>
			<biblScope unit="page" from="66" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Uncertainty-aware label rectification for domain adaptive mitochondria segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_18</idno>
		<idno>978-3-030-87199-4 18</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Cotin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Essert</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12414" to="12424" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
