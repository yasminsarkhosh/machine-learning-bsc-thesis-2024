<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Chenyu</forename><surname>You</surname></persName>
							<email>chenyu.you@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weicheng</forename><surname>Dai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Biomedical Imaging</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yifei</forename><surname>Min</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Statistics and Data Science</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lawrence</forename><surname>Staib</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Biomedical Imaging</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology and Biomedical Imaging</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Statistics and Data Science</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="561" to="571"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">417F56058DBD993D557AEEBDB32874A6</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_54</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Image Segmentation</term>
					<term>Implicit Neural Representation</term>
					<term>Stochastic Mixture-of-Experts</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Integrating high-level semantically correlated contents and low-level anatomical features is of central importance in medical image segmentation. Towards this end, recent deep learning-based medical segmentation methods have shown great promise in better modeling such information. However, convolution operators for medical segmentation typically operate on regular grids, which inherently blur the high-frequency regions, i.e., boundary regions. In this work, we propose MORSE, a generic implicit neural rendering framework designed at an anatomical level to assist learning in medical image segmentation. Our method is motivated by the fact that implicit neural representation has been shown to be more effective in fitting complex signals and solving computer graphics problems than discrete grid-based representation. The core of our approach is to formulate medical image segmentation as a rendering problem in an end-to-end manner. Specifically, we continuously align the coarse segmentation prediction with the ambiguous coordinatebased point representations and aggregate these features to adaptively refine the boundary region. To parallelly optimize multi-scale pixel-level features, we leverage the idea from Mixture-of-Expert (MoE) to design and train our MORSE with a stochastic gating mechanism. Our experiments demonstrate that MORSE can work well with different medical segmentation backbones, consistently achieving competitive performance improvements in both 2D and 3D supervised medical segmentation methods. We also theoretically analyze the superiority of MORSE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical image segmentation is one of the most fundamental and challenging tasks in medical image analysis. It aims at classifying each pixel in the image into an anatomical category. With the success of deep neural networks (DNNs), medical image segmentation has achieved great progress in assisting radiologists in contributing to a better disease diagnosis.</p><p>Until recently, the field of medical image segmentation has mainly been dominated by an encoder-decoder architecture, and the existing state-of-the-art (SOTA) medical segmentation models are roughly categorized into two groups:</p><p>(1) convolutional neural networks (CNNs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b39">38]</ref>, and (2) Transformers <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b36">35]</ref>. However, despite their recent success, several challenges persist to build a robust medical segmentation model: ❶ Classical deep learning methods require precise pixel/voxel-level labels to tackle this problem <ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b31">[31]</ref><ref type="bibr" target="#b32">[32]</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b38">37]</ref>. Acquiring a large-scale medical dataset with exact pixel-and voxel-level annotations is usually expensive and time-consuming as it requires extensive clinical expertise <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b20">20]</ref>. Prior works <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b15">15]</ref> have used pointlevel supervision on medical image segmentation to refine the boundary prediction, where such supervision requires well-trained model weights and can only capture discrete representations on the pixel-level grids. ❷ Empirically, it has been observed that CNNs inherently store the discrete signal values in a grid of pixels or voxels, which naturally blur the high-frequency anatomical regions, i.e., boundary regions. In contrast, implicit neural representations (INRs), also known as coordinate-based neural representations, are capable of representing discrete data as instances of a continuous manifold, and have shown remarkable promise in computer vision and graphics <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28]</ref>. Several questions then arise: how many pixel-or voxel-level labels are needed to achieve good performance? how should those coordinate locations be selected? and how can the selected coordinates and signal values be leveraged efficiently?</p><p>Orthogonally to the popular belief that the model architecture matters the most in medical segmentation (i.e., complex architectures generally perform better), this paper focuses on an under-explored and alternative direction: towards improving segmentation quality via rectifying uncertain coarse predictions. To this end, we propose a new INR-based framework, MORSE (iMplicit anatOmical Rendering with Stochastic Experts). The core of our approach is to formulate medical image segmentation as a rendering problem in an end-to-end manner. We think of building a generic implicit neural rendering framework to have finegrained control of segmentation quality, i.e., to adaptively compose coordinatewise point features and rectify uncertain anatomical regions. Specifically, we encode the sampled coordinate-wise point features into a continuous space, and then align position and features with respect to the continuous coordinate.</p><p>We further hinge on the idea of mixture-of-experts (MoE) to improve segmentation quality. Considering our goal is to rectify uncertain coarse predictions, we regard multi-scale representations from the decoder as experts. During training, experts are randomly activated for features from multiple blocks of the decoder, and correspondingly the INRs of multi-scale representations are sepa- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Let us assume a supervised medical segmentation dataset D = {(x, y)}, where each input x = x 1 , x 2 , ..., x T is a collection of T 2D/3D scans, and y refers to the ground-truth labels. Given an input scan x ∈ R H×W ×d , the goal of medical segmentation is to predict a segmentation map ŷ. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the overview of our MORSE. In the following, we first describe our baseline model f for standard supervised learning, and subsequently present our MORSE. A baseline segmentation model consists of two main components: (1) encoder module, which generates the multi-scale feature maps such that the model is capable of modeling multi-scale local contexts, and (2) decoder module that makes a prediction ŷ using the generated multi-block features of different resolution. The entire model M is trained end-to-end using the supervised segmentation loss L sup <ref type="bibr" target="#b36">[35]</ref> (i.e., equal combination of cross-entropy loss and dice loss).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Stochastic Mixture-of-Experts (SMoE) Module</head><p>Motivation. We want a module that encourages inter-and intra-associations across multi-block features. Intuitively, multi-block features should be specified by anatomical features across each block. We posit that due to the specializationfavored nature of MoE, the model will benefit from explicit use of its own anatomical features at each block by learning multi-scale anatomical contexts with adaptively selected experts. In implementation, our SMoE module follows an MoE design <ref type="bibr" target="#b21">[21]</ref>, where it treats features from multiple blocks of the decoder as experts. To mitigate potential overfitting and enable parameter-efficient property, we further randomly activate experts for each input during training. Our approach makes three major departures compared to <ref type="bibr" target="#b21">[21]</ref> (i.e., SOTA segmention model): (1) implicitly optimized during training since it greatly trims down the training cost and the model scale; (2) using features from the decoder instead of the encoder tailored for our refinement goal; and (3) empirically showing that "self-slimmable" attribute delivers sufficiently exploited expressiveness of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modulization.</head><p>We first use multiple small MLPs with the same size to process different block features and then up-sample the features to the size of the input scans, i.e., H × W × d. With N as the total number of layers (experts) in the decoder, we treat these upsampled features [F 1 , F 2 , ..., F N ] as expert features. We then train a gating network G to re-weight the features from activated experts with the trainable weight matrices [W 1 , W 2 , ..., W N ], where W ∈ R H×W ×d . Specifically, the gating network or router G outputs these weight matrices satisfying i W i = 1 H×W ×d using a structure depicted as follows:</p><formula xml:id="formula_0">W i = [Softmax(Conv([F 1 , F 2 , ..., F N ]))] i , for i ∈ [N ].<label>(1)</label></formula><p>The gating network first concatenates all the expert features along channels and uses several convolutional layers to get</p><formula xml:id="formula_1">Conv([F 1 , F 2 , ..., F N ]) ∈ R C×H×W ×d×N ,</formula><p>where C is the channel dimension. A softmax layer is applied over the last dimension (i.e., N -expert) to output the final weight maps. After that, we feed the resultant output x out to another MLP to fuse multi-block expert features. Finally, the resultant output x out (i.e. the coarse feature) is given as follows:</p><formula xml:id="formula_2">x out = MLP( N i=1 W i • F i ),<label>(2)</label></formula><p>where • denotes the pixel-wise multiplication, and</p><formula xml:id="formula_3">x out ∈ R C×H×W ×d .</formula><p>Stochastic Routing. The prior MoE-based model <ref type="bibr" target="#b21">[21]</ref> are densely activated.</p><p>That is, a model needs to access all its parameters to process all inputs. One drawback of such design often comes at the prohibitive training cost. Moreover, the large model size suffers from the representation collapse issue <ref type="bibr" target="#b26">[26]</ref>, further limiting the model's performance. Our proposed SMoE considers randomly activated expert sub-networks to address the issues. In implementation, we simply apply standard dropout to multiple experts with a dropping probability α. For each training iteration, there are dropout masks placed on experts with the probability α. That is, the omission of experts follows a Bernoulli(α) distribution. As for inference, there is no dropout mask and all experts are activated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Implicit Anatomical Rendering (IAR)</head><p>The existing methods generally assume that the semantically correlated information and fine anatomical details have been captured and can be used to obtain high-quality segmentation quality. However, CNNs inherently operate the discrete signals in a grid of pixels or voxels, which naturally blur the high-frequency anatomical regions, i.e., boundary regions. To address such issues, INRs in computer graphics are often used to replace standard discrete representations with continuous functions parameterized by MLPs <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b28">28]</ref>. Our key motivation is that the task of medical segmentation is often framed as a rendering problem that applies implicit neural functions to continuous shape/object/scene representations <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b27">27]</ref>. Inspired by this, we propose an implicit neural rendering framework to further improve segmentation quality, i.e., to adaptively compose coordinatewise point features and rectify uncertain anatomical regions.</p><p>Point Selection. Given a coarse segmentation map, the rendering head aims at rectifying the uncertain boundary regions. A point selection mechanism is thus required to filter out those pixels where the rendering can achieve maximum segmentation quality improvement. Besides, point selection can significantly reduce computational cost compared to blindly rendering all boundary pixels. Therefore, our MORSE selects N p points for refinement given the coarse segmentation map using an uncertainty-based criterion. Specifically, MORSE first uniformly randomly samples k p N p candidates from all pixels where the hyper-parameter k p ≥ 1, following <ref type="bibr" target="#b9">[9]</ref>. Then, based on the coarse segmentation map, MORSE chooses ρN p pixels with the highest uncertainty from these candidates, where 0.5 &lt; ρ &lt; 1. The uncertainty for a pixel is defined as SecondLargest(v)-max(v), where v is the logit vector of that pixel such that the coarse segmentation is given as Softmax(v). The rest (1ρ)N p pixels are sampled uniformly from all the remaining pixels. This mechanism ensures the selected points contain a large portion of points with uncertain segmentation which require refinement.</p><p>Positional Encoding. It is well-known that neural networks can be cast as universal function approximators, but they are inferior to high-frequency signals due to their limited learning power <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b23">23]</ref>. Unlike <ref type="bibr" target="#b9">[9]</ref>, we explore using the encoded positional information to capture high-frequency signals, which echoes our theoretical findings in Appendix A. Specifically, for a coordinate-based point </p><formula xml:id="formula_4">(x, y) ∈ [H] × [W ],</formula><p>the positional encoding function is given as:</p><formula xml:id="formula_5">ψ(x, y) = [sin(2π(w 1 x + v 1 ỹ)), • • • , sin(2π(w L x + v L ỹ)), cos(2π(w 1 x + v 1 ỹ)), • • • , cos(2π(w L +v L ỹ))],<label>(3)</label></formula><p>where x = 2x/H -1 and ỹ = 2y/W -1 are the standardized coordinates with values in between [-1, 1]. The frequency {w i , v i } L i=1 are trainable parameters with Gaussian random initialization, where we set L = 128 <ref type="bibr" target="#b3">[3]</ref>. For each selected point, its position encoding will then be concatenated with the coarse features of that point (i.e., x out defined in Sect. 2.1), to output the fine-grained features.</p><p>Rendering Head. The fine-grained features are then fed to the rendering head whose goal is to rectify the uncertain predictions with respect to these selected points. Inspired by <ref type="bibr" target="#b9">[9]</ref>, the rendering head adopts 3-layer MLPs design. Since the rendering head is designed to rectify the class label of the selected points, it is trained using the standard cross-entropy loss L rend .</p><p>Adaptive Weight Adjustment. Instead of directly leveraging pre-trained weights, it is more desirable to train the model from scratch in an end-to-end way. For instance, we empirically observe that directly using coarse masks by pretrained weights to modify unclear anatomical regions might lead to suboptimal results (See Sect. 3.1). Thus, we propose to modify the importance of L rend as:</p><formula xml:id="formula_6">λ t = λ rend • 1{t &gt; T/2} • t -T /2 T , (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>where t is the index of the iteration, T denotes the total number of iterations, and 1{•} denotes the indicator function. Training Objective. As such, the model is trained in an end-to-end manner using total loss L total = L sup + λ t × L rend .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Dataset. We evaluate the models on two important medical segmentation tasks. (2) Liver segmentation: Multi-phasic MRI (MP-MRI) dataset is an in-house dataset including 20 patients, each including T1 weighted DCE-MRI images at three-time phases (i.e., pre-contrast, arterial, and venous). Here, our evaluation is conducted via 5-fold cross-validation on the 60 scans. For each fold, the training and testing data includes 48 and 12 cases, respectively.</p><p>Implementation Details. We use AdamW optimizer <ref type="bibr" target="#b17">[17]</ref> with an initial learning rate 5e -4 , and adopt a polynomial-decay learning rate schedule for both datasets. We train each model for 30K iterations. For Synapse, we adopt the input resolution as 256 × 256 and the batch size is 4. For MP-MRI, we randomly crop 96 × 96 × 96 patches and the batch size is 2. For SMoE, following <ref type="bibr" target="#b21">[21]</ref>, all the MLPs have hidden dimensions <ref type="bibr">[256,</ref><ref type="bibr">256]</ref> with ReLU activations, the dimension of expert features [F 1 , F 2 , ..., F N ] are 256. We empirically set α as 0.7. Following <ref type="bibr" target="#b9">[9]</ref>, N p is set as 2048, and 8192 for training and testing, respectively, and k p , ρ are 3, 0.75. We follow the same gating network design <ref type="bibr" target="#b21">[21]</ref>, which includes four 3 × 3 convolutional layers with channels [256, 256, 256, N] and ReLU activations.</p><p>λ rend are set to 0.1. We adopt four representative models, including UNet <ref type="bibr" target="#b25">[25]</ref>, TransUnet <ref type="bibr" target="#b2">[2]</ref>, 3D-UNet <ref type="bibr" target="#b4">[4]</ref>, UNETR <ref type="bibr" target="#b5">[5]</ref>. Specifically, we set N for UNet <ref type="bibr" target="#b25">[25]</ref>, TransUnet <ref type="bibr" target="#b2">[2]</ref>, 3D-UNet <ref type="bibr" target="#b4">[4]</ref>, UNETR <ref type="bibr" target="#b5">[5]</ref> with 5, 3, 3, 3, respectively. We also use Dice coefficient (DSC), Jaccard, 95% Hausdorff Distance (95HD), and Average Surface Distance (ASD) to evaluate 3D results. We conduct all experiments in the same environments with fixed random seeds (Hardware: Single NVIDIA RTX A6000 GPU; Software: PyTorch 1.12.1+cu116, and Python 3.9.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparison with State-of-the-Art Methods</head><p>We adopt classical CNN-and transformer-based models, i.e., 2D-based {UNet <ref type="bibr" target="#b25">[25]</ref>, TransUnet <ref type="bibr" target="#b2">[2]</ref>} and 3D-based {3D-UNet <ref type="bibr" target="#b4">[4]</ref>, UNETR <ref type="bibr" target="#b5">[5]</ref>}, and train them on {2D Synapse, 3D MP-MRI} in an end-to-end manner <ref type="foot" target="#foot_2">3</ref> .</p><p>Main Results. The results for 2D synapse multi-organ segmentation and 3D liver segmentation are shown in Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref>, respectively. Visualization of IAR Modules. To better understand the IAR module, we visualize the point features on the coarse prediction and refined prediction after the IAR module in Appendix Fig. <ref type="figure">4</ref>. As is shown, we can see that IAR help rectify the uncertain anatomical regions for improving segmentation quality (Table <ref type="table" target="#tab_4">4</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>We first investigate our MORSE equipped with UNet by varying α (i.e., stochastic rate) and N (i.e., experts) on Synapse. The comparison results of α and N are reported in Table <ref type="table" target="#tab_3">3</ref>. We find that using α = 0.7 performs the best when the expert capacity is N = 5. Similarly, when reducing the expert number, the performance also drops considerably. This shows our hyperparameter settings are optimal. Moreover, we conduct experiments to study the importance of Adaptive Weight Adjustment (AWA). We see that: (1) Disabling AWA and training L rend from scratch causes unsatisfied performance, as echoed in <ref type="bibr" target="#b9">[9]</ref>. <ref type="bibr" target="#b2">(2)</ref> Introducing AWA shows a consistent advantage compared to the other. This demonstrates the importance of the Adaptive Weight Adjustment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed MORSE, a new implicit neural rendering framework that has fine-grained control of segmentation quality by adaptively composing coordinate-wise point features and rectifying uncertain anatomical regions. We also demonstrate the advantage of leveraging mixture-of-experts that enables the model with better specialization of features maps for improving the performance. Extensive empirical studies across various network backbones and datasets, consistently show the effectiveness of the proposed MORSE. Theoretical analysis further uncovers the expressiveness of our INR-based model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the MORSE pipeline.</figDesc><graphic coords="3,88,98,53,78,274,96,143,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 1 )</head><label>1</label><figDesc>Synapse multi-organ segmentation1 : Synapse multi-organ segmentation dataset contains 30 abdominal CT scans with 3779 axial contrastenhanced abdominal clinical CT images in total. Each volume scan has variable volume sizes 512 × 512 × 85 ∼ 512 × 512 × 198 with a voxel spatial resolution of ([0.54 ∼ 0.54] × [0.98 ∼ 0.98] × [2.5 ∼ 5.0]) mm 3 . For a fair comparison, the data split 2 is fixed with 18 (2211 axial slices) and 12 patients' scans for training and testing, respectively. The entire dataset has a high diversity of aorta, gallbladder, spleen, left kidney, right kidney, liver, pancreas, spleen, and stomach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparisons for multi-organ segmentation on the Synapse multiorgan CT dataset. The best results are indicated in bold.</figDesc><table><row><cell>Method</cell><cell>Average</cell><cell></cell><cell></cell><cell cols="5">Aorta Gallbladder Kidney (L) Kidney (R) Liver Pancreas Spleen Stomach</cell></row><row><cell></cell><cell cols="3">DSC ↑ Jaccard ↑ 95HD ↓ ASD ↓</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UNet (Baseline) [25]</cell><cell>70.11 59.39</cell><cell>44.69</cell><cell cols="2">14.41 84.00 56.70</cell><cell>72.41</cell><cell>62.64</cell><cell>86.98 48.73</cell><cell>81.48 67.96</cell></row><row><cell>+ PointRend [9]</cell><cell>71.52 61.34</cell><cell>43.19</cell><cell cols="2">13.70 85.74 57.14</cell><cell>75.42</cell><cell>63.27</cell><cell>87.32 50.16</cell><cell>81.82 71.29</cell></row><row><cell cols="2">+ Implicit PointRend [3] 67.33 59.73</cell><cell>52.44</cell><cell cols="2">22.15 76.32 51.99</cell><cell>70.28</cell><cell>70.36</cell><cell>81.69 43.77</cell><cell>77.18 67.05</cell></row><row><cell>+ Ours (MoE)</cell><cell>72.83 62.64</cell><cell>40.44</cell><cell cols="2">13.15 86.11 59.51</cell><cell>75.81</cell><cell>67.10</cell><cell>87.82 52.11</cell><cell>83.48 70.86</cell></row><row><cell>+ Ours (SMoE)</cell><cell>74.86 64.94</cell><cell>37.69</cell><cell cols="2">12.66 86.39 63.99</cell><cell>77.96</cell><cell>68.93</cell><cell>88.88 53.62</cell><cell>86.12 72.98</cell></row><row><cell>+ Ours (IAR)</cell><cell>73.11 62.98</cell><cell>34.01</cell><cell cols="2">12.67 86.28 60.25</cell><cell>76.58</cell><cell>65.34</cell><cell>88.32 52.12</cell><cell>83.47 72.51</cell></row><row><cell>+ Ours (IAR+MoE)</cell><cell>75.37 65.65</cell><cell>33.34</cell><cell cols="2">11.43 87.00 64.45</cell><cell>78.14</cell><cell>70.13</cell><cell>89.32 52.33</cell><cell>85.20 76.40</cell></row><row><cell>+ Ours (MORSE)</cell><cell>76.59 66.97</cell><cell cols="3">32.00 10.67 87.28 64.73</cell><cell>80.58</cell><cell>71.87</cell><cell>90.04 54.60</cell><cell>86.67 76.93</cell></row><row><cell cols="2">TransUnet (Baseline) [2] 77.49 64.78</cell><cell>31.69</cell><cell>8.46</cell><cell>87.23 63.13</cell><cell>81.87</cell><cell>77.02</cell><cell>94.08 55.86</cell><cell>85.08 75.62</cell></row><row><cell>+ PointRend [9]</cell><cell>78.30 65.88</cell><cell>34.17</cell><cell>8.62</cell><cell>87.93 63.96</cell><cell>83.47</cell><cell>77.23</cell><cell>94.86 56.45</cell><cell>85.76 76.75</cell></row><row><cell cols="2">+ Implicit PointRend [3] 71.92 60.62</cell><cell>41.42</cell><cell cols="2">18.55 78.39 61.64</cell><cell>79.59</cell><cell>73.20</cell><cell>89.61 50.01</cell><cell>80.17 62.75</cell></row><row><cell>+ Ours (MoE)</cell><cell>77.85 65.30</cell><cell>32.75</cell><cell>7.90</cell><cell>87.40 63.46</cell><cell>82.34</cell><cell>77.88</cell><cell>94.14 56.12</cell><cell>85.24 76.25</cell></row><row><cell>+ Ours (SMoE)</cell><cell>78.68 65.98</cell><cell>31.86</cell><cell>7.00</cell><cell>87.60 66.21</cell><cell>82.62</cell><cell>78.12</cell><cell>94.88 57.59</cell><cell>85.97 76.48</cell></row><row><cell>+ Ours (IAR)</cell><cell>79.37 66.50</cell><cell>30.13</cell><cell>7.25</cell><cell>88.63 66.76</cell><cell>83.70</cell><cell>79.50</cell><cell>95.26 57.10</cell><cell>86.90 77.10</cell></row><row><cell>+ Ours (IAR+MoE)</cell><cell>79.60 66.99</cell><cell>27.59</cell><cell>6.54</cell><cell>88.73 66.83</cell><cell>83.85</cell><cell>80.19</cell><cell>95.98 57.12</cell><cell>86.92 77.21</cell></row><row><cell>+ Ours (MORSE)</cell><cell>80.85 68.53</cell><cell cols="3">26.61 6.46 88.92 67.53</cell><cell>84.83</cell><cell>81.68</cell><cell>96.83 59.70</cell><cell>87.73 79.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparisons for liver segmentation on the Multi-phasic MRI dataset. The best results are indicated in bold.</figDesc><table><row><cell>Method</cell><cell>Average</cell><cell></cell><cell></cell><cell>Method</cell><cell>Average</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">DSC ↑ Jaccard ↑ 95HD ↓ ASD ↓</cell><cell></cell><cell cols="3">DSC ↑ Jaccard ↑ 95HD ↓ ASD ↓</cell></row><row><cell>3D-UNet (Baseline) [4]</cell><cell>89.19 81.21</cell><cell>34.97</cell><cell cols="2">10.63 UNETR (Baseline) [5]</cell><cell>89.95 82.17</cell><cell>24.64</cell><cell>6.04</cell></row><row><cell>+ PointRend [9]</cell><cell>89.55 81.80</cell><cell>30.88</cell><cell cols="2">10.12 + PointRend [9]</cell><cell>90.49 82.36</cell><cell>21.06</cell><cell>5.59</cell></row><row><cell cols="2">+ Implicit PointRend [3] 88.01 79.83</cell><cell>37.55</cell><cell cols="3">12.86 + Implicit PointRend [3] 88.72 80.18</cell><cell>26.63</cell><cell>10.58</cell></row><row><cell>+ Ours (MoE)</cell><cell>89.81 82.06</cell><cell>29.96</cell><cell cols="2">10.15 + Ours (MoE)</cell><cell>90.70 82.80</cell><cell>15.31</cell><cell>5.93</cell></row><row><cell>+ Ours (SMoE)</cell><cell>90.16 82.28</cell><cell>28.36</cell><cell>9.79</cell><cell>+ Ours (SMoE)</cell><cell>91.02 83.29</cell><cell>15.12</cell><cell>5.64</cell></row><row><cell>+ Ours (IAR)</cell><cell>91.22 83.30</cell><cell>27.84</cell><cell>8.89</cell><cell>+ Ours (IAR)</cell><cell>91.63 83.83</cell><cell>14.25</cell><cell>4.99</cell></row><row><cell>+ Ours (IAR+MoE)</cell><cell>92.77 83.94</cell><cell>26.57</cell><cell>7.51</cell><cell>+ Ours (IAR+MoE)</cell><cell>93.01 84.70</cell><cell>13.29</cell><cell>4.84</cell></row><row><cell>+ Ours (MORSE)</cell><cell>93.59 84.62</cell><cell cols="3">19.61 6.57 + Ours (MORSE)</cell><cell>93.85 85.53</cell><cell cols="2">12.33 4.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The following observations can be drawn: (1) Our MORSE demonstrates superior performance compared to all other training algorithms. Specifically, Compared to UNet, Tran-sUnet, 3D-UNet, and UNETR baselines, our MORSE with all experts selected obtains 3.36%∼6.48% improvements in Dice across two segmentation tasks. It validates the superiority of our proposed MORSE.<ref type="bibr" target="#b2">(2)</ref> The stochastic routing policy shows consistent performance benefits across all four network backbones on 2D and 3D settings. Specifically, we can observe that our SMoE framework improves all the baselines, which is within expectation since our model is implic-</figDesc><table><row><cell>itly "optimized" given evolved features. (3) As is shown, we can observe that</cell></row><row><cell>IAR consistently outperforms PointRend across all the baselines (i.e., UNet,</cell></row><row><cell>TransUnet, 3D-UNet, and UNETR) and obtain {1.59%, 1.07%, 2.03%, 1.14%}</cell></row><row><cell>performance boosts on two segmentation tasks, highlighting the effectiveness of</cell></row><row><cell>our proposal in INRs. (4) With Implicit PointRend [3] equipped, all the models'</cell></row><row><cell>performances drop. We find: adding Implicit PointRend leads to significant per-</cell></row><row><cell>formance drops of -2.78%, -5.57%, -1.18%, and -1.23% improvements, compared</cell></row><row><cell>with the SOTA baselines (i.e., UNet, TransUnet, 3D-UNet, and UNETR) on two</cell></row><row><cell>segmentation tasks, respectively. Importantly, we find that: [3] utilizes INRs for</cell></row><row><cell>producing different parameters of the point head for each object with point-level</cell></row><row><cell>supervision. As this implicit function does not directly optimize the anatomical</cell></row><row><cell>regions, we attribute this drop to the introduction of additional noise during</cell></row><row><cell>training, which leads to the representation collapse. This further verifies the</cell></row><row><cell>effectiveness of our proposed IAR. In Appendix Figs. 2 and 3, we provide visual</cell></row><row><cell>comparisons from various models. We can observe that MORSE yields sharper</cell></row><row><cell>and more accurate boundary predictions compared to all the other training</cell></row><row><cell>algorithms.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Effect of stochastic rate α and expert number N .</figDesc><table><row><cell>α</cell><cell cols="3">DSC [%]↑ ASD [voxel ]↓ N</cell><cell cols="2">DSC [%]↑ ASD[voxel ]↓</cell></row><row><cell cols="2">0.1 75.41</cell><cell>11.96</cell><cell cols="2">1 (No MoE) 75.11</cell><cell>11.67</cell></row><row><cell cols="2">0.2 75.68</cell><cell>11.99</cell><cell>2</cell><cell>75.63</cell><cell>11.49</cell></row><row><cell cols="2">0.5 76.06</cell><cell>10.43</cell><cell>3</cell><cell>75.82</cell><cell>11.34</cell></row><row><cell cols="2">0.7 76.59</cell><cell>10.67</cell><cell>4</cell><cell>76.16</cell><cell>11.06</cell></row><row><cell cols="2">0.9 74.16</cell><cell>11.32</cell><cell>5</cell><cell>76.59</cell><cell>10.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Ablation studies of the Adaptive Weight Adjustment (AWA).</figDesc><table><row><cell>Method</cell><cell cols="2">DSC [%]↑ ASD [voxel]↓</cell></row><row><cell cols="2">w/o AWA &amp; train w/ L rend from scratch 70.56</cell><cell>14.89</cell></row><row><cell>w/o AWA &amp; train w/ L rend in T 2</cell><cell>75.42</cell><cell>12.00</cell></row><row><cell>w/ AWA</cell><cell>76.59</cell><cell>10.67</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.synapse.org/#!Synapse:syn3193805/wiki/217789.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/Beckschen/TransUNet/tree/main/lists/lists Synapse.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>All comparison experiments are using their released code.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 54.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Realistic adversarial data augmentation for MR image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="667" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_65</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-865" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Transunet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pointly-supervised instance segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unetr: transformers for 3d medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interpretable minority synthesis for imbalanced classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Tzeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boundary-rendering network for breast lesion segmentation in ultrasound images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">102478</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural tangent kernel: convergence and generalization in neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hongler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pointrend: image segmentation as rendering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Brainsec: automated brain tissue segmentation pipeline for scalable neuropathological analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="49064" to="49079" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sar: self-adaptive refinement on pseudo labels for multiclass-imbalanced semi-supervised learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Chuah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4091" to="4100" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Smoothed adaptive weighting for imbalanced semi-supervised learning: improve reliability against unknown distribution data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C S</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Chuah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11828" to="11843" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A semisupervised learning for segmentation of gigapixel histopathology images from brain tissues</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Dugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Chuah</surname></persName>
		</author>
		<editor>EMBC. IEEE</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Joint semi-supervised and active learning for segmentation of gigapixel pathology images with cost-effective labeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Dugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Chuah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contrastive rendering for ultrasound image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_54</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-054" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="563" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Cascade variational auto-encoder for hierarchical disentanglement</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Tzeng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ACM CIKM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nerf: representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Attention u-net: learning where to look for the pancreas</title>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalizable semi-supervised learning strategies for multiple learning tasks using 1-d biomedical signals</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Siefkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Chuah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS 2022 Workshop on Learning from Time Series for Health</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Patcher: patch transformers with mixture of experts for precise medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-946" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="475" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deepsdf: learning continuous signed distance functions for shape representation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On the spectral bias of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rahaman</surname></persName>
		</author>
		<editor>ICML. PMLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: the sparsely-gated mixtureof-experts layer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Implicit neural representations with periodic activation functions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fourier features let networks learn high frequency functions in low dimensional domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Segan: adversarial network with multi-scale l 1 loss for medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mine your own anatomy: revisiting medical image segmentation with extremely limited labels</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.13476</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Rethinking semi-supervised medical image segmentation: a variancereduction perspective</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.01735</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Action++: improving semi-supervised medical image segmentation with adaptive anatomical contrast</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sekhon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.02689</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bootstrapping semi-supervised medical image segmentation with anatomical-aware contrastive distillation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPMI</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised wasserstein distance guided domain adaptation for 3d multi-domain liver segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IMIMIC/MIL3ID/LABELS -2020</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Cardoso</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12446</biblScope>
			<biblScope unit="page" from="155" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-61166-8_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-61166-817" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Class-aware generative adversarial transformers for medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Momentum contrastive voxel-wise representation learning for semi-supervised volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="639" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simcvd: simple contrastive voxel-wise representation distillation for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Staib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2228" to="2237" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">UNet++: a nested u-net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<editor>Stoyanov, D., et al.</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<idno>DLMIA/ML-CDS -2018</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-51" />
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
