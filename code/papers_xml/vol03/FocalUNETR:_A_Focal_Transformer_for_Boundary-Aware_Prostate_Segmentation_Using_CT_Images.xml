<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chengyin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Wayne State University</orgName>
								<address>
									<settlement>Detroit</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yao</forename><surname>Qiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Wayne State University</orgName>
								<address>
									<settlement>Detroit</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rafi</forename><forename type="middle">Ibn</forename><surname>Sultan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Wayne State University</orgName>
								<address>
									<settlement>Detroit</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hassan</forename><surname>Bagher-Ebadian</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiation Oncology</orgName>
								<orgName type="institution">Henry Ford Cancer Institute</orgName>
								<address>
									<settlement>Detroit</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prashant</forename><surname>Khanduri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Wayne State University</orgName>
								<address>
									<settlement>Detroit</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Indrin</forename><forename type="middle">J</forename><surname>Chetty</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiation Oncology</orgName>
								<orgName type="institution">Henry Ford Cancer Institute</orgName>
								<address>
									<settlement>Detroit</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dongxiao</forename><surname>Zhu</surname></persName>
							<email>dzhu@wayne.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Wayne State University</orgName>
								<address>
									<settlement>Detroit</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F028949935E17A243355298E89A89BA7</idno>
					<idno type="DOI">10.1007/978-3-031-43898-157.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Focal transformer</term>
					<term>Prostate segmentation</term>
					<term>Computed tomography</term>
					<term>Boundary-aware</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computed Tomography (CT) based precise prostate segmentation for treatment planning is challenging due to (1) the unclear boundary of the prostate derived from CT's poor soft tissue contrast and (2) the limitation of convolutional neural network-based models in capturing long-range global context. Here we propose a novel focal transformer-based image segmentation architecture to effectively and efficiently extract local visual features and global context from CT images. Additionally, we design an auxiliary boundary-induced label regression task coupled with the main prostate segmentation task to address the unclear boundary issue in CT images. We demonstrate that this design significantly improves the quality of the CT-based prostate segmentation task over other competing methods, resulting in substantially improved performance, i.e., higher Dice Similarity Coefficient, lower Hausdorff Distance, and Average Symmetric Surface Distance, on both private and public CT image datasets. Our code is available at this link.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Prostate cancer is a leading cause of cancer-related deaths in adult males, as reported in studies, such as <ref type="bibr" target="#b16">[17]</ref>. A common treatment option for prostate cancer is external beam radiation therapy (EBRT) <ref type="bibr" target="#b3">[4]</ref>, where CT scanning is a cost-effective tool for the treatment planning process compared with the more expensive magnetic resonance imaging (MRI). As a result, precise prostate segmentation in CT images becomes a crucial step, as it helps to ensure that the radiation doses are delivered effectively to the tumor tissues while minimizing harm to the surrounding healthy tissues.</p><p>Due to the relatively low spatial resolution and soft tissue contrast in CT images compared to MRI images, manual prostate segmentation in CT images can be time-consuming and may result in significant variations between operators <ref type="bibr" target="#b9">[10]</ref>. Several automated segmentation methods have been proposed to alleviate these issues, especially the fully convolutional networks (FCN) based U-Net <ref type="bibr" target="#b18">[19]</ref> (an encoder-decoder architecture with skip connections to preserve details and extract local visual features) and its variants <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>. Despite good progress, these methods often have limitations in capturing long-range relationships and global context information <ref type="bibr" target="#b1">[2]</ref> due to the inherent bias of convolutional operations. Researchers naturally turn to ViT <ref type="bibr" target="#b4">[5]</ref>, powered with self-attention (SA), for more possibilities: TransUNet first <ref type="bibr" target="#b1">[2]</ref> adapts ViT to medical image segmentation tasks by connecting several layers of the transformer module (multi-head SA) to the FCN-based encoder for better capturing the global context information from the high-level feature maps. TransFuse <ref type="bibr" target="#b24">[25]</ref> and MedT <ref type="bibr" target="#b20">[21]</ref> use a combined FCN and Transformer architecture with two branches to capture global dependency and low-level spatial details more effectively. Swin-UNet <ref type="bibr" target="#b0">[1]</ref> is the first U-shaped network based purely on more efficient Swin Transformers <ref type="bibr" target="#b11">[12]</ref> and outperforms models with FCN-based methods. UNETR <ref type="bibr" target="#b5">[6]</ref> and SiwnUNETR <ref type="bibr" target="#b19">[20]</ref> are Transformer architectures extended for 3D inputs.</p><p>In spite of the improved performance for the aforementioned ViT-based networks, these methods utilize the standard or shifted-window-based SA, which is the fine-grained local SA and may overlook the local and global interactions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>. As reported by <ref type="bibr" target="#b19">[20]</ref>, even pre-trained with a massive amount of medical data using self-supervised learning, the performance of prostate segmentation task using high-resolution and better soft tissue contrast MRI images has not been completely satisfactory, not to mention the lower-quality CT images. Additionally, the unclear boundary of the prostate in CT images derived from the low soft tissue contrast is not properly addressed <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Recently, Focal Transformer <ref type="bibr" target="#b23">[24]</ref> is proposed for general computer vision tasks, in which focal self-attention is leveraged to incorporate both fine-grained local and coarse-grained global interactions. Each token attends its closest surrounding tokens with fine granularity, and the tokens far away with coarse granularity; thus, focal SA can capture both short-and long-range visual dependencies efficiently and effectively. Inspired by this work, we propose the FocalUNETR (Focal U-NEt TRansformers), a novel focal transformer architecture for CTbased medical image segmentation (Fig. <ref type="figure" target="#fig_0">1A</ref>). Even though prior works such as Psi-Net <ref type="bibr" target="#b14">[15]</ref> incorporates additional decoders to enhance boundary detection and distance map estimation, they either lack the capacity for effective global context capture through FCN-based techniques or overlook the significance of considering the randomness of the boundary, particularly in poor soft tissue contrast CT images for prostate segmentation. In contrast, our approach utilizes a multi-task learning strategy that leverages a Gaussian kernel over the boundary of the ground truth segmentation mask <ref type="bibr" target="#b10">[11]</ref> as an auxiliary boundary-aware contour regression task (Fig. <ref type="figure" target="#fig_0">1B</ref>). This serves as a regularization term for the main task of generating the segmentation mask. And the auxiliary task enhances the model's generalizability by addressing the challenge of unclear boundaries in low-contrast CT images. In this paper, we make several new contributions. First, we develop a novel focal transformer model (FocalUNETR) for CT-based prostate segmentation, which makes use of focal SA to hierarchically learn the feature maps accounting for both short-and long-range visual dependencies efficiently and effectively. Second, we also address the challenge of unclear boundaries specific to CT images by incorporating an auxiliary task of contour regression. Third, our methodology advances state-of-the-art performance via extensive experiments on both realworld and benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">FocalUNETR</head><p>Our FocalUNETR architecture (Fig. <ref type="figure" target="#fig_0">1</ref>) follows a multi-scale design similar to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>, enabling us to obtain hierarchical feature maps at different stages. The input medical image X ∈ R C×H×W is first split into a sequence of tokens with dimension H H × W W , where H, W represent spatial height and width, respectively, and C represents the number of channels. These tokens are then projected into an embedding space of dimension D using a patch of resolution (H , W ). The SA is computed at two focal levels <ref type="bibr" target="#b23">[24]</ref>: fine-grained and coarse-grained, as illustrated in Fig. <ref type="figure" target="#fig_1">2A</ref>. The focal SA attends to fine-grained tokens locally, while summarized tokens are attended to globally (reducing computational cost). We perform focal SA at the window level, where a feature map of x ∈ R d×H ×W with spatial size H × W and d channels is partitioned into a grid of windows with size s w × s w . For each window, we extract its surroundings using focal SA.</p><p>For window-wise focal SA <ref type="bibr" target="#b23">[24]</ref>, there are three terms {L, s w , s r }. Focal level L is the number of granularity levels for which we extract the tokens for our focal SA. We present an example, depicted in Fig. <ref type="figure" target="#fig_1">2B</ref>, that illustrates the use of two focal levels (fine and coarse) for capturing the interaction of local and global context for optimal boundary-matching between the prediction and the ground truth for prostate segmentation. Focal window size s l w is the size of the sub-window on which we get the summarized tokens at level l ∈ {1, . . . , L}. Focal region size s l r is the number of sub-windows horizontally and vertically in attended regions at level l. The focal SA module proceeds in two main steps, subwindow pooling and attention computation. In the sub-window pooling step, an input feature map x ∈ R d×H ×W is split into a grid of sub-windows with size {s l w , s l w }, followed by a simple linear layer f l p to pool the sub-windows spatially. The pooled feature maps at different levels l provide rich information at both fine-grained and coarse-grained, where</p><formula xml:id="formula_0">x l = f l p (x) ∈ R d× H s l w × W s l w , and x = Reshape(x) ∈ R (d× H s l w × W s l w</formula><p>)×(s l w ×s l w ) . After obtaining the pooled feature maps</p><p>x l L 1 , we calculate the query at the first level and key and value for all levels using three linear projection layers f q , f k , and f v :</p><formula xml:id="formula_1">Q = f q (x 1 ), K = {K l } L 1 = f k ({x 1 , . . . , x L }), V = {V l } L 1 = f v ({x 1 , . . . , x L }).</formula><p>For the queries inside the i-th window Q i ∈ R d×sw×sw , we extract the s l r × s l r keys and values from K l and V l around the window where the query lies in and then gather the keys and values from all L to obtain</p><formula xml:id="formula_2">K i = {K 1 , . . . , K L } ∈ R s×d and V i = {V 1 , . . . , V L } ∈ R s×d , where s= L l=1 (s l r ) 2 .</formula><p>Finally, a relative position bias is added to compute the focal SA for</p><formula xml:id="formula_3">Q i by Attention(Q i , K i , V i ) = Softmax( Q i K T i √ d + B)V i ,</formula><p>where B = {B l } L 1 is the learnable relative position bias <ref type="bibr" target="#b23">[24]</ref>. The encoder utilizes a patch size of 2×2 with a feature dimension of 2×2×1 = 4 (i.e., a single input channel CT) and a D-dimensional embedding space. The overall architecture of the encoder comprises four stages of focal transformer blocks, with a patch merging layer applied between each stage to reduce the resolution by a factor of 2. We utilize an FCN-based decoder (Fig. <ref type="figure" target="#fig_0">1A</ref>) with skip connections to connect to the encoder at each resolution to construct a "Ushaped" architecture for our CT-based prostate segmentation task. The output of the encoder is concatenated with processed input volume features and fed into a residual block. A final 1 × 1 convolutional layer with a suitable activation function, such as Softmax, is applied to obtain the required number of class-based probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Auxiliary Task</head><p>For the main task of mask prediction (as illustrated in Fig. <ref type="figure" target="#fig_0">1A</ref>), a combination of Dice loss and Cross-Entropy loss is employed to evaluate the concordance of the predicted mask and the ground truth on a pixel-wise level. The objective function for the segmentation head is given by: L seg = L dice (p i , G) + L ce (p i , G), where pi represents the predicted probabilities from the main task and G represents the ground truth mask, both given an input image i. The predicted probabilities, pi , are derived from the main task through the application of the FocalUNETR model to the input CT image.</p><p>To address the challenge of unclear boundaries in CT-based prostate segmentation, an auxiliary task is introduced for the purpose of predicting boundaryaware contours to assist the main prostate segmentation task. This auxiliary task is achieved by attaching another convolution head after the extracted feature maps at the final stage (see Fig. <ref type="figure" target="#fig_0">1B</ref>). The boundary-aware contour, or the induced boundary-sensitive label, is generated by considering pixels near the boundary of the prostate mask. To do this, the contour points and their surrounding pixels are formulated into a Gaussian distribution using a kernel with a fixed standard deviation of σ (in this specific case, e.g., σ = 1.6) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>. The resulting contour is a heatmap in the form of a Heatsum function <ref type="bibr" target="#b10">[11]</ref>. We predict this heatmap with a regression task trained by minimizing mean-squared error instead of treating it as a single-pixel boundary segmentation problem. Given the ground truth of contour G C i , induced from the segmentation mask for input image i, and the reconstructed output probability pC i , we use the following loss function:</p><formula xml:id="formula_4">L reg = 1 N i ||p C i -G C i || 2</formula><p>where N is the total number of images for each batch. This auxiliary task is trained concurrently with the main segmentation task.</p><p>A multi-task learning approach is adopted to regularize the main segmentation task through the auxiliary boundary prediction task. The overall loss function is a combination of L seg and L reg : L tol = λ 1 L seg + λ 2 L reg , where λ 1 and λ 2 are hyper-parameters that weigh the contribution of the mask prediction loss and contour regression loss, respectively, to the overall loss. The optimal setting of λ 1 = λ 2 = 0.5 is determined by trying different settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Implementation Details</head><p>To evaluate our method, we use a large private dataset with 400 CT scans and a large public dataset with 300 CT scans (AMOS <ref type="bibr" target="#b8">[9]</ref>). As far as we know, the AMOS dataset is the only publicly available CT dataset including prostate ground truth. We randomly split the private dataset with 280 scans for training, 40 for validation, and 80 for testing. The AMOS dataset has 200 scans for training and 100 for testing <ref type="bibr" target="#b8">[9]</ref>. Although the AMOS dataset includes the prostate class, it mixes the prostate (in males) and the uterus (in females) into one single class labeled PRO/UTE. We filter out CT scans missing the PRO/UTE ground-truth segmentation.</p><p>Regarding the architecture, we follow the hyperparameter settings suggested in <ref type="bibr" target="#b23">[24]</ref>, with 2 focal levels, transformer blocks of depths <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2]</ref>, and head numbers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">32]</ref> for each of the four stages. We then create FocalUNETR-S and FocalUNETR-B with D as 48 and 64, respectively. These settings have 27.3 M and 48.3 M parameters, which are comparable to other state-of-the-art models in size.</p><p>For the implementation, we utilize a server equipped with 8 Nvidia A100 GPUs, each with 40 GB of memory. All experiments are conducted in PyTorch, and each model is trained on a single GPU. We interpolate all CT scans into an isotropic voxel spacing of [1.0 × 1.0 × 1.5] mm for both datasets. Houndsfield unit (HU) range of <ref type="bibr">[-50, 150</ref>] is used and normalized to [0, 1]. Subsequently, each CT scan is cropped to a 128 × 128 × 64 voxel patch around the prostate area, which is used as input for 3D models. For 2D models, we first slice each voxel patch in the axial direction into 64 slices of 128 × 128 images for training and stack them back for evaluation. For the private dataset, we train models for 200 epochs using the AdamW optimizer with an initial learning rate of 5e -4 . An exponential learning rate scheduler with a warmup of 5 epochs is applied to the optimizer. The batch size is set to 24 for 2D models and 1 for 3D models. We use random flip, rotation, and intensity scaling as augmentation transforms with probabilities of 0.1, 0.1, and 0.2, respectively. We also tried using 10% percent of AMOS training set as validation data to find a better training parameter setting and re-trained the model with the full training set. However, we did not get improved performance compared with directly applying the training parameters learned from tuning the private dataset. We report the Dice Similarity Coefficient (DSC, %), 95% percentile Hausdorff Distance (HD, mm), and Average Symmetric Surface Distance (ASSD, mm) metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments</head><p>Comparison with State-of-the-Art Methods. To demonstrate the effectiveness of FocalUNETR, we compare the CT-based prostate segmentation performance with three 2D U-Net-based methods: U-Net <ref type="bibr" target="#b18">[19]</ref>, UNet++ <ref type="bibr" target="#b25">[26]</ref>, and Attention U-Net (AttUNet) <ref type="bibr" target="#b15">[16]</ref>, two 2D transformer-based segmentation methods: TransUNet <ref type="bibr" target="#b1">[2]</ref> and Swin-UNet <ref type="bibr" target="#b0">[1]</ref>, two 3D U-Net-based methods: U-Net (3D) <ref type="bibr" target="#b2">[3]</ref> and V-Net <ref type="bibr" target="#b13">[14]</ref>, and two 3D transformer-based models: UNETR <ref type="bibr" target="#b5">[6]</ref> and SiwnUNETR <ref type="bibr" target="#b19">[20]</ref>. nnUNet <ref type="bibr" target="#b7">[8]</ref> is used for comparison as well. Both 2D and 3D models are included as there is no conclusive evidence for which type is better for this task <ref type="bibr" target="#b21">[22]</ref>. All methods (except nnUNet) follow the same settings as FocalUNETR and are trained from scratch. TransUNet and Swin-UNet are the only methods that are pre-trained on ImageNet. Detailed information regarding the number of parameters, FLOPs, and average inference time can be found in the supplementary materials.</p><p>Quantitative results are presented in Table <ref type="table" target="#tab_0">1</ref>, which shows that the proposed FocalUNETR, even without co-training, outperforms other FCN and Transformer baselines (2D and 3D) in both datasets for most of the metrics. The AMOS dataset mixes the prostate(males)/uterus(females, a relatively small portion). The morphology of the prostate and uterus is significantly different. Consequently, the models may struggle to provide accurate predictions for this specific portion of the uterus. Thus, the overall performance of FocalUNETR is overshadowed by this challenge, resulting in only moderate improvement over the baselines on the AMOS dataset. However, the performance margin significantly improves when using the real-world (private) dataset. When co-trained with the auxiliary contour regression task using the multi-task training strategy, the performance of FocalUNETRs is further improved. In summary, these observations indicate that incorporating FocalUNETR and multi-task training Qualitative results of several representative methods are visualized in Fig. <ref type="figure" target="#fig_2">3</ref>. The figure shows that our FocalUNETR-B and FocalUNETR-B* generate more accurate segmentation results that are more consistent with the ground truth than the results of the baseline models. All methods perform well for relatively easy cases (1 st row in Fig. <ref type="figure" target="#fig_2">3</ref>), but the FocalUNETRs outperform the other methods. For more challenging cases (rows 2-4 in Fig. <ref type="figure" target="#fig_2">3</ref>), such as unclear boundaries and mixed PRO/UTE labels, FocalUNETRs still perform better than other methods. Additionally, the FocalUNETRs are less likely to produce false positives (see more in supplementary materials) for CT images without a foreground ground truth, due to the focal SA mechanism that enables the model to capture global context and helps to identify the correct boundary and shape of the prostate. Overall, the FocalUNETRs demonstrate improved segmentation capabilities while preserving shapes more precisely, making them promising tools for clinical applications. Ablation Study. To better examine the efficacy of the auxiliary task for FocalUNETR, we selected different settings of λ 1 and λ 2 for the overall loss function L tol on the private dataset. The results (Table <ref type="table" target="#tab_2">2</ref>) indicate that as the value of λ 2 is gradually increased and that of λ 1 is correspondingly decreased (thereby increasing the relative importance of the auxiliary contour regression task), segmentation performance initially improves. However, as the ratio of contour information to segmentation mask information becomes too unbalanced, performance begins to decline. Thus, it can be inferred that the optimal setting for these parameters is when both λ 1 and λ 2 are set to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In summary, the proposed FocalUNETR architecture has demonstrated the ability to effectively capture local visual features and global contexts in CT images by utilizing the focal self-attention mechanism. The auxiliary contour regression task has also been shown to improve the segmentation performance for unclear boundary issues in low-contrast CT images. Extensive experiments on two large CT datasets have shown that the FocalUNETR outperforms state-ofthe-art methods for the prostate segmentation task. Future work includes the evaluation of other organs and extending the focal self-attention mechanism for 3D inputs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The architecture of FocalUNETR as (A) the main task for prostate segmentation and (B) a boundary-aware regression auxiliary task.</figDesc><graphic coords="3,42,30,54,38,339,52,146,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (A) The focal SA mechanism, and (B) an example of perfect boundary matching using focal SA for CT-based prostate segmentation task (lower panel), in which focal SA performs query-key interactions and query-value aggregations in both fine-and coarse-grained levels (upper panel).</figDesc><graphic coords="4,58,47,54,62,335,20,110,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Qualitative results on sample test CT images from the private (first two rows) and AMOS (last two rows) datasets</figDesc><graphic coords="8,56,46,54,35,339,88,160,36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative performance comparison on the private and AMOS datasets with a mean (standard deviation) for 3 runs with different seeds. An asterisk (*) denotes the model is co-trained with the auxiliary contour regression task. The best results with/without the auxiliary task are boldfaced or italicized, respectively.</figDesc><table><row><cell>Method</cell><cell>Private</cell><cell></cell><cell></cell><cell>AMOS</cell><cell></cell></row><row><cell></cell><cell>DSC ↑</cell><cell>HD ↓</cell><cell>ASSD ↓</cell><cell>DSC ↑</cell><cell>HD ↓</cell><cell>ASSD ↓</cell></row><row><cell>U-Net</cell><cell>85.22 (1.23)</cell><cell cols="3">6.71 (1.03) 2.42 (0.65) 83.42 (2.28)</cell><cell cols="2">8.51 (1.56) 2.79 (0.61)</cell></row><row><cell>UNet++</cell><cell>85.53 (1.61)</cell><cell cols="3">6.52 (1.13) 2.32 (0.58) 83.51 (2.31)</cell><cell cols="2">8.47 (1.62) 2.81 (0.57)</cell></row><row><cell>AttUNet</cell><cell>85.61 (0.98)</cell><cell cols="3">6.57 (0.96) 2.35 (0.72) 83.47 (2.34)</cell><cell cols="2">8.43 (1.85) 2.83 (0.59)</cell></row><row><cell>TransUNet</cell><cell>85.75 (2.01)</cell><cell cols="3">6.43 (1.28) 2.23 (0.67) 81.13 (3.03)</cell><cell cols="2">9.32 (1.87) 3.71 (0.79)</cell></row><row><cell>Swin-UNet</cell><cell>86.25 (1.69)</cell><cell cols="3">6.29 (1.31) 2.15 (0.51) 83.35 (2.46)</cell><cell cols="2">8.61 (1.82) 3.20 (0.64)</cell></row><row><cell>U-Net (3D)</cell><cell>85.42 (1.34)</cell><cell cols="3">6.73 (0.93) 2.36 (0.67) 83.25 (2.37)</cell><cell cols="2">8.43 (1.65) 2.86 (0.56)</cell></row><row><cell>V-Net (3D)</cell><cell>84.42 (1.21)</cell><cell cols="3">6.65 (1.17) 2.46 (0.61) 81.02 (3.11)</cell><cell cols="2">9.01 (1.93) 3.76 (0.82)</cell></row><row><cell>UNETR (3D)</cell><cell>82.21 (1.35)</cell><cell cols="3">7.25 (1.47) 2.64 (0.75) 81.09 (3.02)</cell><cell cols="2">8.91 (1.86) 3.62 (0.79)</cell></row><row><cell cols="2">SwinUNETR (3D) 84.93 (1.26)</cell><cell cols="3">6.85 (1.21) 2.48 (0.52) 83.32 (2.23)</cell><cell cols="2">8.63 (1.62) 3.21 (0.68)</cell></row><row><cell>nnUNet</cell><cell>85.86 (1.31)</cell><cell cols="3">6.43 (0.91) 2.09 (0.53) 83.56 (2.25)</cell><cell cols="2">8.36 (1.77) 2.65 (0.61)</cell></row><row><cell>FocalUNETR-S</cell><cell>86.53 (1.65)</cell><cell cols="3">5.95 (1.29) 2.13 (0.29) 82.21 (2.67)</cell><cell cols="2">8.73 (1.73) 3.46 (0.75)</cell></row><row><cell>FocalUNETR-B</cell><cell cols="6">87.73 (1.36) 5.61 (1.18) 2.04 (0.23) 83.61 (2.18) 8.32 (1.53) 2.76 (0.69)</cell></row><row><cell>FocalUNETR-S*</cell><cell>87.84 (1.32)</cell><cell cols="3">5.59 (1.23) 2.12 (0.31) 83.24 (2.52)</cell><cell cols="2">8.57 (1.70) 3.04 (0.67)</cell></row><row><cell cols="2">FocalUNETR-B* 89</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.23 (1.16) 4.85 (1.05) 1.81 (0.21) 83.79 (1.97) 8.31 (1.45)</head><label></label><figDesc>2.71 (0.62)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on different settings of total loss for FocalUNETR-B on the private dataset</figDesc><table><row><cell>L tol</cell><cell>Lseg</cell><cell cols="2">0.8Lseg + 0.2Lreg 0.5Lseg + 0.5Lreg 0.2Lseg + 0.8Lreg</cell></row><row><cell cols="3">DSC ↑ 87.73 ± 1.36 88.01 ± 1.38</cell><cell>89.23 ± 1.16</cell><cell>87.53 ± 2.13</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Swin-unet: unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05537</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Transunet: transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Biochemical outcome after radical prostatectomy, external beam radiation therapy, or interstitial radiation therapy for clinically localized prostate cancer</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>D'amico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jama</title>
		<imprint>
			<biblScope unit="volume">280</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="969" to="974" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An image is worth 16×16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unetr: transformers for 3d medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hatamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="574" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hf-unet: learning hierarchically inter-task relevance in multi-task u-net for accurate prostate segmentation in ct images</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2118" to="2128" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Automated design of deep learning methods for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jäger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08128</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Amos: a large-scale abdominal multi-organ benchmark for versatile medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08023</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An uncertainty-aware deep learning architecture with outlier mitigation for prostate gland segmentation in radiotherapy treatment planning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="311" to="322" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BSDA-Net: a boundary shape and distance aware joint learning framework for segmenting and classifying OCTA images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-37" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="65" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How distance transform maps boost segmentation cnns: an empirical study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<biblScope unit="page" from="479" to="492" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Psi-net: shape and boundary aware joint multi-task deep network for medical image segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Murugesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sarveswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Shankaranarayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sivaprakasam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<biblScope unit="page" from="7223" to="7226" />
			<date type="published" when="2019">2019. 2019</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note>41st Annual International Conference of the</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Attention u-net: learning where to look for the pancreas</title>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The impact of obesity towards prostate diseases</title>
		<author>
			<persName><forename type="first">D</forename><surname>Parikesit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Mochtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Umbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R A H</forename><surname>Hamid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prostate Int</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attcat: explaining transformers via attentive class activation tokens</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="5052" to="5064" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-supervised pre-training of swin transformers for 3d medical image analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20730" to="20740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Medical transformer: gated axial-attention for medical image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-24" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Boundary coding representation for organ segmentation in prostate cancer radiotherapy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="310" to="320" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weighted res-unet for high-quality retina vessel segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 9th International Conference on Information Technology in Medicine and Education (ITME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="327" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00641</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TransFuse: fusing transformers and CNNs for medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-22" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">UNet++: a nested U-net architecture for medical image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00889-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00889-51" />
	</analytic>
	<monogr>
		<title level="m">DLMIA/ML-CDS -2018</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11045</biblScope>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
