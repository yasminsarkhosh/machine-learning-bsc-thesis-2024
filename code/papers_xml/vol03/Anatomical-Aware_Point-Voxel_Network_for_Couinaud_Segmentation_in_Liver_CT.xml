<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT</title>
				<funder ref="#_YkHjtAC">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_gxdwtSJ">
					<orgName type="full">Clinical Research Plan of Shanghai Hospital Development Center</orgName>
				</funder>
				<funder ref="#_zXZRXXz #_tKhrk4T #_vz2E5ZT #_GTYtCwK">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xukun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sharib</forename><surname>Ali</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">University of Leeds</orgName>
								<address>
									<settlement>Leeds</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingyang</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minghao</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiming</forename><surname>Cui</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peixuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Changchun Boli Technologies Co., Ltd</orgName>
								<address>
									<settlement>Jilin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoying</forename><surname>Wang</surname></persName>
							<email>xiaoyingwang@fudan.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="department">Department of Liver Surgery</orgName>
								<orgName type="laboratory">Key Laboratory of Carcinogenesis and Cancer Invasion of Ministry of Education</orgName>
								<orgName type="institution" key="instit1">Liver Cancer Institute</orgName>
								<orgName type="institution" key="instit2">Zhongshan Hospital</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
							<email>lihuazhang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="465" to="474"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">2C7FD4A274A4F51B5468F19D28222AD0</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_45</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Couinaud Segmentation</term>
					<term>Point-Voxel Network</term>
					<term>Liver CT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurately segmenting the liver into anatomical segments is crucial for surgical planning and lesion monitoring in CT imaging. However, this is a challenging task as it is defined based on vessel structures, and there is no intensity contrast between adjacent segments in CT images. In this paper, we propose a novel point-voxel fusion framework to address this challenge. Specifically, we first segment the liver and vessels from the CT image, and generate 3D liver point clouds and voxel grids embedded with vessel structure prior. Then, we design a multi-scale point-voxel fusion network to capture the anatomical structure and semantic information of the liver and vessels, respectively, while also increasing important data access through vessel structure prior. Finally, the network outputs the classification of Couinaud segments in the continuous liver space, producing a more accurate and smooth 3D Couinaud segmentation mask. Our proposed method outperforms several state-of-the-art methods, both pointbased and voxel-based, as demonstrated by our experimental results on two public liver datasets. Code, datasets, and models are released at https://github.com/xukun-zhang/Couinaud-Segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Primary liver cancer is one of the most common and deadly cancer diseases in the world, and liver resection is a highly effective treatment <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b14">14]</ref>. The Couinaud segmentation <ref type="bibr" target="#b6">[7]</ref> based on CT images divides the liver into eight functionally independent regions, which intuitively display the positional relationship between Couinaud segments and intrahepatic lesions, and helps surgeons for make surgical planning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">13]</ref>. In clinics, Couinaud segments obtained from manual annotation are tedious and time-consuming, based on the vasculature used as rough guide (Fig. <ref type="figure" target="#fig_0">1</ref>). Thus, designing an automatic method to accurately segment Couinaud segments from CT images is greatly demanded and has attracted tremendous research attention.</p><p>However, automatic and accurate Couinaud segmentation from CT images is a challenging task. Since it is defined based on the anatomical structure of live vessels, even no intensity contrast (Fig. <ref type="figure" target="#fig_0">1</ref>.(b)) can be observed between different Couinaud segments, and the uncertainty of boundary (Fig. <ref type="figure" target="#fig_0">1.(d)</ref>) often greatly affect the segmentation performance. Previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b19">19]</ref> mainly rely on handcrafted features or atlas-based models, and often fail to robustly handle those regions with limited features, such as the boundary between adjacent Couinaud segments. Recently, with the advancement of deep learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b18">18]</ref>, many CNN-based algorithms perform supervised training through pixel-level Couinaud annotations to automatically obtain segmentation results <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">9,</ref><ref type="bibr" target="#b21">21]</ref>. Unfortunately, the CNN models treat all voxel-wise features in the CT image equally, cannot effectively capture key anatomical regions useful for Couinaud segmentation. In addition, all these methods deal with the 3D voxels of the liver directly without considering the spatial relationship of the different Couinaud segments, even if this relationship is very important in Couinaud segmentation. It can supplement the CNN-based method and improve the segmentation performance in regions without intensity contrast.</p><p>In this paper, to tackle the aforementioned challenges, we propose a pointvoxel fusion framework that represents the liver CT in continuous points to better learn the spatial structure, while performing the convolutions in voxels to obtain the complementary semantic information of the Couinaud segments. Specifically, the liver mask and vessel attention maps are first extracted from the CT images, which allows us to randomly sample points embedded with vessel structure prior in the liver space and voxelize them into a voxel grid. Subsequently, points and voxels pass through two branches to extract features. The point-based branch extracts the fine-grained feature of independent points and explores spatial topological relations. The voxel-based branch is composed of a series of convolutions to learn semantic features, followed by de-voxelization to convert them back to points. Through the operation of voxelization and devoxelization at different resolutions, the features extracted by these two branches can achieve multi-scale fusion on point-based representation, and finally output the Couinaud segment category of each point. Extensive experiments on two publicly available datasets named 3Dircadb <ref type="bibr" target="#b20">[20]</ref> and LiTS <ref type="bibr" target="#b1">[2]</ref> demonstrate that our proposed framework achieves state-of-the-art (SOTA) performance, outperforming cutting-edge methods quantitatively and qualitatively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The overview of our framework to segment Couinaud segments from CT images is shown in Fig. <ref type="figure" target="#fig_1">2</ref>, including the liver segmentation, vessel attention map generation, point data sampling and multi-scale point-voxel fusion network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Liver Mask and Vessel Attention Map Generation</head><p>Liver segmentation is a fundamental step in Couinaud segmentation task. Considering that the liver is large and easy to identify in the abdominal organs, we extracted the liver mask through a trained 3D UNet <ref type="bibr" target="#b5">[6]</ref>. Different from liver segmentation, since we aim to use the vessel structure as a rough guide to improving the performance of the Couinaud segmentation, we employ another 3D UNet <ref type="bibr" target="#b5">[6]</ref> to generate the vessel attention map more easily. Specifically, given a 3D CT image containing only the area covered by the liver mask (L), the 3D UNet <ref type="bibr" target="#b5">[6]</ref> output a binary vessel mask (M ). A morphological dilation is then used to enclose more vessel pixels in the M -covered area, generating a vessel attention map (M ). We employ the BCE loss to supervise the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Couinaud Segmentation</head><p>Based on the above work, we first use the M and the L to sample get point data, which can convert into a voxel grid through re-voxelization. The converted voxel grid embeds the vessel prior and also dilutes the liver parenchyma information. Inspired by <ref type="bibr" target="#b12">[12]</ref>, a novel multi-scale point-voxel fusion network then is proposed to simultaneously process point and voxel data through point-based branch and voxel-based branch, respectively, aiming to accurately perform Couinaud segmentation. The details of this part of our method are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous Spatial Point Sampling Based on the Vessel Attention</head><p>Map. In order to obtain the topological relationship between Couinaud segments, a direct strategy is to sample the coordinate point data with 3D spatial information from liver CT and perform point-wise classification. Hence, we first convert the image coordinate points I = i 1 , i 2 , ..., i t , i t ∈ R 3 in liver CT into the world coordinate points P = p 1 , p 2 , ..., p t , p t ∈ R 3 :</p><formula xml:id="formula_0">P = I * Spacing * Direction + Origin, (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where Spacing represents the voxel spacing in the CT images, Direction represents the direction of the scan, and Origin represents the world coordinates of the image origin. Based on equation(1), we obtain the world coordinate p t = (x t , y t , z t ) corresponding to each point i t in the liver space. However, directly feeding the transformed point data as input into the point-based branch undoubtedly ignores the vessel structure, which is crucial for Couinaud segmentation. </p><formula xml:id="formula_2">O t = O t (R(x t + Δx), R(y t + Δy), R(z t + Δz)) ∈ {0, 1, ..., 7},<label>(2)</label></formula><p>where R denotes the rounding integer function. Based on this, we achieve arbitrary resolution sampling in the continuous space covered by the M . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re</head><formula xml:id="formula_3">V u,v,w,c = n t=1 I[R(x t * r)=u,R(ŷ t * r)=v,R(ẑ t * r)=w] * f t,c N u,v,w ,<label>(3)</label></formula><p>where r denotes the voxel resolution, I [•] is the binary indicator of whether the coordinate pt belongs to the voxel grid (u, v, w), f t,c denotes the cth channel feature corresponding to pt , and N u,v,w is the number of points that fall in that voxel grid. Note that the re-voxelization in the model is used three times (as shown in Fig. <ref type="figure" target="#fig_1">2</ref>), and the f t,c in the first operation is the coordinate and intensity, with c = 4. Moreover, due to the previously mentioned point sampling strategy, the converted voxel grid also inherits the vessel structure from the point data and dilutes the unimportant information in the CT images.</p><p>Multi-scale Point-Voxel Fusion Network. Intuitively, due to the image intensity between different Couinaud segments being similar, the voxel-based CNN model is difficult to achieve good segmentation performance. We propose a multi-scale point-voxel fusion network for accurate Couinaud segmentation, take advantage of the topological relationship of coordinate points in 3D space, and leverage the semantic information of voxel grids. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our method has two branches: point-based and voxel-based. The features extracted by these two branches on multiple scales are fused to provide more accurate and robust Couinaud segmentation performance. Specifically, in the point-based branch, the input point data {(p t , f t )} passes through an MLP, denoted as E p , which aims to extract fine-grained features with topological relationships. At the same time, the voxel grid {V u,v,w } passes the voxel branch based on convolution, denoted as E v , which can aggregate the features of surrounding points and learn the semantic information in the liver 3D space. We re-transform the features extracted from the voxel-based branch to point representation through trilinear interpolation, to combine them with fine-grained features extracted from the point-based branch, which provide complementary information:</p><formula xml:id="formula_4">(p t , f 1 t ) = E p (P (p t , f t )) + tri(E v (V )) t ,<label>(4)</label></formula><p>where the superscript 1 of (p t , f 1 t ) indicates that the fused point data and corresponding features f 1 t are obtained after the first round of point-voxel operation. Then, the point data (p t , f 1 t ) is voxelized again and extracted point features and voxel features through two branches. Note that the resolution of the voxel grid in this round is reduced to half of the previous round. After three rounds of pointvoxel operations, we concatenate the original point feature f t and the features f 1 t , f 2 t , f 3 t with multiple scales, then send them into a point-wise decoder D, parameterized by a fully connected network, to predict the corresponding Couinaud segment category:</p><formula xml:id="formula_5">Ôt = D(cat f t , f 1 t , f 2 t , f 3 t ) ∈ {0, 1, ..., 7},<label>(5)</label></formula><p>where {0, 1, ..., 7} denotes the Couinaud segmentation category predicted by our model for the point p t . We employ the BCE loss and the Dice loss to supervise the learning process. More method details are shown in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Evaluation Metrics</head><p>We evaluated the proposed framework on two publicly available datasets, 3Dircadb <ref type="bibr" target="#b20">[20]</ref> and LiTS <ref type="bibr" target="#b1">[2]</ref>. The 3Dircadb dataset <ref type="bibr" target="#b20">[20]</ref> contains 20 CT images with spacing ranging from 0.56 mm to 0.87 mm, and slice thickness ranging from 1 mm to 4 mm with liver and liver vessel segmentation labels. The LiTS dataset <ref type="bibr" target="#b1">[2]</ref> consists of 200 CT images, with a spacing of 0.56 mm to 1.0 mm and slice thickness of 0.45 mm to 6.0 mm, and has liver and liver tumour labels, but without vessels. We annotated the 20 subjects of the 3Dircadb dataset <ref type="bibr" target="#b20">[20]</ref> with the Couinaud segments and randomly divided 10 subjects for training and another 10 subjects for testing. For LiTS dataset <ref type="bibr" target="#b1">[2]</ref>, we observed the vessel structure on CT images, annotated the Couinaud segments of 131 subjects, and randomly selected 66 subjects for training and 65 for testing.</p><p>We have used three widely used metrics, i.e., accuracy (ACC, in %), Dice similarity metric (Dice, in %), and average surface distance (ASD, in mm) to evaluate the performance of the Couinaud segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>The proposed framework was implemented on an RTX8000 GPU using PyTorch. Based on the liver mask has been extracted, we train a 3D UNet <ref type="bibr" target="#b5">[6]</ref> on the 3Diradb dataset <ref type="bibr" target="#b20">[20]</ref> to generate the vessel attention map of two datasets. Then, we sample T = 20, 000 points in each epoch to train our proposed multi-scale point-voxel fusion network each epoch. We perform scaling within the range of 0.9 to 1.1, arbitrary axis flipping, and rotation in the range of 0 to 5 • C on the input point data as an augmentation strategy. Besides, we use the stochastic gradient descent optimizer with a learning rate of 0.01, which is reduced to 0.9 times for every 50 epochs of training. All our experiments were trained 400 epochs, with a random seed was 2023, and then we used the model with the best performance on the training set to testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with State-of-the-Art Methods</head><p>We compare our framework with several SOTA approaches, including voxelbased 3D UNet <ref type="bibr" target="#b5">[6]</ref>, point-based PointNet2Plus <ref type="bibr" target="#b16">[16]</ref>, and the methods of Jia et al. <ref type="bibr">[9]</ref>. The method of Jia et al. is a 2D UNet <ref type="bibr" target="#b17">[17]</ref> with dual attention to focus on the boundary of the Couinaud segments and is specifically used for the Couinaud segmentation task. We use PyTorch to implement this model and maintain the same implementation details as other methods. The quantitative and qualitative comparisons are shown in Table <ref type="table" target="#tab_2">1</ref> and Fig. <ref type="figure" target="#fig_2">3</ref>, respectively.</p><p>Quantitative Comparison. Table <ref type="table" target="#tab_2">1</ref> summarizes the overall comparison results under three metrics. By comparing the first two rows, we can see that Point-Net2Plus <ref type="bibr" target="#b16">[16]</ref> and 3D UNet <ref type="bibr" target="#b5">[6]</ref> have achieved close performance in the LiTS dataset <ref type="bibr" target="#b1">[2]</ref>, which demonstrates the potential of the point-based methods in the Couinaud segmentation task. In addition, the third row shows that Jia et al.'s [9] 2D UNet <ref type="bibr" target="#b17">[17]</ref> as the backbone method performs worst on all metrics, further demonstrating the importance of spatial relationships. Finally, our proposed point-voxel fusion segmentation framework achieves the best performance. Especially on the 3Diradb dataset <ref type="bibr" target="#b20">[20]</ref> with only 10 training subjects, the ACC and Dice achieved by our method exceed PointNet2Plus <ref type="bibr" target="#b16">[16]</ref> and 3D UNet <ref type="bibr" target="#b5">[6]</ref> by nearly 10 points, and the ASD is also greatly reduced, which demonstrates the effectiveness of the combining point-based and voxel-based methods.</p><p>Qualitative Comparison. To further evaluate the effectiveness of our method, we also provide qualitative results, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. The first two rows show that the vessel structure is used as the boundary guidance for Couinaud segmentation, but voxel-based 3D UNet <ref type="bibr" target="#b5">[6]</ref> fails to accurately capture this key structural relationship, resulting in inaccurate boundary segmentation. Note that, it can be seen that our method can learn the boundary guidance provided by the portal vein (the last two rows), to deal with the uncertain boundary more robustly.</p><p>Besides, compared with the 3D view, it is obvious that the voxel-based CNN methods are easy to pay attention to the local area and produce a large area of error segmentation, so the reconstructed surface is uneven. The point-based method obtains smooth 3D visualization results, but it is more likely to cause  segmentation blur in boundary areas with high uncertainty. Our method combines the advantages of point-based and voxel-based methods, and remedies their respective defects, resulting in smooth and accurate Couinaud segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>To further study the effectiveness of our proposed framework, we compared two ablation experiments: 1) random sampling of T points in the liver space, with-out considering the guidance of vascular structure, and 2) considering only the voxel-based branch, where the Couinaud segments mask is output by a CNN decoder. Figure <ref type="figure" target="#fig_3">4</ref> shows the ablation experimental results obtained on all the Couinaud segments of two datasets, under the Dice and the ASD metrics. It can be seen that our full method is significantly better than the CNN branch joint decoder method on both metrics of two datasets, which demonstrates the performance gain by the combined point-based branch. In addition, compared with the strategy of random sampling, our full-method reduces the average ASD by more than 2mm on eight Couinaud segments. This is because to the vessel structure-guided sampling strategy can increase the important data access between the boundaries of the Couinaud segments. Besides, perturbations are applied to the points in the coverage area of the vessel attention map, so that our full method performs arbitrary point sampling in the continuous space near the vessel, and is encouraged to implicitly learn the Couinaud boundary in countless points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a multi-scale point-voxel fusion framework for accurate Couinaud segmentation that takes advantage of the topological relationship of coordinate points in 3D space, and leverages the semantic information of voxel grids. Besides, the point sampling strategy embedded with vascular prior increases the access of our method to important regions, and also improves the segmentation accuracy and robustness in uncertain boundaries. Experimental results demonstrate the effectiveness of our proposed method against other cutting-edge methods, showing its potential to be applied in the preoperative application of liver surgery.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Couinaud segments (denoted as Roman numbers) in relation to the liver vessel structure. (a) and (b) briefly several Couinaud segments separated by the hepatic vein. (c) and (d) show several segments surrounded by the portal vein, which are divided by the course of the hepatic vein.</figDesc><graphic coords="2,74,31,470,84,275,32,67,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overall framework of our proposed method for Couinaud segmentation.</figDesc><graphic coords="3,61,98,243,71,328,12,181,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of segmentation results of different methods. Different colours represent different Couinaud segments. The first two rows show a subject from the LiTS dataset<ref type="bibr" target="#b1">[2]</ref>, and the last two rows show another subject from the 3Diradb dataset<ref type="bibr" target="#b20">[20]</ref>. In the first column, we show the vessel attention map in 2D and 3D views as an additional reference for the segmentation results.</figDesc><graphic coords="8,42,30,368,69,339,52,76,48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Ablation Study on Two Key Components. Different Table 1, here ACC and Dice are the average performance of each Couinaud segment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>To solve this issue, we propose a strategy of continuous spatial sampling point data based on the M . Specifically, the model randomly samples T points in each training epoch, of which T/2 points fall in the smaller space covered by the M , which enables the model to increase access to important data in the region during training. In addition, we apply a random perturbation Offset = (Δx, Δy, Δz) in the range of [-1, 1] to each point p t = (x t , y t , z t ) ∈ M in this region to obtain a new point p t = (x t + Δx, y t + Δy, z t + Δz), and the intensity in this coordinate obtained by trilinear interpolation. In the training stage of the network, the label of the point p t = (x t + Δx, y t + Δy, z t + Δz) is generated by:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>-voxelization. It is not enough to extract the topological information and fine-grained information of independent points only by point-based branch for accurate Couinaud segmentation. To this end, we transform the point data {(p t , f t )} into voxel grid {V u,v,w } by re-voxelization, where f t ∈ R c is the feature corresponding to point p t , aiming to voxel-based convolution to extract complementary semantic information in the grid. Specifically, we first normalize the coordinates {p t } to [0, 1], which is denoted as { pt }. Note that the point features {f t } remain unchanged during the normalization. Then, we transform the normalized point cloud {( pt , f t )} into the voxel grids {V u,v,w } by averaging all features f t whose coordinate pt = ( xt , ŷt , ẑt ) falls into the voxel grid (u, v, w):</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison with different segmentation methods. ACC and Dice are the averages of all testing subjects, while ASD is the average of the average performance of all segments in all testing subjects. Dice (% ↑) ASD (mm ↓) Acc (% ↑) Dice (% ↑) ASD (mm ↓)</figDesc><table><row><cell>Method</cell><cell cols="2">3Dircadb [20]</cell><cell></cell><cell>LiTS [2]</cell><cell></cell><cell></cell></row><row><cell cols="2">Acc (% ↑) 3D UNet [6] 71.05</cell><cell>82.19</cell><cell>9.13</cell><cell>80.50</cell><cell>88.47</cell><cell>5.81</cell></row><row><cell cols="2">PointNet2Plus [16] 57.96</cell><cell>72.41</cell><cell>13.54</cell><cell>78.75</cell><cell>88.01</cell><cell>6.88</cell></row><row><cell>Jia et al.'s [9]</cell><cell>59.74</cell><cell>72.69</cell><cell>15.61</cell><cell>64.83</cell><cell>77.59</cell><cell>14.71</cell></row><row><cell>Ours</cell><cell>82.42</cell><cell>90.29</cell><cell>5.49</cell><cell>85.51</cell><cell>92.12</cell><cell>5.18</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This project was funded by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">82090052</rs>, <rs type="grantNumber">82090054</rs>, <rs type="grantNumber">82001917</rs> and <rs type="grantNumber">81930053</rs>), <rs type="funder">Clinical Research Plan of Shanghai Hospital Development Center</rs> (No. <rs type="grantNumber">2020CR3004A</rs>), and <rs type="funder">National Key Research and Development Program of China</rs> under Grant (<rs type="grantNumber">2021YFC2500402</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zXZRXXz">
					<idno type="grant-number">82090052</idno>
				</org>
				<org type="funding" xml:id="_tKhrk4T">
					<idno type="grant-number">82090054</idno>
				</org>
				<org type="funding" xml:id="_vz2E5ZT">
					<idno type="grant-number">82001917</idno>
				</org>
				<org type="funding" xml:id="_GTYtCwK">
					<idno type="grant-number">81930053</idno>
				</org>
				<org type="funding" xml:id="_gxdwtSJ">
					<idno type="grant-number">2020CR3004A</idno>
				</org>
				<org type="funding" xml:id="_YkHjtAC">
					<idno type="grant-number">2021YFC2500402</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 45.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning-based landmark localisation in the liver for Couinaud segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ridgway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jandor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Aljabar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-80432-9_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-80432-918" />
	</analytic>
	<monogr>
		<title level="m">MIUA 2021</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Papież</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Yaqub</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">I L</forename><surname>Namburete</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Noble</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12722</biblScope>
			<biblScope unit="page" from="227" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The liver tumor segmentation benchmark (LiTS)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">102680</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Surgical anatomy and anatomical surgery of the liver</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bismuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World J. Surg</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3" to="9" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic anatomical segmentation of the liver by separation planes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Boltcheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Passat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Agnus</surname></persName>
		</author>
		<author>
			<persName><surname>Jacob-Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Col</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ronse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization, Image-Guided Procedures, and Display</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">6141</biblScope>
			<biblScope unit="page" from="383" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Opportunities and obstacles for deep learning in biology and medicine</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ching</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Soc. Interface</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">141</biblScope>
			<biblScope unit="page">20170387</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016, Part II</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Liver anatomy: portal (and suprahepatic) or biliary segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Couinaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dig. Surg</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="459" to="467" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fast method to segment the liver according to Couinaud&apos;s classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIMI 2007</title>
		<editor>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Loomes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Comley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">4987</biblScope>
			<biblScope unit="page" from="270" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-79490-5_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-540-79490-533" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boundary-aware dual attention guided liver segment segmentation model</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KSII Trans. Internet Inf. Syst. (TIIS)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="37" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Secular trend of cancer death and incidence in 29 cancer groups in china, 1990-2017: a joinpoint and age-period-cohort analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Manage. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">6221</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Point-voxel CNN for efficient 3d deep learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Preoperative localization of focal liver lesions to specific liver segments: utility of CT during arterial portography</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chezmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sugarbaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernardino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="94" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Liver resection and surgical strategies for management of primary liver cancer</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Orcutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Anaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Control</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1073274817744621</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic multi-atlas liver segmentation and Couinaud classification from CT volumes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pla-Alemany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Santabárbara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Aliaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Maceira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moratal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2826" to="2829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PointNet++: deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015, Part III</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename><surname>Suk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Rev. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="221" to="248" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully automatic anatomical, pathological, and functional segmentation from CT scans for hepatic surgery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Soler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Aided Surg</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3D image reconstruction for comparison of algorithm database: a patient specific anatomical and medical image database</title>
		<author>
			<persName><forename type="first">L</forename><surname>Soler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRCAD</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010">2010</date>
			<pubPlace>Strasbourg, France</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic Couinaud segmentation from CT volumes on liver using GLC-UNet</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32692-0_32</idno>
		<idno>978-3-030-32692-0 32</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MLMI 2019</title>
		<editor>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11861</biblScope>
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
