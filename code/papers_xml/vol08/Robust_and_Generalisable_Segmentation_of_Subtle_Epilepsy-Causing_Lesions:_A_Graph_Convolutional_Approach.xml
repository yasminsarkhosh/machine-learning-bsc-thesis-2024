<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach</title>
				<funder ref="#_5pvCURN #_dCUSvJ6 #_5pgX7TV #_XxP4mtT">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
				<funder ref="#_sgzhpcd">
					<orgName type="full">Wellcome Trust</orgName>
				</funder>
				<funder>
					<orgName type="full">Commonwealth Scholarship Commission (United Kingdom</orgName>
				</funder>
				<funder ref="#_tp6WAH5">
					<orgName type="full">Epilepsy Research UK</orgName>
				</funder>
				<funder>
					<orgName type="full">Jack Satter Foundation</orgName>
				</funder>
				<funder ref="#_eA8nHpv">
					<orgName type="full">Rosetrees Trust</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hannah</forename><surname>Spitzer</surname></persName>
							<idno type="ORCID">0000-0002-7858-0936</idno>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computational Biology</orgName>
								<orgName type="institution">Helmholtz Center Munich</orgName>
								<address>
									<postCode>85764</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Stroke and Dementia Research (ISD)</orgName>
								<orgName type="institution" key="instit1">LMU University Hospital</orgName>
								<orgName type="institution" key="instit2">LMU Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathilde</forename><surname>Ripart</surname></persName>
							<idno type="ORCID">0000-0002-1761-5859</idno>
							<affiliation key="aff2">
								<orgName type="department">Department of Developmental Neuroscience</orgName>
								<orgName type="institution">UCL Great Ormond Street Institute for Child Health</orgName>
								<address>
									<postCode>WC1N 1EH</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abdulah</forename><surname>Fawaz</surname></persName>
							<idno type="ORCID">0000-0002-7986-1801</idno>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Logan</forename><forename type="middle">Z J</forename><surname>Williams</surname></persName>
							<idno type="ORCID">0000-0001-5392-7043</idno>
							<affiliation key="aff2">
								<orgName type="department">Department of Developmental Neuroscience</orgName>
								<orgName type="institution">UCL Great Ormond Street Institute for Child Health</orgName>
								<address>
									<postCode>WC1N 1EH</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emma</forename><forename type="middle">C</forename><surname>Robinson</surname></persName>
							<idno type="ORCID">0000-0002-7886-3426</idno>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><forename type="middle">Eugenio</forename><surname>Iglesias</surname></persName>
							<idno type="ORCID">0000-0001-7569-173X</idno>
							<affiliation key="aff4">
								<orgName type="department">Martinos Center for Biomedical Imaging, MGH and Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Computer Science and Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Boston</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Centre for Medical Image Computing</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<postCode>WC1V 6LJ</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sophie</forename><surname>Adler</surname></persName>
							<idno type="ORCID">0000-0002-3978-7424</idno>
							<affiliation key="aff2">
								<orgName type="department">Department of Developmental Neuroscience</orgName>
								<orgName type="institution">UCL Great Ormond Street Institute for Child Health</orgName>
								<address>
									<postCode>WC1N 1EH</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Konrad</forename><surname>Wagstyl</surname></persName>
							<email>k.wagstyl@ucl.ac.uk</email>
							<idno type="ORCID">0000-0003-3439-5808</idno>
							<affiliation key="aff7">
								<orgName type="laboratory">Wellcome Centre for Human Neuroimaging</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<postCode>WC1N 3AR</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3C60FDCCE8527AE9B9C14314692EE864</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_41</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Convolutional Network</term>
					<term>lesion segmentation</term>
					<term>structural MRI</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Focal cortical dysplasia (FCD) is a leading cause of drugresistant focal epilepsy, which can be cured by surgery. These lesions are extremely subtle and often missed even by expert neuroradiologists. "Ground truth" manual lesion masks are therefore expensive, limited and have large inter-rater variability. Existing FCD detection methods are limited by high numbers of false positive predictions, primarily due to vertex-or patch-based approaches that lack whole-brain context. Here, we propose to approach the problem as semantic segmentation using graph convolutional networks (GCN), which allows our model to learn spatial relationships between brain regions. To address the specific challenges of FCD identification, our proposed model includes an auxiliary loss to predict distance from the lesion to reduce false positives and a weak supervision classification loss to facilitate learning from uncertain lesion masks. On a multi-centre dataset of 1015 participants with surface-based features and manual lesion masks from structural MRI data, the proposed GCN achieved an AUC of 0.74, a significant improvement against a previously used vertex-wise multi-layer perceptron (MLP)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Structural cerebral abnormalities commonly cause drug-resistant focal epilepsy, which may be cured with surgery. Focal cortical dysplasias (FCDs) are the most common pathology in children and the third most common pathology in adults undergoing epilepsy surgery <ref type="bibr" target="#b0">[1]</ref>. However, 16-43% of FCDs are not identified on routine visual inspection of MRI data by radiologists <ref type="bibr" target="#b9">[10]</ref>. Identification of these lesions on MRI is integral for presurgical planning. Furthermore, accurate identification of lesions assists with complete resection of the structural abnormality, which is associated with improved post-surgical seizure freedom rates <ref type="bibr" target="#b10">[11]</ref>.</p><p>There has been significant work seeking to automate the detection of FCDs, with the aim of identifying subtle structural abnormalities in patients with lesions not identified by visual inspection, termed "MRI negative" <ref type="bibr" target="#b11">[12]</ref>. These algorithms are increasingly being evaluated prospectively on patients who are "MRI negative" with suspected FCD, where radiologists review algorithm outputs and evaluate all putative lesions. However, previous methods operate locally or semi-locally: using multilayer perceptrons (MLPs) which consider voxels or points on the cortical surface (vertices) individually <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>, or convolutional neural networks which have to date typically been trained on patches of cortex <ref type="bibr" target="#b5">[6]</ref>. One widely-available algorithm using such an approach was able to detect 63% of MRI negative examples, with an AUC of 0.64 <ref type="bibr" target="#b9">[10]</ref>. Overall, although these algorithms show significant promise in finding subtle and previously unidentified lesions, they are commonly associated with high false positive rates which hampers clinical utility <ref type="bibr" target="#b11">[12]</ref>. Detecting FCDs is particularly challenging due to small dataset sizes, high inter-annotator variability in manual lesion masks, and the large class imbalance, as FCDs typically only cover around 1% of the total cortex. Nevertheless the urgent clinical need to identify more of these subtle lesions motivates the development of methods to address these challenges.</p><p>Contributions. We propose a robust surface-based semantic segmentation approach to address the particular challenges of identifying FCDs (Fig. <ref type="figure" target="#fig_0">1</ref>). Our three main contributions to address these challenges are: 1) Adapting nnU-Net <ref type="bibr" target="#b7">[8]</ref>, a state-of-the-art U-Net architecture, to a Graph Convolutional Network (GCN) for segmenting cortical surfaces. This creates a novel method for cortical segmentation in general and for FCD segmentation in particular, in which the model is able to learn spatial relationships between brain regions. 2) Inclusion of a distance loss to help reduce false positives, and 3) Inclusion of a hemisphere classification loss to act as form of weak supervision, mitigating the impact of imperfect lesion masks. We directly evaluate the added value of each contribution on performance in comparison to a previously published MLP <ref type="bibr" target="#b9">[10]</ref>. We hypothesised that the proposed GCN to segment FCDs would improve overall performance (AUC), in particular reducing the number of false positives (improved specificity) while retaining sensitivity. This improvement in classifier performance would facilitate clinical translation of automated FCD detection into clinical practice. All code to reproduce these results can be found at github.com/MELDProject/meld_graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Convolutional Network (GCN) for Surface-Based Lesion Segmentation</head><p>We consider the lesion detection problem as a surface-based segmentation task. For this purpose, cortical surface-based features (intensity, curvature, etc.; see Sect. 3.1) are extracted from each brain hemisphere and registered using FreeSurfer <ref type="bibr" target="#b3">[4]</ref> to a symmetrical template. This template was generated by successively upsampling an icosahedral icosphere, S 1 , with 42 vertices and 80 triangular faces. Icospheres S i , with i the resolution level of the icosphere, are triangulated spherical meshes, where S i+1 is generated from S i by adding vertices at every edge. As input to our model we use icosphere S 7 (163842 vertices).</p><p>U-Net Architecture. To segment lesions on the icosphere, we created a graphbased re-implementation of nnU-Net <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Unlike typical imaging data represented on rectangular grids, surface-based data require customised convolutions, downsampling and upsampling steps. Here, we used spiral convolutions <ref type="bibr" target="#b6">[7]</ref> which translates standard 2D convolutions to irregular meshes by defining the filter by an outward spiral. This ends up capturing a ring of information around the current node, similar to how a 2d filter captures a ring of information around the current pixel. We use a spiral length of 7, representing the central and adjacent 6 neighbours on a hexagonal mesh, roughly equivalent to a 3 × 3 2D kernel. For downsampling from S i+1 to S i in the U-Net encoder, a similar translation of 2D max pooling is carried out by aggregating over all neighbours of the vertex at the higher-resolution S i+1 . Upsampling from S i to S i+1 in the decoder is implemented via assigning the mean of each vertex in S i to all neighbours at level S i+1 . In total, the U-Net contains seven levels (mirroring the seven icospheres S 7 -S 1 ), and every level consists of three convolutional layers using spiral convolutions and leaky Relu as activation function (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>Loss Functions. Following best practices for U-Net segmentation models <ref type="bibr" target="#b7">[8]</ref>, we use both cross-entropy and dice as loss functions for the segmentation, where y is true labels, ŷ is predicted, n is the number of vertices:</p><formula xml:id="formula_0">L ce = - n i=1 y i log(ŷ i ) + (1 -y i ) log(1 -ŷi )<label>(1)</label></formula><formula xml:id="formula_1">L dice = 1 - 2 n i=1 y i ŷi n i=1 y 2 i + n i=1 ŷ2 i +<label>(2)</label></formula><p>Distance Loss. To encourage the network to learn whole-brain context thereby reducing the number of false positives, we added an additional distance regression task. We train the model to predict the normalised geodesic distance d to the lesion boundary for every vertex, by applying an additional loss L dist to the non-lesional prediction, ŷi,0 of the segmentation output for vertex i. We use the mean absolute error loss, weighted by the distance so as not to overly penalise small errors in predicting large distances from the lesion:</p><formula xml:id="formula_2">L dist = 1 n n i=1 |d i -ŷi,0 | d i + 1<label>(3)</label></formula><p>Classification Loss. To mitigate uncertainty in the correspondence between lesion masks and lesions, we used a weakly-supervised classification loss L class .</p><p>For the ground truth c, examples were labelled as positive, if any of their vertices were annotated as positive. To predict this sample-level classification, we added a classification head to the deepest level (level 1) of the U-Net. The classification head contained a fully connected layer aggregating over all filters, followed by a fully-connected layer aggregating over all vertices, resulting in the classification output ĉ. This output was trained using cross-entropy:</p><formula xml:id="formula_3">L class = - n i=1 c i log(ĉ i ) + (1 -c i ) log(1 -ĉi )<label>(4)</label></formula><p>Deep Supervision. To encourage the flow of gradients through the entire U-Net, we use deep supervision at levels</p><formula xml:id="formula_4">I ds = [6, 5, 4, 3, 2, 1]. Let L i ce , L i dice , L i</formula><p>dist be the cross-entropy, dice and distance losses applied to outputs at level i, respectively. The model is trained on a weighted sum of all the losses, with w i ds the loss weight at level i:</p><formula xml:id="formula_5">L = L ce + L dice + L dist + L class + i∈I ds w i ds (L i ce + L i dice + L i dist )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Augmentation</head><p>Data augmentations consisted of spatial augmentations and intensity augmentations (Fig. <ref type="figure" target="#fig_0">1</ref>), following recommendations outlined in nnU-Net. Spatial augmentation included rotation, inversion and non-linear deformations of the surfacebased data <ref type="bibr" target="#b2">[3]</ref>. Intensity-based augmentations included adding a Gaussian noise to the features intensity, adjusting the contrast, scaling the brightness by a uniform factor, and adding a gamma intensity transform.</p><p>3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Implementation Details</head><p>Dataset. For the following experiments, we used a dataset of post-processed surface-based features and manual lesion masks from 618 patients with FCD and 397 controls <ref type="bibr" target="#b9">[10]</ref>. This is a heterogeneous, clinically-acquired dataset, collated from 22 international epilepsy surgery centres, including paediatric and adult participants scanned on either 1.5T or 3T MRI scanners. Each centre received local ethical approval from their institutional review board (IRB) or ethics committee (EC) to retrieve and anonymise retrospective, routinely available clinical data. For each participant, MR images were processed using FreeSurfer <ref type="bibr" target="#b3">[4]</ref> and 11 surface-based features (cortical thickness, grey-white matter intensity contrast, intrinsic curvature, sulcal depth, curvature and FLAIR intensity sampled as 6 intra-and sub-cortical depths) were extracted. FCDs were manually drawn by neuroradiologists to create 3D regions of interest (ROI) on T1 or fluidattenuated inversion recovery (FLAIR) images. The ROIs were projected onto individual FreeSurfer surfaces and then the features and ROIs were registered to a bilaterally symmetrical template, fsaverage_sym, using folding-based registration. Post-processing included 10 mm full width at half-maximum surface-based smoothing of the per-vertex features, harmonisation of the data using Combat <ref type="bibr" target="#b4">[5]</ref> (to account for scanners differences), inter-and intra-individual z-scoring to account for inter-regional differences and demographic differences, and computation of the asymmetry index of each feature. The final surface-based feature set consisted of the original, z-scored and asymmetry features, resulting in 33 input features.</p><p>In order to compare performance, the train/validation and test datasets were kept identical to those in the previously published vertex-wise classifier <ref type="bibr" target="#b9">[10]</ref>. The train/validation cohort comprised 50% of the dataset and 5-fold cross validation was used to evaluate the models. The remaining 50% was withheld for final evaluation and comparison of models. Data from two independent sites (35 patients and 18 controls) were used to test the generalisability of the full model. Hardware: High-performance cluster with Single NVIDIA A100 GPU, 1000 GiB RAM; Software: PyTorch 1.10.0+cu11.1, PyTorch Geometric 2.0.4, Python 3.9.13. Combined memory footprint of model and dataset while training is 49 GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation</head><p>Experiments. Using our graph-based adaptation of nnU-Net (GC-nnU-Net) and the previous MLP model as baseline, we ran an ablation study to measure the impact of the proposed auxiliary losses (Table <ref type="table" target="#tab_0">1</ref>). Each model was trained using the train/val cohort 5 times, withholding 20% of the cohort for validation and stopping criteria. Final test performance was computed by ensembling predictions across the 5-fold trained models, with uncertainty estimates calculated through bootstrapping. An additional experiment was carried out subsampling the training cohort at fixed fractions of 0.1, 0.2, 0.3, 0.4, 0.6 and 0.8 using the GC-nnU-Net+dc model (Fig. <ref type="figure" target="#fig_2">S2</ref>).</p><p>Evaluations. Model performances were compared according to their Area Under the Curve (AUC), which was calculated by computing the sensitivity and specificity at a range of prediction thresholds. For sensitivity calculations, due to uncertainty in the lesion masks, a lesion was considered detected if the prediction was within 20 mm of the original mask, as this corresponds with the inter-observer variability measured across annotators <ref type="bibr" target="#b9">[10]</ref>. Specificity was defined by the presence of false positives in non-lesional examples. As an additional measure of model specificity, the number of false positive clusters in both patients and controls were calculated. Model AUCs were statistically compared using t-tests, with correction for multiple comparisons using the Holm-Sidak method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Table <ref type="table" target="#tab_1">2</ref> compares model performances on the withheld test set. GC-nnU-Net+dc, the graph-based implementation of nnU-Net with additional distance and classification losses, outperformed all other models. Examples of individual predictions using the MLP and GC-nnU-Net+dc model, as well as examples of the predicted geodesic distance from the lesion are presented in Fig. <ref type="figure" target="#fig_2">2A,</ref><ref type="figure">B</ref>. Figure <ref type="figure" target="#fig_2">2C</ref> visualises the reduction in number of false positive clusters when using GC-nnU-Net+dc which is reflected in the significantly improved specificity. GC-nnU-Net+dc showed similarly improved specificity relative to the MLP on independent test sites, demonstrating good model generalisability (Table <ref type="table" target="#tab_2">3</ref>). In experiments varying the size of the training cohort, performance increased with sample size until around 220 subjects above which gains were negligible (Fig. <ref type="figure" target="#fig_2">S2</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>This paper presents a robust and generalisable graph convolutional approach for segmenting focal cortical dysplasias using surface-based cortical data. This approach outperforms specificity baselines by 22-27%, which is driven by three newly-proposed components. First, treating the hemispheric surface as a single connected graph allows the network to model spatial context. Second, a classi-fication loss mitigates the impact of imprecise lesion masks by simplifying the task to predicting whether or not a lesion is present in every hemisphere. Third, a distance-from-lesion prediction task penalises false positives and encourages the network to consider the entire hemisphere. The results show a significant increase in specificity, both in terms of presence of any false positive predictions in non-lesional hemispheres and a reduced number of additional clusters in lesional hemispheres. From a translational perspective, this improvement in performance will increase clinical confidence in applying these tools to cases of suspected FCD, while additionally minimising the number of putative lesions an expert neuroradiologist would need to review. Future work will include systematic prospective evaluation of the tool in suspected FCDs and expansion of these approaches to multiple causes of focal epilepsy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Proposed GC-nnU-Net+dc model for lesion segmentation, with auxiliary distance regression and hemisphere classification tasks. Lower left box: Types of data augmentation employed. Examples show the result of gamma intensity augmentation (top) and spinning (bottom).</figDesc><graphic coords="3,44,79,54,53,334,66,180,19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Details. The graph-based convolutional implementation of nnU-Net (GC-nnU-Net) had the following training parameters: batch size: 8, initial learning rate: 10 -4 , learning rate decay: 0.9, momentum: 0.99, maximum epochs: 1000 (1 epoch is 1 complete view of training data), maximum patience: 400 epochs; and augmentation probabilities: inversion: 0.5, rotation &amp; deformation: 0.2, Gaussian noise: 0.15, contrast: 0.15, brightness: 0.15, gamma: 0.15. Deep supervision weights were w ds = [0.5, 0.25, 0.125, 0.0625, 0.03125, 0.0150765] for levels I ds = [6, 5, 4, 3, 2, 1]. Due to class imbalances, non-lesional hemispheres were undersampled during training to ensure 33% of training examples contained a lesion. The model from the epoch with the best validation loss is stored for evaluation (Fig. S1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A &amp; B) Individual predictions for two patients using MLP and GC-nnU-Net+dc, as well as predicted geodesic distance from the lesion. Red: prediction. Black line: manual lesion mask. Patients A and B both have false positive predictions using the MLP, unlike the predictions from GC-nnU-Net+dc. C) Comparison of number of clusters in controls and patients between MLP and GC-nnU-Net+dc on the test dataset. Grey lines: change in number of clusters for individual participants. (Color figure online)</figDesc><graphic coords="8,93,48,54,23,265,78,261,49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Experiments</figDesc><table><row><cell cols="2">Experiment Name Description</cell></row><row><cell>MLP [10]</cell><cell>vertex-wise multilayer perceptron</cell></row><row><cell>GC-nnU-Net</cell><cell>graph-based adaptation of nnU-Net</cell></row><row><cell>GC-nnU-Net+c</cell><cell>adding classification loss</cell></row><row><cell>GC-nnU-Net+d</cell><cell>adding distance loss</cell></row><row><cell cols="2">GC-nnU-Net+dc adding distance loss and classification loss</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of models on the test dataset.</figDesc><table><row><cell>Experiment</cell><cell>AUC (±std)</cell><cell cols="3">Sensitivity Specificity Run time (min)</cell></row><row><cell>MLP [10]</cell><cell>0.64 (n.a.)</cell><cell>67%</cell><cell>49%</cell><cell>n.a.</cell></row><row><cell>GC-nnU-Net</cell><cell cols="2">0.68  † ‡ (±0.004) 67%</cell><cell>64%</cell><cell>396.1</cell></row><row><cell cols="3">GC-nnU-Net+c 0.74  † (±0.008) 67%</cell><cell>66%</cell><cell>373.9</cell></row><row><cell cols="3">GC-nnU-Net+d 0.69  † ‡ (±0.007) 67%</cell><cell>65%</cell><cell>426.9</cell></row><row><cell cols="3">GC-nnU-Net+dc 0.74  † (±0.005) 67%</cell><cell>71%</cell><cell>564.9</cell></row><row><cell cols="4">†Model performance significantly improved compared to MLP.</cell><cell></cell></row><row><cell cols="5">‡Model performance significantly worse compared to GC-nnU-Net+dc.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of models on independent test sites.</figDesc><table><row><cell>Experiment</cell><cell cols="3">Sensitivity Specificity Median FP in</cell><cell>Median clusters</cell></row><row><cell></cell><cell></cell><cell></cell><cell>patients [IQR]</cell><cell>in patients [IQR]</cell></row><row><cell>MLP [10]</cell><cell>79%</cell><cell>17%</cell><cell>2.0 [1.0, 4.0]</cell><cell>1.0 [1.0, 2.75]</cell></row><row><cell cols="2">GC-nnU-Net+dc 79%</cell><cell>44%</cell><cell>1.0 [0.0, 1.0]</cell><cell>1.0 [0.0, 1.0]</cell></row><row><cell cols="3">FP: false positives, IQR: interquartile range.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The <rs type="projectName">MELD</rs> project, MR and SA are supported by the <rs type="funder">Rosetrees Trust</rs> (<rs type="grantNumber">A2665</rs>) and <rs type="funder">Epilepsy Research UK</rs> (<rs type="grantNumber">P2208</rs>). KSW is supported by the <rs type="funder">Wellcome Trust</rs> (<rs type="grantNumber">215901/Z/19/Z</rs>). LZJW is supported by the <rs type="funder">Commonwealth Scholarship Commission (United Kingdom</rs>). JEI is supported by the <rs type="funder">NIH</rs> (<rs type="grantNumber">1RF1MH123195</rs>, <rs type="grantNumber">1R01AG070988</rs>, <rs type="grantNumber">1R01EB031114</rs>, <rs type="grantNumber">1UM1MH130981</rs>) and the <rs type="funder">Jack Satter Foundation</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_eA8nHpv">
					<idno type="grant-number">A2665</idno>
					<orgName type="project" subtype="full">MELD</orgName>
				</org>
				<org type="funding" xml:id="_tp6WAH5">
					<idno type="grant-number">P2208</idno>
				</org>
				<org type="funding" xml:id="_sgzhpcd">
					<idno type="grant-number">215901/Z/19/Z</idno>
				</org>
				<org type="funding" xml:id="_5pvCURN">
					<idno type="grant-number">1RF1MH123195</idno>
				</org>
				<org type="funding" xml:id="_dCUSvJ6">
					<idno type="grant-number">1R01AG070988</idno>
				</org>
				<org type="funding" xml:id="_5pgX7TV">
					<idno type="grant-number">1R01EB031114</idno>
				</org>
				<org type="funding" xml:id="_XxP4mtT">
					<idno type="grant-number">1UM1MH130981</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Histopathological findings in brain tissue obtained during epilepsy surgery</title>
		<author>
			<persName><forename type="first">I</forename><surname>Blumcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">377</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="1648" to="1656" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">External validation of automated focal cortical dysplasia detection using morphometric analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsia</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Benchmarking geometric deep learning for cortical segmentation and neurodevelopmental phenotype prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fawaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">FreeSurfer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="774" to="781" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Harmonization of cortical thickness measurements across scanners and sites</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Fortin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page" from="104" to="120" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multicenter validation of a deep learning detection algorithm for focal cortical dysplasia</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Gill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurology</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SpiralNet++: a fast and highly efficient mesh convolution operator</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interpretable surface-based detection of focal cortical dysplasias: a Multi-centre Epilepsy Lesion Detection study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Spitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3859" to="3871" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Focal cortical dysplasia type IIb: completeness of cortical, not subcortical, resection is necessary for seizure freedom</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Urbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niehusmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Von Lehe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Elger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wellmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsia</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1418" to="1424" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Artificial intelligence for the detection of focal cortical dysplasia: challenges in translating algorithms into clinical practice</title>
		<author>
			<persName><forename type="first">L</forename><surname>Walger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsia</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
