<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dayang</forename><surname>Wang</surname></persName>
							<idno type="ORCID">0000-0003-0531-2110</idno>
							<affiliation key="aff0">
								<orgName type="institution">Subtle Medical Inc</orgName>
								<address>
									<postCode>94025</postCode>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts Lowell</orgName>
								<address>
									<postCode>01854</postCode>
									<settlement>Lowell</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Srivathsa</forename><surname>Pasumarthi</surname></persName>
							<email>srivathsa@subtlemedical.com</email>
							<idno type="ORCID">0000-0003-0531-2110</idno>
							<affiliation key="aff0">
								<orgName type="institution">Subtle Medical Inc</orgName>
								<address>
									<postCode>94025</postCode>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Greg</forename><surname>Zaharchuk</surname></persName>
							<idno type="ORCID">0000-0001-5781-8848</idno>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94035</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Chamberlain</surname></persName>
							<idno type="ORCID">0000-0002-2705-1268</idno>
							<affiliation key="aff0">
								<orgName type="institution">Subtle Medical Inc</orgName>
								<address>
									<postCode>94025</postCode>
									<settlement>Menlo Park</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D90BEB4B4885E457A2F87C5F2FF435DB</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Contrast-enhanced MRI</term>
					<term>Iterative Model</term>
					<term>Vision Transformers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning (DL) based contrast dose reduction and elimination in MRI imaging is gaining traction, given the detrimental effects of Gadolinium-based Contrast Agents (GBCAs). These DL algorithms are however limited by the availability of high quality low dose datasets. Additionally, different types of GBCAs and pathologies require different dose levels for the DL algorithms to work reliably. In this work, we formulate a novel transformer (Gformer) based iterative modelling approach for the synthesis of images with arbitrary contrast enhancement that corresponds to different dose levels. The proposed Gformer incorporates a sub-sampling based attention mechanism and a rotational shift module that captures the various contrast related features. Quantitative evaluation indicates that the proposed model performs better than other state-of-the-art methods. We further perform quantitative evaluation on downstream tasks such as dose reduction and tumor segmentation to demonstrate the clinical utility.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gadolinium-Based Contrast Agents (GBCAs) are widely used in MRI scans owing to their capability of improving the border delineation and internal morphology of different pathologies and have extensive clinical applications <ref type="bibr" target="#b0">[1]</ref>. However, GBCAs have several disadvantages like contraindications in patients with reduced renal function <ref type="bibr" target="#b1">[2]</ref>, patient inconvenience, high operation costs and environmental side effects <ref type="bibr" target="#b2">[3]</ref>. Therefore, there is an increased emphasis on the paradigm of "as low as reasonably achievable" (ALARA) <ref type="bibr" target="#b3">[4]</ref>. To tackle these concerns of GBCAs, several dose reduction <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and elimination approaches <ref type="bibr" target="#b6">[7]</ref> have been proposed. However, these deep learning(DL)-based dose reduction approaches require high quality low-dose contrast-enhanced (CE) images paired with pre-contrast and full-dose CE images. Acquiring such a dataset requires modification of the standard imaging protocol and involves additional training of the MR technicians. Therefore, it is important to simulate the process of T1w low-dose image acquisition, using images from the standard protocol. Moreover, it is crucial for these dose reduction approaches to establish the minimum dose level required for different pathologies as these are dependent on the scanning protocol and the GBCA compound injected. Therefore the simulation tool should also have the ability to synthesize images with multiple contrast enhancement levels, that correspond to multiple arbitrary dose levels.</p><p>Currently MRI dose simulation is done using physics-based models <ref type="bibr" target="#b7">[8]</ref>. However, these physics-based methods are dependent on the protocol parameters and the type of GBCA and their relaxation parameters. Deep learning (DL) models have been widely used in medical imaging application due to their high capacity, generazibility, and transferability <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. The performance of these DL models heavily depend on the availability of high quality data. There is a dearth of datadriven approaches to MRI dose-simulation given the lack of diverse ground truth data of the different dose levels. To this effect, we introduce a vision transformer based DL model<ref type="foot" target="#foot_0">1</ref> that can synthesize brain 2 MRI images that correspond to arbitrary dose levels, by training on a highly imbalanced dataset with only T1w pre-contrast, T1w 10% low-dose, and T1w CE standard dose images. The model backbone consists of a novel Global transformer (Gformer) with subsampling attention that can learn long-range dependencies of contrast uptake features. The proposed method also consists of a rotational shift operation that can further capture the shape irregularity of the contrast uptake regions. We performed extensive quantitative evaluation in comparison to other state-of-the art methods. Additionally, we show the clinical utility of the simulated T1w low-dose images using downstream tasks. To the best of our knowledge, this is the first DL based MRI dose simulation approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Iterative Learning Design: DL based models tend to perform poorly when the training data is highly imbalanced <ref type="bibr" target="#b10">[11]</ref>. Furthermore, the problem of arbitrary dose simulation requires the interpolation of intermediate dose-levels using a minimum number of data points. Iterative models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>   applications as they work on the terminal images to generate step-wise intermediate solutions. We first utilize this design paradigm for the dose simulation task and train an end-to-end model on a highly imbalanced dataset where only T1w pre-contrast, T1w low-dose, and T1w post-contrast are available.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref> </p><formula xml:id="formula_0">P low = k F • F • • • • • F (P post , P pre ),<label>(1)</label></formula><p>where P pre , P post , and P low denote the pre-contrast, post-contrast, and predicted low-dose images, respectively and P i-1 denotes the image with a higher enhancement than P i . This way, the intermediate outputs { P i } k i=1 having different enhancement levels, correspond to images with different contrast dose level with a uniform interval. This iterative model essentially learns a gradual dose reduction process, in which each iteration step removes a certain amount of contrast enhancement from the full-dose image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Functions and Model Convergence:</head><p>The proposed iterative model aims to learn a mapping from the post-contrast &amp; pre-contrast images to the synthesized low-dose images P low and is trained with the true 10% low-dose image P low as the ground truth. We used the L 1 and structural similarity index measure (SSIM) losses. To tackle the problem of gradient explosion or vanishing, "soft labels" are generated using linear scaling. These "soft labels" serve as a reference to the intermediate outputs during the iterative training process and also aid model convergence, without which the model has to directly learn from post-contrast to low-dose. Given k iterations, the soft label {S i } k-1 i=1 for iteration i is calculated as follows:</p><formula xml:id="formula_1">S i = P pre + [γ + (1 -γ) k -i k ] × ReLU( P post -P pre -τ ),<label>(2)</label></formula><p>where P post and P pre denote the skull-stripped post-contrast and pre-contrast images. γ = 0.1 represents the dose level of the final prediction, and τ = 0.1 denotes the threshold to extract the estimated contrast uptake U = ReLU( P post -P preτ ). Finally, the total losses are calculated as</p><formula xml:id="formula_2">L total = α • k-1 i=1 L e ( P i , S i ) + β • L e ( P low , P low ).<label>(3)</label></formula><p>where L e = L L1 + L SSIM and α = 0.1 and β = 1. The "soft labels" are assigned a small loss weight so that they do not overshadow the contribution of the real low-dose image. Additionally, in order to recover the high frequency texture information and to improve the overall perceptual quality, adversarial <ref type="bibr" target="#b13">[14]</ref> and perceptual losses <ref type="bibr" target="#b14">[15]</ref> are applied on ( P low , P low ) with a weight of 0.1.</p><p>Global Transformer (Gformer): Transformer models have risen to prominence in a wide range of computer vision applications <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>. Traditional Swin transformers compute attention on non-overlapping local window patches. To further exploit the global contrast information, we propose a hybrid global transformer (Gformer) as a backbone for the dose simulation task. As illustrated in Fig. <ref type="figure" target="#fig_0">1(b</ref>), the proposed model design includes six sequential Gformer blocks as the backbone module with shortcuts. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>(c), the Gformer block contains a convolution block, a rotational shift module, a sub-sampling process, and a typical transformer module. The convolution layer extracts granular local information of the contrast uptake while the self-attention emphasizes more on the coarse global context, thereby paying attention to the overall contrast uptake structure.</p><p>Subsampling Attention: The sub-sampling is a key element in the Gformer block which generates a number of sub-images from the whole image as attention windows as shown in Fig. <ref type="figure" target="#fig_2">2</ref>. Gformer performs self-attention on the sub-sampled images, which encompasses global contextual information with minimal selfattention overhead on small feature maps. Given the entire feature map M e ∈ R b×c×h×w , where b, c, h, and w are the batch size, channel dimension, height, and width, respectively, the subsampling process aggregates the strided positions to the sub-feature maps as follows, where d denotes sampling a position every d pixels, and</p><formula xml:id="formula_3">{M s } d 2 -1 s=0 = {M[ : , : , i : h : d , j : w : d ]} d-1,d-1 i=0,j=0 ,<label>(4)</label></formula><formula xml:id="formula_4">M s ∈ R b×c× h d × h</formula><p>d is the subsampled feature map. We set h, d = 0 to avoid any information loss during subsampling. These d 2 sub-feature maps are stacked onto the batch dimension as the attention windows for the transformer block shown in Fig. <ref type="figure" target="#fig_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(c).</head><p>Rotational Shift: Image rotation has been widely used as a data augmentation technique in preprocessing and model training. Here, to further capture the heterogeneous nature of the contrast uptake areas, we employ the rotational shift as a module to facilitate the representation power of the Gformer. To prevent information loss on the edges due to rotation, only small angles (e.g., 10 • , 20 • ) are used for rotation and residual shortcuts are also applied. Specifically, given the feature map M o ∈ R b×c×h×w , rotational shift is performed around the vertical axis of height/width. The rotated feature map M r ∈ R b×c×h×w is obtained by the following equation:</p><formula xml:id="formula_5">⎡ ⎢ ⎢ ⎣ p q x y ⎤ ⎥ ⎥ ⎦ = ⎡ ⎢ ⎢ ⎣ 1 0 0 0 0 1 0 0 0 0 cosλ -sinλ 0 0 sinλ cosλ ⎤ ⎥ ⎥ ⎦ ⎡ ⎢ ⎢ ⎣ p q x -h//2 y -w//2 ⎤ ⎥ ⎥ ⎦ + ⎡ ⎢ ⎢ ⎣ 0 0 h//2 w//2 ⎤ ⎥ ⎥ ⎦ , (<label>5</label></formula><formula xml:id="formula_6">) M r (p, q, x, y) = M o (p , q , x , y ), if x ∈ [0, h) and y ∈ [0, w) 0, otherwise,<label>(6)</label></formula><p>where λ is the rotation angle. (p, q, x, y) and (p , q , x , y ) denote the pixel index in the feature map tensor before and after rotational shift, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Dataset: With IRB approval and informed consent, we retrospectively used 126 clinical cases (113 training, 13 testing) from a internal private dataset<ref type="foot" target="#foot_2">3</ref> using Gadoterate meglumine contrast agent (Site A). For downstream task assessment we used 159 patient studies from another site (Site B) using Gadobenate dimeglumine. The detailed cohort description is given in Table <ref type="table" target="#tab_2">1</ref>. The clinical indications for both sites included suspected tumor, post-op tumor follow-up and routine brain. For each patient, 3D T1w MPRAGE scans were acquired for the pre-contrast, low-dose, and post-contrast images. These paired images were then mean normalized and affine co-registered (pre-contrast as the fixed image) using SimpleElastix <ref type="bibr" target="#b16">[17]</ref>. The images were also skull-stripped, to account for differences in fat suppression, using the HD-BET brain extraction tool <ref type="bibr" target="#b17">[18]</ref> for generating the "soft labels".   Evaluation Settings: We quantitatively evaluated the proposed model using PSNR, SSIM, RMSE, and LPIPS perceptual metrics <ref type="bibr" target="#b18">[19]</ref>, between the synthesized and true low-dose images. We replaced the Gformer backbone with other state-of-the-art methods to compare the efficacy of the different methods. Particularly, the following backbone networks were studied: simple linear scaling ("Scaling") approach, Rednet <ref type="bibr" target="#b19">[20]</ref>, Mapnn <ref type="bibr" target="#b12">[13]</ref>, Restormer <ref type="bibr" target="#b20">[21]</ref>, and SwinIR <ref type="bibr" target="#b21">[22]</ref>. Unet <ref type="bibr" target="#b22">[23]</ref> and Swin-Unet <ref type="bibr" target="#b23">[24]</ref> models were not assessed due to their tendency to synthesize blurry artifacts in the iterative modelling. throughput metric (number of images generated per second) was also calculated to assess the inference efficiency.</p><p>Evaluation Results: Figure <ref type="figure" target="#fig_5">4</ref>(a) shows that the proposed model is able to generate images that correspond to different dose levels. As shown in the zoomed inset, the hyperintensity of the contrast uptake in these images gradually reduces at each iteration. Figure <ref type="figure" target="#fig_5">4</ref>(b) shows that the pathological structure in the synthesized low-dose image is similar to that of the ground truth. Figure <ref type="figure" target="#fig_5">4(c</ref>) also shows that the model is robust to hyperintensities that are not related to contrast uptake. Figure <ref type="figure" target="#fig_4">3</ref> and Table <ref type="table" target="#tab_3">2</ref> show that proposed model can synthesize enhancement patterns that look close to the true low-dose and that it performs better than the other competing methods with a reasonable inference throughput. Quantitative Assessment of Contrast Uptake: The above pixel-based metrics do not specifically focus on the contrast uptake region. In order to assess the contrast uptake patterns of the intermediate images, we used the following metrics as described in <ref type="bibr" target="#b24">[25]</ref>: contrast to noise ratio(CNR), contrast to background ratio(CBR), and contrast enhancement percentage(CEP). The ROI for the contrast uptake was computed as the binary mask of the corresponding "soft labels" in Eq. 2. As shown in Fig. <ref type="figure" target="#fig_6">5</ref>, the value of the contrast specific metrics increases in a non-linear fashion as the iteration step increases. Downstream tasks: In order to demonstrate the clinical utility of the synthesized low-dose images, we performed two downstream tasks: 1) Low-dose to full-dose synthesis Using the DL-based algorithm to predict full-dose image from pre-contrast and low-dose images described in <ref type="bibr" target="#b4">[5]</ref>, we synthesized T1CE volumes using true low-dose (T1CE-real-ldose) and Gformer (rot) synthesized low-dose (T1CE-synth-ldose). We computed the PSNR and SSIM metrics of T1CE vs T1CE-synth/T1CE vs T1CE-synth-sim which are 29.82 ± 3.90 dB/28.10 ± 3.20 dB and 0.908 ± 0.031/0.892 ± 0.026 respectively. This shows that the synthesized low-dose images perform similar<ref type="foot" target="#foot_3">4</ref> to that of the low-dose image in the dose reduction task. For this analysis we used the data from Site B.  2) Tumor segmentation Using the T1CE volumes synthesized in the above step, we perform tumor segmentation using the winning solution of BraTS 2018 challenge <ref type="bibr" target="#b25">[26]</ref>. Let M true , M ldose and M ldose-sim be the whole tumor (WT) masks generated using T1CE, T1CE-real-ldose and T1CE-synth-ldose (+ T1, T2 and FLAIR images) respectively. The mean Dice scores Dice(M true , M ldose ) and Dice(M true , M ldose-sim ) on the test set were 0.889±0.099 and 0.876±0.092 respectively. Figure <ref type="figure" target="#fig_7">6</ref> shows visual examples of tumor segmentation performance. This shows that the clinical utility provided by the synthesized low-dose is similar<ref type="foot" target="#foot_4">5</ref> to that of the actual low-dose image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussions and Conclusion</head><p>We have proposed a Gformer-based iterative model to simulate low-dose CE images. Extensive experiments and downstream task performance have verified the efficacy and clinical performance of the proposed model compared to other state-of-the art methods. In the future, further reader studies are required to assess the diagnostic equivalence of the simulated low-dose images. The model can be guided using physics-based models <ref type="bibr" target="#b26">[27]</ref> that estimate contrast enhancement level using signal intensity. This simulation technique can easily be extended to other anatomies and contrast agents.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) The proposed model based on iterative learning design. (b) The overall architecture of the proposed Gformer as the backbone network. (c) Layers inside the Gformer block. (d) Illustration of a typical vision transformer block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>P</head><label></label><figDesc>(a), the proposed iterative model G = F •F •• • ••F learns a transformation from the post-contrast to the low-dose image in k iterations,where F represents the base model. At each iteration i, the higher enhancement image from the previous step and the pre-contrast images are fed into F to predict the image with lower enhancement. The iterative model can be formulated as follows, i = F ( P i-1 , P pre )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Illustration of the sub-sampling process with the stride of 4 as global attention window in Gformer block in comparison to local attention in Swin transformer. All pixels within the same number aggregate to the same sub-image. (b) Depiction of how rotational shift can enhance diverse contextual information fusion across layers compared to cyclic shift in Swin transformer.</figDesc><graphic coords="5,73,80,54,50,276,28,176,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>20 •</head><label>20</label><figDesc>Implementation Details: All experiments were conducted with a single Tesla V100-SXM2 32GB GPU on a Intel(R) Xeon(R) CPU E5-2698 v4. The Subsampling stride for the six levels of Gformer block were {4,8,16,16,8,4}. The Rotational shift angles were {0,10,20,20,10,0} across all blocks. The model was optimized using the Adam optimizer with an initial learning rate of 1e-5 and a batch size of 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The results of the synthesized 10% dose images from different methods. 'Rot' denotes rotational shift and 'Cyc' indicates cyclic shift.</figDesc><graphic coords="6,87,96,411,80,276,40,108,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Model results showing images with different contrast enhancement corresponding to different dose levels along with the synthesized and true low-dose and pre-contrast. (b)-(c) Two representative slices of the synthesized 10% dose images. (d)-(e) Two representative slices using a different GBCA.</figDesc><graphic coords="8,87,96,54,35,276,28,184,36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Contrast uptake related quantitative metrics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Tumor segmentation (green overlay) on synthesized T1CE using real &amp; simulated low-dose in comparison to the tumor segmentation on ground truth T1CE. The corresponding Dice scores are also shown in the bottom. (Color figure online)</figDesc><graphic coords="9,73,80,54,38,276,70,63,34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>are suitable for such Iter 1 Iter 2 Iter i Iter k CE 1 CE i-1 CE k Gformer Gformer Gformer Gformer Post-contrast Low-dose Pre-contrast Pre- contrast Gformer Gformer Block CE i-1 CE i (a) (b) (c) Iterative Design .... Norm MHA Norm MLP (d) Transformer Gformer Block Gformer Block Gformer Block Gformer Block Gformer Block Gformer Block Transformer Subsampling Rotational Shift 3×3 Conv</head><label></label><figDesc></figDesc><table><row><cell>....</cell><cell></cell></row><row><cell>....</cell><cell>....</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>CE i CE: Contrast Enhancement</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Dataset cohort description</figDesc><table><row><cell>Site</cell><cell>Total</cell><cell>Gender</cell><cell>Age</cell><cell>Scanner</cell><cell>Field</cell><cell>TE</cell><cell>TR</cell><cell>Flip</cell></row><row><cell></cell><cell>Cases</cell><cell></cell><cell></cell><cell></cell><cell>Strength</cell><cell>(sec)</cell><cell>(sec)</cell><cell>Angle</cell></row><row><cell cols="2">Site A 126</cell><cell>55 Females</cell><cell cols="2">48 ± 16 Philips</cell><cell>3T</cell><cell>2.97-3.11</cell><cell>6.41-6.70</cell><cell>8 •</cell></row><row><cell></cell><cell></cell><cell>71 Males</cell><cell></cell><cell>Insignia</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Site B 159</cell><cell>78 Females</cell><cell cols="2">52 ± 17 GE</cell><cell>3T</cell><cell>2.99-5.17</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>81 Males</cell><cell></cell><cell>Discovery</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Quantitative evaluation results of different base methods on the test cases. Bold-faced numbers indicate the best results.</figDesc><table><row><cell>Method</cell><cell cols="2">Throughput PSNR (dB)↑ SSIM↑</cell><cell>RMSE↓</cell><cell>LPIPS↓</cell></row><row><cell>Post</cell><cell>-</cell><cell cols="3">33.93 ± 2.88 0.93 ± 0.03 0.34 ± 0.13 0.055 ± 0.016</cell></row><row><cell>Scaling</cell><cell>0.79 Im/s</cell><cell cols="3">38.41 ± 2.22 0.94 ± 0.19 0.20 ± 0.05 0.027 ± 0.015</cell></row><row><cell>Rednet</cell><cell>0.71 Im/s</cell><cell cols="3">40.07 ± 2.72 0.97 ± 0.01 0.17 ± 0.05 0.029 ± 0.009</cell></row><row><cell>Mapnn</cell><cell>0.71 Im/s</cell><cell cols="3">40.56 ± 1.64 0.96 ± 0.01 0.16 ± 0.05 0.023 ± 0.012</cell></row><row><cell>Restormer</cell><cell>0.65 Im/s</cell><cell cols="3">40.04 ± 2.27 0.95 ± 0.01 0.16 ± 0.16 0.038 ± 0.016</cell></row><row><cell>SwinIR</cell><cell>0.58 Im/s</cell><cell cols="3">40.93 ± 2.25 0.96 ± 0.01 0.15 ± 0.06 0.028 ± 0.015</cell></row><row><cell cols="2">Gformer*(Cyc) 0.69 Im/s</cell><cell cols="3">41.46 ± 2.14 0.97 ± 0.02 0.14 ± 0.04 0.021 ± 0.007</cell></row><row><cell cols="2">Gformer*(Rot) 0.65 Im/s</cell><cell cols="3">42.29 ± 0.02 0.98 ± 0.01 0.13 ± 0.03 0.017 ± 0.005</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A part of this work was presented as a poster in the conference of International Society for Magnetic Resonance in Medicine (ISMRM)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2023, held in Toronto.<ref type="bibr" target="#b1">2</ref> Refer Supplementary Material Fig.1(b) for examples on Spine MRI.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The dataset used in this study was obtained from Site A and B and are not available under a data sharing license.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>p &lt; 0.0001 (Wilcoxon signed rank test).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>p &lt; 0.0001 (Wilcoxon signed rank test).</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_9.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contrast-enhanced MRI: history and current recommendations</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Minton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Radiol</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="15" to="19" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gadolinium-a specific trigger for the development of nephrogenic fibrosing dermopathy and nephrogenic systemic fibrosis?</title>
		<author>
			<persName><forename type="first">T</forename><surname>Grobner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nephrol. Dial. Transplant</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1104" to="1108" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Anthropogenic gadolinium in freshwater and drinking water systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Brünjes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Water Res</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="page">115966</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Digital radiography: the balance between image quality and required radiation dose</title>
		<author>
			<persName><forename type="first">M</forename><surname>Uffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schaefer-Prokop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Radiol</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="202" to="208" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A generic deep learning model for reduced gadolinium dose in contrast-enhanced brain MRI</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pasumarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Tamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zaharchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Med</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1687" to="1700" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning enables reduced gadolinium dose for contrast-enhanced brain MRI</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wintermark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zaharchuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="330" to="340" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can virtual contrast enhancement in brain MRI replace gadolinium?: A feasibility study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleesiek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Radiol</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="653" to="660" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tracer kinetic modelling in MRI: estimating perfusion and capillary permeability</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sourbron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TED-Net: convolution-free T2T vision transformerbased encoder-decoder dilation network for low-dose CT denoising</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87589-3_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87589-3_43" />
	</analytic>
	<monogr>
		<title level="m">MLMI 2021</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Rekik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Yan</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12966</biblScope>
			<biblScope unit="page" from="416" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">One model to synthesize them all: Multi-contrast multi-scale transformer for missing data imputation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pasumarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zaharchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Datta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.13738</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face aging with conditional generative adversarial networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2089" to="2093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">IFR-Net: iterative feature refinement network for compressed sensing MRI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput. Imaging</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="434" to="446" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Competitive performance of a modularized deep neural network compared to commercial algorithms for low-dose CT image reconstruction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="269" to="276" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46475-6_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46475-6_43" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SimpleElastix: a user-friendly, multi-lingual library for medical image registration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Marstal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Berendsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="134" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automated brain extraction of multi-sequence MRI using artificial neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schell</surname></persName>
		</author>
		<idno>ECR 2019</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>European Congress of Radiology</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-dose CT with a residual encoder-decoder convolutional neural network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2524" to="2535" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Restormer: efficient transformer for high-resolution image restoration</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SwiniR: image restoration using swin transformer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Swin-Unet: Unet-like pure transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-25066-8_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-25066-8_9" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022 Workshops. ECCV 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">13803</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From dose reduction to contrast maximization: can deep learning amplify the impact of contrast media on brain magnetic resonance image quality? A reader study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bône</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Radiol</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="527" to="535" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3D MRI brain tumor segmentation using autoencoder regularization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-11726-9_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-11726-9_28" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2018</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Kuijf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Keyvan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Van Walsum</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11384</biblScope>
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Quantitative assessment of GD-DTPA contrast agent from signal enhancement: an in-vitro study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mørkenborg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stødkilde-Jørgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Djurhuus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frøkiaer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="637" to="643" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
