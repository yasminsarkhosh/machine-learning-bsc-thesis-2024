Header Number,Header Title,Text
1,Introduction,"Surgical scene reconstruction using stereo endoscopes is crucial to Robotic-Assisted Minimally Invasive Surgery (RAMIS). It aims to recover a 3D model of R. Zha and X. Cheng-Equal contribution. Fig.  the observed tissues from a stereo endoscope video. Compared with traditional 2D monitoring, 3D reconstruction offers notable advantages because it allows users to observe the surgical site from any viewpoint. Therefore, it dramatically benefits downstream medical applications such as surgical navigation  Existing approaches represent a 3D scene in two ways: discretely or continuously. Discrete representations include point clouds  Recently, continuous representations have become popular with the blossoming of neural fields, i.e., neural networks that take space-time inputs and return the required quantities. Neural-field-based methods  We propose EndoSurf: neural implicit fields for Endoscope-based Surf ace reconstruction, a novel neural-field-based method that effectively learns to represent dynamic scenes. Specifically, we model deformation, geometry, and appearance with three separate multi-layer perceptrons (MLP). The deformation network transforms points from the observation space to the canonical space. The  geometry network represents the canonical scene as an SDF field. Compared with density, SDF is more self-contained as it explicitly defines the surface as the zero-level set. We enforce the geometry network to learn a solid surface by designing various regularization strategies. Regarding the appearance network, we involve positions and normals as extra clues to disentangle the appearance from the geometry. Following [25], we adopt unbiased volume rendering to synthesize color images and depth maps. The network is optimized with gradient descent by minimizing the error between the real and rendered results. We evaluate EndoSurf quantitatively and qualitatively on public endoscope datasets. Our work demonstrates superior performance over existing solutions, especially in reconstructing smooth and accurate shapes."
2,Method,
2.1,Overview,"Problem Setting. Given a stereo video of deforming tissues, we aim to reconstruct the surface shape S and texture C. Similar to EndoNeRF  Here T stands for the total number of frames. I i ∈ R H×W ×3 and D i ∈ R H×W refer to the ith left RGB image and depth map with height H and width W . Foreground mask M i ∈ R H×W is utilized to exclude unwanted pixels, such as surgical tools, blood, and smoke. Projection matrix P i ∈ R 4×4 maps 3D coordinates to 2D pixels. t i = i/T is each frame's timestamp normalized to [0, 1]. While stereo matching, surgical tool tracking, and pose estimation are also practical clinical concerns, in this work we prioritize 3D reconstruction and thus take depth maps, foreground masks, and projection matrices as provided by software or hardware solutions. Pipeline. Figure "
2.2,EndoSurf: Representing Scenes as Deformable Neural Fields,"We represent a dynamic scene as canonical neural fields warped to an observed pose. Separating the learning of deformation and canonical shapes has been proven more effective than directly modeling dynamic shapes  In 3D vision, SDF is the orthogonal distance of a point x to a watertight object's surface, with the sign determined by whether or not x is outside the object. In our case, we slightly abuse the term SDF since we are interested in a segment of an object rather than the whole thing. We extend the definition of SDF by imagining that the surface of interest divides the surrounding space into two distinct regions, as shown in Fig.  Compared with the density field used in  Given a surface point p c ∈ R 3 in the canonical space, the surface normal n c ∈ R 3 is the gradient of the neural SDF field Ψ s : n c = ∇ Ψs (p c ). Normal n o of the deformed surface point p o can also be obtained with the chain rule. Neural Radiance Field. We model the appearance of the canonical scene as a neural radiance field Ψ r (x c , v c , n c , f ) → c c that returns the color c c ∈ R 3 of a viewpoint (x c , v c ). Unlike "
2.3,Optimization,"Unbiased Volume Rendering. Given a camera ray r(h) = o o + hv o at time t in the observed space, we sample N points x i in a hierarchical manner along this ray [25] and predict their SDFs ρ i and colors c i via EndoSurf. The color Ĉ and depth D of the ray can be approximated by unbiased volume rendering [25]: where ))/φ(ρ i ), 0) and φ(ρ) = (1 + e -ρ/s ) -1 . Note that s is a trainable standard deviation, which approaches zero as the network training converges. Loss. We train the network with two objectives: 1) to minimize the difference between the actual and rendered results and 2) to impose constraints on the neural SDF field such that it aligns with its definition. Accordingly, we design two categories of losses: rendering constraints and geometry constraints: where λ i=1,••• ,6 are balancing weights. The rendering constraints include the color reconstruction loss L color and depth reconstruction loss L depth : where M (r), { Ĉ, D}, {C, D} and R are ray masks, rendered colors and depths, real colors and depths, and ray batch, respectively. We regularize the neural SDF field Ψ s with four losses: Eikonal loss L eikonal , SDF loss L sdf , visibility loss L visible , and smoothness loss L smooth . (4) Here the Eikonal loss L eikonal "
3,Experiments,
3.1,Experiment Settings,"Datasets and Evaluation. We conduct experiments on two public endoscope datasets, namely ENDONERF  Our approach is compared with EndoNeRF  except those in the radiance field are 10 and 4 for location and direction, respectively. The SDF network is initialized "
3.2,Qualitative and Quantitative Results,As listed in Table  Figure  We present a qualitative ablation study on how geometry constraints can influence the reconstruction quality in Fig. 
4,Conclusion,"This paper presents a novel neural-field-based approach, called EndoSurf, to reconstruct the deforming surgical sites from stereo endoscope videos. Our approach overcomes the geometry limitations of prior work by utilizing a neural SDF field to represent the shape, which is constrained by customized regularization techniques. In addition, we employ neural deformation and radiance fields to model surface dynamics and appearance. To disentangle the appearance learning from geometry, we incorporate normals and locations as extra clues for the radiance field. Experiments on public datasets demonstrate that our method achieves state-of-the-art results compared with existing solutions, particularly in retrieving high-fidelity shapes."
,Fig. 2 .,
,,
,Fig. 4 .,
,Fig. 5 .,
,,
,Table 1 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 2.
1,Introduction,"Surgical treatment remains to this day the main approach to correct severe spinal deformities, which consists of rectifying the alignment of the spine over several Supported by the Canada Research Chairs and NSERC Discovery Grant RGPIN-2020-06558. vertebral bodies with intrinsic forces. However major limiting factors remain the loss in spinal mobility, as well as the increased risk of pain and osteoarthritis in years after surgery  To interpret the positioning effects during surgery, identifying features leading to reliable predictions of shape representations in postural changes is a deciding factor as shown in  Recently, Neural Radiance Fields (NeRF) "
2,Methods,
2.1,Intra-operative 3D Model from Multi-view X-Rays,"The first step of the pipeline consists of inferring a 3D model of the spine in the OR prior to instrumentation, using as input pair of orthogonal C-arm acquisitions I = {I 1 , I 2 } and previously generated pre-op 3D model  representing the m inter-vertebral transformations T i between consecutive vertebrae i and i+1, each consisting of 6 DoF with translation and rotation, expressed with recursive compositions. To accommodate with the Transformer's inability to explore and synthesize multi-view associations at deeper levels, the divergence decay is slowed down within the self-attention layers by augmenting the discrepancy in multi-view embeddings."
2.2,Learnable Shape Deformation Kernel,"We use a training set S of N spine models from a surgical population, where each spine model is articulated into pre-operative, prone and first-erect poses (K = 3), leading to a training set of L = N × K spine models from a population of scoliotic patients. We denote S n,k as the spine model n given a particular shape articulation k, where n ∈ {1, . . . , N}, and k ∈ {1, . . . , K}. We also define s i ∈ R 3 as a sample 3D point from a spine shape in S. We first train a kernel based on neural splines using the embeddings generated with a neural network  with [c : d] concatenating feature vectors c and d, representing the features of sample points i and j taken from a mesh model, respectively, and K NS representing Neural Spline kernel function. A Convolutional Occupancy Network  The neural network's implicit function is determined by finding coefficients α j associated for each point s j , such that: which solves a 2L × 2L linear system to obtain the standing articulations (y j ), with G(S, Ω) as the gram matrix, such that G(S, Ω) ij = K S,Ω (s i , s j ), with s i and s j are the input sample points of the spine models, and regularized by the λ parameter using the identity matrix I. For new feature points s, the regressed function is evaluated using α in the following term: which represents the ShapeNet function and maps feature points s from the original prone embedding onto the standing embedding."
2.3,Articulation SDF,"Each training shape S n,k is also associated with an articulation vector field, ξ ∈ R d . The shape embedding φ n is common across all instances n with various articulation vectors. We then train ArticulationNet, where for each instance, the shape code is maintained and each individual code for shape is updated. The articulation vector y i defined in Eq.(  The A-SDF for the spine shape provided by f θ is represented with an autodecoder model, which includes the trained shape kernel function f s , while f a describes ArticulationNet: with v ∈ R representing a value of the SDF on the output mesh, where the sign specifies if the sampled spine point falls within or outside the generated 3D surface model, described implicitly with the zero level-set f θ (.) = 0. At training, the ground-truth articulation vectors ξ are used to train the parameters of the model and the shape code. Sample points s are combined with the shape code φ to create a feature vector which is used as input to the shape embedding network. A similar process is used for the articulation code ξ, which are also concatenated with sample points s. This vector is used as input to the articulation network in order to predict for each 3D point s the value of the SDF. Hence, the network uses a fully connected layer at the beginning to map the vector in the latent space, while a classification module is added in the final hidden layer to output the series of vertebrae. We define the loss function using the L1 distance term, regressing the SDF values for the M number of 3D points describing each spine shape using the function f θ : where s m ∈ S is a sample point from the shape space, and v m is the SDF value used as ground-truth for m ∈ {1, . . . , M}. The second loss term evaluates the vertebral constellation and alignment with the inter-vertebral transformations: with p m identifying the vertebral level for sample s m and CE represents the cross-entropy loss. This classification loss enables to separate the constellation of vertebral shapes s m . To ensure the predicted shape is anatomically consistent, a regularization term L r is used to ensure generated models fall near a continuous piecewisegeodesic trajectory defined in a spatio-temporal domain. The regularization term integrates pre-generated embeddings capturing the spatiotemporal changes of the spine geometry following surgery  with β p and β θ weighting the parts and regularization terms, respectively. During training, a random initialization of the shape codes based on Gaussian distributions is used. An optimization procedure is then applied during training for each shape code, which are used for all instances of articulations. Hence the global objective seeks to minimize for all N × K shapes, the following energy: At test time, the intra-operative spine model (obtained in 2.1) is given as input, generating the articulated standing pose, with shape and articulation vectors, using back-propagation. The shape and articulation codes φ and ξ are randomly initialized while the network parameters stay fixed. To overcome divergence problems and avoid a local minima for Eq. ( "
3,Experiments,"A mono-centric dataset of 735 spine models was used to train the articulated neural kernel field forecasting model. The dataset included pre-, intra-and postoperative thoraco/lumbar 3D models. Pre-and post-op models were generated from bi-planar X-rays using a stereo-reconstruction method (EOS system, Paris, France), integrating a semi-supervised approach generating vertebral landmarks "
4,Conclusion,"In this paper, we proposed an online forecasting model predicting the first-erect spine shape based on intra-operative positioning in the OR, capturing the articulated shape constellation changes between the prone and the standing posture with neural kernel fields and an articulation network. Geometric consistency is integrated with the network's training with a pre-trained spine correction geodesic trajectory model used to regularize outputs of ArticulationNet. The model yielded results comparable to ground-truth first-erect 3D geometries in upright positions, based on statistical tests. The neural field network implicitly captures the physiological changes in pose, which can be helpful for planning the optimal posture during spine surgery. Future work will entail evaluating the model in a multi-center study to evaluate the predictive robustness and integrate the tool for real-time applications."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,,
1,Introduction,"Tracking of interventional devices plays an important role in aiding surgeons during catheterized interventions such as percutaneous coronary interventions (PCI), cardiac electrophysiology (EP), or trans arterial chemoembolization (TACE). In cardiac image-guided interventions, surgeons can benefit from visual guidance provided by mapping vessel information from angiography (Fig.  For general computer vision applications, transformer  To overcome the limitations of existing works, we propose a generic, end-toend model for target object tracking with both spatial and temporal context. Multiple template images (containing the target) and a search image (where we would identify the target location, usually the current frame) are input to the system. The system first passes them through a feature encoding network to encode them into the same feature space. Next, the features of template and search are fused together by a fusion network, i.e., a vision transformer. The fusion model builds complete associations between the template feature and search feature and identifies the features of the highest association. The fused features are then used for target (catheter tip) and context prediction (catheter body). While this module learns to perform these two tasks together, spatial context information is offered implicitly to provide guidance to the target detection. In addition to the spatial context, the proposed framework also leverages the temporal context information which is generated using a motion flow network. This temporal information helps in further refining the target location. Our main contributions are as follows: 1) Proposed network consists of segmentation branch that provides spatial context for accurate tip prediction; 2) Temporal information is provided by computing the optical flow between adjacent frames that helps in refining the prediction; 3) We incorporate dynamic templates to make the model robust to appearance changes along with the initial template frame that helps in recovery in case of any misdetection; 4) To the best of our knowledge, this is the first transformer-based tracker for real-time  device tracking in medical applications; 5) We conduct numerical experiments and demonstrate the effectiveness of the proposed model in comparison to other state-of-the-art tracking models."
2,Methodology,"Given a sequence of consecutive X-ray images {I t } n t=0 and an initial location of the target catheter tip x 0 = (u 0 , v 0 ), our goal is to track the location of the target x t = (u t , v t ) at any time t, t > 0. The proposed model framework is summarized in Fig. "
2.1,Target Localization with Multi-template Feature Fusion,"To identify the target in the search frame, existing approaches build a correlation map between the template and search features. Limited by definition, the template is a single image, either static or from the last frame tracked result. A transformer naturally extends the bipartite relation between template and search images to complete feature associations which allow us to use multiple templates. This improves model robustness against suboptimal template selection which can be caused by target appearance changes or occlusion. Feature Fusion with Multi-head Attention. In the encoding stage, given a set of template image patches centered around the target {T ti } ti∈H and current frame I s as the search image, we aim to determine the target location by fusing information from multiple templates. H is the set containing historically selected frames for templates. This can be naturally accomplished by multi-head attention (MHA). Specifically, let us denote the ResNet encoder by θ, given the feature map of the search image θ(I s ) ∈ R C×Hs×Ws , and the feature maps of the templates {θ(T ti )}}, we use 1 × 1 convolutions to project and flatten them into d-dimensional vector query, key and value embedding, q s , k s , v s for the search image features and {q ti }, {k ti }, {v ti } for templates features respectively. The attention is based on the concatenated vectors, where Q = Concat(q s , q t1 , q t2 , ..., q tn ), The definition of MHA then follows  Joint Target Localization and Context Segmentation. In the decoding stage, we follow  As the catheter tip represents a sparse object in the image, solely detecting it suffers from class imbalance issue. To guide the catheter tip tracking with spatial information, we incorporate additional contextual information by simultaneously segmenting the catheter body in the same frame. Specifically, two object queries (e 1 , e 2 ) are employed in the decoder, where e 1 defines the position of the catheter tip, and e 2 defines the mask of the catheter body. As is illustrated in Fig.  where x i , m i represent the ground truth annotation of the catheter tip and mask, xs i , ms i are predictions respectively. Here we use sup-script ""s"" to denote the predictions from this spatial stage. G(x i ; μ, σ) := exp(-x iμ 2 /σ 2 ) is the smoothing function that transfers dot location of x i to probability map. λ * bce , λ * dice ∈ R are hyperparameters that are empirically optimized."
2.2,Localization Refinement with Context Flow,"In interventional procedures, one common challenge for visual tracking comes from occlusion. This can be caused by injected contrast medium (in the angiographic image) or interferring devices such as sternal wires, stent and additional guiding catheters. If the target is occluded in the search image, using only spatial information for localization is inadequate. To address this challenge, we impose a motion prior of the target to further refine the tracked location. As the target is a sparse object, this is done via optical flow estimation of the context."
,Context Flow Estimation.,"Obtaining ground truth optical flow in real world data is a challenging task and may require additional hardware such as motion sensors. As such, training a model for optical flow estimation directly in the image space is difficult. Instead, we propose to estimate the flow in the segmentation space, i.e., on the predicted heatmaps of the catheter body between neighboring frames. We use the RAFT  Here corr(g θ (m t-1 ), g θ (m t )) ∈ R H f ×W f ×H f ×W f stands for correlation evaluation: which can be computed via matrix multiplication. Starting with an initial flow f 0 = 0, we follow the same model setup as  We note here that since the segmentations of the catheter body are sparse objects compared to the entire image, computation of the correlation volume and subsequent updates can be restricted to a cropped sub-image which reduces computation cost and flow inference time. As the flow estimation is performed on the segmentation map, one can simply generate synthetic flows and warp them with the existing catheter body annotation to generate data for model training. Refinement with Combined Spatial-temporal Prediction. Finally, we generate a score map with combined information from the spatial localization stage and the temporal prediction by context flow, Here α is a positive scalar. It helps the score map to promote coordinates that are activated jointly on both the spatial prediction xs t and the temporal prediction xf t . Finally, we forward the score map through a refinement module to finalize the prediction. The refinement module consists of a stack of 3 convolutional layers. Similar to the spatial localization stage, a combination of the binary cross-entropy and the dice loss is used as the final loss."
3,Experiments and Results,"Dataset. Our study uses an internal dataset of X-ray sequences captured during percutaneous coronary intervention procedures, featuring a field of view displaying the catheter within the patient's heart. The test dataset is divided into two primary categories: fluoroscopic and angiographic sequences. Fluoroscopic sequences are real-time videos of internal movements captured by low-dose Xrays without radiopaque substances, while angiographic sequences display blood vessels in real-time after the introduction of radiopaque substances. We further separate the test dataset into a third category, ""devices"", presenting a unique challenge for both fluoroscopic and angiographic sequences. In these cases, devices such as wires can obscure the catheter tip and have a similar appearance to the catheter, making tracking more challenging. The dataset includes frames annotated with the coordinates of the catheter tip and, in some cases, a catheter body mask annotation. For training and validation, we use 2,314 sequences consisting of 198,993 frames, of which 44,957 are annotated. As the model training only requires image pairs, i.e.templates and search images, in order to reduce annotation effort, a nonadjacent subset of frames in each sequence is annotated. Their neighboring unannotated frames are also used to provide flow estimation, as is shown in Fig.  Training. The template frame is of size 64 × 64. The search frame is of size 160 × 160. With this, the inference speed reaches 12 fps. We train our model for 300 epochs using a learning rate of 0.0001. Comparison Study. We compare the proposed approach with existing arts and summarize the results in Table  Overall, ConTrack outperforms all other methods, with a median tracking error of less than 1.08 mm. Our model is particularly effective at tracking the catheter tip when other devices are in the field of view, where all other methods tend to underperform. Compared to Cycle Ynet on all test datasets, our model is 45% more accurate, with an average distance of less than 1mm between the prediction and ground truth. Further, we show the accuracy distributions in Fig.  Ablation Study. We conduct an ablation study to investigate the effectiveness of different model components. Results are summarized in Table "
4,Conclusion,"Device tracking is an important task in interventional procedures. In this paper, we propose a generic model framework, ConTrack, that leverages both spatial and temporal information of the surrounding context for accurate target localization and tracking in X-ray. Through extensive experimentation on large datasets, our approach demonstrated superior tracking performance, outperforming other state-of-the-art tracking models, especially in challenging scenarios where occlusions and distractors are present. Current approach has its limitations. Motion estimation is learned from neighboring two frames and thus target historical trajectory information is missing. Further, transformer-based model training require large amount of annotated data, which is challenging to collect in interventional applications. Finally, throughout the paper we follow established setups and focus on the development on the tracking model with manual initialization. In general, long-term visual tracking with automatic (re-)initialization is a challenging problem and require a system of approaches. A safe and automatic system of device and anatomy tracking is of great clinical relevance and will be an important future work for us. Disclaimer. The concepts and information presented in this paper/presentation are based on research results that are not commercially available. Future commercial availability cannot be guaranteed."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Table 1 .,
,Table 2 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 65.
1,Introduction,"In cardiology, endovascular microcatheters are widely used to provide sensing or interventional imaging for diagnostic and therapeutic applications. One example is a pressuresensing microcatheter, which measures blood pressure waveforms within coronary arteries to assess the severity of a stenosis and thereby to guide decisions about stent deployment. A ""rapid exchange"" microcatheter has a lumen in its distal section that allows it to be delivered over a guidewire positioned within the patient's vasculature. Rapidexchange microcatheters are typically guided to their target destination with fluoroscopic (X-ray) imaging. The use of fluoroscopic guidance has several disadvantages, including exposure of the patient and clinician to X-rays, back pain experienced by practitioners from wearing heavy X-ray protective aprons, and the need for X-ray imaging systems that are not always available in resource-constrained environments. Across a wide range of cardiovascular applications, it is of significant interest to explore alternatives to fluoroscopic guidance of microcatheters. Ultrasound (US) tracking is an emerging method for localizing medical devices within the body that involves ultrasonic communication between the device and an external imaging system. This method can be performed in ""receive-mode"" with an ultrasound sensor in the device that receives transmissions from the imaging probe; the time delays between transmission and reception are processed to obtain estimates of the sensor position in 2D  US tracking of microcatheters would potentially enable ultrasound imaging to be used in place of X-ray imaging for guidance, particularly in applications where there are unobstructed ultrasonic views of the vasculature; these devices typically have very low echogenicity due to their small dimensions and polymeric construction. To the authors' knowledge, US microcatheter tracking has not previously been performed. This endeavor leads to several questions that are addressed in this study: first, how can a fiber optic sensor that was originally developed for blood pressure sensing be adapted to obtain concurrent ultrasound signals; second, how does spatial resolution depend on the angular orientation and spatial position of microcatheter; third, how does this combination perform within a clinically realistic environment? In this study, we address these questions, with validation in a porcine model in vivo."
2,Materials and Methods,
2.1,Ultrasonic Tracking and Concurrent Pressure Sensing System,"The ultrasonic tracking system comprised three components: a clinical US imaging system (SonixMDP, Ultrasonix, Canada) with an external 2-D linear array probe (L14-5; 128 elements), a coronary microcatheter with the integrated pressure/ultrasound sensor (as described in Sect. 2.2), and an US tracking console. The tracking console interrogated the fiber optic sensor in the microcatheter to receive transmissions from the array probe, and obtained processed B-mode US images and two triggers (start of each B-mode frame; start of each A-line) from the US imaging system. The US signals received by the fiber optic sensor were parsed according to the onsets of the A-lines using the acquired triggers, and concatenated to create a 2-D tracking image of the sensor that was inherently co-registered to the corresponding B-mode US images. Envelope detection of the US signals was performed with a Hilbert transform. To localize the sensor, a region of interest (9 mm × 9 mm) was selected from the tracking image, which was centered on the maximum value. After zeroing values less than 70% of this maximum value (an empirically-obtained threshold value), the sensor position was calculated as the location of the center of mass within this region. The coordinates of the catheter tip were then superimposed on the acquired US images frame-by-frame to show the tracked position of the catheter tip. Automatic electronic focusing of the B-mode images was performed in order to improve the lateral tracking accuracy "
2.2,Sensor and Microcatheter,"The sensor comprised a cylindrical capillary structure (diameter: 250 µm; length: 1 mm) with an inorganic membrane at the distal end. This membrane was separated by an air gap from the distal end of a single mode fiber, thereby creating a low-finesse Fabry-Pérot (F-P) cavity. As the blood pressure increases, the membrane is deflected inward, and viceversa; this deflection is measured using phase-sensitive low-coherence interferometry with a broadband light source  For US tracking, the sensor was interrogated concurrently with a wavelength that was continuously tuned so that the change of reflectivity with membrane deflection was maximized "
2.3,Relative Tracking Accuracy,The relative tracking accuracy of the system was evaluated on the benchtop with the microcatheter immersed in a water tank. With the sensor and the surrounding region of Fig. 
2.4,Impact of Microcatheter Orientation,"To determine the extent to which the hyperechoic guidewire within the central lumen of the microcatheter shadows ultrasound transmissions by the imaging probe, the signal-tonoise ratio (SNR) of the tracking signals was measured for different axial orientations. These orientations included 0°, where the sensor has a smaller depth than the guidewire and one could expect an absence of shadowing, and 180°, where the sensor has a larger depth so that the guidewire is directly above it and one could expect maximal shadowing. The catheter tip was positioned on a mount at different orientations, with the sensor in the imaging plane at a depth of the 4.5 cm. At each orientation angle, the tracking signals were recorded and the mean SNR was estimated over 100 US tracking frames. For the SNR calculation, the signal was taken as the maximum within the tracking frame; the noise was estimated as the standard deviation from deeper regions of the frame for which signals were visually absent."
2.5,In Vivo Validation,"An initial clinical assessment of the system was performed with a swine model in vivo. All procedures on animals were conducted in accordance with U.K. Home Office regulations and the Guidance for the Operation of Animals (Scientific Procedures) Act (1986). Ethics approval was provided by the joint animal studies committee of the Griffin Institute and the University College London, United Kingdom. Following arterial access, a coronary guidewire was positioned into the right femoral artery, with guidance from the external ultrasound imaging probe. The microcatheter was subsequently inserted over the guidewire and 250 US tracking frames were acquired, while the microcatheter was moved inside the artery. The microcatheter was then removed and the guidewire was positioned in a renal artery with fluoroscopic guidance. A guide catheter delivered over the guidewire allowed for injections of radio-opaque contrast for locating the renal artery. With the guide catheter removed, the microcatheter was then advanced over the guidewire into the renal artery and another 250 tracking frames were acquired with the US imaging probe mechanically fixed in position. Concurrent blood pressure data were obtained from the renal artery."
3,Results and Discussion,"The relative spatial locations of the tracked and actual sensor positions in water were in good agreement for all sensor positions (Fig.  The SNR of the US tracking signals varied with the orientation of the microcatheter. The axial orientation was changed from 0°to 270°in steps of 90°(Fig.  When the microcatheter was in the femoral artery of the swine (depth range: 10 to 20 mm) and also in the imaging plane, SNR values as high as 72 were obtained. As the microcatheter was advanced along the guidewire inside the artery, the tracking images each had singular locations from which strong signals were obtained, which corresponded to received transmissions from the US imaging probe (Fig.  When the microcatheter was in the renal artery (depth range: 30 to 35 mm), SNR values as high as 19 were obtained (Fig.  To the authors' knowledge, this is the first study in which US tracking of a rapidexchange microcatheter was performed, and also the first in which concurrent ultrasound and invasive blood pressure measurements were obtained with a single fiber optic sensor. The multimodal ultrasound/pressure sensing capability that was achieved with one optical fiber could be critically important for vessels with small lumens, to minimize the complexity, size, and flexibility of the device. With its diminutive size, this sensor could be readily incorporated into a wide range of cardiovascular devices and could find widespread utility in cardiovascular medicine. In addition to intracoronary sensing, it could be used for US tracking during endovascular repair of the tricuspid valve, where visualizing therapeutic devices with a transesophageal probe is very challenging, and during renal denervation procedures. This tracking technology is compatible with other types of US transducers than the one used here, such as curvilinear and phased array probes. Future optimizations to the sensor could include reflective surfaces on the distal end of the fiber and the pressure-sensing membrane to increase the ultrasound sensitivity via an increase in optical finesse. Upstream, diffuse delivery of light and temperature measurements from the sensor will enable invasive flow measurements  The system presented here has the advantage of providing tracking images in which signals derive solely from the sensor, which are inherently co-registered with the Bmode US images. The sub-mm tracking accuracy of the system was similar to those previously obtained with receive-and transmit-mode US tracking of medical needles "
,Fig. 3 .,
,,
1,Introduction,"Orthognathic surgery corrects facial skeletal deformities that may cause functional and aesthetic impairments. In orthognathic surgical procedures, the jaws are cut into several bony segments and repositioned to desired locations to achieve an ideal alignment. Surgeons plan the movement of osteotomized bony segments before surgery to obtain the best surgical outcome. While surgeons do not operate on the soft tissue of the face during the procedure, it is passively moved by the underlying bone, causing a change in facial appearance. To visualize the proposed outcome, simulations of the planned procedure may be performed to predict the final facial tissue appearance. Current simulation techniques use the finite element method (FEM) to estimate the change of the facial tissue caused by the movement of the bony segments  A simplistic approach to performing incremental simulations using DL is to break a large simulation into smaller steps and perform them in sequential order, but independently. However, this does not allow the model to correct errors, causing them to accumulate over incremental steps. One previous work implemented a data augmentation strategy to minimize network error in incremental simulations of soft tissue  In this study, we hypothesize that utilizing both spatial and temporal information for incremental simulations can improve facial prediction accuracy over DL networks that perform single-step simulations or incremental simulations while only considering spatial information. Therefore, the purpose of this study is to introduce a spatiotemporal deep learning approach for incremental biomechanics simulation of soft tissue deformation. We designed a network that learns spatial features from incremental simulations and aggregates them across multiple incremental simulations to establish sequential continuity. The contributions of this work are (1) a method for combining spatial and temporal learning in an incremental manner (2) a method for performing incremental simulations while preserving knowledge from previous increments. Our proposed method successfully implements spatiotemporal learning by observing temporal trends from spatial features while minimizing computational complexity. "
2,Method,The Spatiotemporal Incremental Mechanics Modeling (SIMM) method predicts the incremental deformation of a three-dimensional (3D) soft tissue mesh based on incrementally planned input displacement. The SIMM network consists of two main operations: 1) spatial feature encoding and 2) temporal aggregation of spatial features (Fig. 
2.1,Data Representation,"The SIMM method uses an incremental simulation scheme, shown in Fig.  and geometric information from an input facial mesh m F are fed to the spatial sub-network to predict the incremental facial deformation d (t) F,s . The geometric information from the input facial mesh consists of an adjacency matrix a F and a set of edge weights w (t) F . The adjacency matrix is a binary matrix describing the connections between nodes. The edge weights are calculated as the inverse of the Euclidean distance between nodes, providing the network with spatial information. Spatial features x s are extracted in each GNN layer and aggregated using jumping-knowledge connections "
2.2,Network,"Spatial sub-network: We used a PhysGNN network to learn the spatial features  where y s is the spatially aggregated features, and x (l) are the features from layers 0 to l. f aggr can be any form of aggregation function, such as concatenation. In this work, we use a long-short-term memory (LSTM) attention aggregation mechanism to achieve optimal results, which has been demonstrated in our ablation study (Sect. 3.4) "
,Temporal sub-network:,"To capture temporal trends, we implemented additional aggregation layers to process features across a sequence of incremental timepoints. Instead of aggregating features across several spatial ""neighborhoods"" as seen in Eq. (  where y t is the temporally aggregated features and x (t) are the features to be aggregated across timepoints (t) to (t -N ). We hypothesized that including more than one previous timepoint in the aggregation layer may improve the network performance (see Sect. 3.4). We used the same LSTM-attention aggregation mechanism as used in the PhysGNN spatial sub-network. Training Strategy: The SIMM network produces two predictions of the incremental tissue deformation, one from the spatial sub-network and the other after spatiotemporal aggregation (Fig.  where l SIMM is the total loss of the SIMM network, l s is the loss of the spatial sub-network predicted deformation d F,s , and l st is the loss of the spatiotemporal predicted deformation d (t) F,st . The combined loss was used to train the network to ensure the spatial sub-network still learns to adequately encode the spatial features."
3,Experiments and Results,
3.1,Dataset,The SIMM method was evaluated on a dataset of incremental FEM simulations from 18 subjects using a leave-one-out cross-validation (LOOCV). The subjects were chosen from our digital archive of patients who underwent doublejaw orthognathic surgery (IRB# PRO00008890). FEM meshes were generated from CT scans and FE simulations were performed using an existing clinical pipeline 
3.2,Implementation Details and Evaluation Metrics,"All networks were trained in pytorch using the Adam optimizer with an initial learning rate of 5e-3 for 100 epochs on an NVIDIA Tesla V100. We trained all networks in leave-one-out cross-validation across all 18 subjects. The loss was calculated as the mean squared error between the network-predicted and FEM-predicted deformation vectors for each node within the facial mesh. The final accuracy was calculated as the Euclidean distance between the networkpredicted and FEM-predicted node coordinates on the final postoperative face. The distribution of the Euclidean distances was not found to be normal using the Kolmogorov-Smirnov test, so statistical significance between methods was tested using the Wilcoxon signed-rank test and a p-value of 0.05."
3.3,Comparison with Baselines,"We compared our SIMM method with two baseline methods: 1) a single-step method, and 2) an incremental method. For the single-step method, we trained a PhysGNN network to predict the total facial deformation in a single step. The same sub-simulations used in the incremental and SIMM methods (see Sect. 3.1) were used to train the single-step network, however, all intermediate timepoints between the first and final timepoints were removed. For the incremental method, we used PhysGNN to perform incremental simulations. The incremental method used the prediction from timepoint t-1 to update the edge weights in timepoint t, similar to the feedback loop in SIMM (Fig.  Our SIMM method achieved a mean error of 0.42 mm on all subjects (Table "
,Single-step,Incremental SIMM 332 nodes >1mm error 230 nodes >1mm error 130 nodes >1mm error Fig. 
3.4,Ablation Studies,"We also performed ablation studies to investigate the effects of several key components in the SIMM method. First, when splitting subject simulations into sub-simulations, we varied the maximum deformation threshold d max to 1.0, 0.5, and 0.1 mm. We found a d max of 0.5 achieved the best performance in the incremental method, although the best d max may change for different cases (Tab. S2). Second, we investigated the effect of the type of aggregation mechanism used in the PhysGNN network of the incremental method by replacing the LSTM-attention aggregation mechanism with a concatenation aggregation mechanism. The mean error increased to 1.25 mm when using a concatenation aggregation. Third, we tried increasing the number of previous timepoints in the spatial features stack to be used in the temporal aggregation layers from 1 previous timepoint to 5 previous timepoints. We hypothesized that including multiple previous timepoint in the aggregation layer may improve performance. The mean error increased to 0.44 mm when using 5 previous timepoints."
4,Discussions and Conclusions,"The SIMM method achieved a lower mean error than the single-step and incremental methods, as seen in the quantitative results (Table  In conclusion, we successfully created a spatiotemporal incremental mechanics modeling (SIMM) method for simulating incremental facial deformation for surgical planning. The SIMM method shows potential to improve simulation accuracy over methods based on spatial information alone, suggesting the importance of spatiotemporal learning for incremental simulations."
,Fig. 1 .,
,Fig. 2 .,
,,
,Table 1 .,
1,Introduction,"A difficulty faced by surgeons performing endoscopic pituitary surgery is identifying the areas of the bone which are safe to open. This is of particular importance during the sellar phase as there are several critical anatomical structures within close proximity of each other  To solve both tasks simultaneously, PAINet (Pituitary Anatomy Identification Network) is proposed. This paper's contribution is therefore: 1. The automated identification of the ten critical anatomical structures in the sellar phase of endoscopic pituitary surgery. To the best of the authors' knowledge, this is the first work addressing the problem at this granularity. 2. The creation of PAINet, a multi-task neural network capable of simultaneously semantic segmentation and centroid detection of numerous anatomical structures within minimally invasive surgery. PAINet uniquely utilises two loss functions for improved performance over single-task neural networks due to the increased information gain from the complementary task."
2,Related Work,Encoder-decoder architectures are the leading models in semantic segmentation and landmark detection  The most clinically similar works to this paper are: 
3,Methods,"PAINet: A multi-task encoder-decoder network is proposed to improve performance by exchanging information between the semantic segmentation and centroid detection tasks. EfficientNetB3, pre-trained on ImageNet, is used as the encoder because of its accuracy, computational efficiency and proven generalisation capabilities  Semantic Segmentation: First, single-class sella segmentation models were trialed. 8-encoders (pre-trained convolution neural networks) and 15-decoders were used, with their selection based off architecture variety. Two loss functions were also used: (1) distribution-based logits cross-entropy; and (2) region-based Jaccard loss. Boundary-based loss functions were not trialed as: (1) the boundary of the segmentation masks are not well-defined; and (2) in the cases of split structures (Fig.  For multi-class sella and clival recess segmentation, the optimal single-class model was extended by: (1) sending through each class to the loss function separately (multi-class separate); and (2) sending both classes through together (multi-class together). An extension of logits cross-entropy, logits focal loss, was used instead as it accounts for data imbalance between classes. Centroid Detection: 5-models were trialed: 3-models consisted of encoders with a convolution layer and linear activation; and 2-models consisted of encoderdecoders with an average pooling layer and sigmoid activation with 0.3 dropout. Two distance-based loss functions were trialed: (1) mean squared error (MSE); and (2) mean absolute error (MAE). Loss was calculated for all structures simultaneously as a 16 dimensional output (8 centroids × 2 coordinates) and set to 0 for a structure if ground-truth centroids of that structure was not present."
4,Experimental Setup,"Evaluation Metrics: For sella segmentation the evaluation metric is intersection over union (IoU), as commonly used in the field  For centroid detection, the evaluation metric is mean percentage of correct keypoints (MPCK) with the threshold set to 20%, indicating the mean number of predicted centroids falling within 144 pixels of the ground-truth centroid. This is commonly used in anatomical detection tasks as it ensures the predictions are close to the ground-truth while limiting overfitting  Network Parameters: 5-fold cross-validation was implemented with no holdout testing. To account for structure data imbalance, images were randomly split such that the number of structures in each fold is approximately even. Images from a singular video were present in either the training or validation dataset. Each model was run for with a batch size of 5 for 20 epochs, where the epoch with the best primary evaluation metric on the validation dataset was kept. The optimising method was Adam with varying initial learning rates, with a separate optimiser for each loss function during multi-task training. All images were scaled to 736 × 1280 pixels for model compatibility, and training images were randomly augmented within the following parameters: shift in any direction by up to 10%; zooming in or out about the image center by up to 10%; rotation about the image center clockwise or anticlockwise by up to π/6; increasing or decreasing brightness, contrast, saturation, and hue by up to 10%. The code is written in Python 3.8 using PyTorch 1.8.1, run on a single NVIDIA Tesla V100 Tensor Core 32-GB GPU using CUDA 11.2, and is available at https://github.com/dreets/pitnet-anat-public. For PAINet, a batch size of 5 utilised 29-GB and the runtime was approximately 5-min per epoch. Valuation runtime is under 0.1-s per image and therefore a real-time overlay on-top of the endoscope video feed is feasible intra-operatively."
5,Dataset Description,Images: Images come from 64-videos of endoscopic pituitary surgery where the sellar phase is present  The sella is present in all 635-images (Fig. 
6,Results and Discussion,"Quantitative evaluation is calculated for: single-class sella segmentation (Table  The optimal model for single-class sella segmentation achieved 65.4% IoU, utilising an EfficientNetB3 encoder; U-Net++ decoder; Jaccard loss; and a 0.001 initial learning rate. Reductions in IoU are seen when alternative parameters are used, highlighting their impact on model performance. Using the optimal sella model configuration, 53.4% IoU is achieved for singleclass clival recess segmentation. Extending this to multi-class and PAINet training improves both sella and clival recess IoU to 66.1% and 54.1% respectively. The optimal model for centroid detection achieves 51.7% MPCK-20%, with minor deviations during model parameter changes. This model, ResNet18 with MSE loss, outperforms the more sophisticated models, as these models over-learn image features in the training dataset. However, PAINet leverages the additional information from segmentation masks to achieve an improved 53.2%. The per structure PCK-20% indicate performance is positively correlated with the number of images where the structure is present. This implies the limiting factor is the number of images rather than architectural design.   Qualitative predictions of the best performing model, PAINet, are displayed in Fig. "
7,Conclusion,"Identification of critical anatomical structures by neurosurgeons during endoscopic pituitary surgery remains a challenging task. In this paper, the potential of automating anatomical structure identification during surgery was shown. The proposed multi-task network, PAINet, designed to incorporate identification of both large prominent structures and numerous smaller less prominent structures, was trained on images of the sellar phase of endoscopic pituitary surgery. Using 635-images from 64-surgeries annotated by expert neurosurgeons and various model configurations, the robustness of the PAINet was shown over single task networks. PAINet achieved 66.1% (+0.7%) and 54.1% IoU (+0.7%) for sella and clival recess segmentation respectively, a higher performance than other minimally invasive surgeries "
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Table 1 .,
,Table 2 .,
,Table 3 .,
,Table 4 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_45.
1,Introduction,"Needle-based liver tumor ablation techniques (e.g., radiofrequency, microwave, laser, cryoablation) have a great potential for local curative tumor control  In standard clinical settings, the insertion of each needle requires multiple check points during its progression, fine-tune maneuvers, and eventual repositioning. This leads to multiple CT acquisitions to control the progression of the needle with respect to the vessels, the target, and other sensible structures  To address this challenge, two main strategies could be considered: image fusion and image processing techniques. Image fusion typically relies on the estimation of rigid or non-rigid transformations between 2 images, to bring into the intraoperative image structures of interest only visible in the preoperative data. This process is often described as an optimization problem  On the other hand, deep learning techniques have proven to be very efficient at solving image processing challenges  In this paper we propose an alternative approach, where a neural network learns local image features in a NCCT image by leveraging the known preoperative vessel tree geometry and topology extracted from a matching (undeformed) CCT. Then, the augmented CT is generated by fusing the deformed vascular tree with the non-contrasted intraoperative CT. Section 2 presents the method and its integration in the medical workflow. Section 3 presents and discusses the results, and finally we conclude in Sect. 4 and highlight some perspectives."
2,Method,"In this section, we present our method and its compatibility with current clinical workflows. A few days or a week before the intervention, a preoperative diagnostic multiphase contrast-enhanced image (MPCECT) is acquired (Fig. "
2.1,Vessel Map Extraction,"We call Vessel Map (VM) the region of interest defining the vascular tree in the NCCT. Since vascular structures are not visible in non-contrasted images, the extraction of this map is done by segmenting the CCT and then using this segmentation as a mask in the NCCT. Mathematical morphology operators, in particular a dilation operation "
2.2,Data Augmentation,"The preoperative MPCECT provides a couple of registered NCCT and CCT images. This is obviously not sufficient for training purposes, as they do not represent the possible soft tissue deformation that may occur during the procedure. Therefore, we augment the data set by applying multiple random deformations to the original images. Random deformations are created by considering a predefined set of control points for which we define a displacement field with a random normal distribution. The displacement field of the full volume is then obtained by linearly interpolating the control points' displacement field to the rest of the volume. All the deformations are created using the same number of control points and characteristics of the normal distributions."
2.3,Neural Network,Predicting the vascular tree location in the deformed intraoperative NCCT is done using a U-net 
2.4,Augmented CT,"Once the network has been trained on the patient-specific preoperative data, the next step is to augment and visualize the intraoperative NCCT. This is done in 3 steps: -The dilatation operations introduced in Sect. 2.1 are not reversible (i.e. the segmented vessel tree cannot be recovered from the VM by applying the same number of erosion operations). Also, neighboring branches in the vessel tree could end up being fused, thus changing the topology of the vessel map. Therefore, to retrieve the correct segmented (yet deformed) vascular tree, we compute a displacement field between the pre-and intraoperative VMs. This is done with the Elastix library  Fig. "
3,Results and Discussion,
3.1,Dataset and Implementation Details,"To validate our approach, 4 couples of MPCECT abdominal porcine images were acquired from 4 different subjects. For a given subject, each couple corresponds to a preoperative and an intraoperative MPCECT. We recall that an MPCECT contains a set of registered NCCT and CCT images. These images are then cropped and down-sampled to 256 × 256 × 256, and the voxels intensities are scaled between 0 and 255. Finally, we extract the VM from each MPCECT sample and apply 3 dilation operations, which demonstrated the best performance in terms of prediction accuracy and robustness on our data. We note that public data sets such as DeepLesion "
3.2,Results,"To assess our method, we use a dice score to measure the overlap between our predicted segmentation and the ground truth. Being a commonly used metric for segmentation problems, Dice aligns the nature of our problem as well as the clinical impact of our solution. We ha performed tests on 4 different (porcine) data sets. Results are reported in Table "
3.3,Ablation Study and Additional Results,"Vessel Map: We have removed the VM from the network input to demonstrate its impact on our results. Using the data of the Subject 1, a U-net was trained to segment the vessel tree of the intraoperative NCCT image. The network only managed to segment a small portion of the main portal vein branch. Thus, achieving a dice score of 0.16 vs 0.79 when adding the preoperative VM as additional input. We also studied the influence of the diffusion kernel applied to the initial segmentation. We have seen, on our experimental data, that 3 dilation operations were sufficient to compensate for the possible motion between NCCT and CCT acquisitions."
,Comparison with VoxelMorph:,"The problem that we address can be seen from different angles. In particular, we could attempt to solve it by registering the preoperative NCCT to the intraoperative one and then applying the resulting displacement field to the known preoperative segmentation. However, state-of-the-art registration methods such as VoxelMorph "
4,Conclusion,"In this paper, we proposed a method for augmenting intra-operative NCCT images as a means to improve needle CT-guided techniques while reducing the need for contrast agent injection during tumor ablation procedures, or other needle-based procedures. Our method uses a U-net architecture to learn local vessel tree image features in the NCCT by leveraging the known vessel tree geometry and topology extracted from a matching CCT image. The augmented CT is generated by fusing the predicted vessel tree with the NCCT. Our method is validated on several porcine images, achieving an average dice score of 0.81 on the predicted vessel tree location. In addition, it demonstrates robustness even in the presence of large deformations between the preoperative and intraoperative images. Our future steps will essentially involve applying this method to patient data and perform a small user study to evaluate the usefulness and limitations of our approach. Aknowledgments. This work was partially supported by French state funds managed by the ANR under reference ANR-10-IAHU-02 (IHU Strasbourg). The authors would like to thank Paul Baksic and Robin Enjalbert for proofreading the manuscript."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 5 .Fig. 6 .,
,Fig. 7 .,
,Table 1 .,
1,Introduction,"Healthy and cancerous soft tissue display different elastic properties, e.g. for breast  Taking prostate cancer as an example, biomechanical characterization could guide needle placement for improved cancer detection rates while reducing complications associated with increased core counts, e.g. pain and erectile dysfunction  To perform OCE in deep tissue structures, we propose a novel bevel tip OCE needle design for the biomechanical characterization during needle insertions. We consider a dual-fiber setup with temporal multiplexing for the combined load and compression sensing at the needle tip. We design an experimental setup that can simulate friction forces and bulk displacement occurring during needle biopsy (Fig. "
2,Methods,"In the following, we first present our OCE needle probe and outline data processing for elasticity estimates. We then present an experimental setup for simulating friction and bulk displacement and describe the conducted surface and deep tissue indentation experiments."
2.1,OCE Needle for Deep Tissue Indentation,"Our OCE needle approach is illustrated in Fig.  with the force F , the area A, initial sample length L 0 and assuming incompressibility, quasi-static loading and neglecting viscoelasticity. However, the indentation with our bevel tipped needle will not result in uniform stress and we hypothesize instead that the elasticity is only relative to the applied tip force F T and the resulting local strain l . To obtain a single parameter for comparing two measurements, we assume a linear relation in the context of this work. To detect strain (Fiber 1) and applied force (Fiber 2), we consider the phase φ of the complex OCT signals for fiber i at time t and depth z. The phase shift between two A-scans is proportional to the depth dependent displacement δu i (z, t) assuming a refractive index n of 1.45 and 1.5 for tissue (Fiber 1) and epoxy (Fiber 2), respectively. We obtain the deformation u i (z, t) from the unwrapped phase and perform spatial averaging to reduce noise. For fiber 1, we employ a moving average with a window size of 0.1 mm. We estimate local strain based on the finite difference along the spatial dimension over an axial depth Δz of 1 mm. For fiber 2, we calculate the mean ū2 (t) over the entire depth of the epoxy. We assume a linear coefficient a F to model the relation between the applied tip force F T and the mean deformation ū2 of the reference epoxy layer. F T (t) = a F * ū2 (t). (5)"
2.3,Experimental Setup,"We build an experimental setup for surface and deep tissue indentations with simulated force and bulk displacement (Fig.  For surface measurements, we position the tissue phantoms separately without additional springs or tissue around the needle shaft. We use a motorized linear stage (ZFS25B, Thorlabs GmbH, GER) to drive the needle while simultaneously logging motor positions. An external force sensor (KD24s 20N, ME-Meßsysteme GmbH, GER) measures combined axial forces. We consider two gelatin gels as tissue mimicking materials for healthy and cancerous tissue. The two materials (Mat. A and Mat. B) display a Young's modulus of 53.4 kPa and 112.3 kPa, respectively. Reference elasticity is determined by unconfined compression experiments of three cylindrical samples for each material according to Eq. 1, using force and position sensor data (See supplementary material). The Young's modulus is obtained by linear regression for the combined measurements of each material. We calibrate tip force estimation (Fiber 2) by indentation of silicone samples with higher tear resistance to ensure that no partial rupture has taken place. We then determine a linear fit according to Eq. 5 and obtain a F = 174.4 mN mm -1 from external force sensor and motor position measurements (See supplementary material)."
2.4,Indentation Experiments,"In total, we conduct ten OCE indentation measurements for each material. Three surface measurements with fixed samples and seven deep tissue indentations with simulated friction and bulk displacement. For each indentation, we place the needle in front of the surface or deep tissue interface and acquire OCT data while driving the needle for 3 mm (Fig.  To further ensure that measurements occur within the pre-rupture deformation phase "
3,Results,The OCE measurements for surface and deep tissue indentations are displayed in Fig. 
4,Discussion and Conclusion,We demonstrate our approach on two tissue mimicking materials that have similar elastic properties as healthy and cancerous prostate tissue 
,Fig. 1 .Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Fig. 5 .,
,Table 1 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 58.
1,Introduction,"Understanding the movement of surgical instruments and the surgeon's hands is essential in computer-assisted interventions and has various applications, including surgical navigation systems  To fill this gap, we propose a novel synthetic data generation pipeline that goes beyond single image cases to synthesize realistic temporal sequences of surgical tool manipulation from an egocentric perspective. It features a body motion capture module to model realistic body movement sequences during artificial surgeries and a hand-object manipulation generation module to model the grasp evolution sequences. With the proposed pipeline, we generate POV-Surgery, a large synthetic egocentric video dataset of surgical activities that features surgical gloves in diverse textures (green, white, and blue) with various bloodstain patterns and three metallic tools that are commonly used in orthopedic surgeries. In summary, our contributions are: -A novel, easy-to-use, and generalizable synthetic data generation pipeline to generate temporally realistic hand-object manipulations during surgical activities. -POV-Surgery: the first large-scale dataset with egocentric sequences for hand and surgical instrument pose estimation, with diverse, realistic surgical glove textures, and different metallic tools, annotated with accurate 3D/2D handobject poses and 2D hand-object segmentation masks. -Extensive evaluations of existing state-of-the-art (SOTA) hand pose estimation methods on POV-Surgery, revealing their shortcomings when dealing with the unique challenges in surgical cases from egocentric view. -Significantly improved performance for SOTA methods after fine-tuning them on the POV-Surgery training set, on both our synthetic test set and a real-life test set."
2,Method,"We focus on three tools commonly employed in orthopedic procedures -the scalpel, friem, and diskplacer -each of which requires a unique hand motion. The scalpel requires a side-to-side cutting motion, while the friem uses a quick downward punching motion, similar to using an awl. Finally, the diskplacer requires a screwing motion with the hand. Our pipeline to capture these activities and generate egocentric hand-object manipulation sequences is shown in Fig. "
2.1,Multi-view Body Motion Capture,"To capture body movements during surgery, we used four temporally synchronized ZED stereo cameras on participants during simulated surgeries. The intrinsic camera parameters were provided by ZED SDK and the extrinsic parameters between the four different cameras were calibrated with a chessboard. We adopt the popular "
2.2,Hand-Object Manipulation Sequence Generation,"To address this issue, we generate each instrument manipulation sequence by firstly modeling the key poses that are surgically plausible, and then interpolating in between to model pose evolution. The key pose generation pipeline is shown in Fig.  L penetr , L contact , and L keypoint denote the penetration loss, contact loss, and keypoint loss, respectively. And α, β, γ are object-specific scaling factors to balance the loss components. For example, the weight for penetration is smaller for the scalpel than the friem and diskplacer to account for the smaller object size. The penetration loss is defined as the overall penetration distance of the object into the hand mesh: where P in denotes the vertices from the object mesh which are inside the hand mesh, and V i denotes the hand vertex. The P o in is defined as the dot product of the vector from the hand mesh vertices to their nearest neighbors on the object mesh. To encourage hand-object contact, a contact loss is defined to minimize the distance from the hand mesh to the object mesh. where V and P denote vertices from the hand and object mesh, respectively. In addition, we regularize the optimized hand pose by the keypoint displacement, which penalizes hand keypoints that are far away from the initial hand keypoints: where K is the refined hand keypoint position and k is the source keypoint position. After the grasping pose refinement, a small portion of the generated hand poses are still unrealistic due to the poor initialization. To this end, a post-selection technique similar to  For each hand-object manipulation sequence, we select 30 key grasping poses, hold on, and interpolate in between to model pose evolution within the sequence. The number of frames for the transition phase between every two key poses is randomly sampled from 5 to 30. The interpolated hand poses are also optimized via the pose refinement module with the source keypoint in L keypoint defined as the interpolated keypoints between two key poses."
2.3,Body and Hand Pose Fusion and Camera Pose Calculation,"In previous sections, we individually obtained the body motion and hand-object manipulation sequences. To merge the hand pose into the body pose to create a whole-body grasping sequence, we established an optimization-based approach based on the SMPLX model. The vertices to vertices loss is defined as: where v is the vertices in the target grasping hand, v is the vertices in the SMPLX body model, with M being the vertices map from MANO's right hand to SMPLX body. R and T are the rotation matrix and translation vector applied to the right hand. The right-hand pose of SMPLX, R, and T are optimized with the Trust Region Newton Conjugate Gradient method (Trust-NCG) for 300 iterations to obtain an accurate and stable whole-body grasping pose. R and T are then applied to the grasped object. The egocentric camera pose for each frame is calculated with head vertices position and head orientation. Afterwards, outlier removal and moving average filter are applied to the camera pose sequence to remove temporal jitterings between frames. We use blender "
2.4,Rendering and POV-Surgery Dataset Statistics,
3,Experiment,"Fig.  We evaluate and fine-tune two state-of-the-art hand pose estimation methods:  To further evaluate the generalizability of the methods fine-tuned on our dataset, we collect 6,557 real-life images with multiple surgical gloves, tools, and backgrounds as the real-life test set. The data capture setup with four stereo cameras is shown in Fig.  After fine-tuning on our synthetic dataset significant performance improvements are achieved for SOTA methods on the real-life test set. Particularly, we observe a similar performance improvement for unseen purple-texture gloves, showing the effectiveness of our POV-Surgery dataset towards the challenging egocentric surgical hand-object interaction scenarios in general. "
4,Conclusion,"This paper proposes a novel synthetic data generation pipeline that generates hand-tool manipulation temporal sequences. Using the data generation pipeline and focusing on three tools used in orthopedic surgeries: scalpel, diskplacer, and friem, we propose a large, synthetic, and temporal dataset on egocentric surgical hand-object pose estimation, with 88,329 RGB-D frames and diverse bloody surgical gloves patterns. We evaluate and fine-tune three current stateof-the-art methods on the POV-Surgery dataset. We prove the effectiveness of the synthetic dataset by showing the significant performance improvement of the SOTA methods in real-life cases with surgical gloves and tools."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 5 .,
,,
,Table 1 .,
1,Introduction,"Navigating surgical instruments in vitreoretinal procedures requires extreme manual dexterity. Surgeons need to manipulate fine structures, such as an Epiretinal Membrane (ERM), which can measure up to only 60µm  Accurate distance perception regarding the surgical instruments and the retina is of utmost importance for punctuated surgical action. Any unintentional contact with the retina could cause severe damage to important retinal cells. In conventional procedures, surgeons rely only on visual guidance from an operating microscope that only allows a top view. An endo-illumination probe is inserted into the eye to visualize the operating area. When surgical instruments are introduced, they cast a shadow onto the retina, relative to the position of the illumination probe. Such shadows provide one of the most essential cognitive cues to perceive the distance between the instrument and the retina and therefore are important for performing surgical action. Figure  Besides performing a procedure through this conventional stereoscopic view, recently, other imaging modalities have also become available. Technical advances have led to the integration of Swept-Source Optical Coherence Tomography (SS-OCT) into surgical microscopes  In this paper, we propose Semantic Virtual Shadows (SVS), a concept that integrates such visual cues for perceptual distance estimation in 4D OCT. By identifying shadow-casting and shadow-receiving objects in the OCT volume, we augment the shadow of surgical instruments on the retina. In particular, instrument-specific shadows can be generated in one case on the retinal surface, and in other cases, exclusively on deep-seated layers below the surface. Precomputing a semantic shadow volume texture prior to direct volume rendering (DVR) enables real-time performance and more flexibility in the rendering approach. We demonstrate the versatility of SVS by proposing two visualization approaches for different scenarios in retinal surgery, including retinal membrane peeling and subretinal injection."
2,Related Work,"So far, only a few works have addressed the integration of perceptual cues into OCT volume rendering algorithms for interactive surgical visualizations in 4D OCT. Among these, approaches to color voxels based on distance information have been explored as a means of conveying spatial perceptual understanding. In an early study  As opposed to previous works, we integrate an instrument-specific shadow augmented directly in the rendered OCT volume. Hence, our method aims to provide surgeons with familiar cognitive cues for perceptual distance estimation, as described in Sect. 1. Additionally, identifying shadow-casting and shadowreceiving objects allows the generation of instrument shadows on a specific retinal layer below the surface, which is not possible with previous approaches due to the self-shadowing of the retina."
3,Methodology,"We define a Semantic Virtual Shadow (SVS) as a shadow that is only generated by identified shadow-casting objects and is explicitly cast onto identified shadow-receiving objects. In our application, the primary use case of SVS is to generate visual cues for spatial distance perception. Taking into consideration shadow-casting and shadow-receiving objects as well as a virtual light source, a semantic shadow volume V s can be constructed as a 3D texture that assigns a shadowing factor to each voxel associated with a shadow-receiving object in the OCT volume. This precomputed V s is directly consumed by the direct volume rendering (DVR) to generate the instrument shadow augmentation. An overview of the proposed method is shown in Fig.  While in theory any segmentation method could be employed to identify, we combine efficient volume processing with a learning based approach to approximate the segmentations and achieve the required processing rates for 4D OCT. For our interventional scenarios, we define the instrument as the shadow-casting object. Inspired by  from which A-scans containing parts of the instrument signal can be identified. We train a U-net style  Given the position of the virtual light source p l and L(p) = p l -p |p l -p| defining the light direction, V s (p) is calculated with: ( sampling volume intensity values I(p) along the light direction through the entire volume. Note that, in our equations, the z axis corresponds to the axial Ascan direction. We precompute V s prior to volume raymarching using a compute shader program, enabling high update rates and flexibility in the design of the DVR algorithm that directly consumes V s . In the following, we propose two specific visualization approaches that integrate SVS according to equation ( "
3.1,Shadow Augmentation on Surface Structures,"During surgical phases, in which the instrument is located above the retinal surface, unintentional contact needs to be avoided. By defining the retinal surface as the shadow-receiving object, the SVS generates cognitive cues for surface distance perception. We use the following 2D projection for M sr : where t s = 0.17 is an empirically chosen threshold. We render the OCT volume using classic Phong shading to visualize surface structure details while integrating V s to augment the instrument-specific shadow C(p) = C P hong (p) • V s (p). The top row of Fig. "
3.2,Shadow Augmentation on Subsurface Structures,"In intra-and subretinal injection procedures, once reached the retinal surface, surgeons need to carefully guide the cannula to the injection target without damaging the underlying Retinal Pigment Epithelium (RPE) which is a comparably deep-seated layer in OCT. We propose to augment an instrument shadow explicitly on the RPE and visualize the retinal surface semi-transparent. Compared to the previous section, we define the RPE as the shadow-receiving object. Since in OCT imaging, the RPE typically is a hyper-reflective layer  During volume raymarching, we apply Phong shading models to visualize the instrument and render the retinal surface semi-transparent while preserving surface highlights, as presented in  Here, C P hongT ool (p) and C P hongILM (p) are the Phong shading models for the instrument and the retinal surface, respectively. We refer to Fig. "
4,Experiments and Results,
4.1,Implementation and Comparative Time Profiling,"Our instrument segmentation network was trained on 2D projection images generated from OCT volumes using Eq. 1. In total 330 OCT volumes of resolution 391 × 391 × 720 (spiral scanning) acquired from a model eye and 160 volumes of resolution 128 × 512 × 1024 (linear scanning) acquired from ex-vivo porcine eyes were used to generate a data set of 3928 images, including data augmentation strategies. The axial projection images were manually labeled by two biomedical engineers. We used Pytorch 1.13 for model training and TensorRT 8.4 for optimization. We implemented our method using C++ and OpenGL 4.6 (Windows 10 with Intel Core i7-8700K @3.7 GHz, Nvidia RTX 3090Ti), employing a compute shader to generate the 2D projection images and V s , and a fragment shader for DVR. Table "
4.2,User Study,"To evaluate the effect of SVS for distance perception compared to baseline DVR in 4D OCT guided surgery, we conducted a user study with 12 biomedical experts (10 male, 2 female) familiar with OCT. We conducted the study in a virtual environment simulating 4D OCT using the method proposed in  Users were asked to familiarize themselves with the interaction in the virtual environment before the study, reducing the impact of a learning curve on the study results. For each trial, we randomly positioned a small target point close to the retina, simulating anatomical structures that could be targeted in a surgical scenario. Participants were asked to move the instrument tip to the target and press a button to confirm the positioning. Each uses performed 10 trials, each in the following three variants: (i) baseline DVR with Phong shading as in  With SVS, users approached with faster convergence to the target and less error variance toward the end of the trial. In addition, Fig. "
5,Discussion and Conclusion,"The results of our user study indicate that the proposed method is able to generate effective perceptual cues for distance estimation in 4D OCT. This is reflected in improved targeting performance, as well as a high acceptance of the generated perceptual cues. Figure  In conclusion, the proposed SVS augments visual cues that are naturally not present in OCT, but essential in microscopic vitreoretinal surgery. The flexibility of our approach enables object-specific shadow generation not only on surface structures but also exclusively on subsurface structures, supporting various surgical procedures. We provided a general definition of object-specific shadow generation and demonstrated our method on a 4D SS-OCT system for live display. In our user study, SVS was shown to provide intuitive and effective visual cues for targeted instrument maneuvers."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Fig. 5 .,
,Fig. 6 .,
,Table 1 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_39.
1,Introduction,"Spinal nerves play a crucial role in the body's sensory, motor, autonomic, and other physiological functions. Injuries to these nerves carry an extremely high risk and may even lead to paralysis. Minimally invasive endoscopic surgery is a common treatment option for spinal conditions, with great care taken to avoid damage to spinal nerves. However, the safety of such surgeries still heavily relies on the experience of the doctors, and there is an urgent need for computers to provide effective auxiliary information, such as real-time neural labeling and guidance in videos. Ongoing studies using deep learning methods to locate spinal nerves in endoscopic videos can be classified into two categories based on the level of visual granularity: coarse-grained and fine-grained tasks. Coarse-grained vision task focuses on object detection of spinal nerve locations. In this task, object detection models applied to natural images are widely transferred to endoscopic spinal nerve images. Peng Cui et al.  In response to the above two problems, our contribution is as follows: • We innovatively propose inter-frame and channel attention modules for the spinal neural segmentation problem. These two modules can be readily inserted into popular traditional segmentation networks such as Unet "
2,Method,
2.1,Details of Dataset,"The dataset was taken by the professional SPINENDOS, SP081375.030 machines, and we collected nearly 10000 consecutive frames of video images, each with a resolution of up to 1080*1080 (Fig. "
2.2,Network Architecture,"The overall network architecture is based on a classical Unet network  Unet's skip-connection method is a good complement to the information lost in the down-sampling reduction process, but it is difficult to capture the global contextual information due to the local dependency of the convolutional network. However, this global information is also crucial to the spinal nerve segmentation problem in the endoscopic scenario, so we inserted a channel self-attention module (CSA) in each up-sampling section with the global capability of the self-attention mechanism.  "
2.3,IFA Module,"To exploit the rich semantic information (such as blur, blisters, and other background semantics) between endoscopic frames, we designed the IFA to correlate the weight assignment between T frames. (Fig.  Channel Pyramid. Although the multiscale operation in traditional convolutional neural networks can improve the generalization ability of the model to targets of different sizes  Hence, we propose a channel pyramid architecture to compress the channel dimension for capturing the cross-frame multiscale information, as well as to keep the feature map size unchanged in dimensions of height and width for preserving spatial precision. Such channel down-sampling obtains multi-scale information and semantic information in different cross-frame ranges. The result can adjust the frame weight on keyframe segmenting guidance. For detail, the feature matrix obtained by each ADB is compressed in the channel dimension, which avoids the loss of image size information. Like the perceptual field in the multi-scale approach, the number of inter-frame channels in the channel pyramid at different scales represents the magnitude of the scale across frames, and this inter-frame information at different scales is contacted for further fusion calculations, which is used to generate attention weights. Attention Down Block (ADB). This module (Fig. "
2.4,CSA Module,"Inspired by the related work of vision transformer  X ∈ R B×(H ×W )×C is the original feature matrix expanded by the flatten operation. At the same time, to supplement the loss of spatial position information, we supplement the pos embedding vector by addition operation. The multi-headed attention mechanism is implemented by a split operation. For calculation of self-attention, we use the common method  3 Experiments"
3.1,Implementation Details,"Dataset. The self-built dataset is our first contribution. We set up the training set, test set, and validation set. For extended experiments on the polyp dataset, we used the same dataset configuration as PNS-Net  Training. On the self-built dataset and the CVC-612 dataset, we both use learning rate and weight decay of 1e-4 with Adam optimizer. On the self-built dataset, our model converges after about 30 epochs, while on the CVC-616 dataset, we use the same pre-training and fine-tuning approach as PNS-Net. Methods involved in the data augmentation phase include flipping and cropping. A single TITAN RTX 24 GB graphics card is used for training. Testing. Five images (T = 5) are input to the IFA and use the same input resolution of 256*448 as PNSNet to ensure consistency in subsequent tests. Our FUnet is capable of inference at 75 fps on a single TITAN RTX 24 GB, which means that real-time endoscopic spinal nerve segmentation is possible in surgery."
3.2,Experimental Results,"On our spinal nerve dataset, we tested the classical and leading medical segmentation networks Unet  The qualitative comparison is in Fig. "
3.3,Ablation Experiments,The baseline model uses Res2Net 
4,Conclusion,"In this paper, we propose the industry's first semantic segmentation dataset of spinal nerves from endoscopic surgery to date and design the FUnet segmentation network based on inter-frame information and self-attention mechanism. FUnet has achieved state of the art performance on our dataset and shows strong generalization performance on polyp dataset with similar scenes. The IFA and CSA modules of FUnet can be easily incorporated into other networks. We plan to expand the dataset in the future with the help of self-supervised methods, to improve the performance of the model to provide better computer-assisted surgery capabilities for spinal endoscopy."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Fig. 5 .,
,Fig. 6 .,
,Fig. 7 .,
,Table 1 .,
,Table 2 .,
,Table 3 .,
1,Introduction,"Intervention options for the treatment of abdominal aortic aneurysms (AAAs) include open surgery or endovascular repair (EVAR). A minimally invasive procedure, EVAR involves the peripheral delivery of one or more covered endografts to the aneurysmal segment via a catheter-based system. EVAR has been demonstrated to be superior to open surgery with regards to early mortality  With complex AAA repair, it is of paramount importance to maintain the patency of major aortic side branches to allow for end-organ perfusion. Traditionally, open surgery was preferred for these complex cases; however, with the advent of endografts that have pre-made openings (fenestrations) in the graft material, these patients can be treated using fenestrated endovascular repair (FEVAR). The concept of in situ fenestration (ISF) for endografts during FEVAR has been proposed as an alternative to the use of pre-fenestrated endografts. With ISF, the fenestrations are generated within the aorta following deployment of the endograft. Potential benefits of this approach include both greater anatomical conformity and reduced device cost. Once ISF has been performed, a wire is passed through the fenestration into the side-branch. The fenestration is then dilated with incrementally sized non-compliant or cutting balloons and secured with a stent that maintains communication between the aorta and side-branch  A key challenge with ISF that we address here is to visualize aortic side branches in order to precisely identify the locations for fenestration. Pre-procedural imaging in concert with X-ray fluoroscopy can be used for guidance "
2,Methods,
2.1,Device Design,"The custom device comprised two primary fiber optic components which were integrated within a catheter: 1) an OpUS transmitter/receiver pair for visualizing side branches; 2) an ISF fiber for optical fenestration. These optical fibers were secured together using heat shrink and housed within a 6 Fr [inner diameter (ID): 2 mm] catheter (Multipurpose-1, Cordis, Santa Clara, CA, USA), leaving a 0.7 mm diameter channel to allow a 0.014 or 0.018 guidewire to be delivered through a generated fenestration. The catheter was delivered to the endovascular stent graft using a 7 Fr ID steerable sheath (Tourguide, Medtronic, USA), a current standard that allowed for bi-directional 180°/90°deflection with manual control and thereby allowed the catheter to be appropriately positioned to detect side-branches behind the endovascular stent (Fig.  Fig.  The OpUS transmitter comprised a custom nanocomposite material for photoacoustic generation of ultrasound, with optical absorption of excitation light by a near-infrared absorbing dye embedded within a medical-grade elastomeric host "
2.2,Benchtop Imaging and Fenestration,"To assess whether OpUS permits visualization of side branches through endograft material, a benchtop model with ex-vivo swine aorta (Medmeat Ltd, United Kingdom) was used. A section of the aorta was splayed open longitudinally to create a flat surface and secured to a cork board. Endograft stent material (Zenith Flex® AAA, Cook Medical, USA) made from a synthetic polyester Dacron Material (DM) (polyethylene terephthalate) was positioned above the aortic sample. An OpUS imaging scan was performed with 600 lateral steps of length 50 µm using a motorised stage  To test the effects of different laser parameters on optical fenestration, the following were varied: the pulse energy, the pulse duration, and the distance between the distal end of the fenestration optical fiber and the endograft material. Fenestrations with different combinations of these parameters (2.5W, 0.5s; 1.8W, 0.5s; 4.2W, 0.5s; 4.2W, 1s; 4.2W, 5s) were attempted in different sections of aortic tissue. A piece of endograft material was secured above each tissue section; the distance from the endograft material to the tissue was approximately 2 mm. The tissue and endograft material were immersed in anticoagulated sheep blood (TCS Biosciences, Buckingham, UK). A fenestration was deemed to have been successfully achieved if adjacent tissue was visible when the surrounding blood was drained."
2.3,In Vivo Imaging and Fenestration,"To obtain a preliminary validation of the system's potential for guiding EVAR and ISF, OpUS imaging and endograft fenestration were performed in a clinically realistic environment using an in vivo porcine model. The primary objectives of this experiment were a) to visualize an aortic side branch behind a deployed endograft; b) to perform optical fenestration of the endograft. All procedures on animals were conducted in accordance with U.K. Home Office regulations and the Guidance for the Operation of Animals (Scientific Procedures) Act  Following successful anesthesia, transcutaneous ultrasound (US) was used to gain percutaneous access to both common femoral arteries; 8 Fr vascular sheaths were inserted bilaterally. Both fluoroscopic acquisition and digital subtraction angiography with an iodine-based contrast agent were performed to create a road-map for further stages of the experiment. The ISF device was inserted via the Right Femoral Artery (RFA) and directed with the steerable catheter into the aorta. The inner catheter of the ISF device was advanced into the aorta. For the EVAR procedure, a non-fenestrated iliac endograft was used (Zenith Spiral-Z, Cook Medical, USA; proximal/distal diameter: 13 mm/11 mm; length: 129 mm). The choice of an iliac endograft originally for human patients was for the comparatively smaller dimensions of the swine aorta. The sheath in the RFA was up-sized to a 14 Fr in order to accommodate the delivery system. The stent was deployed across the renal artery bifurcation. The ISF device was inserted into the lumen of the endograft and OpUS imaging was performed with a longitudinal pullback across the covered left renal artery. Once positioned over the desired location, a single pulse from the 808 nm fenestration laser (power: 4.2 W; pulse duration: 5 s) was delivered to the endograft to create a fenestration. A coronary guidewire (0.018 ; Balanced Middleweight Universal, Abbott, USA) was then inserted through the ISF device and through the fenestration into the side-branch. Expansion of the fenestration in the endograft was then performed with a series of three balloons (2 mm non-compliant, 3 mm non-compliant and 3 mm cutting) to create a fenestration large enough to accommodate a stent. Following successful ISF, the procedure was ended and the ISF device was removed."
3,Results,
3.1,Benchtop Imaging and Fenestration,"With OpUS imaging of the paired endograft and underlying aortic tissue, the stent graft manifested as an echo-bright linear structure (Fig.  During benchtop attempts, a fenestration was successfully created with a pulse duration of 0.5 s and an optical power of 1.8 W when the distal end of the fiber was in contact with the endograft material. This combination of pulse duration and optical power corresponded to the smallest energy that yielded fenestration with this device. There was no macroscopic tissue damage with these parameters. The size of the fenestration increased with both higher power outputs and longer laser pulse durations. When the distal end of the optical fiber was recessed a distance of 3 mm from the endograft material, both a higher power output and longer pulse duration were required to generate a fenestration. No macroscopic tissue damage was identified during any of the fenestration experiments. As a control, the optical fiber was placed directly on the tissue with no stent material. In this configuration, a power of 4.2 W and a pulse duration of 5 s resulted in a macroscopic thermal ablation of the tissue."
3.2,In Vivo Imaging and Fenestration,"The renal bifurcation was readily observed with fluoroscopic guidance and X-ray contrast injections. Due to the small diameter of the swine aorta relative to that of an adult human, it proved challenging to achieve sufficient bending of the steerable sheath whilst maintaining a gap between the imaging device and the aortic tissue. In this configuration, bend-induced losses in the fiber optic receiver significantly lowered the signal-to-noise (SNR) ratio relative to that obtained during benchtop imaging. Nonetheless, a manual pullback with imaging performed at a slightly non-perpendicular angle relative to the aortic surface proved feasible. With OpUS imaging during this pullback, the endograft material presented as a strongly hyperechoic region and the underlying aorta could be imaged to a depth >3 mm. A side branch presented as a hypoechoic region consistent with an absence of tissue within the renal vessel lumen (Fig.  For ISF, the laser output parameters that were chosen (4.2 W, 5 s) were intentionally higher than the minimum values observed in benchtop experiments. Following the ISF, the guidewire was passed into the renal artery. A post-mortem dissection confirmed a successful optical fenestration (Fig. "
4,Discussion,"A key observation of this study is that OpUS offers direct visualization of aortic tissue beneath an AAA endograft, and detection of vascular side-branches. This capability is important for appropriate ISF positioning, thereby maintaining side-branch patency. The ability to detect a side-branch immediately before performing fenestration with the same device could reduce the risk of inappropriate positioning and the subsequent need for conversion to open surgery. In current clinical practice, there is no device that allows for direct imaging of aortic side branches with the added capability of achieving endograft fenestration. Current methods for generating ISF, although promising, are not guided by intravascular imaging. With these methods, there is a risk of creating a fenestration that is not aligned with the side-branch, leading to a potential endoleak, and a risk of causing injury to underlying tissue structures. The potential cost implication of ISF is significant. The average costs of pre-fenestrated and patient-specific grafts are ca. £15,400 and £24,000 (GBP), respectively; the cost of a non-fenestrated aortic endograft that could be used for ISF is ca. £6,000. Several improvements to the device can be envisaged. An increase in the lateral OpUS imaging resolution could be effected with greater US beam collimation, for instance with the use of a larger-diameter transmitting surface; however, this change would likely result in increased device size and reduced flexibility. Improvements in the SNR of OpUS images could potentially be achieved with depth-dependent frequency filtering and with the application of deep-learning methods  During the in vivo procedure in this study, the power of the fenestration laser was chosen to be higher than the minimum value observed in the benchtop component of this study to increases the chances of success. It remains to be seen whether a lower output setting may be viable in this context, with the presence of device motion and flowing blood and with pathological changes in the arterial wall. In future studies, real-time feedback from OpUS imaging could be incorporated to determine the power and pulse duration of the fenestrating laser; a similar probe could be used for cardiac ablation, for instance in the context of treatment of atrial fibrillation  This study presented a novel imaging probe for performing interventional image guidance for endovascular procedures. We conclude that OpUS imaging is a promising modality for guiding EVAR; in concert with and optical fenestration, it could find particularly utility with identifying aortic side branches and performing ISF during treatment of complex AAAs."
,Fig. 2 .,
,Fig. 3 .,
,,
1,Introduction,"Cataract surgeries are amongst the most frequently performed treatments, with 4,000 to 10,000 annual operations per million people  Nevertheless, the publicly available data for training such systems is limited: given the nature of the surgeries, certain surgical phases take more time than others. Further, there are variances in their length based on the surgeon's skill and the patient's particular needs. Since surgical tool usage is strongly coupled with the surgical phase, certain phases and tools are shown more frequently than others, constituting an inherent imbalance in the data. As displayed in Fig.  One cannot simply gather new data showing unusual tools to perform the required actions during a surgical step. Therefore, we must find different ways to represent them in the data and counteract the imbalance. The usual countermeasures in the form of oversampling can increase prediction accuracy  Although these applications have shown promising results, there is a demand for conditional image generation for Surgical Data Science. Since most downstream applications consist of supervised methods, they require training targets, and the likelihood of unconditionally generating diverse samples for unusual cases is very low. For conditional generation with denoising diffusion models, classifier guidance has been introduced recently  results with diffusion models. Conditional diffusion models have been applied to generate medical images from a few binary label inputs  Precise conditioning beyond a few binary labels is crucial for synthesising valuable surgical data. Instead, we need to train a model that can generate diverse examples based on multi-class or multi-label conditions, e.g. certain surgical phases, combinations of surgical tools, or both. We show that using an adapted denoising diffusion model together with CFG can yield high-quality samples of cataract surgery data, even for rare cases such as the CATARACTS phase and tool combinations shown in Fig.  To the best of our knowledge, ours is the first work combining CFG with diffusion models to efficiently generate realistic cataract surgery data with a complex underlying label structure. Additionally, we examine the cataract video data for the worst-performing phases of a pre-trained tool usage classifier. We then leverage the conditional denoising diffusion model to generate unseen samples for these phases. Our conditioned tools are recognisable by the tool classifier and are hard to differentiate from real images, even for clinicians with more than five years of experience. Further, we demonstrate how our synthetically extended data can alleviate the data sparsity problem for the downstream task. Overall, our evaluations show that the model can generate valuable examples to build the bridge to clinical application."
2,Method,"The following section describes how we build our generative diffusion model and integrate CFG to generate cataract surgery frames conditioned on surgical phases and tools. Furthermore, we provide an analysis of the worst-performing surgical steps for a pre-trained tool classifier model. Finally, we demonstrate the sampling procedure using the generative model to improve the classifier."
2.1,Denoising Diffusion Probabilistic Models,The fundamental underlying idea of Denoising Diffusion Probabilistic Models (DDPMs)  where Denoising Diffusion Implicit Models (DDIMs) 
2.2,Classifier-Free Guidance,"By utilising Classifier-Free Guidance (CFG)  This gradient yields  where w is a weighting hyperparameter. The weighted noise ¯ θ (x t , t, y (p) , y (s) ) can simply replace in Eq. 1. We add an embedding module emb (p) to the UNet architecture, which converts categorical phase labels y (p) into one-hot encoded vectors to include them into the input. To simultaneously include tool labels in the form of (non-exclusive) binary vectors, we compute projections emb (s) of the same size as the time-step and phase label embeddings using stacked dense layers. All embeddings are concatenated as {emb t (t), emb (p) (y (p ), emb (s) (y (s) )} and fed to the conditional denoising UNet model together with the noisy image x t . Figure "
2.3,Tool Usage Analysis and Sample Generation,Following Roychowdhury et al. 
3,Experiments and Results,"In this section we explain our experimental setup, demonstrate the synthesis of high-quality samples and show how these can improve the downstream model's performance on challenging phases. "
3.1,Experimental Setup and Dataset,"To evaluate the quality of synthesised images, we generate 30,000 samples with phase and toolset conditions sampled from p -1 φ (y s , y p ) = (1p φ (y s , y p ))/ (1p φ (y s , y p )), as explained in Sect. 2.3. The resulting number of examples is close to the test split size of CATARACTS sampled at 3 FPS. We compare the proposed approach to state-of-the-art baselines for conditional generative modelling: A conditional LS-GAN (cLS-GAN) "
3.2,Quantitative Image Quality,"We deploy a variety of quantitative metrics to assess the quality of generated images, for which Table "
3.4,Downstream Tool Classification,"Finally, we re-train the tool-set classifier on a combined dataset of the original and the synthesised samples. As shown in Table "
4,Conclusion,"We present a generative model based on denoising diffusion models and classifierfree guidance, powerful enough to synthesise cataract surgery images that are hard to distinguish for a pre-trained tool classifier and clinical experts. For underrepresented phases, state-of-the-art baselines tend to produce frames that show eyes without correct anatomy or barely recognisable tools, resulting in a significant performance gap. Distortions in the dataset further deteriorate their learning. On the contrary, we demonstrate that the proposed approach outperforms these baselines in terms of image quality and tool preservation. As a limitation of our approach, we found that the generalisation capabilities must be strengthened to generate unreasonable samples, e.g. completely wrong tools during a phase. Such samples can happen if they are present in the data. Though, a targeted generation would require a more substantial representation. Additionally, while tool realism is significantly better for the proposed method, the CF1 and CAS scores indicate that it can be further improved. Besides, the underlying class imbalances and lack of available data are even more severe for the downstream task of anatomy and tool segmentation. In future work, we will extend the proposed method to generate segmentation targets, temporally connected data and deploy a tighter structure for conditioning. Overall, we are the first to have shown how conditional diffusion models can successfully be applied to mend data sparsity and generate high-quality cataract surgery images suitable for clinical application. These improvements can bring computer-assisted cataract surgery one step closer to the next level of automation."
,Fig. 1 .,
,Fig. 3 .,
,Fig. 4 .,
,Fig. 5 .,
,Table 1 .,
,experts favouring the gener- ated images 61% of the time,
,Table 2 .,
,Table 3 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 34.
1,Introduction,"The human brain undergoes rapid and dynamic development during the early postnatal period. Research on infant brain development  To address this issue, we propose a flexible multi-task framework to jointly predict cognitive score and cortical morphological development of infant brains at arbitrary time points with longitudinal data scanned irregularly within 24 months of age. Specifically, the cognitive ability of each infant was estimated using the Mullen Scales of Early Learning (MSEL) "
2,Methods,
2.1,Network Architecture,"The framework of our disentangled intensive triplet spherical adversarial autoencoder (DITSAA) is shown in Fig.  The operation indicates the element-wise multiplication and φ denotes an attention module in Fig.  Identity Conditional Block. To preserve the identity-level regression/progression pattern, we extended the identity conditional block (ICB)  Cortical Morphology Prediction Module. To ensure the quality of the predicted cortical morphological maps, the generator G adopts the architecture of the UNet-based least-square GANs  •] denotes the concatenation along the channel dimension."
2.2,Loss Design,"Intensive Triplet Loss. As illustrated in Fig.  ) is also a pair of different subjects, to enhance the relative distance between same subject pair and all the different subject pairs, we apply the intensive triplet loss  Cognitive Prediction Loss. Given the identity-related age condition derived from ICB, we regress the cognitive scores directly by the cognitive prediction module at a selected age t. The loss function to train the cognitive prediction module is defined as: where y is the ground truth of cognitive scores. Adversarial Loss. To improve the quality of the predicted cortical property maps, we used the adversarial loss to train the generator G and discriminator D with respect to least-square GANs  where C t denotes the one-hot age encoding at different age t. Reconstruction Loss. To guarantee the high-quality reconstruction and constrain the vertex-wise similarity of the input cortical morphological features and generated cortical morphological features, we adopted the L1 loss to evaluate the reconstruction: Full Objective. The objective functions of our model are written as: where λ IT , λ AE , λ Cog , λ G , λ D , and λ rec control the relative importance of the loss terms."
3,Experiments,
3.1,Dataset and Experimental Settings,"In our experiments, we used the public BCP dataset  All cortical surface maps were mapped onto the sphere  We trained E, ID, and AgeP using an SGD optimizer under the supervision of L AE and L IT with an initial learning rate of 0.1, momentum of 0.9, and a self-adaption strategy for updating the learning rate, which was reduced by a factor of 0.2 once the training loss stopped decreasing for 5 epochs, and the hyper-parameters in the training loss functions were empirically set as follows: λ AE was 0.1 and λ IT was 0.1. We then fixed E and used an Adam optimizer with an initial learning rate of 0.01, β 1 of 0.5, and β 2 of 0.999 to train ICB, COG, G, and D using Eq. (  We used the cortical thickness map at one time point to jointly estimate the four Mullen cognitive scores, i.e., VRS, FMS, RLS, and ELS, and the cortical thickness map at any other time points. To quantitatively evaluate the cognitive prediction performance, the Pearson's correlation coefficient (PCC) between the ground truth and predicted values was calculated. For the cortical morphology prediction, the PCC and mean absolute error (MAE) between the ground truth and predicted values were calculated. In the testing phase, the mean and standard deviation of the 5-fold results were reported."
3.2,Evaluation and Comparison,"To comprehensively evaluate the mutual benefits of the proposed modules in cognitive and cortical morphological development prediction, we conducted an ablation study on two modules in Table "
4,Conclusions,"In this study, we proposed a novel flexible multi-task joint prediction framework for infant cortical morphological and cognitive development, named disentangled intensive triplet spherical adversarial autoencoder. We effectively and sufficiently leveraged all existing longitudinal infant scans with highly irregular and nonuniform age distribution. Moreover, we leverage the mutual benefits between cortical morphological and cognitive development to improve the performance of both tasks. Our framework enables jointly predicting cortical morphological and cognitive development flexibly at arbitrary ages during infancy, both regression and progression. The promising results on the BCP dataset demonstrate the potential power for individual-level development prediction and modeling."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,,
,,
,Table 1 .,
,850 ± 0.004 * 0.820 ± 0.014 * 0.312 ± 0.032 *,
,Table 2 .,
1,Introduction,Surgical scene understanding is an important prerequisite for artificial intelligence (AI)-empowered surgery 
2,Methods,
2.1,Task Description and Dataset,Our study is based on the CholecTriplet Challenge 2022 
2.2,Concept Overview,As illustrated in Fig. 
2.3,Implementation Details,Swin Transformer. We base our method on Swin Transformer (SwinT) models of the timm 
,Self-distillation. The concept of self-distillation was achieved by training a,"teacher Swin transformer on one-hot encoded hard labels for 20 epochs, with a batch size of 64, an Adam  The five teacher models shared a common weight intialization seed. The five student models shared a separate weight initialization seed. The student models were trained for 40 epochs with the same augmentations as the teacher models. We saved the weights on the epoch with the best mean Average Precision (mAP) score based on the validation split for the current fold. Ensemble. We combined three trained Swin Transformers (SwinT) of different scales (SwinT base/SwinT large) and configurations for our final ensemble (Ens) model: First, we employed a SwinT base model with multi-task learning of instrument, verb, and target and trained it using self-distillation. Second, we used a SwinT large model using the same approach, and added label smoothing to the soft labels. Third, we included phase annotations as an additional task for the multi-task training of a SwinT base model still employing self-distillation. Please note that every single model mentioned here corresponds to the five aggregated student models of the previous paragraph. All the models were trained using Nvidia GPUs Geforce RTX 3090 and Tesla V100 32GB."
3,Experiments and Results,"The purpose of the experiments was to validate the performance of our method and to quantify the (potential) benefit of each individual component. To this end, we conducted (1) comprehensive ablation studies using the CholecT45 official 5-Fold cross-validation split  Ablation Studies. We designed the ablation studies as follows: We first calculated the performance of our Swin Transformer backbone as a stand-alone triplet classifier (SwinT). Next, we added multiple auxiliary targets (instruments, verbs, targets, and phases) for multi-task classification (+MultiT). As a third component, we implemented self-distillation by training a student model on soft labels, acquired by training the teacher model (+SelfD). The fourth step was the ensembling of three student model SwinT (+Ens). The results are shown in Tab. 1. A single SwinT as model backbone yields a higher mAP for triplet classification (mAP=32.3%) than the benchmark (mAP=28.8%), which corresponds to a relative improvement by 10.3%. The biggest boost was achieved by including selfdistillation, which improved the Triplet mAP and top-5 accuracy by 3.8% points (pp) and 2.4pp, respectively, compared to our baseline. The final model yielded a mAP of 38.5% and a top-5 accuracy of 86.5%, which corresponds to a boost of 6.2pp in mAP and 2.7pp in top-5 accuracy compared to our own baseline, and a relative improvement by 33.7% for mAP compared to the state-of-the-art method. For transparency, we also provide per-video results, depicted in Fig.  Analysis of Soft Labels. The addition of self-distillation resulted in the highest boost in performance. This holds true despite the fact that the mAP of the teacher model, trained on hard labels, was about 88% on the training set, which is sub-optimal. The question is thus why the poorer soft labels still yielded a performance improvement. While part of the answer is provided in the literature on soft/noisy labels "
,Table 1. Main quantitative results,"Starting from our backbone model -a Swin Transformer (Swin T) -we gradually added individual components, namely multi-task learning (MultiT), self-distillation (SelfD), and ensembling (Ens). Each component addition leads to an increase in mAP and top-5 accuracy, in both cross-validation (left) and independent external validation (right). This shows that self-distillation specifically leads to increased scores for semantically related classes. Independent External Validation. External validation was conducted on the CholecTriplet challenge test set. The results, shown in Table "
4,Discussion,"This paper pioneers the concept of self-distillation in the medical image analysis domain. Specifically, we are the first to tackle key challenges in surgical action recognition, namely the high number of classes and class imbalance, with selfdistillation. Comprehensive ablation studies combined with external validation yielded the following findings: 1. Swin Transformers, as recently introduced by the computer vision community, can serve as a strong backbone in endoscopic vision tasks. This is suggested by the fact that even our most ablated model, consisting of a single SwinT, surpasses the state-of-the-art surgical action recognition method Rendezvouz. 2. Multi-task learning, here using the classification of instrument, verb, and target as well as of the surgical phase as auxiliary tasks, yielded a notable increase in performance. 3. Self-distillation yielded the biggest boost in performance, suggesting that soft labels are better suited for surgical action recognition. 4. Ensembling increased performance further, as also suggested by various publications in a wide range of fields. Overall, the addition of self-distillation (in combination with the SwinT as a backbone) resulted in the highest performance boost. While label noise has been shown to be beneficial in various work  Related work has so far tackled the challenge of surgical action recognition with various strategies including multi-task learning  In conclusion, our study is the first to demonstrate the benefit of selfdistillation for surgical vision tasks. Based on the substantial performance boost obtained, the usage of soft labels could become a valuable tool in the endoscopic vision community."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 61.
1,Introduction,"Orbital fractures represent a frequent occurrence of orbital trauma, with their incidence on the rise primarily attributed to assault, falls, and vehicle collisions  Orbital wall reconstruction is challenging due to the complex and diverse OBF types, as shown in Fig.  Several automatic segmentation methods for the orbital wall have been explored in previous studies  The main contributions of this paper are summarized as follows: (1) A GAN model guided by SPAK is developed for automatic reconstruction of the orbital wall in OBF surgery, which outperforms the existing methods. (2) The proposed method is the first AI-based automatic reconstruction method of the orbital wall in OBF surgery, which can enhance the repair effectiveness and shorten the surgical planning time."
2,Methods,The structure of the proposed SPAK-guided GAN is illustrated in Fig. 
2.1,Automatic Generation of SPAK Based on Spatial Transformation,"The reconstruction of the orbital wall in OBF is a complex task, made more difficult when severe displacement or defects are present. However, since the left and right orbits are theoretically symmetrical structures, utilizing the normal orbital wall on the symmetrical side as prior anatomical guidance in the reconstruction network can aid in accuracy. Prior knowledge has been demonstrated to be effective in medical image computing "
2.2,SPAK-Guided GAN for Orbital Wall Reconstruction,"Reconstructing the fractured orbital wall requires generative prediction of the damaged area, which is why we adopted the GAN. Our proposed SPAK-guided GAN consists of a generative network (GN) and a discriminative network (DN), as illustrated in Fig. "
2.3,Multiple Loss Function Supervision for GAN,"The GAN network's loss function comprises two components: the loss function of the discriminative network, denoted as Loss D , and the loss function of the generative network, denoted as Loss G . To enhance the reconstruction performance, we adopt a supervision strategy that combines multiple loss functions. To ensure that the GAN network can identify the authenticity of samples, we incorporate the commonly used discriminative loss function in GAN, as presented in Eq.  where D represents the network discriminator, G represents the network generator, G(x) represents reconstruction result, x represents input image, y represents ground truth. To improve the accuracy of the reconstructed orbital wall, a combination of multiple loss functions is utilized in Loss G . First, to better evaluate the area loss, the dice coefficient loss function Loss dice is adopted, which can calculate the loss between the reconstructed orbital wall region and the ground truth using Eq. (  where N represents the total number of voxels, G(x i ) represents the voxels of reconstruction results, y i represents the voxels of ground truth. Furthermore, an adversarial loss function L adv , is also incorporated into Loss G . This function evaluates the loss of the generator's output by the discriminator, thus enabling the network to improve its performance in reconstructing the fractured orbital wall. The equation for L adv is shown in  Finally, we combine the loss function Loss dice , Loss ce and L adv to obtain the final Loss G , whose equation is shown in  3 Experiments and Results"
3.1,Data Set and Settings,"The dataset for this study was obtained from Shanghai Ninth People's Hospital Affiliated to Shanghai Jiao Tong University School of Medicine. It included 150 cases of OBF CT data: 100 for training and 50 for testing. Each case had a blowout orbital fracture on one side and a normal orbit on the other. For normal orbit segmentation training, 70 additional cases of normal orbit CT data were used. The images were 512 × 512 in size, with resolutions ranging from 0.299 mm × 0.299 mm to 0.717 mm × 0.717 mm. The number of slices varied from 91 to 419, with thicknesses ranging from 0.330 mm to 1.0 mm. The ground truth was obtained through semi-automatic segmentation and manual repair by experienced clinicians. To focus on the orbital area, CT scans were resampled to 160 × 160 with a multiple of 32 slices after cutting out both orbits. This resulted in 240 single-orbital CT data for normal orbital segmentation training and 100 for OBF reconstruction training. The proposed networks used patches of size 32 × 160 × 160, with training data augmented using sagittal symmetry. The proposed and comparison networks all adopted the same input patch size of 32 × 160 × 160, a batch size of 1, a learning rate of 0.001, and were trained for 30,000 iterations using TensorFlow 1.14 on an NVIDIA RTX 8000 GPU. Evaluation of the reconstruction results was based on the dice similarity coefficient (DSC), intersection over union (IOU), precision, sensitivity, average surface distance (ASD), and 95% Hausdorff distance (95HD). "
3.2,Ablation Experiment Results,"The proposed network is based on the GN, and an ablation experiment was conducted to verify the effectiveness of the adopted strategy. The experimental results are presented in Table "
3.3,Comparative Experiment Results,"To demonstrate the superior performance of the proposed reconstruction algorithm, we compared it with several state-of-the-art networks used in medical image processing, including U-Net "
4,Conclusion,"In summary, this paper proposes a SPAK-guided GAN for accurately automating OBF wall reconstruction. Firstly, we propose an automatic generation method of SPAK based on spatial transformation, which maps the segmented symmetrical normal orbital wall to a fractured orbit to form an effective SPAK. On this basis, a SPAK-guided GAN network is developed for the automatic reconstruction of the fractured orbital wall through adversarial learning. Furthermore, we use the strategy of multi-loss function supervision to improve the accuracy of network reconstruction. The final experimental results demonstrate that the proposed reconstruction network achieves accurate automatic reconstruction of the fractured orbital wall, with a DSC of 92.35 ± 2.13% and a 95% Hausdorff distance of 0.59 ± 0.23 mm, which is significantly better than other networks. This network achieves the automatic reconstruction of the orbital wall in OBF, which effectively improves the accuracy and efficiency of OBF surgical planning. In the future, it will have excellent application prospects in the repair surgery of OBF."
,Fig. 1 .,
,( 1 )Fig. 2 .,
,Fig. 3 .Fig. 4 .,
,Table 1 .,
,Table 2 .,
1,Introduction,"Vertebra localization and identification from CT scans is an essential step in medical applications, such as pathology diagnosis, surgical planning, and outcome assessment  scan can be very challenging: (i) scans vary greatly in intensity and constrast, (ii) metal implants and other materials can affect the scan quality, (iii) vertebrae might be deformed, crushed or merged together due to medical conditions, (iv) vertebrae might be missing due to accidents or previous surgical operations. Recently, public challenges like the VerSe challenge  In this paper, we introduce a trainable method that performs vertebrae localization, orientation estimation and classification with a single architecture. We replace all hand-crafted rules and post-processing steps with a single trainable Graph Neural Network (GNN) that learns to filter out, associate and classify landmarks. We apply a generalized Message Passing layer that can perform edge and node classification simultaneously. This alleviates the need for sensitive hand-tuned parameter tuning, and increases robustness of the overall pipeline. The main contributions of our work are: (1) introducing a pipeline that uses a single Graph Neural Network to perform simultaneous vertebra identification, landmark association, and false positive pruning, without the need for any heuristic methods and (2) building and releasing a new spine detection dataset that adds pedicles of vertebrae to create a more complex task that includes orientation estimation of vertebrae, which is relevant for clinical applications."
2,Related Work,"Spine Landmark Prediction and Classification. The introduction of standardised spine localization and classification challenges  Various methods have applied GNNs to keypoint detection, however they all apply to 2-dimensional input data. In "
3,Method,"In this paper we tackle vertebra localization and classification, but unlike other methods that only focus on detecting the body of the vertebrae, we also detect the pedicles and associate them with their corresponding body. This allows us to also define the orientation of the vertebra defined by the plane passing through the body and pedicles Our method consists of a two-stage pipeline shown in Fig.  CNN Stage. The CNN stage detects candidate body, left pedicle and right pedicle keypoints and provides segment classifications for the body keypoints as either cervical, thoracic, lumbar or sacral. We use a UNet 2 CNN "
,GNN Stage.,"The second stage employs a generalized message-passing GNN following  1. keypoint association prediction: we model association between body keypoints and their corresponding pedicle keypoints as binary edge classification on the over-connected k-NN graph. 2. body keypoint level prediction: for body keypoints, we model the spine level prediction as multi-class node classification. 3. keypoint legitimacy prediction: to filter out false-positive keypoints, we additionally compute an binary legitimacy prediction for each node. To perform these task, our message-passing GNN maintains edge and node embeddings which are updated in each layer. A message-passing layer performs a node update and edge update operation. Denoting the feature vector of a node v by x v , and the feature vector of a directed edge (u, v) by x uv , the node and edge features are updated as follows: Here denotes a symmetric pooling operation (in our case max pooling) over the neighborhood N u . ψ node and edge are trainable parametric functions: in our case, two distinct two-layer MLPs with ReLU nonlinearities. After N such message-passing layers we obtain an embedding vector for each node and edge. Each node/edge embedding is passed through a linear layer (distinct for nodes and edges) to obtain a vector of node class logits or a single edge prediction logit, respectively. The last entry in the node prediction vector is interpreted as a node legitimacy prediction score: nodes predicted as illegitimate are discarded for the output. The node input features x u ∈ R 7 consist of the one-hot encoded keypoint type (body, left or right pedicle) and the segment input information (a pseudoprobability in [0, 1] for each of the four spine segments of belonging to that segment, computed by applying a sigmoid to the heatmap network's output channels which represent the different spine segments). The edge input features x uv ∈ R 4 consist of the normalized direction vector of the edge and the distance between the two endpoints. The output of the GNN contains finer spine-level classification (i.e. C1-C7, T1-T13, L1-L6, S1-S2), keypoint-level legitimacy (legitimate vs. false-positive detection) and body-pedicle association via edge prediction, implicitly defining the orientation of each vertebra. Prediction scores of corresponding directed edges (u, v) and (v, u) are symmetrized by taking the mean. In our experiments we consider variations to our architecture: weight sharing between consecutive GNN layers, multiple heads with a shared backbone (jointly trained) and dedicated networks (separately trained) for edge/node prediction. 4 Experiments"
4.1,Dataset,"Our main dataset consists of 2118 scans, of which 1949 form the training dataset, and 169 the validation dataset. This includes 360 scans from the VerSe datasets "
4.2,Training,"Heatmap Network. The heatmap is generated by a UNet 2 network  Graph Neural Network. The GNN is implemented in PyTorch Geometric 2.0.4  We only make edge predictions on edges that run between a body and a pedicle keypoint -the other edges are only used as propagation edges for the GNN. Similarly, we only make spine level predictions on body keypoints. Solely these subsets of nodes/edges go into the respective losses. As input data, we use the predicted keypoints of the heatmap network on the training/validation set. The ground-truth keypoints are associated to this graph to create the target of the GNN. We include three synthetic model spines during training (keypoints in a line spaced 30 mm apart) to show the network typical configurations and all potential levels (with all levels/without T13, L6, S2/without T12, T13, L6, S2). We tune various hyperparameters of our method, such as network depth and weight sharing, the k of the k-NN graph, the loss weighting parameters α, β, γ and the number of hidden channels in the message-passing MLP. We use a short notation for our architectures, such as (5 × 1, 4, 1) for 5 independent messagepassing layers followed by 4 message-passing layers with shared weights and another independent message-passing layer. Various data augmentations are used to make our network more robust to overfitting: (i) rotation of the spine by small random angles, (ii) mirroring along the saggital axis (relabeling left/right pedicles to keep consistency), (iii) perturbation of keypoints by small random distances, (iv) keypoint duplication and displacement by a small distance (to emulate false-positive duplicate detections of the same keypoint), (v) keypoint duplication and displacement by a large distance (to emulate false-positive detections in unrelated parts of the scan) and (vi) random falsification of spine segment input features. We define four levels of augmentation strength (no/light/default/heavy augmentations) and refer to the supplementary material for precise definitions of these levels."
4.3,Evaluation,"We evaluate our method on two tasks. The first one is the full task consisting of node and edge classification for vertebra level and keypoint association detection. We evaluate this on the 2118 scan dataset which comes with pedicle annotations. The second task consists only of vertebra localization, which we evaluate on the VerSe 2019 dataset "
4.4,Results,The results on our introduced dataset are shown in Table  Table  Table 
5,Conclusion,"We introduced a simple pipeline consisting of a CNN followed by a single GNN to perform complex vertebra localization, identification and keypoint association. We introduced a new more complex vertebra detection dataset that includes associated pedicles defining the full orientation of each vertebra, to test our method. We show that our method can learn to associate and classify correctly with a single GNN that performs simultaneous edge and node classification. The method is fully trainable and avoids most heuristics of other methods. We also show competitive performance on the VerSe body-identification dataset, a dataset the method was not optimized for. We believe this method is general enough to be usable for many other detection and association tasks, which we will explore in the future."
,Fig. 2 .,
,Table 1 .,
,Table 2 .,
,Table 3 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_46.
1,Introduction,"Pelvic fracture is a severe type of high-energy injury, with a fatality rate greater than 50%, ranking the first among all complex fractures  Several studies have been proposed to provide more efficient tools for operators. A semi-automatic graph-cut method based on continuous max-flow has been proposed for pelvic fracture segmentation, but it still requires the manual selection of seed points and trail-and-error  Fracture segmentation is still a challenging task for the learning-based method because (1) compared to the more common organ/tumor segmentation tasks where the model can implicitly learn the shape prior of an object, it is difficult to learn the shape information of a bone fragment due to the large variations in fracture types and shapes  This paper proposes a deep learning-based method to segment pelvic fracture fragments from preoperative CT images automatically. Our major contribution includes three aspects. "
2,Methods,
2.1,The Overall Segmentation Framework,Our study aims to automatically segment the major and minor fragments of target bones (left and right ilia and sacrum) from CT scans. As illustrated in Fig. 
2.2,Fracture Segmentation Network,"The contact fracture surface (CFS) is the part where the bones collide and overlap due to compression and impact, and is the most challenging part for human operators to draw. We are particularly concerned about the segmentation performance in this region. Therefore, we introduce guidance into the network training using fracture distance map (FDM). A 3D UNet is selected as the base model  Fracture Distance Map. The FDM is computed on the ground-truth segmentation of each data sample before training. This representation provides information about the boundary, shape, and position of the object to be segmented. First, CFS regions are identified by comparing the labels within each voxel's neighbourhood. Then, we calculate the distance of each foreground voxel to the nearest CFS as its distance value D v , and divide it by the maximum. is the indicator function for foreground, and D is the normalized distance. The distance is then used to calculate the FDM weight Ŵ using the following formula: To ensure the equivalence of the loss among different samples, the weights are normalized so that the average is always 1. FDM-Weighted Loss. The FDM weight Ŵ is then used to calculate the weighted Dice and cross-entropy losses to emphasize the performance near CFS by assigning larger weights to those pixels. where L is the number of labels, P l v , Y l v are the network output prediction and the one-hot encoding form of the ground truth for the l th label of the v th voxel. The overall loss is their weighted sum: where λ ce is a balancing weight. Multi-scale Deep Supervision. We use a multi-scale deep supervision strategy in model training to learn different features more effectively  Smooth Transition. To stabilize network training, we use a smooth transition strategy to maintain the model's attention on global features at the early stage of training and gradually shift the attention towards the fracture site as the model evolves  where δ = -ln(1 -t τ ), J is an all-ones matrix with the same size as the input volume, t is the current iteration number, and τ is a hyper-parameter. The dynamic weight W st is adjusted by controlling the relative proportion of J and Ŵ . The transition terminates when the epoch reaches τ ."
2.3,Post-processing,Connected component analysis (CCA) has been widely used in segmentation 
2.4,Data and Annotation,"Although large-scale datasets on pelvic segmentation have been studied in some research  To generate ground-truth labels for bone fragments, a pre-trained segmentation network was used to create initial segmentations for the ilium and sacrum "
3,Experiments and Results,
3.1,Implementation,"We compared the proposed method (FDMSS-UNet) against the network without smooth transition and deep supervision (FDM-UNet) and the network without distance weighting (UNet) in an ablation study. In addition, we also compared the traditional max-flow segmentation method. In the five-fold cross-validation, each model was trained for 2000 epochs per fold. The network input was augmented eight times by mirror flip. The learning rate in ADAM optimizer was set to 0.0001. λ back was set to 0.2. λ ce was set to 1. The initial λ F DM was set to 16. The termination number for smooth transition τ was set to 500 epochs. The models were implemented in PyTorch 1.12. The experiments were performed on an Intel Xeon CPU with 40 cores, a 256 GB memory, and a Quadro RTX 5000 GPU. The comprehensive code and pertinent details are provided at https://github.com/YzzLiu/FracSegNet."
3.2,Evaluation,"We calculate the Dice similarity coefficient (DSC) and the 95th percentile of Hausdorff Distance (HD) to evaluation the performance. In addition, we evaluated the local Dice (LDSC) within the 10 mm range near the CFS to assess the performance in the critical areas. We reported the performance on iliac main fragment (I-main), iliac other fragment(s) (I-other), sacral main fragment (Smain), sacral other fragment(s) (S-other), and all together."
3.3,Results,"Figure  Table  The average inference time for the fracture segmentation network was 12 s. The overall running time for processing a pelvic CT was 0.5 to 2 min, depending on the image size and the number of fractured bones. "
4,Discussion and Conclusion,"We have introduced a pelvic fracture CT segmentation method based on deep convolutional networks. A fracture segmentation network was trained with a distance-weighted loss and multi-scale deep supervision to improve fracture surface delineation. We evaluated our method on 100 pelvic fracture CT scans and made our dataset and ground truth publicly available. The experiments demonstrated the method's effectiveness on various types of pelvic fractures. The FDM weighted loss, along with multi-scale deep supervision and smooth transition, improved the segmentation performance significantly, especially in the areas near fracture lines. Our method provides a convenient tool for pelvis-related research and clinical applications, and has the potential to support subsequent automatic fracture reduction planning. One obstacle for deep learning-based fracture segmentation is the variable number of bone fragments in different cases. The ultimate goal of this study is to perform automatic fracture reduction planning for robotic surgery, where the main bone fragment is held and moved to the planned location by a robotic arm, whereas minor fragments are either moved by the surgeons' hands or simply ignored. In such a scenario, we found isolating the minor fragments usually unnecessary. Therefore, to define a consistent labeling strategy in annotation, we restrict the number of fragments of each bone to three. This rule of labelling applies to all 100 cases we encountered. Minor fragments within each label can be further isolated by CCA or handcrafting when needed by other tasks. We utilize a multi-scale distance-weighted loss to guide the network to learn features near the fracture site more effectively, boosting the local accuracy without compromising the overall performance. In semi-automatic pipelines where human operators are allowed to modify and refine the network predictions, an accurate initial segmentation near the fracture site is highly desirable because the fracture surface itself is much more complicated, often intertwined and hard to draw by manual operations. Therefore, with the emphasis on fracture surface, even when the prediction from the network is inaccurate, manual operations on 3D view can suffice for most modifications, eliminating the need for the inefficient slice-by-slice handcrafting. In future studies, we plan to integrate the proposed method into an interactive segmentation and reduction planning software and evaluate the overall performance."
,Fig. 1 .,
,Fig. 2 .,
,Table 1 .,
1,Introduction,"Skin flap is a widely used technique to close the wound after the resection of a lesion. In a skin flap procedure, a healthy piece of tissue is harvested from a nearby site to cover the defect  In this paper, we build a system to help surgeons pick the most optimal flap orientation. We focus on the rhomboid flap as it is a very versatile flap. According to medical textbooks  -We created a skin flap FEM simulation and validated the simulation outputs of rhomboid flaps against other simulators; -We performed quantitative evaluation of the suture forces for rhomboid flaps and compared the result against textbook knowledge; -We provided an objective and methodical way to make recommendations for aligning rhomboid flaps relative to RSTLs; -We generated a database of rhomboid flap simulations under various material properties and relative angles to RSTLs."
2,Related Works,"There exists various virtual surgical simulators for facial skin flap procedures. In a recent work, Wang and Sifakis et al. built an advanced skin flap simulator that allows users to construct free-form skin flaps on a 3D face model interactively  To gain insights into deformations in a skin flap procedure, there are various FEM simulations with stress/strain analysis. Most of those simulations are built based on a commercial FEM software such as Abaqus "
3,Methods,We built our skin flap FEM simulation based on the open-source library Bartels 
3.1,Constitutive Model of Skin,"The skin patch is modelled as a thin membrane that resides in two dimensional space (planar stress formulation), as it is a common setup in the existing literature  The isochoric component is proportional to μ, the shear modulus and the anisotropic component is parameterized by k 1 and k 2 , which are stiffness parameters. More details of the skin constitutive model can be found in Stowers et al. "
3.2,Cutting and Suturing,Cutting is accomplished by duplicating vertices along the cutting line and remeshing the local neighborhood to setup the correct connections with the duplicated vertices. This mechanism can run very efficiently and is sufficient for our setup since we always triangulate the mesh area based on the specific flap type so cutting can only happen on edges. An illustration of such mechanism is shown in the Supplementary Material. The suture process is implemented by adding zero rest-length springs between vertices. Different stages of a rhomboid flap procedure is shown in Fig. 
3.3,Triangulation,"In our setup, we assume the lesion is within a circular area at the center of the patch. Similar to "
3.4,Validation,"To validate our FEM simulation, we compared it with UWG simulator  Lastly, we compared the suture line of our simulation with the ""post-operative"" image. We used normalized Hausdorff distance "
3.5,Suture Force Analysis,Multi-sutures. As excess force along the suture line can cause wound dehiscence and complications 
,Fig. 3. Pseudocode of two different suture force analyses,"To investigate the effect of material properties, we repeated the same procedure using the material properties listed in Table  Table  def ault small µ large µ small k1 large k1 small k2 large k2 Single-suture. Instead of taking the maximum suture force after completing all sutures, we also experimented with making one suture at a time (see Algorithm 2 in Fig. "
4,Results,The simulation results were generated using a Macbook Pro and an iMac. It required around 30 s -2 min to compute suture force for each rotation.
4.1,Validation with UWG Simulator,Further comparisons of skin flap placed at various locations of the face model are shown in Fig. 
4.2,Suture Force Analysis,Multi-sutures. The suture forces of various flap rotation angles under different material properties are shown in Table 
4.3,Database Creation,"While performing different experiments, we also created a dataset of rhomboid flap simulation videos under different material properties and the corresponding mesh at the end of each simulation. There are in total over 500 video clips of around 30 s. Overall, there are more than 10,000 frames available. A sample video can be found in the Supplementary Material. The dataset can be accessed through the project website: https://medcvr.utm.utoronto.ca/ miccai2023-rhomboidflap.html. "
5,Discussion and Conclusion,"We built a FEM simulation to find the most optimal flap orientation for rhomboid flap based on suture force analysis. Through minimizing suture forces, we found that the optimal orientation occurred at 99 • /281 • and 105 • /284 • for multisutures and single-suture settings, respectively. The range of forces we obtained in our simulation is similar to what was reported in literature  Our experiment suggests that the minimal suture force occurs at a configuration that is close to textbook knowledge recommendation, but we found that rhomboid flaps have to be rotated further 15 • to 20 • away from the LMEs as seen in Fig.  Our current simulation does not take the complex physical structure of the human face into account. In this study, we aim to compare the prediction made by this setup with textbook knowledge based on RSTLs, which also does not take the more complex facial feature differences into account. This means our model will likely not work well on non-planar regions of the face. Further investigation and comparison with a physical setup (with either synthetic model or clinical setting) is also needed to show clinical significance of our result (for both suture forces comparison and flap orientation design). In the future, it would be interesting to continue investigating this topic by comparing behaviors of different flap types."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 4 .,
,Fig. 5 .,
,Fig. 6 .Fig. 7 .,
,Table 2 .,
1,Introduction,"Functional neurosurgical techniques, such as deep brain stimulation (DBS), and MR-guided stereotactic ablation, have been used as effective treatments of neurological and psychiatric disorders for decades. By intervening on a target brain structure, a neurosurgical treatment typically modulates brain activity in disease-related circuits and can often mitigate the disease symptoms and restore brain function when drug treatments are ineffective. An example is the ventral intermediate nucleus of thalamus (Vim), a well-established surgical target in DBS and stereotactic ablation for the treatment of tremor in Parkinson's Disease, essential tremor, and multiple sclerosis  As the structure is not readily visible on conventional MR images, targeting the Vim has relied primarily on standardised coordinates provided by stereotactic atlases, instead of directly targeting the structure based on subject-specific image-derived features. Such standardised stereotactic coordinates/atlases provide a reproducible way to identify the nucleus. However, the atlas-based Vim targeting falls short of accounting for the inter-individual and often interhemispheric anatomical variability, which are often substantial for thalamic nuclei  To overcome this limitation, several recent studies proposed to localise the Vim using its anatomical connectivity features (e.g., with M1 and the dentate nucleus) in vivo on an individual basis  However, the connectivity-driven Vim requires high angular resolution diffusion imaging (HARDI) techniques to allow reconstruction of Vim's connectivity features, which are often not readily available in advanced-care clinical contexts. Furthermore, even with cutting-edge HARDI and higher order diffusion modelling techniques, the connectivity-derived Vim has exhibited considerable variations across different acquisition protocol and processing pipelines, suggesting that they have to be used with caution to ensure that they adhere to the true underlying anatomical variability instead of simply reflecting the methodological confounds erroneously interpreted as variability. Given the limitations of the standardised approaches and connectivity-based methods, we propose a novel approach, HQ-augmentation, to reliably target the Vim, particularly for clinical (or generally lower-quality) data. We utilised the publicly-available high-quality (HQ) Human Connectome Project (HCP) dataset  consistent Vim targets despite compromised data quality, but also preserves inter-individual anatomical variability of the structure. Furthermore, the approach generalises to unseen datasets with different acquisition protocols, showing potential of translating into a reliable clinical routine for MR-guided surgical targeting."
2,Materials and Methods,"HQ Dataset I: Human Connectome Project (HCP) 3T MRI Datasets. We leveraged the 3T diffusion MRI data from HCP  HQ Dataset II: UK Biobank (UKB) 3T MRI Datasets. The UKB 3T MRI datasets  Surrogate Low-Quality (LQ) Datasets. We considered a range of lowquality datasets representing the typical data quality in clinical contexts. This included 1) HCP surrogate low angular resolution diffusion dataset, obtained by extracting the b = 0 s/mm 2 and 32 b = 1000 s/mm 2 volumes from HQ HCP 3T diffusion MRI (""LQ-LowAngular""); 2) HCP surrogate low spatial resolution dataset, obtained by downsampling the HCP 3T diffusion MRI to isotropic 2 mm sptial resolution (""LQ-LowSpatial""); 3) HCP surrogate low angular and spatial resolution dataset, created by downsampling the surrogate low-angularresolution dataset to isotropic 2 mm spatial resolution (""LQ-LowAngular-LowSpatial""); 4) UKB surrogate low angular resolution dataset, created by extracting the b = 0 s/mm 2 and 32 b = 1000 s/mm 2 volumes from the original UKB diffusion dataset (""LQ-UKB""). The Connectivity-Driven Approach. We followed the practice in  The Atlas-Defined Approach. We used a previously published and validated Vim atlas  The HQ-Augmentation Approach. The goal of this approach is to leverage anatomical information in HQ data to infer the likelihood of a voxel belonging to the Vim, given a wide range of tract-density features (multiple distinct tract bundles) derived from low-quality data. The HQ-augmentation model was trained on the HCP dataset for each type of low-quality dataset. Using the HCP HQ data, we first generated the connectivity-driven Vim (referred to as HQ-Vim) as the ""ground truth"" location of the nucleus, serving as training labels in the model. Next, for each low-quality counterpart, we generated an extended set of tract-density features, targeting a wide range of region-of-interests (ROIs), as the input features of the model. The richer set of connectivity features serves to compensate for the primary tract-density features (with M1 and dentate), when those are compromised by less sufficient spatial or angular resolution in low-quality diffusion MRI, thus making Vim identification less reliant on the primary tract-density features used in the connectivity-driven approach and more robust to variations in data quality. During training, the model learns to use the extended set of low-quality connectivity features to identify the Vim that is closest to the one that can be otherwise obtained from its HQ counterpart. Specifically, assume X = [x 1 , x 2 , ...x V ] T is a V ×d connectivity feature matrix for a given subject, where x i is a d × 1 vector representing the connectivity features in voxel i, V is the total number of voxels of the thalamus (per hemisphere) for this subject; y = [y 1 , y 2 , ...y V ] T is a V × 1 vector containing the HQ-Vim labels for the V voxels, in which y i is the label of voxel i. Given the low-quality features X, we seek to maximise the probability of reproducing the exact same HQ-Vim label assignment y on its low-quality counterparts, across the training subjects Here E(y|X) is the cost of the label assignment y given the features X, whilst Z(X) serves as an image-dependent normalising term. Maximising the posterior P (y|X) across subjects is equivalent to minimising the cost of the label assignment y given the features X. Suppose N i is the set of voxels neighbouring voxel i, the cost E(y|X) is modelled as The first component ψ u (y i ) measures the cost (or inverse likelihood) of voxel i taking label y i . Here ψ u (y i ) takes the form ψ u (y i ) = w T yi φ(x i ), where φ(•) maps a feature vector x i = [x 1 , x 2 , ...x d ] to a further expanded feature space in order to provide more flexibility for the parameterisation. W = [w 1 , w 2 ] is the coefficient matrix, each column containing the coefficients for the given class (i.e., belonging to the HQ-Vim or not). Here we chose a series of polynomials along with the group-average Vim probability (registered into native space) to expand the feature space, i.e.,"
,φ(x,"where p 1 = 2, p 2 = 0.5, p 3 = 0.2 are the power of the polynomials, and g i is the group-average probability of voxel i classified as Vim. The second pairwise cost encourages assigning similar labels to neighbouring voxels, particularly for those sharing similar connectivity features. We modelled this component as ) is a kernel function modelling the similarity between voxel i and j in the extended feature space, with length scale γ m , chosen via cross-validation. μ(•) is a label compatibility function where μ(y i , y j ) = 0 if Therefore, in a local neighbourhood, the kernel function penalises inconsistent label assignment of voxels that have similar features, thus allowing modelling local smoothness. ρ m controls the relative strength of this pairwise cost weighted by k m (•). Lastly, the L1 and L2 penalty terms serve to prevent overfitting of the model. We used a mean-field algorithm to iteratively approximate the maximum posterior P (y|X) "
3,Experiments and Discussions,"Accuracy of the HQ-Augmentation Model on the HCP Surrogate Low-Quality Datasets. The HQ-augmentation model was trained using HQ-Vim as labels and the LQ connectivity profiles as features, and then tested on left-out subjects from the same LQ dataset. Its accuracy was evaluated against the HQ-Vim counterpart obtained from the HQ HCP data of these left-out subjects, which served as ground truth. The accuracies of the HQ-augmentation Vim were also compared with those of the atlas-defined Vim and the connectivity-driven Vim, the latter obtained using the low-quality M1 and dentate tract-density features. We considered two accuracy metrics: Dice coefficient, which measures the overlap with the ground truth, and centroid displacement, which calculates the Euclidean distance between the predicted and ground truth centers-of-mass. As the connectivity-driven approach may even fail on HQ data, resulting in unreliable HQ-Vim, the training and evaluation of HQ-augmentation model were conducted only on a subset of ""good subjects"", whose HQ-Vim's center-of-mass was within 4 mm from the atlas's center-of-mass in native space. For the subjects which HQ-Vim regarded as unreliable (i.e., deviating too much from the atlas), the accuracy of HQ-augmentation model was evaluated against the atlas-defined Vim. The HQ-augmentation model produced Vim predictions that are closest to the HQ-Vim (i.e., higher Dice coefficient and smaller centroid displacement) than the LQ connectivity-driven approach and the atlas-defined Vim, evaluated on the reliable subset (see Fig. "
,Generalisability of the HQ-Augmentation,"Model to UK Biobank. We also tested whether the HQ-augmentation models trained on HCP were generalisable to other datasets collected under different protocols. It is crucial for a model to be generalisable to unseen protocols, as collecting large datasets for training purposes in clinical contexts can often be impractical. We therefore applied the HQ-augmentation models trained on different HCP LQ datasets to UK Biobank surrogate low-quality data (LQ-UKB) and averaged the outputs to give a single Fig.  Reliability Analysis of the HQ-Augmentation Model. We also conducted a reliability analysis for the HQ-augmentation model and the connectivity-driven approach to assess their consistency in providing results despite variations in data quality (across-quality reliability) and across scanning sessions (test-retest reliability). To evaluate the HQ-augmentation model's across-quality reliability, we trained it using HQ tract-density maps to produce an ""HQ version"" of HQaugmented Vim. The similarity between HQ-augmentation outputs using high-or low-quality features was assessed using the Dice coefficient and centroid displacement measures. Test-retest reliability was determined by comparing the outputs of the HQ-augmentation model on first-visit and repeat scanning sessions. Similarly, we assessed the across-quality reliability and test-retest reliability for the connectivity-driven approach, applied to high-and low-quality data accordingly. The HQ-augmentation model consistently provided more reliable results than the connectivity-driven approach, not only across datasets of different quality but also across different scanning sessions (Fig.  Discussion. Our study presents the HQ-augmentation technique as a robust method to improve the accuracy of Vim targeting, particularly in scenarios where data quality is less than optimal. Compared to existing alternatives, our approach exhibits superior performance, indicating its potential to evolve into a reliable tool for clinical applications. The enhanced accuracy of this technique has significant clinical implications. During typical DBS procedures, one or two electrodes are strategically placed near predetermined targets, with multiple contact points to maximise the likelihood of beneficial outcomes and minimise severe side effects. Greater accuracy translates into improved overlap with the target area, which consequently increases the chances of successful surgical outcomes. Importantly, the utility of our method extends beyond Vim targeting. It can be adapted to target any area in the brain that might benefit from DBS, thus expanding its clinical relevance. As part of our ongoing efforts, we are developing a preoperative tool based on the HQ-augmentation technique. This tool aims to optimise DBS targeting tailored to individual patients' conditions, thereby enhancing therapeutic outcomes and reducing patient discomfort associated with suboptimal electrode placement."
,Fig. 3 .Fig. 4 .,
,,
1,Introduction,"Gliomas are the most common central nervous system (CNS) tumors in adults, accounting for 80% of primary malignant brain tumors  As the true underlying deformation from brain shift is impossible to obtain and the differences of image features between MRI and US are large, quantitative validation of automatic MRI-US registration algorithms often rely on homologous anatomical landmarks that are manually labeled between corresponding MRI and intra-operative US scans  Previously, many groups have proposed algorithms to label landmarks in anatomical scans "
2,Related Work,Contrastive learning has recently shown great results in a wide range of medical image analysis tasks 
3,Methods and Materials,
3.1,Data and Landmark Annotation,We employed the publicly available EASY-RESECT (REtroSpective Evaluation of Cerebral Tumors) dataset 
3.2,Contrastive Learning Framework,"We used two CNNs with identical architectures in parallel to extract robust image features from MRI and US scans. Specifically, these CNNs are designed to acquire relevant features from MRI and US patches, and maximize the similarity between features of corresponding patches while minimizing those between mismatched patches. Each CNN network contains six successive blocks, and each block consists of one convolution layer and one group norm, with Leaky ReLU as the activation function. Also, the convolution layer of the first and last three blocks of the network has 64 and 32 convolutional filters, respectively, and a kernel size of 3 is used across all blocks. After the convolution layers, the proposed network has two multi-layer perceptron (MLP) layers with 64 and 32 neurons and Leaky ReLU as the activation function. These MLP layers compress the extracted features from convolutional layers and produce the final feature vectors. The resulting CNN network is depicted in Fig. "
3.3,Landmark Matching with a 2.5D Approach,"Working with 3D images is computationally expensive and can make the model training unstable and prone to overfitting, especially when the size of the database is limited. Therefore, instead of a full 3D processing, we decided to implement a 2.5D approach "
3.4,Landmark Matching with 3D SIFT,The SIFT algorithm 
4,Experimental Setup,
4.1,Data Preprocessing,"For CL training, both positive and negative sample pairs need to be created. All 2D patch series were extracted according to Sect. 3.3 with a size of 42 × 42 × 3 voxels. These sample pairs were used to train two CNNs to extract relevant image features across MRI and US leveraging the InfoNCE loss."
4.2,Loss Function,"We used the InfoNCE loss  where F θ and G β are the CNN feature extractors for MR and US patches. X A i and X P i are the cropped image patches around the corresponding landmarks in MR and US scans, respectively, and X N i is a mismatched patch in the US image to that cropped around the MRI reference landmark. Here, F θ • X A i , G β • X P i , and G β • X N j give the extracted feature vectors for MR and US patches."
4.3,Implementation Details and Evaluation,"To train our DL model, we made subject-wise division of the entire dataset into 70%:15%:15% as the training, validation, and testing sets, respectively. Also, to improve the robustness of the network, we used data augmentation for the training data by random rotation, random horizontal flip, and random vertical flip. Furthermore, an AdamW optimizer with a learning rate of 0.00001 was used, and we trained our model for 50 epochs with a batch size of 256. In order to evaluate the performance of our technique, we used the provided ground truth landmarks from the database and calculated the Euclidean distance between the ground truths and predictions. The utilized metric is as follows: where x i and x i , and N are the ground truth landmark location, model prediction, and the total number of landmarks per subject, respectively."
5,Results,Table 
6,Discussion,"Inter-modal anatomical landmark localization is still a difficult task, especially for the described application, where landmarks have no consistent spatial arrangement across different cases and image features in US are rough. We tackled the challenge with the CL framework for the first time. As the first step towards more accurate inter-modal landmark localization, there are still aspects to be improved. First, while the 2.5D approach is memory efficient and quick, 3D approaches may better capture the full corresponding image features. This is partially reflected by the observation that the quality of landmark localization is associated with the level of tissue shift. However, due to limited clinical data, 3D approaches caused overfitting in our network training. Second, in the current setup, we employed landmarks in pre-operative MRIs as references since its contrast is easier to understand and it allows sufficient time for clinicians to annotate the landmarks before surgery. Future exploration will also seek techniques to automatically tag MRI reference landmarks. Finally, we only employed US scans before resection since tissue removal can further complicate feature matching between MRI and US, and requires more elaborate strategies, such as those involving segmentation of resected regions  Besides better landmark identification accuracy, the tighter standard deviations also imply that our DL approach serves a better role in grasping the local image features within the image patches."
7,Conclusions,"In this project, we proposed a CL framework for MRI-US landmark detection for neurosurgery for the first time by leveraging real clinical data, and achieved state-of-the-art results. The algorithm represents the first step towards efficient and accurate inter-modal landmark identification that has the potential to allow intra-operative assessment of registration quality. Future extension of the method in other inter-modal applications can further confirm its robustness and accuracy."
,Fig. 1 :,
,Fig. 2 :,
,Fig. 3 :,
,Table 1 :,
1,Introduction,"Automating systems to interpret complex behaviors in surgical operating rooms (OR) has seen a surge of interest in recent years  have improved patient outcomes by reducing blood loss, recovery periods, and hospitalization times  Recent works have established the necessity of combining data from multiple cameras to obtain better coverage of surgical procedures  However, to adequately represent the distribution of possible events in the surgical domain, large volumes of data must be acquired  To this end, we propose SegmentOR, a weakly-supervised indoor semantic segmentation method for 4D multi-view OR datasets. By leveraging the innate temporal consistency of 4D point cloud sequences, we reduce the annotation burden to only a single click per class (about 0.005% of points), decreasing average annotation time per surgical phase from 3 h to 9.6 min while achieving a higher segmentation mIoU than existing methods that use four times the amount of labels. Furthermore, we establish the soundness of semantic predictions from our model for surgical scene understanding by showing that surgical phase recognition performance can be improved using our segmentation predictions as input. Our main contributions can thus be summarized as follows: -We propose the first 3D weakly-supervised semantic segmentation method for operating room environments and validate it on a manually annotated dataset of 3D point clouds from real surgical acquisitions. -We demonstrate that various temporal priors can be used to exploit consistency in weakly-supervised semantic segmentation, improving performance to 10% mIoU above baseline methods. -We show that the semantic outputs from our model can improve the performance of downstream surgical phase recognition methods, formally establishing the link between these two previously disjoint tasks. -Finally, we release all code and tools, as well as 2577 anonymized and annotated point clouds from the dataset, to advance progress in surgical scene understanding. https://bastianlb.github.io/segmentOR/"
2,Related Work,Surgical Scene Understanding. Workflow recognition is pivotal for contextual awareness in operating room (OR) intelligent systems. Activity recognition has been achieved for single-frame  The effectiveness of such weakly-supervised segmentation methods in dynamic OR environments remains unclear. Unlike static indoor datasets like ScanNet  Temporal Modeling. Optical flow effectively extracts movement from image sequences  Few have combined temporal consistency with 3D semantic segmentation for dynamic point cloud sequences outside autonomous driving settings 
3,Method,"Problem Setting. Given a point cloud In contrast to the supervised setting where dense ground truth labels y i ∈ Y t are available for every point p i , we infer dense semantic labels in an unseen test sequence by training on sparsely annotated point clouds. We refer to this setting as ""weakly-supervised"", meaning ground truth train annotations consist of a randomly chosen point per class (see Fig.  Inspired by recent works, we assume semantic instances in a point cloud X t adhere to geometric boundaries and partition the point cloud into a set of supervoxels S t  where ψ u represents the pooled class predictions, and ψ p is a pairwise similarity between supervoxels S j , S j  To further improve upon this idea, we propose to enforce temporal consistency through the use of a supervoxel matching matrix M [t,t+1] ∈ R m×n where |S t | = m and |S t+1 | = n are the dimensions of the respective supervoxel sets, reducing computational complexity to an additional comparison per supervoxel instead of n as with the OTOC+T. An entry m j,j ∈ M [t,t+1] indicates the probability that supervoxels S j and S j describe a similar region across time steps. Intuitively, this can establish consistency between the pair of point clouds by considering matched supervoxels from a different timestep X t as pseudo-label candidates. To initialize the matching matrix, we explore how nearest neighbor, unsupervised optical  After initialization through any of these priors, we propose to update the matching iteratively during training, establishing a link between temporal consistency and semantic understanding. To strengthen this dynamic and account for potentially incorrect matches, we additionally incorporate relation net R Ψ features to refine the supervoxel matching matrix M . Formally, we can define the matching update for a single entry m j,j ∈ M [t,t+1] as follows: The updated matching is the supervoxel with the highest matching probability, i.e., mj,j = arg max Sj ∈Yt+1 p(S j |S j , M [t,t+1] ). We can then additionally regularize the graph propagation (Eq. 1) using the updated matching matrix for the merged supervoxel sets Ŝ = S t ∪ S t+1 : where ψ u describes the probability of S j being assigned label y j based on the prediction f j = F Θ (S j ). ψ p describes the pairwise similarity between two supervoxels additionally based on r j = R Ψ (S j ), mean supervoxel color c j , and mean coordinate p j ."
4,Experiments,"Dataset Description. All experiments are carried out on an existing dataset of 18 laparoscopic surgeries  Each training split contains around 500 sparsely annotated point clouds, with 5436 click annotations on average per split. The densely labeled validation annotations comprise approximately 93% of the on average one million points per point cloud. To reduce the bias from a subjective annotation, we employed four different data annotators for a total annotation time of ∼115 h. To measure the robustness of our method, we split the train and validation sets into 3 nonoverlapping splits, referring to this as ""cross-validation"" despite the train and validation splits having different types of labels (sparse click and dense, respectively). For more details on data annotation and splits, as well as qualitative examples, please refer to the supplementary materials. Experimental Setup. In contrast to OTOC "
5,Results and Discussion,"Experiment 1: Baseline Comparisons and Color Ablation. To quantify the overall impact of temporal guidance in the weakly-supervised setting, we compare SegmentOR with the baseline OTOC  Results: Table  Experiment 2: Number of Clicks. A model's performance should theoretically increase with the level of supervision. We thus quantify the impact of adding up to three additional clicks on OTOC's performance. This experiment is performed on the first training and validation split, using a varying number of click annotations per class. Results: Increasing annotations by three or four times lead to an improvement of approximately 8% mIoU (see Table  Experiment 3: Application to Surgical Phase Recognition. To further assess the quality of our semantic predictions, we evaluate their impact on surgical workflow analysis (see Table "
,Conclusion.,"This work presents a novel semantic segmentation method for surgical scene understanding that significantly reduces the annotation burden by leveraging the temporal consistency of point cloud sequences. We demonstrate the effectiveness of our approach on point clouds from a surgical phase recognition dataset, which we enrich with manual 3D annotations. By incorporating self-supervised temporal priors, our method achieves a high segmentation mIoU of 73.10% using only 0.005% of annotated points. Furthermore, we establish a formal link between semantic segmentation and workflow analysis by demonstrating that our semantic predictions benefit downstream surgical phase recognition methods. Finally, we release all anonymized point clouds, annotations, and code used to ease the deployment of context-aware systems in surgical environments."
,Fig. 1 .,
,Fig. 2 .,
,Table 1 .,
,99 ± 0.19 82.77 ± 0.13 83.25 ± 0.71 83.31 ± 1.08 83.25 ± 0.71,
,Table 2 .,
,Table 3 . Downstream Surgical Phase Recognition.,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 6.
1,Introduction,"Tracking of tissue and organs in surgical stereo endoscopy is essential to enable downstream tasks in image guidance  We design SENDD to use few parameters (low memory cost) and to scale with the number of points to be tracked (adaptive to different applications). To avoid having to operate a 3D convolution over an entire volume, we use GNNs instead of CNNs. This allows applications to tune how much computation to use by using more/fewer points. SENDD is trained end-to-end. This includes the detection, description, refinement, depth estimation, and 3D flow steps. SENDD can perform frame-to-frame tracking and 3D scene flow estimation, but it could also be used as a more robust data association term for SLAM. As will be shown in Sect. 2, unlike in prior work, our proposed approach combines feature detection, depth estimation and deformation modeling for scene flow all in one. After providing relevant background for tissue tracking, we will describe SENDD, quantify it with a new IR-labelled tissue dataset, and finally demonstrate SENDD's efficiency. The main novelties are that it is both 3D and Efficient by: estimating scene flow anywhere in 3D space by using a GNN on salient points (3D), and reusing salient keypoints to calculate both sparse depth and flow at anywhere (Efficient)."
2,Background and Related Work,"Different components are necessary to enable tissue tracking in surgery: feature detection, depth estimation, deformation modelling, and deformable Simultaneous Localization and Mapping (SLAM). Our model acts as the feature detection, depth estimation, and deformation model for scene flow, all in one. Recently, SuperPoint "
3,Methods,"The SENDD model is described in Fig.  Learned Detector: Like SuperPoint  3D Flow Network: SENDD estimates the 3D flow d 3D (q) ∈ R 3 for each query q ∈ R 2 using two images (I t l , I t+1 l ), and their depth maps (D t , D t+1 ). For a means to query depth at arbitrary points, see the section on sparse depth estimation. To obtain initial matches, we match the detected points in 2D from frame I t l to I t+1 l . We use ReTRo keypoints  The graph, G, is defined with edges connecting each node to its k-nearest neighbors (k-NN) in 3D, with an optional dilation to enable a wider field of view at low cost. The positions and features of each correspondence are combined to act as features in this graph. For each node (detected point in image I t l ), its feature is: denotes a positional encoding layer  Sparse Depth Interpolation: Instead of running a depth CNN in parallel, we estimate disparity sparsely as needed by using the same feature points with another lightweight GNN. We do this as we found CNNs (eg. GANet  Loss: We train SENDD using loss on the warped stereo and flow images:  L p (A, B) is a photometric loss function of input images (A, B) that is used for both the warped depth pairs (L p (I l , I r→l )) and warped flow pairs (L p (I t , I t+1→t )), L s (V ) is a smoothness loss on a flow field V , and c denotes image color channels  We set α = 0.85, β = 150, λ d = 0.001, λ F = 0.01, λ D = 1.0, using values similar to "
4,Experiments,"We train SENDD with a set of rectified stereo videos from porcine clinical labs collected with a da Vinci Xi surgical system. We randomly select images and skip between (1, 45) frames for each training pair. We train using PyTorch with a batch size of 2 for 100,000 steps using a one-cycle learning rate schedule with maxlr = 1e-4, minlr = 4e-6. We use 64 channels for all graph attention operations. Dataset: In order to quantify our results, we generate a new dataset. The primary motivation for this is to have metrics that are not dependent on visible markers or require human labelling (which can bias to salient points). Some other datasets have points that are hand labelled in software  Quantification: Instead of using Intersection Over Union (IOU) which would fail on small segments (eg. IOU = 0 for a one pixel measurement that is one pixel off), we use endpoint error and chamfer distance between segments to quantify SENDD's performance. We compute endpoint error between the centers of each segmentation region. We use chamfer distance as well because it allows us to see error for regions that are larger or non-circular. Chamfer distance provides a metric that has an lower bound on error, in that if true error is zero then this will also be zero. Experiments: The primary motivation for creating a 3D model (SENDD) instead of 2D is that it enables applications that require understanding of motion in 3D (automation, image guidance models). That said, we still compare to the 2D version to compare performance in pixel space. To quantify performance on clips, since these models only estimate motion for frame pairs, we run SENDD for each frame pair over n frames in a clip, where n is the clip length. No relocalization or drift prevention is incorporated (the supplementary material shows endpoint error on strides other than one). For all experiments we select clips with length less than 10 s, as we are looking to quantify short term tracking methods. The 2D model has the exact same parameters and model structure as SENDD (3D), except it does not do any depth map calculation, and only runs in image space. First, we compare SENDD to the equivalent 2D model. We do this with endpoint error as seen in Fig.  Benchmarking and Model Size: SENDD has only 366,195 parameters, compared to other models which estimate just flow (RAFT-s "
5,Conclusion,"SENDD is a flexible model for estimating deformation in 3D. A limitation of SENDD is that it is unable to cope with occlusion or relocalization, and like all methods is vulnerable to drift over long periods. These could be amended by integrating a SLAM system. We demonstrate that SENDD performs better than the equivalent sparse 2D model while additionally enabling parameterization of deformation in 3D space, learned detection, and depth estimation. SENDD enables real-time applications that can rely on tissue tracking in surgery, such as long term tracking, or deformation estimation."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 4 .,
,,
,Table 1 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_23.
1,Introduction,
1.1,Background,"Traditional optical imaging samples the visual spectrum in three diffuse spectral bands (RGB), while hyperspectral imaging (HSI) provides much more detailed spectral information. This information is potentially valuable for making intraoperative decisions, particularly in cases where tissue differentiation is critical but challenging to perform using traditional visualisation techniques. In the case of brain tumour excision, fluorescence-guided resection is commonly used to minimize damage to healthy tissue  While HSI has been integrated into surgical microscope systems  The issue of reduced focal depth in real-time HSI systems could be mitigated by the introduction of a video autofocus system. Autofocus methods are divided into active methods, which use transmission to probe the scene, and passive methods, which rely only on incoming light. Passive methods are further split into phase-based, which require specialised hardware, and contrast-based, which compare images captured at different focal powers. Our investigation focuses on contrast-based methods, which require minimal hardware development. "
1.2,Related Autofocusing Works,"While autofocusing systems are prevalent in consumer device, the scientific literature is sparse, especially for dynamic video autofocusing. Many publications in the field are concerned with benchtop microscope autofocus systems "
1.3,Contributions,"This work aims to improve intraoperative handheld HSI systems by alleviating one of their main usability drawbacks, that of shortened focal depth. We introduce an autofocus system to an existing handheld intraoperative real-time HSI system "
2,Materials and Methods,
2.1,Optical System,"Our intraoperative HSI system, shown in Fig. "
2.2,Datasets,"Software Simulated Focal-Time Scans. We define a focal-time scan as a time series of focal stacks, with a focal stack being a single image captured at multiple focal lengths. In order to assemble a large and diverse focal-time scan dataset, we choose to simulate focal-time scans using existing in-focus video data. To ensure the resulting focal-time scan features diverse camera motion, we implement a smooth random walk to step a cropping rectangle across the video after each frame. This also allows for the construction of plausible focaltime scans from single images, although features such as dynamic subjects or imaging noise will be missing. In order to simulate defocus, we implement another random walk to simulate a dynamic optimal focal power. When an agent is interacting with the simulated scan, a Gaussian filter is used to approximate focal blurring with σ = σ 0 |f *f | where f and f * are the current and optimal focal powers and σ 0 is chosen randomly from the range 2-8 for each scan. We use this technique to create a training and testing dataset consisting of 1000 and 200 simulated focal-time scans based on 200 10-second video clips sampled from Cholec80  Robotic Focal-Time Scan. As a testing dataset similar to our intended use case, we chose to approximate a real focal-time scan by controlling conditions during capture of the individual focal stacks. Our optical system was fixed to a robotic arm, which was then used in a compliant control mode to record a natural hand-guided trajectory whilst imaging a brain phantom. The motion was performed to try to emulate typical usage during a surgery, whilst also trying to cover the range of plausible working distances. The focal range of the liquid lens is discretised into a set of focal powers, and the recorded trajectory is discretised into a sequence of 1184 poses. For each discrete pose, the robotic arm is fixed, and an image captured for each focal power. We randomise the order of the focal powers to reduce systematic bias caused by the response of the liquid lens. Auto-exposure was implemented in order to ensure good exposure across all working distances. To ensure consistency within a given focal stack, autoexposure was only stepped in-between discrete poses. The robotic arm holding our optical system and a sample of the resulting focal-time scan can be seen in Fig.  Integration and Usability Trial. To ensure the validity of our quantitative evaluation, and to get feedback on the system in general, a blinded trial was set up with two practising neurosurgeons. A set was made containing two repeats of three selected autofocus policies. This set was then shuffled, and the surgeons remained blinded to the autofocus policy until after the trial. Each surgeon used our optical system to inspect a brain phantom with each policy in the set. The surgeon was made aware when the policy was changed and prompted to make comments throughout the trial, which were recorded."
2.3,Autofocus Policies,"As seen in Fig.  Traditional Approach. We implement two traditional autofocus policy based on different focal metrics combined with a simple hill-climber optimisation policy. We choose mean gradient magnitude (MGM) and mean local ratio (MLR). Two focal metrics which are conceptually simple but competitive  where p is the set of all pixels in the image, I x and I y are defined as the x and y responses of a Sobel filter, and G σ is a Gaussian blur. The kernel size is chosen as σ = 4 for all our experiments. Our hill-climber optimisation policy O HC sets the focal power f at time t + 1 based on information at time t and is defined as where ) is the direction of the previous step and h is a step size which we set to h = 0.05 for all our experiments. We note that our definition is different from standard hill-climber. A normal hill-climber will repeat a step while the focal metric is increasing, and either stop or change direction with a smaller step size when the focal metric decreases, but this does not translate to a continuous and dynamic environment. Learned Optimisation Policy. Due to our dynamic environment, it seems likely that considering a sequence of the N last focal metrics, rather than the last two, would help to build a strong optimisation policy. However, as N increases, it quickly becomes unclear how to incorporate this information effectively. It is likely that a learning based solution would uncover a better strategy than heuristic approaches. While regression based approcahes may work, reinforcement learning provides a natural framework for this problem by allowing the policy to model the trade-off between maximisation and exploration. By modelling the autofocus task as a Markov process, we can define a Q-function Q(s, a) which maps state-action pairs to expected future rewards. We define our state, actions, and reward function as where f * t is the optimal focal power at t which can only be known in controlled environment. As before, we take h = 0.05. Our learned optimisation policy O RL can then be defined as To model Q(s, a), we use an MLP consisting of 2 hidden layers of 256 ReLUs each and a third layer with 3 outputs corresponding to the 3 possible actions. The MLP takes as input the state vector s containing the N most recent focal metrics and focal powers, we take N = 8 for all our experiments. To train the model, we use Deep Q Learning following the recommendations set out by the DQN method  End-to-End Model. In addition to learning the optimisation policy, we can also learn the focal metric. By learning the two together, we are no longer constrained to a scalar metric and can instead learn a latent vector encoding of the image patches. To do this, we construct a CNN consisting of 4 convolutions with 8 filters each and a stride of 2, outputting a vector of 8 logits for our patch size of 32 × 32. The CNN is run on each of the N most recent image patches as a batch during training, but only the most recent during inference, with the previous encodings stored between steps. The encodings are concatenated with the N most recent focal powers and fed into an MLP. The MLP and training procedure are the same as before."
3,Results,"We evaluated each autofocus policies on both our simulated focal-time scan test set, and the robotically recorded focal-time scan. The mean focal errors are shown in Table  During the usability trial, the surgeon participants were positive about all presented policies. In line with our quantitative results, the participants both showed preference for the CNN-based policy. It was thought by both to be smoother and more deliberate in its adjustments, and felt more robust to minor accidental motions inherent to hand-operated system. One commented that it felt slower to focus but more stable, going on to state that this was desirable behaviour. All algorithms handled the brain fissure well, this is likely due to the small patch size used, allowing for precise targeting. Overall, the surgeons were very positive about the integration of autofocus into optical imaging systems.  "
4,Conclusion,"We have successfully designed a handheld intraoperative HSI imaging system with autofocusing capability. We developed a novel CNN-based autofocus policy suitable for video data. In addition, we performed a robotic focal-time scan to evaluate our methods. Our novel method significantly outperforms a traditional baseline on our robotic focal-time scan, and performs preferably in a usability trial by two neurosurgeons. The comments from the usability trial also suggest that the dynamic video autofocusing systems will be well received among surgeons."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Table 1 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 63.
1,Introduction,"Automatic surgical video captioning is critical to understanding the surgery with complicated operations, and can produce the natural language description with given surgical videos  To generate text descriptions from input videos, existing captioning works  First, existing surgical captioning works  To address these two limitations in surgical video captioning, we propose the Surgical Concept Alignment Network (SCA-Net) to bridge the visual and text modalities through the surgical concepts, as illustrated in Fig. "
2,Surgical Concept Alignment Network,
2.1,Overview of SCA-Net,As illustrated in Fig. 
2.2,Surgical Concept Learning,"Previous surgical captioning works  To guide the visual modality to perceive surgical concepts, we aggregate visual tokens generated by the visual encoder in average, and add a linear layer to predict the surgical concepts of input videos, where the normalized output p v ∈ [0, 1] C estimates the probability of each surgical concept. We perform the multi-label classification using binary sigmoid cross-entropy loss, as follows: In this way, the visual tokens are supervised to contain discriminative semantics related to valid surgical concepts, which can reduce prediction errors in surgical descriptions. For the text modality, we also perform SCL for surgical concept prediction p t c ∈ [0, 1] C and calculate the loss L t SCL in the same way. By optimizing L SCL = L v SCL + L t SCL , the SCL enables visual and text encoders to exploit multi-modal features with the perception of surgical concepts, thereby facilitating the SCA-Net towards the captioning task."
2.3,Mutual-Modality Concept Alignment,"With the help of SCL in Sect. 2.2, our SCA-Net can perceive the shared set of surgical concepts in both visual and text modalities. However, given the differences between two modalities with separate encoders, it is inappropriate for the decoder to directly explore the cross-modal relationship between visual and text tokens  To align these two modalities, we first collect surgical concept representations for each modality. Note that text tokens are separable for surgical concepts, while visual tokens are part of the input video containing multiple surgical concepts. For text modality, we parse the label of each text token and average text tokens of each surgical concept as t c , and update the historical text concept representations { tc } C c=1 using Exponential Moving Average (EMA), as tc ← γ tc +(1-γ)t c , where the coefficient γ controls the updating for stable training and is empirically set as 0.9. For visual modality, we average visual tokens as the representation of each surgical concept present in the input video (i.e., v c if surgical concept label y cpt c = 1), and update the historical visual concept representations {v c } C c=1 with EMA, as vc ← γ vc + (1γ)v c . In this way, we obtain the text and visual concept representations with tailored strategies for the alignment. Then, we mutually align visual and text concept representations with corresponding historical ones in another modality. For visual-to-text alignment, visual concept representations are expected to be similar to corresponding text concept representations, while differing from other text concept representations as possible. Thus, we calculate the alignment objective L v→t MCA with regard to surgical concepts  where V and T denote all visual and text representations respectively, and • is the inner product of vectors. In this way, the MC-Align aligns visual and text representations with each other modality according to the surgical concept, thus benefiting multi-modal decoding for captioning. "
2.4,Optimization,"For the surgical captioning task, we adopt standard captioning loss L Cap to optimize the cross-entropy of each predicted word based on previous words y <t and input video x, as follows: where T is the length of caption prediction. Overall, the final objective of SCA-Net is summarized as L = L Cap + λ 1 L SCL + λ 2 L MCA , where loss coefficients λ 1 and λ 2 control the trade-off of SCL and MC-Align. By optimizing this final objective L, the proposed SCA-Net can achieve multi-modal concept alignment, and generate superior descriptions for the surgical video captioning."
3,Experiment,
3.1,Dataset and Implementation Details,"Neurosurgery Video Captioning Dataset. To evaluate the effectiveness of surgical video captioning, we collect a large-scale dataset with 41 surgical videos of endonasal skull base neurosurgery. These surgical videos are recorded at the Prince of Wales Hospital, Chinese University of Hong Kong, where surgeons remove pituitary tumors through the endonasal corridor to the skull base. After necessary data cleaning, we divide these surgical videos with resolution of 1, 920× 1, 080 into 11, 004 thirty-second video clips with clear surgical purposes. These video clips are annotated under Tool-Tissue Interaction (TTI) principle  EndoVis Image Captioning Dataset. We further compare our method with state-of-the-arts on the public EndoVis-2018 Image Captioning Dataset  Implementation Details. We implement our SCA-Net and state-of-the-art captioning methods  Evaluation Metrics. To evaluate the captioning performance, we adopt standard metrics, including BLEU@4 "
3.2,Comparison on Neurosurgery Video Captioning,"To evaluate the performance of our SCA-Net, we perform a comprehensive comparison with the state-of-the-art captioning methods, including Self-Seq  As illustrated in Table  Ablation Study. To further validate the effectiveness of SCL and MC-Align, we perform the detailed ablation study in Table  Qualitative Analysis. We present qualitative results of our SCA-Net and state-of-the-arts "
3.3,Comparison on EndoVis Image Captioning,"To further confirm the effectiveness of surgical captioning, we perform the comparison on the public EndoVis image captioning dataset. As shown in Table "
4,Conclusion,"To achieve accurate surgical video captioning, we propose the SCA-Net to mitigate the semantic gap of visual and text modalities with surgical concepts. Specifically, we devise the SCL to enable the SCA-Net with the perception of surgical concepts in visual and text modalities, respectively. Moreover, we propose the MC-Align to mutually coordinate visual and text representations with surgical concept representations of the other modality for multi-modal decoding, thereby generating more accurate captions with aligned multi-modal knowledge. Extensive experiments on neurosurgery and nephrectomy datasets confirm the advantage of our SCA-Net over state-of-the-arts on the surgical captioning."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Table 1 .,
,Table 2 .,
,Acknowledgments,. This work is supported by 
1,Introduction,"The recent evolution of large language models (LLMs) is revolutionizing natural language processing and their use across various sectors (e.g., academia, healthcare, business, and IT) and daily applications are being widely explored. In medical diagnosis, recent works  In the medical domain, MedfuseNet  In this work, we develop an end-to-end trainable SurgicalGPT model by exploiting a pre-trained LLM and employing a learnable feature extractor to generate vision tokens. In addition to word tokens, vision tokens (embedded with token type and pose embedding) are introduced into the GPT model, resulting in a Language-Vision GPT (LV-GPT) model. Furthermore, we carefully sequence the word and vision tokens to leverage the GPT model's robust language processing ability to process the question and better infer an answer based on the vision tokens. Through extensive experiments, we show that the SurgicalGPT(LV-GPT) outperforms other state-of-the-art (SOTA) models by ∼ 3-5% on publically available EndoVis18-VQA "
2,Proposed Method,
2.1,Preliminaries,"GPT2  Vision-Language Processing: Employed mostly for language-only tasks, GPT models do not natively process vision tokens "
2.2,LV-GPT: Language-Vision GPT,Overall Network: We design an end-to-end trainable multi-modality (language and vision) LV-GPT model (Fig. 
,Self-Attention,
,Language-Vision Processing:,"The questions are tokenized using the inherent GPT2 tokenizer. The word tokens are further embedded based on token-id, token type (0) and token position by the inherent GPT2 word embedding layers. To tokenize the input surgical scene (image) into vision tokens, the LV-GPT includes a vision tokenizer (feature extractor): ResNet18 (RN18)  where, T t () is type embedding, P pos () is pose embedding, w x and v x are initial word and vision embedding, and v t are vision tokens. Initial word embeds (w x ) are obtained using word embedding based on word token id. Depending on the size (dim) of each vision token, they undergo additional linear layer embedding (f ()) to match the size of the word token. Token Sequencing: LLMs are observed to process long sentences robustly and hold long-term sentence knowledge while generating coherent paragraphs/reports. Considering GPT's superiority in sequentially processing large sentences and its uni-directional attention, the word tokens are sequenced before the vision tokens. This is also aimed at mimicking human behaviour, where the model understands the question before attending to the image to infer an answer. Classification: Finally, the propagated multi-modality features are then passed through a series of linear layers for answer classification."
3,Experiment,
3.1,Dataset,EndoVis18-VQA: We employ publically available EndoVis18-VQA 
,Cholec80-VQA:,The classification subset of the Cholec80-VQA 
,PSI-AVA-VQA:,We introduce a novel PSI-AVA-VQA dataset that consists of Q&A pairs for key surgical frames of 8 cases of the holistic surgical scene dataset (PSI-AVA dataset) 
3.2,Implementation Details,All variants of our models
4,Results,"All our proposed LV-GPT model variants are quantitatively benchmarked (Table  on the accuracy (Acc), recall, and Fscore. In most cases, all our variants, LV-GPT (Swin), LV-GPT (RN18) and LV-GPT (ViT), are observed to significantly outperform SOTA models on all three datasets in terms of Acc. Specifically, the LV-GPT (Swin) variant (balanced performance across all datasets) is observed to outperform all SOTA models on all datasets and significantly improve the performance (∼ 3-5% improvement) on EndoVis18-VQA and Cholec80-VQA dataset. Additionally, it should be noted our model variants can be trained end-to-end, whereas, most of the SOTA models requires a region proposal network to process input image into vision tokens. Figure "
,Early Vision vs Early Word:,The performance of LV-GPT based on word and vision token sequencing (Table 
,Pose Embedding for Vision Tokens:,"The influence of positional embedding of the vision tokens (representing a patch region) in all the LV-GPT variants is studied by either embedded with position information (pos = 1, 2, 3, .., n.) or zero-position (pos = 0). Table "
,Ablation Study on Vision Token,Embedding: An ablation study on the vision token embedding in the LV-GPT model on the EndoVis18-VQA dataset is also shown in Table 
5,Conclusion,"We design an end-to-end trainable SurgicalGPT, a multi-modality Language-Vision GPT model, for VQA tasks in robotic surgery. In addition to GPT's inherent word embeddings, it incorporates a vision tokenizer (trainable feature extractor) and vision token embedding (type and pose) to perform multimodality tasks. Furthermore, by carefully sequencing the word tokens earlier to vision tokens, we exploit GPT's robust language processing ability, allowing the LV-GPT to significantly perform better VQA. Through extensive quantitative analysis, we show that the LV-GPT outperforms other SOTA models on three surgical-VQA datasets and sequencing word tokens early to vision tokens significantly improves the model performance. Furthermore, we introduce a novel surgical-VQA dataset by adding VQA annotations to the publically available holistic surgical scene dataset. While multi-modality models that process vision and language are often referred to as ""vision-language"" models, we specifically name our model ""language-vision GPT"" to highlight the importance of the token sequencing order in GPT models. Integrating vision tokens into GPT also opens up future possibilities of generating reports directly from medical images/videos."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Table 1 .,
,Table 2 .,
,Table 3 .,
,Table 4 .,
1,Introduction,"A cochlear implant (CI) has an electrode array (EA) that is surgically inserted into the cochlea to stimulate the auditory nerves and treat patients with severe-to-profound sensorineural hearing loss. Although CIs have achieved great success, hearing outcomes among recipients vary significantly  To facilitate the EA localization process, automated methods have been proposed by several groups. In  In this work, we present a novel DL-based framework that consists of a multi-task network and a set of postprocessing algorithms to localize the cochlear implant electrode array. Our contribution is three-fold: (1) To the best of our knowledge, it is the first unified DL-based framework designed for localizing both distantly-and closely-spaced EAs in CT and cone-beam CT (CBCT) images. "
2,Method,
2.1,Data,"The data in this study consist of a large-scale clinical dataset from CI recipients (dataset #1) as well as 27 cadaveric samples (dataset #2). Dataset #1 includes 1324 implanted ears from CI recipients treated at two institutions (datasets #1A and #1B). 8 types of distantlyand closely-spaced EA from 3 manufacturers are included in these cases. Dataset #1A has 958 implanted ears of which 97% are scanned with CBCT scanners, and the remaining (3%) are scanned with conventional CT scanners. Dataset #1B includes 366 implanted ears of which most (98%) are scanned with conventional CT scanners and the remaining (2%) are scanned with CBCT scanners. Dataset #2 contains 4 types of distantly-spaced EAs, and these specimens are scanned with conventional CTs. The training and validation sets are all from dataset #1A and constitute 60% and 20% of that dataset, respectively. The remaining 20% of dataset #1A along with dataset #1B (561 implanted ears in total) are used to test the robustness of the proposed framework. Dataset #2 is used to test its accuracy because the gold standard ground truth can be obtained using their paired micro-CT "
2.2,Multi-task U-Net,"The proposed framework is designed to localize various types of EAs in both CT and CBCT. This is challenging because the number of electrodes to be detected and the interelectrode spacing is different among EA models, and the postoperative images can have different intensity characteristics depending on the type of scanner, i.e., CT or CBCT, that is used for their acquisition. To normalize the input image in a way that enhances both the nearby anatomy, which contains contextual information, and the high intensity (usually the electrodes) component, inspired by  To avoid having to train a network for each EA type, we define four tasks that a single multi-task network can learn simultaneously and whose output can be used to localize and order the electrodes in all arrays. Specifically, as shown in Fig.  We train the multi-task network using the electrode positions obtained with the methods described in  Depending on the image quality and the interelectrode spacing, EAs can appear as a whole bright (i.e., with high Hounsfield Unit (HU) values) tubular structure or distinguishable bright blobs representing individual electrodes. We define the centerline of the EA as a line connecting each electrode in sequential order (from most apical to most basal or the opposite). The motivation for segmenting the EA centerline is twofold. First, after we extract all the electrodes from the predicted heatmap (Task #1), we need to order them. We will show later in our postprocessing algorithms that these detected electrodes can be linked in the correct order using the segmented centerline and the detected two endpoints (Algorithm 1 in Fig. "
2.3,Postprocessing Algorithms for EA Localization,"As said above, although the output of the multi-task network contains essential information to identify the EA, it does not provide the desired final output, i.e., the position (coordinates) of the ordered electrodes in the image. To do so, we have designed a series of postprocessing algorithms that can effectively extract the ordered electrode locations from the four output maps. As shown in Fig.  Algorithm 1 takes the heatmap of all the electrodes (Task #1) as input and utilizes a non-maximum suppression (NMS) algorithm, which is a common postprocessing step for object detection  However, for closely-spaced EAs, it is nearly impossible to differentiate the individual electrodes. Algorithm 2 is designed to localize EAs in such situations. After the extraction and refinement of the EA centerline, the centerline is smoothed with a cubic spline, and the final electrode positions are obtained by resampling along it using known interelectrode spacing for the EA "
3,Experiments and Results,
3.1,Implementation Details,The multi-task U-Net is trained with PyTorch 1.12 on an NVIDIA RTX 2080 Ti GPU. We use MONAI 
3.2,Evaluation and Results,"The techniques described in  As introduced in Sect. 2.1, the 561 clinical cases in dataset #1 used for testing are highly heterogeneous. They contain CBCT and CT images from dataset #1A and #1B. Since manual annotations are not available for these images, we ask three experts to evaluate and compare the results obtained with the previous SOTA methods (  Before performing the expert evaluation, we calculate the maximum P2PE between the results from the previous SOTA and the proposed methods. For cases with maximum P2PE larger than 0.3 mm (we refer to them as the large-error subset), there is a high probability that at least one of the methods generates NA results. For cases with maximum P2PE smaller than 0.3 mm (we refer to them as the small-error subset), we presume that the probability of NA results is relatively small. To limit the demand on the experts' time, we ask three experts (R1, R2, and R3) to rate the large error subset (164 cases) and only one expert (R1) to rate the small error subset (397 cases). The expert evaluation results are shown as radar plots in Fig. "
4,Conclusions,"In this work, we have proposed a novel DL-based framework for cochlear implant EA localization. To the best of our knowledge, it is the first unified DL-based framework designed for localizing both distantly-and closely-spaced EAs in CT and CBCT images. Compared to the SOTA methods, the proposed framework is substantially more robust (9% less NA results) when evaluated on a large-scale clinical dataset and achieves slightly more accurate localization results on a dataset containing 27 cadaveric samples with gold standard ground truth. While it may be possible to improve our success rate further, a low percentage of NA results is unavoidable. We are thus developing quality assessment techniques to alert end users when images have poor quality and/or results are unreliable."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
1,Introduction,"Surgical procedures are becoming increasingly complex, requiring intricate coordination between medical staff, patients, and equipment  ating room (OR) management is critical for improving patient outcomes, optimizing surgical team performance, and developing new surgical technologies  In endoscopic video analysis, using temporality has become standard practice  Both of these approaches have some downsides. The two-stage approaches are not trained end-to-end, potentially limiting their performance. Additionally, certain design choices must be made regarding which feature from every timepoint should be used as a temporal summary. For scene graph generation, this can be challenging, as most SGG architectures work with representations per relation and not per scene. The end-to-end 3D methods, on the other hand, are computationally expensive both in training and inference and practical hardware limitations mean they can only effectively capture short-term context. Finally, these methods can only provide limited insight into which temporal information is the most useful. In the computer vision community, multiple studies on scene understanding using scene graphs  In this paper, we propose LABRAD-OR(Lightweight Memory Scene Graphs for Accurate Bimodal ReAsoning in Dynamic Operating Rooms), a novel and lightweight approach for generating accurate and consistent scene graphs using the temporal information available in OR recordings. To this end, we introduce the concept of memory scene graphs, where, for the first time, the scene graphs serve as both the output and the input, integrating temporality into the scene graph generation. Our motivation behind using scene graphs to represent memory is twofold. First, by design, they summarize the most relevant information of a scene, and second, they are lightweight and interpretable, unlike a latent feature-based representation. We design an end-to-end architecture that fuses this temporal information with visual information. This bimodal approach not only leads to significantly higher scene graph generation accuracy than the state-of-the-art but also to better inter-timepoint consistency. Additionally, by choosing lightweight architectures to encode the memory scene graphs, we can integrate the entire temporal information, as human-interpretable scene graphs, with only 40% overhead. We show the effectiveness of our approach through experiments and ablation studies."
2,Methodology,"In this section, we introduce our memory scene graph-based temporal modeling approach (LABRAD-OR), a novel bimodal scene graph generation architecture for holistic OR understanding, where both the visual information, as well as the temporal information in the form of memory scene graphs, are utilized. Our architecture is visualized in Fig. "
2.1,Single Timepoint Scene Graph Generation,We build up on the 4D-OR 
2.2,Scene Graphs as Memory Representations,"In this study, we investigate the potential of using scene graphs from previous timepoints, which we refer to as ""memory scene graphs"", to inform the current prediction. Unlike previous research that treated scene graphs only as the final output, we use them both as input and output. Scene graphs are particularly well-suited for encoding scene information, as they are low-dimensional and interpretable while capturing and summarizing complex semantics. To create a memory representation at a timepoint T, we use the predicted scene graphs from timepoints 0 to T-1 and employ a neural network to compute a feature representation. This generic approach allows us to easily fuse the scene graph memory with other modalities, such as images or point clouds. Memory Modes: While our efficient memory representation allows us to look at all the previous timesteps, this formulation has two downsides. Surgical duration differs between procedures, and despite the efficiency of scene graphs, prolonged interventions can still be costly. Second, empirically we find that seeing the entire memory leads to prolonged training time and can cause overfitting. To address this, we propose four different memory modes, ""All"", ""Short"", ""Long"", and ""LongShort"". The entire surgical history is visible only in the ""All"" mode. In the ""Short"" mode, only the previous S scene graphs are utilized, while in the ""Long"" mode, every S.th scene graph is selected using striding. In ""LongShort"" mode, both ""Long"" and ""Short"" modes are combined. The reasoning behind this approach is that short-term context should be observed in detail, while longterm context can be viewed more sparsely. This reduces computational overhead compared to ""All"" and leads to better results with less overfitting, as observed empirically. The value of S is highly dependent on the dataset and the surgical procedures under analysis (Fig. "
2.3,Architecture Design,"We extract the visual information from point clouds and, optionally, images using a visual scene encoder, as described in Sect. 2.1. To integrate the temporal information, we convert each memory scene graph into a feature vector using a graph neural network. Then we use a Transformer block  Memory Augmentations: While we use the predicted scene graphs from previous timepoints for inference, as these are not available during training, we use the ground truth scene graphs for training. However, training with ground truth scene graphs and evaluating with predicted scene graphs can decrease test performance, as the predicted scene graphs are imperfect. To increase our robustness towards this, we utilize memory augmentations during training. Concretely, we randomly replace part of either the short-term memory (timepoints closest to the timepoint of interest) or the long-term memory (timepoints further away from the timepoint of interest) with a special ""UNKNOWN"" token. Intuitively, this forces our model to rely on the remaining information and better deal with wrong predictions in the memory during inference. Timepoint of Interest(ToI) Positional Ids: Transformers  Multitask Learning: In the scene graph generation task, the visual and temporal information are used together. In practice, we found it valuable to introduce a secondary task, which can be solved only using temporal information. We propose the task of ""main action recognition"", where instead of the scene graph, only the interaction of the head surgeon to the patient is predicted, such as ""sawing"" or ""drilling"". During training, in addition to fusing the memory representation with the visual information, we use a fully connected layer to estimate the main action from the memory representation directly. Learning both scene graph generation and main action recognition tasks simultaneously gives a more direct signal to the memory encoder, resulting in faster training and improved performance. Table "
,Method,4D-OR 
3,Experiments,"Dataset: We use the 4D-OR  Model Training: Our architecture consists of two components implemented in PyTorch 1.10, a visual model and a memory model. For our visual model, we use the current SOTA model from 4D-OR  We use the provided human and object pose predictions from 4D-OR and stick to their training setup and evaluation metrics. We use memory augmentations, timepoint of interest positional ids, end-to-end training, and multitask learning. The memory encoders are purposefully designed to be lightweight and fast. Therefore, we use a hidden dimension of 80 and only two layers. We use S, to control both the stride of the ""Long"" mode and the window size of the ""Short"" mode and set it to 5. The choice of S ensures we do not miss any phase, as all phases last longer than 5 timepoints while reducing the computational cost. Unless otherwise specified, we use ""LongShort"" as memory mode and train all models until the validation performance converges. Evaluation Metrics: We use the official evaluation metrics from 4D-OR for semantic scene graph generation and the role prediction downstream tasks. In both cases, a macro F1 over all the classes is computed. Further, we introduce a consistency metric, where first, for each timepoint, a set of predicates P t , such as {""assisting"", ""drilling"", ""cleaning""} is extracted from the scene graphs. Then, for two timepoints T and T -1, the intersection of union(IoU) between P t and P t-1 is computed. This is repeated for all pairs of adjacent timepoints in a sequence, and the IoU score is averaged over them to calculate the consistency score. Table  Method PC PC+Img PC+T PC+Img+T GT Consistency 0.83 0.84 0.86 0.87 0.9 Fig. "
4,Results and Discussion,Scene Graph Generation. In Table  A qualitative example of the improvement can be seen in Fig.  Clinical Role Prediction. We also compare LABRAD-OR to 4D-OR on the downstream task of role prediction in Table  Ablation Studies. We conduct multiple ablation studies to motivate our design choices. In Table 
5,Conclusion,"We propose LABRAD-OR, a novel lightweight approach based on human interpretable memory scene graphs. Our approach utilizes both the visual information from the current timepoint and the temporal information from the previous timepoints for accurate bimodal reasoning in dynamic operating rooms. Through experiments, we show that this leads to significantly improved accuracy and consistency in the predicted scene graphs and an increased score in the downstream task of role prediction. We believe LABRAD-OR offers the community an effective and efficient way of using temporal information for a holistic understanding of surgeries."
,,
,Table 3 .,
,Table 4 .,
,Table 5 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_29.
1,Introduction,"During colonoscopy screenings, localizing the camera and reconstructing the colon directly from the video feed could improve the detection of polyps and help with navigation. Such tasks can be either treated individually using depth  To address the lack of data in surgery, previous work has explored both synthetic pose and depth data generation  Neural Radiance Field (NeRF)  This paper aims to mitigate the depth and pose data scarcity in colonoscopy. Inspired by work in data generation using NeRF "
2,Method,Our method requires a set of images with known intrinsic and extrinsic camera parameters and sparse depth maps of a colonoscopy sequence. This information is already available in high-quality VSLAM 
2.1,Neural Radiance Fields,"A NeRF  Similarly, the expected ray termination distance D p can be computed from Eq. (3), which is an estimate of how far a ray travels from the camera until it hits solid geometry. D p can be converted to z-depth by knowing the uv coordinates of the corresponding pixel and camera model. In practice, NeRF uses two pairs of MLPs. Initially, a coarse NeRF F Θc is evaluated on N c samples along a ray. The opacity output of the coarse network is used to re-sample rays with more dense samples where opacity is higher. The new N f ray samples are used to query a fine NeRF F Θf . During both training and inference, both networks are working in parallel. Lastly, to enable NeRF to encapsulate high-frequency geometry and color details, every input of F Θ is processed by a hand-crafted positional encoding module γ(•), using Fourier features "
2.2,Extending NeRF for Endoscopy,"Light-Source Location Aware MLP. During a colonoscopy, the light source always moves together with the camera. Light source movement results in illumination changes on the tissue surface as a function of both viewing direction (specularities), camera location (exposure changes), and distance between the tissue and the light source (falloff) Fig.  Depth Supervision. NeRF achieves good 3D reconstruction of scenes using images captured from poses distributed in a hemisphere  Training a NeRF on colonoscopy sequences is hard because the camera moves along a narrow tube-like structure and the colon wall is often texture-less. Supervising depth together with color can guide NeRF to learn a good 3D representation even when pose distribution is sub-optimal  • 1 is the L1 loss, • 2 2 is the L2 loss, U denotes uniform sampling."
3,Experiments and Results,
3.1,Dataset,We train and evaluate our method on C3VD 
3.2,Implementation Details,"Before training, we spatially re-scale and shift the 3D scene from each video sequence such that every point is enclosed within a cube with a length of two, centered at (0,0,0). Prescaling is important for the positional encoding module γ(x) to work properly. We configure positional encoding modules to compute 10 frequencies for each component of x and 4 frequencies for each component of d and o. We train models on images of 270 × 216 resolution to ignore both depth and RGB information outside a circle with a radius of 130 pixels centered at a principal point to avoid noise due to inaccuracies of the calibration model. We used Adam optimizer "
3.3,Model Ablation Study,"We ablate our model showing the effects of conditioning NeRF on the light source location and supervising depth. To assess RGB reconstruction, we measure the peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) between reconstructed and reference images at 270 × 216 resolution. We evaluate depth using the mean squared error (MSE) in the original dataset scale. For each metric, we report the average across all sequences together with the average standard deviation in Table "
3.4,Data Generation,We directly use our proposed model of the d4v2 C3VD scene from the ablation study to render novel views and show results in Fig. 
4,Conclusion,"We presented an approach for expanding existing VSLAM datasets by rendering RGB images and their associated depth maps from user-defined camera poses and models. To achieve this task, we propose a novel variant of NeRF, conditioned on the location of the light source in 3D space and incorporating sparse depth supervision. We evaluate the effects of our contributions on phantom datasets and show that our work effectively adapts NeRF techniques to the lighting conditions present in endoscopy. We further demonstrate the efficacy of our method by showing RGB images and their associated depth maps rendered from novel views of the target endoscopic scene. 3D information and conditioning NeRF based on the light source location made NeRF suitable for use in Endoscopy. Currently, our method assumes a static environment and requires accurate camera intrinsic and extrinsic information. Subsequent work can incorporate mechanisms to represent deformable scenes "
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Table 1 .,
1,Introduction,"Cranial damage is a common outcome of traffic accidents, neurosurgery, and warfare. Each year, thousands of patients require personalized cranial implants  The problem can be formulated as a shape completion task and solved by dedicated neural networks. Its importance motivated researchers to organize two editions of the AutoImplant challenge, during which researchers proposed several unique contributions  The current state-of-the-art solutions, even though they reconstruct the defects accurately, share some common disadvantages. First, they operate in the volumetric domain and require significant computational resources. The GPU memory consumption scales cubically with the volume size. Second, the most successful solutions do not take into account data sparsity. The segmented skulls are binary and occupy only a limited part of the input volume. Thus, using methods dedicated to 3-D multi-channel volumes is resource-inefficient. Third, the final goal of the defect reconstruction is to propose models ready for printing/manufacturing. Working with volumetric representation requires further postprocessing to transfer the reconstructed defect into a manufacturable model. Another approach, yet still unexplored, to cranial defect reconstruction is the use of deep networks dedicated to point clouds (PCs) processing. Since the introduction of PointNet  The problem of cranial defect reconstruction can be reformulated into PC completion which has several advantages. First, the representation is sparse, and thus requires significantly less memory than the volumetric one. Second, PCs are unordered collections and can be easily splitted and combined, enabling further optimizations. Nevertheless, the current PCs completion methods focus mostly on data representing object surfaces and do not explore large-scale PCs representing solid objects. In this work, we reformulate the problem from volumetric segmentation into PC completion. We propose a dedicated method to complete large-scale PCs representing solid objects. We extend the geometric aware transformers "
2,Methods,
2.1,Overview,"The input is a 3-D binary volume representing the defective skull. The output is a PC representing the missing skull fragment and (optionally) its meshed and voxelized representation. The processing pipeline consists of: (i) creating the PC from the binary volume, (ii) splitting the PC into a group of coarse PCs, (iii) calculating the missing PC by the geometric aware transformer for each group, (iv) merging the reconstructed coarse PCs, (v) optional voxelization and postprocessing for evaluation. The pipeline is shown in Fig. "
2.2,Preprocessing,"The preprocessing starts with converting the binary volume to the PC. The coordinates of the positive voxels are created only from the voxels representing the skull. The PC is normalized to [0-1] range, randomly permuted, and split into N equal groups, where N is calculated based on the number of points in the input PC in a manner that each group contains 32768 randomly sampled input points and outputs 16384 points. Thus, the N depends on the number of positive voxels. The higher the resolution, the more groups are being processed. The number of outputs points is lower than the input points because we assumed that the defect is smaller than the skull."
2.3,Network Architecture -Point Cloud Completion Transformer,We adapt and modify the geometry-aware transformers (PoinTr)  We modify the network by replacing the FoldingNet 
2.4,Objective Function,"We train the network using a fully supervised approach where the ground-truth is represented by PCs created from the skull defects. In contrast to other PC completion methods, we employ the Density Aware Chamfer Distance (DACD)  where P r , P gt are the reconstructed and ground-truth PC respectively, S is the number of points in P rec , k is the number of nearest neighbours of point i, α is the weighting parameter. We apply the objective function to all PC ablation studies unless explicitly stated otherwise. The volumetric ablation studies use the soft Dice score. The traditional objective functions like Chamfer Distance (CD) "
2.5,Iterative Completion,"The coarse PCs are processed by the network separately. Afterwards, the reconstructed PCs are combined into the final reconstruction. To improve the results, the process may be repeated M times with a different initial PC split and a small Gaussian noise added. The procedure improves the method's performance and closes empty holes in the voxelized representation. The optional multi-step completion is performed only during the inference. The iterative completion allows one to significantly reduce the GPU memory usage and the number of network parameters. The PCs are unordered collections and can be easily split and merged. There is no need to process large PCs in one shot, resulting in the linear growth of inference time and almost constant GPU memory consumption."
2.6,Postprocessing,"The reconstructed PCs are converted to mesh and voxelized back to the volumetric representation, mainly for evaluation purposes. The mesh is created by a rolling ball pivoting algorithm using the Open3D library "
2.7,Dataset and Experimental Setup,"We use the SkullBreak and SkullFix datasets  We perform several ablation studies. We check the influence of the input physical spacing on the reconstruction quality, training time, and GPU memory consumption. Moreover, we check the generalizability by measuring the gap between the results on the training and the external testing set for each method. We compare our method to the methods dedicated to PC completion: (i) PCNet  We trained our network separately on the SkullBreak and SkullFix datasets. The results are reported for the external test set containing 100 cases for Skull-Break and 110 cases for the SkullFix datasets, the same as in the methods used for comparison. The models are implemented in PyTorch "
3,Results,"The comparison in terms of the Dice coefficient (DSC), boundary Dice coefficient (BDSC), 95th percentile of Hausdorff distance (HD95), and Chamfer distance (CD), are shown in Table  The results present that the quantitative outcomes of the proposed method are comparable to the state-of-the-art methods, however, with significantly lower GPU memory consumption that makes it possible to perform the reconstruction at the highest available resolution. The results are slightly worse when compared to the volumetric methods, however, significantly better than other PC-based approaches. Table "
4,Discussion,"The reconstruction quality of the method is comparable to the volumetric networks, as shown in Table  In future work, we plan to further reformulate the problem and, similarly to Kroviakov et al.  To conclude, we proposed a method for cranial defect reconstruction by formulating the problem as the PC completion task. The proposed algorithm achieves comparable results to the best-performing volumetric methods while requiring significantly less computational resources. We plan to further optimize the model by working directly at the skull contour and heavily augmenting the PCs."
,Fig. 1 .,
,Fig. 2 .,
,Table 2 .,
1,Introduction,"Foundation models pre-trained on large-scale data have recently showed success in various downstream tasks on medical images including classification  Existing work on foundation models for medical tasks, such as X-ray diagnosis  To learn rich spatial-temporal information from endoscopy video data  In this paper, we propose Endo-FM, a novel foundation model designed for endoscopic video analysis. First, we build a video transformer based on ViT "
2,Method,"To begin with, we build a video transformer as the architecture of our Endo-FM (Sect. 2.1). Then, we propose a novel self-supervised spatial-temporal matching scheme (Sect. 2.2). Finally, we describe the overall training objective and specifics in Sect. 2.3. An overview of our method is shown in Fig. "
2.1,Video Transformer for Spatial-Temporal Encoding,"We build a video transformer to encode input endoscopic video. The spatial and temporal attention mechanisms in our model capture long-range dependencies across both spatial and temporal dimensions, with a larger receptive field than conventional convolutional kernels  where MHSA denotes multi-head self-attention, LN denotes LayerNorm  Different from static positional encoding in ViT "
2.2,Self-supervised Pre-train via Spatial-Temporal Matching,"Considering the difficulties of tackling the context information related with lesions, tissues, and dynamic scenes in endoscopic data, we pre-train Endo-FM to be robust to such spatial-temporal characteristics. Inspired by self-supervised vision transformers  Cross-View Matching. Different from image-based pre-training  ) are predicted from the online local views processed by the student ({p s v j l } L j=1 ). By adopting this strategy, our model learns high-level context information from two perspectives: 1) spatial context in terms of the possible neighboring tissue and lesions within a local spatial crop, and 2) temporal context in terms of the possible presence of lesions in the previous or future frames of a local temporal crop. Thus, our method effectively addresses the proportion and existence issues that may be encountered. We minimize the following loss for cross-view matching: (2) Dynamic Motion Matching. In addition to the proportion and existence issues of lesions, a further challenge arises from the inherent dynamic nature of the scenes captured in endoscopy videos. The speeds and ranges of motion can vary greatly across different videos, making it difficult to train a model that is effective across a wide range of dynamic scenarios. The previous model  where 1[•] is an indicator function."
2.3,Overall Optimization Objective and Pre-training Specifics,"The overall training objective for Endo-FM is L pre-train = L cv + L dm . Centering and sharpening schemes  For Endo-FM, we set the patch size P as 16 and embedding dimension D as 768. We create G = 2 global views and L = 8 local views for every input endoscopy video, where T g ∈ "
3,Experiment,
3.1,Datasets and Downstream Setup,"We collect all possible public endoscope video datasets and a new one from Baoshan Branch of Renji Hospital for pre-training. As shown in Table  For downstream fine-tuning, we utilize the following setup: 1) PolypDiag: A randomly initialized linear layer is appended to our pre-trained Endo-FM. We sample 8 frames with spatial size 224 × 224 for every video as the input and train for 20 epochs. 2) CVC-12k: A TransUNet equipped with Endo-FM as the backbone is implemented. We resize the spatial size as 224×224 and train for 150 epochs. 3) KUMC: we implement a STFT "
3.2,Comparison with State-of-the-Art Methods,"We compare our method with recent SOTA video-based pre-training methods, including the TimeSformer  Quantitative comparison results are shown in Table "
3.3,Analytical Studies,"Without loss of generality, we conduct ablation studies on polyp diagnosis task from 3 aspects: 1) components analysis of our pre-training method; 2) varying combinations of global and local views in spatial-temporal matching; 3) varying the construction of global and local views. Components Analysis. We first study each component in our approach, as shown in Fig. "
,Z,
,Fig. 1 .,
,,
,Fig. 2 .,
,Fig. 3 .,
,Table 1 .,
,Table 2 .,
,Acknowledgements,. This work was supported in part by 
,Construction of Global and Local,Views. We conduct a further analysis of the strategies for constructing global (G ∈ 
4,Conclusion and Discussion,"To the best of our knowledge, we develop the first foundation model, Endo-FM, Which is specifically designed for analyzing endoscopy videos. Endo-FM is built upon a video transformer to capture rich spatial-temporal information and pre-trained to be robust to diverse spatial-temporal variations. A largescale endoscope video dataset with over 33K video clips is constructed. Extensive experimental results on 3 downstream tasks demonstrate the effectiveness of Endo-FM, significantly outperforming other state-of-the-art video-based pretraining methods, and showcasing its potential for clinical application. Regarding the recent SAM "
1,Introduction,"Despite that deep learning models have shown success in surgical data science to improve the quality of surgical intervention  Imitation learning has been widely studied in various domains  In this paper, we explore an interesting task of predicting dissection trajectories in ESD surgery via imitation learning on expert video data. We propose Implicit Diffusion Policy Imitation Learning (iDiff-IL), a novel imitation learning approach for dissection trajectory prediction. To effectively model the surgeon's behaviors and handle the large variation of surgical scenes, we leverage implicit modeling to express expert dissection skills. To address the limitations of inefficient training and unstable performance associated with EBM-based implicit policies, we formulate the implicit policy using an unconditional diffusion model, which demonstrates remarkable ability in representing complex high-dimensional data distribution for videos. Subsequently, to obtain predictions from the implicit policy, we devise a conditional action inference strategy with the guidance of forward-diffusion, which further improves the prediction accuracy. For experimental evaluation, we collected a surgical video dataset of ESD procedures, and preprocessed 1032 short clips with dissection trajectories labelled. Results show that our method achieves superior performances in different contexts of surgical scenarios compared with representative popular imitation learning methods."
2,Method,"In this section, we describe our approach iDiff-IL, which learns to predict the dissection trajectory from expert video data using the implicit diffusion policy. An overview of our method is shown in Fig. "
2.1,Implicit Modeling for Surgical Dissection Decision-Making,"In our approach, we formulate the dissection trajectory prediction to an imitation learning from expert demonstrations problem, which defines a Markov Decision Process (MDP) M = (S, A, T , D), comprising of state space S, action set A, state transition distribution T , and expert demonstrations D. The goal is to learn a prediction policy π * (a|s) from a set of expert demonstrations D. The input state of the policy is a clip of video frames s = {I t-L+1 , I t-L+2 , . . . , I t }, I t ∈ R H×W ×3 and the output is an action distribution of a sequence of 2D coordinates a = {y t+1 , y t+2 , ..., y t+N }, y t ∈ R 2 indicating the future dissection trajectory projected to the image space. In order to obtain the demonstrated dissection trajectories from the expert video data, we first manually annotate the dissection trajectories on the video frame according to the moving trend of the instruments observed from future frames, then create a dataset D = {(s, a) i } M i=0 containing M pairs of video clip (state) and dissection trajectory (action). To precisely predict the expert dissection behaviors and effectively learn generalizable features from the expert demonstrations, we use the implicit model as our imitation policy. Extending the formulation in  To learn the implicit policy from the demonstrations, we adopt the Behavior Cloning objective which is to essentially minimize the Kullback-Leibler (KL) divergence between the learning policy π θ (a|s) and the demonstration distribution D, also equivalent to maximize the expected log-likelihood of the joint state-action distribution, as shown: ( In this regard, the imitation of surgical dissection decision-making is converted to a distribution approximation problem."
2.2,Training Implicit Policy as Diffusion Models,"Approximating the joint state-action distribution in Eq. 1 from the video demonstration data is challenging for previous EBM-based methods. To address the learning of implicit policy, we rely on recent advances in diffusion models. By representing the data using a continuous thermodynamics diffusion process, which can be discretized into a series of Gaussian transitions, the diffusion model is able to express complex high-dimensional distribution with simple parameterized functions. In addition, the diffusion process also serves as a form of data augmentation by adding a range of levels of noise to the data, which guarantees a better generalization in high-dimensional state space. As shown in Fig.  The probability of the noised data x t in forward diffusion process is a Gaussian distribution expressed as q(x t |x 0 ) = N (x t , √ α t x 0 , (1α t )I ), where α t is a scheduled variance parameter, which can be referred from  and Σ θ (x t , t) are the means and the variances parameterized by a neural network. To train the implicit diffusion policy, we maximize the log-likelihood of the state-action distribution in Eq. 1. Using the Evidence Lower Bound (ELBO) as the proxy, the likelihood maximization can be simplified to a noise prediction problem, more details can be referred to  where s and a are sampled from N (0, I s ), N (0, I a ) respectively. To better process features from video frames and trajectories of coordinates, we employ a variant of the UNet as the implicit diffusion policy network, where the trajectory information is fused into feature channels via MLP embedding layers. Then the trajectory noise is predicted by an MLP branch at the bottleneck layer."
2.3,Conditional Sampling with Forward-Diffusion Guidance,"Since the training process introduced in Sect. 2.2 is for unconditional generation, the conventional sampling strategy through the reverse process will predict random trajectories in expert data. An intuitive way to introduce the condition into the inference is to input the video clip as the condition state s * to the implicit diffusion policy directly, then only sample the action part. But there is a mismatch between the distribution of the state s * and the s t in the training process, which may lead to inaccurate predictions. Hence, we propose a sampling strategy to correct such distribution mismatch by introducing the forward-process guidance into the reverse sampling procedure. Considering the reverse process of the diffusion model, the transition probability conditioned by s * can be decomposed as: where x t = (s t , a t ), p θ (x t-1 |x t ) denotes the learned denoising function of the implicit diffusion model, and q(s t |s * ) represents a forward diffusion process from the condition state to the t-th diffused state. Therefore, we can attain conditional sampling via the incorporation of forward-process guidance into the reverse sampling process of the diffusion model. The schematic illustration of our sampling approach is shown in Fig. "
3,Experiments,
3.1,Experimental Dataset and Evaluation Metrics,"Dataset. We evaluated the proposed approach on a dataset assembled from 22 videos of ESD surgery cases, which are collected from the Endoscopy Centre of the Prince of Wales Hospital in Hong Kong. All videos were recorded via Olympus microscopes operated by an expert surgeon with over 15 years of experience in ESD. Considering the inference speed, we downsampled the original videos to 2FPS frames which are resized to 128 × 128 in resolution. The input state is a 1.5-s length video clip containing 3 consecutive frames, and the expert dissection trajectory is represented by a 6-point polyline indicating the tool's movements in future 3 s. We totally annotated 1032 video clips, which contain 3 frames for each clip. We randomly selected 742 clips from 20 cases for training, consisting of 2226 frames, where 10% of these are for validation. The remaining 290 clips (consisting of 970 frames) were used for testing. Experiment Setup. First, to study how the model performs on data within the same surgical context as the training data, we define a subset, referred as to the ""in-the-context"" testing set, which consists of consecutive frames selected from the same cases as included in the training data. Second, to assess the model's ability to generalize to visually distinct scenes, we created an ""out-ofthe-context"" testing set that is composed of video clips sampled from 2 unseen surgical cases. The sizes of these two subsets are 224 and 66 clips, respectively. Evaluation Metrics. To evaluate the performance of the proposed approach, we adopt several metrics, including commonly used evaluation metrics for trajectory prediction as used in "
3.2,Comparison with State-of-the-Art Methods,"To evaluate the proposed approach, we have selected popular baselines and stateof-the-art methods for comparison. We have chosen the fully supervised method, Behavior Cloning, as the baseline, which is implemented using a CNN-MLP network. In addition, we have included iBC  As shown in Table "
3.3,Ablation Study,"Implicit Modeling. First, we examined the importance of using implicit modeling as the policy representation. We simulated the explicit form of the imitation policy by training a conditional diffusion model whose conditional input is a video clip. According to the bar charts in Fig.  Forward-Diffusion Guidance. We also investigated the necessity of the forward-diffusion guidance in conditional sampling for prediction accuracy. We remove the forward-diffusion guidance during the action sampling procedure so that the condition state is directly fed into the policy while sampling actions through the reverse process. As shown in Fig.  Value of Synthetic Data. Since the learned implicit diffusion policy is capable of generating synthetic expert dissection trajectory data, which can potentially reduce the expensive annotation cost. To better explore the value of such synthetic expert data for downstream tasks, we train the baseline model with the generated expert demonstrations. We randomly generated 9K video-trajectory pairs by unconditional sampling from the implicit diffusion policy. Then, we train the BC model with different data, the pure expert data (real), synthetic data only (synt) and the mixed data with the real and the synthetic (mix). The table in Fig. "
4,Conclusion,"This paper presents a novel approach on imitation learning from expert video data, in order to achieve dissection trajectory prediction in endoscopic surgical procedure. Our iDiff-IL method utilizes a diffusion model to represent the implicit policy, which enhances the expressivity and visual generalizability of the model. Experimental results show that our method outperforms state-of-the-art approaches on the evaluation dataset, demonstrating the effectiveness of our approach for learning dissection skills in various surgical scenarios. We hope that our work can pave the way for introducing the concept of learning from expert demonstrations into surgical skill modelling, and motivate future exploration on higher-level cognitive assistance in computer-assisted intervention."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Table 1 .,
1,Introduction,"Cochlear implantations (CIs) are considered to be a standard treatment in case of individuals with severe-to-profound hearing loss  During the insertion process, one complication surgeons aim to avoid is called ""tip fold-over,"" where the tip of the array curls in an irregular manner inside the cochlear volume resulting in array folding  Although most CI centers do not currently attempt to detect tip foldovers, the current standard approach among sites that do is visual inspection of intraoperative fluoroscopy. However, these identification methods require experience to align the view optimally and limit the radiation exposure during the fluoroscopy  As tip fold-over cases are reported to be rare, it would be difficult to acquire a substantial number of cases for fully supervised training of any data-driven method. Zuniga et al.  Therefore, in this work we design a dataset of CT images with folded synthetic arrays with realistic metal artifact to assist with training. We propose a multi-task neural network based on the U-Net "
2,Methodology,
2.1,Dataset,"In this study, we utilize CT images from 312 CI patients acquired under local IRB protocols. This included 192 post-implantation CTs (185 normal and 7 tip-fold), acquired either intra-operatively (cone beam) or post-operatively (conventional or cone beam), and 120 pre-implantation CTs used to create synthetic post-implantation CT. As image acquisition parameters (dimensionality, resolution and voxel intensity) varies among the images, we preprocessed all the CT images to homogenize these parameters. First, the intracochlear structures (e.g., Scala Tympani (ST) and Scala Vestibuli (SV)) were segmented from the CT image using previously developed automatic segmentation techniques "
2.2,Synthetic CT Generation,"Our synthetic post-operative CT generation approach is inspired by the process of synthetic preoperative CT generation by Khan et al.  Once we estimated the probable locations of the electrodes, we placed high intensity (around 3-4 times the bone intensity) cubic blocks with a dimensionality of 4×4× 4 voxels in an empty 32 × 32 × 32 grid in locations corresponding to the electrode sites in the preoperative CT. We also added small high intensity cubic blocks (with a dimensionality of 2 × 2 × 2 voxels) between the electrodes to represent the wires that connect to the electrodes. This resulted in an ideal image with a synthetic electrode array (shown in Fig. "
2.3,Multi-task Deep Learning Network,"The neural network model proposed in this study for EA fold-over detection was inspired by the 3D U-Net architectures proposed by Isensee et al. and Ronneberger et al.  The localization pathways collect features at lower spatial resolution where the contextual information is encoded and transfer it to the higher resolution. This is done by using an upsampling step followed by a convolutional layer. The upsampled features are then concatenated with the corresponding context pathway level. Segmentation layers from different levels of the architecture are convolutional layers that are combined by elementwise summation to build the segmentation in a multi-scale fashion and obtain the final segmentation output. This approach was inspired by Kayalibay et al.  A classification branch is added to the network at the point where the contextual information is encoded at the lowest resolution (shown in Fig.  We used binary cross entropy (BCE) loss between the EA ground truth, created by manually selected thresholding of the CT image, and the predicted segmentation. For fold-over classification, we also used BCE loss between the ground truth and the predicted class. However, to place emphasis on the classification performance of the model, the classification loss was weighted 5 times the loss for the segmentation. The learning rate and the batch size were considered 3e-5 and 20, respectively. While training the model, random horizontal flipping and 90°rotation were used as data augmentation techniques for generalization. To evaluate the performance of the proposed model compared to some other neural network models, we implemented 3D versions of ResNet18 "
3,Results,"The training and validation loss curves of multitasking 3D U-Net are presented in Fig.  Next, we compare the performance of different models for fold-over classification. In addition to the performance analysis of these networks for the whole testing data, we separately reported the performance analysis for the synthetic as well as the real portions of the testing data. The separate analysis provides insight about the applicability of the trained model for the real CT images. As reported in Table  In our study, the 3D versions of VAE and the GAN networks rendered promising segmentation results comparable to those of the 3D U-Net. However, in case of tip fold-over detection both the networks classified all the fold-over cases as non fold-overs. This suboptimal performance made our implemented VAE and GAN impractical for intra and/or postoperative fold-over detection. Table  Using Adam optimizer with learning rate 3e-4, batch size of 20, and classification loss weight of 5, we repeated the training process 8 times to investigate training stability. In one out of eight cases, the resulting model could again correctly classify all the real CTs (7 with folded EA and 13 without) in the testing dataset. For the remaining 7 of 8 models, the network could correctly classify 19 out of 20 real CTs (misclassifying one fold-over case), which results in a classification accuracy of 95% for real postoperative CT images. "
4,Discussion and Conclusion,"In a CI surgical procedure, the relative positioning of the EA influences the overall outcome of the surgery "
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Fig. 5 .,
,,
,Table 1 .,
,Table 2 .,
,Table 3 .,
1,Introduction,"The significance of depth information is undeniable in computer-assisted surgical systems  Learning-based approaches have significantly improved monocular depth estimation (MDE) in recent years. As the pioneering work, Eigen et al.  This study presents a novel approach to predict depth values in laparoscopic images using spatio-temporal correspondence. Current self-supervised models for monocular depth estimation face two significant challenges in laparoscopic settings. First, monocular models individually predicted depth maps, ignoring the temporal correlation between adjacent images. Second, accurate point matching is difficult to achieve due to the misleading of large textureless regions caused by the smooth surface of organs. And the homogenous color misleads that the local areas of the edge are regarded with the same depth value. To overcome these obstacles, We introduce multi-view depth estimation (MVDE) with the optimized cost volume to guide the self-supervised monocular depth estimation model. Moreover, we exploit more informative values in a spatio-temporal manner to address the limitation of existing multi-view and monocular models. Our main contributions are summarized as follows. (i) A novel self-supervised monocular depth estimation guided by a multi-view depth model to leverage adjacent images when estimating depth value. (ii) Cost volume construction for multi-view depth estimation under minimum reprojection error and an optimized point cloud consistency for the monocular depth estimation. (iii) An extended deformable patch matching based on the spatial coherence in local regions and a cycled prediction learning for view synthesis and relative poses to exploit the temporal correlation between adjacent images. "
2,Method,
2.1,Self-supervised Monocular Depth Estimation,"Following  where p r and p s are the 2D pixel coordinates in I r and I s . K is the laparoscope's intrinsic parameter matrix. This allows for the generation of a synthetic image I s→r through I s→r (p r ) = I s (p s ). To implement the self-supervised learning strategy, the reprojection error is calculated based on I r and I s→r by with and where structured similarity (SSIM)  where S r and S s→r are based on depth maps from the monocular depth network."
2.2,Improving Depth with Multi-view Guidance,Unlike  where f p r is the pixel coordinates in F r and f p d s is the pixel coordinates in F s based on the depth value d. Then F s is warped to the synthesis feature map by F d s→r ( f p r ) = F s f p d s . Feature volumes V r and V s→r are aggregations of feature maps F r and F s→r . We construct a cost volume C by Fig.  Previous approaches average the difference between V r and all V s→r from adjacent views to generate cost volumes without considering the occlusion problem and uniform differences between the reference and adjacent feature maps  We construct a consistency between MVDE and MDE by 
2.3,Deformable Patch Matching and Cycled Prediction Learning,"Since large areas of textureless areas and reflective parts will cause brightnessbased reprojection errors to become unreliable. Furthermore, the homogeneous color on the edge of organs causes local regions to be regarded in the same depth plane. We introduced deformable patch-matching-based local spatial propagation to MDE. As shown in Fig.  where R r is the deformable local regions for pixel p r . After sharing the same depth value by depth propagation in the local region, we implement the deformable local regions on Eq. 1 to complete patch matching by where R s is the matched local regions in source views I s . Based on R r and R s , I s (R s ) is warped to the synthesised regions I s→r (R r ). The patch-matching-based reprojection error is calculated by To better use the temporal correlation, we considered each image as a reference to construct a cycled prediction learning for depth and pose. The total loss on the final computation is averaged from the error of each combination as where L m r is the reprojection error term for MDE, and L mv r is for MVDE. i is the index number of views in the short clip. L s is the smoothness term "
3,Experiments,
3.1,Datasets and Evaluation Metrics,SCARED 
3.2,Implementation Details,We utilized PyTorch 
3.3,Comparison Experiments,We conducted a comprehensive evaluation of our proposed method by comparing it with several classical and state-of-the-art techniques 
3.4,Ablation Study,We conducted an ablation study to evaluate the influence of different components in our proposed approach. Table 
4,Discussion and Conclusions,"The laparoscopic scenes typically feature large, smooth regions with organ surfaces and homogeneous colors along the edges of organs. This can cause issues while matching pixels, as the points in the boundary area can be mistaken to be in the same depth plane. Our proposed method's depth maps, as shown in Fig  In conclusion, we incorporate more temporal information in the monocular depth model by leveraging the guidance of the multi-view depth model when predicting depth values. We introduce the minimum reprojection error to construct the multi-view depth model's cost volume and optimize the monocular depth model's point cloud consistency module. Moreover, we propose a novel method that matches deformable patches in spatially coherent local regions instead of point matching. Finally, cycled prediction learning is designed to exploit temporal information. The outcomes of the experiments indicate an improved depth estimation performance using our approach."
,Fig. 1 .,
,Fig. 2 .,
,,
,Fig. 4 .,
,,
,Table 1 .,
,Table 2 .,
1,Introduction,"Carrying out a surgical procedure requires not only spatial coordination but also the ability to maintain temporal continuity during time-critical situations, underscoring the crucial importance of auditory perception due to the temporal nature of sound. Perceptual studies have shown that stimuli in different sensory modalities can powerfully interact under certain circumstances, affecting perception or behavior  However, the integration of auditory systems with visual modalities in biomedical research and applications has not been fully achieved. This could be due to the challenges involved in providing perceptually unequivocal and precise sound while reflecting high-resolution and high-dimensional information, as available in medical data. This paper proposes a novel approach, providing an intelligent modeling as a design methodology for interactive medical applications. Considering the limitations of the state-of-the-art, we investigate potential design approaches and the possibility of establishing a new research direction in sonifying high-resolution and multidimensional medical imaging data."
2,State-of-the-Art of Sonification in the Medical Domain,"Sonification is the data-dependent generation of sound, if the transformation is systematic, objective and reproducible, so that it can be used as scientific method  In PMSon, data features are input parameters of a mapping function to determine synthesis parameters. PMSon allows explicit definition of mapping functions in a flexible and adaptive manner, which has made it the most popular approach for medical applications. Medical sonification, as demonstrated by pioneering works such as  Medical sonification has shown great potential, despite being a relatively new field of research. However, there are limitations in terms of design and integrability. The tedious and case-specific process of defining a mapping function that can be perceptually resolved, even in low dimensional space, poses a significant challenge, rendering the achievement of a generalized method nearly impossible. Consequently, practitioners endeavor to minimize data complexity and embed data space into low dimensional space, giving rise to sonification models that offer only an abstract and restricted understanding of the data, which is inadequate in the case of complex medical data. Although these methods achieve adequate perceptual resolution and accuracy, they still are not optimized for integration into the surgical workflow and demonstrate low learning rates, which also demand increased cognitive load and task duration. Furthermore, fine-tuning these models involves a considerable amount of artistic creativity in order to avoid undesirable effects like abrasiveness and fatigue."
3,Towards Model-Based Sonic Interaction with Multimodal Medical Imaging Data,"The need for an advanced multisensory system becomes more critical in scenarios where the anatomy is accessed through limited means, as in minimally invasive surgery. In such cases, an enriched auditory feedback system that conveys precise information about the tissue or surrounding structures, capturing spatial characteristics of the data (such as density, solidity, softness, or sparsity), can enhance the surgeon's perception and, on occasion, compensate for the lack of haptic feedback in such procedures. Embedding such use cases into a low-dimensional space is not feasible. In the same way that tapping on wood or glass imparts information about the object's construction, an effective sonification design can convey information in a manner that is easily interpreted with minimal cognitive effort. Intuitive embodiment of information in auditory feedback, facilitates the learning process to the extent that subconscious association of auditory cues with events can be achieved. Despite the fact that anatomical structures do not produce sound in human acoustic ranges, and surgeons do not have a direct perceptual experience of them, these structures still adhere to the physical principles of dynamics. The hypothesis of this paper is that sounds based on the principles of physics are more straightforward to learn and utilize due to their association with real-world rules. MBS is a technique that utilizes mathematical models to represent data and then convert those models into audible sounds. This approach is often used in scientific or data analysis applications such as clustering  Physical modeling can be approached using the mass interaction method, which is characterized by its modular design and the capacity to incorporate direct gestural interaction, as demonstrated in "
,Contribution,"This paper presents a novel approach for transforming spatial features of multimodal medical imaging data into a physical model that is capable of generating distinctive sound. The physical model captures complex features of the spatial domain of data, including geometric shapes, textures, and complex anatomical structures, and translates them to sound. This approach aims to enhance experts' ability to interact with medical imaging data and improve their mental mapping regarding complex anatomical structures with an unsupervised approach. The unsupervised nature of the proposed approach facilitates generalization, which enables the use of varied input data for the development of versatile soundcapable models."
4,Methods,"The proposed method involves multimodal imaging data serving as input, a topology to capture spatial structures, and an interaction module to establish temporal progress. We consider medical imaging data as members of a R d+m space, where d is the data dimensionality in space domain, and m the dimension of measured features by the imaging systems. A sequence of physics-based sound signals, S, expressed as can be achieved by the sum of n excitations of a physical model P with user interaction act at the position pos with applied force of F , where f transfers a region of interest (RoI) A ∈ R d+m to parameters of the physical model P using the topology matrix T . These components are described in detail in Sect. 4.1. Figure "
4.1,Physical Model,The mass-interaction physics methodology 
,Implementation of a Mass-Interaction System in Discrete Time.,"To represent and compute discretized modular mass-interaction systems, a widely used method involves applying a second-order central difference scheme to Newton's second law, which states that force F is equal to mass m times acceleration a, or the second derivative of its position vector x with respect to time t. The total force exerted by the dampened spring, denoted as where F s represents the elastic force exerted by a linear spring (the interaction) with stiffness K, connecting two masses m 1 , m 2 located at positions x 1 , x 2 , can be expressed using the discrete-time equivalent of Hooke's law. Similarly, the friction force F d applied by a linear damper with damping parameter z can be derived using the Backward Euler difference scheme with the discrete-time inertial parameter Z = z/ΔT . F (t n ) is applied symmetrically to each mass in accordance with Newton's third law: The combination of forces applied to masses and the connecting spring yields a linear harmonic oscillator as described in which is a fundamental type of the mass-interaction system. This system is achieved by connecting a dampened spring between a mass and a fixed point A mass-interaction system can be extended to a physical model network with an arbitrary topology by connecting the masses via dampened springs. The connections are formalized as a routing matrix T of dimensions r × c, where r denotes the number of mass points in the physical model network, and c represents the number of connecting springs, each having only two connections. A single mass can be connected to multiple springs in the network."
,Interaction,"Module. An interaction module, act, excites the model P by applying force to one or more input masses of the model I ∈ T r×c . f maps the intensities of the input data to M, K, Z, and F . Therefore, the input force is propagated through the network according to the Eq. 2 and observed by the output masses O ∈ T r×c . To summarize, the output masses are affected by the oscillation of all masses activated in the model with various frequencies and corresponding amplitudes, resulting in a specific sound profile, i.e., tone color. This tone color represents the spatial structure and physical properties of the RoI, which is transformed into the features of the output sound. The wave propagation is significantly influenced by the model's topology and the structure of the inter-mass connections, which have great impact on activating spatial relevant features and sound quality."
4.2,Experiment and Results,"The objective is to evaluate the feasibility of the proposed method in creating a model that is capable of generating discernible sound profiles in accordance with the underlying anatomical structures. In particular, we aim to differentiate between the sound of a set of tissue types. Through empirical experimentation on an abdominal CT volume  A mel spectrogram is a visual representation of the frequency content of an audio signal, where the frequencies are mapped to a mel scale, which is a perceptual frequency scale based on how humans hear sounds. Therefore, we used this representation to show the frequency content of the resulting sound of the trajectory, presented in Fig. "
5,Discussion and Conclusion,"This paper introduced a general framework for MBS of multimodal medical imaging data. The preliminary study demonstrates perceptually distinguishable sound profiles between various anatomical tissue types. These profiles are achieved through a minimal preprocessing based on the model topology and a basic mapping definition. This indicates that the model is effective in translating intricate imaging data into a discernible auditory representation, which can conveniently be generalized to a wide range of applications. In contrast to the traditional methods that directly convert low-dimensional data into global sound features, this approach maps features of a data model to the features of a sound model in an unsupervised manner and enables processing of high-dimensional data. The proposed method presents opportunities for several enhancements and future directions. In the case of CT imaging, it may be feasible to establish modality-specific generalized configurations and tissue-type-specific transfer functions to standardize the approach to medical imaging sonification. Such efforts could lead to the development of an auditory equivalent of 3D visualization for medical imaging, thereby providing medical professionals with a more immersive and intuitive experience. Another opportunity could be in the sonification of intraoperative medical imaging data, such as ultrasound or fluoroscopy, to augment the visual information that a physician would receive by displaying the tissue type or structure which their surgical instruments are approaching. To accommodate various application cases, alternative interaction modes can be designed that simulate different approaches to the anatomy with different tool materials. Additionally, supervised features can be integrated to improve both local and global perception of the model, such as amplifying regions with anomalies. Different topologies and configurations can be explored for magnifying specific structures in data, such as pathologies, bone fractures, and retina deformation. To achieve a realistic configuration of the model regarding the physical behavior of the underlying anatomy, one can use several modalities which correlate with physical parameters of the model. For instance, the masses can be derived by intensities of CT and spring stiffness from magnetic resonance elastography, both registered as a 3D volume. An evaluation of the model's potential in an interactive setting can be conducted to determine its impact on cognitive load and interaction intuitiveness. Such an assessment can shed light on the practicality of the model's application by considering the user experience, which is influenced by cognitive psychological factors. As with any emerging field, there are constraints associated with the methodology presented in this paper. One of the constraints is the requirement to manually configure the model parameters. Nevertheless, a potential future direction would be to incorporate machine learning techniques and dynamic modeling to automatically determine these parameters from underlying physical structure. For instance, systems such as "
,Fig. 1 .,
,Fig. 2 .,
1,Introduction,"Previous studies have shown that surgeon performance directly affects patient clinical outcomes  Preliminary work has shown favorable results for automated skill assessments on simulated VR environments, demonstrating the benefits of machine learning (ML) methods. ML approaches for automating suturing technical skills leveraged instrument kinematic (motion-tracking) data as the sole input to recurrent networks have been able to achieve effective area-under-ROC-curve (AUC), up to 0.77 for skill assessment in VR sponge suturing exercises  Despite recent advances, automated skill assessment in live scenarios is still a difficult task due to two main challenges: 1) the lack of kinematic data from the da Vinci R system, and 2) the lack of training data due to the labor-intensive labeling task. Unlike simulated VR environments where kinematic data can be readily available, current live surgical systems do not output motion-tracking data, which is a key source of information for determining the movement and trajectory of robotic manipulators and suturing needles. Moreover, live surgical videos do not have a clear and painted target area for throwing stitches, unlike VR videos, which makes the task additionally difficult. On the other hand, due to the labor-intensive task of segmenting and scoring individual stitches from each surgical video, the quantity of available and labeled training data is quite low, rendering traditional supervised learning approaches ineffective. To address these challenges, we propose LiveMAE which learns sim-to-real generalizable representations without requiring any live kinematic annotations. Leveraging available video and sensor data from previous VR studies, LiveMAE can map from surgical images to instrument kinematics and derive surrogate ""kinematic"" automatically by learning to reconstruct images from both VR and live stitches while also predicting the corresponding VR kinematics. This creates a shared encoded representation space between the two visual domains while using available kinematic data from only one domain, the VR domain. Moreover, our pre-training strategy is not skill-specific which brings a bonus in improving data efficiency. LiveMAE enjoys up to six times more training data across the six suturing skills seen in Fig.  1. We propose LiveMAE which learns sim-to-real generalizable representations without requiring any live kinematic annotations. 2. We design a pre-training paradigm that increases the number of effective training samples significantly by combining data across suturing skills. 3. We conduct rigorous evaluations to verify the effectiveness of LiveMAE on surgical data collected and labeled across multiple institutions and surgeons. Finetuning on suturing skill assessment tasks yields better performance on 5/6 skills on live surgical videos compared to supervised learning baselines."
2,Methodology,"Masked autoencoding is a method for self-supervised pre-training of Vision Transformers (ViTs  , and EASE technical skill score y i ∈ {0, 1} for non-ideal vs ideal performance. F denotes the number of frames in the video clip. Live data for s is similarly D L s = {(x i , y i )} Ms i=0 , except there are no aligned kinematics. Kinematic data has 70 features tracking 10 instruments of interest, each pose contains 3 elements for coordinates and 4 elements for quarternions. There are six technical skill labels, see Fig. "
2.1,LiveMAE,"Since D L s lacks kinematic information that is crucial for suturing skill assessment, we propose LiveMAE to automatically derive ""kinematics"" from live videos that can be helpful for downstream prediction. Specifically, we aim to learn a mapping φ : R H×W ×3 → R 70 from images to instrument kinematics using available video and sensor data from D V R s , and subsequently utilizing that mapping φ on live videos. Although the visual style between VR and live surgical videos can differ, this mapping is possible since we know that both simulated VR and live instruments share the exact same dimensions and centered coordinate frames. Our method builds on top of MAE and has three main components: a kinematic decoder, a shared encoder, and an expanded training set. Kinematic Decoder. For mapping from a surgical image to the instrument kinematics, we propose an additional kinematic output head along with a corresponding self-supervised task of reconstructing kinematics from masked input. See Fig.  Expanded Training Set. Since we have limited surgical data, and the mapping from image to instrument kinematics is not specific to any one suturing skill, we can combine visual and kinematic data across different skills during pre-training. Specifically, we pre-train the model on all data combined across 6 suturing skills to help learn the mapping. In addition, we can further break down video clips and kinematic sequences into F * (N s +M s ) (image, kinematics) pairs to increase the effective training set size without needing heavy data augmentations. These two key facts provide is a unique advantage over traditional supervised learning, since training each skill assessment task required the full video clip to learn temporal signature along with skill-specific scorings."
,Finetuning of LiveMAE for Skill Assessment,"After pre-training, we discard the image decoder and only use the pathway from the encoder to the kinematic decoder as our mapping φ. See Fig. "
3,Experiments and Results,Datasets. We utilize a previously validated suturing assessment tool (EASE 
,Metrics and Baselines.,"Across the five institutions, we use 5-fold cross-valid ation to evaluate our model, training and validating on data from 4 institutions while testing on the 5th held-out institution. This allows us to test for generalization on unseen cases across both surgeons and medical centers. We measure and report the mean ± std. dev. for the two metrics: (1) Area-under-the-ROC curve (AUC) and (  To understand the benefits of each data modality, we compare LiveMAE against 3 setups: (1) train/test using only kinematics, (2) train/test using only videos, and (3) train using kinematic and video data while testing only on video (no live kinematics). For kinematics-only baselines, we use two sequential models (1) LSTM recurrent model "
3.1,Understanding the Benefits of Kinematics,Table 
3.2,Evaluation of LiveMAE on Live Videos,Quantitative Results. Table 
4,Conclusion,"Self-supervised learning methods, as utilized in our work, showed that videobased evaluation of suturing technical skills in live surgical videos is achievable with robust performance across multiple institutions. Although current work is limited to using VR data from one setup, namely Surgical Science TM Flex VR, our approach is independent from that system and can be applied on top of other surgical simulation systems with synchronized kinematics and video recordings. Future work will expand on the applications we demonstrated to determine whether it is possible to have a fully autonomous process, or semiautonomously with a ""human-in-the-loop""."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Table 1 .,
,Table 2 .,
,.725 ±0.12 0.903 ±0.06 0.562 ±0.08 0.733 ±0.08 0.634 ±0.06 0.826 ±0.01,
1,Introduction,"Image-to-physical registration is a necessary process for computer-assisted surgery to align preoperative imaging to the intraoperative physical space of the patient to in-form surgical decision making. Most intraoperatively utilized image-to-physical regis-trations are rigid transformations calculated using fiducial landmarks  Intraoperative data available for registration is often sparse and subject to data collection noise. Image-to-physical registration methods that accurately model an elastic soft-tissue environment while also complying with intraoperative data constraints is an active field of research. Determining correspondences between imaging space and geometric data is required for image-to-physical registration, but it is often an inexact and ill-posed problem. Establishing point cloud correspondences using machine learning has been demonstrated on liver and prostate datasets  In addition to a correspondence algorithm, a technique for modeling a deformation field is required. Both  In this work, we propose an image-to-physical registration method that uses regularized Kelvinlet functions as a novel deformation basis for nonrigid registration. Regularized Kelvinlet functions are analytical solutions to the equations for linear elasticity that we superpose to compute a nonrigid deformation field nearly instantaneously  We utilize ""grab"" and ""twist"" regularized Kelvinlet functions with a linearized iterative reconstruction approach (adapted from "
2,Methods,"In this section, closed-form solutions to linear elastic deformation responses in an infinite medium are derived to obtain regularized Kelvinlet functions. Then, methods for constructing a superposed regularized Kelvinlet function deformation basis for achieving registration within an iterative reconstructive framework are discussed. Equation notation is written such that constants are italicized, vectors are bolded, and matrices are double-struck letters. Linear elasticity in a homogeneous, isotropic media is governed by the Navier-Cauchy equations in Eq. ( "
2.1,Regularized Kelvinlet Functions,", and I is the identity matrix. We note that the deformation response is linear with respect to f , which implies that forcing functions can be linearly superposed. However, practical use of Eq. (  To address numerical singularity, regularization is incorporated with a new forcing function Eq. (  The second type of regularized Kelvinlet functions represent ""twist"" deformations which are derived by expanding the previous formulation to accommodate locally affine loads instead of displacement point sources. This is accomplished by associating each component of the forcing function Eq. (  Superpositions of Eq. ( "
2.2,Registration Task,"For registration, x 0 control point positions for k number of total regularized Kelvinlets ""grab"" and ""twist"" functions are distributed in a predetermined configuration. Then, the f grab and f twist vectors are optimized to solve for a displacement field that minimizes distance error between geometric data inputs. For a predetermined configuration of regularized Kelvinlet ""grab"" and ""twist"" functions centered at different x 0 control point locations, an elastically deformed state can be represented as the summation of all regularized Kelvinlet displacement fields where ∼ u (x) is the superposed displacement vector and k = k grab + k twist in Eq.  This formulation decouples the forcing magnitudes from the Kelvinlet response matrix ∼ K (x), which is composed of column u ε,grab (x) and u ε,twist (x) vectors calculated with unit forcing vectors for each K grab (x) and K twist (x) function. This allows for linear scaling of ∼ K (x) using α. By setting x 0 locations, ε grab , and ε twist as hyperparameters, deformation states can be represented by various α vectors with the registration task being to solve for the optimal α vector. An objective function is formulated to minimize misalignment between the moving space x moving and fixed space x fixed through geometric data constraints. For the breast imaging datasets in this work, we used simulated intraoperative data features that realistically could be collected in a surgical environment visualized in Fig.  For a given deformation state, each data feature contributes to the total error measure. For the point data, the error e i point for each point i is simply the distance magnitude between corresponding points in x fixed and x moving space. For the surface data, the error e i surface is calculated as the distance from every point i in the x fixed point cloud surface to the closest point in the x moving surface, projected onto the surface unit normal which allows for sliding contact between surfaces. The optimization using the objective function in Eq. ( "
3,Experiments and Results,"In this section, two experiments are conducted. The first explores sensitivity to regularized Kelvinlet function hyperparameters k grab , k twist , ε grab , and ε twist and establishes optimal hyperparameters in a training dataset of 11 breast deformations. The second validates the registration method in a breast cancer patient and compares registration accuracy and computation time to previously proposed methods."
3.1,Hyperparameters Sensitivity Analysis,"This dataset consists of supine breast MR images simulating surgical deformations of 11 breasts from 7 healthy volunteers. Volunteers (ages 23-57) were enrolled in a study approved by the Institutional Review Board at Vanderbilt University. Prior to imaging, 26 skin fiducials were distributed on the breast surface. MR images (0.391 × 0.391 × 1 mm 3 or 0.357 × 0.357 × 1 mm 3 ) were acquired with the volunteers' arms placed by their sides. This image was used as the x moving space. The volunteers were then instructed to raise one arm above their heads, causing deformation of the ipsilateral breast. A second MR image in the deformed state was acquired to create simulated intraoperative physical data and to use for validation. This second image was used as the x fixed space. The breast in x moving was segmented at the boundary between the chest wall and breast parenchyma to create a 3D model. The posterior surface was labeled to inform x 0 control point locations. The skin fiducials and intra-fiducial surface point clouds were labeled in both images as data features. Sparse tracked ultrasound data collection patterns were projected on the posterior surface for use as the third data feature. Subsurface anatomical targets were labeled in both images and used to compute target error after registration. Three configurations were explored to test different distributions of grab and/or twist regularized Kelvinlet functions: grab functions only, twist functions only, and a combination of grab and twist functions. Grab function control points were distributed evenly on the posterior surface of the breast to approximate forces from the chest wall. Twist function control points were distributed evenly within the breast to approximate internal body forces. Three hyperparameter sweeps were used:  "
3.2,Registration Methods Comparison,"This dataset consists of supine breast MR images simulating surgical deformations from one breast cancer patient. A 71-year-old patient with invasive mammary carcinoma in the left breast was enrolled in a study approved by the Institutional Review Board at Vanderbilt University. Skin fiducial placement, image acquisition, arm placement, and preprocessing steps followed the same protocol detailed in Sect. 3.1. The tumor was segmented in both images by a subject matter expert, and a 3D tumor model was created to evaluate tumor overlap metrics after registration. Regularized Kelvinlet function registration was compared to 3 other registration methods: rigid registration, an FEM-based image-to-physical registration method, and an image-to-image registration method. A point-based rigid registration using the skin fiducials provided a baseline comparator for accuracy without deformable correction. The FEM-based image-to-physical registration method, detailed in  Registration results for the 4 methods are shown in Table "
4,Limitations and Conclusion,"Several limitations should be noted. Regularized Kelvinlet functions describe solutions that assume a physical embedding within an infinite elastic domain, which does not account for organ-specific geometry. This approach may not be well suited for problems where geometry has significant influence. This method is derived from a linear elastic model, and nonlinear models are known to better describe soft tissue mechanics. Additionally, this method assumes homogeneity and isotropy -it does not account for different tissue types and directional structures in the breast. With regards to clinical feasibility, supine MR imaging with skin fiducials is not the standard-of-care. However, using supine MR imaging for surgery is becoming increasingly investigated, and previous work demonstrated the potential of ink-based skin fiducial markings on the breast  In this work, we demonstrated the use of regularized Kelvinlet functions for imageto-physical registration of the breast. We achieved near real-time registration with comparable accuracy to previously proposed methods. We believe that this approach is generalizable to other soft-tissue organ systems and is well-suited for improving navigation during image-guided surgeries."
,Fig. 1 .,
,Fig. 2 .,
,•,
,Fig. 3 .,
,Fig. 4 .,
,Table 1 .,
1,Introduction,"One of the significant logistical challenges facing hospital administrations today is operating room (OR) efficiency. This parameter is determined by many fac-tors, one of which is surgical procedure duration that reflects intracorporeal time, which in itself poses a challenge as, even across the same procedure type, duration can vary greatly. This variability is influenced by numerous elements, including the surgeon's experience, the patient's comorbidities, unexpected events occurring during the procedure, the procedure's complexity and more. Accurate realtime estimation of procedure duration improves scheduling efficiency because it allows administrators to dynamically reschedule before a procedure has run overtime. Another important aspect is the ability to increase patient safety and decrease complications by dosing and timing anesthetics more accurately. Currently, methods aiming for OR workflow optimization through ETC are lacking. One study showed that surgeons underestimated surgery durations by an average of 31 min, while anesthesiologists underestimated the durations by 35 min  As AI capabilities have evolved greatly in recent years, the field of minimally invasive procedures, which is inherently video-based, has emerged as a potent platform for the harnessing of these capabilities to improve both patient care and workflow efficiency. Consequently, ETC has become a technologically achievable and clinically beneficial task."
2,Related Work,"Initially, ETC studies performed preoperative estimates based on surgeon data, patient data, or a combination of these  In this work, we study the key elements important to the development of ETC models and perform an in-depth methodical analysis of this task. First, we suggest an adequate metric for evaluation -SMAPE, and introduce two new architectures, one based on LSTM networks and one on the transformer architecture. Then, we examine how different ETC methods perform when trained with various loss functions and show that their errors are not necessarily correlated. We then test the hypothesis that an ensemble composed of several ETC model variations can significantly improve estimation compared to any single model."
3,Methods,
3.1,Evaluation Metrics,"Mean Absolute Error (MAE). The evaluation metric used in prior work was MAE. where T is a video duration, y is the actual time left until completion, and ŷ is the ETC predictions. A disadvantage of MAE is its reliance on the magnitude of values, consequently, short videos are likely to have small errors while long videos are likely to have large errors. In addition, MAE does not consider the actual video duration or the temporal location for which the predictions are made. Symmetric Mean Absolute Percentage Error (SMAPE). SMAPE is invariant to the magnitude and keeps an equivalent scale for videos of different duration, thus better represents ETC performance "
3.2,Datasets,"We focus on three different surgical video datasets (a total of 3,993 videos) that were curated from several medical centers (MC) and include procedures performed by more than 100 surgeons. The first dataset is Laparoscopic Cholecystectomy that contains 2,400 videos (14 MC and 118 surgeons). This dataset was utilized for the development and ablation study. Additionally, we explore two other datasets: Laparoscopic Appendectomy which contains 1,364 videos (5 MC and 61 surgeons), and Robot-Assisted Radical Prostatectomy (RARP) which contains 229 videos (2 MC and 14 surgeons). The first two datasets are similar, both are relatively linear and straightforward procedures, have similar duration distribution, and are abdominal procedures with similarities in anatomical views. However, RARP is almost four times longer on average. Therefore, it is interesting to explore how methods developed on a relatively short and linear procedure will perform on a much longer procedure type such as RARP. Table "
3.3,Loss Functions,"Loss values are calculated by comparing ETC predictions ( ŷt ) for each timestamp to the actual time left until the procedure is complete (y t ). The final loss for each video is the result of averaging these values across all timestamps. MAE Loss. The MAE loss is defined by: Smooth L1 Loss. The smooth L1 loss is less sensitive to outliers  in which SMAPE Loss. Based on the understanding that SMAPE (Sect. 3.1) is a good representation of the ETC problem, we also formulated it as a loss function: Importantly, SMAPE produces higher loss values for the same absolute error as the procedure progresses, when the denominator is getting smaller. This property is valuable as the models should be more accurate as the surgery nears its end. Corridor Loss. A key assumption overlooked in developing ETC methods is that significant and time-impacting events might occur during a procedure. For example, a prolonged procedure due to significant bleeding occurring after 30 min of surgery is information that is absent from the model when providing predictions at the 10 min timestamp. To tackle this problem, we apply the corridor loss  Interval L1 Loss. The losses described above focus on the error between predictions and labels for each timestamp independently. Influenced by the total variation loss, we suggest considering the video's sequential properties. The interval L1 loss focuses on jittering in predictions between timestamps in a pre-defined interval, aiming to force them to act more continuously. ŷt are the predictions per timestamp, and S is an interval time span (jump) between two timestamps. Total Variation Denoising Loss. This loss is inspired by a 1D total variation denoising loss and was modified to fit as part of the ETCouple model. L total_variation_denoising (y, ŷ) = L squared_error (y, ŷ) + λ • L S=120 IntervalL1 (ŷ) (10)"
3.4,ETC Models,"Feature Representation. All models and experiments described in this work are based on fixed visual features that were extracted from the surgical videos using a pre-trained model. This approach allows for shorter training cycles, less computing requirements, and benefits from a model that was trained on a different task  Inferring ETC. Our ETC architectures end with a single shared fully connected (FC) layer and a Sigmoid that outputs two values: ETC and progress. ETC is inferred by averaging the predicted ETC value and the one calculated from the progress. where T is the video duration and t el marks the elapsed time. Inspired by "
,ETC-LSTM.,"A simple architecture that consists of an LSTM layer with a hidden size of 128. Following hyperparameters tuning on the validation set, the ETC-LSTM was trained using an SGD optimizer with a constant learning rate of 0.1 and a batch size of 32 videos. ETCouple. ETCouple is a different approach to applying LSTM networks. In contrast to ETC-LSTM and similar methods which predict ETC for a single timestamp, here we randomly select one timestamp from the input video and set it as an anchor sample. The anchor is then paired with a past timestamp using a fixed interval of S = 120 s. The model is given two inputs, the features from the beginning of the procedure up to the anchor and the features up to the pair location. Instead of processing the entire video in an end-to-end manner, we only process past information and are thus able to use a bi-directional LSTM (hidden dimension is 128). The rest of the architecture contains a dropout layer (P = 0.5), the shared FC layer, and a Sigmoid function. We explored various hyperparameters and the final model was trained with a batch size of 16, an AdamW optimizer with a learning rate of 5 • 10 -4 , and a weight decay of 5 • 10 -3 . ETCformer. LSTM networks have been shown to struggle with capturing longterm dependencies "
4.1,Ablation Experiments,"This section provides ablation studies on the Cholecystectomy dataset. Loss Functions. Table  Error Analysis. To test whether the errors of the various models are correlated, we compared the predictions made by the different models on a per-video basis. We use SMAPE and analyze the discrepancy by comparing the difference of every two model variations independently. Then, we divided the videos into a similar and a dissimilar group, by using a fixed threshold, i.e., if the SMAPE difference is smaller than the threshold the models are considered as providing similar results. The threshold was empirically set to 2, deduced from the ETC curves, which are almost identical when the SMAPE difference is smaller than 2 (Fig.  Ensemble Analysis. There are many tasks in machine learning in which data can be divided into easy or hard samples. We argue that the ETC task is different in these regards. Based on the error analysis, we explored how an ensemble of models performs and if it produces better results (Table "
4.2,Appendectomy and RARP Results,We examine the results on two additional datasets to showcase the key elements explored in this work (Table 
5,Discussion,"In this work, we examine different architectures trained with several loss functions and show how SMAPE can be utilized as a better metric to compare ETC models. In the error analysis, we conclude that each model learns to operate differently on the same videos. This led us to explore an ensemble of models which eventually achieves the best results. Yet, this conclusion can facilitate future work, focusing on understanding the differences and commonalities of the models' predictions and developing a unified or enhanced model, potentially reducing the complexity of training several ETC models and achieving better generalizability. Future work should also incorporate information regarding the surgeon's experience, which may improve the model's performance. This work has several limitations. First, the proposed models need to be evaluated across other procedures and specialties for their potential to be validated further. Second, the ensemble's main disadvantage is its requirement for more computing resources. In addition, there may be data biases due to variability in the time it takes surgeons to perform certain actions at different stages of their training. Finally, although our model relies on video footage only, and no annotations are required for ETC predictions, manual annotations of surgical steps are still needed for pre-training of the feature extraction model. Real-time ETC holds great potential for surgical management. First, in optimizing OR scheduling, and second as a proxy to complications that cause unusual deviations in anticipated surgery duration. However, optimizing OR efficiency with accurate procedural ETC, based on surgical videos, has yet to be realized. We hope the information in this study will assist researchers in developing new methods and achieve robust performance across multiple surgical specialties, ultimately leading to better OR management and improved patient care."
,Fig. 1 .,
,Table 1 .,
,Table 2 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_16.
1,Introduction,"Background: Minimally invasive surgeries (MIS) such as laparoscopic and endoscopic surgeries have gained widespread popularity due to the significant reduction in the time of surgery and post-op recovery  The Challenge: Most modern techniques for SIIS  Current Solution Strategy: Recently, S3Net  Our Observation: In the last few years, attention-based transformer architectures have outperformed CNN architectures for many computer-vision-based tasks. Recent transformer-based object detection models implement deformable attention "
,Contributions: (1),"We investigate the reason for the failure of transformerbased object detectors for the medical instrument instance segmentation tasks. Our analysis reveals that incorrect query initialization is to blame. We observe that recall of an instrument based on the initialized queries is a lowly 7.48% at 0.9 IOU, indicating that many of the relevant regions of interest do not even appear in the initialized queries, thus leading to lower accuracy at the last stage. (2) We observe that CNN-based object detectors employ a non-maximal suppression (NMS) at the proposal stage, which helps spread the proposal over the whole image. In contrast in transformer-based detection models, this has been replaced by taking the highest confidence boxes. In this paper, we propose to switch back to NMS-based proposal selection in transformers. (3) The NMS uses only bounding boxes and does not allow content interaction for proposal selection. We propose a Query Proposal Decoder block containing multiple layers of selfattention and deformable cross-attention to perform region-aware refinement of the proposals. The refined proposals are used by a transformer-based decoder backbone for the prediction of the class label, bounding box, and segmentation mask. (4) We show an improvement of 1.84% over the best-performing SOTA technique on the Endovis17 and 2.09% on the Endovis18 dataset as measured by ISI-IOU."
2,Related Work,Datasets: Surgical tool recognition and segmentation has been a well-explored research topic  Instance Segmentation Techniques for Medical Instruments: Multiple attempts have been made to perform instance segmentation using these datasets. 
,Transformer-based Instance Segmentation for Natural Images:,"On the other hand, transformer-based instance segmentation architectures "
3,Proposed Methodology,Backbone Architecture: We utilize Mask DINO  Query Proposal Network: Spatial Diversification of the Proposals: The proposed Query Proposal Network (QPN) is shown in Fig. 
,Query Proposal Network: Content-based Inter-proposal Interaction:,"The top k region proposals from NMS are used to initialize the queries for the proposed Query Proposal Decoder (QPD). Note that the NMS works only on the basis of box coordinates and does not take care of content embeddings into account. We try to make up for the gap through the QPD module. The QPD module consists of self-attention, cross-attention and feed-forward layers. The self-attention layer of QPD allows the queries to interact with each other and the duplicates are avoided. We use the standard deformable cross-attention module as proposed in  For L mask , we use cross entropy and IOU (or dice) loss. The total loss is: Through hyper-parameter tuning, we set λ = [0.19, 0.24, 0.1, 0.24, 0.24]. We use a batch size of 8. The initial learning rate is set to 0.0001, which drops by 0.1 after every 20 epochs. We set 0.9 as the Nesterov momentum coefficient. We train the network for 50 epochs on a server with 8 NVidia A100, 40 GB GPUs. Besides QPN we use the exact same architecture as proposed in MaskDINO "
4,Results and Discussion,"Evaluation Methodology: We demonstrate the performance of the proposed methodology on two benchmark Robot-assisted endoscopic surgery datasets Endovis 2017  Quantitative Results on EV17, EV18, and Cadis: The results of our technique for the EV17 dataset are shown in Table "
,Ablation Study:,We perform an ablation on the EV18 dataset to show the importance of each block in the proposed methodology. The results of the same are summarised in Table 
5,Conclusion,"In this paper, we proposed a novel class-agnostic Query Proposal Network (QPN) to better initialize the queries for a transformer-based surgical instrument instance segmentation model. Towards this, we first diversified the queries using the non-maximal suppression and proposed a deformable-cross-attentionbased learnable Query Proposal Decoder (QPD). On average, the proposed QPN improved the recall rate of the query initialization by 52.38% at 0.9 IOU. The improvement translates to an improved ISI-IOU of 1.84% and 2.09% in the publicly available Endovis 2017 and Endovis 2018 datasets, respectively."
,Fig. 2 .,
,Fig. 3 .,
,,
,NMS Enhanced Feature Maps Query Proposal Network Decoder Input Image Queries Initialisation Enhanced Feature Maps Query Proposal Decoder x k Queries Initialisation Pixel-wise Classification and Regression Self Attention Deformable Cross Attention Feed Forward Network Decoder Output Boxes Output Masks Self Attention Feed Forward Network Encoder Self Attention Deformable Cross Attention Feed Forward Network Backbone Feature Maps Fig,
,Table 1 .,
,Table 2 .,
,Table 3 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 70.
1,Introduction,"We address the important problem of intraoperative patient-to-image registration in a new way by relying on preoperative data to synthesize plausible transformations and appearances that are expected to be found intraoperatively. In particular, we tackle intraoperative 3D/2D registration during neurosurgery, where preoperative MRI scans need to be registered with intraoperative surgical views of the brain surface to guide neurosurgeons towards achieving a maximal safe tumor resection  Most existing techniques perform patient-to-image registration using intraoperative MRI  The main limitation of existing intraoperative registration methods is that they rely heavily on processing intraoperative images to extract image features (eg., 3D surfaces, vessels centerlines, contours, or other landmarks) to drive registration, making them subject to noise and low-resolution images that can occur in the operating room "
,Contribution:,We propose a novel approach for patient-to-image registration that registers the intraoperative 2D view through the surgical microscope to preoperative MRI 3D images by learning Expected Appearances. As shown in Fig. 
,Expected Appearances MRI Scan,
,Intraoperative Registration Image Synthesis Pose Regression,Pose Sampling Fig. 
2,Method,
2.1,Problem Formulation,"As illustrated in Fig.  where R ∈ SO(3) and t ∈ R 3 represent a 3D rotation and 3D translation, respectively, and A is the camera intrinsic matrix composed of the focal length and the principal points (center of the image) while {c i } i is a correspondence map and is built so that if a 2D point v i corresponds to a 3D point u j where c i = j for each point of the two sets. Note that the set of 3D points u is expressed in homogenous coordinates in the minimization of the reprojection error. In practice, finding the correspondences set {c i } i between u and v is nontrivial, in particular when dealing with heterogeneous preoperative and intraoperative modality pairs (MRI, RGB Cameras, ultrasound, etc.) which is often the case in surgical guidance. Existing methods often rely on feature descriptors  By defining a synthesize function S Θ that synthesizes a new image I given a projection of a 3D surface mesh for different camera poses, i.e. I = S Θ (A[R|t], M), the optimization problem above can be rewritten as: argmin This new formulation is correspondence-free, meaning that it alleviates the requirement of the explicit matching between u and v. This is one of the major strengths of our approach. It avoids the processing of I at run-time, which is the main source of registration error. In addition, our method is patient-specific, centered around M, since each model is trained specifically for a given patient. These two aspects allow us to transfer the computational cost from the intraoperative to the preoperative stage thereby optimizing intraoperative performance. The following describes how we build the function S Θ and how to solve Eq. 1. "
2.2,Expected Appearances Synthesis,"We define a synthesis network S Θ : (A[R|t], M, T) → I, that will generate a new image resembling a view of the brain surface from the 2D projection of the input mesh M following [R|t], and a texture T. Several methods can be used to optimize Θ. However, they require a large set of annotated data  where G l ij (T ) is the Gram matrix of texture T at the l-th convolutional layer (pre-trained VGG-19 model), and w l,c T class are the normalization factors for each Gram matrix, normalized by the number of pixels in a label class c of T class . This allows for the quantification of the differences between the texture image T and the generated image I as it is being generated. Importantly, computing the inner-most sum over each label class c allows for texture comparison within each class, for instance: the background, the parenchyma, and the cortical vessels. In practice, we assume constant camera parameters A and first sample a set of binary images by randomly varying the location and orientation of a virtual camera [R|t] w.r.t. to the 3D mesh M before populating the binary images with the textures using S Θ (see Fig.  We use the L-BFGS optimizer and 5 convolutional layers of VGG-19 to generate each image following "
2.3,Pose Regression Network,"In order to solve Eq. 1, we assume a known focal length that can be obtained through pre-calibration. To obtain a compact representation of the rotation and since poses are restricted to the upper hemisphere of the 3D mesh (No Gimbal lock), the Euler-Rodrigues representation is used. Therefore, there are six parameters to be estimated: rotations r x , r y , r z and translations t x , t y , t z . We estimate our 6-DoF pose with a regression network P Ω : I → p and optimize its weights Ω to map each synthetic image I to its corresponding camera pose p = [r x , r y , r z , t x , t y , t z ] T . The network architecture of P Ω consists of 3 blocks each composed of two convolutional layers and one ReLU activation. To decrease the spatial dimension, an average pooling layer with a stride of 2 follows each block except the last one. At the end of the last hierarchy, we add three fully-connected layers with 128, 64, and 32 neurons and ReLU activation followed by one fully-connected with 6 neurons with a linear activation. We use the set of generated Expected Appearances T P = {(I i ; p i )} i ; and optimize the following loss function over the parameters Ω of the network P Ω : where t and R vec are the translation and rotation vector, respectively. We experimentally noticed that optimizing these entities separately leads to better results. The model is trained for each case (patient) for 200 epochs using mini-batches of size 8 with Adam optimizer and a learning rate of 0.001 and decays exponentially to 0.0001 over the course of the optimization. Finally, at run-time, given an image I we directly predict the corresponding 3D pose p so that: p ← P(I; Ω), where Ω is the resulting parameters from the training."
3,Results,Dataset. We tested our method retrospectively on 6 clinical datasets from 6 patients (cases) (see Fig.  Metrics. We chose the average distance metric (ADD) as proposed in  Accuracy-Threshold Curves. We calculated the number of 'correct' poses estimated by our model. We varied the distance threshold on the validation sets (excluding 2 textures) in order to reveal how the model performs w.r.t. that threshold. We plotted accuracy-threshold curves showing the percentage of pose accuracy variation with a threshold in a range of 0 mm to 20 mm. We can see in Fig. 
,Validation and Evaluation of Texture Invariance.,"We chose to follow a Leave-one-Texture-out cross-validation strategy to validate our model. This strategy seemed the most adequate to prevent over-fitting on the textures. We measured the ADD errors of our model for each case and report the results in  We observed a variance in the ADD error that depends on which texture is left out. This supports the need for varying textures to improve the pose estimation. However, the errors remain low, with a 2.01 ± 0.58 mm average ADD error, over all cases. The average ADD error per case (over all left-out textures) is reported in Table  Figure "
4,Discussion and Conclusion,"Clinical Feasibility. We have shown that our method is clinically viable. Our experiments using clinical data showed that our method provides accurate registration without manual intervention, that it is computationally efficient, and it is invariant to the visual appearance of the cortex. Our method does not require intraoperative 3D imaging such as intraoperative MRI or ultrasound, which require expensive equipment and are disruptive during surgery. Training patient-specific models from preoperative imaging transfers computational tasks to the preoperative stage so that patient-to-image registration can be performed in near real-time from live images acquired from a surgical microscope. Limitations. The method presented in this paper is limited to 6-DoF pose estimation and does not account for deformation of the brain due to changes in head position, fluid loss, or tumor resection and assumes a known focal length. In the future, we will expand our method to model non-rigid deformations of the 3D mesh and to accommodate expected changes in zoom and focal depth during surgery. We will also explore how texture variability can be controlled and adapted to the observed image to improve model accuracy."
,Conclusion.,"We introduced Expected Appearances, a novel learning-based method for intraoperative patient-to-image registration that uses synthesized expected images of the operative field to register preoperative scans with intraoperative views through the surgical microscope. We demonstrated state-ofthe-art, real-time performance on challenging neurosurgical images using our method. Our method could be used to improve accuracy in neuronavigation and in image-guided surgery in general."
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Fig. 4 -,
,Fig. 5 .,
,Table 1 .,
1,Introduction,"Radiotherapy (RT) has proven effective and efficient in treating cancer patients. However, its application depends on treatment planning involving target lesion and radiosensitive organs-at-risk (OAR) segmentation. This is performed to guide radiation to the target and to spare OAR from inappropriate irradiation. Hence, this manual segmentation step is very time-consuming and must be performed accurately and, more importantly, must be patient-safe. Studies have shown that the manual segmentation task accounts for over 40% of the treatment planning duration  Nowadays, training of DL segmentation models is predominantly based on loss functions defined by geometry-based (e.g., SoftDice loss  In this paper, we propose an end-to-end training loss function for DL-based segmentation models that considers dosimetric effects as a clinically-driven learning objective. Our contributions are: (i) a dosimetry-aware training loss function for DL segmentation models, which (ii) yields improved model robustness, and (iii) leads to improved and safer dosimetry maps. We present results on a clinical dataset comprising fifty post-operative glioblastoma (GBM) patients. In addition, we report results comparing the proposed loss function, called Dose-Segmentation Loss (DOSELO), with models trained with a combination of binary cross-entropy (BCE) and SoftDice loss functions."
2,Methodology,Figure 
2.1,Deep Learning-Based Dose Prediction,"Recent DL methods based on cascaded U-Nets have demonstrated the feasibility of generating accurate dose distribution predictions from segmentation masks, approximating analytical dose maps generated by RT treatment planning systems  Following  Formally, the dose prediction model M D receives as inputs: segmentations masks for the GTV S T ∈ Z W ×H and the OARs S OR ∈ Z W ×H , the CT image (used for tissue attenuation calculation purposes in RT) I CT ∈ R W ×H , and outputs M D (S T , S OR , I CT ) → D P ∈ R W ×H , a predicted dose map where each pixel value in D corresponds to the local predicted dose in Gy. Due to the limited data availability, we present results using 2D-based models but remark that their extension to 3D is straightforward. Working in 2D is also feasible from an RT point of view because the dose predictor is based on co-planar volumetric modulated arc therapy (VMAT) planning, commonly used in this clinical scenario."
2.2,Dose Segmentation Loss (DOSELO),"During the training of the segmentation model, we used the dose predictor model to generate pairs of dose predictions for the model-generated segmentations and the GT segmentations. The difference between these two predicted dose maps is used to guide the segmentation model. The intuition behind this is to guide the segmentation model to yield segmentation results being dosimetrically consistent with the dose maps generated via the corresponding GT segmentations. To guide the training process with dosimetry information stemming from segmentation variations, we propose to use the mean squared error (MSE) between dose predictions for the GT segmentation (S T ) and the predicted segmentation ( S T ), and construct the following dose-segmentation loss, where D i P and D i P denote pixel-wise dose predictions. The final loss is then, where λ is a hyperparameter to weigh the contributions of each loss term. We remark that during training we use standard data augmentations including spatial transformations, which are also subjected to dose predictions, so the model is informed about relevant segmentation variations producing dosimetry changes."
3,Experiments and Results,
3.1,Data and Model Training,We divide the descriptions of the two separate datasets used for the dose prediction and segmentation models.
,Dose Prediction:,"The dose prediction model was trained on an in-house dataset comprising a total of 50 subjects diagnosed with post-operative GBM. This includes CT imaging data, segmentation masks of 13 OARs, and the GTV. GTVs were defined according to the ESTRO-ACROP guidelines  Segmentation Models: To develop and test the proposed approach, we employed a separate in-house dataset (i.e., different cases than those used to train the dose predictor model) of 50 cases from post-operative GMB patients receiving standard RT treatment. We divided the dataset into training (35 cases), validation (5 cases), and testing (10 cases). All cases comprise a planning CT registered to the standard MRI images (T1-post-contrast (Gd), T1-weighted, T2-weighted, FLAIR), and GT segmentations containing OARs as well as the GTV. We note that for this first study, we decided to keep the dose prediction model fixed during the training of the segmentation model for a simpler presentation of the concept and modular pipeline. Hence, only the parameters of the segmentation model are updated."
,Baselines and Implementation Details:,We employed the same U-Net 
3.2,Evaluation,"To evaluate the proposed DOSELO, we computed dose maps for each test case using a standardized clinical protocol with Eclipse (Varian Medical Systems Inc., Palo Alto, USA). We calculated dose maps for segmentations using the stateof-the-art BCE+SoftDice and the proposed DOSELO. For each obtained dose map, we computed the dose score "
,RM AE,Although it has been shown that geometric-based segmentation metrics poorly correlate with the clinical end-goal in RT 
3.3,Results,"Figure  Table  maps. We analyzed case number 3, 4, and 5 from Fig.  Although we are aware that classical segmentation metrics poorly correlate with dosimetric effects  Table "
,Case Input Image,
4,Discussion and Conclusion,"The ultimate goal of DL-based segmentation for RT planning is to provide reliable and patient-safe segmentations for dosimetric planning and optimally targeting tumor lesions and sparing of healthy tissues. However, current loss functions used to train models for RT purposes rely solely on geometric considerations that have been shown to correlate poorly with dosimetric objectives "
,Fig. 1 .,
,,
,,
1,Introduction,"Residual tumor in the cavity after head and neck cancer (HNC) surgery is a significant concern as it increases the risk of cancer recurrence and can negatively impact the patient's prognosis  During transoral robotic surgery (TORS), surgeons may assess the surgical margin via visual inspection, palpation of the excised specimen and intraoperative frozen sections analysis (IFSA)  Label-free mesoscopic fluorescence lifetime imaging (FLIm) has been demonstrated as an intraoperative imaging guidance technique with high classification performance (AUC = 0.94) in identifying in vivo tumor margins at the epithelial surface prior to tumor excision  However, ability of label-free FLIm to identify residual tumors in vivo in the surgical cavity (deep margins) has not been reported. One significant challenge in developing a FLIm-based classifier to detect tumor in the surgical cavity is the presence of highly imbalanced labels. Surgeons aim to perform an en bloc resection, removing the entire tumor and a margin of healthy tissue around it to ensure complete excision. Therefore, in most cases, only healthy tissue in left in the cavity. To address the technical challenge of highly imbalanced label distribution and the need for intraoperative real-time cavity imaging, we developed an intraoperative FLIm guidance model to identify residual tumors by classifying residual cancer as anomalies. Our proposed approach identified all patients with PSM. In contrast, the IFSA reporting a sensitivity of 0.5 "
2,Method,As illustrated in Fig. 
2.1,FLIm Hardware and Data Acquisition,This study used a multispectral fluorescence lifetime imaging (FLIm) device to acquire data  The FLIm device includes a 440 nm continuous wave laser that serves as an aiming beam; this aiming beam enables real-time visualization of the locations where fluorescence (point measurements) is collected by generating visible blue illumination at the location where data is acquired. Segmentation of the 'aiming beam' allows for FLIm data points to be localized as pixel coordinates within a surgical white light image (see Fig. 
2.2,Patient Cohort and FLIm Data Labeling,"The research was performed under the approval of the UC Davis Institutional Review Board (IRB) and with the patient's informed consent. All Patients were anesthetized, intubated, and prepared for surgery as part of the standard of care. N = 22 patients are represented in this study, comprising HNC in the palatine tonsil (N = 15) and the base of the tongue (N = 7). For each patient, the operating surgeon conducted an en bloc surgical tumor resection procedure (achieved by TORS-electrocautery instruments), and the resulting excised specimen was sent to a surgical pathology room for grossing. The tissue specimen was serially sectioned to generate tissue slices, which were then formalin-fixed, paraffin-embedded, sectioned, and stained to create Hematoxylin & Eosin (H&E) slides for pathologist interpretation (see Fig.  After the surgical excision of the tumor, an in vivo FLIm scan of approximately 90 s was conducted within the patient's surgical cavity, where the tumor was excised. To validate optical measurements to pathology labels (e.g., benign tissue vs. residual tumor), pathology labels from the excision margins were digitally annotated by a pathologist on each H&E section. The aggregate of H&E sections was correspondingly labeled on the ex vivo specimen at the cut lines where the tissue specimen was serially sectioned. Thereafter, the labels were spatially registered in vivo within the surgical cavity. This process enables the direct validation of FLIm measurements to the pathology status of the electrocauterized surgical margins (see Table "
2.3,FLIm Preprocessing,"The raw FLIm waveform contains background noise, instrument artifacts, and other types of interference, which need to be carefully processed and analyzed to extract meaningful information (i.e., the fluorescence signal decay characteristics). To account for background noise, the background signal acquired at the beginning of each clinical case was subtracted from the measured raw FLIm waveform. To retrieve the fluorescence function, we used a non-parametric model based on a Laguerre expansion polynomials and a constrained least-square deconvolution with the instrument impulse response function as previously described "
2.4,Novelty Detection Model,"The state-of-the-art novelty detection models were comprehensively reviewed in the literature  min where W 1 , W 2 are the orthonormal frames, min is the Stiefel manifold, η is the sensitivity margin, and was set η = 0.4 for our experiments. ν denote a penalty factor on these soft constraints, and b is the biases. x i denotes the training set containing CDT of the concatenated FLIm decay curve across channels 1-3 along the time axis. The CDT of the concatenated decay curves is computed as follows: Normalize the decay curves to 0-1. Compute and normalize the cumulative distribution function (CDF). Transforming the normalized CDF into the cumulative distribution transform by taking the inverse cumulative distribution function of the normalized CDF "
2.5,Classifier Training and Evaluation,"The novelty detection model used for detecting residual cancer is evaluated at the pointmeasurement level to assess the diagnostic capability of the method over an entire tissue surface. The evaluation followed a leave-one-patient-out cross-validation approach. The study further compared GODS with two other novelty detection models: robust covariance and, one-class support vector machine (OC-SVM)  Results of a binary classification model using SVM are also shown in the supplementary section Table "
2.6,Classifier Augmented Display,"The classifier augmentation depends on three independent processing steps: aiming beam localization, motion correction, and interpolation of the point measurements. A detailed description of implementing the augmentation process is discussed in "
3,Results,"Table  The GODS uses two separating hyperplanes to minimize the distance between the two classifiers by learning a low-dimensional subspace containing FLIm data properties of healthy labels. Residual tumor labels are detected by calculating the distance between the projected data points and the learned subspace. Points that are far from the subspace are classified as residual tumors. We observed that the GODS with the FLIm decay curves in the CDT space achieve the best classification performance compared to other novelty detection models with a mean accuracy of 0.76 ± 0.02. This is mainly due to the robustness of the model, the ability to handle high-dimensional data, and the contrast in the FLIm decay curves. The contrast in the FLIm decay curves was further improved in the CDT space by transforming the FLIm decay curves to a normalized scale and improving linear separability."
4,Discussion,"Curent study demonstrates that label-free FLIm parameters-based classification model, using a novelty detection aproach, enables identification of residual tumors in the surgical cavity. The proposed model can resolve residual tumor at the point-measurement level over a tissue surface. The model reported low point-level false negatives and positives. Moreover, the current approach correctly identified all patients with PSMs (see Fig.  In context to the standard of care, the proposed residual tumor detection model exhibits high patient-level sensitivity (sensitivity = 1) in detecting patients with PSMs. In contrast, defect-driven IFSA reports a patient-level sensitivity of 0.5  The false positive predictions from the classification model presented two trends: false positives in an isolated region and false positives spreading across a larger region. Isolated false positives are often caused by the noise of the FLIm system and are accounted for by the interpolation approach used for the classifier augmentation (refer to supplementary section Fig.  The novelty detection model generalizes to the healthy labels and considers data falling off the healthy distribution as residual cancer. The FLIm properties associated with the healthy labels in the cavity are heterogeneous due to the electrocautery effects. Electrocautery effects are mainly thermal and can be observed by the levels of charring in the tissue. Refining the training labels based on the levels of charring could lead to a more homogeneous representation of the training set and result in an improved classification model with better generalization."
5,Conclusion,This study demonstrates a novel FLIm-based classification method to identify residual cancer in the surgical cavity of the oropharynx. The preliminary results underscore the significance of the proposed method in detecting PSMs. The model will be validated on a larger patient cohort in future work and address the limitations of the point-level false positive and negative predictions. This work may enhance surgical precision for TORS procedures as an adjunctive technique in combination with IFSA.
,Fig. 1 .,
,Fig. 2 .,
,Table 1 .,
,Table 2 .,
1,Introduction,"Transoesophageal echocardiography (TEE) is a valuable diagnostic and monitoring imaging modality with widespread use in cardiovascular surgery for anaesthesia management and outcome assessment, as well as in emergency and intensive care medicine. The quality of TEE views is important for diagnosis and professional organisations publish guidelines for performing TEE exams  To alleviate the domain mismatch, a feasible solution is unsupervised domain adaptation (UDA). UDA aims to increase the performance on the target domain by using labelled source data with unlabelled target data to reduce the domain shift. For example, Mixstyle  In this paper, we propose a novel UDA regression network named SR-AQA that performs style alignment between TEE simulated and real domains while retaining domain-invariant and task-specific information to achieve promising AQA performance. To estimate the uncertainty of intra-domain style offsets in real data, we employ uncertainty-aware feature stylization (UFS) utilizing multivariate Gaussians to regenerate feature statistics (i.e. mean and standard deviation) of real data. To reduce the inter-domain gap, UFS augments simulated features to resemble diverse real styles and obtain real-stylized variants. We then design a style consistency learning (SCL) strategy to learn domaininvariant representations by minimizing the negative cosine similarity between simulated features and real-stylized variants in an extra feature space. Enforcing task-specific learning (TL) in real-stylized variants allows SR-AQA to keep task-specific information useful for AQA. Our work represents the original effort to address the TEE domain shift in AQA tasks. For method evaluation, we present the first simulation-to-real TEE dataset with two AQA tasks (see Fig. "
2,Methodology,
2.1,Dataset Collection,"We collected a dataset of 16,192 simulated and 4,427 real TEE images from 9 standard views. From Fig.  As the number of criteria thus the maximum score varies for different views, we normalise CP as a percentage to provide a consistent measure across all views. Scores from the 3 raters were averaged to obtain the final score for each view. The Pearson product-moment correlation coefficients between CP and GI are 0.81 for simulated data and 0.70 for real data, indicating that these two metrics are correlated but focus on different clinical quality aspects. Inter-rater variability is assessed using the two-way mixed-effects interclass correlation coefficient (ICC) with the definition of absolute agreement. Both CP and GI, show very good agreement between the 3 annotators with ICCs of 0.959, 0.939 and 0.813, 0.758 for simulated and real data respectively."
2.2,Simulation-to-Real AQA Network (SR-AQA),"Overview of SR-AQA. Illustrated in Fig.  2 is the MSE loss calculated from the simulated data result R s and its label y s , while λ 1 and λ 2 are parameters empirically set to ""10"" and ""1"" to get a uniform order of magnitude at the early training stage. The input is fed into one encoder and regressor to predict the score during inference."
,Uncertainty-Aware Feature Stylization (UFS).,"The UFS pipeline is shown in the right part of Fig.  where: μ(f ) and σ(f ) denote channel-wise mean and standard deviation of feature f , respectively. However, due to the complexity of real-world TEE, there are significant intra-domain differences, leading to uncertainties in the feature statistics of real data. To explore the potential space of unknown intra-domain shifts, instead of using fixed feature statistics, we generate multivariate Gaussian distributions to represent the uncertainty of the mean and standard deviation in the real data. Considering this, the new feature statistics of real features f r l , i.e. mean β(f r l ) and standard deviation α(f r l ), are sampled from , respectively and computed as: where: 2 are estimated from the mean and standard deviation of the batch B of real images, I ρ>0.5 is an indicator function and ρ ∼ U(0, 1). Finally, our UFS for l th layer is defined as: To this end, the proposed UFS approach can close the reality gap by generating real-stylized features with sufficient variations, so that the network interprets real data as just another variation."
,Style Consistency Learning (SCL).,"Through the proposed UFS, we obtain the final real-stylized features f s→r f inal that contain a diverse range of real styles. The f s→r f inal can be seen as style perturbations of the final simulated features f s f inal . We thus incorporate a SCL step, that maximizes the similarity between f s f inal and f s→r f inal to enforce their consistency in the feature level, allowing the encoder to learn robust representations. Specifically, the SCL adds a projector independently of the regressor to transform the f s f inal (f s→r f inal ) in an extra feature embedding, and then matches it to the other one. To prevent the Siamese encoder and Siamese projector (i.e. the top two encoders and projectors in Fig.  is the negative cosine similarity between the input features f 1 and f 2 , and • 2 is L2-normalization. The SCL guides the network to learn domain-invariant features, via various style perturbations, so that it can generalize well to the different visual appearances of the real domain."
,Task-Specific Learning (TL).,"While alleviating the style differences between the simulated and real domain, UFS filters out some task-specific information (e.g. semantic content) encoded in the simulated features, as content and style are not orthogonal  The TL performs AQA tasks for style variants to complement the loss of information due to feature stylization."
3,Experiments,
3.1,Experimental Settings,"Experiments are implemented in PyTorch on a Tesla V100 GPU. The maximum training iteration is 40,000 with a batch size of 32. We adopted the SGD optimizer with a weight decay of 5e-4, a momentum of 0.9 and a learning rate of 1e-4. Input images are resized to 224 × 224, the CP and GI scores are normalized to [0, 1]. The MSE is adopted as the evaluation metric for both the CP and GI regression tasks. Following the standard approach for UDA "
3.2,Comparison with State-of-the-Arts,We compare the proposed SR-AQA with MixStyle  As shown in Table 
3.3,Ablation Study,We first explore the impact of the amount of UFS on generalization performance. As shown in the left part of Table 
4,Conclusion,"This paper presents the first annotated TEE dataset for simulation-to-real AQA with 16,192 simulated images and 4,427 real images. Based on this, we propose a novel UDA network named SR-AQA for boosting the generalization of AQA performance. The network transfers diverse real styles to the simulated domain based on uncertainty-award feature stylization. Style consistency learning enables the encoder to learn style-independent representations while taskspecific learning allows our model to naturally adapt to real styles by preserving task-specific information. Experimental results on two AQA tasks for CP and GI scores show that the proposed method outperforms state-of-the-art methods with at least 5.08% and 16.28% MSE reduction, respectively, resulting in superior TEE AQA performance. We believe that our work provides an opportunity to leverage large amounts of simulated data to improve the generalisation performance of AQA for real TEE. Future work will focus on reducing negative transfer to extend UDA methods towards simulated-to-real TEE quality assessment."
,Fig. 1 .,
,Fig. 2 .,
,Table 1 .,
,Table 2 .,
,Table 3 .,
1,Introduction,"Trustworthy and reliable visual question-answering (VQA) models have proved their potential in the medical domain  Meanwhile, catastrophic forgetting has become a largely discussed topic in deep neural networks. Deep neural networks (DNNs) shall abruptly and drastically forget old knowledge when learning new  Furthermore, most medical decision-making tasks shall include classes overlapping with the old tasks and newly appeared classes, as shown in Fig.  In this work, (1) We establish a non-exemplar CS-VQLA framework. While being applied to surgical education and scene understanding, the framework can learn data in a streaming manner and effectively resist catastrophic forgetting. (2) We revisit the distillation method for CL, and propose rigidityplasticity-aware distillation (RP-Dist) and self-calibrated heterogeneous distilla- tion (SH-Dist) for the output logits and intermediate feature maps, respectively. The weight aligning (WA) technique is further integrated to adjust model bias between old and new data. (3) Extensive comparison and ablation studies prove the outstanding performance of our method in mitigating catastrophic forgetting, demonstrating its potential in real-world applications."
2,Methodology,
2.1,Preliminaries,"Problem Definition. We define the continual learning sequence with T P time periods, and t ∈ {1, ..., T P} means the current time period. D t denotes the training dataset at time period t, with x representing a sample of the input question and image pair in D t . C old denotes the classes appearing in previous time period {1, ..., t -1}, and C new represents the classes appearing in current time period t. Furthermore, we define the classes existing in both C old and C new as overlapping classes C op , and define unique classes in C old as old non-overlapping classes C no . F stands for the output feature map from the network backbone. Knowledge Distillation (KD)  in which p o T (x) = SM (z o /T ) and p cl T (x) = SM (z cl /T ) represent the probabilities. SM means Softmax. T is temperature normalization for all old classes. Weight Aligning (WA)  where norm means normalizing all the elements in the vector. In classincremental learning, WA can effectively avoid the model bias towards new classes. "
,Old Model ConƟnual Learning Model,
2.2,Continual Surgical VQLA (CS-VQLA),"Visual-Question Localized Answering. We define our VQLA framework following  , where μ is set as 100 to balance the optimization progress of the two tasks. Rigidity-Plasticity-Aware Distillation (RP-Dist). The current rigidityplasticity trade-off is towards the entire model. However, we shall make the rigidity-plasticity aware in overlapping and non-overlapping classes. There is no overlap between C old and C new in an ideal class-incremental learning setup, so the temperature T in Equ. 1 is set to 2 by  When λ is set to a very high number (e.g., λ ≥ 0.9), we will have a high probability of getting a correct class, allowing the teacher model to have perfect performance. The probability output of the CL model can be optimized with this pseudo-teacher based on the Kullback-Leibler divergence D KL : T is the KD temperature used to generate soft probabilities for the pseudo old model. As discussed above, this naive setting of T is not suitable for general CL scenarios. Therefore, we treat T op and T on differently to strengthen the plasticity on C op and the rigidity on C on respectively. L RKD can thereby be rewritten as: We keep T op > T on to balance the rigidity and plasticity trade-off in the CL model, and set T op = 25, T on = 20 empirically in our implementation."
,Self-Calibrated Heterogeneous Distillation (SH-Dist).,"Works have discussed the use of self-calibration to improve model performance  Subsequently, the self-calibrated feature map F t shall be propagated through the parallel classifier and detector for the multi-task prediction. Overall Framework. Figure  We set α = β = 1 and γ = 5 in our implementation. Furthermore, WA is deployed after training on each time period, to balance the weight bias of new classes on the classification layer. Through the combination of multiple distillation paradigms and model weight adjustment, we successfully realize the general continual learning framework in the VQLA scenario."
3,Experiments,
3.1,Dataset and Setup,"Dataset. We construct our continual procedure as follows: when t = 0, we train on EndoVis18 Dataset, t = 1 on EndoVis17 Dataset, and t = 2 on M2CAI Dataset. Therefore, we can establish our CS-VQLA framework with a large initial step, and several smaller sequential steps. When splitting the dataset, we isolate the training and test sets in different sequences to avoid information leakage. EndoVis18 Dataset is a public dataset with 14 videos on robotic surgery  EndoVis17 Dataset is a public dataset with 10 videos on robotic surgery  M2CAI Dataset is also a public robotic surgery dataset "
3.2,Results,"Except for testing on three datasets separately, we set three specific categories in our continual learning setup: old non-overlapping (old N/O) classes, overlapping classes, and new non-overlapping (new N/O) classes. By measuring the performance in these three categories, we can easily observe the catastrophic forgetting phenomenon and the performance of mitigating catastrophic forgetting. As shown in Table "
4,Conclusion,"This paper introduces CS-VQLA, a general continual learning framework on surgical VQLA tasks. This is a significant attempt to continue learning under complicated clinical tasks. Specifically, we propose the RP-Dist on output logits, and the SH-Dist on the intermediate feature space, respectively. The WA technique is further integrated for model weight bias adjustment. Superior performance on VQLA tasks demonstrates that our method has an excellent ability to deal with CL-based surgical scenarios. Except for giving localized answers for better surgical scene understanding, our solution can conduct continual learning in any questions in surgical applications to solve the problem of class increment, domain shift, and overlapping/non-overlapping classes. Our framework can also be applied when adapting a vision-language foundation model in the surgical domain. Therefore, our solution holds promise for deploying auxiliary surgical education tools across time/institutions. Potential future works also include combining various surgical training systems (e.g., mixed reality-based training, surgical skill assessment) to develop an effective and comprehensive virtual teaching system."
,Fig. 1 .,
,Fig. 2 .,
,Table 1 .,
,.98 74.57 78.26 78.59 54.33 74.02 75.12 77.02 64.73 75.52Table 2 .,
,Table 3 .,
,53 61.08 56.98 74.57 78,
,73 75.52 28.20 68.14 41.04 65.74 44.44 74.41 42.46 66.14,
,Acknowledgements,. This work was funded by 
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 7.
1,Introduction,"Surgical video analysis is a rapidly growing field that aims to improve and gain insights into surgical practice by leveraging increasingly available surgical video footage  Fig.  We retain spatial edges (shown with dotted lines) from the graph encoder in GV . (Color figure online) Such anatomy-based reasoning can be accomplished through object-centric modeling, which is gaining popularity in general computer vision  In this work, we tackle these challenges by proposing to build latent spatiotemporal graph representations of entire surgical videos, with each node representing a surgical tool or anatomical structure and edges representing rela-tionships between nodes across space and time. To build our graphs, rather than use an off-the-shelf object detector, we employ the latent graph encoder of  We evaluate our method on two downstream tasks: critical view of safety (CVS) clip classification and surgical phase recognition. CVS clip classification is a fine-grained task that requires accurate identification and reasoning about anatomy, and is thus an ideal target application for our object-centric approach. On the other hand, phase recognition is a coarse-grained task that requires holistic understanding of longer video segments, which can demonstrate the effectiveness of our temporal edge building framework. We achieve competitive performance in both of these tasks and show that our graph representations can be used with or without task-specific finetuning, thereby demonstrating their value as general-purpose video representations. In summary, we contribute the following: 1. A method to encode surgical videos as latent spatiotemporal graphs that can then be used without modification for two diverse downstream tasks. 2. A framework for effectively modeling long-range relationships in surgical videos via multiple-horizon temporal edges. 3. A Graph Editing Module that can correct errors in the predicted graph based on temporal coherence cues and prior knowledge."
2,Methods,"In this section, we describe our approach to encode a T -frame video V = {I t | 1 ≤ t ≤ T } as a latent spatiotemporal graph G V (illustrated in Fig. "
2.1,Graph Construction,"Object Detection. To construct a latent spatiotemporal graph representation, we must first detect the objects in each frame, along with any additional prop-erties. We do so by employing the graph encoder φ SG proposed in  Spatiotemporal Graph Building. Once we have computed the graphs G t , we add temporal edges to construct a single graph G V that describes the entire video. G V retains the spatial edges from the various G t to describe geometric relations between objects in the same frame (i.e. to the left of, above), while also including temporal edges between spatially and visually similar nodes. It can then be processed with a graph neural network during downstream evaluation to efficiently propagate object-level information across space and time. We add temporal edges to G V based on object bounding box overlap and visual feature similarity, inspired by  (1) Edge Horizon Selection. While φ TE is designed to construct edges between arbitrarily distant graphs, effective selection of temporal horizons W is nontrivial. We could naively include every possible temporal horizon, setting W = {1, 2, ..., T -1} to maximize temporal information flow; however, making W too dense results in redundancies in the resulting graph, which can have an oversmoothing effect during downstream processing with a graph neural network (GNN) "
2.2,Graph Editing Module,"One limitation of object-centric approaches is a reliance on high quality object detection, which is particularly steep in surgical videos. These difficulties in object detection could be tackled by incorporating prior knowledge such as anatomical scene geometry, but incorporating these constraints into the learning process often requires complex constraint formulations and methodologies. We posit that our spatiotemporal graph structure represents a simpler framework to incorporate such constraints; to demonstrate this, we introduce a module to filter detections of anatomical structures, which are particularly difficult to detect, incorporating the constraint that there is only one of each structure in each frame. Specifically, after building the spatiotemporal graph, we compute a dropout probability p i,t = 1 deg(ni,t) for each node, where deg is the degree operator. Then, for each frame t, for each object class r j , we select the highest scoring node n t from {n i,t |r i,t }. During training, we apply graph editing with probability p edit , providing robustness to a wide range of input graphs."
2.3,Downstream Task Decoder,"For downstream prediction from G V , we first apply a GNN using the architecture proposed in "
2.4,Training,"We adopt a two-stage training approach, starting by training φ SG as proposed in "
3,Experiments and Results,"In this section, we describe our evaluation tasks and datasets, describe baseline methods and our model, then present results for each task. We conclude with an ablation study that illustrates the impact of our various model components."
3.1,Evaluation Tasks and Datasets,"Critical View of Safety (CVS) Prediction. The CVS consists of three independent criteria, and can be viewed as a multi-label classification problem  For our experiments, we use the Endoscapes+ dataset introduced in "
3.2,Baseline Methods,"Single-Frame Methods. As CVS clip classification is unexplored, we compare to two recent single-frame methods for reference: LG-CVS  DeepCVS-Temporal. We also extend DeepCVS for clip classification by replacing the last linear layer of the dilated ResNet-18 with a Transformer decoder, referring to this model as DeepCVS-Temporal. STRG. Space-Time Region Graphs (STRG)  TeCNO. TeCNO "
3.3,Main Experiments,CVS. Our first takeaway from Table  Phase. Table 
3.4,Ablation Studies,Table 
4,Conclusion,"We introduce a method to encode surgical videos in their entirety as latent spatiotemporal graph representations. Our graph representations enable finegrained anatomy-driven reasoning as well as coarse long-range video understanding due to the inclusion of edges at multiple-temporal horizons, robustness against errors in object detection provided by a graph editing module, and memory-and computational-efficiency afforded by a two-stage training pipeline. We believe that the resulting graphs are powerful general-purpose representations of surgical videos that can fuel numerous future downstream applications."
,Table 1 .,
,Table 2 .,
,Table 3 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_62.
1,Introduction,"Reduction planning is a crucial phase in pelvic fracture surgery, which aims to restore the anatomical structure and stability of the pelvic ring  The most intuitive method for reduction planning is by matching the fracture surface characteristics  Han et al.  While the SSM method addresses the reduction within single bones, it does not apply well to inter-bone matching for joint dislocations  In this study, we present a two-stage method for automatic pelvic reduction planning, addressing bone fractures and joint dislocations sequentially. A novel SSM-based symmetrical complementary (SSC) registration is designed to register multiple bone fragments to adaptive templates to restore the symmetric structure. Then, an articular surface (AS) detection and matching algorithm is designed to search the optimal target pose of dislocated bones with respect to joint alignment and symmetry constraints. The proposed method was evaluated in simulated experiments on a public dataset, and further validated in typical clinical cases. We have made our clinical dataset publicly available at https:// github.com/Sutuk/Clinical-data-on-pelvic-fractures. "
2,Method,As shown in Fig. 
2.1,Gaussian Process Morphable Models for Pelvic Bone,"We use Gaussian process morphable models (GPMMs)  Parameterized Model. The GPMMs of the left ilium and sacrum are modeled separately by first aligning bone segmentations to a selected reference Γ R . A bone shape Γ B can be obtained by warping the reference Γ R with a deformation field u(x). The GPMMs models deformation as a Gaussian process GP (μ, k) with a mean function μ and covariance function k, and is invariant to the choice of reference Γ R . Parameterized with principal component analysis (PCA) in a finite dimension, the GPMMs can be written as: where P and b are the principal components and the weight vector, respectively. GPMMS modeled with the empirical kernel in a finite domain is equivalent to a statistical shape model (SSM), and the parameter is denoted as b SM . The parameters of the localized model are denoted as b LC . Empirical Kernel for Fracture Reduction. A Gaussian process GP (μ SM , k SM ) that models the distinctive deformations is derived from the and covariance function: where n represents the number of training surfaces, and u i (x) denotes single point deformation on the i-th sample surfaces. Localized Kernel for as Detection. In order to increase the flexibility of the statistical model, we limit the correlation distance between point clouds. The localized kernel is obtained by multiplying the empirical kernel and a Gaussian kernel k g (x, y) = exp(xy 2 /σ 2 ): where denotes element-wise multiplication, the identity matrix I 3×3 is a 3D vector field, and σ determines the range of correlated deformations. The kernel is then used in AS detection."
2.2,Symmetrical Complementary Registration,"A novel SSC registration is designed to register bone fragments to adaptive templates in joint optimization, which alternatingly estimates the desired reduction of fragments and solves the corresponding transformations. A bilateral supplementation strategy is used in model adaptation: mirrored counterparts are aligned to the target point cloud to provide additional guidance. In more detail, both the fragment and its mirrored counterpart are first rigidly aligned to the GPMM with empirical kernel k SM , using the iterative closest point (ICP) algorithm. Then, the GPMM is non-rigidly deformed towards the merged point clouds of the main fragment and its mirrored counterparts. During this step, the model's parameters b SM are optimized using bi-directional vertex correspondences (target to source v → (x) and backward v ← (x)) based on the nearest neighbour. For each source point x, there is a forward displacement δ → (x) = v → (x)x, and one or more backward displacement δ ← (x) = v ← (x)-x. The farthest point within the given threshold ε is selected as the corresponding point displacement δ(x) of the template: where I is the indicator function and O is a zeros vector. The optimal parameter bSM of the statistical model can be calculated from Eq. (  The adaptive shape of the GPMM is regarded as the target morphology of a bone, and the fracture fragments are successively registered to the adaptive template to form the output of the first stage, reduction transformation T l of each bone fragment S l , l = 1, ..., L. As shown in Fig. "
2.3,Articular Surface Detection and Matching,"Articular Surface Detection. The AS of the sacroiliac (SI) joint and pubic symphysis (PS) are detected using the GPMMs for the ilia and sacrum, with the localized kernel. As indicated by the red and blue regions in Fig.  Articular Surface Matching. For each joint, a ray-tracing collision detection algorithm is performed between opposing surfaces using surface normal  Then, a cost function L local is constructed to measure the degree of alignment between two surfaces, including a distance term and a collision term. The former measures the Chamfer distance between complementary surfaces d cd  where S 1 and S 2 are two sets of 3D points, and γ is a balancing weight for the collision term. In addition to the local cost for each joint, a global cost measuring the degree of symmetry is used to regularize the overall anatomy of the pelvis: where the first term measures the paired difference between the point-wise distance within SI lef t and SI right . D lef t and D Right represent the distance between points in SI lef t and SI right , respectively. The second term measures the angle between the mean vector of the PS, denoted as -→ V P S , and the mean vector of the bilateral SI joint -→ V SI . The AS matching problem is formulated as optimizing the transformations with respect to the misalignment of PS and SI joints, plus the pelvic asymmetry: where L local (SI lef t ) or L local (SI right ) can be a constant when the corresponding joint is intact, and can be omitted accordingly. The cost function in Eq. ( "
3,Experiments and Results,
3.1,Experiments on Simulated Data,"We evaluated the proposed fracture reduction planning method in a simulation experiment with leave-one-out cross-validation on the open-source pelvic atlas  The Gaussian kernel parameter σ was set to 30, the collision regularization factor γ was set to 2, and the bidirectional correspondence threshold ε was set to 5. The accuracy of reduction was evaluated by the root-mean-square error (RMSE) on all fragments. Specifically, the average distance between the ground truth and the planned target location of all points was calculated. As shown in Fig.  As shown in Fig. "
3.2,Experiments on Clinical Data,"To further evaluate the proposed method on real clinical data, we collected CT data from four clinical cases (aged 34-40 years, one female), each representing a distinct type of pelvic fractures corresponding to the simulated categories. The data usage is approved by the Ethics Committee of the Beijing Jishuitan Hospital (202009-04). The proposed method was applied to estimate transformations of bone fragments to obtain a proper reduction in each case. Due to the lack of ground-truth target positions for fractured pelvis in clinical data, we employed  geometric measurements to evaluate the planning result. This evaluation method was inspired by Matta's trauma surgery criteria "
4,Discussion and Conclusion,"We have proposed a two-stage method for pelvic fracture reduction planning based on morphable models and structural constraints. The SSM-based symmetrical complementary registration was successfully applied to both bilateral iliac and sacral fractures, which further extended the unilateral iliac CM reduction. Combining the SSM approach with symmetrical complementation provided more complete morphological guidance for model adaptation, and improved the registration performance significantly. The AS detection and matching method which combines local joint matching and global symmetry constraints achieved significantly higher accuracy than the pelvic mirroring reference. The proposed planning method also achieved satisfactory results on simulated data with combined pelvic trauma and real clinical cases. We have demonstrated the synergy of the combined statistical model and anatomical constraints. In future work, we plan to further investigate this direction by incorporating feasibility constraints into SPM-based method to benefit from both cohort-based statistics and individual-specific matching criteria. In the experiments, we simulated the most common fracture types and obtained the ground truth for evaluation. Due to the absence of ground truth in clinical data, we resort to an independent distance metric. We plan to test our method on more clinical data, and combine geometric measurements and manual planning for a comprehensive evaluation and comparison. We also intend to further automate the planning pipeline using a pelvic fracture segmentation network in future research to avoid the tedious manual annotation process."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_31.
1,Introduction,"Resection of early-stage brain tumors can greatly reduce the mortality rate of patients. During the surgery, brain tissue deformation (called brain shift) can occur due to various causes, such as gravity, drug administration, and pressure change after craniotomy. While modern magnetic resonance imaging (MRI) techniques can provide rich anatomical and physiological information with various contrasts (e.g., fMRI) for more elaborate pre-surgical planning, intra-operative MRI that can track brain shift requires a complex setup and is costly. In contrast, intra-operative ultrasound (iUS) has gained popularity for real-time imaging during surgery to monitor tissue deformation and surgical tools because of its lower cost, portability, and flexibility  Recently, automatic quality assessment for medical image registration has attracted increasing attention "
2,Methods and Materials,
2.1,Dataset and Preprocessing,"For methodological development and assessment, we used the RESECT (REtro-Spective Evaluation of Cerebral Tumors) dataset "
2.2,Network Architecture,"We proposed a novel 3D neural network, named FocalErrorNet, based on the recent focal modulation networks "
2.3,Uncertainty Quantification,"For registration error regression in surgical applications, knowledge regarding the reliability of the automated results is instrumental for the safety and wellbeing of the patients. Uncertainty estimation has gained popularity in probing the trustworthiness and credence of DL algorithms. Although the concept has been widely applied in image segmentation and classification, it has not been employed for registration error estimation, especially in the case of multi-modal situations, such as MRI-iUS alignment. Therefore, we incorporated uncertainty estimation in our proposed FocalErrorNet. For each MRI-iUS patch pair, 200 regression samples were collected by random sampling from MC dropouts "
2.4,Experimental Setup and Implementation,"From the transformation augmentation, we acquired 3380 samples of MRI-iUS pairs. For our experiments, we arbitrarily split the subjects into training, validation, and test sets with the proportion of 60%, 20%, and 20%, respectively. To prevent information leakage, we ensured that each patient was included in only one of the split sets. For model training, we adopted the Adam optimization with a learning rate of 5 × 10 -5 and a batch size of 64. For the loss function, we used mean squared error (MSE) to minimize the difference between the predicted MRI-iUS registration error and the ground truths. Furthermore, in addition to the transformation augmentation, we also included additional data augmentation, including random noise addition and random image flipping on training sets to mitigate overfitting and increase the model's generalizability. To assess our proposed FocalErrorNet, we compared it against a 3D CNN [9,12] (see Fig. "
3,Results,
3.1,Error Regression Accuracy,The accuracy comparison between the proposed FocalErrorNet and the baseline 3D CNN 
3.2,Validation of the Uncertainty Evaluation,"We obtained correlations of 0.70 (p < 1e-4) and 0.34 (p < 1e-4) between estimated uncertainty and prediction error for FocalErrorNet and the baseline 3D CNN, respectively. Additionally, the uncertainty vs. mutual information uncertainties was assessed at -0.67 (p < 1e-4) for our proposed method and -0.18 (p < 1e-3) for the baseline. To allow better visual comparisons, the associated scatter plots are illustrated in Fig. "
3.3,Robustness of the Proposed Model,"To examine the performance of our proposed method for image regions that contain fewer potent anatomical features, we acquired additional image pairs from test subjects, according to Sect. 2.4. With the new test set, the prediction errors for our method and the baseline model were 1.28 ± 0.99 mm and 2.49 ± 1.87 mm, respectively. Furthermore, the correlations between estimated and true error were calculated at 0.41 for FocalErrorNet and 0.20 for the baseline. These results supported the benefits of focal modulation in registration error estimation. In this test, patches can contain large areas of zeros (image content out of the scanning FOV of the iUS). The main reason for the observed performance decline is due to the reduction in sufficient image features in iUS. However, despite these challenges, we saw an acceptable outcome from FocalEr-rorNet (absolute error = 1.28 mm or ∼1 voxel in clinical MRIs)."
4,Discussion,"In image-guided interventions, there is an urgent need for automatic assessment of image registration quality. Multi-modal registration quality evaluation poses major challenges due to three main factors. First, dissimilar contrasts between images require more elaborate strategies to derive relevant features for error assessment. Second, unlike segmentation or classification, the ground truths of registration errors are difficult to obtain. Finally, compared with classification, regression tasks tend to be more error-prone for deep learning algorithms. To tackle these challenges, we employed 3D focal modulation with depth-wise convolution to encode contextual information for the image pair. Compared with the ViT and its variants, focal modulation allows a more lightweight setup, which could be desirable for 3D data. Although we admit that residual errors still remain after landmark-based B-Spline nonlinear alignment, this approach has been adopted in different prior studies, considering the residual landmark registration error is fairly low (mTRE of 0.0008 ± 0.0010mm). Although simulated ultrasound has been used to provide a perfect alignment with MRIs, the fidelity of the simulated results is still suboptimal, and this may explain the underperformance of the previous technique in real clinical data "
5,Conclusion,"We proposed FocalErrorNet, a novel DL model for uncertainty-aware inter-modal registration error estimation in iUS-guided neurosurgery, leveraging the latest focal modulation technique and MC dropout. With thorough assessments of the accuracy and uncertainty measures, we have confirmed the performance of the proposed method against a baseline model previously adopted for the same task. As the first to introduce uncertainty measures and 3D focal modulation in registration error evaluation, our work provides the first step for fast and reliable feedback in inter-modal medical image registration to guide clinical decisions in surgery. We plan to adapt the presented framework for other inter-modal/contrast image registration applications in the future."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Fig. 5 .,
,Table 1 .,
1,Introduction,"Surgical step recognition is necessary to enable downstream applications such as surgical workflow analysis  Early statistical methods to recognize surgical workflow, such as Conditional Random Fields  Transformers improve feature representation by effectively modeling longterm dependencies, which is important for recognizing surgical steps in complex videos. In addition, they offer fast processing due to their parallel computing architecture. To exploit these benefits and address the issue of inductive bias (e.g. local connectivity and translation equivariance) in CNNs, recent methods use only transformers as their building blocks, i.e., Vision Transformer. For example, TimesFormer  In this work, we propose a transformer-based model with the following contributions: (1) Spatio-temporal attention is used as the building blocks to address issues with inductive bias and end-to-end learning of surgical steps; (2) A twostream model, called Gated -Long, Short sequence Transformer GLSFormer , is proposed to capture long-range dependencies and a gating module to leverage cross-stream information in its latent space; and (3) The proposed GLSFormer is extensively evaluated on two cataract surgery video datasets to show that it outperforms all compared methods."
2,The GLSFormer Model,"Given an untrimmed video with X[1 : T ] frames, where T represents the total number of frames in the video, the objective is to predict the step Y [t] of a given frame at time t (Fig.  Long-short Sequence. We propose GLSFormer model that can capture both short-term and long-term dependencies in surgical videos. The input to our GLSFormer are two video sequences, a short-term sequence consisting of the last n st frames from time t, and a long-term sequence composed of n lt frames selected from a sub-sampled set of frames with a sampling period of s. The longterm sequence provides a coarse overview of information distant in time and can aid in accurate predictions of the current frame, overcoming false prediction based on common artifacts in short-term sequences. In addition, the overview of information can address the high variability in surgical scenes "
,Patch Embedding. We decompose each frame of dimension H,"Each patch is flattened into a vector x p,t ∈ R 3Q 2 for each frame t and spatial location, p ∈ (1, N). We linearly map the patches of short and long term videos frames into embedding vector of dimension R K using a shared learnable matrix E ∈ R K×3Q 2 . We concatenate the patch embeddings of the short and long-term streams along the frame dimension to form feature representations x st p,t and x lt p,t of size N × n st × K and N×n lt ×K, respectively. along with a learnable positional embedding e st-pos p,t . ( Note that a special learnable vector z st 0,0 ∈ R K representing the step classification token is added in the first position. Our approach is similar to word embeddings in NLP transformer models  Gated Temporal, Shared Spatial Transformer Encoder. Our Transformer Encoder, consisting of Gated Temporal Attention module and Shared Spatial Attention module takes the sequence of embedding vectors z st p,t and z lt p,t as input. In a self-attention module for spatio-termporal models, computational complexity increases non-linearly O(T 2 S 2 ) with increase in spatial resolution(S) or temporal frames(T). Thus, to reduce the complexity, we sequentially process our gated temporal cross attention module and spatial attention module "
,Gated Temporal Attention. The temporal cross-attention module aligns the long-term(z lt,"l-1 ) and short-term features(z st l-1 ) in the temporal domain, allowing the model to better capture the relationship between the long and short term streams. We concatenate both of these streams to form a strong joint stream that has both fine-grained information from the short-term stream and global context information from the long-term stream. Firstly, a query/key/value vector for each patch in the representations z lt l-1 (p, t), z st l-1 (p, t) and z lt,st l-1 (p, t) using linear transformations with weight matrices U lt qkv , U st qkv and U lt,st qkv respectively and normalization using LayerNorm is computed as follows: where a ranges from 1 to A representing attention heads. The total number of attention heads is denoted by A, and each has a latent dimensionality of K h = K A . The computation of these QKV vectors is essential for multi-head attention in transformer. Now for refining the streams, with most relevant cross-stream information, we gate the individual stream's temporal features(I) (QKV ) a;lt/st with the joint stream temporal features(J) (QKV ) a;lt,st . Gating parameters are calculated by concatenating I and J and passing them through linear and softmax layers which predict Gt st and Gt lt for (QKV ) a;st and (QKV ) a;lt , respectively. By gating the individual stream's temporal features with the joint stream temporal features, the model is able to selectively attend to the most relevant features from both streams, resulting in a more informative representation. This helps in capturing complex relationships between the streams and improves the overall performance of the model. This computation can be described as follows Later, temporal attention is computed by comparing each patch (p, t) with all patches at the same spatial location in other frames of both streams, as follows Here, α is separately calculated for the long-stream and shortstream, where (•) = lt or st. Similar to the vision transformer, encoding blocks for each layer (z lt and z st ) are computed by taking the weighted sum of value vectors (SA a (z)) using self-attention coefficients from each attention head as follows Next, the self-attention block (SA a (z)) for each attention head is projected along with a residual connection from the previous layer. This multi-head self-attention (MSA) operation can be described as follows Here, (z ) l is the concatenation of z lt and z st ."
,Shared Spatial Attention.,"Next, we apply self-attention to the patches of the same frame to capture spatial relationships and dependencies within the frame. To accomplish this, we calculate new key/query/value using Eq. (  The encoding blocks are also calculated using Eq. (  The embedding for the entire clip is obtained by taking the output from the final block and passing it through a MLP with one hidden layer. The corresponding computation can be described as y = LN(z L (0,0) ) ∈ R D . The classification token is used as the final input to the MLP for predicting the step class at time t. Our GLSFormer is trained using the cross-entropy loss."
3,Experiments and Results,"Datasets. We evaluate our GLS-Former on two video datasets of cataract surgery, namely Cataract-101  Evaluation Metrics. To accurately evaluate the results of surgical step prediction models, we use four different metrics, namely Accuracy, Precision, Recall, and Jaccard index  Table  Ablation Studies. The top part of Table  The results demonstrate that incorporating a coarse long-term stream is crucial for achieving significant performance gains compared to not using any long-term sequence (as in ViT and TimesFormer). Additionally, we observe that gradually increasing the sampling rate from 2 to 8 improves performance across all metrics, except for a slight decline at a sampling rate of 16. This decline may be due to a loss of information and noisy predictions resulting from the high number of frames skipped between each selected frame. Therefore, we chose a sampling rate of 8, as it provided the optimal balance between capturing valuable information and avoiding noise in our long-stream sequence. To evaluate our gating mechanism's effectiveness, we conducted ablation experiments with three different settings, as summarized in the bottom part of Table "
4,Conclusion,"We propose GLSFormer , a vision transformer-based method for recognizing surgical steps in complex videos. Our approach uses a gated temporal attention mechanism to integrate short and long-term cues, resulting in superior performance compared to recent LSTM and vision transformer-based approaches that only use short-term information. Our end-to-end joint learning captures spatial representations and sequential dynamics more effectively than multi-stage networks. We extensively evaluated GLSFormer and found that it consistently outperformed state-of-the-art models for surgical step recognition."
,Fig. 2 .,
,Table 2 .,
,.67 90.04 ± 0.71 89.45 ± 0.79 81.89 ± 0.92,
1,Introduction,"Craniomaxillofacial (CMF) deformities can affect the skull, jaws, and midface. When the primary cause of disfigurement lies in the skeleton, surgeons cut the bones into pieces and reposition them to restore normal alignment  However, the current bone-driven methods have a major limitation, subjecting to the complex and nonlinear relationship between the bones and the draping soft-tissues. Surgeons estimate the required bony movement through trial and error, while computer-aided surgical simulation (CASS) software  To address the above challenge, this paper proposes a novel soft-tissue driven surgical planning framework. Instead of simulating facial tissue under the guessed movement, our approach directly aims at a desired facial appearance and then determines the optimal bony movements required to achieve such an appearance without the need for iterative revisions. Unlike the bone-driven methods, this soft-tissue driven framework eliminates the need for surgeons to make educated guesses about bony movement, significantly improving the efficiency of the surgical planning process. Our proposed framework consists of two main components, the Bony Planner (BP) and the Facial Simulator (FS). The BP estimates the possible bony movement plans (bony plans) required to achieve the desired facial appearance change, while the FS verifies the effectiveness of the estimated plans by simulating the facial appearance based on the bony plans. Without the intervention of clinicians, the BP automatically creates the most clinically feasible surgical plan that achieves the desired facial appearance. The main contributions of our work are as follows. 1) This is the first softtissue driven approach for CMF surgical planning, which can substantially reduce the planning time by removing the need for repetitive guessing bony movement. 2) We develop a deep learning model as the bony planner, which can estimate the underlying bony movement needed for changing a facial appearance into a targeted one. 3) The developed FS module can qualitatively assess the effect of surgical plans on facial appearance, for virtual validation."
2,Method,Figure 
2.1,Bony Planner (BP),"Data Preprocessing: The bony plan is created by the BP network using preoperative facial F pre , bony surface B pre , and desired facial surface F des . The goal of the BP network is to convert the desired facial change from F pre to F des into rigid bony movements, denoted as T S for each bony segment S, as required for surgical planning. However, it is very challenging to directly estimate the rigid bone transformation from the facial difference. Therefore, we first estimate the non-rigid bony movement vector field and then convert that into the rigid transformations for each bone segment. Figure "
,Non-rigid Bony Movement Vector Estimation:,"We adopt the Attentive Correspondence assisted Movement Transformation network (ACMT-Net)  where N Fpre denotes the number of facial points P Fpre . On the other hand, desired facial movement V F is computed by subtracting P F des and P Fpre . V F is then concatenated with P Fpre and fed into a 1D convolution layer to encode the movement information. Then the movement feature of each bony point is estimtated by the normalized summary of facial features using R. Finally, the transformed bony movement features are decoded into movement vectors after being fed into one 1D convolution layer and normalization: Rigid Bony Movement Regression: Upon obtaining the estimated pointwise bony movements, they are added to the corresponding bony points, resulting in a non-rigidly transformed bony point set denoted as P B pdt . The resulting points are grouped based on their respective bony segments, with point sets P Spre and P S pdt representing each bony segment S before and after movement, respectively. To fit the movement between P Spre and P S pdt , we estimate the rigid transformations [R S , T S ] by minimizing the mean square error as follows: where i and N represent the i-th point and the total number of points in P Spre , respectively. First, we define the centroids of P Spre and P S pdt to be P Spre and P S pdt . The cross-covariance matrix H can be computed as Then we can use singular value decomposition (SVD) to decompose then the alignment minimizing E(R S , T S ) can be solved by Finally, the rigid transformation matrices are applied to their corresponding bone segments for virtual planning."
2.2,Facial Simulator (FS),"While the bony planner can estimate bony plans and we can compare them with ground truth. However, the complex relationship between bone and face makes it unknown whether adopting the bony plan will result in the desired facial outcome or not. To evaluate the effectiveness of the BP network in simulating facial soft tissue, an FS is developed to simulate the facial outcome following the estimated bony plans. For facial simulation, we employ the ACMT-Net, which takes P Fpre , P Bpre , and P B pdt as input and predicts the point-wise facial movement vector V F . The movement vector of all vertices in the facial surface is estimated by interpolating V F . The simulated facial surface F pdt is then reconstructed by adding the predicted movement vectors to the vertices of F pre ."
2.3,Self-verified Virtual Planning,"To generate a set of potential bony plans, we randomly perturbed the surfaces by flipping and translating them up to 10mm in three directions during inference. We repeated this process 10 times in our work. After estimation, the bony surfaces were re-localized to their original position prior to perturbation. Sequentially, the FS module generated a simulated facial appearance for each bony plan estimated from the BP module, serving two purposes. Firstly, it verified the feasibility of the bony plan through facial appearance. Secondly, it allowed us to evaluate the facial outcomes of different bony plans. The simulated facial surfaces were compared with the desired facial surface, and the final plan was selected based on the similarity of the resulting facial outcome. This process verified the efficacy of the selected plan for achieving the desired facial outcome. 3 Experiments and Results"
3.1,Dataset,"We employed a five-fold cross-validation technique to evaluate the performance of the proposed network using 34 sets of patient CT data. We partitioned the data into five groups with {7, 7, 7, 7, 6} sets of data, respectively. During each round of validation, four of these groups (folds) were used for training and the remaining group was used for testing. The CT scans were randomly selected from our digital archive of patients who had undergone double-jaw orthognathic surgery. To obtain the necessary data, we employed a semi-automatic method to segment the facial and bony surface from the CT images "
3.2,Implementation and Evaluation Methods,"To compare our approach, we implemented the state-of-the-art bone-driven approach, i.e., deformation network (DefNet) "
3.3,Results,"The results of the quantitative evaluation are shown in Table  The evaluation of the bony surface showed that the proposed method successfully predicted bony surfaces that were similar to the real postoperative bones. To further assess the performance of the method, a facial simulator was used to qualitatively verify the simulated faces from the surgical plans. Figure "
4,Discussions and Conclusions,"As a result of our approach that considers both the bony and soft-tissue components of the deformity, the accuracy of the estimated bony plan, especially on the DI segment, has significantly improved. Nonetheless, the non-linear relationship between the facial and bony surfaces cannot be adequately learned using only facial and bony surface data, and additional information such as tissue properties can also affect the facial outcome. To account for uncertainty, we introduce random perturbations to generate different plans. In the future, we plan to incorporate additional uncertainty into the bony planner by using stronger perturbations or other strategies such as dropout  In conclusion, we have developed a soft-tissue driven framework to directly predict the bony plans that achieve a desired facial outcome. Specifically, a bony planner and a facial simulator have been proposed for generating bony plans and verifying their effects on facial appearance. Evaluation results on a clinical dataset have shown that our method significantly outperforms the traditional bone-driven approach. By adopting this approach, we can create a virtual surgical plan that can be assessed and adjusted before the actual surgery, reducing the likelihood of complications and enhancing surgical outcomes. The proposed soft-tissue driven framework can potentially improve the accuracy and efficiency of CMF surgery planning by automating the process and incorporating a facial simulator to account for the complex non-linear relationship between bony structure and facial soft-tissue."
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Table 1 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 18.
1,Introduction,"In some ways, surgical data is like the expanding universe: 95% of it is dark and unobservable  Following recent work that enables sim-to-real transfer in the X-ray domain "
2,Related Work,"SPR from video sources is a popular topic, and has benefited from the advent of transformer architectures for analyzing image sequences. The use of convolutional layers as an image encoder has proven effective for recognizing surgical phases in endoscopic video "
3,Method,"The Pelphix pipeline consists of stochastic simulation of X-ray image sequences, based on a large database of annotated CT images, and a transformer architecture for phase recognition with additional task-aware supervision. A statistical shape model is used to propagate landmark and corridor annotations over 337 CTs, as shown in Fig. "
3.1,Image Sequence Simulation for Percutaneous Fixation,"Unlike sequences collected from real surgery  Evaluate View. Given a current view (p, r) and desired view (p * , r * ), we first evaluate whether the current view is acceptable and, if it is not, make a random adjustment. View evaluation considers the principle ray alignment and whether the viewing point is reasonably centered in the image, computing, where the angular tolerance θ t ∈ [3 • , 10 • ] depends on the desired view, ranging from teardrop views (low) to lateral (high tolerance). Sample View. If Eq. 1 is not satisfied, then we sample a new view (p, r) uniformly within a uniform window that shrinks every iteration by the adjustment factor, according to where U • (c, r) is the uniform distribution in the sphere with center c and radius r, and U (r, θ) is the uniform distribution on the solid angle centered on r with colatitude angle θ. This formula emulates observed fluoro-hunting by converging on the desired view until a point, when further adjustments are within the same random window  Evaluate Wire Placement. During wire positioning, we evaluate the current wire position and make adjustments from the current view, iterating until evaluation succeeds. Given the current wire tip x, direction v, and projection matrix P, the wire placement is considered ""aligned"" if it appears to be aligned with the projected target corridor in the image, modeled as a cylinder. In addition, we include a small likelihood of a false positive evaluation, which diminishes as the wire is inserted. Sample Wire Placement. If the wire evaluation determines the current placement is unsuitable, then a new wire placement is sampled. For the down-thebarrel views, this is done similarly to Eq. 2, by bringing the wire closer to the corridor in 3D. For orthogonal views, repositioning consists of a small random adjustment to x, a rotation about the principle ray (the in-plane component), and a minor perturbation orthogonal to the ray (out-of-plane). This strategy emulates real repositioning by only adjusting the degree of freedom visible in the image, i.e. the projection onto the image plane: and θ * is the angle between the wire and the target corridor in the image plane. If the algorithm returns ""Good,"" the sequence either selects a new view to acquire (and stays in the position-wire activity) or proceeds to insert-wire or insert-screw, according to random transitions. In our experiments, we used 337 CT images: 10 for validation, and 327 for generating the training set. (Training images were collected continuously during development, after setting aside a validation set.) A DRR was acquired at every decision point in the simulation, with a maximum of 1000 images per sequence, and stored along with segmentations and anatomical landmarks. We modeled a K-wire with 2 mm diameter and orthopedic screws with lengths from 30 to 130 mm and a 16 mm thread, with up to eight instances of each in a given sequence. Using a customized version of DeepDRR "
3.2,Transformer Architecture for X-ray-based SPR,Figure 
4,Evaluation,"Simulation. We report the results of our approach first on simulated image sequences, generated from the withheld set of CT images, which serves as an upper bound on real X-ray performance. In this context our approach achieves an accuracy of 99.7%, 98.2%, 99.8%, and 99.0% with respect to the corridor, activity, view, and frame level, respectively. Moreover, we achieve an average DICE score of 0.72 and landmark detection error of 1.01 ± 0.153 pixels in simulation, indicating that these features provide a meaningful signal. That the model generalizes so well to the validation set reflects the fact that these sequences are sampled using the same Markov-based simulation as the training data. Cadaver Study. We evaluate our approach on cadaveric image sequences with five screw insertions. An attending orthopedic surgeon performed percutaneous fixation on a lower torso specimen, taking the antegrade approach for the left and right pubic ramus corridors, followed by the left and right teardrop and S1 screws. An investigator acted as the radiological technician, positioning a mobile C-arm according to the surgeon's direction. A total of 257 images were acquired during these fixations, with phase labels based on the surgeon's narration. Our results, shown in Fig. "
5,Discussion and Conclusion,"As our results show, Pelphix is a potentially viable approach to robust SPR based on X-ray images. We showed that stochastic simulation of percutaneous fracture fixation, despite having no access to real image sequences, is a sufficiently realistic data source to enable sim-to-real transfer. While we expect adjustments to the simulation approach will close the gap even further, truly performative SPR algorithms for X-ray may rely on Pelphix-style simulation for pretraining, before fine-tuning on real image sequences to account for human-like behavior. Extending this approach to other procedures in orthopedic surgery, angiography, and interventional radiology will require task-specific simulation capable of modeling possibly more complex tool-tissue interactions and human-in-the-loop workflows. Nevertheless, Pelphix provides a viable first route toward X-ray-based surgical phase recognition, which we hope will motivate routine collection and interpretation of these data, in order to enable advances in surgical data science that ultimately improve the standard of care for patients."
,Fig. 1 .,
,,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,,
,Fig. 5 .,
1,Introduction,"Hip osteoarthritis, with the top 10% occurrence in all diseases, brought a high demand for total hip arthroplasty (THA) in the past few decades  Intraoperatively, leg length and femoral offset can be determined manually using a calliper between two reference points  In contrast, the Electromagnetic (EM) sensor based navigation system can provide fast and accurate tracking without line-of-sight constraints  In the commonly used intraoperative leg length equalisation and offset recovery techniques, both traditional and computer-assisted methods require the femur to be held and stored at the preoperative neutral reference position (0 • flexion, 0 • rotation, and 0 • abduction) prior to hip dislocation, and measure the changes in leg length and femoral offset as the femur is returned to the neutral reference position  In this work, we propose a robust and accurate intraoperative limb length measurement method for THA based on EM sensing. Using the idea that the femoral movement can be mathematically modelled as a vector rotating around a fixed rotation centre, we develop a closed-form optimisation solution that uses a set of sampled poses from EM readings to calculate the intraoperative limb length change. The experiment results demonstrate that the proposed method can be more accurate. In summary, the key advantages of our method include: (i) the optimisation with a closed-form solution is an active compensation  (iii) no need to return the leg back to the neutral reference position again after replacing the damaged hip joint with artificial implants, which effectively avoids measurement errors due to inaccurate abduction/adduction repositioning; (iv) the proposed method does not require the direct line-of-sight, and can be easily integrated clinically, without interrupting the workflow of THA. "
2,Methodology,
2.1,Problem Formulation,"Standard THA Routine. During a standard THA, the surgery is often performed in the lateral position. Two reference points are marked on the pelvis and femur, respectively. The reference can be iliac fixation pins, sensors, or optical trackers. The surgeon then finds the preoperative neutral reference position by experience and records the relative position between the two reference points prior to the femoral head resection. The following routines include damaged femoral head removal, femoral canal broaching, acetabular preparation, and component selection and alignment. The component alignment requires the surgeon to return the femur to the neutral reference position and measure the change in leg length and offset. Our Setup. In our proposed method (Fig.  which are respectively the rotation matrices and translation vectors of frames {P } and {F } w.r.t. the frame of EM tracking board denoted by {B} (Fig.  Problem Statement. Suppose P X pre F 0 = { P R pre F 0 , P t pre F 0 } is the recorded pose of {F } w.r.t. {P } at the preoperative neutral reference position before femoral head resection, and N sampling postoperative femoral poses in frame {P }, denoted by are collected through small motion around the neutral position after femoral head replacement. Since the relative rotations of frame {F } in {P } should be the same at the neutral reference position before and after the femoral head replacement, mathematically, the problem considered in this paper is, given sampling postoperative femoral poses P X F i (i ∈ {1, • • • , N}), accurately estimate the current translation vector P t post F 0 when the relative rotation is P R pre F 0 , and then use it to calculate the change in leg length and offset."
2.2,Limb Length Measurement Without Femoral Repositioning,"Since the joint between the acetabular component and the metal femoral head is a perfect sphere, the femoral movement in the frame {P } can be mathematically modelled as a vector b rotating around a centre c (Fig.  Therefore, the relation among the vectors t, b, and c (refer to Fig.  where P b and F b are the rotating vector b in {P } and {F }, respectively, and P c is the rotation centre c in {P }. The relation (  Then, the postoperative translation vector P t post F 0 can be calculated by the relative rotation P R pre F 0 . In contrast to pivot calibration  Full Least Squares Solution. Through the iterative Gauss-Newton (GN) method, the solution for F b and P c can be obtained by solving the following full nonlinear least squares (Full LS) optimisation problem, arg min where r( P R F i ) and r( P RFi ) are the Euler angles of rotation measurement P R F i and the corresponding rotation variable P RFi , respectively. Ω ti and Ω Ri are the covariance matrices of EM measurement noises w.r.t. translation and rotation. Closed-Form Solution. Since the EM measurements of rotations are accurate enough  which has an easier and more efficient closed-form solution The comparison in Sect. 3 will show that the closed-form solution (  Limb Length Change Measurement. After F b * and P c * are obtained, the postoperative translation vector P t post F 0 at neutral reference position (where the relative rotation is P R pre F 0 ) can be calculated by (1) The change of relative translation vector at the neutral reference position is Δ P t F 0 = P t post F 0 -P t pre F 0 . Further, we denote P L as the projection of Δ P t F 0 onto the sagittal plane. Then, the changes in leg length and offset are computed by the norms P L and Δ P t F 0 -P L , respectively. The covariance of P t post F 0 in (  where I is the identity matrix, Ω ti is the covariance matrices of EM measurement noises w.r.t. translation. It can be proved that (6) tends to be zero as increasing samples if data are all sampled around the neutral position ( P R F i ≈ P R pre F 0 ), although the uncertainty of F b * and P c * in ( "
3,Experiments,
3.1,Simulation and Robustness Assessment,"To compare the closed-form solution (4) and the solution to Full LS (2), five different levels of zero-mean Gaussian noises are added to the rotation angles and translations of sampling femoral poses P X F i (i ∈ {1, • • • , 100}) from EM readings (first row in Table "
3.2,Phantom Experiments,The phantom experiments (Fig. 
,Number of Samples and Comparison with Full LS.,The first setup of phantom experiments (Fig.  As shown in Fig.  Comparison with Other Methods. We compared our proposed method (Closed-form) with three other different measurement approaches in the second phantom experiment setup (Fig. 
3.3,Cadaver Experiments,The proposed method was also tested in cadaver experiments (Fig. 
4,Conclusion,"This paper presents an efficient closed-form solution based on EM sensing to robustly and accurately calculate the intraoperative change in leg length and offset. Simulations, phantom experiments, and cadaver tests demonstrate the efficiency and accuracy of our proposed algorithm compared to the conventional manual gauge method and standard optical tracking based method, showing the potential value in clinical practice. The reasons why the proposed solution can significantly improve the accuracy are: (i) it uses a set of sampled poses from EM readings to optimise the intraoperative limb length change instead of only using one sensor reading; (ii) there is no requirement of repositioning the femur to the neutral position before the measurements. The computational time of our method is only around 0.3 ms mainly due to the closed-form solution. Some studies assessed that metals in the surgical environment might affect the accuracy of EM tracking "
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Fig. 5 .,
,Table 1 .,
,Table 2 .,
,Table 3 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 35.
1,Introduction,"Cancer remains a significant public health challenge worldwide, with a new diagnosis occurring every two minutes in the UK (Cancer Research UK  It is crucial to accurately determine the sensing area, with positive signal potentially indicating cancer or affected lymph nodes. Geometrically, the sensing area is defined as the intersection point between the gamma probe axis and the tissue surface in 3D space, but projected onto the 2D laparoscopic image. However, it is not trivial to determine this using traditional methods due to poor textural definition of tissues and lack of per-pixel ground truth depth data. Similarly, it is also challenging to acquire the probe pose during the surgery. Problem Redefinition. In this study, in order to provide sensing area visualization ground truth, we modified a non-functional 'SENSEI' probe by adding a miniaturized laser module to clearly optically indicate the sensing area on the laparoscopic images -i.e. the 'probe axis-surface intersection'. Our system consists of four main components: a customized stereo laparoscope system for capturing stereo images, a rotation stage for automatic phantom movement, a shutter for illumination control, and a DAQ-controlled switchable laser module (see Fig. "
2,Related Work,"Laparoscopic images play an important role in computer-assisted surgery and have been used in several problems such as object detection  Laparoscopic segmentation is another important task in computer-assisted surgery as it allows for accurate and efficient identification of instrument position, anatomical structures, and pathological tissue. For instance, a unified framework for depth estimation and surgical tool segmentation in laparoscopic images was proposed in  Although the intermediate depth information was not our final aim and can be bypassed, the 3D surface information was necessary in the intersection point inference. ResNet  If the problem of inferring the intersection point is treated as a geometric problem, both data collection and intra-operative registration would be difficult, which inspired us to approach this problem differently. In practice, we utilize the laser module to collect the ground truth of the intersection points when the laser is on. We note that the standard illumination image from the laparoscopic probe is also captured with the same setup when the laser module is on. Therefore, we can establish a dataset with an image pair (RGB image and laser image) that shares the same intersection point ground truth with the laser image (see Fig. "
3,Dataset,"To validate our proposed solution for the newly formulated problem, we acquired and publicly released two new datasets. In this section, we introduce the hardware and software design that was used to achieve our final goal, while Fig.  All data acquisition and devices were controlled by Python and LABVIEW programs, and complete data sets of the above images were collected on visually realistic phantoms for multiple probe and laparoscope positions. This provided 10 tissue surface profiles for a specific camera-probe pose, repeated for 120 different camera-probe poses, mimicking how the probe may be used in practice. Therefore, our first newly acquired dataset, named Jerry, contains 1200 sets of images. Since it is important to report errors in 3D and in millimeters, we recorded another dataset similar to Jerry but also including ground truth depth map for all frames by using structured-lighting system  These datasets have multiple uses such as: -Intersection point detection: detecting intersection points is an important problem that can bring accurate surgical cancer visualization. We believe this is an under-investigated problem in surgical vision. -Depth estimation: corresponding ground truth will be released. -Tool segmentation: corresponding ground truth will be released."
4,Probe Axis-Surface Intersection Detection,
4.1,Overview,"The problem of detecting the intersection point is trivial when the laser is on and can be solved by training a deep segmentation network. However, segmentation requires images with a laser spot as input, while the real gamma probe produces no visible mark and therefore this approach produces inferior results. An alternative approach to detect the intersection point is to reconstruct the 3D tissue surface and estimate the pose of the probe in real time. A tracking and pose estimation method for the gamma probe  In this work, we propose a simple, yet effective regression approach to address this problem. Our approach relies solely on the 2D information and works well without the need for the laser module after training. Furthermore, this simple methodology facilitated an average inference time of 50 frames per second, enabling real-time sensing area map generation for intraoperative surgery. "
4.2,Intersection Detection as Segmentation,We utilized different deep segmentation networks as a first attempt to address our problem 
4.3,Intersection Detection as Regression,"Problem Formulation. Formally, given a pair of stereo images I l , I r , n points {P l 1 , P l 2 , ..., P l n } were sampled along the principal axis of the probe, P l i ∈ R 2 from the left image. The same process was repeated for the right image. The goal was to predict the intersection point P intersect on the surface of the tissue. During the training, the ground truth intersection point position was provided by the laser source, while during testing the intersection was estimated solely based on visual information without laser guidance (see Fig.  Network Architecture. Unlike the segmentation approach, the intersection point was directly predicted using a regression network. The images fed to the network were 'laser off' stereo RGB, but crucially, the intersection point for these images was known a priori from the paired 'laser on' images. The raw image resolution was 4896×3680 but these were binned to 896×896. Principal Component Analysis (PCA) "
4.4,Implementation,"Evaluation Metrics. To evaluate sensing area location errors, Euclidean distance was adopted to measure the error between the predicted intersection points and the ground truth laser points. We reported the mean absolute error, the standard derivation, and the median in pixel units. Implementation Details. The networks were implemented in PyTorch "
5,Results,"Quantitative results on the released datasets are shown in Table  For the 3D error, the ResNet backbone still gave generally better performance than the ViT backbone while under the ResNet backbone, LSTM and MLP gave competitive results and they are all in sub-milimeter level. We note that the 3D error subjected to the quality of the acquired ground truth depth maps, which had limited resolution and non-uniformly distributed valid data due to hardware constraints. Hence, we used the median depth value of a square area of 5 pixels around the points where depth value was not available. Figure "
6,Conclusion,"In this work, a new framework for using a laparoscopic drop-in gamma detector in manual or robotic-assisted minimally invasive cancer surgery was presented, where a laser module mock probe was utilized to provide training guidance and the problem of detecting the probe axis-tissue intersection point was transformed to laser point position inference. Both the hardware and software design of the proposed solution were illustrated and two newly acquired datasets were publicly released. Extensive experiments were conducted on various backbones and the best results were achieved using a simple network design, enabling real time inference of the sensing area. We believe that our problem reformulation and dataset release, together with the initial experimental results, will establish a new benchmark for the surgical vision community."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .Fig. 4 .,
,Fig. 5 .,
,Table 1 .,
,Table 2 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 25.
1,Introduction,"Augmentation of intra-operative X-ray images using the pre-operative data (e.g., treatment plan) has the potential to reduce procedure time and improve patient outcomes in minimally invasive procedures. However, surgeons must rely on their clinical knowledge to perform a mental mapping between pre-and intraoperative information, since the pre-and intra-operative images are based on different coordinate systems. To utilize pre-operative data efficiently, accurate pose estimation of the X-ray source relative to pre-operative images (or called registration between pre-and intra-operative data) is necessary and beneficial to relieve surgeon's mental load and improve patient outcomes. Although 2D/3D registration methods for medical images have been widely researched and systematically reviewed  In this paper, we propose a purely self-supervised and patient-specific end-toend framework for fully automatic registration of single-view X-ray to preoperative CT. Our main contributions are as follows:  (3) The proposed method has been successfully evaluated on X-rays, achieving an average run-time of around 2.5 s, which meets the requirements for clinical applications. "
2,Method,An overview of the proposed framework is illustrated in Fig. 
2.1,Pose Regression Model,"Network Architecture. A regression neural network architecture is developed to estimate the six degrees of freedom (DOF) pose from an input X-ray image, which consists of a regularized autoencoder, a multi-head self-attention block (MHSAB), a learnable 2D position embedding, and a multilayer perceptron (MLP), as shown in Fig.  Regularized Autoencoder. The autoencoder consists of a carefully selected backbone and a decoder. The first six layers of EfficientNet-b0  Here, Y is the output of the decoder, I is the input image, G I is the normalized gradient of the input image, and N is the number of pixels in the input image. E() is the expectation operator, and σ is standard deviation operator. Multi-head Self-attention Block and Positional Embedding. First, inspired by  where n is the number of heads, C is number of channels, and h, w are the height and width of features. Using the computed attention weights, the output of MHSAB f a is computed as, Second, in order to make the proposed method more sensitive to spatial transformation, 2D learnable positional embedding is employed to explicitly incorporate the object's position information. Incremental Learning Strategy. Based on Incremental Learning strategy, the network is trained for 100 epochs with a batch size of 32 using the Adam optimizer (learning rate is 0.002, decay of 0.5 per 10 epochs). After training for 40 epochs, the training dataset will be automatically updated if the loss computed on the validation set does not change frequently (i.e., less than 10 percent of the maximum of loss) within 20 epochs, which allows the network to observe a wider range of poses while avoiding over-fitting. Final loss function is a weighted sum of the structure aware loss L s for autoencoder and L2 Loss between the predicted pose and the ground truth, which is formulated as, where α is set as 5, I is the input DRR image, p is the ground truth of pose, P (I) is predicted pose, and D(I) is the output of autoencoder. Pre-processing and Data Augmentation. In order to improve the contrast of the digitally reconstructed radiographs (DRRs) and reduce the gap to real X-rays, the DRR is first normalized by Z-score and then normalized using the sigmoid function, which maps the image into the interval [0, 1] with a mean of 0.5. Then contrast-limited adaptive histogram equalization (CLAHE) is employed to enhance the contrast. For data augmentation, random brightness adjustment, random adding offset, adding Gaussian noise, and adding Poisson noise are adopted."
2.2,Refinement Model,A novel refinement model is proposed to further refine the pose predicted by regression network as shown in Fig.  3 Experiments
3.1,Dataset,"Our method is evaluated on six DRR datasets and one X-ray dataset. The six DRR datasets are generated from five common preoperative CTs and one specific CT with previous surgical implants, while the X-ray dataset is obtained from a Pelvis phantom containing a metal bead landmark inside. The X-ray dataset includes a CBCT volume and ten X-ray images with varying poses. For each CT or CBCT, 12800 DRR images with a reduced resolution of 256 2 pixels are generated using the DeepDRR method, which are divided into training (50%), validation (25%), and test (25%) sets. The DRR generation system is configured based on the geometry of a mobile C-arm, with a 432 mm detector, 0. "
3.2,Experimental Design and Evaluation Metrics,"To better understand our work, we conduct a series of experiments, for example, 1) A typical pose regression model consists of a CNN backbone and an MLP. To find an efficient backbone for this task, EfficientNet, ResNet  To validate the performance of our method on DRR datasets, we employed five measurements including 2D mean projection distance (mPD), 3D mean target registration error (mTRE) "
4,Results,
4.1,The Choice of Backbone and Ablation Study,The performance of three models with different backbones were evaluated on the first DRR dataset as reported in Table 
4.2,Experimental Results on DRR Datasets and X-Ray Dataset,The experimental results of our proposed method on six DRR datasets are shown in Table 
5,Conclusion and Discussion,"In this paper, we present a patient-specific and self-supervised end-to-end approach for automatic X-ray/CT rigid registration. Our method effectively addresses the primary limitations of existing methods, such as requirement of manual annotation, dependency on conventional derivative-free optimization, and patient-specific concerns. When field of view of CT is not smaller than that of X-ray, which is often satisfied in clinical routines, our proposed method would perform very well without any additional post-process. The quantitative and qualitative evaluation results of our proposed method illustrates its superiority and its ability to generalize to X-rays even when trained solely on DRRs. For our experiments, the validation on X-ray of phantom cannot fully represent the performance on X-ray of real patients, but it shows that the proposed method has high potential. Meanwhile, domain randomization could reduce the gap between DRR and real X-ray images, which would allow methods validated on phantoms to perform better also on real X-ray. For the runtime aspect, our patient-specific regression network can complete the training phase within one hour using an NVIDIA GPU (Quadro RTX A6000), which meets the requirement of clinical application during pre-operative planning phase. Meanwhile, the proposed network achieves an average inference time of 6ms per image with a size of 256 2 , when considering the run-time of the proposed refinement model, the total cost is approximately 2.5 s, which also fully satisfies the requirement for clinical application during intra-operative phase."
,Fig. 1 .,
,,
,Fig. 2 .,
,Table 1 .,
,Table 2 .,
,Table 3 .,
1,Introduction,"If a tooth is missing, decayed, or fractured, its treatment may require a dental crown. Each crown must be customized to the individual patient in a process, as depicted in Fig.  Designing natural grooves and ensuring proper contact points with the opposing jaw present significant challenges, often requiring technicians to rely on trial and error. As such, an automated approach capable of accelerating this process and generating crowns with comparable morphology and quality to that of a human expert would be a groundbreaking advancement for the dental industry. A limited number of studies have focused on how to automate dental crown designs. In  Although all aforementioned methods are potentially applicable to the task of dental crown design, most of them fail to generate noise-free point clouds, which is critical for surface reconstruction. One way to alleviate this problem is to directly generate a crown mesh. In  In this paper, we introduce Dental Mesh Completion (DMC), a novel endto-end network for directly generating dental crowns without using generic templates. The network employs a transformer-based architecture with self-attention to predict features from a 3D scan of dental preparation and surrounding teeth. These features deform a 2D fixed grid into a 3D point cloud, and normals are computed using a simple MLP. A differentiable point-to-mesh module reconstructs the 3D surface. The process is supervised using an indicator grid function and Chamfer loss from the target crown mesh and point cloud. Extensive experiments validate the effectiveness of our approach, showing superior performance compared to existing methods as measured by the CD metric. In summary, our main contributions include proposing the first end-to-end network capable of generating crown meshes for all tooth positions, employing a non-templatebased method for mesh deformation (unlike previous works), and showcasing the advantages of using a differentiable point-to-mesh component to achieve highquality surface meshes."
2,Related Work,"In the field of 3D computer vision, completing missing regions of point clouds or meshes is a crucial task for many applications. Various methods have been proposed to tackle this problem. Since the introduction of PointNet  Mesh completion methods are usually useful when there are small missing regions or large occlusions in the original mesh data. Common approaches based on geometric priors, self-similarity, or patch encoding can be used to fill small missing regions, as demonstrated in previous studies  We combine the advantages of point cloud completion techniques with a differentiable surface reconstruction method to generate a dental mesh. Moreover, we used the approach in "
3,Method,
3.1,Network Architecture,"Our method is an end-to-end supervised framework to generate a crown mesh conditioned on a point cloud context. The overview of our network is illustrated in Fig.  Mesh Completion Layer. In this stage, to directly reconstruct the mesh from the crown points, we use a differentiable Poisson surface reconstruction (DPSR) method introduced by  During training, we obtain the estimated indicator grid from the predicted point cloud by using the differentiable Poisson solver. We similarly acquire the ground truth indicator grid on a dense point cloud sampled from the ground truth mesh, together with the corresponding normals. The entire pipeline is differentiable, which enables the updating of various elements such as point offsets, oriented normals, and network parameters during the training process. At inference time, we leverage our trained model to predict normals and offsets using Differentiable Poisson Surface Reconstruction (DPSR)  Loss Function. We use the mean Chamfer Distance (CD)  (1) 4 Experimental Results"
4.1,Dataset and Preprocessing,"Our dataset consisted of 388 training, 97 validation, and 71 test cases, which included teeth in various positions in the jaw such as molars, canines, and incisors. The first step in the preprocessing was to generate a context from a given 3D scan. To determine the context for a specific prepared tooth, we employed a pre-trained semantic segmentation model "
4.2,Implementation Details,We adapted the architecture of 
4.3,Evaluation and Metrics,"To evaluate the performance of our network and compare it with point cloudbased approaches, we used the Chamfer distance to measure the dissimilarity between the predicted and ground truth points. We employed two versions of CD: CD L1 uses the L 1 -norm, while CD L2 uses the L 2 -norm to calculate the distance between two sets of points. Additionally, we used the F-score  We conducted experiments to compare our approach with two distinct approaches from the literature, as shown in Table "
4.4,Ablation Study,We conducted an ablation study to evaluate the components of our architecture. We started with the baseline PoinTr 
5,Conclusion,"Existing deep learning-based dental crown design solutions require additional steps to reconstruct a surface mesh from the generated point cloud. In this study, we propose a new end-to-end approach that directly generates high-quality crown meshes for all tooth positions. By utilizing transformers and a differentiable Poisson surface reconstruction solver, we effectively reason about the crown points and convert them into mesh surfaces using Marching Cubes. Our experimental results demonstrate that our approach produces accurately fitting crown meshes with superior performance. In the future, incorporating statistical features into our deep learning method for chewing functionality, such as surface contacts with adjacent and opposing teeth, could be an interesting avenue to explore."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Fig. 5 .,
,Table 1 .,
,Table 2 .,
1,Introduction,"Thoracoscopy-assisted mitral valve replacement (MVR) has become routine for the treatment of mitral valve regurgitation  Traditionally, SKA has been reliant on manual observation by experienced surgeons either in the operating room or via recorded videos, as described by Reznick in his work on teaching  To address the above challenges, we collect a new dataset for SKA, which is the first-ever long-form thoracoscopy-assisted MVR video dataset. Our dataset offers longer video duration and more surgical events with corresponding labels in comparison to the currently available public datasets such as JIGSAWS  In summary, our contributions are three-fold: "
2,Method,Figure 
2.1,Basic Regression Module,"The basic regression module aims to regress a surgical skill score, i.e., Y i , for a raw surgical video input V i ∈ R Ti×H×W , where T i is the duration, H and W are the height and width of each frame. As shown in Fig.  Finally, a regression block consisting of several convolutional, max-pooling layers and a fully connected layer is applied to map F G i to the skill score Ŷi . Then, the loss function is to minimize the differences between predicted Ŷi and the ground-truth Y i as follows: where N is the number of videos."
2.2,Local-Global Difference Module,"Surgical Event-Aware Module for Local Information. Unlike prior datasets used for skill assessment, our MVR video dataset is much longer, ranging from 30 min to 1 h, and consists of multiple skill-related events such as thread twining, haemorrhage, and suture repairing; see Fig.  where p t ∈ R 4 consists of 4 values (including the background), which indicates the probability of event category, d s t > 0 and d e t > 0 denote the distance between time t to start and end time of events. Note that if p t equals to zero, d s t > 0 and d e t > 0 are not defined. Then, following  where N + is the number of positive frames, L cls is a focal loss  , where A i ∈ R Ti×1 , a t = max p t is the confidence for each time t and T i is the duration for the V i . Then, the local event-level feature is obtained by the multiplication of the confidence values and the global video feature, i.e., F L i = A i • F S i , where F L i ∈ R Ti×Ds and • is the element-wise multiplication. Local-Global Fusion. We introduce the local-global fusion module to aggregate the local (i.e.event-level) and global (long-form video) semantics. Formally, we can define the local-global fusion as , where F i ∈ R Ti×(Dt+Ds) . This module can be implemented by different types and we will conduct an ablation study to analyze the effect of this module in Table "
,Difference Regression Block.,"Most surgeries of the same type are performed in similar scenes, leading to subtle differences among surgical videos. For example, in MVR, the surgeon first stitches two lines on one side using a needle, and then passes one of the lines through to the other side, connecting it to the extracorporeal circulation tube. Although these procedures are performed in a similar way, the imperceptible discrepancies are very important for accurately assessing surgical skills. Hence, we first leverage the relation block to capture the inter-video semantics. We use the features of the pairwise videos, i.e., F i and F j , for clarity. Since attention  where linear layers, √ D controls the effect of growing magnitude of dot-product with larger D  After that, we use the difference regression block to map F i-j to the difference scores Δ Ŷ . Then, we minimize the error as follows: where ΔY is the ground-truth of the difference scores between the pair videos, which can be computed by |Y i -Y j |. By optimizing L dif f , the model would be able to distinguish differences between videos for precise SKA. Finally, the overall loss function of our proposed method is as follows: where λ dif f is the hyper-parameter to control the weight between two loss functions (set to 1 empirically)."
3,Experiments,Datasets. We collect the data from our collaborating hospitals. The data collection process follows the same protocol in a well-established study  Implementation Details. Our model is implemented on an NVIDIA GeForce RTX 3090 GPU. We use a pre-trained inception-v3 
3.1,Comparison with the State-of-the-Art Methods,We compare our method with existing state-of-the-art methods in action quality assessment (AQA) 
3.2,Ablation Study,"Effectiveness of Proposed Modules.  Bidirectional (Fi-j) 1.83 0.54 Fig.  methods can achieve comparable performance, indicating that different fusion methods can effectively aggregate local and global information. In this paper, we select concatenation as our default fusion method. Effect of the Attention in the Difference Block. In Sect. 2.2, we implement the difference block by the attention, shown in Eq. 4. Here, we conduct the ablation study on the effect of different attentions in Table  Qualitative Results. Figure "
4,Conclusion,"This paper introduces a new surgical video dataset for evaluating thoracoscopyassisted surgical skills. This dataset constitutes the first-ever collection of long surgical videos used for skill assessment from real operating rooms. To address the challenges posed by long-range videos and multiple complex surgical actions in videos, we propose a novel SEDSkill method that incorporates a local-global difference framework. In contrast to current methods that solely rely on intravideo information, our proposed framework leverages local and global difference learning to enhance the model's ability to use inter-video relations for accurate SKA in the MVR scenario."
,Fig. 1 .,
,Fig. 2 .,
,Table 1 .,
,Table 2 .,
,,
,Table 3 .,
,Table 4 .,
1,Introduction,"The design of 3D-printed patient-specific implants, commonly used in cranioplasty and maxillofacial surgery, is a challenging and time-consuming task that is usually performed manually. To speed up the design process and enable point-ofcare implant generation, approaches for automatically deriving suitable implant designs from medical images are needed. This paper presents a novel approach based on a Denoising Diffusion Probabilistic Model for 3D point clouds that reconstructs complete anatomical structures S c from segmented CT images of subjects showing bone defects S d . An overview of the proposed method is shown in Fig.  -We employ 3D point cloud diffusion models for an automatic patient-specific implant generation task. The stochastic sampling process of diffusion models allows for the generation of multiple anatomically reasonable implant designs per subject, from which physicians can choose the most suitable one. -We evaluate our method on the SkullBreak and SkullFix datasets, generating high-quality implants and achieving competitive evaluation scores. Related Work. Previous work on automatic implant generation methods mainly derived from the AutoImplant challenges  For retrieving a dense voxel representation of a point cloud, many approaches rely on a combination of surface meshing "
2,Methods,"As presented in Fig.  Point Cloud Generation. Since the proposed method for shape reconstruction works in the point cloud space, we first need to derive a point cloud c 0 ∈ R N ×3 from S d . We therefore create a surface mesh of S d using Marching Cubes  Then we sample N points from this surface mesh using Poisson Disk Sampling  Diffusion Model for Shape Reconstruction. Reconstructing the shape of an anatomical structure can be seen as a conditional generation process. We train a diffusion model θ to reconstruct the point cloud x 0 = (x 0 , c 0 ) that describes the complete anatomical structure S c . The generation process is conditioned on the points c 0 belonging to the known defective anatomical structure S d . An overview is given in Fig.  This conditional forward diffusion process can be modeled as a Markov chain with a defined number of timesteps T and transfers x0 into a noise distribution: Each transition is modeled as a parameterized Gaussian and follows a predefined variance schedule β 1 , ..., β T that controls the diffusion rate of the process: The goal of the diffusion model is to learn the reverse diffusion process that is able to gradually remove noise from xT ∼ N (0, I). This reverse process is also modeled as a Markov chain with each transition being defined as a Gaussian, with the estimated mean μ θ : As derived in  with ∼ N (0, I), α t = 1β t , and αt = t s=1 α s . To perform shape reconstruction with the trained network, we start with a point cloud x T = (x T , c 0 ) with xT ∼ N (0, I). This point cloud is then passed through the reverse diffusion process with z ∼ N (0, I), for t = T, ..., 1. While this reverse diffusion process gradually removes noise from xT , the points belonging to the defective anatomical structure c 0 remain unchanged. As proposed in  Voxelization. To create an implant, the point cloud of the restored anatomy must be converted back to voxel space. We follow a learning-based pipeline proposed in  During training, the ground truth indicator grid χ can be obtained directly from the ground truth voxel representation S c and, as described in  Due to the differentiability of the used Poisson solver, the networks can be trained with an MSE loss between the estimated and ground truth indicator grid: For further information on the used network architectures, we refer to  Implant Generation. With the predicted complete anatomical structure S c and the defective input structure S d , an implant geometry I can be derived by the Boolean subtraction between S c and S d : To further improve the implant quality and remove noise, we apply a median filter as well as binary opening to the generated implant. Ensembling. As the proposed point cloud diffusion model features a stochastic generation process, we can sample multiple anatomically reasonable implant designs for each defect. This offers physicians the opportunity of selecting from various possible implants and allows the determination of a mean implant from a previously generated ensemble of n different implants. As presented in "
3,Experiments,"We evaluated our method on the publicly available parts of the SkullBreak and SkullFix datasets. For the point cloud diffusion model we chose a total number of 30 720 points (N = 27 648, M = 3072), set T = 1000, followed a linear variance schedule between β 0 = 10 -4 and β T = 0.02, used the Adam optimizer with a learning rate of 2 × 10 -4 , a batch size of 8, and trained the network for 15 000 epochs. This took about 20 d/4 d for the SkullBreak/SkullFix dataset. For training the voxelization network, we used the Adam optimizer with a learning rate of 5 × 10 -4 , a batch size of 2 and trained the networks for 1300/500 epochs on the SkullBreak/SkullFix dataset. This took about 72 h/5 h. All experiments were performed on an NVIDIA A100 GPU using PyTorch as the framework. SkullBreak/SkullFix. Both datasets, SkullBreak and SkullFix "
4,Results and Discussion,"For evaluating our approach, we compared it to three methods from AutoImplant 2021 challenge: the winning 3D U-Net based approach  Qualitative results of the different implant generation methods are shown in Fig. "
5,Conclusion,"We present a novel approach for automatic implant generation based on a combination of a point cloud diffusion model and a voxelization network. Due to the sparse point cloud representation of the anatomical structure, the proposed approach can directly handle high resolution input images without losing context information. We achieve competitive evaluation scores, while producing smoother, more realistic surfaces. Furthermore, our method is capable of producing different implants per defect, accounting for the anatomical variation seen in the training population. Thereby, we can propose several solutions to the physicians, from which they can choose the most suitable one. For future work, we plan to speed up the sampling process by using different sampling schemes, as proposed in "
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Fig. 5 .,
,Fig. 6 .,
,Table 1 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 11.
1,Introduction,"Surgical workflow analysis in endoscopic procedures aims to process large streams of data  A recently conducted endoscopic vision challenge  To tackle these research questions, we propose a fully differentiable twostage pipeline, MCIT-IG, that stands for M ulti-Class Instrument-aware Transformer -Interaction Graph. The MCIT-IG relies on instrument spatial information that we generate with Deformable DETR  We hypothesize that a precise instrument detector can reveal additional instrument-target associations as more instrument instances are detected. To test this hypothesis, we conduct a study to investigate how the accuracy of the instrument detector affects triplet detection. We train an instrument detector with limited spatial data and evaluate the impact on triplet detection. We find that enhancing instrument localization is strongly linked to improved triplet detection performance. Finally, we evaluate our model on the challenge split of CholecT50 "
2,Methodology,"We design a novel deep-learning model that performs triplet detection in twostages. In the first stage, we use a transformer to learn the instrument-aware target class embeddings. In the second stage, we construct an interaction graph from instrument instances to the embeddings, learn verb on the interacting edges and finally associate a triplet label with instrument instances. Using a trained Deformable DETR "
,Backbone:,"To extract visual features, we utilize ResNet50 "
,Learning Instrument-Aware Target Class Embeddings:,"To learn target features, we introduce a M ulti-C lass I nstrument-aware T ransformer ( MCIT ) that generates embeddings for each target class. In the standard transformer  MCIT learns meaningful class embeddings of the target enriched with visual and position semantics of the instruments. This instrument-awareness is useful to identify the interacting instrument-target pairs."
,Learning Instrument-Target Interactions:,"To learn the interaction of the instrument and the target, we introduce a graph based framework I nteraction-Graph ( IG) that relies on the discriminative features of instrument instances and target class embeddings. We create an unidirectional complete bipartite graph, G = (U, V, E), where |U| = O and |V| = N denotes the source and destination nodes respectively, and edges E = {e u v , u ∈ U ∧ v ∈ V}. The node features of U and V correspond to the detected instrument instance features F i and target class embeddings N t respectively. We further project the nodes features to a lower dimensional space d using a linear layer Φ p . This setup provides an intuitive way to model instrument-tissue interactions as a set of active edges. Next, we apply message passing using GAT "
,Learning Verbs:,"We concatenate the source and destination node features of all the edges E in G to construct the edge feature E f = {e f , e f ∈ R 2d }. Then, we compute the edge confidence score E s = {e s , e s ∈ R} for all edges E in G by applying a linear layer Φ e on E f . As a result, shown in Fig.  Triplet Detection: To perform target and verb association for each instrument instance i, first we select the active edge e i j that corresponds to the target class j = argmax(α(E i s )), where α denotes softmax function and E i s = {e u s , ∀e u s ∈ E s ∧ u = i}. For the selected edge e = e i j , we apply softmax on the verb logits to obtain the verb class id, k = argmax(α(y e v )). The final score for the triplet 〈i, k, j 〉 is given by p(e i j ) × p(y e v k ), where p denotes the probability score. Mixed Supervision: We train our model in two stages. In the first stage, we train MCIT to learn target classwise embeddings with target binary presence label with weighted binary cross entropy on target logits y t for multi-label classification task following Eq. 2: where C refers to total number of target classes, y c and ŷc denotes correct and predicted labels respectively, σ is the sigmoid function and W c is the class balancing weight from  To train IG, we apply categorical cross entropy loss on edge set E i s and verb logits y e v for all instrument instances i to obtain losses L e G and L v G respectively following Eq. 3: where M denotes the number of classes which is N for L e G and V + 1 for L v G . The final loss for training follows Eq. 4: where α and β denote the weights to balance the loss contribution."
3,Experimental Results and Discussion,
3.1,Dataset and Evaluation Metrics,Our experiments are conducted on the publicly available CholecT50 
3.2,Implementation Details,"We first train our instrument detector for 50 epochs using a spatially annotated 12 video subset of Cholec80 and generate instrument bounding boxes and pseudo triplet instance labels for CholecT50 training videos. In stage 1, we set b l = 2, t l = 4, and d to 512. We initialize target class embeddings with zero values. We use 2-layer MLP for Φ b , Φ f , and 1-layer MLP for Φ t . We resize the input frame to 256 × 448 resolution and apply flipping as data augmentation. For training, we set learning rate 1e -3 for (backbone, base encoder), and 1e -2 for MCIT. We use SGD optimizer with weight decay 1e -6 and train for 30 epochs. To learn the IG, we fine-tune stage 1 and train stage 2. We use learning rate 1e -4 for (MCIT, base encoder), and 1e -5 for the backbone. In IG, Φ p , Φ e , and Φ v are 1-layer MLP with learning rate set to 1e -3 , and d set to 128 in Φ p to project node features to lower dimensional space. We use Adam optimizer and train both stage 1 and stage 2 for 30 epochs, exponentially decaying the learning rate by 0.99. The loss weights α and β is set to 1 and 0.5 respectively. We set batch size to 32 for both stages. We implement our model in PyTorch and IG graph layers in DGL "
3.3,Results,
,Comparison with the Baseline:,"We obtain the code and weights of Rendezvous (RDV)  Ablation Study on the Spatial Annotation Need: Here, we study the impact of an instrument localization quality on triplet detection and how the target features can supplement fewer spatial annotations of the instruments for better triplet detection. We compare with ResNet-CAM-YOLOv5 "
,Ablation Studies on the Components of MCIT-IG:,We analyze the modules used in MCIT-IG and report our results in Table 
,Comparison with the State-of-the-Art (SOTA) Methods:,Results in Table 
4,Conclusion,"In this work, we propose a fully differentiable two-stage pipeline for triplet detection in laparoscopic cholecystectomy procedures. We introduce a transformerbased method for learning per class embeddings of target anatomical structures in the absence of target instance labels, and an interaction graph that dynamically associates the instrument and target embeddings to detect triplets. We also incorporate a mixed supervision strategy to help train MCIT and IG modules. We show that improving instrument localization has a direct correlation with triplet detection performance. We evaluate our method on the challenge split of the CholecT50 dataset and demonstrate improved performance over the leaderboard."
,Fig. 1 .,
,Table 1 .,
,Table 2 .,
,Table 3 .,
,Table 4 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 48.
1,Introduction,"Surgical videos can provide objective records in addition to medical records. Such videos are used in various applications, including education, research, and information sharing  To overcome this issue, Kumar and Pal  For solid and stable recordings, previous studies have installed cameras on surgical lights. Byrd et al.  -We are the first to fulfill the task of automatic generation of stable virtual single-view video with reduced occlusion for a multi-camera system installed in a surgical light. -We propose an algorithm that detects camera movement timing by measuring the degree of misalignment between the cameras. -We propose an algorithm that finds frames with less occluded surgical fields. -We present experiments showing greater effectiveness of our algorithm than conventional methods."
2,Method,"Given a sequence of image frames captured by five cameras installed in a surgical light x i = [x 1 , x 2 , ..., x T ] (Fig.  We perform an initial alignment using the method by Obayashi et al. "
2.1,Camera Movement Detection,"To find the t c , we use the ""degree of misalignment"" obtained from the sequence y i of the five aligned cameras. Since the misalignment should be zero if the geometric calibration between the cameras works well and each view overlaps perfectly, it can be used as an indication of camera movement. First, the proposed method performs feature point detection in each frame of y i . Specifically, we use the SIFT algorithm  where p and k denote a keypoint position and its index in the i-th camera's coordinates respectively, l represents the corresponding index of k in the j-th camera's coordinates, and n represents the total number of corresponding points. If D t exceeds a certain threshold, our method detects camera movement. However, the calculated misalignment is too noisy to be used as is. To eliminate the noise, the outliers are removed, and smoothing is performed by calculating the movement average. Moreover, to determine the threshold, sample clustering is performed according to the degree of misalignment. Assuming that the multicamera surgical light never moves more than twice in 10 min, the detected degree of misalignment is divided into two classes, one for every 10 min. The camera movement detection threshold is expressed by Eq. (  To make the estimation of t c more robust, this process is performed multiple times on the same group of frames, and the median value is used as t c . This is expected to minimize false detections."
2.2,Detecting the Timing for Obtaining a Homography Matrix,"t h represents the timing when to obtain homography matrix. Although it would be ideal to generate always-aligned camera sequences by performing homography transformation on every frame, this would incur high computational costs if the homography is constantly calculated. Therefore, the proposed method calculates the homography matrix only after the cameras have stopped moving. Unlike a previous work that determined the timing for performing homography transformation manually  where S is calculated every 30 frames, and if it is continuously below a given threshold (0.5 in this method), the corresponding timing is selected as the t h ."
3,Experiments and Results,"We conducted two experiments to quantitatively and qualitatively investigate our method's efficacy. Dataset. We used a multi-camera system attached to a surgical light to capture videos of surgical procedures. We captured three types of actual surgical procedures: polysyndactyly, anterior thoracic keloid skin graft, and posttraumatic facial trauma rib cartilage graft. From these videos, we prepared five videos which were trimmed to one minute each. Videos 1 and 2 show the surgery of polysyndactyly, videos 3 and 4 show the anterior thoracic keloid skin graft scene, and video 5 shows the surgery of posttraumatic facial trauma rib cartilage graft. Implementation Details. We used Ubuntu 20.04 LTS OS, an Intel Core i9-12900 for the CPU, and 62GiB of RAM. We defined the area of surgical field as hue ranging from 0 to 30 or from 150 to 179 in HSV color space. Virtual Single-View Video Generation. Figure  Comparison with Conventional Methods. We compared our auto alignment method (auto-alignment) with two conventional methods. In one of these methods, which is used in a hospital camera switching is performed after manual alignment (manual-alignment). The other method switches between camera views with no alignment (no-alignment)."
3.1,Qualitative Evaluation,"To qualitatively compare our method against baseline methods, we conducted a subjective evaluation. 11 physicians involved in surgical procedures regularly who were expected to actually use the surgical videos were selected as subjects. Figure  To perform a qualitative comparison between the three methods, following a previous work  Although we observed statistically significant differences between the proposed method and the baselines for almost all the test videos, significant differences were not observed only in Video 3, Statement 3 and Video 4, Statements 3 and 4. There may be two reasons for this. One is the small number of participants. Since we limited the participants to experienced surgeons, it was quite difficult to obtain a larger sample size. The second reason is differences in the geometry of the surgical fields. Our method is more effective for scenes with a three-dimensional geometry. If the surgical field is flat with fewer feature points, as in the case of the anterior thoracic keloid skin graft procedure, differences between our method and the manual alignment mehod, which does not take into account three-dimensional structures, are less likely to be observed."
3.2,Quantitative Evaluation,"Our method aims to reduce the misalignment between viewpoints that occurs when swıtching between multiple cameras and generate single-view surgical videos with less occlusion. To investigate the method's effectiveness, we conducted a quantitative evaluation to assess the degree of misalignment between video frames. Following a previous work that calculated degree of misalignment between consecutive time-series frames  where N f is the total number of frames. ITF is higher for videos with less motion blur. AvSpeed expresses the average speed of feature points. With the total number of frames N f and the number of all feature points in a frame N p , AvSpeed is calculated as where z i (t) denotes the image coordinates of the feature point and is calculated as żi (t) = z i (t + 1)z i (t). The results are shown in Table "
4,Conclusion and Discussion,"In this work, we propose a method for generating high-quality virtual singleviewpoint surgical videos captured by multiple cameras attached to a surgical light without occlusion or misalignment through automatic geometric calibration. In evaluation experiments, we compared our auto-alignment method with manual-alignment and no-alignment. The results verified the superiority of the proposed method both qualitatively and quantitatively. The ability to easily confirm the surgical field with the automatically generated virtual single-viewpoint surgical video will contribute to medical treatment. Limitations. Our method relies on visual information to detect the timing of homography calculations (i.e., t h ). However, we may use prior knowledge of a geometric constraint such that cameras are at the pentagon corners (Fig.  We assume that the multi-camera surgical light does not move more than twice in 10 min for a robust calculation of D t . Although surgeons rarely moved the light more often, fine-tuning the parameter may result in further performance improvement. The current implementation shows misaligned images if the cameras move more frequently. In the user-involved study, several participants reported noticeable black regions where no camera views were projected. (e.g., Fig. "
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,1 .,
,Fig. 5 .,
,Table 1 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 26.
1,Introduction,"Specific knowledge in the medical domain needs to be acquired through extensive study and training. When faced with a surgical scenario, patients, medical students, and junior doctors usually come up with various questions that need to be answered by surgical experts, and therefore, to better understand complex surgical scenarios. However, the number of expert surgeons is always insufficient, and they are often overwhelmed by academic and clinical workloads. Therefore, it is difficult for experts to find the time to help students individually  Currently, extensive research and progress have been made on VQA tasks in the computer vision domain  In this case, we propose CAT-ViL DeiT for VQLA tasks in surgical scene understanding. Specifically, our contributions are three-fold: "
2,Methodology,
2.1,Preliminaries,"VisualBERT  Multi-Head Attention  are learnable parameters, and A represents the function of single-head attention aggregation. A linear conversion is then applied for the attention aggregation from multiple heads: h = MA(W o [h 1 . . . h h ]). W o ∈ R po×hpv is the learnable parameters in multiple heads. Each head may focus on a different part of the input to achieve the optimal output."
2.2,CAT-ViL DeiT,"We present CAT-ViL DeiT to process the information from different modalities and implement the VQLA task in the surgical scene. DeiT  Feature Extraction: Taking a given image and the associated question, conventional VQA models usually extract visual features via object proposals "
,CAT-ViL Embedding:,"In the following, the extracted features are processed into visual and text embeddings following VisualBERT "
,Self-Attn,"Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Guided-Attn Guided-Attn Guided-Attn Guided-Attn Guided-Attn Guided-Attn Inspired by  Therefore, the visual embeddings shall be reconstructed with the original query, and the key and value of the text embeddings, which can realize the text embeddings to have instructive information interaction with the visual embeddings, and help the model to focus on the targeted image context related to the question. Six guided-attention layers are applied in our network. Thus, the correlation between questions and image regions can be gradually constructed. Besides, we also build six self-attention blocks for both visual and text embeddings to boost the internal relationship within each modality. This step can also avoid 'over' guidance and seek a trade-off. Then, the attended text embeddings and textguided attended visual embedding shall be output from the co-attention module and propagated through the gated module. Compared to the naive concatenation  Prediction Heads: The classification head, following the normal classification strategy, is a linear layer with Softmax activation. Regarding the localization head, we follow the setup in Detection with Transformers (DETR)  Loss Function: Normally, the cross-entropy loss L CE serves as our classification loss. The combination of L 1 -norm and Generalized Intersection over Union (GIoU) loss "
3,Experiments,
3.1,Dataset,EndoVis 2018 Dataset is a public dataset with 14 robotic surgery videos from MICCAI Endoscopic Vision Challenge  EndoVis 2017 Dataset is also a publicly available dataset from the MIC-CAI Endoscopic Vision Challenge 2017 
3.2,Implementation Details,We conduct our comparison experiments against VisualBERT 
3.3,Results,"Figure  VisualBERT  Finally, we conduct an ablation study on different ViL embedding techniques with the same feature extractors and DeiT backbone in Table "
4,Conclusions,"This paper presents a Transformer model with CAT-ViL embedding for the surgical VQLA tasks, which can give the localized answer based on a specific surgical scene and associated question. It brings up a primary step in the study of VQLA systems for surgical training and scene understanding. The proposed CAT-ViL embedding module is proven capable of optimally facilitating the interaction and fusion of multimodal features. Numerous comparative, robustness, and ablation experiments display the leading performance and stability of our proposed model against all SOTA methods in both question-answering and localization tasks, as well as the potential of real-time and real-world applications. Furthermore, our study opens up more potential VQA-related problems in the medical community. Future work can be focused on quantifying and improving the reliability and uncertainty of these safety-critical tasks in the medical domain."
,Fig. 1 .,
,Fig. 2 .,
,Table 1 .,
,Table 2 .,
,Acknowledgements,. This work was funded by 
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 38.
1,Introduction,"Video segmentation, which refers to assigning pixel-wise annotation to each frame in a video, is one of the most vital tasks in medical image analysis. Thanks to the advance in deep learning algorithms based on Convolutional Neural Networks, medical video segmentation has achieved great progress over recent years  Most of the previous noisy label methods mainly focus on classification tasks. Only in recent years, the problem of noise labels in segmentation tasks has been more explored, but still less involved in medical image analysis. Previous techniques for solving noisy label problems in medical segmentation tasks can be categorized in three directions. The first type of method aims at deriving and modeling the general distribution of noisy labels in the form of Noise Transition Matrix (NTM)  Despite the amazing performance in tackling noisy label issues for medical image segmentation, almost all existing techniques only make use of the information within a single image. To this end, we make the effort in exploring the feature affinity relation between pixels from consecutive frames. The motivation is that the embedding features of pixels from adjacent frames should be close if they belong to the same class, and should be far if they belong to different classes. Hence, if a pixel's feature is far from the pixels of the same class in the adjacent frame and close to the ones of different classes, its label is more likely to be incorrect. Meanwhile, the distribution of noisy labels may vary among different videos and frames, which also motivates us to supervise the network from multiple perspectives. We acquire the embedding feature maps of adjacent frames in the Backbone Section. Then, the temporal affinity is calculated for each pixel in current frame to obtain the positive and negative affinity map indicating possible noisy labels. The affinity maps are then utilized to supervise the network in a multi-scale manner. Inspired by the motivation above and to better resolve noisy label problems with temporal consistency, we propose Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework. Our contributions can be summarized as the following points: 1. In this work, we first propose a novel Temporal Feature Affinity Learning (TFAL) method to evaluate the temporal feature affinity map of an image by calculating the similarity between the same and different classes' features of adjacent frames, therefore indicating possible noisy labels. 2. We further develop a Multi-Scale Supervision (MSS) framework based on TFAL by supervising the network through video, image, and pixel levels. Such a coarse-to-fine learning process enables the network to focus more on correct samples at each stage and rectify the noisy labels, thus improving the generalization ability. 3. Our method is validated on a publicly available dataset with synthetic noisy labels and a real-world label noise dataset and obtained superior performance over other state-of-the-art noisy label techniques. 4. To the best of our knowledge, we are the first to tackle noisy label problems using inter-frame information and discover the superior ability of sequential prior information to resolve noisy label issues."
2,Method,"The proposed Multi-Scale Temporal Feature Affinity Learning Framework is illustrated in Fig.  Then through up-sampling, the Positive Affinity Map a p and Negative Affinity Map a n can be obtained, where a p , a n ∈ R H×W , denote the affinity relation between x t and x t-1 . The positive affinity of clean labels should be high while the negative affinity of clean labels should be low. Therefore, the black areas in a p and the white areas in a n are more likely to be noisy labels. Then we use two affinity maps a p , a n to conduct Multi-Scale Supervision training. Multi-scale refers to video, image, and pixel levels. Specifically, for pixel-level supervision, we first obtain thresholds t p and t n by calculating the average positive and negative affinity over the entire dataset. The thresholds are used to determine the possible noisy label sets based on positive and negative affinity separately. The intersection of two sets is selected as the final noisy set and relabeled with the model prediction p t . The affinity maps are also used to estimate the image-level weights λ I and video-level weights λ V . The weights enable the network to concentrate on videos and images with higher affinity confidence. Our method is a plug-in module that is not dependent on backbone type and can be applied to both image-based backbones and video-based backbones by modifying the shape of inputs and feature maps."
2.1,Temporal Feature Affinity Learning,"The purpose of this section is to estimate the affinity between pixels in the current frame and previous frame, thus indicating possible noisy labels. Specifically, in addition to the aforementioned feature map f t , f t-1 ∈ R h×w×C f , we obtain the down-sampled labels with the same size of feature map ỹ t , ỹ t-1 ∈ R h×w×C , where C means the total class number. We derive the positive and negative label maps with binary variables: M p , M n ⊆ {0, 1} hw×hw . The value corresponds to pixel (i, j) is determined by the label as: where 1 (•) is the indicator function. M p (i, j) = 1 when ith label in ỹ t and jth label in ỹ t-1 are the same class, while M p (i, j) = 0 otherwise; and M n vise versa. The value of cosine similarity map S ∈ R hw×hw corresponds to pixel (i, j) is determined by: S (i, j) = We then use the average cosine similarity of a pixel with all pixels in the previous frame belonging to the same or different class to represent its positive or negative affinity: where a p,f , a n,f ∈ R h×w means the positive and negative map with the same size as the feature map. With simple up-sampling, we could obtain the final affinity maps a p , a n ∈ R H×W , indicating the positive and negative affinity of pixels in the current frame."
2.2,Multi-scale Supervision,"The feature map is first connected with a segmentation head generating the prediction p. Besides the standard cross entropy loss L CE (p, ỹ) = -HW i ỹ (i) logp (i), we applied a label corrected cross entropy loss L CE LC (p, ŷ) = -HW i ŷ (i) logp (i) to train the network with pixel-level corrected labels. We further use two weight factors λ I and λ V to supervise the network in image and video levels. The specific descriptions are explained in the following sections. Pixel-Level Supervision. Inspired by the principle in Confident Learning  Then we update the pixel-level label map ŷ as: where p(i) is the prediction of network. Through this process, we only replace those pixels with both low positive affinity and large negative affinity. Image-Level Supervision. Even in the same video, different frames may contain different amounts of noisy labels. Hence, we first define the affinity confidence value as: q = a p + 1 -a n . The average affinity confidence value is therefore denoted as: q = t p + 1 -t n . Finally, we define the image-level weight as: λ I > 1 if the sample has large affinity confidence and λ I < 1 otherwise, therefore enabling the network to concentrate more on the clean samples. Video-Level Supervision. We assign different weights to different videos such that the network can learn from more correct videos in the early stage. We first define the video affinity confidence as the average affinity confidence of all the frames: x∈V q x . Supposing there are N videos in total, we use k ∈ {1, 2, • • • , N} to represent the ranking of video affinity confidence from small to large, which means k = 1 and k = N denote the video with lowest and highest affinity confidence separately. Video-level weight is thus formulated as: where θ l and θ u are the preseted lower-bound and upper-bound of weight. Combining the above-defined losses and weights, we obtain the final loss as: LC , which supervise the network in a multi-scale manner. These losses and weights are enrolled in training after initialization in an order of video, image, and pixel enabling the network to enhance the robustness and generalization ability by concentrating on clean samples from rough to subtle."
3,Experiments,
3.1,Dataset Description and Experiment Settings,EndoVis 2018 Dataset and Noise Patterns. EndoVis 2018 Dataset is from the MICCAI robotic instrument segmentation dataset
,Rat Colon Dataset.,"For real-world noisy dataset, we have collected rat colon OCT images using 800nm ultra-high resolution endoscopic spectral domain OCT. We refer readers to  Implementation Details. We adopt Deeplabv3+ "
3.2,Experiment Results on EndoVis 2018 Dataset,"Table  Visualization of Temporal Affinity. To prove the effectiveness of using affinity relation we defined to represent the confidence of label, we display comparisons between noise variance and selected noise map in Fig. "
,Method,Deeplabv3+ 
3.3,Experiment Results on Rat Colon Dataset,The comparison results on real-world noisy Rat Colon Dataset are presented in Table 
4,Discussion and Conclusion,"In this paper, we propose a robust MS-TFAL framework to resolve noisy label issues in medical video segmentation. Different from previous methods, we first introduce the novel TFAL module to use affinity between pixels from adjacent frames to represent the confidence of label. We further design MSS framework to supervise the network from multiple perspectives. Our method can not only identify noise in labels, but also correct them in pixel-wise with rich temporal consistency. Extensive experiments under both synthetic and real-world label noise data demonstrate the excellent noise resilience of MS-TFAL."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Table 2 .,
,Acknowledgements,. This work was supported by 
,Data,Method mIOU (%) Sequence mIOU (%) Dice (%)
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 9.
1,Introduction,"Reconstructing deformable tissues in surgical scenes accurately and efficiently from endoscope stereo videos is a challenging and active research topic. Such techniques can facilitate constructing virtual surgery environments for surgery robot learning and AR/VR surgery training and provide vivid and specific training for medics on human tissues. Moreover, real-time reconstruction further expands its applications to intraoperative use, allowing surgeons to navigate and precisely control surgical instruments while having a complete view of the surgical scene. This capability could reduce the need for invasive follow-up procedures and address the challenge of operating within a confined field of view. Neural Radiance Fields (NeRFs)  Recently, explicit and hybrid methods have been developed for modeling static scenes, achieving significant speedups over NeRF by employing explicit spatial data structures  To address the aforementioned challenges, we propose a novel method for fast and accurate reconstruction of deformable tissues in surgical procedures, Neural LerPlane (Linear Interpolation Plane), by leveraging explicitly represented multi-plane fields. Specifically, we treat surgical procedures as 4D volumes, where the time axis is orthogonal to 3D spatial coordinates. LerPlane factorizes 4D volumes into 2D planes and uses space planes to form static fields and space-time planes to form dynamic fields. This factorization results in a compact memory footprint and significantly accelerates optimization compared to previous methods  We summarize our contributions: 1. A fast deformable tissue reconstruction method, with rendering quality comparable to or better than the previous method in just 3 min, which is over 100× faster. 2. An efficient representation of surgical scenes, which includes static and dynamic fields, enabling fast optimization and high reconstruction quality. 3. A novel sampling method that boosts optimization and improves the rendering quality. Compared to previous methods, our LerPlane, achieves much faster optimization with superior quantitative and qualitative performance on 3D reconstruction and deformation tracking of surgical scenes, providing significant promise for further applications."
2,Method,
2.1,Overview,"LerPlane represents surgical procedures using static and dynamic fields, each of which is made up of three orthogonal planes (Sect. 2.3). It starts by using spatiotemporal importance sampling to identify high-priority tissue pixels and build corresponding rays (Sect. 2.4). Then we sample points along each ray and query features using linear interpolation to construct fused features. The fused features and encoded coordinate-time information are input to a lightweight MLP, which predicts color and density for each point (Sect. 2.5). To better optimize LerPlane, we introduce some training schemes, including sample-net, various regularizers, and a warm-up training strategy (Sect. 2.6). Finally, we apply volume rendering to produce predicted color and depth values for each chosen ray. The overall framework is illustrated in Fig. "
2.2,Preliminaries,"Neural Radiance Field (NeRF)  c, σ = Φ r (x, y, z, θ, φ). (1) It calculates the expected color Ĉ(r) and the expected depth D(r) of a pixel in an image captured by a camera by tracing a ray r(t) = o + td from the camera center to the pixel. Here, o is the ray origin, d is the ray direction, and t is the distance from a point to the o, ranging from a pre-defined near bound t n to a far bound t f . w(t) represents a weight function that accounts for absorption and scattering during the propagation of light rays. The pixel color is obtained by classical volume rendering techniques "
2.3,Neural LerPlane Representations for Deformable Tissues,"A surgical procedure can be represented as a 4D volume, and we factorize the volume into 2D planes. Specifically, We represent a surgical scene using six orthogonal feature planes, consisting of three space planes (i.e., XY, YZ, and XZ ) for the static field and three space-time planes (i.e., XT, YT, and ZT ) for the dynamic field. Each space plane has a shape of N × N × D, and each space-time plane owns a shape of N × M × D, where N and M represent spatial and temporal resolution, respectively, and D is the size of the feature. To extract features from an image pixel p ij with color C at a specific timestep τ , we first cast a ray r(t) from o to the pixel. We then sample spatial-temporal points along the ray, obtaining their 4D coordinates. We acquire a feature vector for a point P(x, y, z, τ ) by projecting it onto each plane and using bilinear interpolation B to query features from the six feature planes. v(x, y, z, τ ) = B(F XY , x, y) B(F YZ , y, z) . . . B(F YT , y, τ) B(F ZT , z, τ),  Existing methods "
2.4,Spatiotemporal Importance Sampling,"Tool occlusion in robotic surgery poses a challenge for reconstructing occluded tissues due to their infrequent occurrence in the training set, resulting in varied learning difficulties for different pixels. Besides, we observe that many tissues remain stationary over time, and therefore repeated training on these pixels contributes minor to the convergence, reducing efficiency. We design a novel spatiotemporal importance sampling strategy to address the issues above. In particular, we utilize binary masks {M i } T i=1 and temporal differences among frames to generate sampling weight maps {W} T i=1 . These weight maps represent the sampling probabilities for each pixel/ray, drawing inspiration from  where α is a lower-bound to avoid zero weight among unchanged pixels, Ω i specifies higher importance scaling for those tissue areas with higher occlusion frequencies, and β is a hyper-parameter for balancing augmentation among frequently occluded areas and time-variant areas. By unitizing spatiotemporal importance sampling, LerPlane concentrates on tissue areas and speeds up training, improving the rendering quality of occluded areas and prioritizing tissue areas with higher occlusion frequencies and temporal variability."
2.5,Coordinate-Time Encoding,Previous methods 
2.6,Optimization,"We adopt a joint supervision approach to optimize the tiny MLP Θ and feature planes using rendered color and depth. To further improve the optimization process, we propose several optimization schemes, including a sample-net for better-sampled points, a warm-up strategy to address outliers, and several regularizers. Sample-Net. The sampling of spatiotemporal points is crucial for volume rendering, with a particular focus on sampling around tissue regions for optimal performance. We replaced the conventional two-stage time-consuming sampling strategy with a single sample-net and train it using histogram loss "
,Regularizers.,We apply some regularization to address the limited information available in surgical scene reconstruction. We adopt 2D total variation (TV) loss for space planes in 
,Warm-Up Training Strategy.,"Since single-view captures cannot provide valid scale information, we leverage pseudo ground truth depth maps D(r) generated by STTR-light  where ΔD(r) = | D(r) -D(r)| represents the absolute depth difference among valid depth values, δ is a threshold at which to change loss type. Considering that the predicted depth maps encounter a lot of unreliable depth values and missing areas  3 Experiments"
3.1,Dataset and Evaluation Metrics,We evaluate our proposed method on the EndoNeRF dataset  Ours-10min Reference Fig. 
3.2,Implementation Details,"We normalize the scene into device coordinates (NDC) to handle single-view endoscopy videos and then project rays within the NDC space. The video duration is normalized to [-1, 1]. We use a two-stage sampling network with 128 and 256 dimensional plane features for the sample-net. Oneblob encoding "
3.3,Evaluation,"We compare our proposed method, LerPlane, against two existing SOTA methods: the surfel warping-based method, E-DSSR "
3.4,Ablation Study,"We conduct ablation studies on the EndoNeRF dataset to understand the key components and demonstrate their effectiveness. Table  1. Sampling Strategy. We compare with two different methods: naively avoiding tool masks, assigning equal weights to other pixels (Ours-NS), and assigning higher probabilities to highly occluded areas (Ours-TS), as in  Further analysis of the optimization schemes is available in the Supplementary Materials."
4,Conclusion and Future Work,"In this paper, we introduced LerPlane, a fast and accurate method for reconstructing deformable tissues from endoscopic videos. By utilizing multi-plane fields and spatiotemporal importance sampling, we can handle tool occlusion and large motion while significantly accelerating optimization. Our experiments show that LerPlane achieves rendering quality comparable to or better than EndoNeRF in just three minutes, which is over 100× faster. We believe that LerPlane could improve robotic surgery scene understanding, benefiting various clinical-oriented tasks and intraoperative surgery applications. Currently, the inference speed of our Lerplane is slow, In our future research endeavors, our main emphasis will revolve around improving the inference time of our approach, with the primary goal of efficiently supporting intraoperative operations. Moreover, we will dedicate our efforts to reducing the input data requirements, thereby aiming to broaden the applicability of LerPlane to a wider range of surgical scenarios."
,Fig. 1 .,
,,
,Fig. 2 .,
,,
,Table 1 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_5.
1,Introduction,"Surgery is often an effective therapy that can alleviate disabilities and reduce the risk of death from common conditions  Designing intelligent assistance systems for operating rooms requires an understanding of surgical scenes and procedures  Mistrust is a major barrier to deep-learning-based predictions applied to clinical implementation  Here, to predict accurately micro-action (fine-grained action) categories happening every moment, we achieve it with two modules. Specifically, a novel anchor-context module for action detection is proposed to highlight the spatiotemporal regions that are interacted with the anchors (we extract instrument features as anchors), which includes surrounding tissues and movement information. Then, with the constraints of class distributions and the surgical videos, we propose a conditional diffusion model to cover the whole distribution of our data and to accurately reconstruct new predictions based on full learning. Furthermore, our class conditional diffusion model also accesses uncertainty for each prediction, through the stochasticity of outputs. We summarize our main contributions as follows: 1) We develop an anchorcontext action detection network (ACTNet), including an anchor-context detection (ACD) module and a class conditional diffusion (CCD) module, which combines three tasks: i) where actions locate; ii) what actions are; iii) how confident our model is about predictions. 2) For ACD module, we develop a spatiotemporal anchor interaction block (STAB) to spatially and temporally highlight the context related to the extracted anchor, which provides micro-action location and initial class. 3) By conditioning on the full distribution of action classes in the surgical videos, our proposed class conditional diffusion (CCD) model reconstructs better class prototypes in a stochastic fashion, to provide a more accurate estimations and push the assessment of the model confidence in its predictions. 4) We carry out comparison and ablation study experiments to demonstrate the effectiveness of our proposed algorithm based on cataract surgery."
2,Methodology,The overall framework of our proposed ACTNet for reliable action detection is illustrated in Fig. 
2.1,Our ACD Module,"Anchor Extraction: Assuming a video X with T frames, denoted as X = {x t } T t=1 , where x t is the t-th frame of the video. The task of this work is to estimate all potential locations and classes P = {box n , c n } N n=1 for action instances contained in video X, where box n is the position of the n-th action happened, c n is the action class of n-th action, and N is the number of action instances. For video representation, this work tries to encode the original videos into features based on the backbone ResNet50  Spatio-Temporal Action interaction Block (STAB): For several actions like pushing, pulling, and cutting, there is no difference just inferred from the local region in one frame. Thus we propose STAB to utilize spatial and temporal interactions with an anchor to improve the prediction accuracy of the action class, which finds actions with strong logical links to provide an accurate class. The structure of STAB is shown in Fig.  For spatial interaction: The instrument feature i t acts as the anchor. In order to improve the anchor features, the module has the ability to select value features that are highly active with the anchor features and merge them. The formulation is defined as: ), where j is the index that enumerates all possible positions of f t . A pairwise function h(•) computes the relationship such as affinity between i t and all f tj . In this work, dot-product is employed to compute the similarity. The unary function g(f tj ) computes a representation of the input signal at the position j. The response is normalized by a factor . S j represents the set of all positions j. Through the formulation, the output a t obtains more information from the positions related to the instrument and catches interactions in space for the actions. For temporal interaction: We build memory features consisting of features in consecutive frames: To effectively model temporal interactions of the anchor, the network offers a powerful tool for capturing the complex and dynamic dependencies that exist between elements in sequential data and anchors. Same with the spatial interaction, we take i t as an anchor and calculate the interactions between the memory features and the anchor. The formulation  "
2.2,CCD Module for Reliable Action Detection,"Since the surgical procedures follow regularity, we propose a CCD module to reconstruct the action class predictions considering the full distribution of action classes in videos. The diffusion conditioned on the action classes and surgical videos is adopted in our paper. Let y 0 ∈ R n be a sample from our data distribution. As shown in Fig.  The forward process and reverse process of unconditional diffusion are provided in the supplementary material. Here, for the diffusion model optimization can be better guided by meaningful information, we integrate the ACD and our surgical video data as priors or constraints in the diffusion training process. We design a conditional diffusion model pθ (y t-1 |y t , x) that is conditioned on an additional latent variable x. Specifically, the model pθ (y t-1 |y t , x) is built to approximate the corresponding tractable ground-truth denoising transition step pt (y t-1 |y t , y 0 , x). We specify the reverse process with conditional distributions as  where α t := 1β t , ᾱt := t s=1 (1β s ), ∼ N (0, 1) and θ (•) estimates using a time-conditional network parameterized by θ. β t is a constant hyperparameter. To produce model confidence for each action instance, we mainly calculate the prediction interval width (IW). Specifically, we first sample N class prototype reconstruction with the trained diffusion model. Then calculate the IW between the 2.5 th and 97.5 th percentiles of the N reconstructed values for all test classes. Compared with traditional classifiers to get deterministic outputs, the denoising diffusion model is a preferable modelling choice due to its ability to produce stochastic outputs, which enables confidence generation."
3,Experimental Results,"Cataract Surgical Video Dataset: To perform reliable action detection, we build a cataract surgical video dataset. Cataract surgery is a procedure to remove the lens of the eyes and, in most cases, replace it with an artificial lens. The dataset consists of 20 videos with a frame rate of 1 fps (a total of 17511 frames and 28426 action instances). Under the direction of ophthalmologists, each video is labelled frame by frame with the categories and locations of the actions. 49 types of action bounding boxes as well as class labels are included in our dataset. The surgical video dataset is randomly split into a training set with 15 videos (13583 frames) and a testing set with 5 videos (3928 frames). Implementation Details: The proposed architecture is implemented using the publicly available Pytorch Library. A model with ResNet50 backbone from Faster R-CNN-benchmark  Method Comparison: In order to demonstrate the superiority of the proposed method for surgical action detection, we carry out a comprehensive comparison between the proposed method and the following state-of-the-art methods: 1) single-stage algorithms, including the Single Shot Detector (SSD)  Ablation Study: To validate the effectiveness of our ACTNet, we have done some ablation studies. We train and test the model with spatial interaction, temporal interaction, spatio-temporal interaction (STAB), and finally together with our CCD model. The testing results are shown in Fig.  The results reveal that the spatial and temporal interactions for instruments can provide useful information to detect the actions. What's more, spatial interaction has slightly better performance than temporal interaction. It may be led by the number of spatially related action categories being slightly more than that of temporally related action categories. It is worth noting that spatial interaction and temporal interaction can be enhanced by each other and achieve optimal performance. After being enhanced by the diffusion model conditioned on our obtained class distributions and video frames, we get optimal performance."
,Confidence Analysis:,"To analyze the model confidence, we take the best prediction for each instance to calculate the instance accuracy. We can observe   from Table "
4,Conclusions,"In this paper, we propose a conditional diffusion-based anchor-context spatiotemporal action detection network (ACTNet) to achieve recognition and localization of every occurring action in the surgical scenes. ACTNet improves the accuracy of the predicted action class from two considerations, including spatiotemporal interactions with anchors by the proposed STAB and full distribution of action classes by class conditional diffusion (CCD) module, which also provides uncertainty in surgical scenes. Experiments based on cataract surgery demonstrate the effectiveness of our method. Overall, the proposed ACTNet presents a promising avenue for improving the accuracy and reliability of action detection in surgical scenes."
,Fig. 1 .,
,Fig. 2 .,
,,
,Fig. 3 .,
,,
,Table 1 .,
,Table 2 .,
1,Introduction,"Vitreoretinal surgeries are complex procedures that require extreme manual dexterity. Typically, surgeons operate through a stereoscopic microscope viewing the surgical area exclusively from an overhead perspective while manipulating delicate anatomical structures with sub-millimeter precision. In an effort to achieve improved surgical visualization, Optical Coherence Tomography (OCT) has been integrated into surgical microscopes, providing high-resolution depth-resolved imaging. Advances in spiral scanning and swept-source OCT  GPU-accelerated direct volume rendering (DVR) was shown to be an effective way of visualizing surgical maneuvers, enabling real-time rendering of 4D OCT data on stereo displays  For this reason, we propose an Intelligent Virtual B-scan Mirror (IVBM), a novel visualization concept to improve targeted instrument interactions in 4D OCT-guided surgery. As illustrated in Fig. "
2,Related Work,"Virtual mirrors have been initially proposed to support spatial perception and provide secondary views of virtual objects in augmented reality  As opposed to previous works on perceptual OCT visualization, the IVBM provides depth cues that aid targeted instrument navigation when viewing the surgical scene from a lateral perspective and targeting a specific cross-section, where axial depth perception is not a primary issue. Augmentation methods proposed in previous works are not designed for such scenarios. Previous works on virtual mirrors have mainly been implemented by rendering the scene from an alternative viewpoint and showing the mirror view next to the virtual objects. In contrast, we augment a selective mirror in-situ into a OCT cross-section. "
3,Methodology,
3.1,Definition of an Intelligent Virtual B-Scan Mirror,"We define an Intelligent Virtual B-scan Mirror (IVBM) as an augmentation of a selected virtual B-scan, which fulfills the following two requirements: (i) voxels integrated into the IVBM are highlighted in the volume rendering, while still visualized semi-transparent to preserve volume structures behind the IVBM. (ii) The selected B-scan cross-section acts as an intelligent mirror by only being sensitive to voxels associated with the surgical instruments in proximity to the IVBM. As illustrated in Fig. "
3.2,Method Components,"Since the IVBM integrates a mirror that is sensitive only to surgical instruments, mirror candidate voxels need to be identified in the volume. To achieve the realtime processing rates necessary for 4D OCT visualizations, we first identify the instrument in a 2D projection image of the volume. Inspired by "
3.3,Composition of an Intelligent Virtual B-Scan Mirror,"During volume raymarching, if a camera ray #» r c intersects with the IVBM, as shown in Fig.  where #» n = (n x , n y , n z ) specifies the normal of the plane. We define a distance threshold d IV BM specifying the thickness of the IVBM. During volume raymarching, a voxel p m with position (p x , p y , p z ) is integrated in the IVBM, if the following condition is fulfilled: To visualize the mirror reflections of surgical instruments, a mirror ray # » r m is cast from each point p m obtained by  During sampling along # » r m , we leverage the binary map M tool and an intensity threshold t tool = 0.25 (empirically obtained to discard OCT speckle noise), and select the mirrored instrument voxel p t if the following condition is fulfilled: To mirror the instrument only when close to the IVBM, we limit the number of sampled steps n steps along # » r m . In case of intersection with an instrument as determined by (  where δ * = (i m • 0.7)/n steps is a distance predicate that considers the step index i m ∈ [0, n steps ] along # » r m when reaching the instrument at p t . Further, γ(I) is a scaling factor as introduced in  where RGB(C) is an L * a * b * to RGB color space conversion. We additionally decrease the opacity of the reflections with increasing distance of the instrument to the mirror using σ(p t ) = 1.0 -(step m /n steps ) 2 . The overall appearance of a voxel at p m , integrating instrument reflections while enhancing the natural intensities in the IVBM plane is finally defined by: In practice, μ is a dynamically modifiable parameter to amplify the volume intensities of the virtual B-scan and α(I) is a conventional piece-wise linear opacity function. During volume raymarching, we use alpha blending to integrate the IVBM with the remaining volume structures as determined by "
4,Experimental Setup and User Study,"System Integration and Time Profiling. To evaluate if IVBMs can be deployed for the live display of 4D OCT data, we implemented the proposed concept in a C++ visualization framework. The 2D projection image generation and IVBMintegrated volume raymarching were implemented using OpenGL 4.6 and tested on a Windows 10 system with Intel Core i7-8700K @3.7 GHz and NVidia RTX 3090Ti GPU. We train our instrument segmentation network on a custom data set consisting of 3356 2D projection images generated from OCT volumes with a resolution of 391×391×644 voxels that contain a synthetic eye model and a surgical forceps and include random rotations and flipping for data augmentation. We use PyTorch 1.13 and TensorRT 8.4 for model training and optimization. The data was labeled by two biomedical engineers. The average overall processing and rendering time, based on 20 test volumes with the same resolution, was 44.3 (±3.1) ms (filter: 8.1 (± 1.2) ms, projection image generation and instrument segmentation: 5.7 (±2.6) ms, rendering: 30.5 (±0.6) ms). These benchmarks were achieved with n steps = 120 mirror sample steps. To demonstrate the live 4D interactions, our method was integrated into the 4D SS-OCT system presented in  User Study. To determine if an IVBM could aid users in performing targeted instrument maneuvers under 4D OCT guidance, we conducted a user study in which we asked participants to move the tip of a surgical instrument to defined target locations. To achieve continuous, accurate, and efficient data collection during the study, we employ a virtual environment (Unity 2021.3) with simulated 4D OCT based on the method proposed in "
5,Discussion and Conclusion,"Discussion. When users were provided an IVBM, statistically significant improvements regarding the targeting error were found. In a clinical setting, such as during retinal membrane peeling, similar improvements could make a substantial difference and may lead to a safer and more efficient treatment. The results of the subjective task load assessment were also overall improved when the IVBM was enabled, however, statistical significance could not be obtained in categories such as mental demand or frustration. Potential depth conflicts could be investigated in further studies and evaluated with clinical experts. Interestingly, a higher targeting accuracy was achieved with IVBM, even in cases when users reported a higher effort or judged their performance inferior compared to baseline. An advantage of the IVBM is direct in-situ visualization that also highlights target structures, as users do not need to move their gaze from the surgical area to perceive the mirrored view. We envision an automatic identification of anatomical targets to find optimal IVBM positioning in the future."
,Conclusion.,"We presented a novel visualization concept that augments a selected volume cross-section with an intelligent virtual mirror for targeted instrument navigation in 4D OCT. We have provided a definition and implementation of an IVBM and demonstrated its potential to effectively support surgical tasks and to be integrated into a 4D OCT system. We demonstrated the IVBM in simulated vitreoretinal surgery, however, we intend to further apply this concept also to other 4D medical imaging modalities."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_40.
1,Introduction,"Surgical video activity recognition has become increasingly crucial in surgical data science with the rapid advancement of technology  Most current works in the field of surgical video analysis primarily focus on surgical phase recognition  Although significant progress has been made in surgical activity triplet recognition, they suffer from the same limitation of ambiguous supervision from the triplet components. Learning to predict the triplet in one shot is a highly imbalanced process, as only samples with correct predictions across all components are considered positive. This objective is particularly challenging in the following scenarios. Firstly, multiple surgical activities may occur in a single frame (see Fig.  To solve the above problems, we propose a triplet disentanglement framework for surgical activity triplet recognition. This approach decomposes the learning objectives, thereby reducing the complexity of the learning process. As stated earlier, surgical activity relies on the presence of tools, making them crucial to our mission. Therefore, our approach concentrates on a simplified numerical representation as a means of mitigating these challenges. Initially, we face challenges such as multiple tools appearing at the same time or irrelevant surgical activities. Therefore, we adopt an intuitive approach to first identify the number/category of tools and whether those activities occur or not, instead of directly recognizing the tool itself. This numerical recognition task helps our network roughly localize the tool's location and differentiate irrelevant surgical activities. Subsequently, we employ a weakly supervised method to detect the tools' locations. However, unlike  In summary, this work makes the following contributions: 1) We propose a triplet disentanglement framework for surgical action triplet recognition, which decomposes the learning objectives in endoscopic videos. 2) By further exploit- ing the knowledge of decomposition, our network is extended to a 3D network to better capture temporal information and make use of temporal class activation maps to alleviate the challenge of multiple surgical activities occurring simultaneously. 3) Our experimental results on the endoscopic video dataset demonstrate that our approaches surpass current state-of-the-art methods."
2,Methods,"Our objective is to recognize every triplet at each frame in each video. Let X = {X 1 , ..., X n } be the frames of endoscopic videos and Y = {Y 1 , ..., Y n } be the set of labels of triplet classes where n is the number of frames and the sets of triplet classes. Moreover, each set of labels of triplet classes can be denoted as Y = {Y I , Y V , Y T } where I, V, and T are indicated as Instrument, Verb, and Target. Figure "
2.1,Sampling Video Clips,Unlike RDV 
2.2,Generating Soft Labels,"We generate four different soft labels for the number of instruments, the number of instrument categories, the presence of an unrelated surgical activity, and the presence of a critical surgical activity, which are labeled as <N I , N C , U A , S A > respectively. As for N I , our goal is to know the number of instruments occurrences because the appearance of an instrument also means the appearance of a surgical activity. For N C , it is employed to differentiate situations where multiple instruments of the same type are present. In terms of the other two soft labels, they are binary labels, which refer to presence or absence, respectively. In addition, labels with the format <instrument, null, null > are marked as irrelevant surgical activity. In contrast, those with the format <instrument, verb, target> are marked as critical surgical activity. For example, in the case shown in the second line of Fig. "
2.3,Disentanglement Framework,Our encoder backbone is built on the RGB-I3D 
2.4,Hierarchical Training Schedule,"Our framework is a multi-task recognition approach, but training so many tasks simultaneously poses a huge challenge to balancing hyper-parameters. To address this, we propose a hierarchical training schedule method that divides training into different stages. Initially, we train only our soft label network to recognize soft labels, storing its parameters once training is complete. In the next stage, video clips are fed into the tool network to recognize tool categories while simultaneously identifying soft labels. After successful training, the parameters of the tool network are frozen. In the subsequent stage, the verb and target networks identify their respective components and soft labels. At this point, the tool network passes its class activation map to the verb-target networks without updating its parameters. Besides, following previous Tripnet "
2.5,Separation Processing,"Our framework provides a unique approach to address the impact of multiple tool categories that may be present simultaneously. It enables the handling of different surgical instrument categories individually, which reduces the level of complexity for learning. In contrast to RDV "
2.6,Loss Function,"For the number of tools and the categories of tools, softmax cross-entropy loss is adopted, while for other soft labels, three components, and triplet, we employ sigmoid cross-entropy losses. Taking sigmoid cross-entropy loss as an example: where σ is the sigmoid function, while y c and ŷc are the ground truth label and the prediction for specific class c. Besides, the balanced weights are also adopted based on previous works "
3,Experiments,"Dataset. Following previous works  Metric. The performance of the method is evaluated based on the mean average precision (mAP) metric to predict the triplet classes. In testing, each triplet class is computed its own average precision (AP) score, and then the AP score of a video will be calculated by averaging the AP scores of all the triplets in this video. Finally, the mean average precision (mAP) of the dataset is measured by averaging the AP scores of all tested videos. Besides, Top-N recognition performance is also adopted in our evaluation, which means that given a test sample X i , a model made a correctness if the correct label y i appears in its top N confident predictions Ŷi . We follow previous works "
3.1,Results and Discussion,"Quantitative Evaluation. We demonstrate our experimental results with other state-of-the-art approaches on the CholecT45  Table  Ablation Study. In this section, we conduct ablation studies to showcase the effectiveness of each module in our model. As shown in Table "
4,Conclusion,"In this paper, we introduce a novel triplet disentanglement framework for surgical activity recognition. By decomposing the task into smaller steps, our method demonstrates improved accuracy compared to existing approaches. We anticipate that our work will inspire further research in this area and promote the development of more efficient and accurate techniques."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Table 1 .,
,Table 2 .,
,Table 3 .,
1,Introduction,"Flexible ureteroscopy (FURS) is a routinely performed surgical procedure for renal lithotripsy. This procedure inserts a flexible ureteroscope through the blad-der and ureters to get inside the kidneys for diagnosis and treatment of stones and tumors. Unfortunately, such an examination and treatment depends on skills and experiences of surgeons. On the other hand, surgeons may miss stones and tumors and unsuccessfully orientate the ureteroscope inside the kidneys due to limited field of views, just 2D images without depth information, and the complex anatomical structure of the kidneys. To this end, ureteroscope tracking and navigation is increasingly developed as a promising tool to solve these issues. Many researchers have developed various methods to boost endoscopic navigation. These methods generally consist of vision-and sensor-based tracking. Han et al.  Although these methods mentioned above work well, ureteroscopic navigation is still a challenging problem. Compared to other endoscopes such as colonoscope and bronchoscope, the diameter of the ureteroscope is smaller, resulting in more limited lighting source and field of view. Particularly, ureteroscopy involves much solids (impurities) and fluids (liquids), making ureteroscopic video images low-quality, as well as these solids and fluids inside the kidneys cannot be regularly observed in computed tomography (CT) images. On the other hand, the complex internal structures such as calyx, papilla, and pyramids of the kidneys are difficult to be observed in CT images. These issues introduce a difficulty in directly aligning ureteroscopic video sequences to CT images, leading to a challenge of image-based continuous ureteroscopic navigation. This work aims to explore an accurate and robust vision-based navigation method for FURS procedures without using any external positional sensors. Based on ureteroscopic video images and preoperative computed tomography urogram (CTU) images, we propose a novel video-CTU registration method to precisely locate the flexible ureteroscope in the CTU space. Several highlights of this work are clarified as follows. To the best of our knowledge, this work shows the first study to continuously track the flexible ureteroscope in preoperative data using a vision-based method. Technically, we propose a novel 2D-3D (video-CTU) registration method that introduces a structural point similarity measure without using image pixel intensity information to characterize the difference between the structural regions in real video images and CTU-driven virtual image depth maps. Additionally, our proposed method can successfully deal with solid and fluid ureteroscopic video images and attains higher navigation accuracy than intensity-based 2D-3D registration methods."
2,Video-CTU Registration,"Our proposed video-CTU registration method consists of several steps: (1) ureteroscopic structure extraction, (2) virtual depth map generation, and (3) structural point similarity and optimization. Figure "
2.1,Ureteroscopic Image Characteristics,"The internal kidneys consist of complex anatomical structures such as pelvis and calyx and also contain solid particles (e.g., stones and impurities) floating in fluids (e.g., water, urine and small blood), resulting in poor image quality during ureteroscopy. Therefore, it is a challenging task to extract meaningful features from these low-quality images for achieving accurate 2D-3D registration. Our idea is to introduce specific structures inside the kidneys to boost the video-CTU registration since these structural regions are meaningful features that can facilitate the similarity computation. During ureteroscopy, various anatomical structures observed in ureteroscopic video images indicate different poses of the ureteroscope inside the kidneys. While some structural features such as capillary texture and striations at the tip of the renal pyramids are observed ureteroscopic images, they are not discernible in CT or other preoperative data. Typical structural or texture regions (Columns 1∼3 in Fig. "
2.2,Ureteroscopic Structure Extraction,"Deep learning is widely used for medical image segmentation. Lazo et al.  where s is the output size ratio of the feature representation and D is the output feature dimension. Tokens from layers l = {6, 12, 18, 24} are reassembled in DPT-Base. These feature representations are subsequently fused into the final dense prediction. In the structure extraction, we define three classes: Non-structural regions (background), structural regions, and stones. We manually select and annotate ureteroscopic video images for training and testing. Vision transformers require large datasets for training, so we initialize the encoder with weights pretrained on ImageNet and further train it on our in-house database. Figure "
2.3,Virtual CTU Depth Map Generation and Thresholding,"This step is to compute depth maps of 2D virtual images generated from CTU images by volume rendering  ( The tracing ray R(x, y, z, r x , r y , r z ) will stop when it encounters opaque voxels. For 3D point (x,y,z), its depth value V can be calculated by projecting the ray R(x, y, z, r x , r y , r z ) onto the normal vector N(x, y, z) of the image plane: where symbol • denotes the dot product. To obtain the depth map with structural regions, we define two thresholds t u and t v . Only CT intensity values within [t u , t v ] are opaque voxels that the casting rays cannot pass through in the ray-casting algorithm. According to CTU characteristics  Unfortunately, the accuracy of thresholded structural regions suffers from inaccurate depth maps caused by renal stones and contrast agents. Stones and agents are usually high intensity in CTU images, which result in incorrect depth information of structural regions (e.g., renal papilla). To deal with these issues, we use the segmented stones as a mask to remove these regions with wrong depth. On the other hand, the structural regions usually have larger depth values than the agent-contrasted regions. Therefore, we sort the depth values outside the mask and only use the thresholded structural regions with the largest depth values for the structural point similarity computation."
2.4,Structural Point Similarity and Optimization,"We define a point similarity measure between DPT-base segmented structural regions in ureteroscopic images and thresholded structural regions in virtual depth maps generated by the volume rendering ray casting algorithm. The structural point similarity function (cost function) is defined as an intersection of point sets from the extracted real and virtual structural regions: where I i is the ureteroscopic video image at frame i, point sets P i and P v are from the ureteroscopic image extracted structural region E i (a, b) and the thresholded structural region E v (a, b) ((a, b) denotes a point)from the depth map D v of the 2D virtual rendering image I v (p i , q i ), respectively: where (p i , q i ) is the endoscope position and orientation in the CTU space. Eventually, the optimal pose (p i , qi ) of the ureteroscope in the CTU space can be estimated by maximizing the structural point similarity: where Powell method "
3,Results and Discussion,"We validate our method on clinical ureteroscopic lithotripsy data with video sequences and CTU volumes. Ureteroscopic video images were a size of 400 × 400 pixels, while the space parameters of CTU volumes were 512 × 512 pixels, 361∼665 slices, 0.625∼1.25 mm slice thickness. Three ureteroscopic videos more than 30000 frames were acquired from three ureteroscopic procedures for experiments. While we manually annotated ureteroscopic video images for DPT-base segmentation, three experts also manually generated ureteroscope pose groundtruth data by our developed software, which can manually adjust position and direction parameters of the virtual camera to visually align endoscopic real images to virtual images, evaluating the navigation accuracy of the different methods.  Figure "
,Luo et al. Ours,"The effectiveness of our proposed method lies in several aspects. First, renal interior structures are insensitive to solids and fluids inside the kidneys and can precisely characterize ureteroscopic images. Next, we define a structural point similarity measure as intersection of point sets between real and virtual structural regions. Such a measure does not use any point intensity information for the similarity calculation, leading to an accurate and robust similarity characterization under renal floating solids and fluids. Additionally, CTU images can capture more renal anatomical structures inside the kidneys compared to CT slices, still facilitating an accurate similarity computation. Our method still suffers from certain limitations. Figure "
4,Conclusion,"This paper proposes a new 2D-3D registration approach for vision-based FURS navigation. Specifically, such an approach can align 2D ureteroscopic video sequences to 3D CTU volumes and successfully locate an ureteroscope into CTU space. Different from intensity-based cost function, a novel structural point similarity measure is proposed to effectively and robustly characterize ureteroscopic video images. The experimental results demonstrate that our proposed method can reduce the navigation errors from (11.28 mm, 10.8 • ) to (5.39 mm, 8.13 • )."
,Fig. 1 .,
,Fig. 2 .Fig. 3 .,
,Fig. 4 .,
,Fig. 5 .,
,Fig. 6 .,
,Table 1 .,
,Table 2 .,
1,Introduction,"Adolescent idiopathic scoliosis (AIS), the most prevalent form of spinal deformity among children, and some patients tend to worsen over time, ultimately leading to surgical treatment if not being treated timely  In previous studies, generative adversarial networks (GANs) have been widely used in medical image translation applications. Long et al. proposed an enhanced Cycle-GAN for integrated translation in ultrasound, which introduced a perceptual constraint to increasing the quality of synthetic ultrasound texture  Diffusion and score-matching models, which have emerged as high-fidelity image generation models, have achieved impressive performance in the medical field  In this study, based on the conditional diffusion model, we design an ultrasound-to-X-ray synthesis network, which incorporates an angle-embedding transformer module into the noise prediction model. We have found that the guidance on angle information can rectify for offsets in generated images with different amounts of the inclination of the vertebrae. To learn the posterior probability of X-ray in actual Cobb angle conditions, we present a conditional consistency function to utilize UCA as the prior knowledge to approximate the objective distribution. Our contributions are summarized as follows: • We propose a novel method for the Us-to-X-ray translation using probabilistic denoising diffusion probabilistic model and a new angle embedding attention module, which takes UCA and Cobb angle as the prerequisite for image generation. • Our attention module facilitates the model to close the objective X-ray distribution by incorporating the angle and source domain information. The conditional consistency function ensures that the angle guidance flexibly controls the curvature of the spine during the reverse process. • correlation with authentic Cobb angle indicates its high potential in scoliosis evaluation."
2,Method,
2.1,Conditional Diffusion Models for U2X Translation,"Diffusion models are the latent variable model that attempts to learn the data distribution, followed by a Markovian process. The objective is to optimize the usual variational bound on negative log likelihood denoted as: p θ (x 0:T ) is the joint distribution called the reverse process. q(x 1:T |x 0 ) is the diffusion process, progressively adding Gaussian noise to the previous state of the system. Given x ∼ p(x) ∈ R W ×H be an X-ray image, conditional diffusion models tend to learn the data distribution in condition y. In this work, we partition y into the set of Cobb angle y and paired ultrasound image Then the training objective can be reparameterized as the prediction of mean of noising data distribution with y at all timestep t:"
2.2,Attention Module with Angle Embedding,"Theoretically, if the denoising ˘ θ is correct, then as T → ∞, we can obtain X-ray images that the sample paths are distributed as p θ (x t-1 |x t , c = [y, x src ]). However, acquiring Cobb angles from real X-ray images is not feasible during the sampling stage. Accordingly, we propose an auxiliary model ˜ θ , taking estimated UCA as the prior knowledge, to approximate the objective distribution. Specifically, let ỹ be the UCA of ultrasound images (Fig.  where A is the operation of a standard transformer encoder "
2.3,Consistent Conditional Loss,"Training UXDiffusion. As described in Algorithm 1, the input to the denoising model ˜ θ and ˘ θ are the corrupt image, source image and the list of angle. Rather than learning in the direction of the gradient of the Cobb angle, the auxiliary denoising model instead predicts the score estimates of the posterior probability in UCA conditions for approximating the X-ray distribution under the actual Cobb angle condition. The bound can be denoted as KL(p θ (x t-1 |x t , ỹ, x src )||p θ (x t-1 |x t , y, x src )), where KL denotes Kullback-Leibler divergence. The reverse process mean comes from an estimate x θ (ỹ) ≈ x θ (y) plugged into q(x t-1 |x t , y). Note that the parameterization of the mean of distribution can be simplified to the noise prediction  (5) Sampling with UCA. We follow the sampling scheme in "
3,Experiments,
3.1,Dataset,"The dataset consists of 150 paired coronal ultrasound and X-ray images. Each patient took X-ray radiography and ultrasound scanning at the same day. Patients with BMI indices greater than 25.0 kg/m 2 and patients with scoliosis angles exceeding 60 • were excluded from this study, as the 7.5 MHz ultrasound transducer could not penetrate well for those fatty body and the spine would deform and rotate too much with large Cobb angle, thus affecting ultrasound image quality. The Cobb angle was acquired from an expert with 15 years of experience on scoliosis radiographs, while the UCA was acquired by two raters with at least 5 years of evaluating scoliosis using ultrasound. We manually aligned and cropped the paired images to the same reference space. The criterion for registration was to align the spatial positions of the transverse and spinous processes and the ribs. We resized them into 256 × 512, and grouped the data by 90, 30 and 30 for training, validation and test, respectively."
3.2,Implementation Details,"We followed the same architecture of DDPM, using a U-Net network with attention blocks to predict . For the angle embedding attention module, we transformed the token in ViT  The transformer encoder blocks were used to compute the self-attention on the embedding angle and image tokens. We set T = 2000 and forward variances from 10 -4 to 0.02 linearly. We apply the exponential moving average strategy to update the model weights, the decay rate is 0.999. All the experiments were conducted on a 48GB NVIDIA RTX A6000 GPU."
3.3,Synthesis Performance,"In this section, three different types of generated models were chosen for comparison: 1) GAN-based model for paired image-to-image translation "
3.4,Comparison with Cobb Angle,"Since the Cobb angle is widely used as the gold standard for scoliosis assessment, our objective is to synthesize an X-ray-like image that can be applied to the measurement of the Cobb angle. As depicted in Fig. "
4,Conclusion,"This paper developed a synthesized model for translating coronal ultrasound images to X-ray-like images using a probabilistic diffusion network. Our purpose is to use a single network to parameterize the X-ray distribution, and the generated images can be applied to the Cobb angle measurement. We achieved this by introducing the angular information corresponding to ultrasound and X-ray image to the model for noise prediction. An attention module was proposed to guide the model for generating high-quality images based on embedded image and angle information. Furthermore, to overcome the unavailability of the Cobb angle in the sampling process, we presented a conditional consistency function to train the model to learn the gradient according to the UCA for approximating the X-ray distribution in the condition of Cobb angle. Experiments on paired ultrasound and X-ray coronal images demonstrated that our diffusionbased method advanced the state-of-the-art significantly. In summary, this new model has great potential to facilitate 3D ultrasound imaging to be used for scoliosis assessment with accurate Cobb angle measurement and X-ray-like images obtained without any radiation."
,Fig. 1 .,
,Fig. 2 .,
,Algorithm 1 . 2 / 9 :,
,Fig. 3 .,
,Fig. 4 .,
,Table 1 .,
1,Introduction,"Automated surgical scene segmentation is an important prerequisite for contextaware assistance and autonomous robotic surgery. Recent work showed that deep learning-based surgical scene segmentation can be achieved with high accuracy  Given these gaps in the literature, the contribution of this paper is twofold: 1. We show that geometric domain shifts have disastrous effects on SOA surgical scene segmentation networks for both conventional RGB and HSI data. 2. We demonstrate that topology-altering augmentation techniques adapted from the general computer vision community are capable of addressing these domain shifts."
2,Materials and Methods,"The following sections describe the network architecture, training setup and augmentation methods (Sect. 2.1), and our experimental design, including an overview of our acquired datasets and validation pipeline (Sect. 2.2)."
2.1,Deep Learning-Based Surgical Scene Segmentation,"Our contribution is based on the assumption that application-specific data augmentation can potentially address geometric domain shifts. Rather than changing the network architecture of previously successful segmentation methods, we adapt the data augmentation. Surgery-Inspired Augmentation: Our Organ Transplantation augmentation illustrated in Fig.  Network Architecture and Training: We used a U-Net architecture "
2.2,Experiments,"To study the performance of SOA surgical scene segmentation networks under geometric domain shifts and investigate the generalizability improvements offered by augmentation techniques, we covered the following OOD scenarios: (I) Organs in isolation: Abdominal linens are commonly used to protect soft tissue and organs, counteract excessive bleeding, and absorb blood and secretion. Some surgeries (e.g., enteroenterostomy), even require covering all but a single organ. In such cases, an organ needs to be robustly identified without any information on neighboring organs. (II) Organ resections: In resection procedures, parts or even the entirety of an organ are removed and surrounding organs thus need to be identified despite the absence of a common neighbor. (III) Occlusions: Large parts of the situs can be occluded by the surgical procedure itself, introducing OOD neighbors (e.g., gloved hands). The nonoccluded parts of the situs need to be correctly identified. Real-World Datasets: In total, we acquired 600 intraoperative HSI cubes from 33 pigs using the HSI system Tivita R Tissue (Diaspective Vision GmbH, Am Salzhaff, Germany). These were semantically annotated with background and 18 tissue classes, namely heart, lung, stomach, small intestine, colon, liver, gallbladder, pancreas, kidney with and without Gerota's fascia, spleen, bladder, subcutaneous fat, skin, muscle, omentum, peritoneum, and major veins. Each HSI cube captures 100 spectral channels in the range between 500nm and 1000nm at an image resolution of 640 × 480 pixels. RGB images were reconstructed by aggregating spectral channels in the blue, green, and red ranges. To study organs in isolation, we acquired 94 images from 25 pigs in which all but a specific organ were covered by abdominal linen for all 18 different organ classes (dataset isolation real ). To study the effect of occlusions, we acquired 142 images of 20 pigs with real-world situs occlusions (dataset occlusion), and 364 occlusion-free images (dataset no-occlusion). Example images are shown in Fig.  Manipulated Data: We complemented our real-world datasets with four manipulated datasets. To simulate organs in isolation, we replaced every pixel in an image I that does not belong to the target label l either with zeros or spectra copied from a background image. We applied this transformation to all images in the dataset original and all target labels l, yielding the datasets isolation zero and isolation bgr. Similarly, we simulated organ resections by replacing all pixels belonging to the target label l either with zeros or background spectra, yielding the datasets removal zero and removal bgr. Example images are shown in Fig. "
,Train-Test Split and Hyperparameter Tuning:,"The SOA surgical scene segmentation algorithms are based on a union of the datasets occlusion and no-occlusion, termed dataset original, which was split into a hold-out test set (166 images from 5 pigs) and a training set (340 images from 15 pigs). To enable a fair comparison, the same train-test split on pig level was used across all networks and scenarios. This also holds for the occlusion scenario, in which the dataset no-occlusion was used instead of original for training. All networks used the geometric transformations shift, scale, rotate, and flip from the SOA prior to applying the augmentation under examination. All hyperparameters were set according to the SOA. Only hyperparameters related to the augmentation under examination, namely the probability p of applying the augmentation, were optimized through a grid search with p ∈ {0.2, 0.4, 0.6, 0.8, 1}. We used five-fold-cross-validation on the datasets original, isolation zero, and isolation bgr to tune p such that good segmentation performance was achieved on both in-distribution and OOD data. Validation Strategy: Following the recommendations of the Metrics Reloaded framework "
3,Results,"Effects of Geometric Domain Shifts: When applying a SOA segmentation network to geometric OOD data, the performance drops radically (cf. Fig.  Performance of Our Method: Figure "
,Comparison to SOA Augmentations:,There is no consistent ranking across all six OOD datasets except for Organ Transplantation always ranking first and baseline usually ranking last (cf. Fig. 
4,Discussion,"To our knowledge, we are the first to show that SOA surgical scene segmentation networks fail under geometric domain shifts. We were particularly surprised by the large performance drop for HSI data, rich in spectral information. Our results clearly indicate that SOA segmentation models rely on context information. Aiming to address the lack of robustness to geometric variations, we adapted so far unexplored topology-altering data augmentation schemes to our target application and analyzed their generalizability on a range of six geometric OOD datasets specifically designed for this study. The Organ Transplantation augmentation outperformed all other augmentations and resulted in similar performance to in-distribution performance on real OOD data. Besides its effectiveness and computational efficiency, we see a key advantage in its potential to reduce the amount of real OOD data required in network training. Our augmentation networks were optimized on simulated OOD data, indicating that image manipulations are a powerful tool for judging geometric OOD performance if real data is unavailable, such as in our resection scenario, which would have required an unfeasible number of animals. With laparoscopic HSI systems only recently becoming available, the investigation and compensation of geometric domain shifts in minimally-invasive surgery could become a key direction for future research. Our proposed augmentation is model-independent, computationally efficient and effective, and thus a valuable tool for addressing geometric domain shifts in semantic scene segmentation of intraoperative HSI and RGB data. Our implementation and models will be made publicly available."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 59.
1,Introduction,"The technical advances in endoscopes have extended the diagnostic and therapeutic value of endoluminal interventions in a wide range of clinical applications. Due to the restricted field of view, it is challenging to control the flexible endoscopes inside the lumen. Therefore, the development of navigation systems, which locates the position of the end-tip of endoscopes and enables the depth-wise visualization, is essential to assisting the endoluminal interventions. A typical task is to visualize the depth-wise information and estimate the six-degree-of-freedom (6DoF) pose of endoscopic camera based on monocular imaging. Due to the clinical limitations, the ground truth of depth and pose trajectory of endoscope imaging is difficult to acquire. Previous works jointly estimated the depth and the pose via the unsupervised frameworks  The first issue is the insufficient visual diversity of endoscopic datasets. Compared with the large-scaled KITTI dataset  The second issue is the appearance inconsistency in image triplets. Due to the complicated environment of endoluminal structures, the illumination changes, motion blurness and specular artefacts are frequently observed in endoscopy images. The appearance-inconsistent area may generate substantial photometric losses even in the well-aligned adjacent frames. These photometric losses caused by the inconsistent appearance impede the training process and remain unable to optimize. To handle this problem, AF-SfMLearner  In this paper, we propose a triplet-consistency-learning framework (TCL) for unsupervised depth and pose estimation of monocular endoscopes. To improve the visual diversity of image triplets, the perspective view synthesis is introduced, considering the geometric consistency of camera motion. Specifically, the depthconsistency and pose-consistency are preserved via specific losses. To reduce the appearance inconsistency in the image triplets, a triplet-masking strategy is proposed by measuring the differences between the triplet-level and the framelevel representations. The proposed framework does not involve additional model parameters, which can be easily embedded into previous SfM methods. Experiments on public datasets demonstrate that TCL can effectively improve the accuracy of depth and pose estimation even with small amounts of training samples."
2,Methodology,
2.1,Unsupervised SfM with Triplet Consistency Learning,"Unsupervised SfM Method. The unsupervised SfM methods adopt Depth-Net and PoseNet to predict the depth and the pose, respectively. With depth and pose prediction, the reference frames I ri (i = 0, 1) are warped to the warped frames I ti (i = 0, 1). The photometric loss, denoted by L P , is introduced to measure the differences between I ti (i = 0, 1) and the target frame I t . The loss can be implemented by L 1 norm  In addition to L P , auxiliary regularization loss functions, such as depth map smoothing loss  Previous unsupervised methods  Framework Architecture. As shown in Fig. "
2.2,Learning with Geometric Consistency,"Since the general data augmentation methods cannot generate synthesis with sufficient variance of camera views, 3DCC "
,Synthesis Triplet Generation.,"The perspective view synthesis method aims to warp the original image I to generate a new image I . The warping process is based on the camera intrinsic matrix K, the depth map D of the original image and perturbation pose P 0 . For any point q in I, its depth value is denoted as z in D. The corresponding point q on the new image I is calculated by Eq.(  Given the depth maps generated from a pre-trained model, we perform perspective view synthesis with the same pose transformation P 0 on the three frames of raw triplet respectively.  Pose Consistent Loss. As shown in Fig. "
2.3,Learning with Appearance Inconsistency,"The complicated environment of endoluminal structures may cause appearance inconsistency in image triplets, leading to the misalignment of reference and target frames. Previous works  Specifically, for the selected triplet [ I r0 , I t , I r1 ], two warped frames [ I t0 , I t1 ] are generated from two reference frames. To obtain the frame-level representations, we used the encoder of DepthNet to extract the feature maps of the three frames [ I t0 , I t , I t1 ] respectively. The feature maps are upsampled to the size of the original image, denoted by [F r0 , F t , F r1 ]. As in Eq.(  where N (a,b) (•) normalizes the input to the range [a, b]."
2.4,Overall Loss,"The final loss of TCL L t is formulated as follows: where denotes that the Triplet Mask MT i (i = 0, 1) is applied to the photometric loss calculation of the two reference frames respectively. The final photometric loss is obtained by averaging the photometric losses of the two reference frames after applying MT i (i = 0, 1). λ d , λ p are weights of L dc and L pc . Since the early adoption of L dc and L pc may lead to overfitting, the DepthNet and PoseNet are warmed up with N w epochs before adding the two loss functions. The synthesis method may inherently generate invalid (black) areas in the augmented samples. This arises from the single-image-based augmentation process, which lacks the additional information to fill the new areas generated from the viewpoint transformation. The invalid regions should be masked in the related loss functions."
3,Experiments,"Dataset and Implementation Details. The public datasets, including SCARED "
,Results.,"To evaluate the effectiveness of the proposed method, TCL was applied to MonoDepth2  Table  Ablations on Proposed Modules. We introduce two proposed modules to the baseline (MonoDepth2) separately. We additionally propose a simple version of AiC that computes Triplet Masks directly using two reference frames without warping, denoted as AiC*. From Table  Ablations on Different Dataset Amounts. To verify the effect of our proposed method on different amounts of training and validation sets, we utilize the main depth metric RMSE and pose metric ATE for comparison. In Fig. "
4,Conclusion,"We present a triplet-consistency-learning framework (TCL) to improve the effect of monocular endoscopy unsupervised depth and pose estimation. The GC module generates synthesis triplets to increase the diversity of the endoscopic samples. Furthermore, we constrain the depth and pose consistency using two loss functions. The AiC module generates Triplet Mask(MT) based on the triplet information. MT can effectively mask the appearance inconsistency in the triplet, which leads to more efficient training of the photometric loss. Extensive experiments demonstrate the effectiveness of TCL, which can be easily embedded into various SfM methods without additional model parameters."
,Fig. 1 .,
,Fig. 2 .,
,Table 2 .Fig. 3 .,
,,
,Table 1 .,
1,Introduction,"Colorectal cancer (CRC) is the third most commonly diagnosed cancer but ranks second in terms of mortality worldwide  Many researchers employ U-shaped network  Unfortunately, limited field of view and illumination variations usually result in insufficient boundary contrast between intestinal lesions and their surrounding tissues. On the other hand, various polyps and adenomas with different pathological features have similar visual characteristics to intestinal folds. To address these issues mentioned above, we explore a new deep learning architecture called cascade transformer encoded boundary-aware multibranch fusion (CTBMF) networks with cascade transformers and multibranch fusion for polyp and adenoma segmentation in colonoscopic white-light and narrow-band video images. Several technical highlights of this work are summarized as follows. First, we construct cascade transformers that can extract global semantic and subtle boundary features at different resolutions and establish weighted links between global semantic cues and local spatial ones for intermediate reasoning, providing long-range dependencies and a global receptive field for pixel-level segmentation. Next, a hybrid spatial-frequency loss function is defined to compensate for loss features in the spatial domain but available in the frequency domain. Additionally, we built a new colonoscopic lesion image database and will make it publicly available, while this work also conducts a thorough evaluation and comparison on our new database and four publicly available ones (Fig. "
2,Approaches,"This section details our CTBMF networks that can refine inaccurate lesion location, rough or blurred boundaries, and unclear textures. Figure "
2.1,Transformer Cascade Encoding,"This work employs a pyramid transformer  where Ω(•) denotes the output parameters of position embedding, ⊕ is the element-wise addition, W Ki indicates the parameters that reduces the dimension of K i or V i , and R i is the reduction ratio of the attention layers at stage i. As the output of LSR is fed into multihead attention, we can obtain attention feature map A j i from head j (j = 1, 2, • • • , N, N is the head number of the attention layer) at stage i: where Attention(•) is calculated as the original transformer  where is the concatenation and W Ai is the linear projection parameters. Then, where DC is a 3 × 3 depth-wise convolution "
2.2,Boundary-Aware Multibranch Fusion Decoding,"Boundary-Aware Attention Module. Current methods  where ⊗ indicates the elementwise product. Subsequently, the detail enhanced feature map Z i of the channel-spatial attention is where SMP indicates spatial maxpooling. We subtract the feature map Xi from the enhanced map Z i to obtain the augmented boundary attention map B i , and also establish the correlation between the neighbor layers X i+1 and X i to generate multilevel boundary map G i : where and US indicate subtraction and upsampling. Residual Multibranch Fusion Module. To highlight salient regions and suppress task-independent feature responses (e.g., blurring), we linearly aggregate B i and G i to generate discriminative boundary attention map D i : where i = 1, 2, 3 and RELU is the rectified linear unit function. We obtain the fused feature representation map M i (i = 1, 2, 3, 4) from the elementwise addition or summation of M i+1 , D i , and the residual feature Xi by Eventually, the output M 1 of the boundary-aware multibranch fusion decoder is represented by the following equation: which precisely combines global semantic features with boundary or appearance details of colorectal lesions."
2.3,Hybrid Spatial-Frequency Loss,"This work proposes a hybrid spatial-frequency loss function H L to train our network architecture for colorectal polyp and adenoma segmentation: where S L and F L are a spatial-domain loss and a frequency-domain loss to calculate the total difference between prediction P and ground truth G, respectively. The spatial-domain loss S L consists of a weighted intersection over union loss and a weighted binary cross entropy loss  The frequency-domain loss F L can be computed by  where W × H is the image size, λ is the coefficient of F L , G(u, v) and P(u, v) are a frequency representation of ground truth G and prediction P using 2-D discrete Fourier transform. γ(u, v) is a spectrum weight matrix that is dynamically determined by a non-uniform distribution on the current loss of each frequency."
3,Experiments,"Our clinical in-house colonoscopic videos were acquired from various colonoscopic procedures under a protocol approved by the research ethics committee of the university. These white-light and narrow-band colonoscopic images contain four types of colorectal lesions with different pathological features classified by surgeons: (1) 268 cases of hyperplastic polyp, (2) 815 cases of inflammatory polyp, (3) 1363 cases of tubular adenoma, and (4) 143 cases of tubulovillous adenoma. Additionally, four public datasets including Kvasir, ETIS-LaribPolypDB, CVC-ColonDB, and CVC-ClinicDB were also used to evaluate our network model. We implemented CTBMF on PyTorch and trained it with a single NVIDIA RTX3090 to accelerate the calculations for 100 epochs at mini-batch size 16. Factors λ (Eq. ( "
4,Results and Discussion,"Figure  Figure  Our method generally works better than the other three methods. Several reasons are behind this. First, the cascade-transformer encoder can extract local and global semantic features of colorectal lesions with different pathological characteristics due to its pyramid representation and linear spatial reduction attention. While the pyramid operation extracts multiscale local features, the attention mechanism builds global semantic cues. Both pyramid and attention strategies facilitate the representation of small and textureless intestinal lesions in encoding, enabling to characterize the difference between intestinal folds (linings) and subtle-texture polyps or small adenomas. Next, the boundary-aware attention mechanism drives the multibranch fusion, enhancing the representation of intestinal lesions in weak boundary and nonuniform lighting. Such a mechanism first extracts the channel-spatial attention feature map, from which subtracts the current pyramid transformer's feature map to enhance the boundary information. Also, the multibranch fusion generates multilevel boundary maps by subtracting the next pyramid transformer's upsampling output from the current pyramid transformer's output, further improving the boundary contrast. Additionally, the hybrid spatial-frequency loss was also contributed to the improvement of colorectal lesion segmentation. The frequency-domain information can compensate loss feature information in the spatial domain, leading to a better supervision in training."
5,Conclusion,"This work proposes a new deep learning model of cascade pyramid transformer encoded boundary-aware multibranch fusion networks to automatically segment different colorectal lesions of polyps and adenomas in colonoscopic imaging. While such an architecture employs simple and convolution-free cascade transformers as an encoder to effectively and accurately extract global semantic features, it introduces a boundary-aware attention multibranch fusion module as a decoder to preserve local and global features and enhance structural and boundary information of polyps and adenomas, as well as it uses a hybrid spatialfrequency loss function for training. The thorough experimental results show that our method outperforms the current segmentation models without any preprocessing. In particular, our method attains much higher accuracy on colonoscopic images with small, illumination changes, weak-boundary, textureless, and motion blurring lesions, improving the average dice similarity coefficient and intersection over union from (89.5%, 84.1%) to (90.3%, 84.4%) on our in-house database, from (78.9%, 72.6%) to (83.4%, 76.5%) on the four public databases, and from (84.3%, 78.4%) to (87.0%, 80.5%) on the five databases."
,Fig. 1 .,
,Fig. 2 .,
,Fig. 3 .,
,Fig. 4 .,
,Fig. 5 .,
,Table 1 .,
,Table 2 .,
1,Introduction,"Automation in robot-assisted minimally invasive surgery (RMIS) may reduce human error that is linked to fatigue, lack of attention and cognitive overload  The assumption that surgical tools remain centrally is, however, simplistic, as in many cases the surgeon may want to observe the surrounding anatomy to decide their next course of action. Contrary to rule-based approaches, data-driven methods are capable to capture more complex control policies. Example data-driven methods suitable for camera motion automation include reinforcement learning (RL) and imitation learning  Recent efforts to make vast amounts of laparoscopic intervention videos publicly available  In this work, we build on "
2,Materials and Methods,The proposed approach to learning camera motion prediction is summarized in Fig. 
2.1,Theoretical Background,"Points on a plane, as observed from a moving camera, transform by means of the 3 × 3 projective homography matrix G in image space. Thus, predicting future camera motion (up to scale) may be equivalently treated as predicting future projective homographies. It has been shown in  is better behaved for deep learning applications than the 3 × 3 matrix representation of a homography. Therefore, in this work, we treat camera motion C as a sequence of four point homographies on a time horizon [T 0 , T N +M ), N being the recall horizon's length, M being the preview horizon's length. Time points lie Δt apart, that is T i+1 = T i + Δt. For image sequences of length N+M, we work with four point homography sequences"
2.2,Data and Data Preparation,"Three datasets are curated to train and evaluate the proposed method: two cholecystectomy datasets (laparoscopic gallbladder removal), namely Cholec80  To remove status indicator overlays from the laparoscopic videos, which may hinder the camera motion estimator, we identify the bounding circle of the circular field of view using  All three datasets are split into training, validation, and testing datasets. We split the videos by frame count into 80 ± 1% training and 20 ± 1% testing. Training and testing videos never intersect. We repeat this step to further split the training dataset into (pure) training and validation datasets. Due to errors during processing the raw data, we exclude videos 19, 21, and 23 from HeiChole, as well as videos 22, 40, 65, and 80 from Cholec80. This results in dataset sizes of: Cholec80 -4.4e6 frames at 25 fps, HeiChole -9.5e5 frames at 25 fps, and AutoLaparo -7.1e4 frames at 25 fps."
2.3,Proposed Pipeline,"Video Database and Importance Sampling. The curated data from Sect. 2.2 is accumulated into a video database. Image sequences of length N + M are sampled at a frame increment of Δn between subsequent frames and with Δc frames between the sequence's initial frames. Prior to adding the videos to the database, an initial offline run is performed to estimate camera motion Δuv between the frames. This creates image-motion correspondences of the form (I n , I n+Δn , Δuv n ). Image-motion correspondences where E(||Δuv n || 2 ) > σ, with sigma being the standard deviation over all motions in the respective dataset, define anchor indices n. Image sequences are sampled such that the last image in the recall horizon lies at index n = N -1, marking the start of a motion. The importance sampling samples indices from the intersection of all anchor indices, shifted by -N , with all possible starting indices for image sequences. Geometric and Photometric Transforms. The importance sampled image sequences are fed to a data augmentation stage. This stage entails geometric and photometric transforms. The distinction is made because downstream, the pipeline is split into two branches. The upper branch serves as camera motion prediction whereas the lower branch serves as camera motion estimation, also refer to the next section. As it acts as the source of pseudo-ground-truth, it is crucial that the camera motion estimator performs under optimal conditions, hence no photometric transforms, i.e. transforms that change brightness/contrast/fog etc., are applied. Photometrically transformed images shall further be denoted as Ĩ. To encourage same behavior under different perspectives, geometric transforms are applied, i.e. transforms that change orientation/up to down/left to right etc. Transforms are always sampled randomly, and applied consistently to the entire image sequence. "
,Camera Motion,
3,Experiments and Evaluation Methodology,The following two sections elaborate the experiments we conduct to investigate the proposed pipeline from Fig. 
3.1,Camera Motion Estimator,"Camera Motion Distribution. To extract the camera motion distribution, we run the camera motion estimator from  Online Camera Motion Estimation. Since the camera motion estimator is executed online, memory footprint and computational efficiency are of importance. Therefore, we evaluate the estimator from "
3.2,Camera Motion Predictor,"Model Architecture. For all experiments, the camera motion predictor is a ResNet-18/34/50, with the number of input features equal to the recall horizon N × 3 (RGB), where N = 14. We set the preview horizon M = 1. The frame increment is set to 0.25 s, or Δn = 5 for the 25 fps videos. The number of frames between clips is also set to 0.25 s, or Δc = 5. Training Details. The camera motion predictor is trained on each dataset from Sect. 2.2 individually. For training on Cholec80/HeiChole/AutoLaparo, we run 80/50/50 epochs on a batch size of 64 with a learning rate of 2.5e -5/1.e -4/1.e -4. The learning rates for Cholec80 and HeiChole relate approximately to the dataset's training sizes, see Table  Evaluation Metrics. For evaluation we compute the mean pairwise distance between estimated and predicted motion E(||Δ ũv t -Δuv t || 2 ). All camera motion predictors are benchmarked against a baseline, that is a O(1)/O(2)-Taylor expansion of the estimated camera motion Δuv t . Furthermore, the model that is found to perform best is evaluated on the multi-class labels (left, right, up, down) that are provided in AutoLaparo."
4,Results,
4.1,Camera Motion Estimator,Camera Motion Distribution. The camera motion distributions for all datasets are shown in Fig.  Online Camera Motion Estimation. The results of the online camera motion estimation are summarized in Table 
4.2,Camera Motion Prediction,The camera motion prediction results for all datasets are highlighted in Table 
5,Conclusion and Outlook,"To the best of our knowledge, this work is the first to demonstrate that camera motion can indeed be learned from retrospective videos of laparoscopic interventions, with no manual annotation. Self-supervision is achieved by harvesting image-motion correspondences using a camera motion estimator, see Fig.  A current limitations of this work is the preview horizon M of length 1. One might want to extend it for model predictive control. Furthermore, to improve explainability to the surgeon, but also to improve the prediction in general, it would be beneficial to include auxiliary tasks, e.g. tool and organ segmentation, surgical phase recognition, and audio. There also exist limitations for the camera motion estimator. The utilized camera motion estimator is efficient and isolates object motion well from camera motion, but is limited to relatively small camera motions. Improving the camera motion estimator to large camera motions would help increase the preview horizon M . In future work, we will execute this model in a real setup for investigating transferability. This endeavor is backed by "
,Fig. 1 .,
,,
,( a ),
,Fig.,
,Fig. 4 .,
,Table 1 .,
,Table 2 .,
,,
