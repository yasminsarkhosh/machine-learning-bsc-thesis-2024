{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import fitz  # PyMuPDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path to folder where output files will be stored\n",
    "output_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/database_analysis_output/'\n",
    "\n",
    "# Base path to folders \n",
    "base_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/'\n",
    "\n",
    "# Path to the MICCAI 2023 pdfs\n",
    "pdf_path = base_path + 'miccai_2023/'\n",
    "\n",
    "# Path to the MICCAI 2023 database of all 730 papers and their metadata\n",
    "database_path = base_path + 'databases/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miccai = pd.read_csv(database_path +'database_miccai_2023.csv', index_col=[0], header=[0], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 731 entries, 0 to 730\n",
      "Data columns (total 6 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Title                731 non-null    object\n",
      " 1   Authors              731 non-null    object\n",
      " 2   Page numbers         731 non-null    object\n",
      " 3   DOI                  731 non-null    object\n",
      " 4   Year of publication  731 non-null    int64 \n",
      " 5   Part of publication  731 non-null    int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 40.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_miccai.info()\n",
    "\n",
    "#731 entries, 0 to 730 \n",
    "#6 columns in total\n",
    "#title, authors, page numbers, doi, year of publication, part of publication\n",
    "#dtype: int64(2), object(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a total of 730 papers in MICCAI 2023. However, the dataframe contains 731. Examining the dataframe, \n",
    "I will first look into the number of papers by publication (Part of Publication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Page numbers</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Year of publication</th>\n",
       "      <th>Part of publication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PET-Diffusion: Unsupervised PET Enhancement Ba...</td>\n",
       "      <td>Caiwen Jiang, Yongsheng Pan, Mianxin Liu, Lei ...</td>\n",
       "      <td>3-12</td>\n",
       "      <td>10.1007/978-3-031-43907-0_1</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MedIM: Boost Medical Image Representation via ...</td>\n",
       "      <td>Yutong Xie, Lin Gu, Tatsuya Harada, Jianpeng Z...</td>\n",
       "      <td>13-23</td>\n",
       "      <td>10.1007/978-3-031-43907-0_2</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UOD: Universal One-Shot Detection of Anatomica...</td>\n",
       "      <td>Heqin Zhu, Quan Quan, Qingsong Yao, Zaiyi Liu,...</td>\n",
       "      <td>24-34</td>\n",
       "      <td>10.1007/978-3-031-43907-0_3</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S2^2ME: Spatial-Spectral Mutual Teaching and E...</td>\n",
       "      <td>An Wang, Mengya Xu, Yang Zhang, Mobarakol Isla...</td>\n",
       "      <td>35-45</td>\n",
       "      <td>10.1007/978-3-031-43907-0_4</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Modularity-Constrained Dynamic Representation ...</td>\n",
       "      <td>Qianqian Wang, Mengqi Wu, Yuqi Fang, Wei Wang,...</td>\n",
       "      <td>46-56</td>\n",
       "      <td>10.1007/978-3-031-43907-0_5</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  PET-Diffusion: Unsupervised PET Enhancement Ba...   \n",
       "1  MedIM: Boost Medical Image Representation via ...   \n",
       "2  UOD: Universal One-Shot Detection of Anatomica...   \n",
       "3  S2^2ME: Spatial-Spectral Mutual Teaching and E...   \n",
       "4  Modularity-Constrained Dynamic Representation ...   \n",
       "\n",
       "                                             Authors Page numbers  \\\n",
       "0  Caiwen Jiang, Yongsheng Pan, Mianxin Liu, Lei ...         3-12   \n",
       "1  Yutong Xie, Lin Gu, Tatsuya Harada, Jianpeng Z...        13-23   \n",
       "2  Heqin Zhu, Quan Quan, Qingsong Yao, Zaiyi Liu,...        24-34   \n",
       "3  An Wang, Mengya Xu, Yang Zhang, Mobarakol Isla...        35-45   \n",
       "4  Qianqian Wang, Mengqi Wu, Yuqi Fang, Wei Wang,...        46-56   \n",
       "\n",
       "                           DOI  Year of publication  Part of publication  \n",
       "0  10.1007/978-3-031-43907-0_1                 2023                    1  \n",
       "1  10.1007/978-3-031-43907-0_2                 2023                    1  \n",
       "2  10.1007/978-3-031-43907-0_3                 2023                    1  \n",
       "3  10.1007/978-3-031-43907-0_4                 2023                    1  \n",
       "4  10.1007/978-3-031-43907-0_5                 2023                    1  "
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_miccai.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PET-Diffusion: Unsupervised PET Enhancement Based on\\xa0the\\xa0Latent Diffusion Model',\n",
       " 'MedIM: Boost Medical Image Representation via\\xa0Radiology Report-Guided Masking',\n",
       " 'UOD: Universal One-Shot Detection of\\xa0Anatomical Landmarks',\n",
       " 'S2^2ME: Spatial-Spectral Mutual Teaching and\\xa0Ensemble Learning for\\xa0Scribble-Supervised Polyp Segmentation',\n",
       " 'Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI']"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_list_2023 = df_miccai['Title'].tolist()\n",
    "title_list_2023[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Publication 1: 73\n",
      "Number of papers in Publication 2: 74\n",
      "Number of papers in Publication 3: 72\n",
      "Number of papers in Publication 4: 75\n",
      "Number of papers in Publication 5: 76\n",
      "Number of papers in Publication 6: 77\n",
      "Number of papers in Publication 7: 75\n",
      "Number of papers in Publication 8: 65\n",
      "Number of papers in Publication 9: 70\n",
      "Number of papers in Publication 10: 74\n",
      "Total number of papers: 731\n"
     ]
    }
   ],
   "source": [
    "# count the number of papers for each publication. There is 10 publications in total\n",
    "\n",
    "print('Number of papers in Publication 1:', df_miccai['Part of publication'].value_counts()[1]) #73\n",
    "print('Number of papers in Publication 2:', df_miccai['Part of publication'].value_counts()[2]) #74\n",
    "print('Number of papers in Publication 3:', df_miccai['Part of publication'].value_counts()[3]) #72\n",
    "print('Number of papers in Publication 4:', df_miccai['Part of publication'].value_counts()[4]) #75\n",
    "print('Number of papers in Publication 5:', df_miccai['Part of publication'].value_counts()[5]) #76\n",
    "print('Number of papers in Publication 6:', df_miccai['Part of publication'].value_counts()[6]) #77\n",
    "print('Number of papers in Publication 7:', df_miccai['Part of publication'].value_counts()[7]) #75\n",
    "print('Number of papers in Publication 8:', df_miccai['Part of publication'].value_counts()[8]) #65\n",
    "print('Number of papers in Publication 9:', df_miccai['Part of publication'].value_counts()[9]) #70\n",
    "print('Number of papers in Publication 10:', df_miccai['Part of publication'].value_counts()[10]) #74\n",
    "\n",
    "# count the total number of papers in the dataframe\n",
    "print('Total number of papers:', df_miccai['Part of publication'].value_counts().sum()) #731"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe: papers with cancer in their title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "# selecting papers with cancer in the title, add them into a new dataframe\n",
    "df_cancer = df_miccai.loc[df_miccai.Title.str.contains('Cancer', regex=False, na=False)]\n",
    "\n",
    "print(len(df_cancer)) # 23 papers with cancer in the title\n",
    "\n",
    "# saving dataframe for papers with cancer in the title\n",
    "df_cancer.to_csv(database_path +'database_miccai_2023_cancer.csv', index=True, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list_cancer = df_cancer[\"Title\"]\n",
    "author_list_cancer = df_cancer[\"Authors\"]\n",
    "\n",
    "author_list_cancer = author_list_cancer.to_list()\n",
    "title_list_cancer = title_list_cancer.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(author_list_cancer)) #23\n",
    "print(len(title_list_cancer))  #23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Selecting a scope of papers from MICCAI 2023**\n",
    "\n",
    "**Scope criteria:** Selecting papers, that researched within the field of cancer-related illnesses by searching for cancer-related keywords in the text of each research paper. The text is defined from the start of Abstraction ending with the last line of Conclusion, exluding the Title of the paper, the authors and affiliations, the Acknowlegdement and the References. \n",
    "\n",
    "Cancer-related keywords could be words such as 'cancer', 'tumor' and/or 'tumours'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted titles from 189 selected papers.\n",
      "With the keyword(s) being ['cancer'], 189 papers were selected as relevant to cancer research.\n"
     ]
    }
   ],
   "source": [
    "# Function to extract the full text from the PDF\n",
    "def extract_text(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "            full_text += page.get_text()\n",
    "    return full_text\n",
    "\n",
    "# Function to find if any of the keywords appear in the section between Abstract and Conclusion\n",
    "def find_keywords_section(full_text, keywords):\n",
    "     # Use regular expressions to find the end of the affiliations section\n",
    "    affiliations_end = re.search(r'\\d{1,2}\\s+(?:\\w+\\.)+@\\w+\\.\\w{2,}', full_text)\n",
    "    \n",
    "    # Start searching from the end of affiliations if found, otherwise from the start of the text\n",
    "    start_idx = affiliations_end.end() if affiliations_end else 0\n",
    "    \n",
    "    # Look for the Abstract and Conclusion sections\n",
    "    abstract_idx = full_text.lower().find(\"abstract\", start_idx)\n",
    "    conclusion_idx = full_text.lower().rfind(\"conclusion\", abstract_idx)\n",
    "    acknowledgements_idx = full_text.lower().find(\"acknowledgements\", conclusion_idx)\n",
    "    \n",
    "    # Adjust the end index to stop at Acknowledgements if it exists, otherwise use Conclusion index\n",
    "    end_search_idx = acknowledgements_idx if acknowledgements_idx != -1 else conclusion_idx\n",
    "    \n",
    "    # If neither Abstract nor Conclusion is found, search the entire text\n",
    "    if abstract_idx == -1 and conclusion_idx == -1:\n",
    "        searchable_text = full_text[start_idx:]\n",
    "    else:\n",
    "        # Search from Abstract to Conclusion or Acknowledgements\n",
    "        searchable_text = full_text[abstract_idx:end_search_idx].lower()\n",
    "    \n",
    "    # Search for each keyword within the determined section, stop at first match\n",
    "    for keyword in keywords:\n",
    "        if keyword.lower() in searchable_text:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Function to extract the title from the PDF\n",
    "def extract_title(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        first_page_text = doc[0].get_text(\"text\")\n",
    "        \n",
    "        # Regular expression to find the start of affiliations or author names\n",
    "        # Looks for sequences typical in author lists or affiliations, such as numbers and parentheses\n",
    "        author_or_affiliations_start = re.search(r'\\b[A-Z][a-z]+ [A-Z]\\.|\\b[A-Z][a-z]+\\s[A-Z][a-z]+[1-9]', first_page_text)\n",
    "\n",
    "        title = \"\"\n",
    "        if author_or_affiliations_start:\n",
    "            # Extract text up to the start of the author list or affiliations as potential title text\n",
    "            potential_title_text = first_page_text[:author_or_affiliations_start.start()].strip()\n",
    "            title_lines = potential_title_text.split('\\n')\n",
    "            \n",
    "            # The title is expected to be a continuous block of text at the top of the page,\n",
    "            # possibly after a journal header or similar. We look for a large continuous block of text.\n",
    "            for line in reversed(title_lines):\n",
    "                if line.strip():  # Non-empty line suggests part of the title\n",
    "                    # Prepend to keep the title in the correct order\n",
    "                    title = line + \" \" + title\n",
    "                else:\n",
    "                    # An empty line might indicate the end of the title block\n",
    "                    break\n",
    "        else:\n",
    "            # If no author list or affiliation section is identified, use the first non-empty line\n",
    "            for line in first_page_text.split('\\n'):\n",
    "                if line.strip():\n",
    "                    title = line\n",
    "                    break\n",
    "\n",
    "        title = title.strip()  # Clean up whitespace\n",
    "        return title\n",
    "\n",
    "selected_papers = []\n",
    "titles = []\n",
    "\n",
    "# List of keywords to search for\n",
    "keywords = [\"cancer\"]\n",
    "\n",
    "# Iterate over each volume and search for keywords\n",
    "for i in range(1, 11):  # Volumes 1 to 10\n",
    "    folder_name = f\"miccai23vol{i}\"\n",
    "    folder_path = os.path.join(pdf_path, folder_name)\n",
    "    \n",
    "    for pdf in os.listdir(folder_path):\n",
    "        if pdf.endswith(\".pdf\"):\n",
    "            pdf_path_ = os.path.join(folder_path, pdf)\n",
    "            full_text = extract_text(pdf_path_)\n",
    "            if find_keywords_section(full_text, keywords):\n",
    "                selected_papers.append(os.path.join(folder_name, pdf))\n",
    "\n",
    "# Extract titles from selected papers\n",
    "for paper_path in selected_papers:\n",
    "    full_paper_path = os.path.join(pdf_path, paper_path)\n",
    "    title = extract_title(full_paper_path)\n",
    "    titles.append(title)\n",
    "\n",
    "print(f\"Extracted titles from {len(titles)} selected papers.\")\n",
    "print(f\"With the keyword(s) being {keywords}, {len(selected_papers)} papers were selected as relevant to cancer research.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the selected papers and their paths to a CSV file\n",
    "selected_papers_path = pd.DataFrame({\"Path\": selected_papers})\n",
    "selected_papers_path.to_csv(output_path + 'selected_papers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extracted titles to a CSV file\n",
    "titles = pd.DataFrame({'Title': titles})\n",
    "titles\n",
    "titles.to_csv(output_path + 'titles.csv', index=False, header=True, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Geometry-Invariant Abnormality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3D Arterial Segmentation via Single 2D Project...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TPRO: Text-Prompting-Based Weakly Supervised H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vox2vec: A Framework for Self-supervised Contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pick the Best Pre-trained Model: Towards Trans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Solving Low-Dose CT Reconstruction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Noise2Aliasing: Unsupervised Deep Learning for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Revealing Anatomical Structures in PET to Gene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Geometric Ultrasound Localization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>FreeSeed: Frequency-Band-Aware and Self-guided...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title\n",
       "0                       Geometry-Invariant Abnormality\n",
       "1    3D Arterial Segmentation via Single 2D Project...\n",
       "2    TPRO: Text-Prompting-Based Weakly Supervised H...\n",
       "3    vox2vec: A Framework for Self-supervised Contr...\n",
       "4    Pick the Best Pre-trained Model: Towards Trans...\n",
       "..                                                 ...\n",
       "184                 Solving Low-Dose CT Reconstruction\n",
       "185  Noise2Aliasing: Unsupervised Deep Learning for...\n",
       "186  Revealing Anatomical Structures in PET to Gene...\n",
       "187                  Geometric Ultrasound Localization\n",
       "188  FreeSeed: Frequency-Band-Aware and Self-guided...\n",
       "\n",
       "[189 rows x 1 columns]"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the datasets\n",
    "titles_df = pd.read_csv(output_path + 'titles.csv')\n",
    "database_df = df_miccai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a search pattern from the first two words of each title\n",
    "titles_df['search_pattern'] = titles_df['Title'].str.split().str.slice(0, 2).str.join(' ').str.lower()\n",
    "\n",
    "# Find the rows in the database that match the search pattern\n",
    "matched_rows = []\n",
    "for pattern in titles_df['search_pattern']:\n",
    "    matched_rows.extend(database_df[database_df['Title'].str.lower().str.startswith(pattern)].index.tolist())\n",
    "matched_df = database_df.loc[matched_rows]\n",
    "\n",
    "# Save the matched papers and the search pattern to CSV files\n",
    "matched_df.to_csv(output_path + 'matched_papers.csv', index=False) #187 rows, missing two papers\n",
    "titles_df.to_csv(output_path + 'search_pattern.csv') # 189 titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_df = pd.read_csv(output_path + 'matched_papers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard coding: adding the two missing papers into the dataframe \"matched_df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Page numbers</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Year of publication</th>\n",
       "      <th>Part of publication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PET-Diffusion: Unsupervised PET Enhancement Ba...</td>\n",
       "      <td>Caiwen Jiang, Yongsheng Pan, Mianxin Liu, Lei ...</td>\n",
       "      <td>3-12</td>\n",
       "      <td>10.1007/978-3-031-43907-0_1</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S2^2ME: Spatial-Spectral Mutual Teaching and E...</td>\n",
       "      <td>An Wang, Mengya Xu, Yang Zhang, Mobarakol Isla...</td>\n",
       "      <td>35-45</td>\n",
       "      <td>10.1007/978-3-031-43907-0_4</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  PET-Diffusion: Unsupervised PET Enhancement Ba...   \n",
       "3  S2^2ME: Spatial-Spectral Mutual Teaching and E...   \n",
       "\n",
       "                                             Authors Page numbers  \\\n",
       "0  Caiwen Jiang, Yongsheng Pan, Mianxin Liu, Lei ...         3-12   \n",
       "3  An Wang, Mengya Xu, Yang Zhang, Mobarakol Isla...        35-45   \n",
       "\n",
       "                           DOI  Year of publication  Part of publication  \n",
       "0  10.1007/978-3-031-43907-0_1                 2023                    1  \n",
       "3  10.1007/978-3-031-43907-0_4                 2023                    1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Locating the missing papers in the original database for all 730 papers \n",
    "missing_papers = df_miccai.loc[df_miccai['Title'].isin(\n",
    "    ['PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model', \n",
    "     'S2^2ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation'])]\n",
    "\n",
    "missing_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the matched and missing papers into a single dataframe\n",
    "selected_papers_df = pd.merge(matched_df, missing_papers, on='Title', how='outer')\n",
    "\n",
    "# Drop the duplicate columns from the merge\n",
    "#selected_papers_df.drop(columns=['Part of publication_y', 'Authors_y', 'Page numbers_y', 'DOI_y', 'Year of publication_y'], inplace=True)\n",
    "#selected_papers_df.drop(columns=['Page numbers_y', 'DOI_y', 'Year of publication_y'], inplace=True)\n",
    "\n",
    "# Rename/refine the columns to match the original database \n",
    "#selected_papers_df.rename(columns={'Title': 'title', 'Part of publication_x': 'vol_number', 'Authors_x': 'authors', \n",
    "#                                   'DOI_x': 'doi', 'Year of publication_x': 'publication_year',\n",
    "#                                   'Page numbers_x': 'page_numbers'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the selected papers to a CSV file\n",
    "selected_papers_df.to_csv(output_path + 'selected_papers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>page_numbers</th>\n",
       "      <th>doi</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>vol_number</th>\n",
       "      <th>Authors_y</th>\n",
       "      <th>Page numbers_y</th>\n",
       "      <th>DOI_y</th>\n",
       "      <th>Year of publication_y</th>\n",
       "      <th>Part of publication_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3D Arterial Segmentation via Single 2D Project...</td>\n",
       "      <td>Alina F. Dima, Veronika A. Zimmer, Martin J. M...</td>\n",
       "      <td>141-151</td>\n",
       "      <td>10.1007/978-3-031-43907-0_14</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3D Mitochondria Instance Segmentation with Spa...</td>\n",
       "      <td>Omkar Thawakar, Rao Muhammad Anwer, Jorma Laak...</td>\n",
       "      <td>613-623</td>\n",
       "      <td>10.1007/978-3-031-43993-3_59</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Spatial-Temporal Deformable Attention Based ...</td>\n",
       "      <td>Chao Qin, Jiale Cao, Huazhu Fu, Rao Muhammad A...</td>\n",
       "      <td>479-488</td>\n",
       "      <td>10.1007/978-3-031-43895-0_45</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Spatial-Temporally Adaptive PINN Framework f...</td>\n",
       "      <td>Yubo Ye, Huafeng Liu, Xiajun Jiang, Maryam Tol...</td>\n",
       "      <td>163-172</td>\n",
       "      <td>10.1007/978-3-031-43990-2_16</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Texture Neural Network to Predict the Abnorm...</td>\n",
       "      <td>Weiguo Cao, Benjamin Howe, Nicholas Rhodes, Su...</td>\n",
       "      <td>470-480</td>\n",
       "      <td>10.1007/978-3-031-43993-3_46</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>WeakPolyp: You only Look Bounding Box for Poly...</td>\n",
       "      <td>Jun Wei, Yiwen Hu, Shuguang Cui, S. Kevin Zhou...</td>\n",
       "      <td>757-766</td>\n",
       "      <td>10.1007/978-3-031-43898-1_72</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Weakly-Supervised Positional Contrastive Learn...</td>\n",
       "      <td>Emma Sarfati, Alexandre Bône, Marc-Michel Rohé...</td>\n",
       "      <td>227-237</td>\n",
       "      <td>10.1007/978-3-031-43907-0_22</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>X2Vision: 3D CT Reconstruction from Biplanar X...</td>\n",
       "      <td>Alexandre Cafaro, Quentin Spinat, Amaury Leroy...</td>\n",
       "      <td>699-709</td>\n",
       "      <td>10.1007/978-3-031-43999-5_66</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>YONA: You Only Need One Adjacent Reference-Fra...</td>\n",
       "      <td>Yuncheng Jiang, Zixun Zhang, Ruimao Zhang, Gua...</td>\n",
       "      <td>44-54</td>\n",
       "      <td>10.1007/978-3-031-43904-9_5</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>vox2vec: A Framework for Self-supervised Contr...</td>\n",
       "      <td>Mikhail Goncharov, Vera Soboleva, Anvar Kurmuk...</td>\n",
       "      <td>605-614</td>\n",
       "      <td>10.1007/978-3-031-43907-0_58</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    3D Arterial Segmentation via Single 2D Project...   \n",
       "1    3D Mitochondria Instance Segmentation with Spa...   \n",
       "2    A Spatial-Temporal Deformable Attention Based ...   \n",
       "3    A Spatial-Temporally Adaptive PINN Framework f...   \n",
       "4    A Texture Neural Network to Predict the Abnorm...   \n",
       "..                                                 ...   \n",
       "184  WeakPolyp: You only Look Bounding Box for Poly...   \n",
       "185  Weakly-Supervised Positional Contrastive Learn...   \n",
       "186  X2Vision: 3D CT Reconstruction from Biplanar X...   \n",
       "187  YONA: You Only Need One Adjacent Reference-Fra...   \n",
       "188  vox2vec: A Framework for Self-supervised Contr...   \n",
       "\n",
       "                                               authors page_numbers  \\\n",
       "0    Alina F. Dima, Veronika A. Zimmer, Martin J. M...      141-151   \n",
       "1    Omkar Thawakar, Rao Muhammad Anwer, Jorma Laak...      613-623   \n",
       "2    Chao Qin, Jiale Cao, Huazhu Fu, Rao Muhammad A...      479-488   \n",
       "3    Yubo Ye, Huafeng Liu, Xiajun Jiang, Maryam Tol...      163-172   \n",
       "4    Weiguo Cao, Benjamin Howe, Nicholas Rhodes, Su...      470-480   \n",
       "..                                                 ...          ...   \n",
       "184  Jun Wei, Yiwen Hu, Shuguang Cui, S. Kevin Zhou...      757-766   \n",
       "185  Emma Sarfati, Alexandre Bône, Marc-Michel Rohé...      227-237   \n",
       "186  Alexandre Cafaro, Quentin Spinat, Amaury Leroy...      699-709   \n",
       "187  Yuncheng Jiang, Zixun Zhang, Ruimao Zhang, Gua...        44-54   \n",
       "188  Mikhail Goncharov, Vera Soboleva, Anvar Kurmuk...      605-614   \n",
       "\n",
       "                              doi  publication_year  vol_number Authors_y  \\\n",
       "0    10.1007/978-3-031-43907-0_14            2023.0         1.0       NaN   \n",
       "1    10.1007/978-3-031-43993-3_59            2023.0         8.0       NaN   \n",
       "2    10.1007/978-3-031-43895-0_45            2023.0         2.0       NaN   \n",
       "3    10.1007/978-3-031-43990-2_16            2023.0         7.0       NaN   \n",
       "4    10.1007/978-3-031-43993-3_46            2023.0         8.0       NaN   \n",
       "..                            ...               ...         ...       ...   \n",
       "184  10.1007/978-3-031-43898-1_72            2023.0         3.0       NaN   \n",
       "185  10.1007/978-3-031-43907-0_22            2023.0         1.0       NaN   \n",
       "186  10.1007/978-3-031-43999-5_66            2023.0        10.0       NaN   \n",
       "187   10.1007/978-3-031-43904-9_5            2023.0         5.0       NaN   \n",
       "188  10.1007/978-3-031-43907-0_58            2023.0         1.0       NaN   \n",
       "\n",
       "    Page numbers_y DOI_y  Year of publication_y  Part of publication_y  \n",
       "0              NaN   NaN                    NaN                    NaN  \n",
       "1              NaN   NaN                    NaN                    NaN  \n",
       "2              NaN   NaN                    NaN                    NaN  \n",
       "3              NaN   NaN                    NaN                    NaN  \n",
       "4              NaN   NaN                    NaN                    NaN  \n",
       "..             ...   ...                    ...                    ...  \n",
       "184            NaN   NaN                    NaN                    NaN  \n",
       "185            NaN   NaN                    NaN                    NaN  \n",
       "186            NaN   NaN                    NaN                    NaN  \n",
       "187            NaN   NaN                    NaN                    NaN  \n",
       "188            NaN   NaN                    NaN                    NaN  \n",
       "\n",
       "[189 rows x 11 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the selected papers\n",
    "''' The papers were choosen by searching for papers with the keyword \"cancer\" in the title and in the text starting from the abstract and \n",
    "ending with the conclusion section of the paper'''\n",
    "\n",
    "selected_papers_df = pd.read_csv(output_path + 'selected_papers.csv')\n",
    "selected_papers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preliminary analysis of MICCAI 2023 - Selected papers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preprocessing and Data Extraction\n",
    "\n",
    "- **Objective**: Extract relevant information (e.g., mentions of demographic data, ethical considerations, methodologies for bias mitigation) from the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: PyMuPDF in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (1.23.26)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp39-cp39-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/85.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy) (68.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy) (23.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: click in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2023.12.25-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.22 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from PyMuPDF) (1.23.22)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.16.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Downloading spacy-3.7.4-cp39-cp39-macosx_11_0_arm64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp39-cp39-macosx_11_0_arm64.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.10-cp39-cp39-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp39-cp39-macosx_11_0_arm64.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.3-cp39-cp39-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached regex-2023.12.25-cp39-cp39-macosx_11_0_arm64.whl (291 kB)\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp39-cp39-macosx_11_0_arm64.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.3-cp39-cp39-macosx_11_0_arm64.whl (795 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m795.4/795.4 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, typer, tqdm, spacy-loggers, spacy-legacy, smart-open, regex, pydantic-core, murmurhash, langcodes, joblib, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, nltk, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 joblib-1.3.2 langcodes-3.3.0 murmurhash-1.0.10 nltk-3.8.1 preshed-3.0.9 pydantic-2.6.4 pydantic-core-2.16.3 regex-2023.12.25 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 tqdm-4.66.2 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n",
      "/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Setup and installation of the required packages\n",
    "#!pip install spacy nltk PyMuPDF\n",
    "#!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the paths to the selected papers\n",
    "selected_papers_paths = []\n",
    "for i in range(0, len(selected_papers)):  # Volumes 1 to 10\n",
    "    selected_papers_paths.append([base_path + 'miccai_2023/' + selected_papers[i]])\n",
    "\n",
    "# Check if the total number of paths is equal to the number of selected papers\n",
    "len(selected_papers_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: PDF Text Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the full text from the PDF\n",
    "def extract_text(pdf_path):\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "            full_text += page.get_text()\n",
    "            # Use regular expressions to find the end of the affiliations section\n",
    "            affiliations_end = re.search(r'\\d{1,2}\\s+(?:\\w+\\.)+@\\w+\\.\\w{2,}', full_text)\n",
    "            \n",
    "            # Start searching from the end of affiliations if found, otherwise from the start of the text\n",
    "            start_idx = affiliations_end.end() if affiliations_end else 0\n",
    "            \n",
    "            # Look for the Abstract and Conclusion sections\n",
    "            abstract_idx = full_text.lower().find(\"abstract\", start_idx)\n",
    "            conclusion_idx = full_text.lower().rfind(\"conclusion\", abstract_idx)\n",
    "            acknowledgements_idx = full_text.lower().find(\"acknowledgements\", conclusion_idx)\n",
    "            \n",
    "            # Adjust the end index to stop at Acknowledgements if it exists, otherwise use Conclusion index\n",
    "            end_search_idx = acknowledgements_idx if acknowledgements_idx != -1 else conclusion_idx\n",
    "            \n",
    "            # If neither Abstract nor Conclusion is found, search the entire text\n",
    "            if abstract_idx == -1 and conclusion_idx == -1:\n",
    "                searchable_text = full_text[start_idx:]\n",
    "            else:\n",
    "                # Search from Abstract to Conclusion or Acknowledgements\n",
    "                searchable_text = full_text[abstract_idx:end_search_idx].lower()          \n",
    "        \n",
    "    return searchable_text\n",
    "    \n",
    "def extract_relevant_sentences(text, keywords):\n",
    "    relevant_sentences = []\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        if any(keyword in sent.text.lower() for keyword in keywords):\n",
    "            relevant_sentences.append(sent.text.strip())\n",
    "    return relevant_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    # Optionally, further preprocessing steps like removing stopwords, lemmatization, etc., can be added here\n",
    "    return \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Extracting Relevant Information\n",
    "For the purpose of extracting relevant information about demographic data, ethical considerations, and methodologies for bias mitigation, keywords or phrases that signify these concepts are defined, and search for sentences containing these keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf:\n",
      "changes in image resolution or observed ﬁeld of view can result in\n",
      "inaccurate predictions, even with signiﬁcant data pre-processing and aug-\n",
      "mentation.\n",
      "we showcase that this spatial condi-\n",
      "tioning mechanism statistically-signiﬁcantly improves model performance\n",
      "on whole-body data compared to the same model without conditioning,\n",
      "while allowing the model to perform inference at varying data geometries.\n",
      "most recent approaches\n",
      "have focused on improvements in performance rather than ﬂexibility, thus lim-\n",
      "iting approaches to speciﬁc input types – little research has been carried out to\n",
      "generate models unhindered by variations in data geometries.\n",
      "often, research\n",
      "assumes certain similarities in data acquisition parameters, from image dimen-\n",
      "sions to voxel dimensions and ﬁelds-of-view (fov).\n",
      "this strong assumption can often be\n",
      "complex to maintain in the real-world and although image pre-processing steps\n",
      "can mitigate some of this complexity, test error often largely increases as new\n",
      "data variations arise.\n",
      "usually training\n",
      "data, especially when acquired from diﬀering sources, undergoes signiﬁcant pre-\n",
      "processing such that data showcases the same fov and has the same input\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0_29.\n",
      "https://doi.org/10.1007/978-3-031-43907-0_29\n",
      "geometry-invariant abnormality detection\n",
      "301\n",
      "dimensions, e.g. by registering data to a population atlas.\n",
      "given\n",
      "this, the task of generating an anomaly detection model that works on inputs\n",
      "with a varying resolution, dimension and fov is a topic of importance and the\n",
      "main focus of this research.\n",
      "unsupervised methods have become an increasingly prominent ﬁeld for auto-\n",
      "matic anomaly detection by eliminating the necessity of acquiring accurately\n",
      "labelled data [4,7] therefore relaxing the stringent data requirements of medical\n",
      "imaging.\n",
      "in [22], the authors explore the advan-\n",
      "tage of tractably maximizing the likelihood of the normal data to model the\n",
      "long-range dependencies of the training data.\n",
      "even though these methods are state-of-the-art, they have stringent data\n",
      "requirements, such as having a consistent geometry of the input data, e.g., in a\n",
      "whole-body imaging scenario, it is not possible to crop a region of interest and\n",
      "feed it to the algorithm, as this cropped region will be wrongly detected as an\n",
      "anomaly.\n",
      "furthermore, we show that\n",
      "the performance of our model with spatial conditioning is at least equivalent to,\n",
      "and sometimes better, than a model trained on whole-body data in all testing\n",
      "scenarios, with the added ﬂexibility of a “one model ﬁts all data” approach.\n",
      "we greatly reduce the pre-processing requirements for generating a model (as\n",
      "visualised in fig. 1), demonstrating the potential use cases of our model in more\n",
      "ﬂexible environments with no compromises on performance.\n",
      "speciﬁcally, a vq-vae plus a transformer are jointly used to learn the proba-\n",
      "bility density function of 3d pet images as explored in prior research [21,22,24].\n",
      "302\n",
      "a. patel et al.\n",
      "fig.\n",
      "the vq-vae\n",
      "is composed of an encoder that maps an image x ∈ rh×w ×d onto a com-\n",
      "pressed latent representation z ∈ rh×w×d×nz where nz is the latent embedding\n",
      "vector dimension.\n",
      "each spatial code\n",
      "zijl ∈ rnz is then replaced by its nearest codebook element ek ∈ rnz, k ∈ 1, ..., k\n",
      "where k denotes the codebook vocabulary size, thus obtaining zq.\n",
      "the vq-vae codebook used\n",
      "had 256 atomic elements (vocabulary size), each of length 128.\n",
      "see\n",
      "appendix a for implementation details.\n",
      "2.2\n",
      "transformer\n",
      "after training a vq-vae model, the next stage is to learn the probability den-\n",
      "sity function of the discrete latent representations.\n",
      "using the vq-vae, we can\n",
      "obtain a discrete representation of the latent space by replacing the codebook\n",
      "elements in zq with their respective indices in the codebook yielding ziq.\n",
      "the\n",
      "transformer is then trained to maximize the log-likelihoods of the latent tokens\n",
      "sequence in an autoregressive manner.\n",
      "the performer\n",
      "used in this work corresponds to a decoder transformer architecture with 14\n",
      "layers, each with 8 heads, and an embedding dimension of 256.\n",
      "similarly the\n",
      "embedding dimension for the ct data and the spatial conditioning data had an\n",
      "embedding dimension of 256.\n",
      "see appendix b for implementation details.\n",
      "we then resample anomalous\n",
      "tokens p(si) < t where t is the resampling threshold chosen empirically using\n",
      "the validation set performance.\n",
      "this generates multiple healed representations of\n",
      "the original image.\n",
      "our kde implementation used 60 samples for each\n",
      "anomalous token in s, followed by ﬁve decodings with dropout, yielding 300\n",
      "“healed” reconstructions that are then used to calculate the kde.\n",
      "3\n",
      "method\n",
      "3.1\n",
      "vq-vae spatial conditioning\n",
      "to date, there has been little research on generating autoencoder models capable\n",
      "of using images of varying sizes and resolutions (i.e. the input tensor shape to\n",
      "a autoencoder is assumed to be ﬁxed).\n",
      "although fully convolutional models can\n",
      "304\n",
      "a. patel et al.\n",
      "ingest images of varying dimensions, we have found that using training data with\n",
      "varying resolutions resulted in poor auto-encoder reconstructions.\n",
      "a coordconv layer is a concatenation of channels to the input image refer-\n",
      "encing a predeﬁned coordinate system.\n",
      "the advantage of the coordconv implementation is the constant scale of\n",
      "0–1 across the channels regardless of image resolution.\n",
      "for example, two whole-\n",
      "body images with large diﬀerences in voxel-size will have coordconv channels\n",
      "from 0–1 along each axis, thus conveying the notion of spatial resolution to the\n",
      "network.\n",
      "we found when training the vq-vae model on data with varying reso-\n",
      "lutions and dimensions that reconstructions showcased unwanted and signiﬁcant\n",
      "artifacts, while by adding the coordconv channels this issue was not present\n",
      "(see appendix c for examples).\n",
      "furthermore, when dealing with images of a\n",
      "ranging fov, we adapted the [0, 1] channel values to convey the image’s fov.\n",
      "for example, suppose a whole body image (neck to upper leg) represented our\n",
      "range\n",
      "in that case, we can\n",
      "contract this range to represent the area displayed in the image (fig. 2).\n",
      "for the implementation of the coordconv layer, these channels are\n",
      "added once to the original input image and at the beginning of the vq-vae\n",
      "decoder, concatenated to the latent space, using the same value ranges but at a\n",
      "lower resolution given the reduced spatial dimension of the latent space.\n",
      "3.2\n",
      "transformer spatial conditioning\n",
      "numerous approaches have used transformers in the visual domain [7,8].\n",
      "given\n",
      "that transformers work natively on 1d sequences, the spatial information in\n",
      "images is often lost.\n",
      "while various works have aimed to convey the spatial infor-\n",
      "mation of the original image when projected onto a 1d sequence [14,28], we\n",
      "require our spatial positioning to encode both where in the image ordering a\n",
      "token belongs, and where the token belongs in the context of the whole body.\n",
      "as the images have diﬀerent fovs and the image resolution, this results in\n",
      "geometry-invariant abnormality detection\n",
      "305\n",
      "fig.\n",
      "2. coordconv example showing whole-body image with values from 0 to 1 vs. a\n",
      "cropped image with values from 0.2 to 0.7 to reﬂect the ﬁeld of view\n",
      "varying token sequence lengths.\n",
      "in order to map image coordinates to the token latent represen-\n",
      "tation, we apply average pooling to each coordconv channel separately, with\n",
      "kernel size and stride equal to the downsampling used in the vq-vae (8 used\n",
      "in this research).\n",
      "this gives us three channels i, j, k in the range of [0, 1], the\n",
      "same dimension as our latent space, but at lower spatial resolution to the original\n",
      "input.\n",
      "the choice of b = 20 bins was\n",
      "empirically chosen to closely resemble the average latent dimension of images.\n",
      "during training, whole-body images and random crops are used.\n",
      "the spatial\n",
      "conditioning tokens are then generated and fed through an embedding layer of\n",
      "equal dimension to the ct embedding.\n",
      "3.\n",
      "3.3\n",
      "data\n",
      "for this work we leveraged whole-body pet/ct data from diﬀerent sources to\n",
      "explore the eﬃcacy of our approach for varying image geometries.\n",
      "211 scans from\n",
      "nsclc radiogenomics [2,3,10,16] combined with 83 scans from a proprietary\n",
      "dataset constitute our lower resolution dataset with voxel dimensions of 3.6 ×\n",
      "3.6×3 mm.\n",
      "our higher resolution dataset uses autopet [10,15] (1014 scans)\n",
      "with voxel dimensions of 2.036 × 2.036 × 3 mm.\n",
      "all baseline models work in a single space with constant dimensions, obtained\n",
      "by registering the autopet images to the space of the nsclc dataset.\n",
      "as the cropped and rotated dataset cannot be fed into the baseline\n",
      "models, we pad the images to the common image sizing before inference.\n",
      "we then test\n",
      "our model and baselines on 4 hold-out test sets: a low-resolution whole-body set,\n",
      "a low-resolution cropped set, a high-resolution rotated set and a high-resolution\n",
      "test set of pet images with varying cancers.\n",
      "4. columns display (1st) the input image; (2nd) the gold standard segmentation;\n",
      "(3rd) residual for the vae, (4th) ae spatial, (5th) a kde anomaly map for vq-vae\n",
      "transformer trained on the whole body, (6th) trained with varied geometries, (7th)\n",
      "with spatial conditioning.\n",
      "performance using the dice score, obtained by thresholding the residual/density\n",
      "score maps.\n",
      "in addition, we calculate the area under the precision-recall curve\n",
      "(auprc) as a suitable measure for segmentation performance under class imbal-\n",
      "ance.\n",
      "we additionally showcase the performance of the classic vq-vae + trans-\n",
      "former approach trained on whole-body data only (without the proposed spatial\n",
      "conditioning), as well as the proposed coordconv model trained with varying\n",
      "image geometries but without the transformer spatial conditioning to explicitly\n",
      "showcase the added contribution of both spatial conditionings.\n",
      "4. we can observe\n",
      "that the addition of spatial conditioning improves performance even against the\n",
      "same model without conditioning trained on whole-body data (mann whitney\n",
      "u test, p < 0.01 on high resolution and p < 0.001 on cropped data for dice\n",
      "and auprc).\n",
      "note that the vq-vae + transformer trained on\n",
      "varying geometries still shows adequate performance, highlighting the resilience\n",
      "of the transformer network to varying sequence lengths without any form of\n",
      "spatial conditioning.\n",
      "however, by adding the transformer spatial conditioning,\n",
      "we see improvements across all test sets (most signiﬁcantly on cropped data and\n",
      "the rotated data p < 0.001) for both evaluation metrics.\n",
      "for the rotated data,\n",
      "we see little performance degradation in the conditioned model thanks to the\n",
      "spatial conditioning.\n",
      "the same model without conditioning showed much lower\n",
      "performance with higher false positives likely due to the model’s inability to\n",
      "comprehend the anatomical structures present due to the rotated orientation.\n",
      "308\n",
      "a. patel et al.\n",
      "5\n",
      "conclusion\n",
      "detection and segmentation of anomalous regions, particularly for cancer\n",
      "patients, is essential for staging, treatment and intervention planning.\n",
      "not only does\n",
      "the proposed model showcase strong and statistically-signiﬁcant performance\n",
      "improvements on varying image resolutions and fov, but also on whole-body\n",
      "data.\n",
      "through this, we demonstrate that one can improve the adaptability and\n",
      "ﬂexibility to varying data geometries while also improving performance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf:\n",
      "automated segmentation of the blood vessels in 3d volumes\n",
      "is an essential step for the quantitative diagnosis and treatment of many\n",
      "vascular diseases.\n",
      "3d vessel segmentation is being actively investigated\n",
      "in existing works, mostly in deep learning approaches.\n",
      "however, training\n",
      "3d deep networks requires large amounts of manual 3d annotations from\n",
      "experts, which are laborious to obtain.\n",
      "this is especially the case for 3d\n",
      "vessel segmentation, as vessels are sparse yet spread out over many slices\n",
      "and disconnected when visualized in 2d slices.\n",
      "in this work, we pro-\n",
      "pose a novel method to segment the 3d peripancreatic arteries solely\n",
      "from one annotated 2d projection per training image with depth\n",
      "supervision.\n",
      "we perform extensive experiments on the segmentation of\n",
      "peripancreatic arteries on 3d contrast-enhanced ct images and demon-\n",
      "strate how well we capture the rich depth information from 2d pro-\n",
      "jections.\n",
      "we demonstrate that by annotating a single, randomly chosen\n",
      "projection for each training sample, we obtain comparable performance\n",
      "to annotating multiple 2d projections, thereby reducing the annotation\n",
      "eﬀort.\n",
      "furthermore, by mapping the 2d labels to the 3d space using\n",
      "depth information and incorporating this into training, we almost close\n",
      "the performance gap between 3d supervision and 2d supervision.\n",
      "our\n",
      "code is available at: https://github.com/alinafdima/3dseg-mip-depth.\n",
      "keywords: vessel segmentation · 3d segmentation · weakly\n",
      "supervised segmentation · curvilinear structures · 2d projections\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0_14.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43907-0_14\n",
      "142\n",
      "a. f. dima et al.\n",
      "1\n",
      "introduction\n",
      "automated segmentation of blood vessels in 3d medical images is a crucial step\n",
      "for the diagnosis and treatment of many diseases, where the segmentation can aid\n",
      "in visualization, help with surgery planning, be used to compute biomarkers, and\n",
      "further downstream tasks.\n",
      "automatic vessel segmentation has been extensively\n",
      "studied, both using classical computer vision algorithms\n",
      "[8], or more recently with deep learning [3,5,6,11,19,21], where state-of-\n",
      "the-art performance has been achieved for various vessel structures.\n",
      "this is especially the case for 3d vessel segmentation.\n",
      "manually delineating 3d vessels typically involves visualizing and annotating\n",
      "a 3d volume through a sequence of 2d cross-sectional slices, which is not a good\n",
      "medium for visualizing 3d vessels.\n",
      "in order to segment a vessel, the annotator has\n",
      "to track the cross-section of that vessel through several adjacent slices, which\n",
      "is especially tedious for curved or branching vessel trees.\n",
      "projecting 3d vessels\n",
      "to a 2d plane allows for the entire vessel tree to be visible within a single 2d\n",
      "image, providing a more robust representation and potentially alleviating the\n",
      "burden of manual annotation.\n",
      "[13] propose to annotate up to\n",
      "three maximum intensity projections (mip) for the task of centerline segmen-\n",
      "tation\n",
      "compared to\n",
      "centerline segmentation, where the vessel diameter is disregarded, training a 3d\n",
      "vessel segmentation model from 2d annotations poses additional segmentation-\n",
      "speciﬁc challenges, as 2d projections only capture the outline of the vessels,\n",
      "providing no information about their interior.\n",
      "furthermore, the axes of projec-\n",
      "tion are crucial for the model’s success, given the sparsity of information in 2d\n",
      "annotations.\n",
      "to achieve 3d vessel segmentation with only 2d supervision from projec-\n",
      "tions, we ﬁrst investigate which viewpoints to annotate in order to maximize\n",
      "segmentation performance.\n",
      "we show that it is feasible to segment the full extent\n",
      "of vessels in 3d images with high accuracy by annotating only a single randomly-\n",
      "selected 2d projection per training image.\n",
      "secondly, by mapping the 2d annotations to the 3d space using the depth of\n",
      "the mips, we obtain a partially segmented 3d volume that can be used as an\n",
      "additional supervision signal.\n",
      "we demonstrate the utility of our method on the\n",
      "challenging task of peripancreatic arterial segmentation on contrast-enhanced\n",
      "arterial-phase computed tomography (ct) images, which feature large variance\n",
      "in vessel diameter.\n",
      "our contribution to 3d vessel segmentation is three-fold:\n",
      "◦ our work shows that highly accurate automatic segmentation of 3d vessels\n",
      "can be learned by annotating single mips.\n",
      "◦ based on extensive experimental results, we determine that the best annota-\n",
      "tion strategy is to label randomly selected viewpoints, while also substantially\n",
      "reducing the annotation cost.\n",
      "3d arterial segmentation via single 2d projections and depth supervision\n",
      "143\n",
      "◦ by incorporating additional depth information obtained from 2d annotations\n",
      "at no extra cost to the annotator, we almost close the gap between 3d super-\n",
      "vision and 2d supervision.\n",
      "weak annotations have been used in\n",
      "deep learning segmentation to reduce the annotation eﬀort through cheaper,\n",
      "less accurate, or sparser labeling [20].\n",
      "[1] learn to perform aortic image\n",
      "segmentation by sparsely annotating only a subset of the input slices.\n",
      "[12] use this approach to segment cancer on histopathology\n",
      "images successfully.\n",
      "annotating 2d projections for 3d data is another approach\n",
      "to using weak segmentation labels, which has garnered popularity recently in\n",
      "the medical domain.\n",
      "[22] use multi-planar mips for multi-organ\n",
      "segmentation of the abdomen.\n",
      "kozinski et al.[13] propose to segment vessel cen-\n",
      "terlines using as few as 2-3 annotated mips.\n",
      "[4] train a vessel segmen-\n",
      "tation model from unsupervised 2d labels transferred from a publicly available\n",
      "dataset, however, there is still a gap to be closed between unsupervised and\n",
      "supervised model performance.\n",
      "our work uses weak annotations in the form of\n",
      "annotations of 2d mips for the task of peripancreatic vessel segmentation, where\n",
      "we attempt to reduce the annotation cost to a minimum by only annotating a\n",
      "single projection per training input without sacriﬁcing performance.\n",
      "loss of depth information occurs whenever 3d data is projected onto a\n",
      "lower dimensional space.\n",
      "in natural images, depth loss is inherent through image\n",
      "acquisition, therefore attempts to recover or model depth have been employed\n",
      "for 3d natural data.\n",
      "[9] use neural implicit ﬁelds to\n",
      "semantically segment images by transferring labels from 3d primitives to 2d\n",
      "images.\n",
      "[14] propose to segment 3d point clouds by projecting\n",
      "them onto 2d and training a 2d segmentation network.\n",
      "at inference time, the\n",
      "predicted 2d segmentation labels are remapped back to the original 3d space\n",
      "using the depth information.\n",
      "we use depth information to map the 2d annotations to the original\n",
      "3d space at annotation time and generate partial 3d segmentation volumes,\n",
      "which we incorporate in training as an additional loss term.\n",
      "(1)\n",
      "for simplicity, we only describe mips along the z-axis, but they can be performed\n",
      "on any image axis.\n",
      "fig.\n",
      "we train a 3d network to segment vessels from 2d annota-\n",
      "tions.\n",
      "given an input image i, depth-encoded mips pfw, pbw are generated by project-\n",
      "ing the input image to 2d.\n",
      "2d binary labels a are generated by annotating one 2d\n",
      "projection per image.\n",
      "the 2d annotation is mapped to the 3d space using the depth\n",
      "information, resulting in a partially labeled 3d volume d. during training, both 2d\n",
      "annotations and 3d depth maps are used as supervision signals in a combined loss,\n",
      "which uses both predicted 3d segmentation y and its 2d projection mip(y ).\n",
      "exploiting the fact that arteries are hyperintense in arterial phase cts, we\n",
      "propose to annotate mips of the input volume for binary segmentation.\n",
      "given a binary 2d annotation of a mip a ∈ {0, 1}nx×ny, we map the fore-\n",
      "ground pixels in a to the original 3d image space.\n",
      "furthermore, we can partially ﬁll this surface volume, resulting in a 3d depth\n",
      "map d, which is a partial segmentation of the vessel tree.\n",
      "we use the 2d anno-\n",
      "tations as well as the depth map to train a 3d segmentation network in a weakly\n",
      "supervised manner.\n",
      "3d arterial segmentation via single 2d projections and depth supervision\n",
      "145\n",
      "an overview of our method is presented in fig.\n",
      "1. in the following, we describe\n",
      "these components and how they are combined to train a 3d segmentation net-\n",
      "work in more detail.\n",
      "the reason\n",
      "why the maximum intensity is achieved multiple times along a ray is because\n",
      "our images are clipped, which removes a lot of the intensity ﬂuctuations.\n",
      "fig.\n",
      "the input images are\n",
      "contrast-enhanced.(color ﬁgure online)\n",
      "depth-enhanced mip.\n",
      "foreground pixels from the 2d annotations are\n",
      "mapped to the 3d space by combining a 2d annotation with the forward and\n",
      "backward depth, resulting in a 3d partial vessel segmentation:\n",
      "146\n",
      "a. f. dima et al.\n",
      "1.\n",
      "if the ﬂuctuation in intensity between zfw and zbw along the ray rxy is\n",
      "below a certain threshold in the source image i, the intermediate pixels\n",
      "are also labeled as foreground in d.\n",
      "training loss.\n",
      "we train a 3d segmentation network to predict 3d binary vessel\n",
      "segmentation given a 3d input volume using 2d annotations.\n",
      "notably, the 2d loss constrains\n",
      "the shape of the vessels, while the depth loss promotes the segmentation of the\n",
      "vessel interior.\n",
      "4\n",
      "experimental design\n",
      "dataset.\n",
      "we use an in-house dataset of contrast-enhanced abdominal computed\n",
      "tomography images (cts) in the arterial phase to segment the peripancreatic\n",
      "arteries\n",
      "the cohort consists of 141 patients with pancreatic ductal adeno-\n",
      "carcinoma, of an equal ratio of male to female patients.\n",
      "details of the exact preprocessing steps can be found in table 2 of the supple-\n",
      "mentary material.\n",
      "the 2d annotations we use in our experiments are projections of these\n",
      "3d annotations.\n",
      "for more information about the dataset, see [6].\n",
      "image augmentation and transformation.\n",
      "as the annotations lie on a 2d\n",
      "plane, 3d spatial augmentation cannot be used due to the information sparsity\n",
      "in the ground truth.\n",
      "a\n",
      "detailed description of the augmentations and transformations used can be found\n",
      "in table 1 in the supplementary material.\n",
      "3d arterial segmentation via single 2d projections and depth supervision\n",
      "147\n",
      "training and evaluation.\n",
      "2 in the supplementary material.\n",
      "the loss weight α\n",
      "is tuned at 0.5, as this empirically yields the best performance.\n",
      "our experiments\n",
      "are averaged over 5-fold cross-validation with 80 train samples, 20 validation\n",
      "samples, and a ﬁxed test set of 41 samples.\n",
      "the network initialization is diﬀer-\n",
      "ent for each fold but kept consistent across diﬀerent experiments run on the same\n",
      "fold.\n",
      "to measure the performance of our models, we use the\n",
      "dice score, precision, recall, and mean surface distance (msd).\n",
      "we also compute\n",
      "the skeleton recall as the percentage of the ground truth skeleton pixels which\n",
      "are present in the prediction.\n",
      "experiment\n",
      "model selection dice ↑\n",
      "precision ↑\n",
      "recall ↑\n",
      "skeleton recall ↑ msd ↓\n",
      "3d\n",
      "3d\n",
      "92.18 ± 0.35 93.86 ± 0.81 90.64 ± 0.64\n",
      "76.04 ± 4.51\n",
      "1.15 ± 0.11\n",
      "ﬁxed 3vp\n",
      "3d\n",
      "92.02 ± 0.52\n",
      "93.05 ± 0.61\n",
      "91.13 ± 0.79\n",
      "78.61 ± 1.52\n",
      "1.13 ± 0.11\n",
      "ﬁxed 2vp\n",
      "3d\n",
      "91.29 ± 0.78\n",
      "91.46 ± 2.13\n",
      "91.37 ± 1.45 78.51 ± 2.78\n",
      "1.13 ± 0.09\n",
      "ﬁxed 3vp\n",
      "2d\n",
      "90.78 ± 1.30\n",
      "90.66 ± 1.30\n",
      "91.18 ± 3.08\n",
      "81.77 ± 2.13\n",
      "1.16 ± 0.13\n",
      "ﬁxed 2vp\n",
      "2d\n",
      "90.22 ± 1.19\n",
      "88.16 ± 2.86\n",
      "92.74 ± 1.63\n",
      "82.18 ± 2.47\n",
      "1.14 ± 0.09\n",
      "ﬁxed 1vp\n",
      "2d\n",
      "60.76 ± 24.14 50.47 ± 23.21 92.52 ± 3.09\n",
      "81.19 ± 2.39\n",
      "2.96 ± 3.15\n",
      "random 1vp−d 2d\n",
      "91.29 ± 0.81\n",
      "91.42 ± 0.92 91.45 ± 1.00\n",
      "80.16 ± 2.35\n",
      "1.13 ± 0.04\n",
      "random 1vp+d 2d\n",
      "91.69 ± 0.48 90.77 ± 1.76\n",
      "92.79 ± 0.95 81.27 ± 2.02\n",
      "1.15 ± 0.11\n",
      "5\n",
      "results\n",
      "the eﬀectiveness of 2d projections and depth supervision.\n",
      "we implement [13] as a baseline on our dataset, training on up to 3\n",
      "ﬁxed orthogonal projections.\n",
      "we distinguish between models selected according\n",
      "to the 2d performance on the validation set (2d) which is a fair baseline, and\n",
      "models selected according to the 3d performance on the validation set (3d),\n",
      "which is an unfair baseline as it requires 3d annotations on the validation set.\n",
      "with the exception of the single ﬁxed viewpoint baselines where the models have\n",
      "the tendency to diverge towards over- or segmentation, we perform binary hole-\n",
      "ﬁlling on the output of all of our other models, as producing hollow objects is a\n",
      "common under-segmentation issue.\n",
      "randomly selecting view-\n",
      "points for training acts as powerful data augmentation, which is why we are\n",
      "able to obtain performance comparable to using more ﬁxed viewpoints.\n",
      "under\n",
      "ideal 3d-based model selection, three views would come even closer to full 3d\n",
      "performance; however, with realistic 2d-based model selection, ﬁxed viewpoints\n",
      "are more prone to diverge.\n",
      "this occurs because sometimes 2d-based model selec-\n",
      "tion favors divergent models which only segment hollow objects, which cannot\n",
      "be ﬁxed in postprocessing.\n",
      "single ﬁxed viewpoints contain so little information\n",
      "on their own that models trained on such input fail to learn how to segment\n",
      "the vessels and generally converge to over-segmenting in the blind spots in the\n",
      "projections.\n",
      "we theorize that this is because\n",
      "the dataset itself contains noisy annotations and fully supervised models better\n",
      "overﬁt to the type of data annotation, whereas our models converge to follow-\n",
      "ing the contrast and segmenting more vessels, which are sometimes wrongfully\n",
      "labeled as background in the ground truth.\n",
      "msd are not very telling in our\n",
      "dataset due to the noisy annotations and the nature of vessels, as an under- or\n",
      "over-segmented vessel branch can quickly translate into a large surface distance.\n",
      "our depth loss oﬀers\n",
      "consistent improvement across multiple dataset sizes and reduces the overall per-\n",
      "formance variance.\n",
      "the performance boost is noticeable across the board, the\n",
      "only exception being precision.\n",
      "the smaller the dataset size is, the greater the\n",
      "performance boost from the depth.\n",
      "we\n",
      "conclude that the depth information complements the segmentation eﬀectively.\n",
      "3d arterial segmentation via single 2d projections and depth supervision\n",
      "149\n",
      "table 2.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf:\n",
      "most existing weakly-supervised segmentation methods rely\n",
      "on class activation maps (cam) to generate pseudo-labels for training\n",
      "segmentation models.\n",
      "although some recent methods have attempted to\n",
      "extend cam to cover more areas, the fundamental problem still needs\n",
      "to be solved.\n",
      "we believe this problem is due to the huge gap between\n",
      "image-level labels and pixel-level predictions and that additional infor-\n",
      "mation must be introduced to address this issue.\n",
      "thus, we propose a\n",
      "text-prompting-based weakly supervised segmentation method (tpro),\n",
      "which uses text to introduce additional information.\n",
      "tpro employs a\n",
      "vision and label encoder to generate a similarity map for each image,\n",
      "which serves as our localization map.\n",
      "pathological knowledge is gathered\n",
      "from the internet and embedded as knowledge features, which are used to\n",
      "guide the image features through a knowledge attention module.\n",
      "our approach outperforms other weakly super-\n",
      "vised segmentation methods on benchmark datasets luad-histoseg and\n",
      "bcss-wsss datasets, setting a new state of the art.\n",
      "code is available\n",
      "at: https://github.com/zhangst431/tpro.\n",
      "keywords: histopathology tissue segmentation · weakly-supervised\n",
      "semantic segmentation · vision-language\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0_11.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43907-0_11\n",
      "110\n",
      "s. zhang et al.\n",
      "1\n",
      "introduction\n",
      "automated segmentation of histopathological images is crucial, as it can quantify\n",
      "the tumor micro-environment, provide a basis for cancer grading and prognosis,\n",
      "and improve the diagnostic eﬃciency of clinical doctors [6,13,19].\n",
      "however, pixel-\n",
      "level annotation of images is time-consuming and labor-intensive, especially for\n",
      "histopathology images that require specialized knowledge.\n",
      "therefore, there is an\n",
      "urgent need to pursue weakly supervised solutions for pixel-wise segmentation.\n",
      "nonetheless, weakly supervised histopathological image segmentation presents\n",
      "a challenge due to the low contrast between diﬀerent tissues, intra-class vari-\n",
      "ations, and inter-class similarities [4,11].\n",
      "additionally, the tissue structures in\n",
      "histopathology images can be randomly arranged and dispersed, which makes it\n",
      "diﬃcult to identify complete tissues or regions of interest [7].\n",
      "1. comparison of activation maps extracted from cam and our method, from left\n",
      "to right: origin image, ground truth, three activation maps of tumor epithelial (red),\n",
      "necrosis (green), and tumor-associated stroma (orange) respectively.\n",
      "on the right side,\n",
      "there are some examples of the related language knowledge descriptions used in our\n",
      "method.\n",
      "it shows that cam only highlights a small portion of the target, while our\n",
      "method, which incorporates external language knowledge, can encompass a wider and\n",
      "more precise target tissue.\n",
      "(color ﬁgure online)\n",
      "recent studies on weakly supervised segmentation primarily follow class acti-\n",
      "vation mapping (cam) [20], which localizes the attention regions and then\n",
      "generates the pseudo labels to train the segmentation network.\n",
      "however, the\n",
      "cam generated based on the image-level labels can only highlight the most dis-\n",
      "criminative region, but fail to locate the complete object, leading to defective\n",
      "pseudo labels, as shown in fig.\n",
      "accordingly, many attempts have been made\n",
      "to enhance the quality of cam and thus boost the performance of weakly super-\n",
      "vised segmentation.\n",
      "[11] utilized the conﬁdence method to remove any noise that may\n",
      "exist in the pseudo labels and only included the conﬁdent pixel labels for the\n",
      "segmentation training.\n",
      "[18] leveraged the transformer to model the\n",
      "long-distance dependencies on the whole histopathological images to improve the\n",
      "cam’s ability to ﬁnd more complete regions.\n",
      "however, these improved variants still face diﬃculties in capturing the\n",
      "tpro for weakly supervised histopathology tissue segmentation\n",
      "111\n",
      "complete tissues.\n",
      "the primary limitation is that the symptoms and manifesta-\n",
      "tions of histopathological subtypes cannot be comprehensively described by an\n",
      "abstract semantic category.\n",
      "as a result, the image-level label supervision may\n",
      "not be suﬃcient to pinpoint the complete target area.\n",
      "to remedy the limitations of image-level supervision, we advocate for the inte-\n",
      "gration of language knowledge into weakly supervised learning to provide reliable\n",
      "guidance for the accurate localization of target structures.\n",
      "to this end, we pro-\n",
      "pose a text-prompting-based weakly supervised segmentation method (tpro)\n",
      "for accurate histopathology tissue segmentation.\n",
      "the text information originates\n",
      "from the task’s semantic labels and external descriptions of subtype manifesta-\n",
      "tions.\n",
      "for each semantic label, a pre-trained medical language model is utilized\n",
      "to extract the corresponding text features that are matched to each feature point\n",
      "in the image spatial space.\n",
      "a higher similarity represents a higher possibility of\n",
      "this location belonging to the corresponding semantic category.\n",
      "additionally,\n",
      "the text representations of subtype manifestations, including tissue morphol-\n",
      "ogy, color, and relationships to other tissues, are extracted by the language\n",
      "model as external knowledge.\n",
      "the discriminative information can be explored\n",
      "from the text knowledge to help identify and locate complete tissues accurately\n",
      "by jointly modeling long-range dependencies between image and text.\n",
      "we con-\n",
      "duct experiments on two weakly supervised histological segmentation bench-\n",
      "marks, luad-histoseg and bcss-wsss, and demonstrate the superior quality\n",
      "of pseudo labels produced by our tpro model compared to other cam-based\n",
      "methods.\n",
      "our contributions are summarized as follows: (1) to the best of our knowl-\n",
      "edge, this is the ﬁrst work that leverages language knowledge to improve the\n",
      "quality of pseudo labels for weakly-supervised histopathology image segmenta-\n",
      "tion.\n",
      "(2) the proposed text prompting models the correlation between image\n",
      "representations and text knowledge, eﬀectively improving the quality of pseudo\n",
      "labels.\n",
      "tumor epithelial tissue is ....\n",
      "necrosis tissue is ....\n",
      "lymphocyte tissue is ....\n",
      "tumor-associated stroma tissue is.....\n",
      "knowledge \n",
      "input\n",
      "knowledge \n",
      "features\n",
      "image \n",
      "features\n",
      "reshape\n",
      "label \n",
      "input\n",
      "tumor epithelial tissue\n",
      "necrosis tissue\n",
      "lymphocyte tissue\n",
      "tumor-associated stroma tissue\n",
      "gap\n",
      "gap\n",
      "gap\n",
      "stage 1\n",
      "stage 2\n",
      "stage 3\n",
      "stage 4\n",
      "sim\n",
      "sim\n",
      "sim\n",
      "input image\n",
      "knowledge input\n",
      "label input\n",
      "input image\n",
      "bert\n",
      "clip\n",
      "bert: clinicalbert  \n",
      "clip: medclip\n",
      "sim: pixel-label correlation\n",
      "knowledge attention\n",
      "search from internet\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "reshape\n",
      "reshape\n",
      "fc: fc+relu+fc\n",
      "fig.\n",
      "112\n",
      "s. zhang et al.\n",
      "2\n",
      "method\n",
      "figure 2 displays the proposed tpro framework, a classiﬁcation network\n",
      "designed to train a suitable model and extract segmentation pseudo-labels.\n",
      "the vision encoder is composed of four stages that encode\n",
      "the input image into image features.\n",
      "the image features are denoted as ts ∈\n",
      "rms×cs, where 2 ≤ s ≤ 4 indicates the stage number.\n",
      "the label encoder encodes the text labels in the dataset into\n",
      "n label features, denoted as l ∈ rn×cl, where n represents the number of\n",
      "classes in the dataset and cl represents the dimension of label features.\n",
      "since\n",
      "the label features will be used to calculate the similarity with image features, it\n",
      "is important to choose a language model that has been pre-trained on image-text\n",
      "pairs.\n",
      "the knowledge encoder is responsible for embedding\n",
      "the descriptions of subtype manifestations into knowledge features, denoted as\n",
      "k ∈ rn×ck.\n",
      "the knowledge features guide the image features to focus on regions\n",
      "relevant to the target tissue.\n",
      "to encode the subtype manifestations description\n",
      "into more general semantic features, we employ clinicalbert\n",
      "clinicalbert is a language model that has been ﬁne-tuned on the\n",
      "mimic-iii\n",
      "after the input image and text labels are embedded.\n",
      "we employ the inner product to compute the similarity between image features\n",
      "and label features, denoted as fs.\n",
      "specially, we ﬁrst reshape the image features\n",
      "from a token format into feature maps.\n",
      "(1)\n",
      "then, we perform a global average-pooling operation on the produced similarity\n",
      "map to obtain the class prediction, denoted as ps ∈ r1×n.\n",
      "tpro for weakly supervised histopathology tissue segmentation\n",
      "113\n",
      "ls = − 1\n",
      "n\n",
      "n\n",
      "\u0002\n",
      "n=1\n",
      "y\n",
      "to leverage the shallow features in the network, we employ\n",
      "a deep supervision strategy by calculating the similarity between the image fea-\n",
      "tures from diﬀerent stages and the label features from diﬀerent adaptive layers.\n",
      "(3)\n",
      "2.2\n",
      "knowledge attention module\n",
      "to enhance the model’s understanding of the color, morphology, and relation-\n",
      "ships between diﬀerent tissues, we gather text representations of diﬀerent sub-\n",
      "type manifestations from the internet and encode them into external knowledge\n",
      "via the knowledge encoder.\n",
      "the knowledge attention module uses this exter-\n",
      "nal knowledge to guide the image features toward relevant regions of the target\n",
      "tissues.\n",
      "the image features t4 ∈ rm4×c4 and knowledge features\n",
      "after adaptive layer k ∈ rn×c4 are concatenated in the token dimension to\n",
      "obtain tfuse ∈ r(m4+n)×c4.\n",
      "the output tokens\n",
      "are split, and the part corresponding to the image features is taken out.\n",
      "noted\n",
      "that the knowledge attention module is added only after the last stage of the\n",
      "vision encoder to save computational resources.\n",
      "2.3\n",
      "pseudo label generation\n",
      "in the classiﬁcation process, we calculate the similarity between image features\n",
      "and label features to obtain a similarity map f, and then directly use the result\n",
      "of global average pooling on the similarity map as a class prediction.\n",
      "referring to [1] and combined with our own experiments, we set α to\n",
      "114\n",
      "s. zhang et al.\n",
      "10.\n",
      "in order to make full use of the shallow information of the network,\n",
      "we perform weighted fusion on the localization maps from diﬀerent stages by the\n",
      "following formula:\n",
      "fall = γ2 ˆf2 +\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "dataset\n",
      "luad-histoseg2\n",
      "[7] is a weakly-supervised histological semantic segmenta-\n",
      "tion dataset for lung adenocarcinoma.\n",
      "bcss-wsss3 is a weakly supervised tissue\n",
      "semantic segmentation dataset extracted from the fully supervised segmenta-\n",
      "tion dataset bcss [3], which contains 151 representative h&e-stained breast\n",
      "cancer pathology slides.\n",
      "3.2\n",
      "implementation details\n",
      "for the classiﬁcation part, we adopt mixtransformer\n",
      "tpro for weakly supervised histopathology tissue segmentation\n",
      "115\n",
      "knowledge encoder, respectively.\n",
      "the hyperparameters during training and eval-\n",
      "uation can be found in the supplementary materials.\n",
      "we conduct all of our\n",
      "experiments on 2 nvidia geforce rtx 2080 ti gpus.\n",
      "[18] consists of a classiﬁcation and a segmentation branch, and table 1\n",
      "displays the pseudo-label scores generated by the classiﬁcation branch.\n",
      "[18] for\n",
      "single-label image segmentation, with the segmentation branch simpliﬁed to\n",
      "binary segmentation to reduce the diﬃculty, while our dataset consists of multi-\n",
      "label images.\n",
      "[20] in terms of the quality of the generated pseudo-labels, with\n",
      "its proposed progressive dropout attention eﬀectively expanding the coverage\n",
      "of target regions beyond what cam [20] can achieve.\n",
      "our proposed method\n",
      "outperformed all previous methods on both luad-histoseg and bcss-wsss\n",
      "datasets, with improvements of 2.64% and 5.42% over the second-best method,\n",
      "respectively (table 2).\n",
      "table 2. comparison of the ﬁnal segmentation results between our method and the\n",
      "methods in previous years.\n",
      "[7]\n",
      "73.90\n",
      "77.48\n",
      "73.61\n",
      "69.53\n",
      "73.63\n",
      "74.54\n",
      "64.45\n",
      "52.54\n",
      "58.67\n",
      "62.55\n",
      "tpro (ours)\n",
      "75.80 80.56 78.14 72.69 76.80 77.95 65.10 54.55 64.96 65.64\n",
      "comparison on segmentation results.\n",
      "to further evaluate our proposed\n",
      "method, we trained a segmentation model using the extracted pseudo-labels and\n",
      "compared its performance with previous methods.\n",
      "as we have previously analyzed since the datasets\n",
      "we used are all multi-label images, it was challenging for the segmentation branch\n",
      "of transws\n",
      "experimental results also indicate that the iou scores of its segmentation\n",
      "116\n",
      "s. zhang et al.\n",
      "table 3. comparison the eﬀectiveness of\n",
      "label text(lt), knowledge text(kt), and\n",
      "deep supervision(ds).\n",
      "lt ds kt te\n",
      "nec\n",
      "lym\n",
      "tas\n",
      "miou\n",
      "68.11\n",
      "75.24\n",
      "64.95\n",
      "66.57\n",
      "68.72\n",
      "✓\n",
      "72.39\n",
      "72.44\n",
      "71.37\n",
      "68.67\n",
      "71.22\n",
      "✓\n",
      "✓\n",
      "72.41\n",
      "72.11\n",
      "74.21\n",
      "70.07\n",
      "72.20\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "74.82 77.55 76.40 70.98 74.94\n",
      "table 4. comparison of pseudo labels\n",
      "extracted from the single stage and our\n",
      "fused version.\n",
      "te\n",
      "nec\n",
      "lym\n",
      "tas\n",
      "miou\n",
      "stage2 67.16\n",
      "65.28\n",
      "67.38\n",
      "55.09\n",
      "63.73\n",
      "stage3 72.13\n",
      "70.83\n",
      "73.47\n",
      "69.46\n",
      "71.47\n",
      "stage4 72.69\n",
      "77.57\n",
      "76.06\n",
      "69.81\n",
      "74.03\n",
      "fusion\n",
      "74.82 77.55 76.40 70.98 74.94\n",
      "branch were even lower than the pseudo-labels of the classiﬁcation branch.\n",
      "by\n",
      "training the segmentation model of oeem [11] using the pseudo-labels extracted\n",
      "by cam [20] in table 1, we can observe a signiﬁcant improvement in the ﬁnal\n",
      "segmentation results.\n",
      "the ﬁnal segmentation results of mlps [7] showed some\n",
      "improvement compared to its pseudo-labels, indicating the eﬀectiveness of the\n",
      "multi-layer pseudo supervision and classiﬁcation gate mechanism strategy pro-\n",
      "posed by mlps [7].\n",
      "our segmentation performance surpassed all previous meth-\n",
      "ods.\n",
      "additionally,\n",
      "it is worth noting that we did not use any strategies speciﬁcally designed for the\n",
      "segmentation stage.\n",
      "3.4\n",
      "ablation study\n",
      "the results of our ablation experiments are presented in table 3.\n",
      "these ﬁndings demonstrate the signiﬁcant\n",
      "contribution of each proposed module to the overall improvement of the results.\n",
      "in order to demonstrate the eﬀectiveness of fusing pseudo-labels from the last\n",
      "three stages, we have presented in table 4 the iou scores for each stage’s pseudo-\n",
      "labels as well as the fused pseudo-labels.\n",
      "it can be observed that after fusing the\n",
      "pseudo-labels, not only have the iou scores for each class substantially increased,\n",
      "but the miou score has also increased by 0.91% compared to the fourth stage.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf:\n",
      "the fpn is pre-trained to produce sim-\n",
      "ilar representations for the same voxel in diﬀerent augmented contexts\n",
      "and distinctive representations for diﬀerent voxels.\n",
      "this results in uni-\n",
      "ﬁed multi-scale representations that capture both global semantics (e.g.,\n",
      "body part) and local semantics (e.g., diﬀerent small organs or healthy\n",
      "versus tumor tissue).\n",
      "we use vox2vec to pre-train a fpn on more than\n",
      "6500 publicly available computed tomography images.\n",
      "we evaluate the\n",
      "pre-trained representations by attaching simple heads on top of them\n",
      "and training the resulting models for 22 segmentation tasks.\n",
      "moreover, a non-linear head trained on top of the frozen\n",
      "vox2vec representations achieves competitive performance with the fpn\n",
      "trained from scratch while having 50 times fewer trainable parameters.\n",
      "the code is available at https://github.com/mishgon/vox2vec.\n",
      "keywords: contrastive self-supervised representation learning ·\n",
      "medical image segmentation\n",
      "1\n",
      "introduction\n",
      "medical image segmentation often relies on supervised model training [14], but\n",
      "this approach has limitations.\n",
      "firstly, it requires costly manual annotations.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0_58.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "even small changes in the task may result in a signiﬁcant drop in performance,\n",
      "requiring re-training from scratch [18].\n",
      "self-supervised learning (ssl) is a promising solution to these limitations.\n",
      "then, a simple linear or non-linear head on top of the frozen\n",
      "pre-trained backbone can be trained for various downstream tasks in a supervised\n",
      "manner (linear or non-linear probing).\n",
      "pre-training the backbone\n",
      "in a self-supervised manner enables scaling to larger datasets across multiple\n",
      "data and task domains.\n",
      "in contrastive learning, the model\n",
      "is trained to produce similar vector representations for augmented views of\n",
      "the same image and dissimilar representations for diﬀerent images.\n",
      "contrastive\n",
      "methods can also be used to learn dense, i.e., patch-level or even pixel- or voxel-\n",
      "level representations: pixels of augmented image views from the same region\n",
      "of the original image should have similar representations, while diﬀerent pixels\n",
      "should have dissimilar ones\n",
      "[23].\n",
      "several works have implemented contrastive learning of dense representa-\n",
      "tions in medical imaging [2,7,25,26,29]. representations in [7,25] do not resolve\n",
      "nearby voxels due to the negative sampling strategy and the architectural rea-\n",
      "sons.\n",
      "this makes them unsuitable for full-resolution segmentation, especially in\n",
      "linear and non-linear probing regimes.\n",
      "in\n",
      "[29], separate global and voxel-wise representations are learned in a contrastive\n",
      "manner to implement eﬃcient dense image retrieval.\n",
      "the common weakness of all the above works is that they do not evaluate\n",
      "their ssl models in linear or non-linear probing setups, even though these setups\n",
      "are de-facto standards for evaluation of ssl methods in natural images [8,13,23].\n",
      "our simple negative sampling\n",
      "strategy and the idea of storing voxel-level representations in a feature pyramid\n",
      "form result in high-dimensional, ﬁne-grained, multi-scale representations suitable\n",
      "for the segmentation of diﬀerent organs and tumors in full resolution.\n",
      "second,\n",
      "we employ vox2vec to pre-train a fpn architecture on a diverse collection of six\n",
      "unannotated datasets, totaling over 6,500 ct images of the thorax and abdomen.\n",
      "we make the pre-trained model publicly available to simplify the reproduction\n",
      "of our results and to encourage practitioners to utilize this model as a starting\n",
      "vox2vec\n",
      "607\n",
      "point for the segmentation algorithms training.\n",
      "finally, we compare the pre-\n",
      "trained model with the baselines on 22 segmentation tasks on seven ct datasets\n",
      "in three setups: linear probing, non-linear probing, and ﬁne-tuning.\n",
      "several methods produce dense or pixel-wise vector representations [6,23,28]\n",
      "to pre-train models for downstream tasks like segmentation or object detection.\n",
      "the methods initially proposed for natural images are often used to pre-\n",
      "train models on medical images.\n",
      "in [25], authors propose the 3d adaptation of\n",
      "jigsaw puzzle, rotation prediction, patch position prediction, and image-level\n",
      "contrastive learning.\n",
      "another common way for pre-training on medical images\n",
      "is to combine diﬀerent approaches such as rotation prediction [26], restorative\n",
      "autoencoders\n",
      "[2,26], and image-level contrastive learning\n",
      "the model [29] maxi-\n",
      "mizes the consistency of local features in the intersection between two diﬀerently\n",
      "augmented images.\n",
      "[29] was mainly proposed for image retrieval\n",
      "and uses only feature representations in the largest and smallest scales in separate\n",
      "contrastive losses, while vox2vec produce voxels’ representations via concatena-\n",
      "tion of feature vectors from a feature pyramid and pre-train them in a uniﬁed\n",
      "manner using a single contrastive loss.\n",
      "finally, a number of works propose semi-\n",
      "supervised contrastive learning methods [20], however, they require additional\n",
      "task-speciﬁc manual labeling.\n",
      "left: two overlapping aug-\n",
      "mented 3d patches are sampled from each volume in a batch.\n",
      "we also describe the methodology of the evaluation of the pre-trained\n",
      "representations on downstream segmentation tasks in sect.\n",
      "we apply color augmentations to them, including random gaussian\n",
      "vox2vec\n",
      "609\n",
      "blur, random gaussian sharpening, adding random gaussian noise, clipping the\n",
      "intensities to the random hounsﬁeld window, and rescaling them to the (0, 1)\n",
      "interval.\n",
      "in our experiments we set (h, w, d) = (128, 128, 32), n = 10 and m = 1000.\n",
      "however,\n",
      "our experiments show that this feature map alone is insuﬃcient for modeling\n",
      "self-supervised voxel-level representations.\n",
      "meanwhile, to be suitable for many downstream tasks, rep-\n",
      "resentations should have a dimensionality of about 1000, as in [8].\n",
      "to address this issue, we utilize a 3d fpn architecture instead of a stan-\n",
      "dard 3d unet.\n",
      "each next pyramid level has twice as many channels\n",
      "and two times lower resolution than the previous one.\n",
      "we use fpn with six pyramid levels, which results in 1008-dimensional\n",
      "representations.\n",
      "i , i = 1, . . . , n. following [8], instead of\n",
      "penalizing the representations directly, we project them on 128-dimensional unit\n",
      "sphere via a trainable 3-layer perceptron g(·) followed by l2-normalization: z(1)\n",
      "i\n",
      "=\n",
      "g(h(1)\n",
      "i )/∥g(h(1)\n",
      "i )∥, z(2)\n",
      "610\n",
      "m. goncharov et al.\n",
      "3.4\n",
      "evaluation protocol\n",
      "we evaluate the quality of self-supervised voxel-level representations on down-\n",
      "stream segmentation tasks in three setups: 1) linear probing, 2) non-linear prob-\n",
      "ing, and 3) end-to-end ﬁne-tuning.\n",
      "linear or non-linear probing means training a voxel-wise linear or non-linear\n",
      "classiﬁer on top of the frozen representations.\n",
      "if the representations are modeled\n",
      "by the unet model, such classiﬁer can be implemented as one or several 1 × 1\n",
      "convolutional layers with a kernel size 1 on top of the output feature map.\n",
      "a\n",
      "linear voxel-wise head (linear fpn head) can be implemented as follows.\n",
      "4\n",
      "experiments\n",
      "4.1\n",
      "pre-training\n",
      "we use vox2vec to pre-train both fpn and unet models (further vox2vec-\n",
      "fpn and vox2vec-unet) in order to ablate the eﬀect of using a feature pyramid\n",
      "instead of single full-resolution feature map for modeling voxel-wise representa-\n",
      "tions.\n",
      "[1,3,5,15,21,27], totaling\n",
      "more than 6550 cts, covering abdomen and thorax domains.\n",
      "we do not use\n",
      "the annotations for these datasets during the pre-training stage.\n",
      "pre-processing\n",
      "includes the following steps: 1) cropping to the minimal volume containing all\n",
      "the voxels with the intensity greater than −500 hu; 2) interpolation to the voxel\n",
      "spacing of 1 × 1 × 2 mm3 (intensities are clipped and rescaled at the augmen-\n",
      "tation step, see sect. 3.1).\n",
      "both models are trained on\n",
      "a single a100-40gb gpu for an average of 3 days.\n",
      "further details about the\n",
      "pre-training setup can be found in supplementary materials.\n",
      "4.2\n",
      "evaluation\n",
      "we evaluate our method on the beyond the cranial vault abdomen (btcv)\n",
      "[19]\n",
      "and medical segmentation decathlon (msd)\n",
      "we test our\n",
      "method on 6 ct msd datasets, which include 9 diﬀerent organ and tumor\n",
      "segmentation tasks.\n",
      "a 5 fold cross-validation is used for btcv experiments, and\n",
      "vox2vec\n",
      "611\n",
      "a 3 fold cross-validation for msd experiments.\n",
      "the segmentation performance\n",
      "of each model on btcv and msd datasets is evaluated by the dice score.\n",
      "for our method, the pre-processing steps are the same for all datasets, as at\n",
      "the pre-training stage, but in addition, intensities are clipped to (−1350, 1000)\n",
      "hu window and rescaled to (0, 1).\n",
      "in these experiments, we keep the crucial pipeline hyperpa-\n",
      "rameters (e.g., spacing, clipping window, patch size) the same as in the original\n",
      "works.\n",
      "we\n",
      "demonstrate an example of the excellent performance of vox2vec-fpn in both\n",
      "linear and non-linear probing regimes in supplementary materials.\n",
      "we reproduce the key results on msd challenge ct datasets, which contain\n",
      "tumor and organ segmentation tasks.\n",
      "a t-sne embedding of vox2vec\n",
      "representations on msd is available in the supplementary materials.\n",
      "average cross validation dice scores\n",
      "on btcv multi-organ segmentation dataset.\n",
      "2.\n",
      "dice score on btcv cross-\n",
      "validation averaged for all organs w.r.t.\n",
      "we pre-train a fpn architecture on\n",
      "more than 6500 ct images and test it on various segmentation tasks, including\n",
      "diﬀerent organs and tumors segmentation in three setups: linear probing, non-\n",
      "linear probing, and ﬁne-tuning.\n",
      "we plan to investigate\n",
      "further how the performance of vox2vec scales with the increasing size of the\n",
      "vox2vec\n",
      "613\n",
      "pre-training dataset and the pre-trained architecture size.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf:\n",
      "transfer learning is a critical technique in training deep neu-\n",
      "ral networks for the challenging medical image segmentation task that\n",
      "requires enormous resources.\n",
      "with the abundance of medical image data,\n",
      "many research institutions release models trained on various datasets\n",
      "that can form a huge pool of candidate source models to choose from.\n",
      "to make up for its deﬁciency when applying trans-\n",
      "fer learning to medical image segmentation, in this paper, we therefore\n",
      "propose a new transferability estimation (te) method.\n",
      "we ﬁrst\n",
      "analyze the drawbacks of using the existing te algorithms for medical\n",
      "image segmentation and then design a source-free te framework that\n",
      "considers both class consistency and feature variety for better estima-\n",
      "tion.\n",
      "extensive experiments show that our method surpasses all current\n",
      "algorithms for transferability estimation in medical image segmentation.\n",
      "keywords: transferability estimation · model selection · medical\n",
      "image analysis · deep learning\n",
      "1\n",
      "introduction\n",
      "the development of deep neural networks has greatly promoted medical imaging-\n",
      "based computer-aided diagnosis.\n",
      "however, the labeling process of medical images is tedious and time-\n",
      "consuming.\n",
      "to address this problem, the common paradigm of transfer learning,\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0_64.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43907-0_64\n",
      "pick the best pre-trained model\n",
      "675\n",
      "which ﬁrst pre-trains a model on upstream image datasets and then ﬁne-tunes it\n",
      "on various target tasks, has been widely investigated in recent years\n",
      "[18] enable researchers\n",
      "to experiment across a large number of downstream datasets and tasks.\n",
      "these\n",
      "pre-trained models require less training time and have better performance and\n",
      "robustness compared with the learning-from-scratch models.\n",
      "when the knowledge is transferred from a less\n",
      "relevant source, it may not improve the performance or even negatively aﬀect\n",
      "the intended outcome [24].\n",
      "however, most of these works\n",
      "require source information available while medical images have more privacy and\n",
      "ethical issues and fewer datasets are publicly available than natural images.\n",
      "considering the issues mentioned above, this work focused on source-free\n",
      "pre-trained model selection for segmentation tasks in the medical image.\n",
      "logme [27] computed evidence based on the linear parameters assumption and\n",
      "eﬃciently leverages the compatibility between features and labels.\n",
      "these methods have achieved\n",
      "promising performance on classiﬁcation and regression tasks without fully con-\n",
      "sidering the properties of medical image segmentation.\n",
      "first, unlike classiﬁcation\n",
      "and regression problems that can use a single n-dimensional feature vector to rep-\n",
      "resent each image, segmentation problems lack a global semantic representation,\n",
      "which poses diﬃculties for direct transferability estimation.\n",
      "third, medical images face severe class imbalance problems,\n",
      "with excessive diﬀerences between foreground and background.\n",
      "676\n",
      "y. yang et al.\n",
      "fine-tune\n",
      "performance\n",
      "model bank \n",
      "dnn1\n",
      "upstream data\n",
      "downstream data\n",
      "dnn2\n",
      "dnn3\n",
      "pre-train-then-fine-tune process\n",
      "pre-train\n",
      "match or not?\n",
      "our main goal is to\n",
      "predict the performance of models in the model bank after ﬁne-tuning on downstream\n",
      "tasks without actually ﬁne-tuning.\n",
      "besides, for semantic segmentation tasks, the feature pyramid is critical for\n",
      "the segmentation output of multi-scale objects while existing works neglect it.\n",
      "in our work, we propose a new method using class consistency and feature\n",
      "variety(cc-fv) with an eﬃcient framework to estimate the transferability in\n",
      "medical image segmentation tasks.\n",
      "extensive experiments\n",
      "have proved the superiority of our method compared with baseline methods.\n",
      "= {xj,yj}n\n",
      "j=1, where\n",
      "xj is the image and yj is the ground truth of segmentation.\n",
      "after ﬁne-tuning,\n",
      "the performance of mi can be measured with the segmentation metric (e.g. dice\n",
      "score), which is denoted by pi\n",
      "s→t in this paper.\n",
      "given a pair of target data xj and xj′, the distribution of the features is\n",
      "modeled with the n-dimensional gaussian distribution.\n",
      "and σf k\n",
      "j\n",
      "and\n",
      "σf k\n",
      "j′ are covariance matrices of fk\n",
      "j and fk\n",
      "j′. compared to some commonly used\n",
      "metrics like kl-divergence or bhattacharyya distance [17], wasserstein distance\n",
      "is more stable during the computation of high-dimensional matrices because it is\n",
      "unnecessary to compute the determinant or inverse of a high-dimensional matrix,\n",
      "which can easily lead to an overﬂow in numerical computation.\n",
      "we calculate the\n",
      "wasserstein distance of the distribution with voxels of the same class in a sample\n",
      "pair comprised of every two samples in the dataset, and obtained the following\n",
      "deﬁnition of class consistency ccons\n",
      "ccons =\n",
      "1\n",
      "n(n − 1)\n",
      "c\n",
      "\u0007\n",
      "k=1\n",
      "\u0007\n",
      "i\u0002j\n",
      "w2(fk\n",
      "i , fk\n",
      "j )\n",
      "(2)\n",
      "given that 3d medical images are computationally intensive, and prone to\n",
      "causing out-of-memory problems, in the sliding window inference process for\n",
      "678\n",
      "y. yang et al.\n",
      "each case, we do not concatenate the output of each patch into the ﬁnal predic-\n",
      "tion result, but directly sample from the patched output and concatenate them\n",
      "into the ﬁnal sampled feature matrix.\n",
      "in the calculation of class consistency, we\n",
      "only sample the foreground voxels with a pre-deﬁned sampling number which is\n",
      "proportional to the voxel number of each class in the image because of the severe\n",
      "class imbalance problem.\n",
      "we\n",
      "believe that the essential reason for this phenomenon is that class consistency is\n",
      "only concerned with local homogeneity of information while neglecting the inte-\n",
      "gral feature quality assessment.\n",
      "i\u0002j log\n",
      "\u0003\u0002\u0002vi − vj\n",
      "\u0002\u0002−1\u0004\n",
      ",\n",
      "s = 0\n",
      "(3)\n",
      "here v is sampled feature of each image with point-wise embedding vi and\n",
      "l is the length of the feature, which is also the number of sampled voxels.\n",
      "as for semantic segmentation problems, the feature pyra-\n",
      "mid structure is critical for segmentation results [14,29].\n",
      "pick the best pre-trained model\n",
      "679\n",
      "3\n",
      "experiment\n",
      "3.1\n",
      "experiment on msd dataset\n",
      "the medical segmentation decathlon (msd)\n",
      "[2] dataset is composed of ten dif-\n",
      "ferent datasets with various challenging characteristics, which are widely used in\n",
      "the medical image analysis ﬁeld.\n",
      "to evaluate the eﬀectiveness of cc-fv, we con-\n",
      "duct extensive experiments on 5 of the msd dataset, including task03 liver(liver\n",
      "and tumor segmentation), task06 lung(lung nodule segmentation), task07 pan-\n",
      "creas(pancreas and pancreas tumor segmentation), task09 spleen(spleen seg-\n",
      "mentation), and task10 colon(colon cancer segmentation).\n",
      "all of the datasets\n",
      "are 3d ct images.\n",
      "the public part of the msd dataset is chosen for our experi-\n",
      "ments, and each dataset is divided into a training set and a test set at a scale of\n",
      "80% and 20%.\n",
      "for each dataset, we use the other four datasets to pre-train the\n",
      "model and ﬁne-tune the model on this dataset to evaluate the performance as\n",
      "well as the transferability using the correlation between two ranking sequences\n",
      "of upstream pre-trained models.\n",
      "[15] are also implemented.\n",
      "figure 2 visualizes the average dice score and the estimation value on task\n",
      "03 liver.\n",
      "[20]\n",
      "and unetr [8] are applied in the experiment and each model is pre-trained\n",
      "for 250k iterations and ﬁne-tuned for 100k iterations with batch size 2 on a\n",
      "single nvidia a100 gpu.\n",
      "besides, we use the model at the end of training\n",
      "for inference and calculate the ﬁnal dsc performance on the test set.\n",
      "[27] and pearson correlation coeﬃcient for the cor-\n",
      "relation between the te results and ﬁne-tuning performance.\n",
      "the kendall’s τ\n",
      "ranges from [-1, 1], and τ=1 means the rank of te results and performance\n",
      "are perfectly correlated(t i\n",
      "s→t >\n",
      "it is clear that the\n",
      "te results of our method have a more positive correlation with respect to dsc\n",
      "performance.\n",
      "most\n",
      "of the existing methods are inferior to ours because they are not designed for seg-\n",
      "mentation tasks with a serious class imbalance problem.\n",
      "2. correlation between the ﬁne-tuning performance and transferability metrics\n",
      "using task03 as an example.\n",
      "the vertical axis represents the average dice of the\n",
      "model, while the horizontal axis represents the transferability metric results.\n",
      "we have\n",
      "standardized the various metrics uniformly, aiming to observe a positive relationship\n",
      "between higher performance and higher transferability estimations.\n",
      "pre-trained models tend to have a more consistent\n",
      "distribution within a class than the randomly initialized model and after ﬁne-tuning\n",
      "they often have a better dice performance than the randomly initialized models.\n",
      "then we com-\n",
      "pare the performance of our method at single and multiple scales to prove the\n",
      "eﬀectiveness of our multi-scale strategy.\n",
      "kl-divergence and bha-distance are unstable in\n",
      "high dimension matrics calculation and the performance is also inferior to the\n",
      "wasserstein distance.\n",
      "we can easily ﬁnd that with models with a pre-training process\n",
      "have a more compact intra-class distance and a higher ﬁne-tuning performance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf:\n",
      "cell segmentation plays a critical role in diagnosing various\n",
      "cancers.\n",
      "in this paper, we present\n",
      "a novel framework for cross-tissue domain adaptative cell segmentation\n",
      "without access both source domain data and model parameters, namely\n",
      "multi-source black-box domain adaptation (mbda).\n",
      "given the target\n",
      "domain data, our framework can achieve the cell segmentation based on\n",
      "knowledge distillation, by only using the outputs of models trained on\n",
      "multiple source domain data.\n",
      "sec-\n",
      "ond, we design a pseudo-label cutout and selection strategy for these pre-\n",
      "dictions to facilitate the knowledge distillation from local cells to global\n",
      "pathological images.\n",
      "experimental results on four types of pathologi-\n",
      "cal tissues demonstrate that our proposed black-box domain adaptation\n",
      "approach can achieve comparable and even better performance in com-\n",
      "parison with state-of-the-art white-box approaches.\n",
      "the code and dataset\n",
      "are released at: https://github.com/neuronxjtu/mbda-cellseg.\n",
      "keywords: multi-source domain adaptation · black-box model · cell\n",
      "segmentation · knowledge distillation\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43907-0_71\n",
      "750\n",
      "x. wang et al.\n",
      "1\n",
      "introduction\n",
      "semantic segmentation plays a vital role in pathological image analysis.\n",
      "it can\n",
      "help people conduct cell counting, cell morphology analysis, and tissue analysis,\n",
      "which reduces human labor [19].\n",
      "however, data acquisition for medical images\n",
      "poses unique challenges due to privacy concerns and the high cost of manual\n",
      "annotation.\n",
      "moreover, pathological images from diﬀerent tissues or cancer types\n",
      "often show signiﬁcant domain shifts, which hamper the generalization of mod-\n",
      "els trained on one dataset to others.\n",
      "due to the abovementioned challenges,\n",
      "some researchers have proposed various white-box domain adaptation methods\n",
      "to address these issues.\n",
      "recently, [8,16] propose to use generative adversarial networks to align the\n",
      "distributions of source and target domains and generate source-domain look-\n",
      "alike outputs for target images.\n",
      "there are also many studies\n",
      "on multi-source white-box domain adaptation.\n",
      "[13] extend the above work to semantic segmentation and proposed\n",
      "a method named model-invariant feature learning, which takes full advantage of\n",
      "the diverse characteristics of the source-domain models.\n",
      "nonetheless, several recent investigations have demonstrated that the domain\n",
      "adaptation methods for source-free white-box models still present a privacy risk\n",
      "due to the potential leakage of model parameters [4].\n",
      "such privacy breaches\n",
      "may detrimental to the privacy protection policies of hospitals.\n",
      "we thus present\n",
      "a more challenging task of relying solely on black-box models from vendors to\n",
      "avoid parameter leakage.\n",
      "in clinical applications, various vendors can oﬀer output\n",
      "interfaces for diﬀerent pathological images.\n",
      "while black-box models are proﬁcient\n",
      "in speciﬁc domains, their performances greatly degrade when the target domain\n",
      "is updated with new pathology slices.\n",
      "therefore, how to leverage the existing\n",
      "knowledge of black-box models to eﬀectively train new models for the target\n",
      "domain without accessing the source domain data remains a critical challenge.\n",
      "in this paper, we present a novel source-free domain adaptation framework\n",
      "for cross-tissue cell segmentation without accessing both source domain data\n",
      "and model parameters, which can seamlessly integrate heterogeneous models\n",
      "from diﬀerent source domains into any cell segmentation network with high gen-\n",
      "erality.\n",
      "to the best of our knowledge, this is the ﬁrst study on the exploration of\n",
      "multi-source black-box domain adaptation for cross-tissue cell segmentation.\n",
      "in\n",
      "this setting, conventional multi-source ensemble methods are not applicable due\n",
      "to the unavailability of model parameters, and simply aggregating the black-box\n",
      "outputs would introduce a considerable amount of noise, which can be detri-\n",
      "mental to the training of the target domain model.\n",
      "therefore, we develop two\n",
      "black-box domain adaptative cell segmentation\n",
      "751\n",
      "multi-source \n",
      "models\n",
      "(black-box)\n",
      "logits map\n",
      "prediction uncertainty boundary ambiguity\n",
      "score\n",
      "weighted logits map\n",
      "confidence threshold\n",
      "adaptive\n",
      "unlabeled\n",
      "labeled\n",
      "pseudo-cutout \n",
      "label\n",
      "student model\n",
      "pixel-level weight\n",
      "target domain\n",
      "0.97 0.90\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.92\n",
      "0.93 0.99\n",
      "0.92 0.92\n",
      "0.95 0.92\n",
      "vote\n",
      "teacher model\n",
      "ema\n",
      "fig.\n",
      "this method eﬀectively addresses two signiﬁcant\n",
      "challenges encountered in the analysis of cellular images, namely, the uncertainty\n",
      "in source domain output and the ambiguity in cell boundary semantics.\n",
      "secondly,\n",
      "we also take into account the structured information from cells to images, which\n",
      "may be overlooked during distillation, and design an adaptive knowledge vot-\n",
      "ing strategy.\n",
      "2\n",
      "method\n",
      "overview: figure 1 shows a binary cell segmentation task with three source\n",
      "models trained on diﬀerent tissues and a target model, i.e., the student model\n",
      "in fig.\n",
      "the η\n",
      "and η′ indicate that diﬀerent perturbations are added to the target images.\n",
      "sub-\n",
      "sequently, we feed the perturbed images into the source domain predictor to\n",
      "generate the corresponding raw segmentation outputs.\n",
      "finally, we obtain a weighted logit for knowledge distillation\n",
      "from pixel level and a high-conﬁdence pseudo-cutout label for further structured\n",
      "distillation from cell to global pathological image.\n",
      "accordingly, direct knowledge transfer using the output of the source domain\n",
      "predictor may lead to feature bias in the student model due to the unavoid-\n",
      "able covariance [20] between the target and source domains.\n",
      "for a given target image xt\n",
      "to leverage the rich\n",
      "semantic information from the source domain predictor predictions, we utilize\n",
      "predictive entropy of the softmax outputs to measure the prediction uncertainty\n",
      "scores.\n",
      "in the semantic segmentation scenario of c-classes classiﬁcation, we deﬁne\n",
      "the pixel-level uncertainty score u(i,j)\n",
      "n\n",
      "as follow:\n",
      "u(i,j)\n",
      "n\n",
      "= −\n",
      "c\n",
      "\u0002\n",
      "c=1\n",
      "on(i,j,c)\n",
      "s\n",
      "log on(i,j,c)\n",
      "s\n",
      "(2)\n",
      "where on\n",
      "s denotes softmax output,i.e.,on\n",
      "s = softmax(pn\n",
      "s ) from nth source predic-\n",
      "tor.\n",
      "after that, we determine the degree of\n",
      "black-box domain adaptative cell segmentation\n",
      "753\n",
      "impurity in an area of interest by analyzing the statistics of the boundary region,\n",
      "which represents the level of semantic information ambiguity.\n",
      "by assigning lower weights to the pixels with high uncertainty and boundary\n",
      "ambiguity, we can obtain pixel-level weight scores wn for each pn\n",
      "s , i.e.,\n",
      "wn = − log\n",
      "\u0004\n",
      "exp (un ⊙ pn)\n",
      "\u0005n\n",
      "n=1 exp (un ⊙ pn)\n",
      "\u0006\n",
      "(4)\n",
      "where ⊙ denotes element-wise matrix multiplication.\n",
      "adaptive pseudo-cutout label: as previously mentioned, the outputs from\n",
      "the source domain black-box predictors have been adjusted by the pixel-level\n",
      "weight.\n",
      "we have revised the method in [7]\n",
      "to generate high-quality pseudo labels that resemble the cutout augmentation\n",
      "technique.\n",
      "[15].\n",
      "we adopt the classical and eﬀective mean-teacher framework as a baseline\n",
      "for semi-supervised learning and update the teacher model parameters by expo-\n",
      "nential moving average.\n",
      "3\n",
      "experiments\n",
      "dataset and setting: we collect four pathology image datasets to validate our\n",
      "proposed approach.\n",
      "firstly, we acquire 50 images from a cohort of patients with\n",
      "triple negative breast cancer (tnbc), which is released by naylor et al\n",
      "[10] publish a dataset of nucleus segmentation containing 5,060 seg-\n",
      "mented slides from 10 tcga cancer types.\n",
      "in this work, we use 98 images from\n",
      "black-box domain adaptative cell segmentation\n",
      "755\n",
      "fig.\n",
      "2. visualized segmentation on the brca and kirc target domains respectively.\n",
      "we have also included 463 images\n",
      "of kidney renal clear cell carcinoma (kirc) in our dataset, which are made\n",
      "publicly available by irshad et al\n",
      "[2] publicly release a dataset\n",
      "containing tissue slide images and associated clinical data on colorectal cancer\n",
      "(crc), from which we randomly select 200 patches for our study.\n",
      "in our exper-\n",
      "iments, we transfer knowledge from three black-box models trained on diﬀerent\n",
      "source domains to a new target domain model (e.g.,from crc, tnbc, kirc\n",
      "to brca).\n",
      "the backbone network for the student model and source domain\n",
      "black-box predictors employ the widely adopted residual u-net [12], which is\n",
      "commonly used for medical image segmentation.\n",
      "for each source domain net-\n",
      "work, we conduct full-supervision training on the corresponding source domain\n",
      "data and directly evaluate its performance on target domain data.\n",
      "the upper\n",
      "performance metrics (source-only upper) are shown in the table 1.\n",
      "experimental results: to validate our method, we compare it with the fol-\n",
      "lowing approaches: (1) cellsegssda\n",
      "for single-source domain\n",
      "adaptation approach, cellsegssda and sfda-dpl, we employ two strategies\n",
      "to ensure the fairness of the experiments: (1) single-source, i.e. performing adap-\n",
      "tation on each single source, where we select the best results to display in the\n",
      "table 1; (2) source-combined, i.e. all source domains are combined into a tra-\n",
      "ditional single source.\n",
      "2 demonstrate that our proposed\n",
      "method exhibits superior performance, even when compared to these white-box\n",
      "methods, surpassing them in various evaluation metrics and visualization results.\n",
      "in addition, the experimental results also show that simply combining multiple\n",
      "756\n",
      "x. wang et al.\n",
      "table 1.\n",
      "quantitative comparison with unsupervised and semi-supervised domain\n",
      "adaptation methods under 3 segmentation metrics.\n",
      "crc&kirc&brca to tnbc\n",
      "wl pcl mmi\n",
      "dice\n",
      "hd95\n",
      "assd\n",
      "×\n",
      "×\n",
      "×\n",
      "0.6708\n",
      "56.9111\n",
      "16.3837\n",
      "✓\n",
      "×\n",
      "×\n",
      "0.6822\n",
      "54.3386 14.9817\n",
      "✓\n",
      "✓\n",
      "×\n",
      "0.6890\n",
      "57.0889\n",
      "12.9512\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "0.7075 58.8798\n",
      "10.7247\n",
      "source data into a traditional single source will result in performance degrada-\n",
      "tion in some cases, which also proves the importance of studying multi-source\n",
      "domain adaptation methods.\n",
      "ablation study: to evaluate the impact of our proposed methods of weighted\n",
      "logits(wl), pseudo-cutout label(pcl) and maximize mutual information(mmi)\n",
      "on the model performance, we conduct an ablation study.\n",
      "the results of these experiments, presented in the table 2, show that our pro-\n",
      "posed modules are indeed useful.\n",
      "4\n",
      "conclusion\n",
      "our proposed multi-source black-box domain adaptation method achieves com-\n",
      "petitive performance by solely relying on the source domain outputs, without\n",
      "the need for access to the source domain data or models, thus avoiding informa-\n",
      "tion leakage from the source domain.\n",
      "additionally, the method does not assume\n",
      "black-box domain adaptative cell segmentation\n",
      "757\n",
      "the same architecture across domains, allowing us to learn lightweight target\n",
      "models from large source models, improving learning eﬃciency.\n",
      "by leveraging multi-source domain\n",
      "knowledge, we aim to improve the reliability of the target model and enable more\n",
      "eﬃcient annotation for better model performance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf:\n",
      "statistical shape modeling is the computational process of\n",
      "discovering signiﬁcant shape parameters from segmented anatomies cap-\n",
      "tured by medical images (such as mri and ct scans), which can fully\n",
      "describe subject-speciﬁc anatomy in the context of a population.\n",
      "the\n",
      "presence of substantial non-linear variability in human anatomy often\n",
      "makes the traditional shape modeling process challenging.\n",
      "we propose mesh2ssm, a new approach that leverages\n",
      "unsupervised, permutation-invariant representation learning to estimate\n",
      "how to deform a template point cloud to subject-speciﬁc meshes, form-\n",
      "ing a correspondence-based shape model.\n",
      "mesh2ssm can also learn a\n",
      "population-speciﬁc template, reducing any bias due to template selec-\n",
      "tion.\n",
      "keywords: statistical shape modeling · representation learning ·\n",
      "point distribution models\n",
      "1\n",
      "introduction\n",
      "statistical shape modeling (ssm) is a powerful tool in medical image analysis\n",
      "and computational anatomy to quantify and study the variability of anatomical\n",
      "structures within populations.\n",
      "[19,25], and treatment\n",
      "planning [27].\n",
      "ssm has enabled researchers to better understand the underlying\n",
      "biological processes, leading to the development of more accurate and personal-\n",
      "ized diagnostic and treatment plans\n",
      "[3,9,14,17].\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0 59.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "[5].\n",
      "ssm performance depends on the underlying process used to generate shape\n",
      "correspondences and the quality of the input data.\n",
      "non-optimized methods\n",
      "manually label a reference shape and warp the annotated landmarks using reg-\n",
      "istration techniques [10,16,18].\n",
      "[15] and shapeflow [11] operate\n",
      "on surface meshes and use neural networks to parameterize the deformations ﬁeld\n",
      "between two shapes in a low dimensional latent space and rely on an encoder-\n",
      "free setup.\n",
      "[1] learn the pdm directly from unsegmented ct/mri images, and\n",
      "hence alleviate the need for pdm optimization given new samples and can bypass\n",
      "anatomy segmentation by operating directly on unsegmented images.\n",
      "however,\n",
      "these methods rely on supervised losses and require volumetric images, seg-\n",
      "mented images, and established/optimized pdms for training.\n",
      "mesh2ssm leverages unsupervised, permutation-invariant representation learn-\n",
      "ing to learn the low dimensional nonlinear shape descriptor directly from mesh\n",
      "data and uses the learned features to generate a correspondence model of the\n",
      "population.\n",
      "mesh2ssm also includes an analysis network that operates on the\n",
      "learned correspondences to obtain a data-driven template point cloud (i.e., tem-\n",
      "plate point cloud), which can replace the initial template, and hence reducing\n",
      "the bias that could arise from template selection.\n",
      "we performed experiments with three templates: medoid, sphere,\n",
      "and box without the bump.\n",
      "moreover, flowssm fails to identify the correct mode\n",
      "of variation, the horizontal movement of the bump as the primary variation,\n",
      "which can also be inferred by comparing the compactness curves in fig.\n",
      "flowssm\n",
      "fails to capture the horizontal movement as the primary mode of variation.\n",
      "this goal is achieved by learning a low dimensional represen-\n",
      "tation of the surface mesh zm ∈ rl using the mesh autoencoder and then zm is\n",
      "mesh2ssm\n",
      "619\n",
      "used to transform the template point cloud via the implicit ﬁeld decoder (im-\n",
      "net)\n",
      "by capturing the underlying structure of the\n",
      "pdm through a low-dimensional representation, sp-vae allows for the estima-\n",
      "tion of the mean shape of the learned correspondences.\n",
      "importantly, the sp-vae maintains\n",
      "the same ordering of correspondences at the input and output, so it does not\n",
      "use permutation-invariant layers or operations like pooling.\n",
      "2.3\n",
      "training\n",
      "we begin with a burn-in stage, where only the correspondence generation mod-\n",
      "ule is trained while the analysis module is frozen.\n",
      "after the burn-in stage, alter-\n",
      "nate optimization of the correspondence and analysis module begins.\n",
      "the mean template is deﬁned by taking the average of these gener-\n",
      "ated samples.\n",
      "all hyperparameters and network\n",
      "architecture details are mentioned in the supplementary material.\n",
      "3\n",
      "experiments and discussion\n",
      "dataset: we use the publicly available decath-pancreas dataset of 273 seg-\n",
      "mentations from patients who underwent pancreatic mass resection\n",
      "the segmentations were isotropi-\n",
      "cally resampled, smoothed, centered, and converted to meshes with roughly 2000\n",
      "vertices.\n",
      "although the dgcnn mesh autoencoder used in mesh2ssm does not\n",
      "require the same number of vertices, uniformity across the dataset makes it com-\n",
      "putationally eﬃcient; hence, we pad the smallest mesh by randomly repeating\n",
      "the vertices (akin to padding image for convolutions).\n",
      "the color map and arrows show\n",
      "the signed distance and direction from the mean shape.\n",
      "3.1\n",
      "results\n",
      "we perform experiments with two templates: sphere and medoid.\n",
      "we compare\n",
      "the performance of flowssm\n",
      "generalization measures the average sur-\n",
      "face distance between all test shapes and their reconstructions, and speciﬁcity\n",
      "measures the distance between randomly generated pca samples.\n",
      "using\n",
      "the analysis module of mesh2ssm, we visualized the top three modes of varia-\n",
      "tion identiﬁed by sorting the latent dimensions of sp-vae based on the stan-\n",
      "dard deviations of the latent embeddings of the training dataset.\n",
      "like most deep learning models, perfor-\n",
      "mance of mesh2ssm could be aﬀected by small dataset size, and it can produce\n",
      "overconﬁdent estimates.\n",
      "an augmentation scheme and a layer uncertainty cali-\n",
      "bration are could improve its usability in medical scenarios.\n",
      "additionally, enforc-\n",
      "ing disentanglement in the latent space of sp-vae can make the analysis module\n",
      "interpretable and allow for eﬀective non-linear shape analysis by clinicians.\n",
      "[13,21] analysis module helps in mitigating bias and\n",
      "capturing non-linear characteristics of the data.\n",
      "the method is demonstrated to\n",
      "have superior performance in identifying shape variations using fewer parameters\n",
      "on synthetic and clinical datasets.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf:\n",
      "multiple instance learning is an ideal mode of analysis for\n",
      "histopathology data, where vast whole slide images are typically anno-\n",
      "tated with a single global label.\n",
      "in such cases, a whole slide image is\n",
      "modelled as a collection of tissue patches to be aggregated and classi-\n",
      "ﬁed.\n",
      "although powerful compression algo-\n",
      "rithms, such as deep pre-trained neural networks, are used to reduce\n",
      "the dimensionality of each patch, the sequences arising from whole slide\n",
      "images remain excessively long, routinely containing tens of thousands\n",
      "of patches.\n",
      "across experiments\n",
      "in metastasis detection, cancer subtyping, mutation classiﬁcation, and\n",
      "multitask learning, we demonstrate the competitiveness of this new class\n",
      "of models with existing state of the art approaches.\n",
      "keywords: multiple instance learning · whole slide images · state\n",
      "space models\n",
      "1\n",
      "introduction\n",
      "precision medicine eﬀorts are shifting cancer care standards by providing novel\n",
      "personalised treatment plans with promising outcomes.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0 57.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43907-0_57\n",
      "state space models in digital pathology\n",
      "595\n",
      "treatment regimes is based principally on the assessment of tissue biopsies and\n",
      "the characterisation of the tumor microenvironment.\n",
      "this is typically performed\n",
      "by experienced pathologists, who closely inspect chemically stained histopatho-\n",
      "logical whole slide images (wsis).\n",
      "the resulting images are of gigapixel size, rendering their computational analysis\n",
      "challenging.\n",
      "in\n",
      "such schemes, the wsi is typically divided into a grid of patches, with general\n",
      "purpose features derived from pretrained imagenet\n",
      "in this paper, we present the\n",
      "ﬁrst use of state space models for wsi mil. extensive experiments on three\n",
      "publicly available datasets show the potential of such models for the processing\n",
      "of gigapixel-sized images, under both weakly and multi-task schemes.\n",
      "moreover,\n",
      "comparisons with other commonly used mil schemes highlight their robust per-\n",
      "formance, while we demonstrate empirically the superiority of state space models\n",
      "in processing the longest of wsi sequences with respect to commonly used mil\n",
      "methods.\n",
      "similar to our multitask experiments, [6] explores combining slide-level and tile-\n",
      "level annotations with a minimal point-based annotation strategy.\n",
      "the hippo mode of memorisation is shown empirically to be better suited to\n",
      "modeling long-range dependencies (lrd) than other neural memory layers, for\n",
      "which it serves as a drop-in replacement.\n",
      "note\n",
      "that as a linear operator, the inverse discrete fourier transform is amenable\n",
      "to backpropagation in the context of a neural network.\n",
      "3\n",
      "method\n",
      "given that the patch extraction of whole slide images at high magniﬁcations\n",
      "results in long sequences of patches, we propose to incorporate a state space\n",
      "layer in a mil aggregation network to better represent each patch sequence.\n",
      "note also that although eq. 3 is posed as modeling a one-\n",
      "dimensional signal, in practice multi-dimensional inputs are modelled simply by\n",
      "stacking ssm layers together, followed by an aﬃne “mixing” layer.\n",
      "fig.\n",
      "a pretrained resnet50 is then used\n",
      "to extract a 1024-dimensional feature vector from each patch {u1, u2, . .\n",
      "(5)\n",
      "the architecture of f is composed of an initial linear projection layer, used to\n",
      "lower the dimensionality of each vector in the input sequence.\n",
      "a linear “mixing” layer is applied\n",
      "token-wise, doubling the dimensionality of each token, followed by a gated linear\n",
      "unit [5] acting as an output gate, which restores the input dimensionality.\n",
      "for\n",
      "598\n",
      "l. fillioux et al.\n",
      "the ssm layer, we used the oﬃcial implementation of s4d1.\n",
      "the model is trained\n",
      "according to,\n",
      "lmil = − 1\n",
      "m\n",
      "m\n",
      "\u0002\n",
      "m=1\n",
      "log ˆycm,\n",
      "(6)\n",
      "where ˆycm denotes the probability corresponding to cm, the slide-level label of\n",
      "the sequence corresponding to the mth of m whole slide images.\n",
      "3.3\n",
      "multitask training\n",
      "one advantage of processing an entire slide as a sequence is the ease with\n",
      "which additional supervision may be incorporated, when available.\n",
      "3.4\n",
      "implementation details\n",
      "we extracted patches of size 256 × 256 from the tissue regions of wsis at 20x\n",
      "magniﬁcation.\n",
      "[13] was used as a feature extractor, followed by a mean pooling oper-\n",
      "ation, resulting in a 1024-dimensional representation for each patch.\n",
      "all model training was performed under\n",
      "a 10-fold cross-validation, and all reported results are averaged over the vali-\n",
      "dation sets of the folds, aside from camelyon16, for which the predeﬁned\n",
      "test set was utilized.\n",
      "thus, for camelyon16, we report test set performances\n",
      "averaged over the validation.\n",
      "our vanilla transformer is composed of two stacked\n",
      "self-attention blocks, with four attention heads, a model dimension of 256, and\n",
      "1 https://github.com/hazyresearch/state-spaces.\n",
      "state space models in digital pathology\n",
      "599\n",
      "a hidden dimension of 256.\n",
      "for the s4\n",
      "models, the dimension of the state matrix a was tuned to 32 for camelyon16\n",
      "and tcga-rcc, and 128 for tcga-luad.\n",
      "our implementation is publicly available2.\n",
      "4\n",
      "experiments and discussion\n",
      "4.1\n",
      "data\n",
      "camelyon16\n",
      "in multitask experiments, we use\n",
      "this annotation to give each patch a label indicating local tumour presence.\n",
      "in our experiments, the average patch sequence length arising from\n",
      "camelyon16 is 6129 (ranging from 127 to 27444).\n",
      "the average sequence length is 10557\n",
      "(ranging from 85 to 34560).\n",
      "the average sequence length is 12234 (ranging from 319 to 62235).\n",
      "for multiclass classiﬁcation, these were computed in a one-versus-rest\n",
      "manner.\n",
      "similarly, in the tcga-luad dataset the proposed model achieves comparable\n",
      "performance with both clam models, while outperforming transmil and the\n",
      "other methods.\n",
      "moreover, our method outperforms clam models on the\n",
      "tcga-rcc dataset, while reporting very similar performance with respect to\n",
      "transmil.\n",
      "overall, looking at the average metrics per model across all three\n",
      "datasets, our proposed method achieves the highest accuracy and the second\n",
      "highest auroc, only behind clam-mb.\n",
      "a pairwise t-test between the pro-\n",
      "posed method, clam, and transmil shows that there is no statistical signiﬁ-\n",
      "cance performance diﬀerence (see supplementary material).\n",
      "the number of parameters is computed with all models con-\n",
      "ﬁgured to be binary classiﬁers, and the inference time is computed as the average\n",
      "time over 100 samples for processing a random sequence of 1024-dimensional vec-\n",
      "tors of length 30000.\n",
      "for our proposed method, we report both models with the\n",
      "diﬀerent state dimensions (ours (ssm32)) and (ours (ssm128)).\n",
      "compared\n",
      "table 1. comparison of accuracy and auroc on three datasets camelyon16,\n",
      "tcga-luad, tcga-rcc, and on average.\n",
      "all metrics in the table are the average\n",
      "of 10 runs.\n",
      "∗ indicates results from [19].\n",
      "dataset\n",
      "camelyon16\n",
      "tcga-luad\n",
      "tcga-rcc\n",
      "average\n",
      "metric\n",
      "acc.\n",
      "auroc acc.\n",
      "auroc acc.\n",
      "auroc acc.\n",
      "auroc\n",
      "mean-pooling\n",
      "0.5969\n",
      "0.5810\n",
      "0.6261\n",
      "0.6735\n",
      "0.8608\n",
      "0.9612\n",
      "0.6946\n",
      "0.7386\n",
      "max-pooling\n",
      "0.7078\n",
      "0.7205\n",
      "0.6328\n",
      "0.6686\n",
      "0.8803\n",
      "0.9659\n",
      "0.7403\n",
      "0.7850\n",
      "transformer [21] 0.5419\n",
      "0.5202\n",
      "0.5774\n",
      "0.6214\n",
      "0.7932\n",
      "0.9147\n",
      "0.6375\n",
      "0.6854\n",
      "lstm\n",
      "models a and\n",
      "b show that stacking multiple ssm layers results in lower accuracy, which was\n",
      "observed over all three datasets, while models c and d show that modifying the\n",
      "state dimension of the ssm module can have an impact on the accuracy.\n",
      "the\n",
      "optimal state space dimension varies depending on the dataset.\n",
      "this\n",
      "indicates that the use of patch-level annotations complements the learning of the\n",
      "slide-level label.\n",
      "we map the sequence of out-\n",
      "put probabilities to their slide coordinates giving a heatmap localising metastasis\n",
      "(see supplementary material).\n",
      "model ssm layers state dimension accuracy auroc\n",
      "a\n",
      "2\n",
      "32\n",
      "0.9236\n",
      "0.9813\n",
      "b\n",
      "3\n",
      "32\n",
      "0.9179\n",
      "0.9834\n",
      "c\n",
      "1\n",
      "16\n",
      "0.9352\n",
      "0.9846\n",
      "d\n",
      "1\n",
      "64\n",
      "0.9352\n",
      "0.9861\n",
      "ours\n",
      "1\n",
      "32\n",
      "0.9426\n",
      "0.9885\n",
      "table 4. comparison of accuracy and auroc for models trained as multitask classi-\n",
      "ﬁers on the camelyon16 dataset.\n",
      "[19] 0.8403\n",
      "0.8828\n",
      "ours\n",
      "0.8488\n",
      "0.8998\n",
      "602\n",
      "l. fillioux et al.\n",
      "performance on longest sequences.\n",
      "in order to highlight the inherent\n",
      "ability of ssm models to eﬀectively model long sequences, we performed an\n",
      "experiment on only the largest wsis of the tcga-rcc dataset.\n",
      "table 5 shows the\n",
      "obtained average accuracy (weighted by the number of long sequences in each\n",
      "validation set) and auroc on both clam models, transmil, and our proposed\n",
      "method.\n",
      "both in terms of auroc and accuracy, our method outperforms the\n",
      "other methods on long sequences, while the performances are comparable to\n",
      "table 1, albeit slightly lower, illustrating the challenge of processing large wsis.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf:\n",
      "we developed vesselvae, a recursive\n",
      "variational neural network that fully exploits the hierarchical organiza-\n",
      "tion of the vessel and learns a low-dimensional manifold encoding branch\n",
      "connectivity along with geometry features describing the target surface.\n",
      "by leveraging the power of deep neural networks, we\n",
      "generate 3d models of blood vessels that are both accurate and diverse,\n",
      "which is crucial for medical and surgical training, hemodynamic simula-\n",
      "tions, and many other purposes.\n",
      "these meshes are typically generated using\n",
      "either image segmentation or synthetic methods.\n",
      "despite signiﬁcant advances in\n",
      "vessel segmentation [26], reconstructing thin features accurately from medical\n",
      "images remains challenging [2].\n",
      "manual editing of vessel geometry is a tedious\n",
      "and error prone task that requires expert medical knowledge, which explains the\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0_7.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43907-0_7\n",
      "68\n",
      "p. feldman et al.\n",
      "scarcity of curated datasets.\n",
      "in recent years, deep neural networks led to the development of powerful gen-\n",
      "erative models [30], such as generative adversarial networks [8,12] and diﬀusion\n",
      "models [11], which produced groundbreaking performance in many applications,\n",
      "ranging from image and video synthesis to molecular design.\n",
      "our generative framework is based on a\n",
      "recursive variational neural network (rvnn), that has been applied in various\n",
      "contexts, including natural language\n",
      "[23,24], shape semantics modeling\n",
      "[14,15],\n",
      "and document layout generation [20].\n",
      "in contrast to previous data-driven meth-\n",
      "ods, our recursive network fully exploits the hierarchical organization of the ves-\n",
      "sel and learns a low-dimensional manifold encoding branch connectivity along\n",
      "with geometry features describing the target surface.\n",
      "experiments show that synth and real blood vessel\n",
      "geometries are highly similar measured with the cosine similarity: radius (.97),\n",
      "length (.95), and tortuosity (.96).\n",
      "formally, each tree is deﬁned as a tuple (t, e), where t is the set of\n",
      "nodes, and e is the set of directed edges connecting a pair of nodes (n, m), with\n",
      "n, m ∈ t. in order to encode a 3d model into this representation, vessel segments\n",
      "v are parameterized by a central axis consisting of ordered points in euclidean\n",
      "space: v = v1, v2, . . .\n",
      "similarly, the decoder only uses\n",
      "right/left dec-mlps when the node classiﬁer predicts bifurcations.\n",
      "where each node ni represents a vessel segment v and contains an attribute vector\n",
      "xi =\n",
      "the encoder transforms a tree struc-\n",
      "ture into a hierarchical encoding on the learned manifold.\n",
      "this\n",
      "70\n",
      "p. feldman et al.\n",
      "is implemented as a multi-layer perceptron trained to predict a three-category\n",
      "bifurcation probability based on the encoded vector as input.\n",
      "complementing\n",
      "the node classiﬁer, the features dec-mlp is responsible for reconstructing the\n",
      "attributes of each node, speciﬁcally its coordinates and radius.\n",
      "in addition to the core architecture, our model is further augmented with\n",
      "three auxiliary, shallow, fully-connected neural networks: fμ, fσ, and gz.\n",
      "lectively, these supplementary networks streamline the data transformation pro-\n",
      "cess through the model.\n",
      "see the appendix for implementation details.\n",
      "objective.\n",
      "our generative model is trained to learn a probability distribution\n",
      "over the latent space that can be used to generate new blood vessel segments.\n",
      "the implemented\n",
      "method iterates through the points in the curve generating a coarse quadrilateral\n",
      "recursive variational autoencoders for 3d blood vessel synthesis\n",
      "71\n",
      "fig.\n",
      "mesh along the segments and joints.\n",
      "3\n",
      "experimental setup\n",
      "materials.\n",
      "this subset consisted of 1694\n",
      "healthy vessel segments reconstructed from 2d mra images of patients.\n",
      "the centerline points were determined based on the ratio\n",
      "between the sphere step and the local maximum radius, which was computed\n",
      "using the advancement ratio speciﬁed by the user.\n",
      "to improve compu-\n",
      "tational eﬃciency during recursive tree traversal, we implemented an algorithm\n",
      "that balances each tree by identifying a new root.\n",
      "we additionally trimmed trees\n",
      "to a depth of ten in our experiments.\n",
      "this decision reﬂects a balance between\n",
      "the computational demands of depth-ﬁrst tree traversal in each training step\n",
      "and the complexity of the training meshes.\n",
      "72\n",
      "p. feldman et al.\n",
      "that exhibited greater depth, nodes with more than two children, or with loops.\n",
      "implementation details.\n",
      "for the centerline extraction, we set the advance-\n",
      "ment ratio in the vmtk script to 1.05.\n",
      "in those cases, we selected the\n",
      "sample with the lowest radius, which ensures proper alignment with the center-\n",
      "line principal direction.\n",
      "the data pre-processing pipeline and network code were implemented in\n",
      "python and pytorch framework.\n",
      "in all stages, we set the batch size to 10 and used the adam optimizer\n",
      "with β1 = 0.9, β2 = 0.999, and a learning rate of 1 × 10−4.\n",
      "we set α = .3\n",
      "and γ = .001 for eq. 1 in our experiments.\n",
      "to enhance computation speed, we\n",
      "implemented dynamic batching [16], which groups together operations involving\n",
      "input trees of dissimilar shapes and diﬀerent nodes within a single input graph.\n",
      "this means that the amount of memory required to store\n",
      "and manipulate our training data structures is minimal.\n",
      "during preliminary experiments, we observed that accurately classifying nodes\n",
      "closer to the tree root is critical.\n",
      "we deﬁned a set of metrics to evaluate our trained network’s perfor-\n",
      "mance.\n",
      "we analyzed\n",
      "tortuosity per branch, the vessel centerline total length, and the average radius\n",
      "of the tree.\n",
      "3. (a) shows the histograms of total length, average radius and tortuosity per\n",
      "branch for both, real and synthetic samples.\n",
      "average radius were used in previous work to distinguish healthy vasculature\n",
      "from cancerous malformations.\n",
      "4\n",
      "results\n",
      "we conducted both quantitative and qualitative analyses to evaluate the model’s\n",
      "performance.\n",
      "for the quantitative analyses, we implemented a set of metrics\n",
      "commonly used for characterizing blood vessels.\n",
      "we\n",
      "measured the closeness of histograms with the cosine similarity by projecting\n",
      "the distribution into a vector of n-dimensional space (n is the number of bins in\n",
      "the histogram).\n",
      "we\n",
      "obtain a radius cosine similarity of .97, a total length cosine similarity of .95,\n",
      "74\n",
      "p. feldman et al.\n",
      "and a tortuosity cosine similarity of .96.\n",
      "overall, we believe that our\n",
      "proposed approach holds great promise for advancing 3d blood vessel geometry\n",
      "synthesis and contributing to the development of new clinical tools for healthcare\n",
      "professionals.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf:\n",
      "current approaches thus use weakly supervised object detec-\n",
      "tion to learn the (rough) localization of pathologies from image-level\n",
      "annotations, which is however limited in performance due to the lack of\n",
      "bounding box supervision.\n",
      "we study two training\n",
      "approaches: supervised training using anatomy-level pathology labels\n",
      "and multiple instance learning (mil) with image-level pathology labels.\n",
      "keywords: pathology detection · anatomical regions · chest x-rays\n",
      "1\n",
      "introduction\n",
      "chest radiographs (chest x-rays) represent the most widely utilized type of medi-\n",
      "cal imaging examination globally and hold immense signiﬁcance in the detection\n",
      "of prevalent thoracic diseases, including pneumonia and lung cancer, making\n",
      "them a crucial tool in clinical care\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0_6.\n",
      "https://doi.org/10.1007/978-3-031-43907-0_6\n",
      "58\n",
      "p. müller et al.\n",
      "however, while image classiﬁcation labels can be automatically extracted\n",
      "from electronic health records or radiology reports [7,20], this is typically not\n",
      "possible for bounding boxes, thus limiting the availability of large datasets\n",
      "for pathology detection.\n",
      "additionally, manually annotating pathology bound-\n",
      "ing boxes is a time-consuming task, further exacerbating the issue.\n",
      "however, as these meth-\n",
      "ods are not guided by any form of bounding boxes, their performance is limited.\n",
      "– we study two training approaches: using localized (anatomy-level) pathology\n",
      "labels for our model loc-adpd and using image-level labels with multiple\n",
      "instance learning (mil) for our model mil-adpd.\n",
      "– we train our models on the chest imagenome\n",
      "due to the scarcity of bounding box\n",
      "annotations, pathology detection on chest x-rays is often tackled using weakly\n",
      "supervised object detection with class activation mapping (cam) [25], which\n",
      "only requires image-level classiﬁcation labels.\n",
      "after training a classiﬁcation\n",
      "model with global average pooling (gap), an activation heatmap is com-\n",
      "puted by classifying each individual patch (extracted before pooling) with the\n",
      "trained classiﬁer, before thresholding this heatmap for predicting bounding\n",
      "boxes.\n",
      "unlike\n",
      "our method, none of these methods utilize anatomical regions as proxies for\n",
      "predicting pathology bounding boxes, therefore leading to inferior performance.\n",
      "along with the chest imagenome dataset\n",
      "[21] several localized pathology classiﬁcation models have been proposed which\n",
      "use a faster r-cnn\n",
      "this\n",
      "one-to-one assignment of tokens and regions allows us to remove the hungarian\n",
      "60\n",
      "p. müller et al.\n",
      "box\n",
      "prediction\n",
      "pneumonia: 0.74\n",
      "pneumonia: 0.71\n",
      "pneumonia: 0.72\n",
      "weighted\n",
      "box fusion\n",
      "pneumonia\n",
      "infiltration\n",
      "cardiomegaly\n",
      "0.27\n",
      "0.15\n",
      "0.42\n",
      "pneumonia\n",
      "infiltration\n",
      "cardiomegaly\n",
      "pneumonia\n",
      "infiltration\n",
      "cardiomegaly\n",
      "0.74\n",
      "0.71\n",
      "0.61\n",
      "0.15\n",
      "0.54\n",
      "0.16\n",
      "fig.\n",
      "we experimented with more complex pathology predictors like an mlp or\n",
      "a transformer layer but did not observe any beneﬁts.\n",
      "we also did not observe\n",
      "improvements when using several decoder layers and observed degrading perfor-\n",
      "mance when using roi pooling to compute region features.\n",
      "3.2\n",
      "inference\n",
      "during inference, the trained model predicts anatomical region bounding boxes\n",
      "and per-region pathology probabilities, which are then used to predict pathology\n",
      "bounding boxes in two steps, as shown in fig.\n",
      "[19] merges bounding boxes of the same pathology\n",
      "with iou-overlaps above 0.03 and computes weighted averages (using box scores\n",
      "as weights) of their box coordinates.\n",
      "as many anatomical regions are at least\n",
      "partially overlapping, and we use a small iou-overlap threshold, this allows the\n",
      "model to either pull the predicted boxes to relevant subparts of an anatomical\n",
      "region or to predict that pathologies stretch over several regions.\n",
      "for training the pathology\n",
      "classiﬁer, we experiment with two diﬀerent levels of supervision (fig. 3).\n",
      "mil-adpd: region predictions are ﬁrst aggregated\n",
      "using lse pooling and then trained using image-level supervision.\n",
      "[17] loss\n",
      "function independently on each region-pathology pair and average the results\n",
      "over all regions and pathologies.\n",
      "the decoder feature dimension is set to 512.\n",
      "for our mil-adpd model, we experiment with a weaker form of supervision,\n",
      "where pathology classiﬁcation labels are only available on the per-image level.\n",
      "we utilize multiple instance learning (mil), where an image is considered a bag\n",
      "of individual instances (i.e. the anatomical regions), and only a single label (per\n",
      "pathology) is provided for the whole bag, which is positive if any of its instances\n",
      "is positive.\n",
      "to train using mil, we ﬁrst aggregate the predicted pathology prob-\n",
      "abilities of each region over all detected regions in the image using lse pooling\n",
      "the resulting per-image\n",
      "probability for each pathology is then trained using the asl\n",
      "in this\n",
      "model, the decoder feature dimension is set to 256.\n",
      "in both models, the asl loss is weighted by a factor of 0.01 before adding\n",
      "it to the detr loss.\n",
      "we train on the chest imagenome dataset [4,21,22]1, con-\n",
      "sisting of roughly 240 000 frontal chest x-ray images with corresponding scene\n",
      "graphs automatically constructed from free-text radiology reports.\n",
      "amongst other information, each scene graph contains bounding boxes for 29\n",
      "1 https://physionet.org/content/chest-imagenome/1.0.0\n",
      "(physionet\n",
      "credentialed\n",
      "health data license 1.5.0).\n",
      "we con-\n",
      "sider the image-level label for a pathology to be positive if any region is positively\n",
      "labeled with that pathology.\n",
      "we use the provided jpg-images [11]2 and follow the oﬃcial mimic-cxr\n",
      "training split but only keep samples containing a scene graph with at least ﬁve\n",
      "valid region bounding boxes, resulting in a total of 234 307 training samples.\n",
      "during training, we use random resized cropping with size 224 × 224, apply\n",
      "contrast and brightness jittering, random aﬃne augmentations, and gaussian\n",
      "blurring.\n",
      "we evaluate our method on the subset\n",
      "of 882 chest x-ray images with pathology bounding boxes, annotated by radiol-\n",
      "ogists, from the nih chestxray-8 (cxr8) dataset\n",
      "all images are center-cropped and resized to 224 × 224.\n",
      "for some evaluation classes, we therefore use a many-to-one map-\n",
      "ping where the class probability is computed as the mean over several training\n",
      "classes.\n",
      "material for a detailed study on class mappings.\n",
      "4\n",
      "experiments and results\n",
      "4.1\n",
      "experimental setup and baselines\n",
      "we compare our method against several weakly supervised object detection\n",
      "methods (chexnet [14], stl\n",
      "[13]), trained on the cxr8 training set using only\n",
      "image-level pathology labels.\n",
      "note that some of these methods focus on (image-\n",
      "level) classiﬁcation and do not report quantitative localization results.\n",
      "it was trained on mimic-cxr (sharing the images with our method) with\n",
      "labels from radgraph [8] and ﬁnetuned on the cxr8 training set with image-\n",
      "level labels.\n",
      "our models loc-adpd and\n",
      "mil-adpd, trained using anatomy (an) bounding boxes, both outperform all weakly\n",
      "supervised methods trained with image-level pathology (pa) and anatomy-level pathol-\n",
      "ogy (an-pa) labels by a large margin.\n",
      "we report the standard object detection metrics average preci-\n",
      "sion (ap) at diﬀerent iou-thresholds and the mean ap (map) over thresholds\n",
      "(0.1, 0.2, . . .\n",
      "compared to the best\n",
      "weakly supervised method with image-level supervision (chexnet) our methods\n",
      "improve by large margins (mil-adpd by δ+35.2%, loc-adpd by δ+87.8% in\n",
      "map).\n",
      "improvements are especially high when considering larger iou-thresholds\n",
      "and huge improvements are also achieved in loc-acc at all thresholds.\n",
      "(color\n",
      "ﬁgure online)\n",
      "margins (mil-adpd by δ + 47.9% and loc-adpd by δ + 105.5% map),\n",
      "while improvements on larger thresholds are smaller here.\n",
      "while\n",
      "using image-level annotations (mil-adpd) already gives promising results, the\n",
      "full potential is only achieved using anatomy-level supervision (loc-adpd).\n",
      "we found\n",
      "that the improvements of mil-adpd are mainly due to improved performance\n",
      "on cardiomegaly and mass detection, while loc-adpd consistently outperforms\n",
      "all baselines on all classes except nodule, often by a large margin.\n",
      "ablation study.\n",
      "combining the training strategies of loc-adpd\n",
      "and mil-adpd does not lead to an improved performance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf:\n",
      "endoscopy is the gold standard procedure for early detec-\n",
      "tion and treatment of numerous diseases.\n",
      "obtaining 3d reconstructions\n",
      "from real endoscopic videos would facilitate the development of assis-\n",
      "tive tools for practitioners, but it is a challenging problem for current\n",
      "structure from motion (sfm) methods.\n",
      "feature extraction and match-\n",
      "ing are key steps in sfm approaches, and these are particularly diﬃcult in\n",
      "the endoscopy domain due to deformations, poor texture, and numerous\n",
      "artifacts in the images.\n",
      "in our experiments, superpoint-\n",
      "e obtains more and better features than any of the baseline detectors\n",
      "used as supervision.\n",
      "keywords: deep learning · structure from motion · local features ·\n",
      "endoscopy\n",
      "1\n",
      "introduction\n",
      "endoscopy is an important medical procedure with many applications, from\n",
      "routine screening to detection of early signs of cancer and minimally invasive\n",
      "treatment.\n",
      "automatic analysis and understanding of these videos raises many\n",
      "opportunities for novel assistive and automatization tasks on endoscopy proce-\n",
      "dures.\n",
      "obtaining 3d models from the intracorporeal scenes captured in endo-\n",
      "scopies is an essential step to enable these novel tasks and build applications,\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0 56.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43907-0_56\n",
      "584\n",
      "o. l. barbed et al.\n",
      "for example, for improved monitoring of existing patients or augmented reality\n",
      "during training or real explorations.\n",
      "endoscopic images are a challenging\n",
      "case for feature detection and matching, due to several well known challenges\n",
      "for these tasks, such as lack of texture, or the presence of frequent artifacts, like\n",
      "specular reﬂections.\n",
      "these problems are accentuated when all the elements in\n",
      "the scene are deformable, as it is the case in most endoscopy scenarios, and in\n",
      "particular in the real use case studied in our work, the lower gastrointestinal\n",
      "tract explored with colonoscopies.\n",
      "this work introduces superpoint-e, a new model to extract interest points\n",
      "from endoscopic images.\n",
      "we select good features with the colmap\n",
      "sfm pipeline [21], generating training examples with feature points that can\n",
      "be tracked across several images according to colmap result.\n",
      "[7] have evaluated the perfor-\n",
      "mance of modern slam approaches on endoscopic sequences.\n",
      "[13] improved the performance of such methods in laparoscopic sequences.\n",
      "however, trans-\n",
      "ferring that performance to endoscopy settings remains a diﬃcult task due to\n",
      "1\n",
      "https://www.cs.ubc.ca/research/image-matching-challenge/2021/leaderboard/.\n",
      "tracking adaptation to improve superpoint for reconstruction in endoscopy\n",
      "585\n",
      "several challenges.\n",
      "notable mentions are superpoint\n",
      "exporting\n",
      "this progress to the matching stage, disk\n",
      "[24] proposes a formulation of the\n",
      "problem to optimize in an end-to-end manner.\n",
      "other recent works have extended\n",
      "the networks to take advantage of the advances in attention for the matching\n",
      "task, as in superglue [20] and loftr\n",
      "[23].\n",
      "in this work we improve the performance of superpoint\n",
      "[5] on endoscopy\n",
      "images.\n",
      "we chose superpoint because it is a seminal work that has inspired\n",
      "many follow up works, and it is still among the top performers on current feature\n",
      "matching challenges [10]. similar to detone et al.\n",
      "[4], we explore improvements\n",
      "on feature extraction that provide good properties for downstream tasks.\n",
      "instead,\n",
      "we propose to use 3d reconstructions of points tracked along image sequences.\n",
      "uration parameters are detailed in the supplementary materials.\n",
      "we additionally compute the 3d reconstruction for the\n",
      "586\n",
      "o. l. barbed et al.\n",
      "same sequences with a modiﬁed colmap pipeline that uses the oﬃcial super-\n",
      "point and superglue2 implementation with the indoor set of weights.\n",
      "the\n",
      "reliable track for this point is the green segment.\n",
      "(c) movement of the point along the video.\n",
      "a successful 3d recon-\n",
      "struction includes the computed positions of the cameras that took the images\n",
      "and a point cloud with 3d coordinates of the triangulated points.\n",
      "we use the\n",
      "camera poses, the points’ coordinates and the camera calibration parameters\n",
      "to reproject the 3d point cloud points into every image.\n",
      "if they were “originally” detected and matched in a particular\n",
      "image, we set them to green.\n",
      "it achieves this by using as supervision y the average detections over sev-\n",
      "eral random homographic deformations of the same image.\n",
      "the feature extrac-\n",
      "tion network then is run on an image\n",
      "instead of an image\n",
      "i and a warped version i′, we use\n",
      "diﬀerent images ia and ib from the same sequence.\n",
      "t is the set of all the tracks that appear in both images.\n",
      "two descriptors from diﬀerent images dai and dbj\n",
      "are a positive pair if they belong to the same track (i = j), and negative pair\n",
      "otherwise (i ̸= j).\n",
      "4\n",
      "experiments\n",
      "the following experiments demonstrate the proposed feature detection eﬃcacy\n",
      "to obtain 3d models on real colonoscopy videos, comparing diﬀerent variations\n",
      "of our approach and relevant baseline methods.\n",
      "dataset.\n",
      "the exact details are in the supplementary material.\n",
      "all models were trained with a\n",
      "modiﬁcation of a pytorch implementation of superpoint [9].\n",
      "training parame-\n",
      "ters in supplementary material.\n",
      "conﬁguration of the training (left), and average reconstruc-\n",
      "tion results, i.e., quality metrics (right).\n",
      "loss (loss used for training): sp: original superpoint training loss; tr-2 or tr-n: track-based loss.\n",
      "tr-2 means that the loss is computed for every pair of images in the track.\n",
      "tr-n means we optimize\n",
      "simultaneously n views of the track (n=4 in our experiments).\n",
      "table 1 (last ﬁve columns) summarizes the performance of\n",
      "our approach variations.\n",
      "matches between the points in two images are obtained with\n",
      "bi-directional nearest neighbor algorithm with l2 distance.\n",
      "points and matches\n",
      "are given to colmap and the mapper module (conﬁguration in supplementary\n",
      "material) attempts to generate a 3d reconstruction.\n",
      "the reconstruction quality\n",
      "statistics used to illustrate the performance of each detector are:\n",
      "– ∥3dim∥: fraction of images from the subsequence successfully introduced\n",
      "in the reconstruction.\n",
      "the more\n",
      "points the better, since it means a denser coverage of the scene.\n",
      "– err: mean reprojection error of the 3d points after being reprojected onto\n",
      "the images of the subsequence.\n",
      "– err-10k: mean reprojection error of the best 10000 points of the recon-\n",
      "struction.\n",
      "since all reconstructions have outliers that skew the average, this\n",
      "metric is more representative of the performance of the models.\n",
      "tracking adaptation to improve superpoint for reconstruction in endoscopy\n",
      "589\n",
      "– len(tr): mean track length represents the average number of images where\n",
      "a point is being consecutively matched, tracked.\n",
      "this experiment compares the performance of\n",
      "the considered baselines against the best conﬁguration of our feature extraction\n",
      "model.\n",
      "in most metrics we observe a\n",
      "signiﬁcant improvement using sp-e compared to the others.\n",
      "each point in each image\n",
      "has been reconstructed after the corresponding colmap reconstruction process.\n",
      "subsequence 001 1\n",
      "002 1\n",
      "014 1\n",
      "016 1\n",
      "017 1\n",
      "095 1\n",
      "095 2\n",
      "avg\n",
      "(std)\n",
      "reconstructed images (∥3dim∥)\n",
      "total+\n",
      "+ total number of images in the subsequence.\n",
      "* if 10k points are not available, average is computed over all available reconstructed points.\n",
      "the mean reprojection error of all the points is the lowest\n",
      "for sift, possibly due to it being more restrictive in all other aspects (number\n",
      "of images reconstructed, number of points, track length).\n",
      "note that even though sp-e\n",
      "obtains many more points, it is not at the cost of quality.\n",
      "we analyze additional aspects of our detected features to showcase the higher\n",
      "quality with respect to other methods in table 3. to measure the spread of the\n",
      "features over the images we deﬁned a 16 × 16 grid over each image and computed\n",
      "the percentage of those cells that have at least one reconstructed point.\n",
      "we also\n",
      "measure how many extracted points fall on top of specularities (we consider\n",
      "a pixel as part of a specularity if its intensity is higher than 180).\n",
      "sp only reconstructed 3 out of the 5 sequences\n",
      "while sift and sp-e correctly reconstructed the 5 sequences, with an average\n",
      "rmse of 4.61mm and 4.71 mm respectively.\n",
      "simulated data lacks some of the\n",
      "biggest challenges of endoscopy images (e.g. specularities, deformations), but\n",
      "this experiment suggests that the camera motion estimation quality is similarly\n",
      "good for all methods when they manage to converge.\n",
      "5\n",
      "conclusions\n",
      "this work presents a novel training strategy for superpoint to improve its per-\n",
      "formance in sfm from endoscopy images.\n",
      "our proposed model is able to obtain more suitable features\n",
      "for 3d reconstruction, and to reconstruct larger sets of images with much denser\n",
      "point clouds.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf:\n",
      "fully-supervised polyp segmentation has accomplished sig-\n",
      "niﬁcant triumphs over the years in advancing the early diagnosis of col-\n",
      "orectal cancer.\n",
      "however, label-eﬃcient solutions from weak supervision\n",
      "like scribbles are rarely explored yet primarily meaningful and demand-\n",
      "ing in medical practice due to the expensiveness and scarcity of densely-\n",
      "annotated polyp data.\n",
      "besides, various deployment issues, including data\n",
      "shifts and corruption, put forward further requests for model generaliza-\n",
      "tion and robustness.\n",
      "concretely, for the ﬁrst time\n",
      "in weakly-supervised medical image segmentation, we promote the dual-\n",
      "branch co-teaching framework by leveraging the intrinsic complemen-\n",
      "tarity of features extracted from the spatial and spectral domains and\n",
      "encouraging cross-space consistency through collaborative optimization.\n",
      "ultimately, we formulate a\n",
      "holistic optimization objective to learn from the hybrid supervision of\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0 4.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "extensive experiments and evaluation on\n",
      "four public datasets demonstrate the superiority of our method regarding\n",
      "in-distribution accuracy, out-of-distribution generalization, and robust-\n",
      "ness, highlighting its promising clinical signiﬁcance.\n",
      "keywords: polyp image segmentation · weakly-supervised learning ·\n",
      "spatial-spectral dual branches · mutual teaching · ensemble learning\n",
      "1\n",
      "introduction\n",
      "colorectal cancer is a leading cause of cancer-related deaths worldwide [1].\n",
      "early\n",
      "detection and eﬃcient diagnosis of polyps, which are precursors to colorectal\n",
      "cancer, is crucial for eﬀective treatment.\n",
      "recently, deep learning has emerged\n",
      "as a powerful tool in medical image analysis, prompting extensive research into\n",
      "its potential for polyp segmentation.\n",
      "besides, scribbles provide\n",
      "a more robust supervision signal, which can be prone to noise and outliers [5].\n",
      "hence, this work investigates the feasibility of conducting polyp segmentation\n",
      "using scribble annotation as supervision.\n",
      "the eﬀectiveness of medical applica-\n",
      "tions during in-site deployment depends on their ability to generalize to unseen\n",
      "data and remain robust against data corruption.\n",
      "dual-branch learning has been widely adopted in annotation-eﬃcient learn-\n",
      "ing to encourage mutual consistency through co-teaching.\n",
      "while existing\n",
      "approaches are typically designed for learning in the spatial domain [21,25,29,\n",
      "30], a novel spatial-spectral dual-branch structure is introduced to eﬃciently\n",
      "leverage domain-speciﬁc complementary knowledge with synergistic mutual\n",
      "teaching.\n",
      "furthermore, the outputs from the spatial-spectral branches are aggre-\n",
      "gated to produce mixed pseudo labels as supplementary supervision.\n",
      "1. overview of our spatial-spectral dual-branch mutual teaching and pixel-level\n",
      "entropy-guided pseudo label ensemble learning (s2me) for scribble-supervised polyp\n",
      "segmentation.\n",
      "spatial-spectral cross-domain consistency is encouraged through mutual\n",
      "teaching.\n",
      "overall, the contributions of this work are threefold: first,\n",
      "we devise a spatial-spectral dual-branch structure to leverage cross-space knowl-\n",
      "edge and foster collaborative mutual teaching.\n",
      "to our best knowledge, this is the\n",
      "ﬁrst attempt to explore the complementary relations of the spatial-spectral dual\n",
      "branch in boosting weakly-supervised medical image analysis.\n",
      "lastly, our proposed hybrid loss optimization, comprising scribbles-\n",
      "supervised loss, mutual training loss with domain-speciﬁc pseudo labels, and\n",
      "ensemble learning loss with fused-domain pseudo labels, facilitates obtaining\n",
      "a generalizable and robust model for polyp image segmentation.\n",
      "an extensive\n",
      "assessment of our approach through the examination of four publicly accessible\n",
      "datasets establishes its superiority and clinical signiﬁcance.\n",
      "[26] has gained increasing popularity in medical image\n",
      "analysis [23] for its ability to identify subtle frequency patterns that may not be\n",
      "well detected by the pure spatial-domain network like unet [20].\n",
      "in addi-\n",
      "tion, spectrum learning also exhibits advantageous robustness and generaliza-\n",
      "tion against adversarial attacks, data corruption, and distribution shifts [19].\n",
      "in\n",
      "label-eﬃcient learning, some preliminary works have been proposed to encourage\n",
      "38\n",
      "a. wang et al.\n",
      "mutual consistency between outputs from two networks [3], two decoders\n",
      "this has motivated\n",
      "us to develop the cross-domain cooperative mutual teaching scheme to leverage\n",
      "the favorable properties when learning in the spectral space.\n",
      "besides consistency constraints, utilizing pseudo labels as supplementary\n",
      "supervision is another principle in label-eﬃcient learning [11,24].\n",
      "in contrast to prior\n",
      "weakly-supervised learning methods that have merely emphasized spatial con-\n",
      "siderations, our approach designs a dual-branch structure consisting of a spatial\n",
      "branch fspa(x, θspa) and a spectral branch fspe(x, θspe), with x and θ being the\n",
      "input image and randomly initialized model parameters.\n",
      "1,\n",
      "the spatial and spectral branches take the same training image as the input and\n",
      "extract domain-speciﬁc patterns.\n",
      "through cross-domain engagement, these two\n",
      "branches complement each other, with each providing valuable domain-speciﬁc\n",
      "insights and feedback to the other.\n",
      "in addition to mutual\n",
      "teaching, we consider aggregating the pseudo labels from the spatial and spec-\n",
      "tral branches in ensemble learning, aiming to take advantage of the distinctive\n",
      "1 for convenience, we omit the input x and model parameters θ.\n",
      "s2me: spatial-spectral mutual teaching and ensemble learning\n",
      "39\n",
      "yet complementary properties of the cross-domain features.\n",
      "that the pixels of the polyp boundary exhibit greater diﬃcul-\n",
      "ties in accurate segmentation, presenting with higher entropy values (the white\n",
      "contours).\n",
      "unlike previous\n",
      "image-level ﬁxed-ratio mixing or random mixing as eq.\n",
      "besides the\n",
      "scribble annotations for partial pixels, the aforementioned three types of pseudo\n",
      "labels ˆyspa, ˆyspe, and ˆys2 can oﬀer complementary supervision for every pixel,\n",
      "with diﬀerent learning regimes.\n",
      "the hybrid loss considers all possible\n",
      "supervision signals in the spatial-spectral dual-branch network and exceeds par-\n",
      "tial combinations of its constituent elements, as evidenced in the ablation study.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "experimental setup\n",
      "datasets.\n",
      "we employ the sun-seg [10] dataset with scribble annotations for\n",
      "training and assessing the in-distribution performance.\n",
      "implementation details.\n",
      "we implement our method with pytorch\n",
      "[18] and\n",
      "run the experiments on a single nvidia rtx3090 gpu.\n",
      "the sgd optimizer\n",
      "is utilized for training 30k iterations with a momentum of 0.9, a weight decay\n",
      "of 0.0001, and a batch size of 16.\n",
      "the execution time for each experiment is\n",
      "approximately 4 h. the initial learning rate is 0.03 and updated with the poly-\n",
      "scheduling policy [15].\n",
      "all\n",
      "the images are randomly cropped at the border with maximally 7 pixels and\n",
      "resized to 224×224 in width and height.\n",
      "[6] as the respective segmentation model in\n",
      "the spatial and spectral branches.\n",
      "the performance of the scribble-supervised\n",
      "model with partial cross entropy [13] loss (scrib-pce) and the fully-supervised\n",
      "2 some exemplary polyp frames are presented in the supplementary materials.\n",
      "[15] are employed as the compar-\n",
      "ative baselines and implemented with unet [20] as the segmentation backbone\n",
      "referring to the wsl4mis3 repository.\n",
      "quantitative comparison of the in-distribution segmentation performance.\n",
      "[3]\n",
      "0.658±0.004\n",
      "0.539±0.005\n",
      "0.676±0.005\n",
      "5.092±0.063\n",
      "dmpls [15]\n",
      "0.656±0.006\n",
      "0.539±0.005\n",
      "0.659±0.011\n",
      "5.208±0.061\n",
      "s2me (ours) 0.674±0.003 0.565±0.001 0.719±0.003 4.583±0.014\n",
      "fully-ce\n",
      "0.713±0.021\n",
      "0.617±0.023\n",
      "0.746±0.027\n",
      "4.405±0.119\n",
      "the performance of weakly-supervised methods is assessed with four metrics,i.e.,\n",
      "dice similarity coeﬃcient (dsc), intersection over union (iou), precision\n",
      "(prec), and a distance-based measure of hausdorﬀ distance (hd).\n",
      "2, our s2me achieves superior in-distribution performance quan-\n",
      "titatively and qualitatively compared with other baselines on the sun-seg [10]\n",
      "dataset.\n",
      "these results suggest the eﬃcacy and reliability of the pro-\n",
      "posed solution s2me in fulﬁlling polyp segmentation tasks with only scribble\n",
      "annotations.\n",
      "notably, the encouraging performance on unseen datasets exhibits\n",
      "promising clinical implications in deploying our method to real-world scenarios.\n",
      "4 complete results of all four metrics are present in the supplementary materials.\n",
      "42\n",
      "a. wang et al.\n",
      "fig.\n",
      "2. qualitative performance comparison of one camouﬂaged polyp image with dsc\n",
      "values on the left top.\n",
      "as shown in table 3, the spatial-spectral conﬁguration of our s2me\n",
      "yields superior performance compared to single-domain counterparts with me,\n",
      "conﬁrming the signiﬁcance of utilizing cross-domain features.\n",
      "as demonstrated in table 4,\n",
      "our method achieves improved performance compared to two image-level fusion\n",
      "strategies, i.e., random [15] and equal mixing.\n",
      "[15]\n",
      "image 0.665 ± 0.008\n",
      "4.750 ± 0.169\n",
      "equal (0.5)\n",
      "image 0.667 ± 0.001\n",
      "4.602 ± 0.013\n",
      "entropy (ours) pixel\n",
      "0.674 ± 0.003 4.583 ± 0.014\n",
      "table 5. ablation study on the loss com-\n",
      "ponents on the sun-seg [10] dataset.\n",
      "4\n",
      "conclusion\n",
      "to our best knowledge, we propose the ﬁrst spatial-spectral dual-branch net-\n",
      "work structure for weakly-supervised medical image segmentation that eﬃciently\n",
      "leverages cross-domain patterns with collaborative mutual teaching and ensem-\n",
      "ble learning.\n",
      "our pixel-level entropy-guided fusion strategy advances the relia-\n",
      "bility of the aggregated pseudo labels, which provides valuable supplementary\n",
      "supervision signals.\n",
      "moreover, we optimize the segmentation model with the\n",
      "hybrid mode of loss supervision from scribbles and pseudo labels in a holistic\n",
      "manner and witness improved outcomes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf:\n",
      "longitudinal lesion or tumor tracking is an essential task in\n",
      "diﬀerent clinical workﬂows, including treatment monitoring with follow-\n",
      "up imaging or planning of re-treatments for radiation therapy.\n",
      "the multi-scale approach allows the\n",
      "eﬃcient and robust learning of a similarity map between multi-timepoint\n",
      "image acquisitions to derive correspondence, while the self-supervised\n",
      "learning formulation enables the generic application to diﬀerent types\n",
      "of lesions and image modalities.\n",
      "we train our approach at large scale\n",
      "with more than 50,000 computed tomography (ct) scans and validate\n",
      "it on two diﬀerent applications: 1) tracking of generic lesions based on\n",
      "the deeplesion dataset, including liver tumors, lung nodules, enlarged\n",
      "lymph-nodes, for which we report highest matching accuracy of 92%,\n",
      "with localization accuracy that is nearly 10% higher than the state-of-\n",
      "the-art; and 2) tracking of lung nodules based on the nlst dataset\n",
      "for which we achieve similarly high performance.\n",
      "keywords: self-supervised learning · multi-scale · longitudinal lesion\n",
      "tracking\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0 55.\n",
      "https://doi.org/10.1007/978-3-031-43907-0_55\n",
      "574\n",
      "a. vizitiu et al.\n",
      "1\n",
      "introduction\n",
      "longitudinal lesion or tumor tracking is a fundamental task in treatment moni-\n",
      "toring workﬂows, and for planning of re-treatments in radiation therapy.\n",
      "this\n",
      "information can be leveraged to assess treatment response, e.g., by analyzing the\n",
      "evolution of size and morphology for a given tumor [1], but also for adaptation\n",
      "of (re-)treatment radiotherapy plans that take into account new tumors.\n",
      "in practice, the development of automatic and reliable lesion tracking solu-\n",
      "tions is hindered by the complexity of the data (over diﬀerent modalities), the\n",
      "absence of large, annotated datasets, and the diﬃculties associated with lesion\n",
      "identiﬁcation (i.e., varying sizes, poses, shapes, and sparsely distributed loca-\n",
      "tions).\n",
      "in addition, as imaging\n",
      "oﬀers contextual information about the human body that is naturally consis-\n",
      "tent, we design the model to beneﬁt from biologically-meaningful points (i.e.,\n",
      "anatomical landmarks).\n",
      "the reasoning behind this strategy is that simple data\n",
      "augmentation methods cannot faithfully model inter-subject variability or pos-\n",
      "sible organ deformations.\n",
      "our proposed method brings two elements of novelty from a technical point\n",
      "of view: (1) the multi-scale approach for the anatomical embedding learning\n",
      "and (2) a positive sampling approach that incorporates anatomically signiﬁcant\n",
      "landmarks across diﬀerent subjects.\n",
      "furthermore, a signiﬁcant\n",
      "focus and contribution of our research is the experimental study at a very large\n",
      "scale: we (1) train a pixel-wise self-supervised system using a very large and\n",
      "diverse dataset of 52,487 ct volumes and (2) evaluate on two publicly available\n",
      "datasets.\n",
      "2\n",
      "background and motivation\n",
      "the problem of lesion tracking in longitudinal data is typically divided into two\n",
      "steps: (1) detection of lesions and (2) tracking the same lesion over multiple\n",
      "multi-scale self-supervised learning for longitudinal lesion tracking\n",
      "575\n",
      "input image\n",
      "(e.g., 3d-ct)\n",
      "augmenta\u0002on\n",
      "4d mul\u0002-scale\n",
      "embeddings \n",
      "mul\u0002-scale\n",
      "self-supervised \n",
      "op\u0002miza\u0002on\n",
      "hard and diverse \n",
      "nega\u0002ve sampling \n",
      "landmarks\n",
      "op\u0002onal supervision:\n",
      "dur-\n",
      "ing training, we randomly extract positive samples (optionally, include same anatom-\n",
      "ical landmarks from diﬀerent volumes), hard-negative samples, and diverse negative\n",
      "samples of pixels from augmented 3d paired patches.\n",
      "during inference, the extracted\n",
      "embeddings are used to generate a cascade of cosine similarity maps that initially locate\n",
      "the corresponding location in a follow-up image within a larger area and subsequently\n",
      "improve the matching accuracy through gradual reﬁnement.\n",
      "time points.\n",
      "classical methods to solve this problem rely on image registration,\n",
      "where tracking is performed via image alignment and rule-based correspondence\n",
      "matching [15,16,21].\n",
      "[5] uses a\n",
      "self-supervised anatomical embedding model (sam) to create semantic embed-\n",
      "dings for each image pixel, avoiding the detection step.\n",
      "training exclusively on\n",
      "augmented paired data prevents sam from accurately representing anatomical\n",
      "changes and deformations that occur over time.\n",
      "3\n",
      "method\n",
      "3.1\n",
      "problem deﬁnition\n",
      "let i1 (i.e., template or baseline image) and i2 (i.e., query or follow-up image)\n",
      "be two 3d-ct scans acquired at time t1 and t2, respectively, additionally, let p1\n",
      "576\n",
      "a. vizitiu et al.\n",
      "and p2 denote the point of interest (i.e., the lesion center) in both images.\n",
      "the\n",
      "problem of lesion tracking can be formulated as ﬁnding the optimal transforma-\n",
      "tion that maps p1 to its corresponding location, p2, in i2.\n",
      "3.2\n",
      "training stage\n",
      "let d = {x1, x2, ..., xn} be a set of n unpaired and unlabeled 3d-ct volumes.\n",
      "as shown in fig. 1, given an image x ∈ rd×h×w from the training dataset d,\n",
      "we randomly select two overlapping 3d patches (anchor and query), namely xa\n",
      "and xq.\n",
      "to create synthetic paired data that mimics appearance changes across\n",
      "diﬀerent images, we apply random data augmentation (i.e., random spatial and\n",
      "intensity-related transformations) to the content of xa and xq.\n",
      "we implement a\n",
      "similar augmentation strategy to that described in [5].\n",
      "given xa and xq, we use\n",
      "an embedding extraction model to construct a hierarchy of multi-scale semantic\n",
      "embeddings for each image pixel, labeled fa and fq respectively.\n",
      "given the nature of contrastive learning, the sampling strategy (extract-\n",
      "ing negative and positive pixel pairs from augmented 3d paired patches) is\n",
      "essential to achieving discriminative pixel-wise embeddings.\n",
      "the ﬁnal loss is then calculated as\n",
      "the average of all these individual losses.\n",
      "multi-scale self-supervised learning for longitudinal lesion tracking\n",
      "577\n",
      "3.3\n",
      "inference stage\n",
      "let xa be a 3d-ct volume template with an input point of interest pa ∈ xa,\n",
      "and xq a corresponding query 3d-ct volume.\n",
      "the ﬁrst step is to project the\n",
      "image xa into a multi-scale feature space, creating a hierarchy of multi-scale\n",
      "semantic embeddings fa for each pixel in the image (i.e., a 4d feature map).\n",
      "next, we follow a similar process for the query image xa and acquire the pixel-\n",
      "level embeddings fq.\n",
      "to measure the similarity between the embeddings of the input xa at the\n",
      "point of interest pa and the query embeddings fq, we compute cosine similarity\n",
      "maps at each scale:\n",
      "si =\n",
      "f i\n",
      "a(pa) ·\n",
      "4\n",
      "experiments\n",
      "4.1\n",
      "datasets and setup\n",
      "datasets: we train the universal and ﬁne-grained anatomical point matching\n",
      "model using an in-house ct dataset (variousct).\n",
      "the training dataset contains\n",
      "52,487 unlabeled 3d ct volumes capturing various anatomies, including chest,\n",
      "head, abdomen, pelvis, and more.\n",
      "for nlst, we randomly selected\n",
      "a subset of 1045 test images coming from 420 patients with up to 3 studies.\n",
      "system training: our learning model is implemented in pytorch and uses\n",
      "the torchio library [13] for medical data manipulation and augmentation.\n",
      "the exact same test set was used to com-\n",
      "pute the performance of each approach listed in the table; however, we retrained only\n",
      "sam.\n",
      "[5]\n",
      "81.67\n",
      "90.21\n",
      "2.9 ± 8.0\n",
      "2.5 ± 3.8\n",
      "3.6 ± 5.2\n",
      "6.5 ± 9.6\n",
      "ours\n",
      "83.13†\n",
      "91.87†\n",
      "2.9 ± 6.0\n",
      "2.2 ± 3.2\n",
      "3.1 ± 3.9\n",
      "5.9 ± 7.1†\n",
      "† improvement is statistically signiﬁcant compared to sam\n",
      "in the standard resnet to 3d convolutions and allows the use of pre-trained\n",
      "imagenet weights.\n",
      "the model is trained with adamw optimizer [6] for 64 epochs using an early\n",
      "stopping strategy with a patience of 5 epochs, a batch size of 8 augmented 3d\n",
      "paired patches of 32 × 96 × 96, and a learning rate of 0.0001.\n",
      "for data augmentation, we apply random cropping, scaling, rotation, and\n",
      "gaussian noise injections.\n",
      "evaluation metrics: we use mean euclidean distance (med) to measure\n",
      "the distance between predicted lesion center and ground truth, and the center\n",
      "point matching accuracy (i.e., percentage of accurately matched lesions given the\n",
      "annotated lesion radius), denoted with cpm@radius.\n",
      "to ensure that such small nodules are\n",
      "not missed during evaluation, we relax the minimum distance requirement and\n",
      "consider a distance of 6 mm as a permissible matching error.\n",
      "hence, for performance comparison against\n",
      "self-supervised anatomical embedding tracker, we retrain sam\n",
      "[5] with images\n",
      "from variousct dataset.\n",
      "to conﬁrm the signiﬁcance of the\n",
      "improvement achieved by our method compared to sam\n",
      "[5], we conduct a paired\n",
      "t-test for statistical analysis and show that the improvement is statistically sig-\n",
      "niﬁcant (p-value < 10−6).\n",
      "compared to the self-supervised version of dlt, the\n",
      "diﬀerence in performance is signiﬁcantly greater, the proposed systems outper-\n",
      "forms dlt-ssl by more than 10%.\n",
      "when imposing a maximum distance limit\n",
      "of 10 mm between the ground truth and prediction, our method increases perfor-\n",
      "mance by 1.46%, showing the importance of the multi-scale approach in lesion\n",
      "fig.\n",
      "we denote the\n",
      "ground-truth points using green markers in both the baseline and follow-up images,\n",
      "whereas the predicted points are indicated by red markers.\n",
      "to illustrate the extent of\n",
      "the lesions, we also display the annotated bounding boxes on the follow-up images.\n",
      "(color ﬁgure online)\n",
      "580\n",
      "a. vizitiu et al.\n",
      "location reﬁnement.\n",
      "through large-scale experiments and validation on two longitudinal datasets, we\n",
      "highlight the superiority of the proposed method in comparison to state-of-the-\n",
      "art.\n",
      "our future work aims to enhance the matching accuracy by examining the\n",
      "implications of correlation magnitude, conducting robustness studies on slight\n",
      "variations in tracking initialization, and implementing a more advanced fusion\n",
      "strategy for the multi-scale similarity maps.\n",
      "in addition, we aim to expand to\n",
      "more applications, e.g., treatment monitoring for brain cancer using mri.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf:\n",
      "the displacement estimation step of ultrasound elastogra-\n",
      "phy (use) can be done by optical ﬂow convolutional neural networks\n",
      "(cnn).\n",
      "even though displacement estimation in use and computer\n",
      "vision share some challenges, use displacement estimation has two dis-\n",
      "tinct characteristics that set it apart from the computer vision coun-\n",
      "terpart: high-frequency nature of rf data, and the physical rules that\n",
      "govern the motion pattern.\n",
      "however, insuﬃcient attention has been\n",
      "placed on the integration of physical laws of deformation into the dis-\n",
      "placement estimation.\n",
      "in use, lateral displacement estimation, which\n",
      "is highly required for elasticity and poisson’s ratio imaging, is a more\n",
      "challenging task compared to the axial one since the motion in the lat-\n",
      "eral direction is limited, and the sampling frequency is much lower than\n",
      "the axial one.\n",
      "picture tries to limit the range of the lateral\n",
      "displacement by the feasible range of poisson’s ratio and the estimated\n",
      "high-quality axial displacement.\n",
      "despite the improvement, the regular-\n",
      "ization was only applied during the training phase.\n",
      "we exploit the concept of\n",
      "known operators to incorporate iterative reﬁnement optimization meth-\n",
      "ods into the network architecture so that the network is forced to remain\n",
      "within the physically plausible displacement manifold.\n",
      "the reﬁnement\n",
      "optimization methods are embedded into the diﬀerent pyramid levels of\n",
      "the network architecture to improve the estimate.\n",
      "our results on exper-\n",
      "imental phantom and in vivo data show that the proposed method sub-\n",
      "stantially improves the estimated displacements.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0_45.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "ultrasound (us) data before and after the tissue deformation (which\n",
      "can be caused by an external or internal force) are collected and compared\n",
      "to calculate the displacement map, indicating each individual sample’s relative\n",
      "motion.\n",
      "the strain is computed by taking the derivative of the displacement\n",
      "ﬁelds.\n",
      "convolutional neural networks (cnn) have been successfully employed\n",
      "for use displacement estimation [11,12,15].\n",
      "unsupervised and semi-supervised\n",
      "training methods have been proposed, which enable the networks to use real us\n",
      "images for training [1,14,17].\n",
      "this method aims to improve lat-\n",
      "eral displacement by exploiting the high-quality axial displacement estimation\n",
      "and the relation between the lateral and axial strains deﬁned by the physics of\n",
      "motion.\n",
      "despite the substantial improvement, the regularization is only applied\n",
      "during the training phase.\n",
      "maier\n",
      "et al. investigated known operators in diﬀerent applications such as computed\n",
      "tomography, magnetic resonance imaging, and vessel segmentation, and showed\n",
      "a substantial reduction in the maximum error bounds\n",
      "[7].\n",
      "in this paper, we aim to embed two lateral displacement reﬁnement algo-\n",
      "rithms in the cnns to improve the lateral strains.\n",
      "the second algorithm employs the reﬁnement method\n",
      "proposed be gou et al.\n",
      "[2] which exploits incompressibility constraint to reﬁne\n",
      "known operators for ultrasound elastography\n",
      "469\n",
      "the lateral displacement.\n",
      "1- detect out-of-range eprs by:\n",
      "m(i, j) =\n",
      "\u0002\n",
      "0 vemin < \u0003ve(i, j) < vemax\n",
      "1\n",
      "otherwise\n",
      "\u0004\n",
      "(1)\n",
      "where \u0003ve is the epr obtained from the estimated displacements.\n",
      "(2)\n",
      "where < \u0003ve > is the average of epr values within the feasible range.\n",
      "it should be noted in con-\n",
      "trast to [5] in which only out-of-range samples were contributing to the loss,\n",
      "in this work, all samples contribute to lvd to reduce the estimation bias.\n",
      "we employ two known operators to impose physically known constraints\n",
      "on the lateral displacement.\n",
      "it should be noted that the\n",
      "algorithm can be employed for compressible tissues as well, and the incompress-\n",
      "ibility constraint is employed for the reﬁnement of the obtained displacement.\n",
      "while the\n",
      "latter involves adjusting trained weights based on the training data and keep-\n",
      "ing them ﬁxed during testing, the former relies on iterative reﬁnement that is\n",
      "adaptable to the test data and does not require any learnable weights.\n",
      "2.3\n",
      "unsupervised training\n",
      "we followed a similar unsupervised training approach presented in [5] for both\n",
      "picture and kpicture methods.\n",
      "λv lv\n",
      "(6)\n",
      "known operators for ultrasound elastography\n",
      "471\n",
      "algorithm 1: poisson’s ratio clipper\n",
      "input : lateral displacement wl, axial displacement wa, vemin,vemax, iteration\n",
      "output: reﬁned lateral displacement wref\n",
      "1 wref ← wl\n",
      "2 for q ← 1 to iteration do\n",
      "3\n",
      "e22 ← ∂wl\n",
      "∂l\n",
      "// gradient in lateral direction.\n",
      "epr × e11\n",
      "// use the displacement of previous line and the clipped epr to find the displacement of the next line\n",
      "algorithm 2: guo et al. reﬁnement [2] employed as known operator\n",
      "input : lateral displacement wl, axial displacement wa of size w × h,\n",
      "iteration, λ1, λ2\n",
      "output: reﬁned lateral displacement wref\n",
      "1 wref ← wl\n",
      "2 for q ← 1 to iteration do\n",
      "3\n",
      "for i, j in w, h do\n",
      "4\n",
      "δ = wl(i, j − 1) − 2wl(i, j) + wl(i, j + 1) + wa(i\n",
      "the known operators are added after optical ﬂow estimation,\n",
      "and reﬁne the estimated lateral displacement in each pyramid level (added from level\n",
      "3) to provide improved lateral displacement to the next pyramid level.\n",
      "where ld denotes photometric loss which is obtained by comparing the pre-\n",
      "compressed and warped compressed rf data, ls is smoothness loss in both\n",
      "axial and lateral directions.\n",
      "the young’s mod-\n",
      "ulus of the experimental phantom was 20 kpa and contains several inclusions\n",
      "with young’s modulus of higher than 40 kpa.\n",
      "in vivo data was collected at johns hopkins hospital from patients with liver\n",
      "cancer during open-surgical rf thermal ablation by a research antares siemens\n",
      "system using a vf 10-5 linear array with the sampling frequency of 40 mhz and\n",
      "the center frequency of 6.67 mhz.\n",
      "this has the advantage of correcting lateral\n",
      "displacements in diﬀerent pyramid levels.\n",
      "the hyper-parameters’ values\n",
      "of unsupervised training and the known operators are given in supplementary\n",
      "materials.\n",
      "3\n",
      "results and discussions\n",
      "3.1\n",
      "compared methods\n",
      "kpicture is compared to the following methods:\n",
      "known operators for ultrasound elastography\n",
      "473\n",
      "fig.\n",
      "2. lateral strains in the experimental phantom obtained by diﬀerent methods.\n",
      "the target and background windows for calculation of cnr and sr are marked in the\n",
      "b-mode images.\n",
      "the inclusion on the bottom of sample (1) is highlighted in picture\n",
      "and kpicture strain images by purple and blue arrows.\n",
      "axial strains\n",
      "are available in supplementary materials.\n",
      "[2], which employs the output of\n",
      "overwind as the initial displacement (overwind+ guo et al.).\n",
      "we also\n",
      "employed a similar hyper-parameters and training schedule for experimental\n",
      "phantom and in vivo data.\n",
      "3.2\n",
      "results and discussions\n",
      "the lateral strains of ultrasound rf data collected from three diﬀerent loca-\n",
      "tions of the tissue-mimicking breast phantom are depicted in fig. 2, and the\n",
      "quantitative results are given in table 1.\n",
      "[2] improves the displacement obtained by\n",
      "overwind.\n",
      "the strain images obtained by kpicture have a much higher\n",
      "quality than those of picture.\n",
      "furthermore, kpicture has the highest qual-\n",
      "ity strain images among the compared methods.\n",
      "for example, the inclusion on\n",
      "474\n",
      "a. k. z. tehrani and h. rivaz\n",
      "the bottom in sample 1 (highlighted by the arrows) is clearly visible in kpic-\n",
      "ture, a substantial improvement over all other methods that do not even show\n",
      "the inclusion.\n",
      "quantitative results of lateral strains for experimental phantoms.\n",
      "the pair marked by † is not statistically signiﬁcant (p-value > 0.05,\n",
      "using friedman test).\n",
      "the histograms of epr values of overwind+gou et al., picture and\n",
      "kpicture are illustrated for the experimental phantom sample (1).\n",
      "3 (b), and axial\n",
      "strains are given in the supplementary materials (the quality of axial strains is\n",
      "high in all methods).\n",
      "while picture may produce an adequate strain image,\n",
      "it still contains noisy regions.\n",
      "on the other hand, kpicture delivers excep-\n",
      "tionally reﬁned strain images and surpasses the other compared methods.\n",
      "the histogram of epr values for experimental phantom sample 1 (a).\n",
      "their performance on anisotropic materials can be investigated by\n",
      "experiments on anisotropic tissues such as muscles.\n",
      "it should be noted that after incorporating the known operators, the inference\n",
      "time of the network increased from an average of 195 ms to 240 ms (having 10\n",
      "iterations for algorithm 1 and 100 iterations for algorithm 2).\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf:\n",
      "recently, weakly supervised nuclei segmentation methods\n",
      "using only points are gaining attention, as they can ease the tedious\n",
      "labeling process.\n",
      "we detect and segment nuclei by combining a binary segmentation mod-\n",
      "ule, an oﬀset regression module, and a center detection module to deter-\n",
      "mine foreground pixels, delineate boundaries and identify instances.\n",
      "next, segmentation pre-\n",
      "dictions are used to repeatedly generate pseudo oﬀset maps that indi-\n",
      "cate the most likely nuclei center.\n",
      "experimental results\n",
      "show that our model consistently outperforms state-of-the-art methods\n",
      "on public datasets regardless of the point annotation accuracy.\n",
      "keywords: weakly supervised nuclei segmentation · instance\n",
      "segmentation · point reﬁnement · oﬀset map · geodesic distance\n",
      "1\n",
      "introduction\n",
      "nuclei segmentation in histopathology images is an important task for cancer\n",
      "diagnosis and immune response prediction [1,13,18].\n",
      "while several fully super-\n",
      "vised deep learning approaches to segment nuclei exist [2,6,8,9,19,25], labeling\n",
      "s. nam and j. jeong—equal contribution.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0_51.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43907-0_51\n",
      "pronet for nuclei instance segmentation\n",
      "529\n",
      "thousands of instances are tedious and the ambiguous nature of nuclei bound-\n",
      "aries requires high-level expert annotators.\n",
      "to address this, weakly-supervised\n",
      "nuclei segmentation methods\n",
      "as\n",
      "point labels alone do not provide suﬃcient foreground information, it is com-\n",
      "mon to use euclidean distance-based voronoi diagrams and k-means clustering\n",
      "[7] to generate pseudo segmentation labels for training.\n",
      "in real-world scenarios, point annotation locations may shift from nuclei centers\n",
      "as a result of the expert labeling process, leading to a lower performance after\n",
      "model training.\n",
      "to overcome these challenges, we propose a novel weakly supervised instance\n",
      "segmentation method that eﬀectively distinguishes adjacent nuclei and is robust\n",
      "to point shifts.\n",
      "the proposed model consists of three modules responsible for\n",
      "binary segmentation, boundary delineation, and instance separation.\n",
      "to train\n",
      "the binary segmentation module, we generate pseudo binary segmentation masks\n",
      "using geodesic distance-based voronoi labels and cluster labels from point anno-\n",
      "tations.\n",
      "to train the oﬀset map module, we\n",
      "generate pseudo oﬀset maps by computing the oﬀset distance between binary\n",
      "segmentation pixel predictions and the point label.\n",
      "this reﬁnement process ensures that the model maintains high\n",
      "performance even when the point annotation is not exactly located at the center\n",
      "of the nuclei.\n",
      "the contributions of this paper are as follows: (1) we propose an end-to-\n",
      "end weakly supervised segmentation model that simultaneously predicts binary\n",
      "mask, oﬀset map, and center map to accurately identify and segment nuclei.\n",
      "(3) we\n",
      "introduce an em algorithm-based reﬁnement process to encourage model robust-\n",
      "ness on center-shifted point labels.\n",
      "2\n",
      "methodology\n",
      "we propose an end-to-end nuclei segmentation method that only uses point\n",
      "annotations p to predict nuclei instance segmentation masks ˆs.\n",
      "it consists of an encoder and three modules\n",
      "for binary segmentation, oﬀset map and center map prediction.\n",
      "to train oﬀset map\n",
      "and center map modules(blue lines), pseudo labels are generated using point label and\n",
      "predicted binary segmentation mask(green lines).\n",
      "during inference, the instance map,\n",
      "obtained by predicted oﬀset map and center map, is multiplied with predicted binary\n",
      "mask to produce instance segmentation prediction(orange lines).\n",
      "(color ﬁgure online)\n",
      "model consists of three modules: 1) binary segmentation module, 2) oﬀset map\n",
      "module, and 3) center map module (fig. 1).\n",
      "for a given input image, we extract\n",
      "feature maps with an imagenet-pretrained vgg16 backbone encoder.\n",
      "the fea-\n",
      "ture maps are further processed through a series of residual units (rus) and\n",
      "attention units (aus) to predict a binary segmentation mask ˆb, an oﬀset map\n",
      "ˆo, and a center map ˆc.\n",
      "the rus are employed to maintain feature information\n",
      "so that subsequent modules can reuse the features from early-stage modules.\n",
      "in the training stage, we ﬁrst generate a voronoi label v and a cluster label k\n",
      "along the green lines in fig.\n",
      "1 to train the segmentation module.\n",
      "then, we gener-\n",
      "ate the pseudo oﬀset map o by using ˆb and p. next, following [29], we generate\n",
      "the center map c by expanding the point label p with gaussian kernel within\n",
      "a radius r. herein, our model is trained wih a segmentation loss lb(v,k, ˆb), an\n",
      "oﬀset map loss lo(o, ˆo), and a center map loss lc(c, ˆc).\n",
      "thus, we\n",
      "employ an em algorithm to search the optimal model parameters θ to obtain\n",
      "more reliable points p ′.\n",
      "pronet for nuclei instance segmentation\n",
      "531\n",
      "in the inference stage, ˆb, ˆo, ˆc are predicted following the orange lines in\n",
      "fig.\n",
      "finally, the instance segmentation output ˆs is obtained\n",
      "by ˆb × i.\n",
      "fig.\n",
      "(a) input\n",
      "image; (b) ground truth; (c) the cluster labels generated by euclidean distance, and\n",
      "(d) those by geodesic distance.\n",
      "(color ﬁgure online)\n",
      "2.1\n",
      "loss functions using pseudo labels\n",
      "segmentation loss.\n",
      "we generate v and k to train the binary segmentation\n",
      "module.\n",
      "to train the binary segmentation module using\n",
      "v and k, we employ a voronoi loss lv and a cluster loss lk based on the\n",
      "cross-entropy:\n",
      "lv =\n",
      "1\n",
      "nωv\n",
      "\u0002\n",
      "x,y∈ωv\n",
      "v(x, y)log( ˆb(x, y))\n",
      "following [17], we deﬁne\n",
      "the ﬁnal segmentation loss as lb =\n",
      "w and h are the\n",
      "width and height of the input image.\n",
      "3. (a) input image (top) and ground truth (bottom), (b) instance map (top) and\n",
      "center map (bottom) generated by the optimal nuclei center points, (c) those by shifted\n",
      "points (6–8), and (d) those by reﬁned points.\n",
      "inspired by [2], we deﬁne an oﬀset\n",
      "vector o(x, y) that indicates the displacement of a point (x, y) to the center of\n",
      "its corresponding nucleus.\n",
      "to train the oﬀset module, we ﬁrst compute o(x, y)\n",
      "of each nucleus segmented by ˆb.\n",
      "(4)\n",
      "pronet for nuclei instance segmentation\n",
      "533\n",
      "it is worth noting that in the early stages of training, the pseudo oﬀset map o\n",
      "generated by ˆb and p is unreliable.\n",
      "2.2\n",
      "reﬁnement via expectation maximization algorithm\n",
      "training with nuclei (center) shifted point labels can lead to blurry center map\n",
      "predictions (see fig.\n",
      "this in turn limits model optimization and it’s ability\n",
      "to distinguish objects, resulting in poor adjacent nuclei segmentation.\n",
      "to address\n",
      "this, we propose an em based center point reﬁnement process.\n",
      "(6)\n",
      "since reliable ˆo is necessary to reﬁne nuclei centers, reﬁnement starts after 30\n",
      "epochs.\n",
      "3\n",
      "experiments\n",
      "dataset.\n",
      "to validate the eﬀectiveness of our model, we use two public nuclei\n",
      "segmentation datasets i.e., cpm17\n",
      "[12]. cpm17 contains\n",
      "64 h&e stained images with 7,570 annotated nuclei boundaries sized from\n",
      "500×500 to 600×600.\n",
      "the set is split into 32/32 images for training and testing.\n",
      "images were normalized and cropped to 300×300.\n",
      "monuseg is a multi-organ\n",
      "nuclei segmentation dataset consisting of 30 h&e stained images (1000×1000)\n",
      "extracted from seven diﬀerent organs.\n",
      "we used 16 images (4 images from the\n",
      "breast, liver, kidney, and prostate) as training and 14 images (2 images from each\n",
      "breast, liver, kidney, prostate, bladder, brain, and stomach) as testing.\n",
      "for a fair\n",
      "comparison, images were pre-processed before training/testing i.e., normalized\n",
      "and cropped to 250×250 patches following the setting used in [17].\n",
      "534\n",
      "s. nam et al.\n",
      "to make point labels, we use the center point of full mask annotations.\n",
      "the shift is performed in\n",
      "pixels and is randomly selected between the minimum and maximum values.\n",
      "implementation details.\n",
      "for training, all evaluated models were run for 150\n",
      "epochs with the adam optimizer\n",
      "the gaussian kernel r was set as\n",
      "r = 6 and δ was set as 8 for reﬁnement on cpm17.\n",
      "finally, a variety of augmentations were employed i.e.,\n",
      "random resizing, cropping, and rotations etc., following [17], with loss weights\n",
      "λb, λo and λc empirically set to 1.\n",
      "performance comparison of nuclei segmentation on two public datasets.\n",
      "table 1 shows the performance of our method against state-\n",
      "of-the-art\n",
      "weakly supervised nuclei segmentation methods\n",
      "as opposed to the dice score,\n",
      "aji is key when evaluating adjacent nuclei separation in instance segmentation\n",
      "tasks.\n",
      "regarding reﬁnement, we observed that our strategy is more bene-\n",
      "ﬁcial when points exhibit signiﬁcant shifts i.e., on both cpm and monuseg.\n",
      "figure 3 showcases the eﬀectiveness of the reﬁnement process wherein the model\n",
      "generates precise instance and center maps.\n",
      "with the geodesic distance and the\n",
      "reﬁnement process, our proposed method achieved state-of-the-art performance.\n",
      "this demonstrates that our method separates adjacent nuclei accurately, and\n",
      "maintains its robustness, achieving consistent performance even when the point\n",
      "annotations are not located at the center of the nuclei.\n",
      "additionally, in fig. 4, we\n",
      "qualitatively show the results to highlight how our method precisely separates\n",
      "adjacent nuclei.\n",
      "pronet for nuclei instance segmentation\n",
      "535\n",
      "table 2. evaluation on the eﬀect of oﬀset and center maps.\n",
      "4.\n",
      "nuclei instance segmentation results on cpm17 (top 2 rows) and monuseg\n",
      "(bottom 2 rows) images.\n",
      "the images and the ground truth (gt) are shown in the left\n",
      "column.\n",
      "we conducted ablation studies to assess the impact of\n",
      "the oﬀset regression module, geodesic distance, and point reﬁnement process\n",
      "(table 2).\n",
      "when the binary segmentation module is combined only with the cen-\n",
      "ter map module without the oﬀset module, the model could separate nuclei only\n",
      "trained by the ideal label.\n",
      "on the other hand, since there was no reﬁnement pro-\n",
      "cess due to the absence of the oﬀset map, inaccurate points extracted from the\n",
      "center map are obtained in the real-world scenario.\n",
      "we also demonstrate that\n",
      "labels with geodesic distance help improve overall performance.\n",
      "the geodesic distance and reﬁnement\n",
      "process also improved the accuracy by contributing to more accurate pseudo\n",
      "labels.\n",
      "especially, most variants show a signiﬁcant drop in performance when\n",
      "the annotations shift was over 4 pixels.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_1.pdf:\n",
      "to address this dilemma, pet enhancement\n",
      "methods have been developed by improving the quality of low-dose pet\n",
      "(lpet) images to standard-dose pet (spet) images.\n",
      "however, previ-\n",
      "ous pet enhancement methods rely heavily on the paired lpet and\n",
      "spet data which are rare in clinic.\n",
      "thus, in this paper, we propose\n",
      "an unsupervised pet enhancement (upete) framework based on the\n",
      "latent diﬀusion model, which can be trained only on spet data.\n",
      "speciﬁ-\n",
      "cally, our spet-only upete consists of an encoder to compress the input\n",
      "spet/lpet images into latent representations, a latent diﬀusion model\n",
      "to learn/estimate the distribution of spet latent representations, and a\n",
      "decoder to recover the latent representations into spet images.\n",
      "more-\n",
      "over, from the theory of actual pet imaging, we improve the latent\n",
      "diﬀusion model of upete by 1) adopting pet image compression\n",
      "for reducing the computational cost of diﬀusion model, 2) using pois-\n",
      "son diﬀusion to replace gaussian diﬀusion for making the perturbed\n",
      "samples closer to the actual noisy pet, and 3) designing ct-guided\n",
      "cross-attention for incorporating additional ct images into the inverse\n",
      "process to aid the recovery of structural details in pet.\n",
      "with extensive\n",
      "experimental validation, our upete can achieve superior performance\n",
      "over state-of-the-art methods, and shows stronger generalizability to the\n",
      "dose changes of pet imaging.\n",
      "the code of our implementation is avail-\n",
      "able at https://github.com/jiang-cw/pet-diﬀusion.\n",
      "keywords: positron emission tomography (pet) ·\n",
      "enhancement ·\n",
      "latent diﬀusion model · poisson diﬀusion · ct-guided cross-attention\n",
      "1\n",
      "introduction\n",
      "positron emission tomography (pet) is a sensitive nuclear imaging technique,\n",
      "and plays an essential role in early disease diagnosis, such as cancers and\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "however, acquiring high-quality pet images requires\n",
      "injecting a suﬃcient dose (standard dose) of radionuclides into the human body,\n",
      "which poses unacceptable radiation hazards for pregnant women and infants\n",
      "even following the as low as reasonably achievable (alara) principle\n",
      "[19].\n",
      "to reduce the radiation hazards, besides upgrading imaging hardware, design-\n",
      "ing advanced pet enhancement algorithms for improving the quality of low-dose\n",
      "pet (lpet) images to standard-dose pet (spet) images is a promising alter-\n",
      "native.\n",
      "in recent years, many enhancement algorithms have been proposed to\n",
      "improve pet image quality.\n",
      "[22], which are quite robust but tend to over-smooth\n",
      "images and suppress the high-frequency details.\n",
      "subsequently, with the devel-\n",
      "opment of deep learning, the end-to-end pet enhancement networks [9,14,21]\n",
      "were proposed and achieved signiﬁcant performance improvement.\n",
      "consequently, unsupervised pet enhancement\n",
      "methods such as deep image prior\n",
      "fortunately, the recent glowing diﬀusion model [6] provides us with the idea\n",
      "for proposing a clinically-applicable pet enhancement approach, whose train-\n",
      "ing only relies on spet data.\n",
      "generally, the diﬀusion model consists of two\n",
      "reversible processes, where the forward diﬀusion adds noise to a clean image\n",
      "until it becomes pure noise, while the reverse process removes noise from pure\n",
      "noise until the clean image is recovered.\n",
      "by combining the mechanics of diﬀusion\n",
      "model with the observation that the main diﬀerences between lpet and spet\n",
      "are manifested as levels of noises in the image [11], we can view lpet and spet\n",
      "as results at diﬀerent stages in an integrated diﬀusion process.\n",
      "however, extending the diﬀusion\n",
      "model developed for 2d photographic images to pet enhancement still faces two\n",
      "problems: a) three-dimensionsal (3d) pet images will dramatically increase the\n",
      "computational cost of diﬀusion model; b) pet is the detail-sensitive images and\n",
      "may be introduced/lost some details during the procedure of adding/removing\n",
      "noise, which will aﬀect the downstream diagnosis.\n",
      "taking all into consideration, we propose the spet-only unsupervised pet\n",
      "enhancement (upete) framework based on the latent diﬀusion model.\n",
      "speciﬁ-\n",
      "cally, upete has an encoder-<diﬀusion model>-decoder structure that ﬁrst uses\n",
      "the encoder to compress input the lpet/spet images into latent representa-\n",
      "tions, then uses the latent diﬀusion model to learn/estimate the distribution\n",
      "of spet latent representations, and ﬁnally uses the decoder to recover spet\n",
      "images from the estimated spet latent representations.\n",
      "the keys of our upete\n",
      "pet-diﬀusion: unsupervised pet enhancement\n",
      "5\n",
      "fig.\n",
      "(a) and (b) provide the framework of upete as\n",
      "well as depict its implementation during both the training and testing phases, and (c)\n",
      "illustrates the details of ct-guided cross-attention.\n",
      "include 1) compressing the 3d pet images into a lower dimensional space for\n",
      "reducing the computational cost of diﬀusion model, 2) adopting the poisson\n",
      "noise, which is the dominant noise in pet imaging\n",
      "[20], to replace the gaussian\n",
      "noise in the diﬀusion process for avoiding the introduction of details that are not\n",
      "existing in pet images, and 3) designing ct-guided cross-attention to incorpo-\n",
      "rate additional ct images into the inverse process for helping the recovery of\n",
      "structural details in pet.\n",
      "our work had three main features/contributions: i) proposing a clinically-\n",
      "applicable unsupervised pet enhancement framework, ii) designing three tar-\n",
      "geted strategies for improving the diﬀusion model, including pet image com-\n",
      "pression, poisson diﬀusion, and ct-guided cross-attention, and iii) achieving\n",
      "better performance than state-of-the-art methods on the collected pet datasets.\n",
      "1. when given an input pet\n",
      "image x (i.e., spet for training and lpet for testing), x is ﬁrst compressed\n",
      "into the latent representation z0 by the encoder e. subsequently, z0 is fed into a\n",
      "latent diﬀusion model followed by the decoder d to output the expected spet\n",
      "image ˆx\n",
      "in addition, a specialized encoder ect is used to compress the ct\n",
      "image corresponding to the input pet image into the latent representation zct ,\n",
      "which is fed into each denoising network for ct-guided cross-attention.\n",
      "in the\n",
      "following, we introduce the details of image compression, latent diﬀusion model,\n",
      "and implementation.\n",
      "6\n",
      "c. jiang et al.\n",
      "2.1\n",
      "image compression\n",
      "the conventional diﬀusion model is computationally-demanding due to its\n",
      "numerous inverse denoising steps, which severely restricts its application to\n",
      "3d pet enhancement.\n",
      "to overcome this limitation, we adopt two strategies\n",
      "including 1) compressing the input image and 2) reducing the diﬀusion steps (as\n",
      "described in sect.\n",
      "similar to [10,18], we adopt an autoencoder (e and d) to compress the\n",
      "3d pet images into a lower dimensional but more compact space.\n",
      "the crucial\n",
      "aspects of this process is to ensure that the latent representation contains the\n",
      "necessary and representative information for the input image.\n",
      "among\n",
      "them, the perceptual loss, designed on a pre-trained 3d resnet [2], constrains\n",
      "higher-level information such as texture and semantic content, and the patch-\n",
      "based adversarial loss ensures globally coherent while remaining locally realistic.\n",
      "let x ∈ rh,w,z denote the input image and z0 ∈ rh,w,z,c denote the latent\n",
      "representation.\n",
      "in this way, we compress the input image by a factor of f = h/h =\n",
      "w/w = z/z. the results of spet estimation under diﬀerent compression rates\n",
      "f are provided in the supplement.\n",
      "2.2\n",
      "latent diﬀusion model\n",
      "after compressing the input pet image, its latent representation is fed into the\n",
      "latent diﬀusion model, which is the key to achieving the spet-only unsupervised\n",
      "pet enhancement.\n",
      "but the diﬀusion model is developed from photographic images, which\n",
      "have signiﬁcant diﬀerence with the detail-sensitive pet images.\n",
      "to improve\n",
      "its applicability for pet images, we design several targeted strategies for the\n",
      "diﬀusion process and inverse process, namely poisson diﬀusion and ct-guided\n",
      "cross-attention, respectively.\n",
      "however, in\n",
      "pet images, the dominant source of noise is poisson noise, rather than gaussian\n",
      "noise.\n",
      "considering this, in our upete we choose to adopt poisson diﬀusion to\n",
      "perturb the input samples, which facilitates the diﬀusion model for achieving\n",
      "better performance on the pet enhancement task.\n",
      "(1)\n",
      "pet-diﬀusion: unsupervised pet enhancement\n",
      "7\n",
      "at each diﬀusion step, we apply the perturb function to the previous perturbed\n",
      "sample zt−1 by imposing a poisson noise with an expectation of λt, which is\n",
      "linearly interpolated from [0, 1] and incremented with t. in our implementation,\n",
      "we apply the same poisson noise imposition operation as in [20], i.e., applying\n",
      "poisson deviates on the projected sinograms, to generate a sequence of perturbed\n",
      "samples with increasing poisson noise intensity as the step number t increases.\n",
      "the attenuation correction of pet typically\n",
      "relies on the corresponding anatomical image (ct or mr), resulting in a pet\n",
      "scan usually accompanied by a ct or mr scan.\n",
      "to fully utilize the extra-\n",
      "modality images (i.e., ct in our work) as well as improve the applicability of\n",
      "diﬀusion models, we design a ct-guided cross-attention to incorporate the ct\n",
      "images into the reverse process for assisting the recovery of structural details.\n",
      "as shown in fig. 1, to achieve a particular spet estimation, the correspond-\n",
      "ing ct image is ﬁrst compressed into the latent representation zct by encoder\n",
      "ect .\n",
      "denoting the output of previous layer\n",
      "as zp et , the ct-guided cross-attention can be formulated as follows:\n",
      "output = softmax(qct kt\n",
      "ct\n",
      "√\n",
      "d\n",
      "+ b) · vp et ,\n",
      "qct = convq(zct ),\n",
      "kct = convk(zct ),\n",
      "vp et = convv (zp et ),\n",
      "(2)\n",
      "where d is the number of channels, b is the position bias, and conv(·) denotes\n",
      "the 1 × 1 × 1 convolution with stride of 1.\n",
      "2.3\n",
      "implementation details\n",
      "typically, the trained diﬀusion model generates target images from random\n",
      "noise, requiring a large number of steps t to make the ﬁnal perturbed sam-\n",
      "ple (zt ) close to pure noise.\n",
      "however, in our task, the target spet image is\n",
      "generated from a given lpet image during testing, and making zt as close to\n",
      "pure noise as possible is not necessary since the remaining pet-related informa-\n",
      "tion can also beneﬁt the image recovery.\n",
      "therefore, we can considerably reduce\n",
      "the number of diﬀusion steps t to accelerate the model training, and t is set\n",
      "to 400 in our implementation.\n",
      "2. generalizability to dose changes.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "dataset\n",
      "our dataset consists of 100 spet images for training and 30 paired lpet and\n",
      "spet images for testing.\n",
      "among them, 50 chest-abdomen spet images are\n",
      "collected from (total-body) uexplorer pet/ct scanner\n",
      "[25], and 20 paired\n",
      "chest-abdomen images are collected by list mode of the scanner with 256 mbq of\n",
      "[18f]-fdg injection.\n",
      "speciﬁcally, the spet images are reconstructed by using\n",
      "the 1200 s data between 60–80 min after tracer injection, while the corresponding\n",
      "lpet images are simultaneously reconstructed by 120 s data uniformly sampled\n",
      "from 1200 s data.\n",
      "as a basic data preprocessing, all images are resampled to voxel spacing of\n",
      "2 × 2 × 2 mm3 and resolution of 256 × 256 × 160, while their intensity range is\n",
      "normalized to [0, 1] by min-max normalization.\n",
      "for increasing the training sam-\n",
      "ples and reducing the dependence on gpu memory, we extract the overlapped\n",
      "patches of size 96 × 96 × 96 from every whole pet image.\n",
      "all methods use the same experi-\n",
      "mental settings, and their quantitative results are given in table 1.\n",
      "from table 1, we can have the following observations.\n",
      "(1) ldm-p achieves\n",
      "better performance than ldm.\n",
      "this proves that the poisson diﬀusion is more\n",
      "appropriate than the gaussian diﬀusion for pet enhancement.\n",
      "(2) ldm-ct\n",
      "with the corresponding ct image for assisting denoising achieves better results\n",
      "than ldm.\n",
      "this can be reasonable as the ct image can provide anatomical infor-\n",
      "mation, thus beneﬁting the recovery of structural details (e.g., organ boundaries)\n",
      "in spet images.\n",
      "(3) ldm-p-ct achieves better results than all other variants\n",
      "pet-diﬀusion: unsupervised pet enhancement\n",
      "9\n",
      "table 2.\n",
      "quantitative comparison of our upete with several state-of-the-art pet\n",
      "enhancement methods, in terms of psnr and ssim, where ∗ denotes unsupervised\n",
      "method and † denotes fully-supervised method.\n",
      "on both psnr and ssim, which shows both of our proposed strategies con-\n",
      "tribute to the ﬁnal performance.\n",
      "these three comparisons conjointly verify the\n",
      "eﬀective design of our proposed upete, where the poisson diﬀusion process and\n",
      "ct-guided cross-attention both beneﬁt the pet enhancement.\n",
      "3.3\n",
      "comparison with state-of-the-art methods\n",
      "we further compare our upete with several state-of-the-art pet enhancement\n",
      "methods, which can be divided into two classes: 1) fully-supervised methods,\n",
      "including la-gan [21], transformer-gan (trans-gan)\n",
      "[9], and ar-gan [14]; 2) unsupervised methods, including\n",
      "deep image prior (dip)\n",
      "compared to the fully-supervised method ar-gan which\n",
      "achieves sub-optimal performance, our upete does not require paired lpet and\n",
      "spet, yet still achieves improvement.\n",
      "additionally, upete also achieves notice-\n",
      "able performance improvement to noise2void (which is a supervised method).\n",
      "speciﬁcally, the average improvement in psnr and ssim on spet estimation\n",
      "are 1.554 db and 0.005, respectively.\n",
      "first, compared to unsupervised methods such\n",
      "as dip and noise2void, the spet images estimated by our upete have less noise\n",
      "but clearer boundaries.\n",
      "second, our upete performs better on the structural\n",
      "details compared to the fully-supervised methods, i.e., missing unclear tissue\n",
      "(trans-gan) or introducing non-existing artifacts in pet image (df-gan).\n",
      "3. visual comparison of estimated spet images on two typical cases.\n",
      "3.4\n",
      "generalization evaluation\n",
      "we further evaluate the generalizability of our upete to tracer dose changes by\n",
      "simulating poisson noise on spet to produce diﬀerent doses for lpet, which\n",
      "is a common way to generate noisy pet data [20].\n",
      "the main reason is that the unsupervised learning has the ability to extract\n",
      "patterns and features from the data based on the inherent structure and distri-\n",
      "bution of the data itself [15].\n",
      "pet-diﬀusion: unsupervised pet enhancement\n",
      "11\n",
      "4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf:\n",
      "uda allows for the use of\n",
      "large-scale datasets from various domains for model deployment, but it\n",
      "can face diﬃculties in performing adaptive feature extraction when deal-\n",
      "ing with unlabeled data in an unseen target domain.\n",
      "this helps the framework to converge to a local mini-\n",
      "mum that is better-suited for the target domain, allowing for improved\n",
      "performance in domain adaptation tasks.\n",
      "to evaluate our framework, we\n",
      "carried out experiments on uda segmentation tasks using breast can-\n",
      "cer datasets acquired from multiple domains.\n",
      "our experimental results\n",
      "demonstrated that our framework achieved state-of-the-art performance,\n",
      "outperforming other competing uda models, in segmenting breast can-\n",
      "cer on ultrasound images from an unseen domain, which supports its\n",
      "clinical potential for improving breast cancer diagnosis.\n",
      "keywords: unsupervised domain adaptation · test-time tuning ·\n",
      "breast cancer · segmentation · ultrasound imaging\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0_52.\n",
      "https://doi.org/10.1007/978-3-031-43907-0_52\n",
      "540\n",
      "k. lee et al.\n",
      "1\n",
      "introduction\n",
      "in recent years, deep learning (dl) methods have demonstrated remarkable\n",
      "performance in detecting and localizing tumors on ultrasound images [2,27].\n",
      "compared with conventional image processing methods, dl methods provide\n",
      "an accurate feature extraction capability on ultrasound images, despite their\n",
      "low resolution and noise disturbance, leading to superior segmentation accu-\n",
      "racy\n",
      "yet, acquiring large training datasets and\n",
      "their corresponding labels, especially from a cohort of patients, can be costly\n",
      "or even infeasible, which poses a signiﬁcant challenge in developing a dl model\n",
      "with high performance [7].\n",
      "second, even when large-scale datasets are available\n",
      "through collaborative research from multiple sites, dl models trained on such\n",
      "datasets may yield sub-optimal solutions due to domain gaps caused by diﬀer-\n",
      "ences in images acquired from diﬀerent sites [20].\n",
      "third, due to the small number\n",
      "of datasets from each domain, the images for each individual domain may not\n",
      "capture representative features, limiting the ability of dl models to generalize\n",
      "across domains [3].\n",
      "domain adaptation (da) has been extensively studied to alleviate the afore-\n",
      "mentioned limitations, the goal of which is to reduce the domain gap caused by\n",
      "the diversity of datasets from diﬀerent domains\n",
      "nonetheless,\n",
      "unlike natural images, generating labels can be a challenging task, making it dif-\n",
      "ﬁcult to apply general da methods; thus bridging domain gaps by da methods\n",
      "remains limited\n",
      "this can lead to\n",
      "signiﬁcant degradation of the performance of dl models, as errors can compound\n",
      "and become more pronounced over time [17,25].\n",
      "to alleviate the problem of pseudo-label-based uda, in this work, we propose\n",
      "an advanced uda framework based on self-supervised da with a test-time ﬁne-\n",
      "tuning network.\n",
      "the distinctive feature\n",
      "of our test-time self-supervised da is that it enables the dl network (i) to learn\n",
      "knowledge about the features of target domains by ﬁne-tuning the network itself\n",
      "during the test-time phase, rather than generating pseudo-labels and then (ii) to\n",
      "provide precise predictions on images in target domains, by using the ﬁne-tuned\n",
      "network.\n",
      "our framework was tested on the task of breast\n",
      "self-supervised domain adaptive segmentation of breast cancer\n",
      "541\n",
      "cancer segmentation in ultrasound images, but it could also be applied to other\n",
      "lesion segmentation tasks.\n",
      "to summarize, our contributions are three-fold:\n",
      "• we design a self-supervised da framework that includes a parameter search\n",
      "method and provide a mathematical justiﬁcation for it.\n",
      "with our framework,\n",
      "we are able to identify the best-performing parameters that result in improved\n",
      "performance in da tasks.\n",
      "• we applied our framework to the task of segmenting breast cancer from ultra-\n",
      "sound imaging data, demonstrating its superior performance over competing\n",
      "uda methods.\n",
      "our results indicate that our framework is eﬀective in improving the accuracy of\n",
      "breast cancer segmentation from ultrasound images, which could have potential\n",
      "implications for improving the diagnosis and treatment of breast cancer.\n",
      "1. in\n",
      "the main task, an encoder (e), a decoder for segmentation (dseg), and a seg-\n",
      "mentation header (h) are included.\n",
      "the main task is the segmentation task,\n",
      "(h◦dseg◦e)(x).\n",
      "in predicting segmentation labels in the target domain (t ), dft\n",
      "is also involved in the main task, and the ﬁnal prediction after the ﬁne-tuning is\n",
      "542\n",
      "k. lee et al.\n",
      "provided by\n",
      "\u0002\n",
      "h ◦ (dseg ⊕ dft) ◦ e\n",
      "\u0003\n",
      "(x), where ⊕ is the concatenation operation.\n",
      "the pretext task aims to generate synthetic images, (dgen ◦ e)(t).\n",
      "however, since the headers of image reconstruction and generating segmen-\n",
      "tation mask are diﬀerent (diﬀerent output), a new header incorporating df t\n",
      "and dseg is devised and leverages the outputs of two decoders.\n",
      "besides, dgen =\n",
      "df t is ﬁne-tuned during the ﬁne-tuning step, and the df t learns the knowledge\n",
      "of the input domain via image reconstruction.\n",
      "the model m is ﬁrst trained in s in a\n",
      "supervised manner with (s, ¯s) ∼ s in both main and pretext tasks as below:\n",
      "θm\n",
      "s , θp\n",
      "s = argmin\n",
      "θm\n",
      "s ,θp\n",
      "s\n",
      "\u0004\n",
      "s\n",
      "\u0005\n",
      "lbce\n",
      "\u0006\n",
      "(h ◦ dseg ◦ e)(s), ¯s\n",
      "\u0007\n",
      "+ lgan\n",
      "\u0006\n",
      "(dgen ◦ e)(s), s\n",
      "\n",
      ",\n",
      "(1)\n",
      "where lbce and lgan represent the loss functions for binary cross-entropy and\n",
      "generative adversarial network\n",
      "to this end, in the pretext task, for self-supervised learning, the model\n",
      "is ﬁne-tuned in t to generate synthetic images identical to the input images as\n",
      "below:\n",
      "θp\n",
      "t = argmin\n",
      "θp\n",
      "t\n",
      "\u0004\n",
      "t\n",
      "lgan\n",
      "\u0006\n",
      "(ds\n",
      "gen ◦ es)(t), t\n",
      "\u0007\n",
      "⇒\n",
      "θp\n",
      "t ⊇ es ∪ ds→t\n",
      "gen\n",
      ",\n",
      "(2)\n",
      "where only dgen is ﬁne-tuned to achieve memory eﬃciency and to decrease the\n",
      "ﬁne-tuning time, and ds\n",
      "gen is ﬁne-tuned as ds→t\n",
      "gen\n",
      ".\n",
      "(3)\n",
      "since ds\n",
      "seg is fully optimized for s in a supervised manner, it guarantees a\n",
      "baseline segmentation performance.\n",
      "furthermore, since dt\n",
      "ft is ﬁne-tuned in t\n",
      "self-supervised domain adaptive segmentation of breast cancer\n",
      "543\n",
      "fig.\n",
      "wi ·x. since ds\n",
      "seg provides the baseline segmentation performance, dt\n",
      "f t\n",
      "should provide similar feature maps to achieve the baseline performance.\n",
      "(5)\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "experimental set-ups\n",
      "to evaluate the segmentation performance of our ttft framework, we used\n",
      "three diﬀerent ultrasound databases: bus\n",
      "all three databases contain ultrasound\n",
      "imaging data and segmentation masks for breast cancer, with the masks labeled\n",
      "as 0 (background) and 1 (lesion) using a one-hot encoding.\n",
      "the bus database\n",
      "consists of 163 images along with corresponding labels.\n",
      "the busi database con-\n",
      "tains 780 images, with 133 images belonging to the normal class and having\n",
      "labels containing only 0 values.\n",
      "while the database also provides\n",
      "labels for the detection task, we processed these labels as segmentation masks\n",
      "using a region growing method [15].\n",
      "[21] were employed as our baseline models, since u-net\n",
      "is a widely used basic model for segmentation, and fusionnet contains advanced\n",
      "residual modules, compared with u-net.\n",
      "additionally, mib-\n",
      "net [28], which is a state-of-the-art model for breast cancer segmentation using\n",
      "ultrasound images, was employed for comparison.\n",
      "our experimen-\n",
      "tal set-ups included: (i) individual databases were used to assess the baseline\n",
      "segmentation performance (appendix); (ii) the domain adaptive segmentation\n",
      "performance was assessed using the three databases, where two databases were\n",
      "regarded as the source domain, and the remaining database was regarded as the\n",
      "target domain; and (iii) the ablation study was carried out to evaluate the pro-\n",
      "posed network architecture along with the randomized re-initialization method.\n",
      "3.2\n",
      "comparison analysis\n",
      "since all compared dl models show similar d. coef, only uda performance is\n",
      "comparable as a control in our experiments.\n",
      "in this experiment, two databases\n",
      "were used for training, and the remaining database was used for testing.\n",
      "3 illustrates the bus database was used for testing, and\n",
      "self-supervised domain adaptive segmentation of breast cancer\n",
      "545\n",
      "fig.\n",
      "3. comparison analysis of our framework and comparison models: performance\n",
      "comparison table (left) and box-and-whisker plot (right).\n",
      "5 shows the sample segmentation results.\n",
      "unlike the experiment using the individual database, u-net, fusionnet, and\n",
      "mib-net showed signiﬁcantly inferior scores due to domain gaps.\n",
      "in contrast,\n",
      "uda methods of cbst and ct-net showed superior scores, compared with\n",
      "others, and the scores were not strongly reduced, compared with the experiment\n",
      "with the single database.\n",
      "note that, our ttft framework achieved the best\n",
      "performance compared with other dl models.\n",
      "5. segmentation results by ours and comparison models on each database.\n",
      "fig.\n",
      "6. illustration of feature maps: style loss comparison (left) and a t-sne plot of\n",
      "generated images by diﬀerent decoders (right)\n",
      "3.3\n",
      "ablation study\n",
      "in order to assess the eﬀectiveness of each of the proposed modules, includ-\n",
      "ing the parameter ﬂuctuation and ﬁne-tuning methods, the ablation study was\n",
      "carried out.\n",
      "the higher d. coef value (+3.4%) of pre-train + pf than\n",
      "that of pre-train + random init and pre-train + oﬀset conﬁrms the eﬀec-\n",
      "tiveness of the parameter ﬂuctuation in the uda performance.\n",
      "additionally,\n",
      "the higher score (+11%) of fine-tuning than pre-train shows an outstanding\n",
      "uda performance of the ﬁne-tuning pipeline.\n",
      "using dual-pipeline and parameter ﬂuctuation yielded\n",
      "the best performance.\n",
      "however, the utilization of ensemble pipelines of multiple\n",
      "ﬁne-tuning modules was ineﬃcient, since negligible performance improvements\n",
      "(+0.002) were observed, despite the heavy memory utilization.\n",
      "our framework was ﬁne-tuned as ds\n",
      "seg → dﬂ\n",
      "seg → ds→t\n",
      "seg\n",
      "self-supervised domain adaptive segmentation of breast cancer\n",
      "547\n",
      "table 1.\n",
      "additionally, the generated images\n",
      "by decoders, including ds\n",
      "seg, dﬂ\n",
      "seg, and ds→t\n",
      "seg\n",
      "in s and t are plotted with t-\n",
      "sne, where the short distance represents the similar features [19].\n",
      "the generated\n",
      "images became similar to t in order of ds\n",
      "seg, dﬂ\n",
      "seg, and ds→t\n",
      "seg\n",
      ", which conﬁrmed\n",
      "the eﬀectiveness of the ﬁne-tuning method in terms of knowledge distillation.\n",
      "additionally, the parameters were successfully re-positioned from the local min-\n",
      "imum in s by parameter ﬂuctuation, which was conﬁrmed by the distances from\n",
      "s to ds\n",
      "gen and dﬂ\n",
      "gen.\n",
      "4\n",
      "discussion and conclusion\n",
      "in this work, we proposed a dl-based segmentation framework for multi-domain\n",
      "breast cancer segmentation on ultrasound images.\n",
      "due to the low resolution of\n",
      "ultrasound images, manual segmentation of breast cancer is challenging even\n",
      "for expert clinicians, resulting in a sparse number of labeled data.\n",
      "to address\n",
      "this issue, we introduced a novel self-supervised da network for breast cancer\n",
      "segmentation in ultrasound images.\n",
      "since uda is susceptible to error accumulation due to\n",
      "imprecise pseudo-labels, which can lead to degraded performance, we employed\n",
      "a self-supervised learning-based pretext task.\n",
      "speciﬁcally, we utilized an auto-\n",
      "encoder-based network architecture to generate synthetic images that matched\n",
      "the input images.\n",
      "this approach enabled our framework to eﬃciently\n",
      "ﬁne-tune the network in the target domain and achieve better segmentation\n",
      "performance.\n",
      "experimental results, carried out with three ultrasound databases\n",
      "from diﬀerent domains, demonstrated the superior segmentation performance of\n",
      "our framework over other competing methods.\n",
      "[21] as baseline models to evaluate the basic performance of our\n",
      "ttft framework.\n",
      "however, the use of more advanced baseline models could lead\n",
      "to even better segmentation performance, which is a subject for our future work.\n",
      "moreover, our proposed framework is not limited to breast cancer segmentation\n",
      "on ultrasound images acquired from diﬀerent domains.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf:\n",
      "automated detection of gallbladder cancer (gbc) from\n",
      "ultrasound (us) images is an important problem, which has drawn\n",
      "increased interest from researchers.\n",
      "in this paper, we focus on gbc detection using\n",
      "only image-level labels.\n",
      "however, our analysis reveals that it is diﬃ-\n",
      "cult to train a standard image classiﬁcation model for gbc detection.\n",
      "this is due to the low inter-class variance (a malignant region usually\n",
      "occupies only a small portion of a us image), high intra-class variance\n",
      "(due to the us sensor capturing a 2d slice of a 3d object leading to large\n",
      "viewpoint variations), and low training data availability.\n",
      "we posit that\n",
      "even when we have only the image level label, still formulating the prob-\n",
      "lem as object detection (with bounding box output) helps a deep neural\n",
      "network (dnn) model focus on the relevant region of interest.\n",
      "our proposed method demon-\n",
      "strates an improvement of ap and detection sensitivity over the sota\n",
      "transformer-based and cnn-based wsod methods.\n",
      "project page is at\n",
      "https://gbc-iitd.github.io/wsod-gbc.\n",
      "keywords: weakly supervised object detection · ultrasound ·\n",
      "gallbladder cancer\n",
      "1\n",
      "introduction\n",
      "gbc is a deadly disease that is diﬃcult to detect at an early stage [12,15].\n",
      "non-ionizing radiation,\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0 20.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14220, pp.\n",
      "206–215, 2023.\n",
      "https://doi.org/10.1007/978-3-031-43907-0_20\n",
      "gall bladder cancer detection from us images\n",
      "207\n",
      "low cost, and accessibility make us a popular non-invasive diagnostic modality for\n",
      "patients with suspected gall bladder (gb) aﬄictions.\n",
      "in recent\n",
      "years, automated gbc detection from us images has drawn increased interest\n",
      "[3,5] due to its potential for improving diagnosis and treatment outcomes.\n",
      "many\n",
      "of these works formulate the problem as an object detection, since training a\n",
      "image classiﬁcation model for gbc detection seems challenging due to the reasons\n",
      "outlined in the abstract (also see fig. 1).\n",
      "fig.\n",
      "however, the appearance of the gb\n",
      "in all three images is very similar.\n",
      "all three images\n",
      "have been scanned from the same patient, but due to the sensor’s scanning plane, the\n",
      "appearances change drastically.\n",
      "recently, gbcnet [3], a cnn-based model, achieved sota performance on clas-\n",
      "sifying malignant gb from us images.\n",
      "gbcnet uses a two-stage pipeline consisting\n",
      "of object detection followed by classiﬁcation, and requires bounding box annota-\n",
      "tions for gb as well as malignant regions for training.\n",
      "in another recent work, [5]\n",
      "has exploited additional unlabeled video data for learning good representations\n",
      "for downstream gbc classiﬁcation and obtained performance similar to [3] using\n",
      "a resnet50 [13] classiﬁer.\n",
      "on the other hand, the image-level\n",
      "malignancy label is usually available at a low cost, as it can be obtained readily\n",
      "from the diagnostic report of a patient without additional eﬀort from clinicians.\n",
      "however, since we only have image-level labels avail-\n",
      "able, we formulate the problem as a weakly supervised object detection (wsod)\n",
      "problem.\n",
      "however, in our initial experiments sota wsod methods for\n",
      "transformers failed miserably.\n",
      "four images from\n",
      "each of the disease and non-disease classes are shown on the left and right, respectively.\n",
      "disease locations are shown by drawing bounding boxes.\n",
      "in this, one generates region proposals for images, and then con-\n",
      "siders the images as bags and region proposals as instances to solve the instance\n",
      "classiﬁcation (object detection) under the mil constraints\n",
      "our experiments val-\n",
      "idate the utility of this approach in circumventing the challenges in us images\n",
      "and detecting gbc accurately from us images using only image-level labels.\n",
      "contributions: the key contributions of this work are:\n",
      "– we design a novel detr variant based on mil with self-supervised instance\n",
      "learning towards the weakly supervised disease detection and localization task\n",
      "in medical images.\n",
      "– we formulate the gbc classiﬁcation problem as a weakly supervised object\n",
      "detection problem to mitigate the eﬀect of low inter-class and large intra-class\n",
      "variances, and solve the diﬃcult gbc detection problem on us images without\n",
      "using the costly and diﬃcult to obtain additional annotation (bounding box)\n",
      "or video data.\n",
      "– our method provides a strong baseline for weakly supervised gbc detection\n",
      "and localization in us images, which has not been tackled earlier.\n",
      "further, to\n",
      "assess the generality of our method, we apply our method to polyp detection\n",
      "from colonoscopy images.\n",
      "gall bladder cancer detection from us images\n",
      "209\n",
      "2\n",
      "datasets\n",
      "gallbladder cancer detection in ultrasound images: we use the pub-\n",
      "lic gbc us dataset\n",
      "[3] consisting of 1255 image samples from 218 patients.\n",
      "the dataset contains 990 non-malignant (171 patients) and 265 malignant (47\n",
      "patients) gb images (see fig.\n",
      "2 for some sample images).\n",
      "the dataset contains\n",
      "image labels as well as bounding box annotations showing the malignant regions.\n",
      "note that, we use only the image labels for training.\n",
      "we did the cross-validation splits at the patient level, and all\n",
      "images of any patient appeared either in the train or validation split.\n",
      "fig.\n",
      "polyp detection in colonoscopy images: we use the publicly available\n",
      "kvasir-seg\n",
      "[17] dataset consisting of 1000 white light colonoscopy images show-\n",
      "ing polyps (see fig.\n",
      "since kvasir-seg does not contain any control images,\n",
      "we add 600 non-polyp images randomly sampled from the polypgen [1] dataset.\n",
      "for gbc classiﬁcation, if the model generates\n",
      "bounding boxes for the input image, then we predict the image to be malignant,\n",
      "since the only object present in the data is the cancer.\n",
      "mil setup: the decoder of the ﬁne-tuning detr generates r d-dimensional\n",
      "output embeddings.\n",
      "the two\n",
      "matrices are element-wise multiplied and summed over the proposal dimension\n",
      "to generate the image-level classiﬁcation predictions, φ ∈ rnc:\n",
      "φj =\n",
      "r\n",
      "\u0002\n",
      "i=1\n",
      "cij · dij\n",
      "(1)\n",
      "notice, φj ∈ (0, 1) since cij and dij are normalized.\n",
      "finally, the negative log-\n",
      "likelihood loss between the predicted labels, and image labels y ∈ rnc is com-\n",
      "puted as the mil loss:\n",
      "+ (1 − yi) log (1 − φi)]\n",
      "(2)\n",
      "the mil classiﬁer further suﬀers from overﬁtting to the distinctive classiﬁcation\n",
      "features due to the mismatch of classiﬁcation and detection probabilities [24].\n",
      "to tackle this, we further use a self-supervised module to improve the instances.\n",
      "gall bladder cancer detection from us images\n",
      "211\n",
      "self-supervised instance learning: inspired by [24], we design a instance\n",
      "learning module with nr blocks in a self-supervised framework to reﬁne the\n",
      "instance scores with instance-level supervision.\n",
      "weakly supervised disease detection performance comparison of our method\n",
      "and sota baselines in gbc and polyps.\n",
      "we report average precision at iou 0.25\n",
      "(ap25).\n",
      "performance of mil-framework variants on detr.\n",
      "5:\n",
      "l = lmil + λlins\n",
      "(5)\n",
      "4\n",
      "experiments and results\n",
      "experimental setup: we use a machine with intel xeon gold 5218@2.30ghz\n",
      "processor and 8 nvidia tesla v100 gpus for our experiments.\n",
      "the model is\n",
      "trained using sgd with lr 0.001 (for mil head), weight decay 10−6, and\n",
      "momentum 0.9 for 100 epochs with batch size 32.\n",
      "our method surpasses all latest sota wsod techniques by 9\n",
      "points, and establishes itself as a strong wsod baseline for gbc localization in us\n",
      "images.\n",
      "generality of the method: we assess the generality of our method by apply-\n",
      "ing it to polyp detection on colonoscopy images.\n",
      "the applicability of our method\n",
      "on two diﬀerent tasks - (1) gbc detection from us and (2) polyp detection from\n",
      "colonoscopy, indicates the generality of the method across modalities.\n",
      "gall bladder cancer detection from us images\n",
      "213\n",
      "ablation study: we show the detection sensitivity to the self-supervised\n",
      "instance learning module in table 2 for two variants, (1) vanilla mil head\n",
      "on detr, and (2) mil with self-supervised instance learning on detr.\n",
      "table 2\n",
      "shows the average precision and detection sensitivity for both diseases.\n",
      "other\n",
      "ablations related to the hyper-parameter sensitivity is given in supplementary\n",
      "fig.\n",
      "classiﬁcation performance: we compare our model with the standard cnn-\n",
      "based and transformer-based classiﬁers, sota wsod-based classiﬁers, and sota\n",
      "classiﬁers using additional data or annotations (table 3).\n",
      "performance comparison of our method and other sota methods in gbc\n",
      "classiﬁcation.\n",
      "0.861 ± 0.089\n",
      "table 4. comparison with sota wsod baselines in classifying polyps from colonoscopy\n",
      "images.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf:\n",
      "pretraining strategies, like contrastive learning (cl)\n",
      "methods, can leverage unlabeled or weakly-annotated datasets.\n",
      "these\n",
      "methods typically require large batch sizes, which poses a diﬃculty in the\n",
      "case of large 3d images at full resolution, due to limited gpu memory.\n",
      "we illustrate our method on cirrhosis prediction using a large volume of\n",
      "weakly-labeled images, namely radiological low-conﬁdence annotations,\n",
      "and small strongly-labeled (i.e., high-conﬁdence) datasets.\n",
      "it is however possible to obtain lower conﬁdence assessments for a large\n",
      "amount of images, either by a clinical questioning, or directly by a radiological\n",
      "diagnosis.\n",
      "to take advantage of large volumes of unlabeled or weakly-labeled\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0 22.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14220, pp.\n",
      "https://doi.org/10.1007/978-3-031-43907-0_22\n",
      "228\n",
      "e. sarfati et al.\n",
      "images, pre-training encoders with self-supervised methods showed promising\n",
      "results in deep learning for medical imaging [1,4,21,27–29].\n",
      "in particular, con-\n",
      "trastive learning (cl) is a self-supervised method that learns a mapping of\n",
      "the input images to a representation space where similar (positive) samples are\n",
      "moved closer and diﬀerent (negative) samples are pushed far apart.\n",
      "in this work, we focus on the scenario where radiological meta-data\n",
      "(thus, low-conﬁdence labels) are available for a large amount of images, whereas\n",
      "high-conﬁdence labels, obtained by histological analysis, are scarce.\n",
      "naive extensions of contrastive learning methods, such as [5,10,11], from 2d\n",
      "to 3d images may be diﬃcult due to limited gpu memory and therefore small\n",
      "batch size.\n",
      "how-\n",
      "ever, these methods pose two diﬃculties: they reduce the spatial context (lim-\n",
      "ited by the size of the patch), and they require similar spatial resolution across\n",
      "images.\n",
      "furthermore, we also propose to simultaneously leverage weak discrete attributes\n",
      "during pre-training by using a novel and eﬃcient contrastive learning composite\n",
      "kernel loss function, denoting our global method weakly-supervised positional\n",
      "(wsp).\n",
      "2\n",
      "method\n",
      "let xt be an input 2d image, usually called anchor, extracted from a 3d volume,\n",
      "yt a corresponding discrete weak variable and dt a related continuous variable.\n",
      "in this paper, yt refers to a weak radiological annotation and dt corresponds\n",
      "to the normalized depth position of the 2d image within its corresponding 3d\n",
      "volume: if vmax corresponds to the maximal depth-coordinate of a volume v ,\n",
      "we compute dt =\n",
      "pt\n",
      "vmax with pt ∈ [0, vmax] being the original depth coordinate.\n",
      "weakly-supervised positional contrastive learning\n",
      "229\n",
      "let x−\n",
      "j and x+\n",
      "i be two semantically diﬀerent (negative) and similar (positive)\n",
      "images with respect to xt, respectively.\n",
      "for instance, in unsupervised cl, methods such as simclr [5,\n",
      "6] choose as positive samples random augmentations of the anchor x+\n",
      "i = t(xt),\n",
      "where t ∼ t is a random transformation chosen among a user-selected family t .\n",
      "negative images x−\n",
      "j are all other (transformed) images present in the batch.\n",
      "once x−\n",
      "j and x+\n",
      "i are deﬁned, the goal of cl is to compute a mapping function\n",
      "fθ : x → sd, where x is the set of images and sd the representation space, so\n",
      "that similar samples are mapped closer in the representation space than dissimi-\n",
      "lar samples.\n",
      "[13], the authors deﬁne\n",
      "as positives all images with the same discrete label y. however, when working\n",
      "with continuous labels d, one cannot use the same strategy since all images are\n",
      "somehow positive and negative at the same time.\n",
      "[13].\n",
      "in this work, we propose to leverage both continuous d and discrete y labels,\n",
      "by combining (here by multiplying) the previously deﬁned kernels, wσ and wδ,\n",
      "into a composite kernel loss function.\n",
      "wσ(dt, di)\n",
      "\u0002\n",
      "\u0003\u0004\n",
      "\u0005\n",
      "composite kernel wti\n",
      "(stj − sti) ≤ 0\n",
      "∀t, i, j ̸= i\n",
      "(2)\n",
      "where the indices t, i, j traverse all n images in the batch since there are no\n",
      "“hard” positive or negative samples, as in simclr or supcon, but all images\n",
      "are considered as positive and negative at the same time.\n",
      "= {i : yi = yt} as the set of indices of images xi in the\n",
      "batch with the same discrete label yi as the anchor xt, we can rewrite our ﬁnal\n",
      "loss function as:\n",
      "lw sp = −\n",
      "n\n",
      "\u0006\n",
      "t=1\n",
      "\u0006\n",
      "i∈p (t)\n",
      "wσ(dt, di) log\n",
      "\t\n",
      "exp(sti)\n",
      "\n",
      "n\n",
      "j̸=i exp(stj)\n",
      "\u000b\n",
      "(4)\n",
      "where wσ(dt, di) is normalized over i ∈ p(t).\n",
      "a robustness study is available in the supplementary material.\n",
      "for\n",
      "the experiments, we ﬁx σ = 0.1.\n",
      "3\n",
      "experiments\n",
      "we compare the proposed method with diﬀerent contrastive and non-contrastive\n",
      "methods, that either use no meta-data (simclr [5], byol [10]), or leverage\n",
      "weakly-supervised positional contrastive learning\n",
      "231\n",
      "only discrete labels (supcon [13]), or continuous labels (depth-aware [8]).\n",
      "in all experiments, we work with 2d slices rather\n",
      "than 3d volumes due to the anisotropy of abdominal ct-scans in the depth\n",
      "direction and the limited spatial context or resolution obtained with 3d patch-\n",
      "based or downsampling methods, respectively, which strongly impacts the cir-\n",
      "rhosis diagnosis that is notably based on the contours irregularity.\n",
      "3.1\n",
      "datasets\n",
      "three datasets of abdominal ct images are used in this study.\n",
      "all images\n",
      "have a 512 × 512 size, and we clip the intensity values between -100 and 400.\n",
      "dradio.\n",
      "first, dradio contains 2,799 ct-scans of patients in portal venous\n",
      "phase with a radiological (weak) annotation, i.e. realized by a radiologist, indi-\n",
      "cating four diﬀerent stages of cirrhosis: no cirrhosis, mild cirrhosis, moderate\n",
      "cirrhosis and severe cirrhosis (yradio).\n",
      "in all datasets, we select the slices based on the liver segmentation of the\n",
      "patients.\n",
      "to gain in precision, we keep the top 70% most central slices with\n",
      "respect to liver segmentation maps obtained manually in dradio, and automati-\n",
      "cally for d1\n",
      "histo and d2\n",
      "histo using a u-net architecture pretrained on dradio\n",
      "[18].\n",
      "for the latter pretraining dataset, it presents an average slice spacing of 3.23 mm\n",
      "with a standard deviation of 1.29 mm.\n",
      "for the x and y axis, the dimension is\n",
      "0.79 mm per voxel on average, with a standard deviation of 0.10 mm.\n",
      "3.2\n",
      "architecture and optimization\n",
      "backbones.\n",
      "in comparison, resnet-18 has 11.2m parameters, a represen-\n",
      "tation space of dimension 512 and a latent space of dimension 128.\n",
      "more details\n",
      "and an illustration of tinynet are available in the supplementary material, as\n",
      "well as a full illustration of the algorithm ﬂow.\n",
      "data augmentation, sampling and optimization.\n",
      "[5,10,11]\n",
      "require strong data augmentations on input images, in order to strengthen the\n",
      "association between positive samples [22].\n",
      "in our work, we leverage three types\n",
      "of augmentations: rotations, crops and ﬂips.\n",
      "data augmentations are computed\n",
      "on the gpu, using the kornia library [17].\n",
      "during inference, we remove the aug-\n",
      "mentation module to only keep the original input images.\n",
      "as we work\n",
      "with 2d slices rather than 3d volumes, we compute the average probability per\n",
      "patient of having the pathology.\n",
      "finally, we run our experiments on a tesla v100 with 16gb of ram and\n",
      "a 6 cpu cores, and we used the pytorch-lightning library to implement our\n",
      "models.\n",
      "all models share the same data augmentation module, with a batch size\n",
      "of b = 64 and a ﬁxed number of epochs nepochs = 200.\n",
      "for all experiments, we\n",
      "ﬁx a learning rate (lr) of α = 10−4 and a weight decay of λ = 10−4.\n",
      "for byol, we\n",
      "initialize the moving average decay at 0.996.\n",
      "evaluation protocol.\n",
      "= we use the pretrained weights from\n",
      "imagenet with resnet-18 and run a logistic regression on the frozen representations.\n",
      "0.77 (±0.08)\n",
      "supcon\n",
      "✓\n",
      "✗\n",
      "0.76 (±0.09)\n",
      "0.93 (±0.07)\n",
      "0.72 (±0.06)\n",
      "depth-aware\n",
      "✗\n",
      "✓\n",
      "0.80 (±0.13)\n",
      "0.81 (±0.08)\n",
      "0.77 (±0.08)\n",
      "ours\n",
      "✓\n",
      "✓\n",
      "0.84 (±0.12)\n",
      "0.91 (±0.11)\n",
      "0.79 (±0.11)\n",
      "resnet-18\n",
      "supervised\n",
      "✗\n",
      "✗\n",
      "0.77 (±0.10)\n",
      "0.56 (±0.29)\n",
      "0.72 (±0.08)\n",
      "none (random)\n",
      "✗\n",
      "✗\n",
      "0.69 (±0.19)\n",
      "0.73 (±0.12)\n",
      "0.68 (±0.09)\n",
      "imagenet*\n",
      "✗\n",
      "✗\n",
      "0.72 (±0.17)\n",
      "0.76 (±0.04)\n",
      "0.66 (±0.10)\n",
      "simclr\n",
      "✗\n",
      "✗\n",
      "0.79 (±0.09)\n",
      "0.82 (±0.14)\n",
      "0.79 (±0.08)\n",
      "byol\n",
      "✗\n",
      "✗\n",
      "0.78 (±0.09)\n",
      "0.77 (±0.11)\n",
      "0.78 (±0.08)\n",
      "supcon\n",
      "✓\n",
      "✗\n",
      "0.69 (±0.07)\n",
      "0.69 (±0.13)\n",
      "0.76 (±0.12)\n",
      "depth-aware\n",
      "✗\n",
      "✓\n",
      "0.83 (±0.07)\n",
      "0.82 (±0.11)\n",
      "0.80 (±0.07)\n",
      "ours\n",
      "✓\n",
      "✓\n",
      "0.84 (±0.07)\n",
      "0.85 (±0.10)\n",
      "0.84 (±0.07)\n",
      "fig.\n",
      "blue = healthy\n",
      "subjects.\n",
      "4\n",
      "results and discussion\n",
      "we present in table 1 the results of all our experiments.\n",
      "supcon per-\n",
      "forms well on the training set of dradio (ﬁgure available in the supplementary\n",
      "material), as well as d2\n",
      "histo with tinynet, but it poorly generalizes to d1\n",
      "histo\n",
      "and d1+2\n",
      "histo.\n",
      "the method depth-aware manages to correctly encode the depth\n",
      "position but not the diagnostic class label.\n",
      "to assess the clinical performance of the pretraining methods, we also com-\n",
      "pute the balanced accuracy scores (bacc) of the trained classiﬁers, which is\n",
      "compared in table 2 to the bacc achieved by radiologists who were asked to\n",
      "visually assess the presence or absence of cirrhosis for the n=106 cases of d1\n",
      "histo.\n",
      "pretraining method bacc models bacc radiologists\n",
      "supervised\n",
      "0.78 (±0.04)\n",
      "none (random)\n",
      "0.71 (±0.13)\n",
      "imagenet\n",
      "0.74 (±0.13)\n",
      "simclr\n",
      "0.78 (±0.08)\n",
      "byol\n",
      "0.77 (±0.04)\n",
      "0.82\n",
      "supcon\n",
      "0.77 (±0.10)\n",
      "depth-aware\n",
      "0.84 (±0.04)\n",
      "ours\n",
      "0.85 (±0.09)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf:\n",
      "improving the feature representation ability is the founda-\n",
      "tion of many whole slide pathological image (wsis) tasks.\n",
      "aiming towards\n",
      "slide-level representations, we propose slide-level prototypical distil-\n",
      "lation (slpd) to explore intra- and inter-slide semantic structures for\n",
      "context modeling on wsis.\n",
      "speciﬁcally, we iteratively perform intra-slide\n",
      "clustering for the regions (4096 × 4096 patches) within each wsi to yield\n",
      "the prototypes and encourage the region representations to be closer\n",
      "to the assigned prototypes.\n",
      "slpd\n",
      "achieves state-of-the-art results on multiple slide-level benchmarks and\n",
      "demonstrates that representation learning of semantic structures of slides\n",
      "can make a suitable proxy task for wsi analysis.\n",
      "keywords: computational pathology · whole slide images(wsis) ·\n",
      "self-supervised learning\n",
      "1\n",
      "introduction\n",
      "in computational histopathology, visual representation extraction is a fundamen-\n",
      "tal problem [14], serving as a cornerstone of the (downstream) task-speciﬁc learn-\n",
      "ing on whole slide pathological images (wsis).\n",
      "our community has witnessed\n",
      "the progress of the de facto representation learning paradigm from the super-\n",
      "vised imagenet pre-training to self-supervised learning (ssl)\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0_25.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "[12,22,27] directly apply the ssl\n",
      "algorithms developed for natural images (e.g., simclr\n",
      "[11]) to wsi analysis tasks, and the improved performance proves the\n",
      "eﬀectiveness of ssl.\n",
      "however, wsi is quite diﬀerent from natural images in that\n",
      "it exhibits a hierarchical structure with giga-pixel resolution.\n",
      "since the pretext tasks encourage to mine the patho-\n",
      "logically relevant patterns, the learned representations are expected to be more\n",
      "suitable for wsi analysis.\n",
      "[8], a milestone work,\n",
      "introduces hierarchical pre-training (dino [6]) for the patch-level (256 × 256)\n",
      "and region-level (4096 × 4096) in a two-stage manner, achieving superior per-\n",
      "formance on slide-level tasks.\n",
      "[13] uses eﬃcientnet-b0 for image\n",
      "compression in the ﬁrst stage and then derives multi-task learning on the com-\n",
      "pressed wsis, which assumes the primary site information, e.g., the organ type,\n",
      "is always available and can be used as pseudo labels.\n",
      "[35] also pro-\n",
      "poses a two-stage pre-training framework for wsis using contrastive learning\n",
      "(simclr [10]), where the diﬀerently subsampled bags1 from the same wsi are\n",
      "positive pairs in the second stage.\n",
      "a similar idea can be found in giga-ssl [20]\n",
      "with delicate patch- and wsi-level augmentations.\n",
      "the aforementioned meth-\n",
      "ods share the same two-stage pre-training paradigm, i.e., patch-to-region/slide.\n",
      "however, they are essentially instance discrimination\n",
      "where only the self-invariance of region/slide is considered, leaving the intra-\n",
      "and inter-slide semantic structures unexplored.\n",
      "in this paper, we propose to encode the intra- and inter-slide semantic struc-\n",
      "tures by modeling the mutual-region/slide relations, which is called slpd: slide-\n",
      "level prototypical distillation for wsis.\n",
      "in order to learn this intra-slide semantic structure, we\n",
      "encourage the region representations to be closer to the assigned prototypes.\n",
      "by\n",
      "representing each slide with its prototypes, we further select semantically simi-\n",
      "1 by formulating wsi tasks as a multi-instance learning problem, the wsi is treated\n",
      "as a bag with corresponding patches as instances.\n",
      "slpd\n",
      "261\n",
      "(e) intra-slide dis\u0002lla\u0002on \n",
      "(f) inter-slide dis\u0002lla\u0002on \n",
      "(d) global (le\u0003) vs. slide-level clustering (right)\n",
      "(a) hierarchical structure of wsi\n",
      "(b) two-stage pretraining\n",
      "probability \n",
      "distribu\u0002on \n",
      "prototype \n",
      "within slide \n",
      "prototypes \n",
      "across slides \n",
      "(c) slide-level prototypical dis\u0002lla\u0002on \n",
      "wsi\n",
      "region\n",
      "patch\n",
      "image\n",
      "fig.\n",
      "1. (a) a wsi possesses the hierarchical structure of wsi-region-patch-image, from\n",
      "coarse to ﬁne.\n",
      "(b) two-stage pre-training paradigm successively performs the image-to-\n",
      "patch and patch-to-region aggregations.\n",
      "slpd explores the\n",
      "semantic structure by slide-level clustering.\n",
      "then, we learn the inter-slide\n",
      "semantic structure by building correspondences between region representations\n",
      "and cross-slide prototypes.\n",
      "we conduct experiments on two benchmarks, nsclc\n",
      "subtyping and brca subtyping.\n",
      "slpd achieves state-of-the-art results on mul-\n",
      "tiple slide-level tasks, demonstrating that representation learning of semantic\n",
      "structures of slides can make a suitable proxy task for wsi analysis.\n",
      "1(a), a wsi exhibits hierarchical structure at varying resolu-\n",
      "tions under 20× magniﬁcation: 1) the 4096×4096 regions describing macro-scale\n",
      "organizations of cells, 2) the 256 × 256 patches capturing local clusters of cells,\n",
      "3) and the 16 × 16 images characterizing the ﬁne-grained features at the cell-\n",
      "level.\n",
      "slpd\n",
      "262\n",
      "z. yu et al.\n",
      "is built upon the two-stage pre-training paradigm proposed by hipt, which\n",
      "will be described in sect.\n",
      "we\n",
      "characterize the semantic structure of slides in sect.\n",
      "2.2, which is leveraged to\n",
      "establish the relationship within and across slides, leading to the proposed intra-\n",
      "and inter-slide distillation in sect.\n",
      "2.5.\n",
      "2.2\n",
      "preliminaries\n",
      "we revisit hierarchical image pyramid transformer (hipt)\n",
      ", hipt proposes a two-stage pre-training paradigm\n",
      "considering the hierarchical structure of wsis.\n",
      "in stage one, a patch-level vision\n",
      "transformer, denoted as vit256-16, aggregates non-overlapping 16 × 16 images\n",
      "within 256 × 256 patches to form patch-level representations.\n",
      "in stage two, the\n",
      "pre-trained vit256-16 is freezed and leveraged to tokenize the patches within\n",
      "4096 × 4096 regions.\n",
      "hipt leverages dino\n",
      "taking stage two as an\n",
      "example, dino distills the knowledge from teacher to student by minimizing the\n",
      "cross-entropy between the probability distributions of two views at region-level:\n",
      "lself = ex∼pdh(gt(ˆz), gs(z)),\n",
      "(1)\n",
      "where h(a, b) = −a log b, and pd is the data distribution that all regions are\n",
      "drawn from.\n",
      "the parameters of the student\n",
      "are exponentially moving averaged to the parameters of the teacher.\n",
      "2.3\n",
      "slide-level clustering\n",
      "many histopathologic features have been established based on the morpho-\n",
      "logic phenotypes of the tumor, such as tumor invasion, anaplasia, necrosis and\n",
      "mitoses, which are then used for cancer diagnosis, prognosis and the estimation\n",
      "of response-to-treatment in patients [3,9]. to obtain meaningful representations\n",
      "of slides, we aim to explore and maintain such histopathologic features in the\n",
      "latent space.\n",
      "however, the obtained\n",
      "clustering centers, i.e., the prototypes, are inclined to represent the visual bias\n",
      "slpd\n",
      "263\n",
      "related to staining or scanning procedure rather than medically relevant fea-\n",
      "tures [33].\n",
      "each group of proto-\n",
      "types is expected to encode the semantic structure (e.g., the combination of\n",
      "histopathologic features) of the wsi.\n",
      "2.4\n",
      "intra-slide distillation\n",
      "the self-distillation utilized by hipt in stage two encourages the correspondence\n",
      "between two views of a region at the macro-scale because the organizations of\n",
      "cells share mutual information spatially.\n",
      "2.3, a\n",
      "slide can be abstracted by a group of prototypes, which capture the semantic\n",
      "structure of the wsi.\n",
      ", we assume that the representation\n",
      "z and its assigned prototype c also share mutual information and encourage z\n",
      "to be closer to c with the intra-slide distillation:\n",
      "lintra = ex∼pdh (gt(c), gs(z)) ,\n",
      "(2)\n",
      "we omit super-/sub-scripts of z for brevity.\n",
      "through eq. 2, we can leverage more\n",
      "intra-slide correspondences to guide the learning process.\n",
      "for further understand-\n",
      "ing, a prototype can be viewed as an augmented representation aggregating the\n",
      "slide-level information.\n",
      "thus this distillation objective is encoding such informa-\n",
      "tion into the corresponding region embedding, which makes the learning process\n",
      "semantic structure-aware at the slide-level.\n",
      "previous self-supervised learning methods applied to histopatho-\n",
      "logic images only capture such correspondences with positive pairs at the patch-\n",
      "level [22,23], which overlooks the semantic structure of the wsi.\n",
      "due to the heterogeneity of the slides, comparing them with the local\n",
      "crops or the averaged global features are both susceptible to being one-sided.\n",
      "to\n",
      "264\n",
      "z. yu et al.\n",
      "address this, we bridge the slides with their semantic structures and deﬁne the\n",
      "semantic similarity between two slides wi and wj through an optimal bipartite\n",
      "matching between two sets of prototypes:\n",
      "d(wi, wj) = max{ 1\n",
      "m\n",
      "m\n",
      "\u0002\n",
      "m=1\n",
      "cos(cm\n",
      "i , cσ(m)\n",
      "[−1, 1],\n",
      "(3)\n",
      "where cos(·, ·) measures the cosine similarity between two vectors, and sm enu-\n",
      "merates the permutations of m elements.\n",
      "speciﬁcally, for a region embedding z belonging to the slide w and\n",
      "assigned to the prototype c, we ﬁrst search the top-k nearest neighbors of w in\n",
      "the dataset based on the semantic similarity, denoted as { ˆwk}k\n",
      "k=1.\n",
      "finally, we encourage z to be\n",
      "closer to ˆck with the inter-slide distillation:\n",
      "(4)\n",
      "the inter-slide distillation can encode the sldie-level information complementary\n",
      "to that of intra-slide distillation into the region embeddings.\n",
      "we believe the performance\n",
      "can be further improved by tuning this.\n",
      "3\n",
      "experimental results\n",
      "datasets.\n",
      "we conduct experiments on two public wsi datasets2.\n",
      "we extract 62,852 and 60,153 regions at 20× magniﬁcation from\n",
      "tcga-nsclc and tcga-brca for pre-training vit4096-256 in stage two.\n",
      "we\n",
      "leverage the pre-trained vit256-16 in stage one provided by hipt to tokenize\n",
      "the patches within each region.\n",
      "we adopt the 10-fold cross validated accuracy (acc.)\n",
      "and area under the curve (auc) to evaluate the weakly-supervised classiﬁcation\n",
      "performance.\n",
      "“mean” leverages the averaged pre-extracted embed-\n",
      "dings to evaluate knn performance.\n",
      "bold and underlined numbers highlight the best\n",
      "and second best performance\n",
      "#\n",
      "feature\n",
      "aggragtor\n",
      "feature\n",
      "extraction\n",
      "pretrain\n",
      "method\n",
      "nsclc\n",
      "brca\n",
      "acc.\n",
      "auc\n",
      "acc.\n",
      "auc\n",
      "weakly supervised classiﬁcation\n",
      "1\n",
      "patch-level\n",
      "dino\n",
      "0.780±0.126\n",
      "0.864±0.089\n",
      "0.822±0.047\n",
      "0.783±0.056\n",
      "2\n",
      "mil\n",
      "[22]\n",
      "region-level\n",
      "dino\n",
      "0.841±0.036\n",
      "0.917±0.035\n",
      "0.854±0.032\n",
      "0.848±0.075\n",
      "5\n",
      "region-level\n",
      "slpd\n",
      "0.858±0.040\n",
      "0.938±0.026\n",
      "0.854±0.039\n",
      "0.876±0.050\n",
      "6\n",
      "region-level\n",
      "dino\n",
      "0.843±0.044\n",
      "0.926±0.032\n",
      "0.849±0.037\n",
      "0.854±0.069\n",
      "7\n",
      "region-level\n",
      "dino+lintra\n",
      "0.850±0.042\n",
      "0.931±0.041\n",
      "0.866±0.030\n",
      "0.881±0.069\n",
      "8\n",
      "vitwsi-4096 [8]\n",
      "region-level\n",
      "dino+linter\n",
      "0.850±0.043\n",
      "0.938±0.028\n",
      "0.860±0.030\n",
      "0.874±0.059\n",
      "9\n",
      "region-level\n",
      "slpd\n",
      "0.864±0.042\n",
      "0.939±0.022\n",
      "0.869±0.039\n",
      "0.886±0.057\n",
      "k-nearest neighbors (knn) evaluation\n",
      "10\n",
      "mean\n",
      "region-level\n",
      "dino\n",
      "0.770±0.031\n",
      "0.840±0.038\n",
      "0.837±0.014\n",
      "0.724±0.055\n",
      "11\n",
      "region-level\n",
      "dino+lintra\n",
      "0.776±0.039\n",
      "0.850±0.023\n",
      "0.841±0.012\n",
      "0.731±0.064\n",
      "12\n",
      "region-level\n",
      "dino+linter\n",
      "0.782±0.027\n",
      "0.854±0.025\n",
      "0.845±0.014\n",
      "0.738±0.080\n",
      "13\n",
      "region-level\n",
      "slpd\n",
      "0.792±0.035\n",
      "0.863±0.024\n",
      "0.849±0.014\n",
      "0.751±0.079\n",
      "3.1\n",
      "weakly-supervised classiﬁcation\n",
      "we conduct experiments on two slide-level classiﬁcation tasks, nsclc subtyp-\n",
      "ing and brca subtyping, and report the results in table 1.\n",
      "this illustrates that learning rep-\n",
      "resentations with broader image contexts is more suitable for wsi analysis.\n",
      "compared with the strong baseline, i.e., the two-stage pre-training method pro-\n",
      "posed by hipt (#6), slpd achieves performance increases of 1.3% and 3.2%\n",
      "auc on nsclc and brca (#9).\n",
      "nontrivial performance improvements are\n",
      "also observed under knn evaluation (#10 vs.#13): +2.3% and +3.1% auc\n",
      "on nsclc and brca.\n",
      "the superior performance of slpd demonstrates that\n",
      "learning representations with slide-level semantic structure appropriately can\n",
      "signiﬁcantly narrow the gap between pre-training and downstream slide-level\n",
      "3 the feature extraction of the patch-level is impracticable for the vit-based model\n",
      "due to its quadratic complexity in memory usage.\n",
      "moreover, intra-slide and inter-slide distillation show consistent perfor-\n",
      "mance over the baseline, corroborating the eﬀectiveness of these critical compo-\n",
      "nents of slpd.\n",
      "3.2\n",
      "ablation study\n",
      "diﬀerent clustering methods.\n",
      "the inferior performance of the\n",
      "global clustering is due to the visual bias underlying the whole dataset.\n",
      "the proposed inter-slide distillation is\n",
      "semantic structure-aware at the slide-level, since we build the correspondence\n",
      "between the region embedding and the matched prototype (#4 in table 2).\n",
      "as can be seen, the region-level correspondences\n",
      "lead to inferior performances, even worse than the baseline (#5 in table 1),\n",
      "because the learning process is not guided by the slide-level information.\n",
      "as shown in table 2(#5∼7), the performance of\n",
      "slpd is relatively robust to the number of prototypes on nsclc, but is some-\n",
      "what aﬀected by it on brca.\n",
      "as demonstrated in table 2(#5∼7), the per-\n",
      "formance of slpd is robust to the number of slide neighbors.\n",
      "for more results, please refer to the supplementary.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf:\n",
      "the adoption of multi-instance learning (mil) for classi-\n",
      "fying whole-slide images (wsis) has increased in recent years.\n",
      "our work aims to unleash the\n",
      "full potential of pyramidal structured wsi; to do so, we propose a graph-\n",
      "based multi-scale mil approach, termed das-mil, that exploits mes-\n",
      "sage passing to let information ﬂows across multiple scales.\n",
      "by means of\n",
      "a knowledge distillation schema, the alignment between the latent space\n",
      "representation at diﬀerent resolutions is encouraged while preserving the\n",
      "diversity in the informative content.\n",
      "the source code is\n",
      "available at https://github.com/aimagelab/mil4wsi.\n",
      "keywords: whole-slide images · multi-instance learning ·\n",
      "knowledge distillation\n",
      "1\n",
      "introduction\n",
      "modern microscopes allow the digitalization of conventional glass slides into\n",
      "gigapixel whole-slide images (wsis)\n",
      "[18], facilitating their preservation and\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43907-0 24.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14220, pp.\n",
      "furthermore, a knowledge distillation mechanism encourages the\n",
      "agreement between the predictions delivered by diﬀerent scales.\n",
      "retrieval, but also introducing multiple challenges.\n",
      "on the other hand,\n",
      "feeding modern neural networks with the entire gigapixel image is not a feasible\n",
      "approach, forcing to crop data into small patches and use them for training.\n",
      "this\n",
      "process is usually performed considering a single resolution/scale among those\n",
      "provided by the wsi image.\n",
      "mil approaches consider the image slide as a bag composed of many\n",
      "patches, called instances; afterwards, to provide a classiﬁcation score for the\n",
      "entire bag, they weigh the instances through attention mechanisms and aggregate\n",
      "them into a single representation.\n",
      "[15],\n",
      "which have been proven to be more eﬀective than single-resolution [4,13,15,19].\n",
      "however, to the best of our knowledge, none of the existing proposals leverage the\n",
      "full potential of the wsi pyramidal structure.\n",
      "a proﬁcient learning app-\n",
      "roach should instead consider the heterogeneity between global structures and\n",
      "local cellular regions, thus allowing the information to ﬂow eﬀectively across the\n",
      "image scales.\n",
      "through knowledge distillation, we encour-\n",
      "250\n",
      "g. bontempo et al.\n",
      "age agreement across the predictions delivered at diﬀerent resolutions, while indi-\n",
      "vidual scale features are learned in isolation to preserve the diversity in terms\n",
      "of information content.\n",
      "the authors of [7] leverage dino [5] as feature extrac-\n",
      "tor, highlighting its eﬀectiveness for medical image analysis.\n",
      "typically, a tailored learning objective encourages the stu-\n",
      "dent to mimic the behaviour of its teacher.\n",
      "recently, self-supervised representa-\n",
      "tion learning approaches have also employed such a schema: as an example, [5,9]\n",
      "exploit kd to obtain an agreement between networks fed with diﬀerent views\n",
      "of the same image.\n",
      "while existing works [19,20,25] take into account inter-\n",
      "scales interactions by mostly leveraging trivial operations (such as concatenation\n",
      "of related feature representations), we instead provide a novel technique that\n",
      "builds upon: i) a gnn module based on message passing, which propagates\n",
      "patches’ representation according to the natural structure of multi-resolutions\n",
      "wsi; ii) a regulation term based on (self) knowledge distillation, which pins the\n",
      "most eﬀective resolution to further guide the training of the other one(s).\n",
      "we hence devise an initial\n",
      "stage with multiple self-supervised feature extractors f(·; θ1), . .\n",
      "to\n",
      "perform message passing, we adopt graph attention layers (gat)\n",
      "however, as these learned\n",
      "metrics are inferred from diﬀerent wsi zooms, a disagreement may emerge:\n",
      "indeed, we have observed (see table 4) that the higher resolutions generally yield\n",
      "better classiﬁcation performance.\n",
      "further than improving\n",
      "the results of the lowest scale only, we expect its beneﬁts to propagate also to\n",
      "the shared message-passing module, and so to the higher resolution.\n",
      "it encourages the two\n",
      "resolutions to assign criticality scores in a consistent manner: intuitively, if a low-\n",
      "resolution patch has been considered critical, then the average score attributed\n",
      "to its children patches should be likewise high.\n",
      "we encourage such a constraint\n",
      "by minimizing the euclidean distance between the low-resolution criticality grid\n",
      "map z1 and its subsampled counterpart computed by the high-resolution branch:\n",
      "lcrit = ∥z1 − graphpooling(z2)∥2\n",
      "2.\n",
      "(2)\n",
      "in the equation above, graphpooling identiﬁes a pooling layer applied over the\n",
      "higher scale: to do so, it considers the relation “part of” between scales and then\n",
      "averages the child nodes, hence allowing the comparison at the instance level.\n",
      "das-mil: distilling across scales for mil classiﬁcation of wsis\n",
      "253\n",
      "4\n",
      "experiments\n",
      "wsis pre-processing.\n",
      "we remove background patches through an approach\n",
      "similar to the one presented in the clam framework [20]: after an initial seg-\n",
      "mentation process based on otsu [22] and connected component analysis [2],\n",
      "non-overlapped patches within the foreground regions are considered.\n",
      "optimization.\n",
      "the dino feature extractor has been trained with two rtx5000\n",
      "gpus: diﬀerently, all subsequent experiments have been performed with a single\n",
      "rtx2080 gpu using pytorch-geometric\n",
      "to asses the performance of our\n",
      "approach, we adhere to the protocol of [19,28] and use the accuracy and auc\n",
      "metrics.\n",
      "as can be observed: i) the joint\n",
      "exploitation of multiple resolutions is generally more eﬃcient; ii) our das-mil\n",
      "yields robust and compelling results, especially on camelyon16, where it provides\n",
      "0.945 of accuracy and 0.973 auc (i.e., an improvement of +3.3% accuracy and\n",
      "+1.9% auc with respect to the sota).\n",
      "to assess its merits, we con-\n",
      "ducted several experiments varying the values of the corresponding balancing\n",
      "coeﬃcients (see table 2).\n",
      "= 0, i.e., no distillation is performed) negatively aﬀects the performance.\n",
      "such a statement holds not only for the lower resolution (as one could expect),\n",
      "but also for the higher one, thus corroborating the claims we made in sect.\n",
      "0.816\n",
      "20×\n",
      "20×\n",
      "0.891\n",
      "0.931\n",
      "5×, 20×\n",
      "5×, 20×\n",
      "0.891\n",
      "0.938\n",
      "5×, 20×\n",
      "5×, [5× ∥ 20×]\n",
      "0.898\n",
      "0.941\n",
      "10×, 20×\n",
      "10×, 20×\n",
      "0.945\n",
      "0.973\n",
      "10×, 20×\n",
      "10×, [10× ∥ 20×] 0.922\n",
      "0.953\n",
      "we have also performed an\n",
      "assessment on the temperature\n",
      "τ, which controls the smooth-\n",
      "ing factor applied to teacher’s\n",
      "predictions (table 3).\n",
      "for single-\n",
      "scale experiments, the model is fed only with patches extracted at a single ref-\n",
      "erence scale.\n",
      "we ascribe it to the specimen-\n",
      "level pixel size relevant for cancer diagnosis task; diﬀerent datasets/tasks may\n",
      "beneﬁt from diﬀerent scale combinations.\n",
      "in doing so, we\n",
      "ﬁx the input resolutions to 5× and 20×. we draw the following conclusions: i)\n",
      "when our das-mil feature propagation layer is used, the selection of the optimal\n",
      "feature extractor (i.e., simclr vs dino) has less impact on performance, as the\n",
      "message-passing can compensate for possible lacks in the initial representation;\n",
      "ii) das-mil appears a better features propagator w.r.t. h2-mil.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf:\n",
      "medical education is essential for providing the best patient\n",
      "care in medicine, but creating educational materials using real-world\n",
      "data poses many challenges.\n",
      "for example, the diagnosis and treatment\n",
      "of a disease can be aﬀected by small but signiﬁcant diﬀerences in med-\n",
      "ical images; however, collecting images to highlight such diﬀerences is\n",
      "often costly.\n",
      "therefore, medical image editing, which allows users to\n",
      "create their intended disease characteristics, can be useful for educa-\n",
      "tion.\n",
      "however, existing image-editing methods typically require manu-\n",
      "ally annotated labels, which are labor-intensive and often challenging to\n",
      "represent ﬁne-grained anatomical elements precisely.\n",
      "herein, we present\n",
      "a novel algorithm for editing anatomical elements using segmentation\n",
      "labels acquired through self-supervised learning.\n",
      "our self-supervised seg-\n",
      "mentation achieves pixel-wise clustering under the constraint of invari-\n",
      "ance to photometric and geometric transformations, which are assumed\n",
      "not to change the clinical interpretation of anatomical elements.\n",
      "the user\n",
      "then edits the segmentation map to produce a medical image with the\n",
      "intended detailed ﬁndings.\n",
      "evaluation by ﬁve expert physicians demon-\n",
      "strated that the edited images appeared natural as medical images and\n",
      "that the disease characteristics were accurately reproduced.\n",
      "keywords: image editing · self-supervised segmentation · education\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43895-0_38.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14221, pp.\n",
      "1. editing of anatomical elements.\n",
      "(a) users can edit the segmentation map\n",
      "obtained from an input image to express intended ﬁne-grained disease characteristics.\n",
      "(c) a synthetic rectal\n",
      "tumor (c-1) and the contrasting tumor with t2 hyperintensity of extracellular mucin\n",
      "suspicious for mucinous adenocarcinoma (c-2).\n",
      "for example, although small but signif-\n",
      "icant disease characteristics (e.g., depth of cancer invasion) can sometimes alter\n",
      "diagnosis and treatment, collecting pairs with and without these characteristics\n",
      "is cumbersome.\n",
      "another major challenge is longitudinal tracking of pathologi-\n",
      "cal progression over time (e.g., from the early stage of cancer to the advanced\n",
      "stage), which is diﬃcult to understand because medical images are often snap-\n",
      "shots.\n",
      "privacy is also a concern since images of educational materials are widely\n",
      "distributed.\n",
      "therefore, medical image editing that allows users to generate their\n",
      "intended disease characteristics is useful for precise medical education\n",
      "[3].\n",
      "image editing can synthesize low- or high-level image contents\n",
      "our goal\n",
      "is to develop high-precision medical image editing according to the ﬁne-grained\n",
      "characteristics of individual diseases, rather than at the level of disease cate-\n",
      "gories.\n",
      "these ﬁne-grained characteristics consist of low- to mid-level image\n",
      "medical image editing\n",
      "405\n",
      "features to distinguish the substructures of organs and diseases, which we call\n",
      "anatomical elements.\n",
      "several types of image editing techniques for medical imaging have been\n",
      "introduced, mainly using generative adversarial networks [5] and, more recently,\n",
      "diﬀusion models [2].\n",
      "nevertheless, editing speciﬁc anatomical elements remains\n",
      "a challenge [1,11].\n",
      "latent space manipulation generates images by controlling\n",
      "latent feature axes [4,14], but the editable attributes are often global rather than\n",
      "ﬁne-grained.\n",
      "conditional generation can precisely edit image content by using\n",
      "class or segmentation labels.\n",
      "however, it requires manually provided labels\n",
      "image interpolation\n",
      "[17] requires actual images with targeted\n",
      "content, which limits its applicability.\n",
      "here, we propose a novel framework for image editing called u3-net that\n",
      "allows the generation of anatomical elements with precise conditions.\n",
      "the core\n",
      "technique is self-supervised segmentation, which aims to achieve pixel-wise clus-\n",
      "tering without manually annotated labels [6,7].\n",
      "1a, u3-net\n",
      "converts an input image into a segmentation map corresponding to the anatomi-\n",
      "cal elements.\n",
      "once the user has completed editing, u3-net synthesizes an image\n",
      "in which the targeted anatomical element has been modiﬁed.\n",
      "as a result, our\n",
      "synthesized medical images can highlight hypothetical pathological changes and\n",
      "signiﬁcant clinical diﬀerences in a single image.\n",
      "1b shows\n",
      "that whether or not rectal cancer invades the muscularis propria (i.e., b-2 vs.\n",
      "b-3) aﬀects cancer staging (i.e., t1 vs. t2) as well as treatment strategy (i.e.,\n",
      "endoscopic resection vs. surgery).\n",
      "these synthetic images can help trainees intu-\n",
      "itively comprehend clinically signiﬁcant ﬁndings and alleviate privacy concerns.\n",
      "five expert physicians evaluated the edited images from a clinical perspective\n",
      "using two datasets: a pelvic mri dataset and chest ct dataset.\n",
      "contributions: our contributions are as follows:\n",
      "– we propose a novel image-editing algorithm, u3-net, to synthesize images\n",
      "for medical education via self-supervised segmentation.\n",
      "– u3-net can faithfully synthesize intended anatomical elements according to\n",
      "the editing operation on the segmentation labels.\n",
      "– evaluation by ﬁve expert physicians showed that the edited images were\n",
      "natural as medical images with the intended features.\n",
      "the encoder achieves self-supervised segmentation with a fea-\n",
      "ture extraction (fe) module and a pixel-wise clustering (cl) module.\n",
      "we apply two random transformations to\n",
      "the input image to produce images in diﬀerent views, i1 and i2.\n",
      "the encoder converts\n",
      "the transformed images into quantized embedding as well as segmentation maps con-\n",
      "sisting of cluster indices, s1 and s2.\n",
      "pixel-wise clustering, which should be consistent\n",
      "between views, is performed for the self-supervised segmentation.\n",
      "the decoder gen-\n",
      "erates reconstructed images, r1 and r2, from the quantized embedding maps.\n",
      "the\n",
      "discriminator adversarially enhances the natural appearance by judging whether the\n",
      "images are real or fake on a pixel-by-pixel basis.\n",
      "geometric transformations [6], with the assumption that these transformations\n",
      "should not change the clinical interpretation of the anatomical elements.\n",
      "given\n",
      "a pair of diﬀerently transformed images, the fe module produces embedding\n",
      "maps corresponding to the input images.\n",
      "the cl module then performs k-\n",
      "means clustering on the embedding maps to produce two interchangeable out-\n",
      "puts: segmentation maps and corresponding quantized embedding maps.\n",
      "the decoder then\n",
      "estimates the corresponding images from the quantized embedding maps, while\n",
      "the discriminator forces the decoder to produce more realistic images.\n",
      "2.1\n",
      "first training stage for self-supervised segmentation\n",
      "the training process for u3-net is two-stage.\n",
      "first, we train the encoder and\n",
      "decoder (excluding the discriminator) to conduct k-class self-supervised seg-\n",
      "mentation.\n",
      "to achieve pixel-wise clustering that is consistent between two trans-\n",
      "formed views of the input images, we introduce four constraints:\n",
      "random image transformation: we consider a sequence of image transfor-\n",
      "mations\n",
      ", tn] speciﬁed by the type (e.g., image rotation) and magnitude\n",
      "(e.g., degree of rotation) of each transformation: t = tn ◦\n",
      "two\n",
      "random transformation sequences are applied to an input image i ∈ rc×h×w\n",
      "to produce two transformed images, t1(i) = i1 and t2(i) = i2.\n",
      "medical image editing\n",
      "407\n",
      "fig.\n",
      "cluster assignment and update: in the cl module, k-means clustering\n",
      "in the ﬁrst iteration initializes k mean vectors µk ∈ rd.\n",
      "the cluster indices form the segmentation maps\n",
      "s =\n",
      "the mean vectors µk are updated by\n",
      "using the exponential moving average [9].\n",
      "cross-view consistency: the segmentation maps from the diﬀerent views,\n",
      "s1 and s2, should overlap after re-transforming to align the coordinates.\n",
      "using the re-transformed seg-\n",
      "mentation maps, we impose a third term, cross-view consistency loss, which\n",
      "forces the embedding vectors of one view to match the mean vector of the other\n",
      "(see fig. 3), as deﬁned: lcross = \u0002\n",
      "i∈h×w ∥µyi2 −ei1∥2+\u0002\n",
      "i∈h×w ∥µyi1 −ei2∥2.\n",
      "reconstruction loss: without user editing, the decoder reconstructs the\n",
      "input images from quantized embedding maps h(eq)\n",
      "we thus\n",
      "employ reconstruction loss, which minimizes the mean squared error between\n",
      "the reconstructed and input images.\n",
      "learning objective: the weighted sum of the loss functions is set to be\n",
      "minimized: ltotal = wclusterlcluster + wdistldist + wcrosslcross + wreconlrecon.\n",
      "2.2\n",
      "second training stage for faithful image synthesis\n",
      "in the second stage, we train the decoder and discriminator (excluding the\n",
      "encoder) to produce naturally appearing images from the quantized embedding\n",
      "maps.\n",
      "the decoder, initially optimized in the ﬁrst training stage, undergoes fur-\n",
      "ther training to enhance its image generation capabilities.\n",
      "lapp = wmselmse + wﬄlﬄ + wlpipsllpips + wintlint, where intermediate\n",
      "loss lint refers to the l2 distance of the intermediate features of the discriminator\n",
      "between the reconstructed and input images.\n",
      "learning objective: we impose generator loss lgen for the decoder to pro-\n",
      "duce more faithful images by deceiving the discriminator, and discriminator loss\n",
      "ldis for the discriminator to judge the real or fake of the images as the per-\n",
      "pixel feedback\n",
      "we also add cutmix augmentation lcutmix and consistency\n",
      "regularization lcons to the latter [16].\n",
      "in this stage, the decoder and discrimi-\n",
      "nator are trained by alternately minimizing the following competing objectives:\n",
      "ldec = lapp + wgenlgen and ldis = wdisldis + wcutmixlcutmix + wconslcons.\n",
      "2.3\n",
      "inference stage for medical image editing\n",
      "after training, the encoder can output a segmentation map from a testing image.\n",
      "1a, when a user edits the segmentation map s → s′ by\n",
      "changing the cluster indices yi → y′\n",
      "i, the quantized embedding map is sub-\n",
      "sequently updated eq → e′\n",
      "q by reassigning the mean vectors according to\n",
      "the edited indices µyi → µy′\n",
      "i. finally, the decoder converts the quantized\n",
      "embedding map into a synthetic image with the intended disease characteris-\n",
      "tics h(e′\n",
      "q) = r ∈ rc×h×w .\n",
      "medical image editing\n",
      "409\n",
      "3\n",
      "experiments and results\n",
      "implementation and datasets: all neural networks were implemented in\n",
      "python 3.8 using the pytorch library 1.10.0\n",
      "the encoder, decoder, and discriminator were imple-\n",
      "mented based on u-net\n",
      "[13] (see supplementary information for details).\n",
      "the pelvic mri dataset with rectal cancer contained 289 image series for train-\n",
      "ing and 100 image series for testing.\n",
      "for each image series, the min-max nor-\n",
      "malization converted the pixel values to [−1, 1].\n",
      "the chest ct dataset with lung\n",
      "cancer contained 500 image series for training and 100 image series for testing.\n",
      "every image series comprises\n",
      "two-dimensional (2d) consecutive slices, and we applied our algorithm on a per\n",
      "2d slice basis.\n",
      "self-supervised medical image segmentation: we began by optimiz-\n",
      "ing the hyperparameters to achieve self-supervised segmentation.\n",
      "because anatomical elements,\n",
      "including the substructures of organs and diseases, are too detailed for human\n",
      "annotators to segment, it was diﬃcult to create ground-truth labels.\n",
      "by comparing diﬀerent settings on the\n",
      "pelvic mri training dataset (see supplementary information), the number\n",
      "of segmentation classes of 10, the combination of t1, t2, and t3 with moderate\n",
      "magnitude, the weakly imposed reconstruction loss, and a certain value of the\n",
      "margin parameter were considered suitable for self-supervised segmentation.\n",
      "in\n",
      "particular, we found that reconstruction loss is essential for obtaining segmen-\n",
      "tation maps corresponding to anatomical elements, although such a loss term\n",
      "was not included in previous studies\n",
      "the resultant segmentation maps are shown\n",
      "in fig.\n",
      "the anatomical substructures, including the histological structure\n",
      "of the colorectal wall and subregions within the lung, corresponded well with\n",
      "the segmentation maps in both the pelvic mri and chest ct testing datasets.\n",
      "because our self-supervised segmentation extracts low- to mid-level image con-\n",
      "tent, a semantic object (e.g., rectum or lung cancer) typically consists of multiple\n",
      "segmentation classes shared with other objects (see the magniﬁed images in\n",
      "fig.\n",
      "these anatomical elements may be too detailed for humans to\n",
      "annotate, demonstrating the necessity of self-supervised segmentation for high-\n",
      "precision medical-image editing.\n",
      "evaluation of the synthesized images: we measured the quality of image\n",
      "reconstruction using mean square error (mse), structural similarity (ssim), and\n",
      "peak signal-to-noise ratio (psnr).\n",
      "4. results of the image segmentation and editing.\n",
      "the segmentation maps\n",
      "were well aligned with the anatomical elements in both (a) the pelvic mri and (b)\n",
      "the chest ct testing datasets.\n",
      "(c) a synthetic image generated by editing the testing\n",
      "image with the caption, “axial t2-weighted mr image shows a tumor approximately\n",
      "4 cm in size on the dorsal wall of the rectum.\n",
      "(d) a synthetic\n",
      "image with the caption, “axial ct image showing a pulmonary nodule with a length of\n",
      "2–3 cm and a cavity on the dorsal side of the right upper lobe of the lung.”\n",
      "ssim, and psnr were 1.41×10−2 ±1.04×10−2, 7.40×10−1 ±0.57×10−1, and\n",
      "22.5±2.7 in the pelvic mri testing dataset and 5.03×10−4 ±3.03×10−4, 9.08×\n",
      "10−1 ±0.34×10−1, and 38.6±1.7 in the chest ct testing dataset.\n",
      "subsequently,\n",
      "segmentation maps from the testing images were edited to generate images with\n",
      "the intended characteristics (see fig.\n",
      "first, we tested whether the evaluators could identify\n",
      "real or synthesized images from 20 images, which include ten real images and ten\n",
      "synthesized images.\n",
      "the accuracies (i.e., the ratio of images correctly identiﬁed\n",
      "as real or synthetic) were 0.69 ± 0.11 and 0.65 ± 0.11, for the pelvic mri and\n",
      "chest ct testing datasets, respectively.\n",
      "note that when the synthetic images\n",
      "cannot be distinguished at all, the accuracy should be 0.5.\n",
      "second, we presented\n",
      "medical image editing\n",
      "411\n",
      "image captions explaining the radiological features, which also represented the\n",
      "editing intention for the synthetic images.\n",
      "we asked the evaluators to rate each\n",
      "presented image from a to c. a: the image is natural as a medical image,\n",
      "and the caption is consistent with the image.\n",
      "b: the image is natural as a\n",
      "medical image, but the caption is not consistent with the image.\n",
      "c: the image\n",
      "is not natural as a medical image.\n",
      "this test was conducted after informing\n",
      "the evaluators of the assumption that all 20 images could be synthetic, without\n",
      "indicating which image was real or synthetic.\n",
      "as a result, the ratio of synthetic\n",
      "images (vs. that of real images) categorized as a, b, and c were 0.80 ± 0.15\n",
      "(vs. 0.78 ± 0.20), 0.02 ± 0.04 (vs. 0.08 ± 0.07), and 0.18 ± 0.11 (vs. 0.14 ±\n",
      "0.13) for the pelvic mri testing dataset, and 0.74 ± 0.28 (vs. 0.76 ± 0.30), 0.08\n",
      "± 0.09 (vs. 0.12 ± 0.15), and 0.18 ± 0.21 (vs. 0.12 ± 0.14) for the chest ct\n",
      "testing dataset.\n",
      "there were no signiﬁcant diﬀerences between real and synthetic\n",
      "images (t-test: p > 0.05).\n",
      "consequently, the majority of the edited images were\n",
      "natural-looking medical images with accurately reproduced disease features.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf:\n",
      "we perform unsupervised disentan-\n",
      "glement of latent clinical signatures and leverage time-distance scaled\n",
      "self-attention to jointly learn from clinical signatures expressions and\n",
      "chest computed tomography (ct) scans.\n",
      "evaluation on 227 subjects with challenging\n",
      "spns revealed a signiﬁcant auc improvement over a longitudinal mul-\n",
      "timodal baseline (0.824 vs 0.752 auc), as well as improvements over\n",
      "a single cross-section multimodal scenario (0.809 auc) and a longitu-\n",
      "dinal imaging-only scenario (0.741 auc).\n",
      "this work demonstrates sig-\n",
      "niﬁcant advantages with a novel approach for co-learning longitudinal\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43895-0 61.\n",
      "[1,5,12,21], but approaches\n",
      "that only consider imaging are fundamentally limited.\n",
      "previous studies overcome these chal-\n",
      "lenges by aggregating over visits and binning time series within a bidirectional\n",
      "encoder representations from transformers (bert) architecture [2,14,20,25],\n",
      "limiting their scope to data collected on similar time scales, such as icu mea-\n",
      "surements, [11,29], or leveraging graph guided transformers to handle asyn-\n",
      "chrony\n",
      "[17]\n",
      "or continuous variables as latent functions [16]. operating with the hypothesis\n",
      "that distinct disease mechanisms manifest independently of one another in a\n",
      "probabilistic manner, one can learn a transformation that disentangles latent\n",
      "sources, or clinical signatures, from these longitudinal curves.\n",
      "clinical signa-\n",
      "tures learned in this way are expert-interpretable and have been well-validated\n",
      "to reﬂect known pathophysiology across many diseases [15,18].\n",
      "in this work, we jointly learn from longitudinal medical imag-\n",
      "ing, demographics, billing codes, medications, and lab values to classify spns.\n",
      "we leverage a transformer-based encoder to fuse features from\n",
      "both longitudinal imaging and clinical signature expressions sampled at inter-\n",
      "vals ranging from weeks to up to ﬁve years.\n",
      "ica learns independent latent signatures, s, in an unsupervised manner on a\n",
      "large non-imaging cohort.\n",
      "input embeddings are the sum of 1) token embedding derived\n",
      "from signatures or imaging, 2) a ﬁxed positional embedding indicating the token’s\n",
      "position in the sequence, and 3) a learnable segment embedding indicating imaging\n",
      "or non-imaging modality.\n",
      "we set c = 2000 and estimated s in an unsupervised manner\n",
      "using fastica [13].\n",
      "given longitudinal curves for another cohort, for instance\n",
      "dimage-ehr = {x′\n",
      "k | k = 1, . . .\n",
      "we represent our multi-\n",
      "modal datasets dimage-ehr and dimage-ehr-spn = {(ek, gk) | k\n",
      ", ek,t } sampled at the same dates\n",
      "as images gk = {gk,1, . .\n",
      "embeddings that incorporate positional and segment information\n",
      "are computed for each item in the sequence (fig. 1, right).\n",
      "token embeddings\n",
      "for images are a convolutional embeddings of ﬁve concatenated\n",
      "likewise, token embeddings for clinical sig-\n",
      "nature expressions are linear transformations to the same dimension as imag-\n",
      "ing token embeddings.\n",
      "following [5,19,32], we intuit that if medical\n",
      "data is sampled as a cross-sectional manifestation of a continuously progressing\n",
      "longitudinal multimodal transformer integrating imaging\n",
      "653\n",
      "phenotype, we can use a temporal emphasis model (tem) emphasize the impor-\n",
      "tance of recent observations over older ones.\n",
      "formally, if subject k has a sequence of t images at rel-\n",
      "ative acquisition days t1 . .\n",
      "the trans-\n",
      "former encoder computes query, key, and value matrices as linear transformations\n",
      "of input embedding h = { ˆe ∥ ˆg} at attention head p\n",
      "qp = hpw q\n",
      "p\n",
      "kp = hpw k\n",
      "p\n",
      "vp = hpw v\n",
      "p\n",
      "tem-scaled self-attention is computed via element-wise multiplication of the\n",
      "query-key product and ˆr:\n",
      "softmax\n",
      "\u0002\n",
      "relu(qpk⊤\n",
      "p + m) ◦ ˆr\n",
      "√\n",
      "d\n",
      "\u0003\n",
      "vp\n",
      "(3)\n",
      "where m is the padding mask [31] and d is the dimension of the query and key\n",
      "matrices.\n",
      "our search for contextual embeddings for med-\n",
      "ications and laboratory values did not yield any robust published models that\n",
      "were compatible with our ehr’s nomenclature, so these were not included in\n",
      "tdcode2vec.\n",
      "we also performed experiments using only image sequences as\n",
      "input, which we call tdimage.\n",
      "finally, we implemented single cross-sectional\n",
      "versions of tdimage, tdcode2vec, and tdsig, csimage, cscode2vec, and\n",
      "cssig respectively, using the scan date closest to the lung malignancy diag-\n",
      "nosis for cases or spn date for controls.\n",
      "all baselines except csimage, which\n",
      "654\n",
      "t. z. li et al.\n",
      "table 1. breakdown of modalities, size, and longitudinality of each dataset.\n",
      "modalities\n",
      "counts (cases/controls)\n",
      "demo img code med lab subjects scans\n",
      "ehr-pulmonary\n",
      "–\n",
      "288,428\n",
      "–\n",
      "nlst\n",
      "–\n",
      "–\n",
      "–\n",
      "533/801\n",
      "1066/1602\n",
      "image-ehr\n",
      "257/665\n",
      "641/1624\n",
      "image-ehr-spn\n",
      "58/169\n",
      "76/405\n",
      "demo: demographics, img: chest cts, code:\n",
      "this work was supported by pytorch 1.13.1, cuda 11.7.\n",
      "3\n",
      "experimental setup\n",
      "datasets.\n",
      "next, ehr-\n",
      "pulmonary was the unlabeled dataset used to learn clinical signatures in an\n",
      "unsupervised manner.\n",
      "additionally, image-\n",
      "ehr was a labeled dataset with paired imaging and ehrs.\n",
      "in the ehr-image cohort, malignant cases were labeled as those with a billing\n",
      "code for lung malignancy and no cancer of any type prior.\n",
      "finally, image-ehr-spn was a\n",
      "subset of image-ehr with the inclusion criteria that subjects had a billing code\n",
      "for an spn and no cancer of any type prior to the spn.\n",
      "a description of the billing codes used to\n",
      "deﬁne spn and lung cancer events are provided in supplementary 1.2.\n",
      "longitudinal multimodal transformer integrating imaging\n",
      "655\n",
      "fig.\n",
      "while this was the\n",
      "only pretraining step for image-only models (csimage and tdimage), the mul-\n",
      "timodal models underwent another stage of pretraining using the image-ehr\n",
      "cohort with subjects from image-ehr-spn subtracted.\n",
      "in this stage, we ran-\n",
      "domly selected one scan and the corresponding clinical signature expressions for\n",
      "each subject and each training epoch.\n",
      "models were trained until the running\n",
      "mean over 100 global steps of the validation loss increased by more than 0.2.\n",
      "for evaluation, we performed ﬁve-fold cross-validation with image-ehr-spn,\n",
      "using up to three of the most recent scans in the longitudinal models.\n",
      "we report\n",
      "the mean auc and 95% conﬁdence interval from 1000 bootstrapped samples,\n",
      "sampling with replacement from the pooled predictions across all test folds.\n",
      "we performed a reclassiﬁcation analysis of low,\n",
      "medium, and high-risk tiers separated by thresholds of 0.05 and 0.65, which are\n",
      "the cutoﬀs used to guide clinical management.\n",
      "4\n",
      "results\n",
      "the signiﬁcant improvement with tdsig over cssig demonstrates the advan-\n",
      "tage of longitudinally in the context of combining images and clinical signatures\n",
      "(table 2).\n",
      "there were large performance gaps between tdsig and tdcode2vec,\n",
      "as well as between cssig and cscode2vec, demonstrating the advantage of\n",
      "656\n",
      "t. z. li et al.\n",
      "table 2. performance on spn classiﬁcation using diﬀerent approaches and modalities.\n",
      "[95% ci]\n",
      "img demo code med lab nlst image-ehr\n",
      "csimage\n",
      "0.7392\n",
      "[0.8075, 0.8120]\n",
      "tdimage\n",
      "0.7406 [0.7381, 0.7432]\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "tdcode2vec 0.7524\n",
      "however, the\n",
      "subject’s highest expressed clinical signature at the 3-month mark was a new pattern\n",
      "of bacterial pneumonia (b), oﬀering to the model a benign explanation of an image\n",
      "that it would otherwise be less correctly conﬁdent in.\n",
      "cross-sectional embed-\n",
      "ded billing codes did not signiﬁcantly improve performance over images alone\n",
      "(cscode2vec vs csimage, p = 0.56), but adding clinical signatures did (cssig vs\n",
      "csimage, p < 0.01; tdsig vs tdimage, p < 0.01) and the greatest improvement\n",
      "in longitudinal data over single cross sections occurred when clinical signatures\n",
      "were included.\n",
      "for control subjects, tdsig correctly/incorrectly reclassiﬁed 40/18 from\n",
      "tdcode2vec, 54/8 from tdimage, 12/18 from cssig, 104/7 from cscode2vec,\n",
      "and 125/5 from csimage.\n",
      "for case subjects, tdsig correctly/incorrectly reclas-\n",
      "siﬁed 13/10 from tdcode2vec, 17/8 from tdimage, 12/2 from cssig, 23/16\n",
      "from cscode2vec, and 29/16 from csimage (fig. 2).\n",
      "full reclassiﬁcation matri-\n",
      "ces are reported in supplementary 6.1.\n",
      "we demonstrated large performance gains in spn classiﬁ-\n",
      "cation compared with baselines, although calibration of our models is needed\n",
      "to assess clinical utility.\n",
      "in this setting, we found that adding clinical context increased the performance\n",
      "gap between longitudinal data and single cross-sections.\n",
      "we release our implemen-\n",
      "tation at https://github.com/masilab/lmsignatures.\n",
      "we were able to\n",
      "overcome our small cohort size (image-ehr-spn) by leveraging unsupervised\n",
      "learning on datasets without imaging (ehr-pulmonary), pretraining on public\n",
      "datasets without ehrs (nlst), and pretraining on paired multimodal data with\n",
      "noisy labels (image-ehr) within a ﬂexible transformer architecture.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_9.pdf:\n",
      "the process of annotating histological gigapixel-sized whole\n",
      "slide images (wsis) at the pixel level for the purpose of training a super-\n",
      "vised segmentation model is time-consuming.\n",
      "region-based active learn-\n",
      "ing (al) involves training the model on a limited number of annotated\n",
      "image regions instead of requesting annotations of the entire images.\n",
      "these annotation regions are iteratively selected, with the goal of opti-\n",
      "mizing model performance while minimizing the annotated area.\n",
      "we evaluate our method\n",
      "using the task of breast cancer metastases segmentation on the public\n",
      "camelyon16 dataset and show that it consistently achieves higher\n",
      "sampling eﬃciency than the standard method across various al step\n",
      "sizes.\n",
      "with only 2.6% of tissue area annotated, we achieve full annota-\n",
      "tion performance and thereby substantially reduce the costs of annotat-\n",
      "ing a wsi dataset.\n",
      "keywords: active learning · region selection · whole slide images\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43895-0 9.\n",
      "90–100, 2023.\n",
      "https://doi.org/10.1007/978-3-031-43895-0_9\n",
      "adaptive region selection for al in wsi semantic segmentation\n",
      "91\n",
      "1\n",
      "introduction\n",
      "semantic segmentation on histological whole slide images (wsis) allows precise\n",
      "detection of tumor boundaries, thereby facilitating the assessment of metas-\n",
      "tases\n",
      "however, pixel-level anno-\n",
      "tations of gigapixel-sized wsis (e.g. 100, 000 × 100, 000 pixels) for training a\n",
      "segmentation model are diﬃcult to acquire.\n",
      "[18]. identifying potentially informative image regions (i.e., providing\n",
      "useful information for model training) allows requesting the minimum amount\n",
      "of annotations for model optimization, and a decrease in annotated area reduces\n",
      "both localization and delineation workloads.\n",
      "the challenge is to eﬀectively select\n",
      "annotation regions in order to achieve full annotation performance with the least\n",
      "annotated area, resulting in high sampling eﬃciency.\n",
      "we use region-based active learning (al) [13] to progressively identify anno-\n",
      "tation regions, based on iteratively updated segmentation models.\n",
      "first, the prediction of the most recently\n",
      "trained segmentation model is converted to a priority map that reﬂects infor-\n",
      "mativeness of each pixel.\n",
      "[5] and highest disagreement between a set of models [19]).\n",
      "the enhancement of priority maps, such as highlighting easy-to-label pixels [13],\n",
      "edge pixels [6] or pixels with a low estimated segmentation quality [2], is also\n",
      "a popular area of research.\n",
      "all of these works selected square regions of a manually predeﬁned\n",
      "size, disregarding the actual shape and size of informative areas.\n",
      "this work focuses on region selection methods, a topic that has been largely\n",
      "neglected in literature until now, but which we show to have a great impact on al\n",
      "sampling eﬃciency (i.e., the annotated area required to reach the full annotation\n",
      "performance).\n",
      "we discover that the sampling eﬃciency of the aforementioned\n",
      "standard method decreases as the al step size (i.e., the annotated area at each\n",
      "al cycle, determined by the multiplication of the region size and the number of\n",
      "selected regions per wsi) increases.\n",
      "(image resolution: 0.25 µm\n",
      "px )\n",
      "region by ﬁrst identifying an informative area with connected component detec-\n",
      "tion and then detecting its bounding box.\n",
      "we test our method using a breast\n",
      "cancer metastases segmentation task on the public camelyon16 dataset and\n",
      "demonstrate that determining the selected regions individually provides greater\n",
      "ﬂexibility and eﬃciency than selecting regions with a uniform predeﬁned shape\n",
      "and size, given the variability in histological tissue structures.\n",
      "the train-select-annotate process is\n",
      "repeated until a certain performance of g or annotation budget is reached.\n",
      "the informativeness measure is not the focus\n",
      "adaptive region selection for al in wsi semantic segmentation\n",
      "93\n",
      "of this study, we therefore adopt the most commonly used one that quantiﬁes\n",
      "model uncertainty (details in sect.\n",
      "standard (non-square) we\n",
      "implement a generalized version of the standard method that allows non-square\n",
      "region selections by including multiple region candidates centered at each pixel\n",
      "with various aspect ratios.\n",
      "note that standard (non-square)\n",
      "can be understood as an ablation study of the proposed method adaptive to\n",
      "examine the eﬀect of variable region shape by maintaining constant region size.\n",
      "2.3\n",
      "wsi semantic segmentation framework\n",
      "this section describes the breast cancer metastases segmentation task we use\n",
      "for evaluating the al region selection methods.\n",
      "data augmentation includes random ﬂip, random rotation, and stain augmenta-\n",
      "tion [12]. inference.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "dataset\n",
      "we used the publicly available camelyon16 challenge dataset\n",
      "the collection of the data was\n",
      "approved by the responsible ethics committee (commissie mensgebonden onder-\n",
      "zoek regio arnhem-nijmegen).\n",
      "the test set contains 48 wsis with and 80 wsis without metastases 1.\n",
      "3.2\n",
      "implementation details\n",
      "training schedules.\n",
      "[15] initialized with imagenet\n",
      "adaptive region selection for al in wsi semantic segmentation\n",
      "95\n",
      "fully-connected layers with sizes of 512 and 2, followed by a softmax activation\n",
      "layer.\n",
      "[1] to validate the segmentation framework.\n",
      "to evaluate the wsi segmentation performance directly, we use mean intersec-\n",
      "tion over union (miou).\n",
      "for comparison, we follow [3] to use a threshold of 0.5\n",
      "to generate the binary segmentation map and report miou (tumor), which is\n",
      "the average miou over the 48 test wsis with metastases.\n",
      "we evaluate the model\n",
      "trained at each al cycle to track performance change across the al procedure.\n",
      "3.3\n",
      "results\n",
      "full annotation performance.\n",
      "to validate our segmentation framework, we\n",
      "ﬁrst train on the fully-annotated data (average performance of ﬁve repetitions\n",
      "reported).\n",
      "with our framework, reducing s to 128 pixels\n",
      "improves both metastases identiﬁcation and segmentation (froc score: 0.779,\n",
      "miou (tumor): 0.758).\n",
      "this makes an al experiment, which involves multiple rounds of\n",
      "wsi inference, extremely costly.\n",
      "therefore, we use s = 256 pixels for all fol-\n",
      "lowing al experiments to compromise between performance and computation\n",
      "costs.\n",
      "because wsis without metastases do not require pixel-level annotation,\n",
      "we exclude the 159 training and validation wsis without metastases from all\n",
      "following al experiments.\n",
      "this reduction leads to a slight decrease of full anno-\n",
      "tation performance (miou (tumor) from 0.749 to 0.722).\n",
      "results show average and min/max (shaded)\n",
      "performance over three repetitions with distinct initial labeled sets.\n",
      "experiments with large al step sizes\n",
      "perform 10 al cycles (fig. 4 (e), (f), (h) and (i)); others perform 15 al cycles.\n",
      "all experiments (except for random) use uncertainty sampling.\n",
      "when using region selection method standard, the sampling eﬃciency advan-\n",
      "tage of uncertainty sampling over random sampling decreases as al step size\n",
      "increases.\n",
      "a small al step size minimizes the annotated tissue area for a certain\n",
      "high level of model performance, such as an miou (tumor) of 0.7, yet requires a\n",
      "large number of al cycles to achieve full annotation performance (fig. 4 (a–d)),\n",
      "adaptive region selection for al in wsi semantic segmentation\n",
      "97\n",
      "table 1.\n",
      "annotated tissue area (%) required to achieve full annotation performance.\n",
      "the symbol “/” indicates that the full annotation performance is not achieved in the\n",
      "corresponding experimental setting in fig.\n",
      "a large al step size allows for full anno-\n",
      "tation performance to be achieved in a small number of al cycles, but at the\n",
      "expense of rapidly expanding the annotated tissue area (fig.\n",
      "table 1 shows that adaptive achieves full annotation performance with fewer\n",
      "al cycles than standard for small al step sizes and less annotated tissue area\n",
      "for large al step sizes.\n",
      "this is advantageous because extensive\n",
      "al step size tuning to balance the annotation and computation costs can be\n",
      "avoided.\n",
      "4(h) that the full annotation performance is not achieved\n",
      "with adaptive within 15 al cycles; in fig.\n",
      "s1 in the supplementary materials\n",
      "we show that allowing for oversampling of previously selected regions can be a\n",
      "solution to this problem.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf:\n",
      "deep learning based medical image recognition systems often\n",
      "require a substantial amount of training data with expert annotations,\n",
      "which can be expensive and time-consuming to obtain.\n",
      "recently, syn-\n",
      "thetic augmentation techniques have been proposed to mitigate the issue\n",
      "by generating realistic images conditioned on class labels.\n",
      "to further reduce the depen-\n",
      "dency on annotated data, we propose a synthetic augmentation method\n",
      "called histodiﬀusion, which can be pre-trained on large-scale unlabeled\n",
      "datasets and later applied to a small-scale labeled dataset for augmented\n",
      "training.\n",
      "in particular, we train a latent diﬀusion model (ldm) on\n",
      "diverse unlabeled datasets to learn common features and generate real-\n",
      "istic images without conditional inputs.\n",
      "then, we ﬁne-tune the model\n",
      "with classiﬁer guidance in latent space on an unseen labeled dataset\n",
      "so that the model can synthesize images of speciﬁc categories.\n",
      "with histodiﬀusion augmentation, the\n",
      "classiﬁcation accuracy of a backbone classiﬁer is remarkably improved\n",
      "by 6.4% using a small set of the original labels.\n",
      "1\n",
      "introduction\n",
      "the recent advancements in medical image recognition systems have greatly ben-\n",
      "eﬁted from deep learning techniques [15,28].\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43895-0 71.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43895-0_71\n",
      "synthetic augmentation with large-scale unconditional pre-training\n",
      "755\n",
      "fig.\n",
      "1. comparison between diﬀerent deep generative models for synthetic augmenta-\n",
      "tion.\n",
      "(a) cgan-based method which requires relatively large-scale annotated training\n",
      "data; (b) diﬀusion model (dm) which cannot take conditional input; (c) our proposed\n",
      "histodiﬀusion model that can be pretrained on large-scale unannotated data and later\n",
      "applied to unseen small-scale annotated data for augmentation.\n",
      "are one of the key components for training deep learning models to achieve sat-\n",
      "isfactory results [3,17].\n",
      "however, unlike natural images in computer vision, the\n",
      "number of medical images with expert annotations is often limited by the high\n",
      "labeling cost and privacy concerns.\n",
      "to overcome this challenge, a natural choice\n",
      "is to employ data augmentation to increase the number of training samples.\n",
      "although conventional augmentation techniques [23] such as ﬂipping and crop-\n",
      "ping can be directly applied to medical images, they merely improve the diver-\n",
      "sity of datasets, thus leading to marginal performance gains\n",
      "[10] to\n",
      "synthesize visually appealing medical images that closely resemble those in the\n",
      "original datasets\n",
      "while existing works have proven eﬀective in improv-\n",
      "ing the performance of downstream models to some extent, a suﬃcient amount\n",
      "of labeled data is still required to adequately train models to generate decent-\n",
      "quality images.\n",
      "more recently, diﬀusion models have become popular for natural\n",
      "image generation due to their impressive results and training stability [4,13,31].\n",
      "a few studies have also demonstrated the potential of diﬀusion models for med-\n",
      "ical image synthesis [19,24].\n",
      "although annotated data is typically hard to acquire for medical images,\n",
      "unannotated data is often more accessible.\n",
      "to mitigate the issue existed in cur-\n",
      "rent cgan-based synthetic augmentation methods [8,36–38], in this work, we\n",
      "propose to leverage the diﬀusion model with unlabeled pre-training to reduce\n",
      "the dependency on the amount of labeled data (see comparisons in fig.\n",
      "we\n",
      "propose a novel synthetic augmentation method, named histodiﬀusion, which\n",
      "can be pre-trained on large-scale unannotated datasets and adapted to small-\n",
      "scale annotated datasets for augmented training.\n",
      "this large-scale pre-training enables the model to learn\n",
      "756\n",
      "j. ye et al.\n",
      "common yet diverse image characteristics and generate realistic medical images.\n",
      "synthetic images are then generated with classiﬁer guidance [4] in\n",
      "the latent space.\n",
      "following the prior work [36], we select generated images based\n",
      "on the conﬁdence of target labels and feature similarity to real labeled images.\n",
      "we evaluate our proposed method on a histopathology image dataset of colorec-\n",
      "tal cancer (crc).\n",
      "experiment results show that when presented with limited\n",
      "annotations, the classiﬁer trained with our augmentation method outperforms\n",
      "the ones trained with the prior cgan-based methods.\n",
      "our experimental results\n",
      "show that once histodiﬀusion is well pre-trained using large datasets, it can be\n",
      "applied to any future incoming small dataset with minimal ﬁne-tuning and may\n",
      "substantially improve the ﬂexibility and eﬃcacy of synthetic augmentation.\n",
      "to enable conditional image synthesis, we also train a latent classiﬁer on\n",
      "the same labeled dataset to guide the diﬀusion model in ldm.\n",
      "once the classiﬁer\n",
      "is trained, we apply the ﬁne-tuned ldm to generate a pool of candidate images\n",
      "conditioned on the target class labels.\n",
      "these candidate images are then passed\n",
      "through the image selection module to ﬁlter out any low-quality results.\n",
      "finally,\n",
      "we can train downstream classiﬁcation models on the expanded training data,\n",
      "which includes the selected images, and then use them to perform inference on\n",
      "test data.\n",
      "(2)\n",
      "synthetic augmentation with large-scale unconditional pre-training\n",
      "757\n",
      "fig.\n",
      "the architecture of our proposed histodiﬀusion, which consists of a pre-training\n",
      "process (blue solid lines), a ﬁne-tuning process (blue dashed lines), and a selective aug-\n",
      "mentation process (orange lines).\n",
      "during pre-training, a latent autoencoder (lae) and\n",
      "a diﬀusion model (dm) are trained on large-scale unlabeled datasets for unconditional\n",
      "image synthesis.\n",
      "histodiﬀusion is then ﬁne-tuned on a small-scale dataset for condi-\n",
      "tional image synthesis under the guidance of a trained latent classiﬁer.\n",
      "during selective\n",
      "augmentation, given a target class label, the synthetic images generated by the ﬁne-\n",
      "tuned model are selected and added to the training set based on their distances to the\n",
      "class centroids in the feature space.\n",
      "the denoising\n",
      "model ϵθ is typically implemented using a time-conditioned u-net [27] with\n",
      "residual blocks\n",
      "our proposed histodiﬀusion is built on latent diﬀu-\n",
      "sion models (ldm) [26], which requires fewer computational resources without\n",
      "degradation in performance, compared to prior works [4,15,28].\n",
      "[16] to encode images as lower-dimensional latent\n",
      "representations and then learns a diﬀusion model (dm) for image synthesis by\n",
      "758\n",
      "j. ye et al.\n",
      "modeling the latent space of the trained lae.\n",
      "particularly, the encoder e of\n",
      "the lae encodes the input image x ∈ rh×w ×3 into a latent representation\n",
      "z = e(x) ∈ rh×w×c in a lower-dimensional latent space z. here h and w are\n",
      "the height and width of image x, and h, w, and c are the height, width, and\n",
      "channel of latent z, respectively.\n",
      "the latent z is then passed into the decoder\n",
      "d to reconstruct the image ˆx = d(z).\n",
      "through this process, the compositional\n",
      "features from the image space x can be extracted to form the latent space z,\n",
      "and we then model the distribution of z by learning a dm.\n",
      "for the dm in\n",
      "ldm, both the forward and reverse sampling processes are performed in the\n",
      "latent space z instead of the original image space x.\n",
      "unconditional large-scale pre-training.\n",
      "speciﬁcally, we gather unlabeled\n",
      "images from m diﬀerent sources to construct a large-scale set of datasets\n",
      "s = {s1, s2, . .\n",
      "we then train an lae using the data from s with the\n",
      "following self-reconstruction loss to learn a powerful latent space z that can\n",
      "describe diverse features:\n",
      "llae = lrec(ˆx, x) + λkldkl(q(z)||n(0, i)) ,\n",
      "(4)\n",
      "where lrec is the loss measuring the diﬀerence between the output reconstructed\n",
      "image ˆx and the input ground truth image x.\n",
      "here we implement lrec with a\n",
      "combination of a pixel-wise l1 loss, a perceptual loss\n",
      "in the dm\n",
      "reverse sampling process to synthesize a novel latent ˜z0 ∈ rh×w×c and employ\n",
      "the trained decoder d to generate a new image ˜x = d(˜z0), which should satisfy\n",
      "the similar distribution as the data in s.\n",
      "conditional small-scale fine-tuning.\n",
      "using the lae and dm pretrained\n",
      "on s, we can only generate the new image ˜x following the similar distribution in\n",
      "s. to generalize our histodiﬀusion to the small-scale labeled dataset s′ collected\n",
      "from a diﬀerent source (i.e., s′ ̸⊂ s), we further ﬁne-tune histodiﬀusion using\n",
      "the labeled data from s′. let y be the label of image x in s′. to minimize the\n",
      "training cost, we ﬁx both the trained encoder e and trained dm model ϵθ to\n",
      "keep latent space z unchanged.\n",
      "then we only ﬁne-tune the decoder d using\n",
      "labeled data (x, y) from s′ with the following loss function:\n",
      "ld = lrec(ˆx, x) + λcelce(ϕ(ˆx), y) ,\n",
      "(5)\n",
      "where lrec(ˆx, x) is the self-reconstruction loss between the output reconstructed\n",
      "image ˆx = d(e(x)) and the input ground truth image x. to enhance the corre-\n",
      "lation between the decoder output ˆx and label y, we also add an auxiliary image\n",
      "synthetic augmentation with large-scale unconditional pre-training\n",
      "759\n",
      "classiﬁer ϕ trained with (x, y) on the top of d and impose the cross-entropy\n",
      "classiﬁcation loss lce when ﬁne-tuning d. λce is the balancing parameter.\n",
      "to enable conditional image gen-\n",
      "eration with our histodiﬀusion, we further apply the classiﬁer-guided diﬀusion\n",
      "sampling proposed in [4,29,30,33] using the labeled data (x, y) from small-scale\n",
      "labeled dataset s′. we ﬁrst utilize the trained encoder e to encode the data x\n",
      "from s′ as latent z0.\n",
      "(8)\n",
      "the ﬁnal image ˜x of class y can be generated by applying the ﬁne-tuned decoder\n",
      "d′, i.e., ˜x = d′( ˜z0).\n",
      "selective augmentation.\n",
      "to further improve the eﬃcacy of synthetic aug-\n",
      "mentation, we follow [36] to selectively add synthetic images to the original\n",
      "labeled training data based on centroid feature distance.\n",
      "the augmentation ratio\n",
      "is deﬁned as the ratio between the selected synthetic images and the original\n",
      "training images.\n",
      "more results are demonstrated later in table 1.\n",
      "3\n",
      "experiments\n",
      "datasets.\n",
      "we employ three public datasets of histopathology images during\n",
      "the large-scale pre-training procedure.\n",
      "[2], containing 312,320 patches extracted from the hematoxylin & eosin\n",
      "(h&e) stained human breast cancer tissue micro-array (tma) images [18].\n",
      "the second dataset is pannuke [9],\n",
      "a pan-cancer histology dataset for nuclei instance segmentation and classiﬁca-\n",
      "tion.\n",
      "3. comparison of real images from training subset, synthesized images generated\n",
      "by stylegan2\n",
      "as for ﬁne-tuning and evaluation, we employ the\n",
      "nct-crc-he-100k dataset that contains 100,000 patches from h&e stained\n",
      "histological images of human colorectal cancer (crc) and normal tissue.\n",
      "the\n",
      "patches have been divided into 9 classes: adipose (adi), background (back),\n",
      "debris (deb), lymphocytes (lym), mucus (muc), smooth muscle (mus), nor-\n",
      "synthetic augmentation with large-scale unconditional pre-training\n",
      "761\n",
      "mal colon mucosa (norm), cancer-associated stroma (str), colorectal adeno-\n",
      "carcinoma epithelium (tum).\n",
      "this subset has been carefully selected through an even sampling without\n",
      "replacement from each tissue type present in the train set.\n",
      "by ensuring that the ﬁne-tuning process is representative\n",
      "of the entire dataset through even sampling from each tissue type, we can elim-\n",
      "inate bias towards any particular tissue type.\n",
      "the related data use declaration and acknowledgment\n",
      "can be found in our supplementary materials.\n",
      "evaluation metrics.\n",
      "[12]\n",
      "to assess the image quality of the synthetic samples.\n",
      "we further compute the\n",
      "accuracy, f1-score, sensitivity, and speciﬁcity of the downstream classiﬁers to\n",
      "evaluate the performance gain from diﬀerent augmentation methods.\n",
      "model implementation.\n",
      "our implementation of histodiﬀusion basically\n",
      "follows the ldm-4 [26] architecture, where the input is downsampled by a factor\n",
      "of 4, resulting in a latent representation with dimensions of 64 × 64 × 3.\n",
      "we use the same architecture for the auxiliary image classiﬁer ϕ. for down-\n",
      "stream evaluation, we implement the classiﬁer using the vit-b/16 architecture\n",
      "[5] in all experiments to ensure fair comparisons.\n",
      "to\n",
      "ensure a fair comparison, all images synthesized by stylegan2 and histodiﬀu-\n",
      "sion model are further selected based on feature centroid distances [36].\n",
      "more\n",
      "implementation details of our proposed histodiﬀusion, stylegan2, and baseline\n",
      "classiﬁer can also be found in our supplementary materials.\n",
      "as shown in table 1, under the same synthetic augmenta-\n",
      "tion setting, histodiﬀusion shows better fid scores and outperforms the state-\n",
      "of-the-art cgan model stylegan2 in all classiﬁcation metrics.\n",
      "a qualitative\n",
      "comparison between synthetic images by histodiﬀusion and stylegan2 can be\n",
      "762\n",
      "j. ye et al.\n",
      "table 1.\n",
      "quantitative comparison results of synthetic image quality and augmented\n",
      "classiﬁcation.\n",
      "“random” refers to directly augmenting the training dataset with syn-\n",
      "thesized images without any image selections while “selective” indicates applying selec-\n",
      "tive module [36] to ﬁlter out low-quality images.\n",
      "the number (x%) suggests that the\n",
      "number of the synthesized images is x% of the original training set.\n",
      "fid↓\n",
      "accuracy↑ f1 score↑ sensitivity↑ speciﬁcity↑\n",
      "baseline (5% real images) /\n",
      "0.855\n",
      "0.850\n",
      "0.855\n",
      "0.983\n",
      "stylegan2\n",
      "3, where histodiﬀusion consistently generates more realistic images\n",
      "matching the given class conditions than sytlegan2, especially for classes adi\n",
      "and back.\n",
      "when augmenting the training dataset with diﬀerent numbers of images syn-\n",
      "thesized from histodiﬀusion and stylegan2, one can observe that when increas-\n",
      "ing the ratio of synthesized data to 100%, the fid score of stylegan2 increases\n",
      "quickly and can become even worse than the one without using image selection\n",
      "strategy.\n",
      "in contrast, histodiﬀusion can keep synthesizing high-quality images\n",
      "until the augmentation ratio reaches 300%.\n",
      "regarding classiﬁcation performance\n",
      "improvement of the baseline classiﬁer, the accuracy and f1 score of using his-\n",
      "todiﬀusion augmentation are increased by up to 6.4% and 6.6%, respectively.\n",
      "even when not using the image selection module to ﬁlter out the low-quality\n",
      "results (i.e., +random 50%), our histodiﬀusion can still improve the accuracy\n",
      "by 1.5%.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf:\n",
      "to tackle this problem, we pro-\n",
      "pose reveal to revise (r2r), a framework entailing the entire explain-\n",
      "able artiﬁcial intelligence (xai) life cycle, enabling practitioners to iter-\n",
      "atively identify, mitigate, and (re-)evaluate spurious model behavior with\n",
      "a minimal amount of human interaction.\n",
      "secondly (2), the responsi-\n",
      "ble artifacts are detected and spatially localized in the input data, which\n",
      "is then leveraged to (3) revise the model behavior.\n",
      "concretely, we apply\n",
      "the methods of rrr, cdep and clarc for model correction, and (4)\n",
      "(re-)evaluate the model’s performance and remaining sensitivity towards\n",
      "the artifact.\n",
      "using two medical benchmark datasets for melanoma detec-\n",
      "tion and bone age estimation, we apply our r2r framework to vgg,\n",
      "resnet and eﬃcientnet architectures and thereby reveal and correct\n",
      "real dataset-intrinsic artifacts, as well as synthetic variants in a con-\n",
      "trolled setting.\n",
      "completing the xai life cycle, we demonstrate multiple\n",
      "r2r iterations to mitigate diﬀerent biases.\n",
      "keywords: xai life cycle · bias identiﬁcation · model correction\n",
      "1\n",
      "introduction\n",
      "deep neural networks (dnns) have successfully been applied in research\n",
      "and industry for a multitude of complex tasks.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43895-0 56.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43895-0_56\n",
      "reveal to revise: an xai life cycle for iterative bias correction of dnns\n",
      "597\n",
      "fig.\n",
      "at this point,\n",
      "the artifact localization can be leveraged for (3) model correction, and (4) to evaluate\n",
      "the model’s performance on a poisoned test set and measure its remaining attention\n",
      "on the artifact.\n",
      "applications for which dnns have even shown to be superior to medical experts,\n",
      "such as with melanoma detection\n",
      "however, the reasoning of these highly\n",
      "complex and non-linear models is generally not transparent [23,24], and as such,\n",
      "their decisions may be biased towards unintended or undesired features, poten-\n",
      "tially caused by shortcut learning [2,9,14,27].\n",
      "as such, local xai methods reveal\n",
      "(input) features that are most relevant to a model, which, for image data, can\n",
      "be presented as heatmaps.\n",
      "while multiple approaches exist\n",
      "for either revealing or revising model biases, only few combine both steps, to\n",
      "be applicable as a framework.\n",
      "such frameworks, however, either rely heavily on\n",
      "human feedback [25,29], are limited to speciﬁc bias types\n",
      "[13,25].\n",
      "598\n",
      "f. pahde et al.\n",
      "to that end, we propose reveal to revise (r2r), an iterative xai life cycle\n",
      "requiring low amounts of human interaction that consists of four phases, illus-\n",
      "trated in fig.\n",
      "the\n",
      "generated annotations are then leveraged to (3) correct and (4) (re-)evaluate the\n",
      "model, followed by a repetition of the entire life cycle if required.\n",
      "for revealing\n",
      "model bias, we propose two orthogonal xai approaches: while spectral rele-\n",
      "vance analysis (spray)\n",
      "the arti-\n",
      "fact masks are further used for evaluation on a poisoned test set and to measure\n",
      "the remaining attention on the bias.\n",
      "we demonstrate the applicability and high\n",
      "automation of r2r on two medical tasks, including melanoma detection and bone\n",
      "age estimation, using the vgg-16, resnet-18 and eﬃcientnet-b0 dnn architec-\n",
      "tures.\n",
      "in our experiments, we correct model behavior w.r.t.\n",
      "lastly, we showcase the r2r life\n",
      "cycle through multiple iterations, unveiling and unlearning diﬀerent biases.\n",
      "2\n",
      "related work\n",
      "among other methods, e.g., leveraging auxiliary information [15,18,19,21],\n",
      "or training on de-biased representations [4,16], shortcut unlearning is often\n",
      "approached with xai.\n",
      "the former is based on presenting individual local explanations to a\n",
      "human, who, if necessary, provides feedback used for model correction\n",
      "in our r2r framework, we automate the annotation\n",
      "by following [2] for data labeling through spray outlier clusters, or by collecting\n",
      "the most representative samples of bias concepts according to crp.\n",
      "3.1, thereby considerably easing the step from bias identiﬁcation\n",
      "to correction.\n",
      "reveal to revise: an xai life cycle for iterative bias correction of dnns\n",
      "599\n",
      "existing works for model correction measure the performance on the original\n",
      "or clean test set, with corrected models often showing an improved generaliza-\n",
      "tion [13,20].\n",
      "3\n",
      "reveal to revise framework\n",
      "our reveal to revise (r2r) framework comprises the entire xai life cycle, includ-\n",
      "ing methods for (1) the identiﬁcation of model bias, (2) artifact labeling and local-\n",
      "ization, (3) the correction of detected misbehavior, and (4) the evaluation of the\n",
      "improved model.\n",
      "the\n",
      "spray clusters then naturally allow us to label data containing the bias.\n",
      "the artifact localization is given by a modiﬁed backward\n",
      "pass on the biased model with lrp for an artifact sample x, where we initialize\n",
      "the relevances rl(x) at layer l as\n",
      "rl(x) = al(x) ◦ hl\n",
      "(1)\n",
      "600\n",
      "f. pahde et al.\n",
      "with activations al and element-wise multiplication operator ◦.\n",
      "3.2\n",
      "methods for model correction\n",
      "in the following, we present the methods used for mitigating model biases.\n",
      "clarc for latent space correction.\n",
      "the framework consists of two methods, namely\n",
      "augmentive clarc (a-clarc) and projective clarc (p-clarc).\n",
      "dependent on input x. parameter γ(x) is chosen\n",
      "such that the activation in direction of the cav is as high as the average value\n",
      "over non-artifactual or artifactual samples for p-clarc or a-clarc, respectively.\n",
      "rrr and cdep for correction through prior knowledge.\n",
      "\u0004\u0004\u0004\u0004\n",
      "1\n",
      ".\n",
      "(4)\n",
      "4\n",
      "experiments\n",
      "the experimental section is divided into the two parts of (1) identiﬁcation, miti-\n",
      "gation and evaluation of spurious model behavior with various correction meth-\n",
      "ods and (2) showcasing the whole r2r framework in an iterative fashion.\n",
      "reveal to revise: an xai life cycle for iterative bias correction of dnns\n",
      "601\n",
      "fig.\n",
      "shown are band-aid, ruler,\n",
      "skin marker, and synthetic artifacts for the isic dataset, as well as “l”-marker and\n",
      "synthetic artifacts for the bone age dataset.\n",
      "4.1\n",
      "experimental setup\n",
      "we train vgg-16\n",
      "[26], resnet-18 [11] and eﬃcientnet-b0 [28] models on the\n",
      "isic 2019 dataset [7,8,30] for skin lesion classiﬁcation and pediatric bone age\n",
      "dataset\n",
      "[10] for bone age estimation based on hand radiographs.\n",
      "see appendix a.1 for additional experiment details.\n",
      "4.2\n",
      "revealing and revising spurious model behavior\n",
      "revealing bias: in the ﬁrst step of the r2r life cycle, we can reveal the use\n",
      "of several artifacts by the examined models, including the well-known band-aid,\n",
      "ruler and skin marker [6] and our synthetic clever hans for the isic dataset, as\n",
      "shown in fig. 2 for vgg-16.\n",
      "besides the synthetic clever hans for bone age classiﬁcation, we\n",
      "encountered the use of “l” markings, resulting from physical lead markers placed\n",
      "by radiologist to specify the anatomical side.\n",
      "interestingly, the “l” markings are\n",
      "larger for hands of younger children, as all hands are scaled to similar size [10],\n",
      "oﬀering the model to learn a shortcut by estimating the bone age based on the\n",
      "relative size of the “l” markings, instead of valid features.\n",
      "while we revealed\n",
      "the “l” marking bias using crp, we did not ﬁnd corresponding spray clusters,\n",
      "underlining the importance of both approaches for model investigation.\n",
      "we evaluate the eﬀectiveness of model corrections based on two metrics:\n",
      "the attributed fraction of relevance to artifacts and prediction performance on\n",
      "both the original and a poisoned test set (in terms of f1-score and accuracy).\n",
      "note that artifacts might\n",
      "overlap clinically informative features in poisoned samples, limiting the compa-\n",
      "rability of poisoned and original test performance.\n",
      "as shown in tab. 1 (isic\n",
      "2019) and appendix a.2 (bone age), we are generally able to improve model\n",
      "behavior with all methods.\n",
      "the only exception is the synthetic artifact for vgg-\n",
      "16, where only rrr mitigates the bias to a certain extent, indicating that\n",
      "the artifact signal is too strong for the model.\n",
      "interestingly,\n",
      "despite successfully decreasing the models’ output sensitivity towards artifacts,\n",
      "1 cdep is not applied to eﬃcientnets, as existing implementations are incompatible.\n",
      "reveal to revise: an xai life cycle for iterative bias correction of dnns\n",
      "603\n",
      "fig.\n",
      "overall, rrr yields the most consistent results, constantly reduc-\n",
      "ing the artifact relevance while increasing the model performance on poisoned\n",
      "test sets.\n",
      "604\n",
      "f. pahde et al.\n",
      "5\n",
      "conclusion\n",
      "we present r2r, an xai life cycle to reveal and revise spurious model behavior\n",
      "requiring minimal human interaction via high automation.\n",
      "to reveal model bias,\n",
      "r2r relies on crp and spray.\n",
      "moreover, crp\n",
      "is ideal for large datasets, as the concept space dimension remains constant.\n",
      "by automatically localizing artifacts, we successfully perform model revision,\n",
      "thereby reducing attention on the artifact and leading to improved performance\n",
      "on corrupted data.\n",
      "when applying r2r iteratively, we did not ﬁnd the emergence\n",
      "of new biases, which, however, might happen if larger parts of the model are ﬁne-\n",
      "tuned or retrained to correct strong biases.\n",
      "future research directions include the\n",
      "application to non-localizable artifacts, and addressing fairness issues in dnns.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf:\n",
      "interpretability is often an essential requirement in medical\n",
      "imaging.\n",
      "advanced deep learning methods are required to address this\n",
      "need for explainability and high performance.\n",
      "we propose\n",
      "an innovative solution called proto-caps that leverages the beneﬁts of\n",
      "capsule networks, prototype learning and the use of privileged informa-\n",
      "tion.\n",
      "evaluating the proposed solution on the lidc-idri dataset shows\n",
      "that it combines increased interpretability with above state-of-the-art\n",
      "prediction performance.\n",
      "keywords: explainable ai · capsule network · prototype learning\n",
      "1\n",
      "introduction\n",
      "deep learning-based systems show remarkable predictive performance in many\n",
      "computer vision tasks, including medical image analysis, and are often compa-\n",
      "rable to human performance.\n",
      "a common misconception is that the additional\n",
      "explanation comes with a decrease in performance.\n",
      "our work proves this once again by providing a powerful and explainable\n",
      "solution for medical image classiﬁcation.\n",
      "besides using\n",
      "the additional knowledge to improve performance, it can also help to increase\n",
      "explainability, as has already been shown using the lidc-idri dataset [3].\n",
      "prototype networks are another line of research implementing the idea\n",
      "that the representations of images cluster around a prototypical representation\n",
      "for each class [17].\n",
      "the goal is to ﬁnd embedded prototypes (i.e. examples) that\n",
      "best separate the images by their classes [5].\n",
      "however, these networks can only tell which prototyp-\n",
      "ical samples resemble the query image, not why.\n",
      "it is up to the user to\n",
      "guess which features of the image regions are relevant to the network and are\n",
      "exempliﬁed by the prototypes.\n",
      "our method addresses the limitations of privileged information-based and\n",
      "prototype-based explanation by combining case-based visual reasoning through\n",
      "exemplary representation of high-level attributes to achieve explainability and\n",
      "high-performance.\n",
      "the proposed method is an image classiﬁer that satisﬁes\n",
      "explainable-by-design with two elements: first, decisive intermediate results of a\n",
      "high-performance cnn are trained on human-deﬁned attributes which are being\n",
      "predicted during application.\n",
      "second, the model provides prototypical natural\n",
      "images to validate the attribute prediction.\n",
      "proto-caps: interpretable medical image classiﬁcation\n",
      "437\n",
      "the main contributions of our work are:\n",
      "– a novel method that, for the ﬁrst time to our knowledge, combines privileged\n",
      "information and prototype learning to provide increased explanatory power\n",
      "for medical classiﬁcation tasks.\n",
      "– a prototype network architecture based on a capsule network that leverages\n",
      "the beneﬁts of both techniques.\n",
      "– an explainable solution outperforming state-of-the-art explainable and non-\n",
      "explainable methods on the lidc-idri dataset.\n",
      "an attribute head is used to ensure that each\n",
      "capsule represents a single attribute, a reconstruction head learns the original\n",
      "segmentation, and the main target prediction head learns the ﬁnal classiﬁcation.\n",
      "1.\n",
      "the backbone of our approach is a capsule network consisting of three layers:\n",
      "features of the input image of size 1×32×32 are extracted by a 2d convolutional\n",
      "layer containing 256 kernels of size 9×9.\n",
      "we decided not to use 3d convolutional\n",
      "layers, as preliminary experiments showed only marginal diﬀerences (within std.\n",
      "dev. of results), but required signiﬁcantly more computing time.\n",
      "the ﬁnal dense capsule layer\n",
      "consists of one capsule for each attribute and extracts high-level features, overall\n",
      "producing eight 16-dimensional vectors.\n",
      "[11], where the distribution of radiologist malignancy annotations is opti-\n",
      "mized with the kullback-leibler divergence lmal to reﬂect the inter-observer\n",
      "agreement and thus uncertainty.\n",
      "the reconstruction branch to predict the\n",
      "segmentation mask of the nodule consists of a simple decoder with three fully\n",
      "connected layers with the output ﬁlters 512, 1024, and the size of the resulting\n",
      "image 1 × 32 × 32.\n",
      "the reconstruction loss lrecon implements the mean square\n",
      "error between the output and the binary segmentation mask.\n",
      "it has been shown\n",
      "that incorporating reconstruction learning is beneﬁcial to performance [11].\n",
      "438\n",
      "l. gallée et al.\n",
      "fig.\n",
      "for the attribute head, we propose to use fully connected layers, instead of\n",
      "determining the attribute manifestation by the length of the capsule encoding,\n",
      "as was done previously [11].\n",
      "during the training, a combined loss\n",
      "function encourages a training sample to be close to a prototype of the correct\n",
      "attribute class and away from prototypes dedicated to others, similar to existing\n",
      "approaches [6].\n",
      "proto-caps: interpretable medical image classiﬁcation\n",
      "439\n",
      "lclu = 1\n",
      "a\n",
      "a\n",
      "\u0002\n",
      "a\n",
      "min\n",
      "pj∈pas\n",
      "∥oa − pj∥2 .\n",
      "the original image of the\n",
      "training sample is stored and used for prototype visualization.\n",
      "during inference,\n",
      "the predicted attribute value is set to the ground truth attribute value of the\n",
      "closest prototype, ignoring the learned dense layers in the attribute head at this\n",
      "stage.\n",
      "+ 0.125 · (lclu + 0.1 · lsep)\n",
      "(4)\n",
      "3\n",
      "experiments\n",
      "data.\n",
      "each lung nodule with a mini-\n",
      "mum size of 3 mm was segmented and annotated with a malignancy score rang-\n",
      "ing from 1-highly unlikely to 5-highly suspicious by one to four expert raters.\n",
      "experiment designs.\n",
      "440\n",
      "l. gallée et al.\n",
      "the algorithm was implemented using the pytorch framework version 1.13 and\n",
      "cuda version 11.6.\n",
      "with a maximum of 1000 epochs, but stopping\n",
      "early if there was no improvement in target accuracy within 100 epochs, the\n",
      "experiments lasted an average of three hours on a geforce rtx 3090 graphics\n",
      "card.\n",
      "besides pure performance, the eﬀect of reduced availability of attribute anno-\n",
      "tations was investigated.\n",
      "this was done by using attribute information only for\n",
      "a randomly selected fraction of the nodules during the training.\n",
      "to investigate the eﬀect of prototypes on the network performance, an abla-\n",
      "tion study was performed.\n",
      "the respective original image for each attribute prototype is being saved dur-\n",
      "ing the training process and used for visualization during inference.\n",
      "table 1 shows the results of our experiments compared to other\n",
      "state-of-the-art approaches, with results taken from original reports.\n",
      "the experiments indicate that the performance\n",
      "of the given approach is maintained up to a fraction of 10 %.\n",
      "using no attribute\n",
      "annotations at all, i.e. no privileged information, achieves a similar performance,\n",
      "but results in a loss of explainability, as the high-level features extracted in the\n",
      "capsules are not understandable to humans.\n",
      "this result suggests that privileged\n",
      "proto-caps: interpretable medical image classiﬁcation\n",
      "441\n",
      "information here leads to an increase in interpretability for humans by providing\n",
      "attribute predictions and prototypes without interfering with the model perfor-\n",
      "mance.\n",
      "fig.\n",
      "the average diﬀerence in attribute accuracy compared to the proposed\n",
      "methods is 1.7 % and 1.5 % better, respectively, and is more robust across exper-\n",
      "iments.\n",
      "the best result was obtained when the prototypes were learned but not\n",
      "used, possibly indicating that the prototypes may have a regularising eﬀect dur-\n",
      "ing training, but further experiments are needed to conﬁrm this due to the close\n",
      "results.\n",
      "to give an indication of the decoder performance, proto-capsw/o use\n",
      "achieved a dice score of 79.7 %.\n",
      "mean μ and standard deviation σ calculated from 5-fold exper-\n",
      "iments.\n",
      "mean μ and standard deviation σ calculated\n",
      "from 5-fold experiments.\n",
      "attribute prediction accuracy in %\n",
      "malig-\n",
      "nancy\n",
      "sub\n",
      "is\n",
      "cal\n",
      "sph mar lob spic tex\n",
      "100 % attribute labels μ\n",
      "89.1 99.8 95.4 96.0 88.3 87.9 89.1 93.3\n",
      "93.0\n",
      "σ\n",
      "5.2\n",
      "0.2\n",
      "1.3\n",
      "2.2\n",
      "3.1\n",
      "0.8\n",
      "1.3\n",
      "1.0\n",
      "1.5\n",
      "10 % attribute labels μ\n",
      "92.6 99.8 95.7 94.9 90.3 88.8 86.9 92.3\n",
      "92.4\n",
      "σ\n",
      "0.9\n",
      "0.2\n",
      "0.9\n",
      "4.1\n",
      "1.6\n",
      "1.6\n",
      "2.4\n",
      "1.4\n",
      "0.8\n",
      "1 % attribute labels μ\n",
      "91.0 99.8 92.8 95.5 79.9 85.7 85.6 91.2\n",
      "90.2\n",
      "σ\n",
      "4.5\n",
      "0.2\n",
      "1.4\n",
      "2.3 13.1\n",
      "4.4\n",
      "6.8\n",
      "1.7\n",
      "1.1\n",
      "0 % attribute labels μ\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "92.4\n",
      "σ\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "–\n",
      "1.0\n",
      "5\n",
      "discussion and conclusion\n",
      "we propose a new method, named proto-caps, which combines the advantages\n",
      "of privileged information, and prototype learning for an explainable network,\n",
      "achieving more than 6 % better accuracy than the state-of-the-art explainable\n",
      "method.\n",
      "proto-caps: interpretable medical image classiﬁcation\n",
      "443\n",
      "the experiments demonstrate that it outperforms state-of-the-art methods that\n",
      "provide less explainability.\n",
      "while we did see a reduction in performance with too few labels, our\n",
      "results suggest that this is mainly due to inhomogeneous coverage of individ-\n",
      "ual attribute values.\n",
      "in this respect, it would be interesting to ﬁnd out how a\n",
      "speciﬁc selection of the annotated samples, e.g. with extremes, aﬀects the accu-\n",
      "racies, especially since our results show that the overall performance is robust\n",
      "even when the attributes are not explicitly trained, i.e. without additional priv-\n",
      "ileged information.\n",
      "in conclusion, we believe that the approach of leveraging privileged informa-\n",
      "tion with comprehensible architectures and prototype learning is promising for\n",
      "various high-risk application domains and oﬀers many opportunities for further\n",
      "research.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf:\n",
      "experiments on ﬁne-grained classiﬁ-\n",
      "cation of pathology images show that openal can signiﬁcantly improve\n",
      "the query quality of target class samples and achieve higher performance\n",
      "than current state-of-the-art al methods.\n",
      "keywords: active learning · openset · pathology image classiﬁcation\n",
      "1\n",
      "introduction\n",
      "deep learning techniques have achieved unprecedented success in the ﬁeld of\n",
      "medical image classiﬁcation, but this is largely due to large amount of annotated\n",
      "data [5,18,20].\n",
      "however, obtaining large amounts of high-quality annotated data\n",
      "is usually expensive and time-consuming, especially in the ﬁeld of pathology\n",
      "image processing [5,12–14,18].\n",
      "therefore, a very important issue is how to obtain\n",
      "the highest model performance with a limited annotation budget.\n",
      "1. description of the open-set al scenario for pathology image classiﬁcation.\n",
      "the unlabeled sample pool contains k target categories (red-boxed images) and l\n",
      "non-target categories (blue-boxed images).\n",
      "(color ﬁgure online)\n",
      "active learning (al) is an eﬀective approach to address this issue from a\n",
      "data selection perspective, which selects the most informative samples from an\n",
      "unlabeled sample pool for experts to label and improves the performance of the\n",
      "trained model with reduced labeling cost [1,2,9,10,16,17,19].\n",
      "[11]. figure 1 shows an al\n",
      "scenario for pathology image classiﬁcation in an open world, which is very com-\n",
      "mon in clinical practice.\n",
      "in this scenario, the whole slide images (wsis) are cut\n",
      "into many small patches that compose the unlabeled sample pool, where each\n",
      "patch may belong to tumor, lymph, normal tissue, fat, stroma, debris, back-\n",
      "ground, and many other categories.\n",
      "therefore, for real-world open-set pathology image classiﬁcation scenarios,\n",
      "an al method that can accurately query the most informative samples from the\n",
      "target classes is urgently needed.\n",
      "[11] proposed the ﬁrst al algorithm for open-set anno-\n",
      "tation in the ﬁeld of natural images.\n",
      "although promising performance is achieved,\n",
      "their detection of target class samples is based on the activation layer values\n",
      "of the detection network which has limited accuracy and high uncertainty with\n",
      "small initial training samples.\n",
      "in this paper, we propose a novel al framework under an open-set scenario,\n",
      "and denote it as openal, which cannot only query as many target class samples\n",
      "as possible but also query the most informative samples from the target classes.\n",
      "openal adopts an iterative query paradigm and uses a two-stage sample selec-\n",
      "tion strategy in each query.\n",
      "in the ﬁrst stage, we do not rely on a detection\n",
      "network to select target class samples and instead, we propose a feature-based\n",
      "target sample selection strategy.\n",
      "speciﬁcally, we ﬁrst train a feature extractor\n",
      "using all samples in a self-supervised learning manner, and map all samples to\n",
      "the feature space.\n",
      "in the second\n",
      "stage, we select the most informative samples from the candidate set by utilizing\n",
      "a model-based informative sample selection strategy.\n",
      "in this stage, we measure\n",
      "the uncertainty of all unlabeled samples in the candidate set using the classiﬁer\n",
      "trained with the target class samples labeled in previous iterations, and select\n",
      "the samples with the highest model uncertainty as the ﬁnal selected samples in\n",
      "this round of query.\n",
      "after the second stage, the queried samples are sent for\n",
      "annotation, which includes distinguishing target and non-target class samples\n",
      "and giving a ﬁne-grained label to every target class sample.\n",
      "we conducted two experiments with diﬀerent matching ratios (ratio of the\n",
      "number of target class samples to the total number of samples) on a public 9-class\n",
      "colorectal cancer pathology image dataset.\n",
      "the experimental results demonstrate\n",
      "that openal can signiﬁcantly improve the query quality of target class samples\n",
      "and obtain higher performance with equivalent labeling cost compared with the\n",
      "current state-of-the-art al methods.\n",
      "to the best of our knowledge, this is the\n",
      "ﬁrst open-set al work in the ﬁeld of pathology image analysis.\n",
      "2\n",
      "method\n",
      "we consider the al task for pathology image classiﬁcation in an open-set sce-\n",
      "nario.\n",
      "iterative queries are\n",
      "performed to query a ﬁxed number of samples in each iteration, and the objec-\n",
      "tive is to select as many target class samples as possible from pu in each query,\n",
      "while selecting as many informative samples as possible in the target class sam-\n",
      "ples.\n",
      "openal\n",
      "performs a total of n iterative queries, and each query is divided into two stages.\n",
      "in stage 1, openal uses a feature-based target sample selection (ftss) strategy\n",
      "to query the target class samples from the unlabeled sample pool to form a\n",
      "candidate set.\n",
      "in stage 2, openal\n",
      "adopts a model-based informative sample selection (miss) strategy.\n",
      "here, we adopt dino [3,4] as the self-supervised network because\n",
      "of its outstanding performance.\n",
      "2.3\n",
      "model-based informative sample selection\n",
      "to select the most informative samples from the candidate set, we utilize the\n",
      "model-based informative sample selection strategy in stage 2.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "dataset, settings, metrics and competitors\n",
      "to validate the eﬀectiveness of openal, we conducted two experiments with\n",
      "diﬀerent matching ratios (the ratio of the number of samples in the target class\n",
      "to the total number of samples) on a 9-class public colorectal cancer pathology\n",
      "image classiﬁcation dataset (nct-crc-he-100k)\n",
      "the dataset contains a\n",
      "total of 100,000 patches of pathology images with ﬁne-grained labeling, with\n",
      "nine categories including adipose (adi 10%), background (back 11%), debris\n",
      "(deb 11%), lymphocytes (lym 12%), mucus (muc 9%), smooth muscle (mus\n",
      "14%), normal colon mucosa (norm 9%), cancer-associated stroma (str 10%),\n",
      "and colorectal adenocarcinoma epithelium (tum, 14%).\n",
      "in the\n",
      "two experiments, we set the matching ratio to 33% (3 target classes, 6 non-target\n",
      "classes), and 42% (3 target classes, 4 non-target classes), respectively.\n",
      "metrics.\n",
      "following [11], we use three metrics, precision, recall and accuracy to\n",
      "compare the performance of each al method.\n",
      "we use precision and recall to\n",
      "measure the performance of diﬀerent methods in target class sample selection.\n",
      "we measure the ﬁnal performance of each al method using the accuracy\n",
      "of the ﬁnal classiﬁer on the test set of target class samples.\n",
      "after each query\n",
      "round, we train a resnet18 model of 100 epochs, using sgd as the optimizer\n",
      "with momentum of 0.9, weight decay of 5e-4, initial learning rate of 0.01, and\n",
      "batchsize of 128.\n",
      "for\n",
      "each method, we ran four experiments and recorded the average results for four\n",
      "randomly selected seeds.\n",
      "3.2\n",
      "performance comparison\n",
      "figure 3 a and b show the precision, recall and model accuracy of all comparing\n",
      "methods at 33% and 42% matching ratios, respectively.\n",
      "the\n",
      "inferior performance of the al methods based on the closed-set assumption\n",
      "is due to the fact that they are unable to accurately identify more target class\n",
      "samples, thus wasting a large amount of annotation budget.\n",
      "although lfosa [11]\n",
      "utilizes a dedicated network for target class sample detection, the performance of\n",
      "the detection network is not stable when the number of training samples is small,\n",
      "thus limiting its performance.\n",
      "in contrast, our method uses a novel feature-based\n",
      "target sample selection strategy and achieves the best performance.\n",
      "3. a. selection and model performance results under a 33% matching ratio.\n",
      "b.\n",
      "selection and model performance results under a 42% matching ratio.\n",
      "this severe sample\n",
      "imbalance weakens the performance of lfosa compared to random selection\n",
      "initially.\n",
      "it can be seen that the distance modeling of both the target class samples\n",
      "and the non-target class samples is essential in the ftss strategy, and missing\n",
      "either one results in a decrease in performance.\n",
      "although the miss strategy does\n",
      "not signiﬁcantly facilitate the selection of target class samples, it can eﬀectively\n",
      "help select the most informative samples among the samples in the candidate\n",
      "set, thus further improving the model performance with a limited labeling bud-\n",
      "get.\n",
      "in contrast, when the samples are selected based on uncertainty alone, the\n",
      "performance decreases signiﬁcantly due to the inability to accurately select the\n",
      "target class samples.\n",
      "the above experiments demonstrate the eﬀectiveness of\n",
      "each component of openal.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_46.pdf:\n",
      "the spatial-temporal\n",
      "deformable attention module enables deep feature aggregation in each\n",
      "stage of both encoder and decoder.\n",
      "the exper-\n",
      "iments on the public breast lesion ultrasound video dataset show that\n",
      "our stnet obtains a state-of-the-art detection performance, while oper-\n",
      "ating twice as fast inference speed.\n",
      "https://doi.org/10.1007/978-3-031-43895-0_45\n",
      "480\n",
      "c. qin et al.\n",
      "most existing breast lesion detection methods can be categorized into image-\n",
      "based [10,11,16,17,19] and video-based [1,9] breast lesion detection approaches.\n",
      "image-based breast lesion detection approaches perform detection in each frame\n",
      "independently.\n",
      "compared to image-based breast lesion detection approaches,\n",
      "methods based on videos are capable of utilizing temporal information for\n",
      "improved detection performance.\n",
      "although the recent cva-net aggregates clip and video\n",
      "level features, we distinguish two key issues that hamper its performance.\n",
      "second, cva-\n",
      "net only performs one-frame prediction based on multiple frame inputs, which\n",
      "is very time-consuming.\n",
      "to address the aforementioned issues, we propose a spatial-temporal\n",
      "deformable attention based network, named stnet, for detecting the breast\n",
      "lesions in ultrasound videos.\n",
      "we conduct extensive experiments on a public breast lesion ultra-\n",
      "sound video dataset, named bluvd-186\n",
      "the experimental results validate\n",
      "the eﬃcacy of our proposed stnet that has a superior detection performance.\n",
      "afterwards, stda performs cross-attention\n",
      "operation between these feature maps and the queries, where the key elements\n",
      "are these output feature maps of st-encoder.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "dataset and implementation details\n",
      "dataset.\n",
      "we conduct the experiments on the public bluvd-186 dataset [9],\n",
      "comprising 186 videos including 112 malignant and 74 benign cases.\n",
      "our approach achieves a superior perfor-\n",
      "mance on three diﬀerent metrics.\n",
      "method\n",
      "type\n",
      "backbone\n",
      "ap\n",
      "ap50 ap75\n",
      "gfl [7]\n",
      "image resnet-50 23.4\n",
      "46.3\n",
      "22.2\n",
      "cascade rpn\n",
      "[14]\n",
      "image resnet-50 24.8\n",
      "42.4\n",
      "27.3\n",
      "faster r-cnn\n",
      "[12]\n",
      "image resnet-50 25.2\n",
      "49.2\n",
      "22.3\n",
      "vfnet\n",
      "[20]\n",
      "image resnet-50 28.0\n",
      "47.1\n",
      "31.0\n",
      "retinanet\n",
      "[8]\n",
      "image resnet-50 29.5\n",
      "50.4\n",
      "32.4\n",
      "dff [24]\n",
      "video\n",
      "resnet-50 25.8\n",
      "48.5\n",
      "25.1\n",
      "fgfa\n",
      "three commonly-used metrics are employed for perfor-\n",
      "mance evaluation of breast lesion detection methods on the ultrasound videos,\n",
      "namely average precision (ap), ap50, and ap75.\n",
      "implementation details.\n",
      "we train the model on a single\n",
      "nvidia a100 gpu and set the batch size as 1.\n",
      "3.2\n",
      "state-of-the-art comparison\n",
      "our proposed approach is compared with eleven state-of-the-art methods, com-\n",
      "prising image-based and video-based methods.\n",
      "we report the detection perfor-\n",
      "mance of these state-of-the-art methods generated by cva-net [9].\n",
      "speciﬁcally,\n",
      "cva-net acquires the detection performance of these methods by utilizing their\n",
      "publicly available codes or re-implementing them if no publicly available codes.\n",
      "our stnet achieves improved detection performance, com-\n",
      "pared to cva-net.\n",
      "as a general trend, video-based methods tend to yield\n",
      "higher average precision (ap), ap50, and ap75 scores compared to image-based\n",
      "breast lesion detection methods.\n",
      "speciﬁcally, our stnet\n",
      "achieves a signiﬁcant improvement in the overall ap score from 36.1 to 40.0,\n",
      "the ap50 score from 65.1 to 70.3, and the ap75 score from 38.5 to 43.3.\n",
      "the\n",
      "signiﬁcant improvement demonstrates the eﬃcacy of our approach for detecting\n",
      "breast lesions in ultrasound videos.\n",
      "further, although cva-net manages to identify the breast lesions\n",
      "in the ﬁrst and ﬁfth frames, the classiﬁcation results are inaccurate (as high-\n",
      "lighted by the blue rectangle in fig. 3).\n",
      "3 accurately detects the breast lesions in all video frames and\n",
      "achieves accurate classiﬁcation performance for each frame.\n",
      "486\n",
      "c. qin et al.\n",
      "table 2. ablation study with diﬀerent design choices.\n",
      "our proposed stnet achieves\n",
      "a superior performance compared to the baseline and some diﬀerent designs.\n",
      "we present the inference speed comparison\n",
      "between our proposed stnet and cva-net on an nvidia rtx 3090 gpu\n",
      "using the same environment.\n",
      "we use fps (frames per second) as the performance\n",
      "metric.\n",
      "speciﬁcally, our proposed stnet achieves an averaged inference speed\n",
      "of 21.84 fps, while cva-net achieves an averaged speed of 12.17 fps.\n",
      "furthermore, our proposed stnet\n",
      "improves the ap by 5.1 and 4.2 compared to “st-encoder + da-decoder” and\n",
      "“da-encoder + st-decoder”, respectively, indicating that the integration of\n",
      "stda in both the encoder and decoder is crucial for achieving superior detec-\n",
      "tion performance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf:\n",
      "medical image analysis using deep learning is often challenged\n",
      "by limited labeled data and high annotation costs.\n",
      "fine-tuning the entire\n",
      "network in label-limited scenarios can lead to overﬁtting and suboptimal\n",
      "performance.\n",
      "however, previous work has overlooked the importance\n",
      "of selective labeling in downstream tasks, which aims to select the most\n",
      "valuable downstream samples for annotation to achieve the best perfor-\n",
      "mance with minimum annotation cost.\n",
      "to address this, we propose a frame-\n",
      "work that combines selective labeling with prompt tuning (slpt) to boost\n",
      "performance in limited labels.\n",
      "we evaluate our method on\n",
      "liver tumor segmentation and achieve state-of-the-art performance, out-\n",
      "performing traditional ﬁne-tuning with only 6% of tunable parameters,\n",
      "also achieving 94% of full-data performance by labeling only 5% of the data.\n",
      "keywords: active learning · prompt tuning · segmentation\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43895-0 2.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43895-0_2\n",
      "slpt: selective labeling meets prompt tuning\n",
      "15\n",
      "1\n",
      "introduction\n",
      "deep learning has achieved promising performance in computer-aided diagnosis\n",
      "[1,12,14,24], but it relies on large-scale labeled data to train, which is challenging\n",
      "in medical imaging due to label scarcity and high annotation cost [3,25]. specif-\n",
      "ically, expert annotations are required for medical data, which can be costly and\n",
      "time-consuming, especially in tasks such as 3d image segmentation.\n",
      "[5,18] is emerging from natural language processing (nlp), which introduces\n",
      "additional tunable prompt parameters to the pre-trained model and updates only\n",
      "prompt parameters using supervision signals obtained from a few downstream\n",
      "training samples while keeping the entire pre-trained unchanged.\n",
      "however, previous prompt tuning research [18,28], whether on language or\n",
      "visual models, has focused solely on the model-centric approach.\n",
      "vpt [13] explores prompt tuning with\n",
      "a vision transformer, and spm [17] attempts to handle downstream segmen-\n",
      "tation tasks through prompt tuning on cnns, which are also model-centric.\n",
      "in al, given the initial labeled\n",
      "data, the model actively selects a subset of valuable samples for labeling and\n",
      "improves performance with minimum annotation eﬀort.\n",
      "therefore, this paper proposes the ﬁrst framework for selective labeling and\n",
      "prompt tuning (slpt), combining model-centric and data-centric methods to\n",
      "improve performance in medical label-limited scenarios.\n",
      "the results\n",
      "show that slpt outperforms ﬁne-tuning with just 6% of tunable parameters and\n",
      "achieves 94% of full-data performance by selecting only 5% of labeled data.\n",
      "2\n",
      "methodology\n",
      "given a task-agnostic pre-trained model and unlabeled data for an initial med-\n",
      "ical task, we propose slpt to improve model performance.\n",
      "2.1\n",
      "prompt-based visual model\n",
      "the pre-trained model, learned by supervised or unsupervised training, is a pow-\n",
      "erful tool for improving performance on label-limited downstream tasks.\n",
      "finally, the attention output and f out\n",
      "i−1 are element-wise multiplied and\n",
      "added to obtain the updated feature fi.\n",
      "speciﬁcally, we set the foreground to 1 and the background to 0 in the ground-\n",
      "truth mask, and then average all masks and downsample to 1 × d\n",
      "2 × h\n",
      "2 × w\n",
      "2 .\n",
      "to enhance prompt diversity, we introduce a prompt diversity loss ldiv that\n",
      "regularizes the cosine similarity between the generated prompts and maximizes\n",
      "their diversity.\n",
      "to achieve this, we design k diﬀerent data augmentation, heads,\n",
      "and losses based on corresponding k prompts.\n",
      "by varying hyperparameters, we\n",
      "can achieve diﬀerent data augmentation strengths, increasing the model’s diver-\n",
      "sity and generalization.\n",
      "to achieve this, we leverage the pre-trained model\n",
      "to obtain feature representations for all unlabeled data.\n",
      "slpt: selective labeling meets prompt tuning\n",
      "19\n",
      "in the latter, we evaluate intra-prompts uncertainty by computing the mean\n",
      "prediction of the prompts and propose to estimate prompt-based gradients as\n",
      "the model’s performance depends on the update of prompt parameters θp.\n",
      "sg =\n",
      "\u0002\n",
      "θp\n",
      "||∇θp(−\n",
      "\u0002\n",
      "ymean ∗ log ymean)||2\n",
      "(6)\n",
      "to avoid manual weight adjustment, we employ multiplication instead of\n",
      "addition.\n",
      "we sort the unlabeled data by their\n",
      "corresponding s values in ascending order and select the top b data to annotate.\n",
      "3\n",
      "experiments and results\n",
      "3.1\n",
      "experimental settings\n",
      "datasets and pre-trained model.\n",
      "we conducted experiments on automating\n",
      "liver tumor segmentation in contrast-enhanced ct scans, a crucial task in liver\n",
      "cancer diagnosis and surgical planning [1].\n",
      "although there are publicly available\n",
      "liver tumor datasets [1,24], they only contain major tumor types and diﬀer in\n",
      "image characteristics and label distribution from our hospital’s data.\n",
      "we col-\n",
      "lected a dataset from our in-house hospital comprising 941 ct scans with eight\n",
      "categories: hepatocellular carcinoma, cholangioma, metastasis, hepatoblastoma,\n",
      "hemangioma, focal nodular hyperplasia, cyst, and others.\n",
      "our objective is to segment all types of lesions accurately.\n",
      "we utilized a pre-trained model for liver segmentation using supervised learning\n",
      "on two public datasets [24] with no data overlap with our downstream task.\n",
      "to evaluate the performance, we employed a 5-fold cross-\n",
      "validation (752 for selection, 189 for test).\n",
      "we evaluated lesion segmentation performance using pixel-wise and\n",
      "lesion-wise metrics.\n",
      "evaluation of diﬀerent tunings on the lesion segmentation with limited data\n",
      "(40 class-balanced patients).\n",
      "in the prompt tuning experiment, we compared\n",
      "our method with three types of tuning: full parameter update (fine-tuning,\n",
      "learn-from-scratch), partial parameter update (head-tuning, encoder-tuning,\n",
      "decoder-tuning), and prompt update (spm [17]).\n",
      "in the unsupervised diversity\n",
      "selection experiment, we compared our method with random sampling.\n",
      "in the\n",
      "supervised uncertainty selection experiment, we compared our method with ran-\n",
      "dom sampling, diversity sampling (coreset\n",
      "we conducted the experiments using the pytorch frame-\n",
      "work on a single nvidia tesla v100 gpu.\n",
      "the nnunet [12] framework was\n",
      "used for 3d lesion segmentation with training 500 epochs at an initial learn-\n",
      "ing rate of 0.01.\n",
      "during training, we set\n",
      "k = 3 and employed diverse data augmentation techniques such as scale, elas-\n",
      "tic, rotation, and mirror.\n",
      "to ensure fairness and eliminate model ensemble eﬀects, we\n",
      "only used the model’s prediction with k = 1 during testing.\n",
      "we used ﬁxed ran-\n",
      "dom seeds and 5-fold cross-validation for all segmentation experiments.\n",
      "3.2\n",
      "results\n",
      "evaluation of prompt tuning.\n",
      "using this sub-dataset, we evaluated various tuning methods for limited\n",
      "slpt: selective labeling meets prompt tuning\n",
      "21\n",
      "table 2. comparison of data selection methods for label-limited lesion segmentation.\n",
      "the pre-trained model is crucial for downstream tasks with limited data,\n",
      "as it improves performance by 9.52% compared to learn-from-scratch.\n",
      "among\n",
      "the three partial tuning methods, the number of tuning parameters positively\n",
      "correlates with the model’s performance, but they are challenging to surpass\n",
      "ﬁne-tuning.\n",
      "evaluation of selective labeling.\n",
      "as\n",
      "shown in table 2, the complete tesla achieved the best performance, outper-\n",
      "forming the version without sd by 1.84% and the version without sg by 1.98%.\n",
      "it shows that each component plays a critical role in improving performance.\n",
      "4\n",
      "conclusions\n",
      "we proposed a pipeline called slpt that enhances model performance in label-\n",
      "limited scenarios.\n",
      "slpt pipeline is a promising solution for practical medical tasks\n",
      "with limited data, providing good performance, few tunable parameters, and low\n",
      "labeling costs.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf:\n",
      "the recent develop-\n",
      "ments of artiﬁcial neural networks in computational pathology have shown that\n",
      "these methods hold great potential for improving the accuracy and quality of\n",
      "cancer diagnosis.\n",
      "herein, we propose a centroid-aware\n",
      "feature recalibration network that can conduct cancer grading in an accurate and\n",
      "robust manner.\n",
      "the proposed network maps an input pathology image into an\n",
      "embedding space and adjusts it by using centroids embedding vectors of different\n",
      "cancer grades via attention mechanism.\n",
      "equipped with the recalibrated embed-\n",
      "ding vector, the proposed network classiﬁers the input pathology image into a\n",
      "pertinent class label, i.e., cancer grade.\n",
      "we evaluate the proposed network using\n",
      "colorectal cancer datasets that were collected under different environments.\n",
      "the\n",
      "experimental results conﬁrm that the proposed network is able to conduct cancer\n",
      "grading in pathology images with high accuracy regardless of the environmental\n",
      "changes in the datasets.\n",
      "in cancer diagnosis, treatment, and management, pathology-\n",
      "driven information plays a pivotal role.\n",
      "cancer grade is, in particular, one of the major\n",
      "factors that determine the treatment options and life expectancy.\n",
      "however, the current\n",
      "pathology workﬂow is sub-optimal and low-throughput since it is, by and large, manu-\n",
      "ally conducted, and the large volume of workloads can result in dysfunction or errors in\n",
      "cancer grading, which have an adversarial effect on patient care and safety [2].\n",
      "there-\n",
      "fore, there is a high demand to automate and expedite the current pathology workﬂow\n",
      "and to improve the overall accuracy and robustness of cancer grading.\n",
      "recently, many computational tools have shown to be effective in analyzing pathol-\n",
      "ogy images [3].\n",
      "to further improve the efﬁciency and effec-\n",
      "tiveness of dcnns in pathology image analysis, advanced methods that are tailored to\n",
      "pathology images have been proposed.\n",
      "[9] proposed to re-formulate cancer\n",
      "classiﬁcation in pathology images as both categorical and ordinal classiﬁcation prob-\n",
      "lems.\n",
      "moreover, attention mechanisms have been utilized for an improved pathology\n",
      "image analysis.\n",
      "in this study, we propose a centroid-aware feature recalibration network (cafenet)\n",
      "for accurate and robust cancer grading in pathology images.\n",
      "the feature extractor is\n",
      "utilized to obtain the feature representation of pathology images.\n",
      "this indicates that the centroid embedding vectors can be used to recal-\n",
      "ibrate the input embedding vectors of pathology images.\n",
      "in\n",
      "this manner, the feature representations of the input pathology images are re-calibrated\n",
      "and stabilized for a reliable cancer classiﬁcation.\n",
      "the experimental results demonstrate\n",
      "that cafenet achieves the state-of-the-art cancer grading performance in colorectal can-\n",
      "cer grading datasets.\n",
      "1. cafenet employs a\n",
      "deep convolutional neural network as a feature extractor and an attention mechanism to\n",
      "produce robust feature representations of pathology images and conducts cancer grading\n",
      "with high accuracy.\n",
      "cafenet consists of a feature extractor, a cafe module, a cup\n",
      "module, and a classiﬁcation layer.\n",
      "2.1\n",
      "centroid-aware feature recalibration\n",
      "let {xi, yi}n\n",
      "i=1 be a set of pairs of pathology images and ground truth labels where n\n",
      "is the number of pathology image-ground truth label pairs, xi ∈ rh×w×c is the i th\n",
      "pathology image, yi ∈ {c1, . . .\n",
      "speciﬁcally, cup module adds up the embedding vectors of different class labels over\n",
      "the iterations per epoch, computes the average embedding vectors, and updates the\n",
      "centroid embedding vectors ec =\n",
      "\u0002\n",
      "ec\n",
      "j |j = 0, . . .\n",
      "efﬁcientnet-b0 is composed of\n",
      "one convolution layer and 16 stages of mobile inverted bottleneck blocks, of which each\n",
      "with a different number of layers and channels.\n",
      "264\n",
      "192\n",
      "8394\n",
      "md\n",
      "2997\n",
      "370\n",
      "738\n",
      "61985\n",
      "pd\n",
      "1391\n",
      "234\n",
      "205\n",
      "11895\n",
      "3\n",
      "experiments and results\n",
      "3.1\n",
      "datasets\n",
      "two publicly available colorectal cancer datasets [9] were employed to evaluate the\n",
      "effectiveness of the proposed cafenet.\n",
      "both\n",
      "datasets provide colorectal pathology images with ground truth labels for cancer grad-\n",
      "ing.\n",
      "the ﬁrst dataset\n",
      "includes 1600 bn, 2322 wd, 4105 md, and 1830 pd image patches that were col-\n",
      "lected between 2006 and 2008 using an aperio digital slide scanner (leica biosystems)\n",
      "at 40x magniﬁcation.\n",
      "each image patch has a spatial size of 1024 × 1024 pixels.\n",
      "the second dataset, designated as ctestii, contains 27986 bn, 8394\n",
      "wd, 61985 md, and 11985 pd image patches of size 1144 × 1144 pixels.\n",
      "3.2\n",
      "comparative experiments\n",
      "we conducted a series of comparative experiments to evaluate the effectiveness of\n",
      "cafenet for cancer grading, in comparison to several existing methods: 1) three dcnn-\n",
      "based models: resnet\n",
      "[9], which\n",
      "demonstrates the state-of-the-art performance on the two colorectal cancer datasets under\n",
      "consideration.\n",
      "3.3\n",
      "implementation details\n",
      "we initialized all models using the pre-trained weights on the imagenet dataset, and then\n",
      "trained them using the adam optimizer with default parameter values (β1= 0.9, β2 =\n",
      "0.999, ε = 1.0e-8) for 50 epochs.\n",
      "after data augmentation,\n",
      "all patches, except for those used in vit [17] and swin\n",
      "we implemented all models using the pytorch platform and trained on a workstation\n",
      "equipped with two rtx 3090 gpus.\n",
      "to increase the variability of the dataset during\n",
      "the training phase, we applied several data augmentation techniques, including afﬁne\n",
      "transformation, random horizontal and vertical ﬂip, image blurring, random gaussian\n",
      "noise, dropout, random color saturation and contrast conversion, and random contrast\n",
      "transformations.\n",
      "all these techniques were implemented using the aleju library (https://\n",
      "github.com/aleju/imgaug).\n",
      "[18]\n",
      "87.4\n",
      "0.847\n",
      "0.820\n",
      "0.832\n",
      "0.941\n",
      "mmae−ceo[9]\n",
      "87.7\n",
      "–\n",
      "–\n",
      "0.843\n",
      "0.940\n",
      "cafenet (ours)\n",
      "87.5\n",
      "0.853\n",
      "0.816\n",
      "0.832\n",
      "0.940\n",
      "3.4\n",
      "result and discussions\n",
      "we evaluated the performance of colorectal cancer grading by the proposed cafenet\n",
      "and other competing models using ﬁve evaluation metrics, including accuracy (acc),\n",
      "218\n",
      "j. lee et al.\n",
      "precision, recall, f1-score (f1), and quadratic weighted kappa (κw).\n",
      "table 2 demon-\n",
      "strates the quantitative experimental results on ctesti.\n",
      "metric learning was able to improve the classiﬁcation\n",
      "performance.\n",
      "effcientnet was the worst model among them, but with the help of triplet\n",
      "loss (triplet) or supervised contrastive loss (sc), the overall performance increased by\n",
      "≥2.8% acc, ≥0.023 precision, ≥0.001 recall, ≥0.010 f1, and ≥0.047 κw.\n",
      "among the\n",
      "transformer-based models, swin was one of the best performing models, but vit showed\n",
      "much lower performance in all evaluation metrics.\n",
      "table 3. result of colorectal cancer grading on ctestii.\n",
      "in a head-\n",
      "to-head comparison of the classiﬁcation results between ctesti and ctestii, there was\n",
      "a consistent performance drop in the proposed cafenet and other competing models.\n",
      "in regard to such differences, it is\n",
      "striking that the proposed cafenet achieved the best performance on ctestii.\n",
      "however,\n",
      "resnet, swin, and mmae−ceo showed a higher performance drop in all evaluation\n",
      "metrics.\n",
      "cafenet had a minimal performance drop except efﬁcientnet.\n",
      "efﬁcientnet,\n",
      "however, obtained poorer performance on both ctesti and ctestii.\n",
      "these results suggest\n",
      "that cafenet has the better generalizability so as to well adapt to unseen histopathology\n",
      "image data.\n",
      "we conducted ablation experiments to investigate the effect of the cafe module on\n",
      "cancer classiﬁcation.\n",
      "the exclusion of the cafe\n",
      "centroid-aware feature recalibration for cancer grading\n",
      "219\n",
      "module, i.e., efﬁcientnet, resulted in much worse performance than cafenet.\n",
      "using\n",
      "only the recalibrated embedding vectors er, a substantial drop in performance was\n",
      "observed.\n",
      "these two results indicate that the recalibrated embedding vectors complement\n",
      "to the input embedding vectors e.\n",
      "using addition, instead of concatenation, there was\n",
      "a consistent performance drop, indicating that concatenation is the superior approach\n",
      "for combining the two embedding vectors together.\n",
      "these results conﬁrm that the proposed cafenet is computational\n",
      "efﬁcient and it does not achieve its superior learning capability and generalizability at\n",
      "the expense of the model complexity.\n",
      "4\n",
      "conclusions\n",
      "herein, we propose an attention mechanism-based deep neural network, called cafenet,\n",
      "for cancer classiﬁcation in pathology images.\n",
      "in the experiments on colorectal cancer datasets against several competing mod-\n",
      "els, the proposed network demonstrated that it has a better learning capability as well as\n",
      "a generalizability in classifying pathology images into different cancer grades.\n",
      "however,\n",
      "the experiments were only conducted on two public colorectal cancer datasets from a\n",
      "single institute.\n",
      "additional experiments need to be conducted to further verify the ﬁnd-\n",
      "ings of our study.\n",
      "therefore, future work will focus on validating the effectiveness of\n",
      "the proposed network for other types of cancers and tissues in pathology images.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf:\n",
      "skin image datasets often suﬀer from imbalanced data distri-\n",
      "bution, exacerbating the diﬃculty of computer-aided skin disease diag-\n",
      "nosis.\n",
      "despite achieving signiﬁcant performance,\n",
      "these scl-based methods focus more on head classes, yet ignoring the\n",
      "utilization of information in tail classes.\n",
      "in this paper, we propose class-\n",
      "enhancement contrastive learning (ecl), which enriches the informa-\n",
      "tion of minority classes and treats diﬀerent classes equally.\n",
      "for infor-\n",
      "mation enhancement, we design a hybrid-proxy model to generate class-\n",
      "dependent proxies and propose a cycle update strategy for parameters\n",
      "optimization.\n",
      "experimental results on the clas-\n",
      "siﬁcation of imbalanced skin lesion data have demonstrated the superior-\n",
      "ity and eﬀectiveness of our method.\n",
      "keywords: contrastive learning · dermoscopic image · long-tailed\n",
      "classiﬁcation\n",
      "1\n",
      "introduction\n",
      "skin cancer is one of the most common cancers all over the world.\n",
      "serious skin\n",
      "diseases such as melanoma can be life-threatening, making early detection and\n",
      "treatment essential [3].\n",
      "as computer-aided diagnosis matures, recent advances\n",
      "with deep learning techniques such as cnns have signiﬁcantly improved the per-\n",
      "formance of skin lesion classiﬁcation [7,8].\n",
      "however, as data-hungry approaches,\n",
      "deep learning models require large balanced and high-quality datasets to meet the\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43895-0 23.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43895-0_23\n",
      "ecl: class-enhancement contrastive learning\n",
      "245\n",
      "sample\n",
      "enhanced proxy\n",
      "anchor sample\n",
      "anchor proxy\n",
      "many class\n",
      "medium class\n",
      "few class\n",
      "aggregation\n",
      "separation\n",
      "(a) supervised contrastive learning  \n",
      "(b) class-enhancement contrastive learning\n",
      "over-treatment \n",
      "equal-treatment \n",
      "fig.\n",
      "accuracy and robustness requirements in applications, which is hard to suﬃce due\n",
      "to the long-tailed occurrence of diseases in the real-world.\n",
      "thus, existing public skin\n",
      "datasets usually suﬀer from imbalanced problems which then results in class bias\n",
      "of classiﬁer, for example, poor model performance especially on tail lesion types.\n",
      "to tackle the challenge of learning unbiased classiﬁers with imbalanced data,\n",
      "many previous works focus on three main ideas, including re-sampling data [1,\n",
      "18], re-weighting loss [2,15,22] and re-balancing training strategies [10,23].\n",
      "re-\n",
      "sampling methods over-sample tail classes or under-sample head classes, re-\n",
      "weighting methods adjust the weights of losses on class-level or instance-level,\n",
      "and re-balancing methods decouple the representation learning and classiﬁer\n",
      "learning into two stages or assign the weights between features from diﬀerent\n",
      "sampling branches\n",
      "despite the great results achieved, these methods either\n",
      "manually interfere with the original data distribution or improve the accuracy\n",
      "of minority classes at the cost of reducing that of majority classes [12,13].\n",
      "[11] aggregates semantically similar samples\n",
      "and separates diﬀerent classes by training in pairs, leading to impressive success\n",
      "in long-tailed classiﬁcation of both natural and medical images [16].\n",
      "to address the above issues, we propose a class-enhancement contrastive\n",
      "learning (ecl) method for skin lesion classiﬁcation, diﬀerences between scl\n",
      "and ecl are illustrated in fig.\n",
      "(2)\n",
      "we present a balanced-hybrid-proxy loss to balance the optimization of each\n",
      "class and leverage relations among samples and proxies.\n",
      "(3) a new balanced-\n",
      "weighted cross-entropy loss is designed for an unbiased classiﬁer, which considers\n",
      "both “imbalanced data” and “imbalanced diagnosis diﬃculty”.\n",
      "(4) experimental\n",
      "results demonstrate that the proposed framework outperforms other state-of-the-\n",
      "art methods on two imbalanced dermoscopic image datasets and the ablation\n",
      "study shows the eﬀectiveness of each element.\n",
      "the two branches take in diﬀerent\n",
      "augmentations t\n",
      "i, i ∈ {1, 2} from input images x and the backbone is shared\n",
      "between branches to learn the features ˜xi, i ∈ {1, 2}.\n",
      "we use a fully connected\n",
      "layer as a logistic projection for classiﬁcation g(·) : ˜\n",
      "x → ˜y and a one-hidden\n",
      "layer mlp h(·) : ˜\n",
      "x → z ∈ rd as a sample embedding head where d denotes the\n",
      "dimension.\n",
      "l2-normalization is applied to z by using inner product as distance\n",
      "measurement in cl.\n",
      "for\n",
      "better representation, we design a cycle update strategy to optimize the proxies’\n",
      "parameters in hybrid-proxy model, together with a curriculum learning schedule\n",
      "for achieving unbiased classiﬁers.\n",
      "ecl: class-enhancement contrastive learning\n",
      "247\n",
      "training set\n",
      "sampling\n",
      "classifier\n",
      "mlp\n",
      "mlp\n",
      "logits\n",
      "backbone\n",
      "backbone\n",
      "mini-batch\n",
      "mini-batch\n",
      "hybrid-proxy model\n",
      "cycle update strategy  \n",
      "aggregation\n",
      "separation\n",
      "∙\n",
      "(∙)\n",
      "classifier branch\n",
      "contrastive learning branch\n",
      "fig.\n",
      "with such\n",
      "248\n",
      "y. zhang et al.\n",
      "algorithm 1: training process of ecl.\n",
      "input: training set x, validation set xval, training epochs e, iterations t,\n",
      "batch size b, learning rate lr, stages in balanced-weighted cross-entropy\n",
      "loss e2\n",
      "1 initialize model parameters θ and hybrid-proxy model p parameters φ\n",
      "2 for e in e do\n",
      "3\n",
      "for t in t do\n",
      "4\n",
      "getting a batch of samples\n",
      "\u0002\n",
      "x(1,2)\n",
      "− lr ∗ gradt\n",
      "θ// update parameters θ of model\n",
      "11\n",
      "φ ← φ − \u0004t\n",
      "t lr ∗ gradt\n",
      "φ // update parameters φ of p\n",
      "12\n",
      "if e > e2 then\n",
      "13\n",
      "f e = v alidate(model, xval)\n",
      "a strategy, tail proxies can be optimized in a view of whole data distribution,\n",
      "thus playing better roles in class information enhancement.\n",
      "for an anchor sample zi ∈ z in class c, we unify the positive image\n",
      "set as z+ = {zj|yj = yi = c, j ̸= i}.\n",
      "− 1\n",
      "\b\n",
      "sj∈{z+∪p+}\n",
      "log exp(si · sj/τ)\n",
      "e\n",
      "(2)\n",
      "ecl: class-enhancement contrastive learning\n",
      "249\n",
      "e =\n",
      "\b\n",
      "c∈c\n",
      "1\n",
      "2bc + n p\n",
      "c\n",
      "the average operation in the denominator of balanced-\n",
      "hybrid-proxy loss can eﬀectively reduce the gradients of the head classes, making\n",
      "an equal contribution to optimizing each class.\n",
      "moreover, as\n",
      "the skin datasets are often small, richer relations can eﬀectively help form a\n",
      "high-quality distribution in the embedding space and improve the separation of\n",
      "features.\n",
      "2.3\n",
      "balanced-weighted cross-entropy loss\n",
      "taking both “imbalanced data” and “imbalanced diagnosis diﬃculty” into con-\n",
      "sideration, we design a curriculum schedule and propose balanced-weighted\n",
      "cross-entropy loss to train an unbiased classiﬁer.\n",
      "the training phase are divided\n",
      "into three stages.\n",
      "we ﬁrst train a general classiﬁer, then in the second stage we\n",
      "assign larger weight to tail classes for “imbalanced data”.\n",
      "in the last stage, we\n",
      "utilize the results on the validation set as the diagnosis diﬃculty indicator of\n",
      "skin disease types to update the weights for “imbalanced diagnosis diﬃculty”.\n",
      "we assume\n",
      "f e\n",
      "c is the evaluation result of class c on validation set after epoch e and we use\n",
      "f1-score in our experiments.\n",
      "the network is trained for e epochs, e1 and e2 are\n",
      "hyperparameters for stages.\n",
      "3\n",
      "experiment\n",
      "3.1\n",
      "dataset and implementation details\n",
      "dataset and evaluation metrics.\n",
      "3. the results of confusion matrix illustrate that ecl obtains great performance\n",
      "on most classes especially for minority classes.\n",
      "dataset consists of 10015 images in 7 classes while a larger 2019 dataset provides\n",
      "25331 images in 8 classes.\n",
      "we adopt ﬁve metrics for evaluation: accuracy (acc), average precision (pre),\n",
      "average sensitivity (sen), macro f1-score (f1) and macro area under curve\n",
      "(auc).\n",
      "implementation details.\n",
      "the proposed algorithm is implemented in python\n",
      "with pytorch library and runs on a pc equipped with an nvidia a100 gpu.\n",
      "[9] as backbone and the embedding dimension d is set to 128.\n",
      "we use the default data augmentation\n",
      "strategy on imagenet in [9] as t1 for classiﬁcation branch.\n",
      "we conduct experiments in 3 independent\n",
      "runs and report the standard deviations in the supplementary material.\n",
      "3.2\n",
      "experimental results\n",
      "quantitative results.\n",
      "to evaluate the performance of our ecl, we compare\n",
      "our method with 10 advanced methods.\n",
      "to ensure fairness, we re-train all methods by rerun their\n",
      "released codes on our divided datasets with the same experimental settings.\n",
      "ecl: class-enhancement contrastive learning\n",
      "251\n",
      "table 1. comparison results on isic2018 and isic2019 datasets.\n",
      "it can be seen that ecl has\n",
      "a signiﬁcant advantage with the highest level in most metrics on two datasets.\n",
      "to further verify the eﬀectiveness of the designs in ecl, we\n",
      "conduct a detailed ablation study shown in table 2 (the results on isic2018 are\n",
      "shown in supplementary material table s2).\n",
      "we can see from the results that\n",
      "adding cl branch can signiﬁcantly improve the network’s data representation\n",
      "ability with better performance than only adopting a classiﬁer branch.\n",
      "and our\n",
      "bwce loss can help in learning a more unbiased classiﬁer with an improvement\n",
      "of 2.7% in f1 compared to ce in dual branch setting.\n",
      "the overall performance of the network has declined\n",
      "compared with training w/ the strategy, indicating that this strategy can bet-\n",
      "ter enhance proxies learning through the whole data distribution.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf:\n",
      "to\n",
      "achieve high accuracy, cts in diﬀerent phases are integrated to provide\n",
      "more information than single-phase images.\n",
      "we propose a hybrid model called\n",
      "transliver, which has a transformer backbone and complementary con-\n",
      "volutional modules.\n",
      "extensive experiments are conducted, in which we achieve\n",
      "an overall accuracy of 90.9% on an in-house dataset of four ct phases\n",
      "and seven liver lesion classes.\n",
      "the results also show distinct advantages\n",
      "in comparison to state-of-art approaches in classiﬁcation.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43895-0 31.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43895-0_31\n",
      "330\n",
      "x. wang et al.\n",
      "hemangioma (hh), and hepatic cyst (hc) or malignant tumors, such as intra-\n",
      "hepatic cholangiocarcinoma (icc), hepatic metastases (hm), and hepatocellular\n",
      "carcinoma (hcc).\n",
      "dynamic\n",
      "contrast-enhanced ct is a common technique for liver cancer diagnosis, where\n",
      "four diﬀerent phases of imaging, namely, non-contrast (nc), arterial (art), por-\n",
      "tal venous (pv), and delayed (dl) provide complementary information about\n",
      "the liver.\n",
      "in each image, the phase sequence from left to\n",
      "right and top to bottom is nc, art, pv, and dl, respectively.\n",
      "with the development of deep learning, computer-aided liver lesion diag-\n",
      "nosis has attracted much attention [5,8,16] in recent years.\n",
      "[8] combined liver segmentation and classiﬁcation using transfer learning and\n",
      "joint learning to increase the performance of cnn.\n",
      "as a manner to elevate the\n",
      "accuracy of cnns, frid-adar et al.\n",
      "[5] designed a gan-based network to gener-\n",
      "ate synthetic liver lesion images, improving the classiﬁcation performance based\n",
      "on cnn.\n",
      "it is reported in many studies [9,18] that using multi-phase data,\n",
      "like most professionals do in practice, can help the network get a more accu-\n",
      "rate result, which also acts in liver lesion classiﬁcation [15,23,24].\n",
      "hybrid transformer model for multi-phase liver lesion classiﬁcation\n",
      "331\n",
      "moreover, while most works have attached much importance to liver lesion seg-\n",
      "mentation\n",
      "additional\n",
      "eﬀort will be needed when consolidating segmentation and multi-phase classiﬁ-\n",
      "cation.\n",
      "self-attention based transformers [19] have shown strong capability in nat-\n",
      "ural language processing tasks.\n",
      "[4] have\n",
      "been shown to replace cnn with a transformer encoder in computer vision tasks\n",
      "and can achieve obvious advantages on large-scale datasets.\n",
      "[6], including ignoring local information within each patch, extracting only\n",
      "single-scale features, and lacking inductive bias.\n",
      "to alleviate the limitations of pure transformers,\n",
      "we propose a multi-stage pyramid structure and add convolutional layers to the\n",
      "original transformer encoder.\n",
      "we use additional cross phase tokens at the last\n",
      "stage to complete a multi-phase fusion, which can focus on cross-phase com-\n",
      "munication and improve the fusion eﬀectiveness as compared with conventional\n",
      "modes.\n",
      "as the backbone of the whole framework, transformer encoder\n",
      "employs a 4-stage pyramid structure extracting multi-scale features, with each\n",
      "stage connected by a convolutional down-sampler.\n",
      "extracted features from diﬀerent phases\n",
      "are averaged and classiﬁed by two successive fully connected networks.\n",
      "we also use auxiliary\n",
      "dice loss function between ﬁxed image lesion masks and moved image lesion\n",
      "masks to help the registration ﬁeld learning.\n",
      "in [1], the network needs to specify\n",
      "an atlas image, otherwise, pairs of images will be registered to each other.\n",
      "hybrid transformer model for multi-phase liver lesion classiﬁcation\n",
      "333\n",
      "2.2\n",
      "convolutional encoder and convolutional down-sampler\n",
      "in pure vision transformer, input images are converted to tokens by patch embed-\n",
      "ding and added with positional encoding to keep the positional information.\n",
      "for an input image x ∈ rb×h×w ×1,\n",
      "b is the batch size, and h × w is the size of the input.\n",
      "we add convolutional down-samplers between stages of transformer encoder\n",
      "so that they can produce hierarchical representation like cnn structure.\n",
      "we also utilize a convolutional\n",
      "layer with a kernel size of 2 and stride of 2, which halves the image resolution\n",
      "and doubles the number of channels.\n",
      "2.3\n",
      "single-phase liver transformer block\n",
      "vision transformers can get excellent performance on large-scale datasets such\n",
      "as imagenet\n",
      "(1)\n",
      "where q, k, v are the same with original vit, dh is the head dimension, and\n",
      "p is the relative positional encoding.\n",
      "spatial reduction sr consists of a k × k\n",
      "depthwise convolution with a stride of k and a batch normalization, where k is\n",
      "the spatial reduction ratio set in each stage.\n",
      "the ﬁrst and third convolutions are pointwise for dimension translation,\n",
      "which has a similar eﬀect to the original linear layers.\n",
      "the second convolution\n",
      "with a shortcut connection extracts local information in a higher dimension and\n",
      "improves the gradient propagation ability across layers\n",
      "the structure also\n",
      "has two gelu activation layers between convolutional layers and three batch\n",
      "normalizations after the gelus and the last convolutional layer for better per-\n",
      "formance.\n",
      "inspired by [14], in stage 4, we design a multi-phase\n",
      "liver transformer block (mpltb) for communication between phases.\n",
      "then, they are sepa-\n",
      "rated and averaged for the next layer.\n",
      "compared to the direct fusion of input images\n",
      "or output features like average and concatenation, cross phase tokens can also\n",
      "reduce fusion granularity to suﬃciently explore the relationship among phases.\n",
      "the fusion is conducted in deep\n",
      "layers because the semantic concepts are learned in higher layers which beneﬁts\n",
      "the cross phase connection.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "liver lesion classiﬁcation\n",
      "dataset.\n",
      "the employed single-phase annotated dataset is collected from sir run\n",
      "run shaw hospital (srrsh), aﬃliated with the zhejiang university school of\n",
      "medicine, and has received the ethics approval of irb.\n",
      "the collection process\n",
      "can be found in supplementary materials.\n",
      "lesions from the same patient are either\n",
      "assigned to the training and validation set or the test set, but not both.\n",
      "implementations.\n",
      "the data is augmented by ﬂip, rotation, crop, shift, and scale.\n",
      "our\n",
      "models are implemented by pytorch1.12.1 and timm0.6.13\n",
      "we\n",
      "measured performance by precision (pre.), sensitivity (sen.), speciﬁcity (spe.),\n",
      "f1-score (f1), area under the curve (auc), and accuracy (acc.).\n",
      "results.\n",
      "in the results of our method, hm has a relatively low performance\n",
      "of 62.5%, mainly due to its low proportion in our dataset.\n",
      "the details can be\n",
      "found in supplementary materials.\n",
      "considering the fairness, all the models below are initialized with pre-trained\n",
      "weights and adopt 2-d structures using the same slice-level classiﬁcation strategy.\n",
      "as illustrated in table 1, our proposed transliver model gets better performance\n",
      "than other models in all metrics.\n",
      "behind our model, cmt-s achieves the best\n",
      "performance, indicating the eﬀect of convolutional structures in transformer.\n",
      "table 1. performance of transliver and other sota classiﬁcation methods.\n",
      "[7] 71.7\n",
      "72.6\n",
      "96.1\n",
      "71.2\n",
      "92.6\n",
      "77.1\n",
      "vit-s [4]\n",
      "79.6\n",
      "79.4\n",
      "97.2\n",
      "78.6\n",
      "92.9\n",
      "82.9\n",
      "swin-s [12]\n",
      "77.7\n",
      "78.1\n",
      "97.1\n",
      "77.3\n",
      "93.8\n",
      "82.3\n",
      "cmt-s [6]\n",
      "80.5\n",
      "80.5\n",
      "97.6\n",
      "80.0\n",
      "94.1\n",
      "85.7\n",
      "transliver\n",
      "88.7 87.4 98.5 87.3 95.1\n",
      "90.9\n",
      "336\n",
      "x. wang et al.\n",
      "3.2\n",
      "ablation study\n",
      "to verify the improvement of our modules, we conduct three baseline experi-\n",
      "ments for comparison.\n",
      "a 3-d version of baseline 2 utilizing 3-d\n",
      "patch embedding is also studied in baseline 3 to validate the advantage of our\n",
      "2-d model.\n",
      "it is worth mentioning that the 2-d structure is prone to redun-\n",
      "dancy between axial slices and ignores the relation between slices compared with\n",
      "the 3-d structure but gets observably higher accuracy.\n",
      "furthermore, vision\n",
      "transformers are mostly pretrained in 2-d images, causing poor performance\n",
      "when transferring to 3-d pipeline.\n",
      "we also evaluate the model performance under diﬀerent phase combinations\n",
      "by cutting the branch of certain phases.\n",
      "it shows that information from vari-\n",
      "ous phases can signiﬁcantly inﬂuence the classiﬁcation performance.\n",
      "figure 4 contains average results\n",
      "of phase number and details with all phase combinations can be found in sup-\n",
      "plementary materials.\n",
      "we report per-\n",
      "formance of an overall 90.9% classiﬁcation accuracy on a four-phase seven-class\n",
      "dataset through quantitative experiments and show obvious improvement com-\n",
      "pared with sota classiﬁcation methods.\n",
      "in future work, we will extend classi-\n",
      "ﬁcation to instance segmentation and provide an end-to-end eﬀective model for\n",
      "liver lesion diagnosis.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf:\n",
      "colonoscopy analysis, particularly automatic polyp segmen-\n",
      "tation and detection, is essential for assisting clinical diagnosis and treat-\n",
      "ment.\n",
      "however, as medical image annotation is labour- and resource-\n",
      "intensive, the scarcity of annotated data limits the eﬀectiveness and gen-\n",
      "eralization of existing methods.\n",
      "although recent research has focused on\n",
      "data generation and augmentation to address this issue, the quality of the\n",
      "generated data remains a challenge, which limits the contribution to the\n",
      "performance of subsequent tasks.\n",
      "inspired by the superiority of diﬀusion\n",
      "models in ﬁtting data distributions and generating high-quality data,\n",
      "in this paper, we propose an adaptive reﬁnement semantic diﬀusion\n",
      "model (arsdm) to generate colonoscopy images that beneﬁt the down-\n",
      "stream tasks.\n",
      "speciﬁcally, arsdm utilizes the ground-truth segmentation\n",
      "mask as a prior condition during training and adjusts the diﬀusion loss\n",
      "for each input according to the polyp/background size ratio.\n",
      "further-\n",
      "more, arsdm incorporates a pre-trained segmentation model to reﬁne\n",
      "the training process by reducing the diﬀerence between the ground-truth\n",
      "mask and the prediction mask.\n",
      "extensive experiments on segmentation\n",
      "and detection tasks demonstrate the generated data by arsdm could\n",
      "signiﬁcantly boost the performance of baseline methods.\n",
      "keywords: diﬀusion models · colonoscopy · polyp segmentation ·\n",
      "polyp detection\n",
      "y. du and y. jiang—equal contributions.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43895-0_32.\n",
      "deep learning methods have shown powerful abilities in\n",
      "automatic colonoscopy analysis, including polyp segmentation [5,22,26,27,29]\n",
      "and polyp detection\n",
      "however, the scarcity of annotated data due to high\n",
      "manual annotation costs results in poorly trained and low generalizable models.\n",
      "[9,25]\n",
      "or data augmentation methods [3,13,28] to enhance learning features, but\n",
      "these methods yielded limited improvements in downstream tasks.\n",
      "gt masks\n",
      "original images\n",
      "synthesis images\n",
      "combine\n",
      "segmentation\n",
      "detection\n",
      "downstream tasks\n",
      "… …\n",
      "arsdm\n",
      "e.g.\n",
      "e.g.\n",
      "diffusion sampler\n",
      "fig.\n",
      "2.\n",
      "despite recent progress in these methods for medical image analysis, existing\n",
      "models face two major challenges when applied to colonoscopy image analysis.\n",
      "firstly, the foreground (polyp) of colonoscopy images contains rich pathological\n",
      "information yet is often tiny compared with the background (intestine wall) and\n",
      "can be easily overwhelmed during training.\n",
      "thus, naive generative models may\n",
      "generate realistic colonoscopy images but those images seldom contain polyp\n",
      "regions.\n",
      "in addition, in order to generate high-quality annotated samples, it is\n",
      "crucial to maintain the consistency between the polyp morphologies in synthe-\n",
      "sized images and the original masks, which current generative models struggle\n",
      "to achieve.\n",
      "to tackle these issues and inspired by the remarkable success achieved by dif-\n",
      "fusion models in generating high-quality ct or mri data [8,11,23], we creatively\n",
      "propose an eﬀective adaptive reﬁnement semantic diﬀusion model (arsdm) to\n",
      "generate polyp-contained colonoscopy images while preserving the original anno-\n",
      "tations.\n",
      "speciﬁcally, we use the original segmentation masks as condi-\n",
      "tions to train a conditional diﬀusion model, which makes the generated sam-\n",
      "ples share the same masks with the input images.\n",
      "in addition,\n",
      "we ﬁne-tune the diﬀusion model by minimizing the distance between the original\n",
      "ground truth masks and the prediction masks from synthesis images via a pre-\n",
      "trained segmentation network.\n",
      "in summary, our contributions are three-fold: (1) adaptive reﬁnement\n",
      "sdm: based on the standard semantic diﬀusion model [21], we propose a novel\n",
      "arsdm with the adaptive loss re-weighting and the prediction-guided sample\n",
      "reﬁnement mechanisms, which is capable of generating realistic polyp-contained\n",
      "colonoscopy images while preserving the original annotations.\n",
      "to the best of our\n",
      "knowledge, this is the ﬁrst work for adapting diﬀusion models to colonoscopy\n",
      "image synthesis.\n",
      "(2) large-scale colonoscopy generation: the proposed\n",
      "approach can be used to generate large-scale datasets with no/arbitrary anno-\n",
      "tations, which signiﬁcantly beneﬁts the medical image society, laying the foun-\n",
      "dation for large-scale pre-training models in automatic colonoscopy analysis.\n",
      "(3)\n",
      "qualitative and quantitative evaluation: we conduct extensive experi-\n",
      "ments to evaluate our method on ﬁve public benchmarks for polyp segmentation\n",
      "and detection.\n",
      "the results demonstrate that our approach could help deep learn-\n",
      "ing methods achieve better performances.\n",
      "= n\n",
      "\u0003\n",
      "xt;\n",
      "\u0004\n",
      "1 − βtxt−1, βti\n",
      "\u0005\n",
      ",\n",
      "(1)\n",
      "where q (x0) is the original data distribution with x0 ∼ q (x0), x1:t are latents\n",
      "with the same dimension of x0 and βt is a variance schedule.\n",
      "(3)\n",
      "342\n",
      "y. du et al.\n",
      "diffusion\n",
      "process\n",
      "sampler\n",
      "u-net\n",
      "re-weighting\n",
      "module\n",
      "diffusion loss\n",
      "ℒ\n",
      "refinement loss\n",
      "ℒ\n",
      "weights map \n",
      "pranet\n",
      "condition\n",
      "input \n",
      "gt mask\n",
      "sample  \n",
      "prediction mask  \n",
      "noised\n",
      "estimated  \n",
      " \n",
      "fig.\n",
      "in this paper, we propose an adaptive reﬁnement semantic diﬀusion model,\n",
      "a variant of ddpm, which has three key parts, i.e., mask conditioning, adaptive\n",
      "loss re-weighting, and prediction-guided sample reﬁnement.\n",
      "2.\n",
      "2.1\n",
      "mask conditioning\n",
      "unlike the previous generative methods, our work aims to generate a synthetic\n",
      "image with an identical segmentation mask to the original annotation.\n",
      "speciﬁcally, for an input\n",
      "image x0 ∈\n",
      "\u0003\n",
      "+ σiz\n",
      "4 end for\n",
      "5 ˜c0 = p(˜x0)\n",
      "6 take gradient descent step on ∇θltotal\n",
      "2.2\n",
      "adaptive loss re-weighting\n",
      "the polyp regions in the colonoscopy images diﬀer from the background regions,\n",
      "which contain more pathological information and should be adequately treated\n",
      "to learn a better model.\n",
      "(8)\n",
      "2.3\n",
      "prediction-guided sample reﬁnement\n",
      "the downstream tasks of polyp segmentation and detection require rich semantic\n",
      "information on polyp regions to train a good model.\n",
      "through extensive exper-\n",
      "iments, we found inaccurate sample images with coarse polyp boundary that\n",
      "is not aligned properly with the original masks may introduce large biases and\n",
      "noises to the datasets.\n",
      "the model can be confused by several conﬂicting training\n",
      "images with the same annotation.\n",
      "to this end, we design a reﬁnement strategy\n",
      "344\n",
      "y. du et al.\n",
      "table 1. comparisons of diﬀerent settings applied on three polyp segmentation base-\n",
      "lines.\n",
      "methods endoscene\n",
      "clinicdb\n",
      "kvasir\n",
      "colondb\n",
      "etis\n",
      "overall\n",
      "mdice miou mdice miou mdice miou mdice miou mdice miou mdice miou\n",
      "pranet\n",
      "87.1\n",
      "79.7\n",
      "89.9\n",
      "84.9\n",
      "89.8\n",
      "84.0\n",
      "70.9\n",
      "64.0\n",
      "62.8\n",
      "56.7\n",
      "74.0\n",
      "67.5\n",
      "+ldm\n",
      "83.7\n",
      "76.9\n",
      "88.2\n",
      "83.5\n",
      "88.4\n",
      "83.0\n",
      "62.6\n",
      "56.0\n",
      "56.2\n",
      "50.3\n",
      "67.8\n",
      "61.7\n",
      "+sdm\n",
      "89.9\n",
      "83.2\n",
      "89.2\n",
      "83.7\n",
      "88.4\n",
      "82.6\n",
      "74.2\n",
      "66.5\n",
      "66.4\n",
      "60.3\n",
      "76.4\n",
      "69.6\n",
      "+ours\n",
      "89.7\n",
      "82.7\n",
      "93.3\n",
      "88.5\n",
      "89.9\n",
      "84.5\n",
      "76.1\n",
      "68.9\n",
      "75.5\n",
      "68.1\n",
      "80.0\n",
      "73.2\n",
      "sanet\n",
      "88.8\n",
      "81.5\n",
      "91.6\n",
      "85.9\n",
      "90.4\n",
      "84.7\n",
      "75.3\n",
      "67.0\n",
      "75.0\n",
      "65.4\n",
      "79.4\n",
      "71.4\n",
      "+ldm\n",
      "72.7\n",
      "60.5\n",
      "88.8\n",
      "82.8\n",
      "88.7\n",
      "82.7\n",
      "64.3\n",
      "55.4\n",
      "58.0\n",
      "49.2\n",
      "68.3\n",
      "59.8\n",
      "+sdm\n",
      "90.2\n",
      "83.0\n",
      "89.9\n",
      "84.1\n",
      "90.9\n",
      "85.4\n",
      "77.6\n",
      "69.3\n",
      "74.7\n",
      "66.8\n",
      "80.4\n",
      "72.9\n",
      "+ours\n",
      "90.2\n",
      "83.2\n",
      "91.4\n",
      "86.1\n",
      "91.1\n",
      "85.6\n",
      "77.7\n",
      "70.0\n",
      "78.0\n",
      "69.5\n",
      "81.5\n",
      "74.1\n",
      "pvt\n",
      "90.0\n",
      "83.3\n",
      "93.7\n",
      "88.9\n",
      "91.7\n",
      "86.4\n",
      "80.8\n",
      "72.7\n",
      "78.7\n",
      "70.6\n",
      "83.3\n",
      "76.0\n",
      "+ldm\n",
      "88.2\n",
      "81.2\n",
      "92.3\n",
      "87.1\n",
      "91.2\n",
      "85.7\n",
      "78.7\n",
      "70.4\n",
      "78.0\n",
      "69.6\n",
      "81.9\n",
      "74.2\n",
      "+sdm\n",
      "88.8\n",
      "81.7\n",
      "93.9\n",
      "89.2\n",
      "91.2\n",
      "86.1\n",
      "81.3\n",
      "73.5\n",
      "78.7\n",
      "71.1\n",
      "83.4\n",
      "76.3\n",
      "+ours\n",
      "88.2\n",
      "81.2\n",
      "92.2\n",
      "87.5\n",
      "91.5\n",
      "86.3\n",
      "81.7\n",
      "73.8\n",
      "80.6\n",
      "72.9\n",
      "84.0\n",
      "76.7\n",
      "that uses the prediction of a pre-trained segmentation model on the sampled\n",
      "images to guide the training process and restore the proper polyp boundary\n",
      "information.\n",
      "speciﬁcally, at each iteration of training, the output ˜ϵ = ϵθ (xt, t, c0)\n",
      "will go into the sampler to generate sample image ˜x0.\n",
      "then, we take the sample\n",
      "image as the input of the segmentation model to predict the pseudo masks ˜c0.\n",
      "we propose the following reﬁnement loss based on iou loss and binary cross\n",
      "entropy (bce) loss between ˜c0 and c0.\n",
      "the reﬁnement loss is:\n",
      "lreﬁne = l(c, ˜cg) +\n",
      "i=5\n",
      "\f\n",
      "i=3\n",
      "l ( ˜ci) ,\n",
      "˜c0 = { ˜c3, ˜c4, ˜c5, ˜cg} = p (s (˜ϵ)) ,\n",
      "(9)\n",
      "where l = liou + lbce is the sum of the iou loss and bce loss, ˜c0 is the\n",
      "collection of the three side-outputs ( ˜c3, ˜c4, ˜c5) and the global map ˜cg as described\n",
      "in [5]. p(·) represents the pranet model and s(·) is the ddim [16] sampler.\n",
      "(10)\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "arsdm experimental settings\n",
      "we conducted our experiments on ﬁve public polyp segmentation datasets:\n",
      "endoscene\n",
      "following the standard of pranet, 1,450 image-mask pairs from kvasir\n",
      "and cvc-clinicdb are taken as the training set.\n",
      "the training image-mask pairs are padded to have the same height and\n",
      "width and then resized to the size of 384 × 384.\n",
      "experiments with prediction-\n",
      "guided sample reﬁnement are trained with around one-half nvidia a100 days,\n",
      "while others are trained with approximately one day for convergence.\n",
      "we use\n",
      "the ddim sampler with a maximum timestep of 200 for sampling images.\n",
      "3.2\n",
      "downstream experimental settings\n",
      "we conduct the evaluation of our methods and the state-of-the-art counterparts\n",
      "on polyp segmentation and detection tasks.\n",
      "[22], and polyp-pvt [2] as baseline segmentation models with default set-\n",
      "tings, and evaluated them using mean intersection over union (iou) and mean\n",
      "dice metrics.\n",
      "evaluated them using average precision (ap) and f1-scores.\n",
      "3.3\n",
      "quantitative comparisons\n",
      "the experimental results presented in table 1 and 2 demonstrate the eﬀective-\n",
      "ness of our proposed method in training better downstream models to achieve\n",
      "superior performance.\n",
      "speciﬁcally, data generated by our approach assists the\n",
      "346\n",
      "y. du et al.\n",
      "table 3. ablation study of diﬀerent com-\n",
      "ponents on polyp segmentation tasks.\n",
      "ap\n",
      "f1\n",
      "ap\n",
      "f1\n",
      "✗\n",
      "✗\n",
      "61.8\n",
      "79.1\n",
      "65.2\n",
      "76.7\n",
      "✓\n",
      "✗\n",
      "62.2\n",
      "80.1\n",
      "65.8\n",
      "77.2\n",
      "✗\n",
      "✓\n",
      "64.0\n",
      "80.4\n",
      "66.0\n",
      "77.6\n",
      "✓\n",
      "✓\n",
      "65.7 81.3 66.4 79.0\n",
      "ori. images\n",
      "masks\n",
      "samples\n",
      "fig.\n",
      "3. illustration of generated samples with the corresponding masks and original\n",
      "images for comparison reference.\n",
      "signiﬁcant improvements for each model in mdice and miou, with increases of\n",
      "6.0% and 5.7% over pranet, 2.1% and 2.7% over sanet, and 0.7% and 0.7% over\n",
      "polyp-pvt.\n",
      "moreover, we\n",
      "conducted a comprehensive comparison with sota models, noting that these\n",
      "models were not speciﬁcally designed for colonoscopy images and may generate\n",
      "data that hinder the training process or lack the ability for eﬀective improvement.\n",
      "nevertheless, our experimental results conﬁrm the superiority of our proposed\n",
      "method.\n",
      "ablation study.\n",
      "the results demonstrate both components contribute to the accuracy\n",
      "improvement of baseline models, indicating their essential roles in achieving the\n",
      "best ﬁnal performance.\n",
      "3.4\n",
      "qualitative analyses\n",
      "to further investigate the generative performance of our approach, we present\n",
      "visualization results in fig.\n",
      "3, which displays the generated samples and their\n",
      "corresponding masks, alongside the original images for reference.\n",
      "the gener-\n",
      "ated samples demonstrate diﬀerences from the original images in both the polyp\n",
      "arsdm\n",
      "347\n",
      "regions and the backgrounds while maintaining alignment with the masks.\n",
      "addi-\n",
      "tionally, we sought evaluations from medical professionals to assess the authen-\n",
      "ticity of the generated samples, and non-medical professionals to locate polyps\n",
      "in the images, which yielded positive feedback on the quality of the generated\n",
      "samples.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf:\n",
      "the presence of corrupted labels is a common problem in the\n",
      "medical image datasets due to the diﬃculty of annotation.\n",
      "meanwhile,\n",
      "corrupted labels might signiﬁcantly deteriorate the performance of deep\n",
      "neural networks (dnns), which have been widely applied to medical\n",
      "image analysis.\n",
      "extensive experiments on three popular med-\n",
      "ical image datasets demonstrate the superior performance of our frame-\n",
      "work over recent state-of-the-art methods.\n",
      "however, either of them is very\n",
      "diﬃcult to be obtained for conducting medical image analysis with dnns.\n",
      "in\n",
      "particular, obtaining high-quality labels needs professional experience so that\n",
      "corrupted labels can often be found in medical datasets, which can seriously\n",
      "degrade the eﬀectiveness of medical image analysis.\n",
      "based on whether correcting corrupted labels, previous methods can be roughly\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1_16.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "robustness-based methods are designed to utilize vari-\n",
      "ous techniques, such as dropout, augmentation and loss regularization, to avoid\n",
      "the adverse impact of corrupted labels, thereby outputting a robust model.\n",
      "for example, co-correction [9] simultaneously trains two models and cor-\n",
      "rects labels for medical image analysis, and lcc\n",
      "to address the aforementioned issues, in this paper, we propose a new\n",
      "co-assistant framework, namely co-assistant networks for label correction\n",
      "(cnlc) (shown in fig.\n",
      "first, we propose a new label correction method (i.e., a co-assistant framework)\n",
      "to boost the model robustness for medical image analysis by two sequential\n",
      "modules.\n",
      "speciﬁcally, the samples with n1 smallest losses\n",
      "are regarded as clean samples and the samples with n1 largest loss values are\n",
      "regarded as corrupted samples, where n1 is experimentally set as 5.0% of all\n",
      "training sample for each class.\n",
      "in particular, based on the partition mentioned in the above section, the\n",
      "clean samples within the same class should have the same label and the cor-\n",
      "rupted samples should have diﬀerent labels from clean samples within the same\n",
      "class.\n",
      "we list the optimization\n",
      "details of our proposed algorithm in the supplemental materials.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "experimental settings\n",
      "the used datasets are breakhis\n",
      "breakhis\n",
      "consists of 7,909 breast cancer histopathological images including 2,480 benigns\n",
      "and 5,429 malignants.\n",
      "we randomly select 5,537 images for training and 2,372\n",
      "ones for testing.\n",
      "isic has 12,000 digital skin images where 6,000 are normal and\n",
      "6,000 are with melanoma.\n",
      "nihcc has 10,280 frontal-view x-ray images,\n",
      "where 5,110 are normal and 5,170 are with lung diseases.\n",
      "we randomly select\n",
      "8,574 images for training and the rest of images for testing.\n",
      "in particular, the\n",
      "random selection in our experiments guarantees that three datasets (i.e., the\n",
      "training set, the testing set, and the whole set) have the same ratio for each\n",
      "co-assistant networks for label correction\n",
      "165\n",
      "table 1.\n",
      "the classiﬁcation results (average ± std) on three datasets.\n",
      "we compare our proposed method with six popular methods, including one\n",
      "fundamental baseline (i.e., cross-entropy (ce)), three robustness-based meth-\n",
      "ods (i.e., co-teaching (ct) [6], nested co-teaching (nct)\n",
      "for fair-\n",
      "ness, in our experiments, we adopt the same neural network for all comparison\n",
      "methods based on their public codes and default parameter settings.\n",
      "due\n",
      "to the space limitation, we present the results at ϵ = 0.0 of all methods in the\n",
      "supplemental materials.\n",
      "the classiﬁcation results (average ± std) of the ablation study on isic.\n",
      "for example, our method on average improves by 2.4% and 15.3%,\n",
      "respectively, compared to the best comparison method (i.e., ct) and the worst\n",
      "comparison method (i.e., ce), on all cases.\n",
      "second, all methods out-\n",
      "perform the fundamental baseline (i.e., ce) on all cases.\n",
      "addi-\n",
      "tionally, cnlc obtains better performance than mlp because it considers the\n",
      "relationship among samples.\n",
      "both of the above observations verify the conclusion\n",
      "mentioned in the last section again.\n",
      "to verify the eﬀectiveness of the resistance loss in eq.\n",
      "(1) for medical image analysis, which has been theoretically and experimen-\n",
      "tally veriﬁed in the application of natural images [12].\n",
      "co-assistant networks for label correction\n",
      "167\n",
      "4\n",
      "conclusion\n",
      "in this paper, we proposed a novel co-assistant framework, to solve the prob-\n",
      "lem of dnns with corrupted labels for medical image analysis.\n",
      "experiments\n",
      "on three medical image datasets demonstrate the eﬀectiveness of the proposed\n",
      "framework.\n",
      "although our method has achieved promising performance, its accu-\n",
      "racy might be further boosted by using more powerful feature extractors, like\n",
      "pre-train models on large-scale public datasets or some self-supervised methods,\n",
      "e.g., contrastive learning.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf:\n",
      "in this work, we propose a few-shot colorectal tissue image\n",
      "generation method for addressing the scarcity of histopathological train-\n",
      "ing data for rare cancer tissues.\n",
      "our few-shot generation method, named\n",
      "xm-gan, takes one base and a pair of reference tissue images as input\n",
      "and generates high-quality yet diverse images.\n",
      "within our xm-gan, a\n",
      "novel controllable fusion block densely aggregates local regions of refer-\n",
      "ence images based on their similarity to those in the base image, resulting\n",
      "in locally consistent features.\n",
      "to the best of our knowledge, we are the ﬁrst\n",
      "to investigate few-shot generation in colorectal tissue images.\n",
      "we evaluate\n",
      "our few-shot colorectral tissue image generation by performing extensive\n",
      "qualitative, quantitative and subject specialist (pathologist) based evalu-\n",
      "ations.\n",
      "speciﬁcally, in specialist-based evaluation, pathologists could dif-\n",
      "ferentiate between our xm-gan generated tissue images and real images\n",
      "only 55% time.\n",
      "moreover, we utilize these generated images as data aug-\n",
      "mentation to address the few-shot tissue image classiﬁcation task, achiev-\n",
      "ing a gain of 4.4% in terms of mean accuracy over the vanilla few-shot clas-\n",
      "siﬁer.\n",
      "keywords: few-shot image generation · cross modulation\n",
      "1\n",
      "introduction\n",
      "histopathological image analysis is an important step towards cancer diagno-\n",
      "sis.\n",
      "however, shortage of pathologists worldwide along with the complexity of\n",
      "histopathological data make this task time consuming and challenging.\n",
      "there-\n",
      "fore, developing automatic and accurate histopathological image analysis meth-\n",
      "ods that leverage recent progress in deep learning has received signiﬁcant atten-\n",
      "tion in recent years.\n",
      "in this work, we investigate the problem of diagnosing\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1_13.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43898-1_13\n",
      "few-shot image generation for colorectal tissue classiﬁcation\n",
      "129\n",
      "colorectal cancer, which is one of the most common reason for cancer deaths\n",
      "around the world and particularly in europe and america [23].\n",
      "existing deep learning-based colorectal tissue classiﬁcation methods [18,21,22]\n",
      "typically require large amounts of annotated histopathological training data for all\n",
      "tissue types to be categorized.\n",
      "to this end, it is desirable to develop\n",
      "a few-shot colorectal tissue classiﬁcation method, which can learn from seen tissue\n",
      "classes having suﬃcient training data, and be able to transfer this knowledge to\n",
      "unseen (novel) tissue classes having only a few exemplar training images.\n",
      "[6] have been utilized to syn-\n",
      "thesize images, they typically need to be trained using large amount of real\n",
      "images of the respective classes, which is not feasible in aforementioned few-shot\n",
      "setting.\n",
      "therefore, we propose a few-shot (fs) image generation approach for\n",
      "generating high-quality and diverse colorectal tissue images of novel classes using\n",
      "limited exemplars.\n",
      "moreover, we demonstrate the applicability of these generated\n",
      "images for the challenging problem of fs colorectal tissue classiﬁcation.\n",
      "contributions: we propose a few-shot colorectal tissue image generation\n",
      "framework, named xm-gan, which simultaneously focuses on generating high-\n",
      "quality yet diverse images.\n",
      "within our tissue image generation framework, we\n",
      "introduce a novel controllable fusion block (cfb) that enables a dense aggrega-\n",
      "tion of local regions of the reference tissue images based on their congruence to\n",
      "those in the base tissue image.\n",
      "our cfb employs a cross-attention based feature\n",
      "aggregation between the base (query) and reference (keys, values) tissue image\n",
      "features.\n",
      "consequently, colorectal tissue images are generated with reduced artifacts.\n",
      "to further enhance the diversity and quality of the generated tissue images,\n",
      "we introduce a mapping network along with a controllable cross-modulated layer\n",
      "normalization (cln) within our cfb.\n",
      "our mapping network generates ‘meta-\n",
      "weights’ that are a function of the global-level features of the reference tissue\n",
      "image and the control parameters.\n",
      "this enables\n",
      "the cross-attended tissue image features to be re-weighted and enriched in a\n",
      "controllable manner, based on the reference tissue image features and associated\n",
      "control parameters.\n",
      "consequently, it results in improved diversity of the tissue\n",
      "images generated by our transformer-based framework (see fig. 3).\n",
      "we validate our xm-gan on the fs colorectral tissue image generation task\n",
      "by performing extensive qualitative, quantitative and subject specialist (pathol-\n",
      "ogist) based evaluations.\n",
      "our xm-gan generates realistic and diverse colorectal\n",
      "tissue images (see fig.\n",
      "in our subject specialist (pathologist) based evalua-\n",
      "tion, pathologists could diﬀerentiate between our xm-gan generated colorec-\n",
      "tral tissue images and real images only 55% time.\n",
      "furthermore, we evaluate the\n",
      "eﬀectiveness of our generated tissue images by using them as data augmentation\n",
      "during training of fs colorectal tissue image classiﬁer, leading to an absolute gain\n",
      "of 4.4% in terms of mean classiﬁcation accuracy over the vanilla fs classiﬁer.\n",
      "130\n",
      "a. kumar et al.\n",
      "2\n",
      "related work\n",
      "the ability of generative models [6,15] to ﬁt to a variety of data distributions\n",
      "has enabled great strides of advancement in tasks, such as image generation [3,\n",
      "12,13,19], and so on.\n",
      "in contrast, few-\n",
      "shot (fs) image generation approaches [2,4,7,9,16] strive to generate natural\n",
      "images from disjoint novel categories from the same domain as in the training.\n",
      "existing fs natural image generation approaches can be broadly divided into\n",
      "three categories based on transformation [1], optimization\n",
      "the transformation-based approach learns to perform generalized data\n",
      "augmentations to generate intra-class images from a single conditional image.\n",
      "on\n",
      "the other hand, optimization-based approaches typically utilize meta-learning\n",
      "techniques to adapt to a diﬀerent image generation task by optimizing on a few\n",
      "reference images from the novel domain.\n",
      "diﬀerent from these two paradigms\n",
      "that are better suited for simple image generation task, fusion-based approaches\n",
      "ﬁrst aggregate latent features of reference images and then employ a decoder to\n",
      "generate same class images from these aggregated features.\n",
      "our approach: while the aforementioned works explore fs generation in nat-\n",
      "ural images, to the best of our knowledge, we are the ﬁrst to investigate fs gener-\n",
      "ation in colorectal tissue images.\n",
      "generating\n",
      "colorectal tissue images of these diverse categories is a challenging task, espe-\n",
      "cially in the fs setting.\n",
      "generating realistic and diverse tissue images require\n",
      "ensuring both global and local texture consistency (patterns).\n",
      "[5,20] from all relevant local regions of the reference\n",
      "tissue images at a global-receptive ﬁeld along with a controllable mechanism for\n",
      "modulating the tissue image features by utilizing meta-weights computed from\n",
      "the input reference tissue image features.\n",
      "as a result, this leads to high-quality\n",
      "yet diverse colorectal tissue image generation in fs setting.\n",
      "3\n",
      "method\n",
      "problem formulation: in our few-shot colorectal tissue image generation\n",
      "framework, the goal is to generate diverse set of images from k input exam-\n",
      "ples x of a unseen (novel) tissue classes.\n",
      "let ds and du be the set of seen and\n",
      "unseen classes, respectively, where ds ∩du = ∅. in the training stage, we sample\n",
      "images from ds and train the model to learn transferable generation ability to\n",
      "produce new tissue images for unseen classes.\n",
      "during inference, given k images\n",
      "from an unseen class in du, the trained model strives to produce diverse yet\n",
      "plausible images for this unseen class without any further ﬁne-tuning.\n",
      "few-shot image generation for colorectal tissue classiﬁcation\n",
      "131\n",
      "fig.\n",
      "our xm-gan comprises a cnn encoder, a transformer-based controllable\n",
      "fusion block (cfb), and a cnn decoder for tissue image generation.\n",
      "for k-shot set-\n",
      "ting, a shared encoder fe takes a base tissue image xb\n",
      "along with k−1 reference\n",
      "tissue images {xref\n",
      "i\n",
      "}k−1\n",
      "i=1\n",
      "and outputs visual features hb and {href\n",
      "i\n",
      "}k−1\n",
      "i=1 , respectively.\n",
      "the\n",
      "cross-attended features fi are fused and input to a decoder fd that generates an image\n",
      "ˆx.\n",
      "overall architecture: figure 1 shows the overall architecture of our proposed\n",
      "framework, xm-gan.\n",
      "here, we randomly assign a tissue image from x as a base\n",
      "image xb, and denote the remaining k−1 tissue images as reference {xref\n",
      "i\n",
      "}k−1\n",
      "i=1 .\n",
      "given the input images x, we obtain feature representation of the base tis-\n",
      "sue image and each reference tissue image by passing them through the shared\n",
      "encoder fe.\n",
      "the resulting\n",
      "fused representation f is input to a decoder fd to generate tissue image ˆx.\n",
      "here, the cross-transformer\n",
      "is based on multi-headed cross-attention mechanism that densely aggregates rel-\n",
      "evant input image features, based on pairwise attention scores between each posi-\n",
      "tion in the base tissue image with every region of the reference tissue image.\n",
      "2. cross-attending the base\n",
      "and reference tissue image features\n",
      "using controllable cross-modulated\n",
      "layer norm (cln) in our cfb.\n",
      "as a result of this control-\n",
      "lable feature modulation, the out-\n",
      "put features fi enable the gen-\n",
      "eration of tissue images that are\n",
      "diverse yet aligned with the seman-\n",
      "tics of the input tissue images.\n",
      "next, we introduce a controllable fea-\n",
      "ture modulation mechanism in our cross-\n",
      "transformer to further enhance the diversity\n",
      "and quality of generated images.\n",
      "controllable feature modulation: the\n",
      "standard cross-attention mechanism, described\n",
      "above, computes locally consistent features\n",
      "that generate images with reduced artifacts.\n",
      "however, given the deterministic nature of the\n",
      "cross-attention and the limited set of reference\n",
      "images, simultaneously generating diverse and\n",
      "high-quality images in the few-shot setting is\n",
      "still a challenge.\n",
      "to this end, we introduce\n",
      "a controllable feature modulation mechanism\n",
      "within our cfb that aims at improving the\n",
      "diversity and quality of generated images.\n",
      "the\n",
      "proposed modulation incorporates stochastic-\n",
      "ity as well as enhanced control in the fea-\n",
      "ture aggregation and reﬁnement steps.\n",
      "gref\n",
      "i\n",
      "is\n",
      "global-level feature computed from the reference features href\n",
      "i\n",
      "through a linear\n",
      "transformation and a global average pooling operation.\n",
      "controllable cross-modulated layer normalization (cln): our cln learns\n",
      "sample-dependent modulation weights for normalizing features since it is desired\n",
      "few-shot image generation for colorectal tissue classiﬁcation\n",
      "133\n",
      "to generate images that are similar to the few-shot samples.\n",
      "such a dynamic\n",
      "modulation of features enables our framework to generate images of high-quality\n",
      "and diversity.\n",
      "here, λ(wi)\n",
      "is computed as the element-wise multiplication between meta-weights wi and\n",
      "sample-independent learnable weights λ ∈ rd, as λ⊙wi.\n",
      "consequently, our proposed normalization mechanism\n",
      "achieves a controllable modulation of the input features based on the reference\n",
      "image inputs and enables enhanced diversity and quality in the generated images.\n",
      "the resulting features oi are then passed through a feed-forward network (ffn)\n",
      "followed by another cln for preforming point-wise feature reﬁnement, as shown\n",
      "in fig.\n",
      "finally, the decoder fd generates the ﬁnal image ˆx.\n",
      "3.2\n",
      "training and inference\n",
      "training: the whole framework is trained end-to-end following the hinge ver-\n",
      "sion gan [17] formulation.\n",
      "(4)\n",
      "additionally, to encourage the generated image ˆx to be perceptually similar\n",
      "to the reference images based on the speciﬁed control parameters α, we use a\n",
      "parameterized formulation of the standard perceptual loss [11], given by\n",
      "lp =\n",
      "\u0003\n",
      "i\n",
      "αilp\n",
      "i ,\n",
      "where\n",
      "lp\n",
      "i = e[∥φ(ˆx)\n",
      "(5)\n",
      "moreover, a classiﬁcation loss lcl enforces that the images generated by the\n",
      "decoder are classiﬁed into the corresponding class of the input few-shot samples.\n",
      "inference: during inference, multiple high-quality and diverse images ˆx are\n",
      "generated by varying the control parameter αi for a set of ﬁxed k-shot samples.\n",
      "while a base image xb and αi can be randomly selected, our framework enables\n",
      "a user to have control over the generation based on the choice of αi values.\n",
      "134\n",
      "a. kumar et al.\n",
      "4\n",
      "experiments\n",
      "we conduct experiments on human colorectal cancer dataset [14].\n",
      "to enable few-shot\n",
      "setting, we split the 8 categories into 5 seen (for training) and 3 unseen categories\n",
      "(for evaluation) with 40 images per category.\n",
      "[8] and learned perceptual image\n",
      "patch similarity (lpips)\n",
      "the input and generated image size is 128 × 128.\n",
      "the linear transformation\n",
      "ψ(·) is implemented as a 1 × 1 convolution with input and output channels set\n",
      "to d. the weights ηp and ηcl are set to 50 and 1.\n",
      "we set k = 3 in all the\n",
      "experiments, unless speciﬁed otherwise.\n",
      "our xm-gan is trained with a batch-\n",
      "size of 8 using the adam optimizer and a ﬁxed learning rate of 10−4.\n",
      "4.1\n",
      "state-of-the-art comparison\n",
      "fs tissue image generation: in tab.\n",
      "1, we compare our xm-gan approach\n",
      "for fs tissue image generation with state-of-the-art lofgan\n",
      "our xm-gan achieves consis-\n",
      "tent gains in performance on both fid and\n",
      "lpips scores, outperforming lofgan on\n",
      "[14] dataset.\n",
      "[7]\n",
      "85.9\n",
      "0.44\n",
      "ours: xm-gan\n",
      "55.8\n",
      "0.48\n",
      "low-data classiﬁcation: here, we\n",
      "evaluate the applicability of the tissue\n",
      "images generated by our xm-gan as\n",
      "a source of data augmentation for the\n",
      "downstream task of low-data colorec-\n",
      "tal tissue classiﬁcation for unseen cate-\n",
      "gories.\n",
      "images of an unseen\n",
      "class are split into 10:15:15.\n",
      "then, we\n",
      "augment dtr with 30 tissue images generated by our xm-gan using the same\n",
      "dtr as few-shot samples for each unseen class.\n",
      "table 2 shows the classiﬁcation\n",
      "performance comparison.\n",
      "few-shot image generation for colorectal tissue classiﬁcation\n",
      "135\n",
      "table 2. low-data image classi-\n",
      "ﬁcation.\n",
      "the proposed xm-gan\n",
      "achieves superior classiﬁcation per-\n",
      "formance\n",
      "compared\n",
      "to\n",
      "recently\n",
      "introduced lofgan.\n",
      "3. on the left: few-shot input images of colorectal tissues.\n",
      "in the middle: images\n",
      "generated by lofgan.\n",
      "on the right: images generated by our xm-gan. compared\n",
      "to lofgan, our xm-gan generates images that are high-quality yet diverse.\n",
      "additional results are provided in the supplementary material.\n",
      "we conduct an additional experiment using random values of αi\n",
      "s.t.\n",
      "2. this is denoted\n",
      "here as baseline+ppl+cln†. our approach based on the novel cfb achieves the\n",
      "best performance amongst all baselines.\n",
      "4.3\n",
      "human evaluation study\n",
      "we conducted a study with a group of ten pathologists having an average subject\n",
      "experience of 8.5 years.\n",
      "each pathologist is shown a random set of 20 images\n",
      "136\n",
      "a. kumar et al.\n",
      "(10 real and 10 xm-gan generated) and asked to identify whether they are\n",
      "real or generated.\n",
      "the study shows that pathologists could diﬀerentiate between\n",
      "the ai-generated and real images only 55% time, which is comparable with a\n",
      "random prediction in a binary classiﬁcation problem, indicating the ability of\n",
      "our proposed generative framework to generate realistic colorectal images.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf:\n",
      "delivering meaningful uncertainty estimates is essential for\n",
      "a successful deployment of machine learning models in the clinical prac-\n",
      "tice.\n",
      "although many methods have been proposed to improve calibration,\n",
      "no technique can match the simple, but expensive approach of train-\n",
      "ing an ensemble of deep neural networks.\n",
      "we show that the resulting averaged\n",
      "predictions can achieve excellent calibration without sacriﬁcing accuracy\n",
      "in two challenging datasets for histopathological and endoscopic image\n",
      "classiﬁcation.\n",
      "our experiments indicate that multi-head multi-loss clas-\n",
      "siﬁers are inherently well-calibrated, outperforming other recent cali-\n",
      "bration techniques and even challenging deep ensembles’ performance.\n",
      "code to reproduce our experiments can be found at https://github.com/\n",
      "agaldran/mhml_calibration.\n",
      "keywords: model calibration · uncertainty quantiﬁcation\n",
      "1\n",
      "introduction and related work\n",
      "when training supervised computer vision models, we typically focus on improv-\n",
      "ing their predictive performance, yet equally important for safety-critical tasks\n",
      "is their ability to express meaningful uncertainties about their own predictions\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1_11.\n",
      "nonetheless, if we ask\n",
      "the model about a colon biopsy with ambiguous visual content, i.e. a hard-to-\n",
      "diagnose image, then it could express aleatoric uncertainty, as it may not know\n",
      "how to solve the problem, but the ambiguity comes from the data.\n",
      "broadly speaking, one can attempt to promote calibration during training,\n",
      "by means of a post-processing stage, or by model ensembling.\n",
      "training-time calibration.\n",
      "these techniques often rely on correctly tuning a hyper-parameter controlling the\n",
      "trade-oﬀ between discrimination ability and conﬁdence, and can easily achieve\n",
      "better calibration at the expense of decreasing predictive performance [22].\n",
      "examples of medical image analysis works adopting this approach are diﬀerence\n",
      "between conﬁdence and accuracy regularization [20] for medical image diag-\n",
      "nosis, or spatially-varying and margin-based label smoothing [14,27], which\n",
      "extend and improve label smoothing for biomedical image segmentation tasks.\n",
      "their\n",
      "greatest shortcoming is the dependence on the i.i.d. assumption implicitly made\n",
      "when using validation data to learn the mapping: these approaches suﬀer to gen-\n",
      "eralize to unseen data [28]. other than that, these techniques can be combined\n",
      "with training-time methods and return compounded performance improvements.\n",
      "an obvious weakness of deep ensembles is the requirement of\n",
      "training and then keeping for inference purposes a set of models, which results\n",
      "in a computational overhead that can be considerable for larger architectures.\n",
      "examples of applying ensembling in medical image computing include [17,24].\n",
      "in this work we achieve model calibration by means of multi-head models\n",
      "trained with diverse loss functions.\n",
      "detailed derivations of all the results below are\n",
      "provided in the online supplementary materials.\n",
      "2.1\n",
      "multi-head ensemble diversity\n",
      "consider a k-class classiﬁcation problem, and a neural network uθ taking an\n",
      "image x and mapping it onto a representation uθ(x) ∈ rn, which is linearly\n",
      "transformed by f into a logits vector z = f(uθ(x))\n",
      "we now wish to implement a multi-head ensemble model like the one shown\n",
      "in fig.\n",
      "the resulting probability vectors pm = σ(zm) are then averaged to obtain a ﬁnal\n",
      "prediction pμ = (1/m) \u0002\n",
      "m pm.\n",
      "an image x goes through a\n",
      "neural network uθ\n",
      "the ﬁnal loss lmh is the\n",
      "sum of per-head weighted-ce losses lω m-ce(pm, y) and the ce loss lce(pμ, y) of the\n",
      "average prediction pμ = μ(p1, ..., pm).\n",
      "as a consequence, diversity in the predictions that make up the\n",
      "output pμ of the network would be damaged.\n",
      "the total loss of the complete model is the addition of the per-head\n",
      "losses and the overall loss acting on the average prediction:\n",
      "lmh(p, y) = lce(pμ, y) +\n",
      "m\n",
      "\u0003\n",
      "m=1\n",
      "lω m-ce(pm, y),\n",
      "(2)\n",
      "where p = (p1, ..., pm) is an array collecting all the predictions the network\n",
      "makes.\n",
      "this\n",
      "can be quantiﬁed by the expected calibration error (ece), given by:\n",
      "ece =\n",
      "n\n",
      "\u0003\n",
      "s=1\n",
      "|bs|\n",
      "n |acc(bs) − conf(bs)|,\n",
      "(5)\n",
      "where \u0006\n",
      "s bs form a uniform partition of the unit interval, and acc(bs), conf(bs)\n",
      "are accuracy and average conﬁdence (maximum softmax value) for test samples\n",
      "predicted with conﬁdence in bs.\n",
      "in practice, the ece alone is not a good measure in terms of practical usabil-\n",
      "ity, as one can have a perfectly ece-calibrated model with no predictive power\n",
      "[29].\n",
      "finally, we show as summary\n",
      "metric the average rank when aggregating rankings of ece, nll, and accuracy.\n",
      "3\n",
      "experimental results\n",
      "we now describe the data we used for experimentation, carefully analyze per-\n",
      "formance for each dataset, and end up with a discussion of our ﬁndings.\n",
      "3.1\n",
      "datasets and architectures\n",
      "we conducted experiments on two datasets: 1) the chaoyang dataset1, which\n",
      "contains colon histopathology images.\n",
      "it has 6,160 images unevenly distributed in\n",
      "4 classes (29%, 19%, 37%, 15%), with some amount of label ambiguity, reﬂecting\n",
      "high aleatoric uncertainty.\n",
      "2) kvasir2, a dataset for the task of endoscopic image\n",
      "classiﬁcation.\n",
      "the annotated part of this dataset contains 10,662 images, and it\n",
      "represents a challenging classiﬁcation problem due a high amount of classes (23)\n",
      "and highly imbalanced class frequencies [2].\n",
      "for the sake of readability we do not\n",
      "show measures of dispersion, but we add them to the supplementary material\n",
      "(appendix b), together with further experiments on other datasets.\n",
      "we implement the proposed approach by optimizing several popular neural\n",
      "network architectures, namely a common resnet50 and two more recent mod-\n",
      "els: a convnext [23] and a swin-transformer [23].\n",
      "code to reproduce our results\n",
      "and hyperparameter speciﬁcations are shared at https://github.com/agaldran/\n",
      "mhml_calibration.\n",
      "3.2\n",
      "performance analysis\n",
      "notation: we train three diﬀerent multi-head classiﬁers: 1) a 2-head model\n",
      "where each head optimizes for standard (unweighted) ce, referred to as 2hsl\n",
      "(2 heads-single loss); 2) a 2-head model but with each head minimizing a\n",
      "diﬀerently weighed ce loss as described in sect.\n",
      "we also show the performance of deep ensembles (d-ens\n",
      "we also expect\n",
      "to achieve good calibration without sacriﬁcing predictive performance\n",
      "(high accuracy).\n",
      "finally we would ideally\n",
      "observe improved performance as we increase the diversity (comparing\n",
      "2hsl to 2hml) and as we add heads (comparing 2hml to 4hml).\n",
      "note that models minimizing the dca loss do manage to bring\n",
      "the ece down, although by giving up accuracy.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf:\n",
      "a predominant source of failures in image classiﬁca-\n",
      "tion is distribution shifts between training data and deployment data.\n",
      "on the basis of various examples,\n",
      "we demonstrate how this tool can help researchers gain insight into the\n",
      "requirements for safe application of classiﬁcation systems in the medi-\n",
      "cal domain.\n",
      "a primary reason is the lack of reliability, i.e. failure cases produced\n",
      "by the system, which predominantly occur when deployment data diﬀers from\n",
      "the data it was trained on, a phenomenon known as distribution shifts.\n",
      "in med-\n",
      "ical applications, these shifts can be caused by image corruption (“corruption\n",
      "shift”), unseen variants of pathologies (“manifestation shift”), or deployment in\n",
      "new clinical sites with diﬀerent scanners and protocols (“acquisition shift”)\n",
      "the robustness of a classiﬁer, i.e. its ability to generalize across these shifts, is\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1 39.\n",
      "https://doi.org/10.1007/978-3-031-43898-1_39\n",
      "silent failures in medical image classiﬁcation\n",
      "401\n",
      "fig.\n",
      "note that true/false positives/negatives (t/f p/n) do not\n",
      "refer to the classiﬁer decision, but to the failure detection outcome, i.e.\n",
      "the assessment of the csf.\n",
      "b) sf-visuals allows to identify and analyze silent failures in a dataset\n",
      "based on an interactive scatter plot in the classiﬁer’s latent space (each dot repre-\n",
      "sents one image, which is displayed when selecting the dot).\n",
      "[3] studied failure detection on several biomedical datasets,\n",
      "but only assessed the performance of csfs in isolation without considering the\n",
      "classiﬁer’s ability to prevent failures.\n",
      "speciﬁcally, we introduce corruptions of various intensity\n",
      "levels to the images in four datasets in the form of brightness, motion blur, elas-\n",
      "tic transformations and gaussian noise.\n",
      "we further simulate acquisition shifts\n",
      "and manifestation shifts by splitting the data into “source domain” (development\n",
      "data) and “target domain” (deployment data) according to sub-class informa-\n",
      "tion from the meta-data such as lesion subtypes or clinical sites.\n",
      "we emulate two acquisition shifts by deﬁning either images\n",
      "from the memorial sloan kettering cancer center (mskcc) or hospital clinic\n",
      "barcelona (hcb) as the target domain and the remaining images as the source\n",
      "silent failures in medical image classiﬁcation\n",
      "403\n",
      "domain.\n",
      "further, a manifestation shift is designed by deﬁning the lesion sub-\n",
      "types “keratosis-like” (benign) and “actinic keratosis” (malignant) as the tar-\n",
      "get domain.\n",
      "since the images\n",
      "were acquired in 51 deviating acquisition steps, we deﬁne 10 of these batches as\n",
      "target-domain to emulate an acquisition shift.\n",
      "average malignancy ratings (four raters\n",
      "per nodule, scores between 1 and 5) > 2 are considered malignant and all others\n",
      "as benign.\n",
      "we emulate two manifestation shifts by deﬁning nodules with high\n",
      "spiculation (rating > 2), and low texture (rating < 3) as target domains.\n",
      "we ﬁrst reduce the dimensionality of the classiﬁer’s\n",
      "latent space to 50 using principal component analysis and use t-sne to obtain\n",
      "the ﬁnal 3-dimensional embedding.\n",
      "the associated images are displayed\n",
      "upon selection of a dot to establish a direct visual link between input space\n",
      "and embedding.\n",
      "therefore,\n",
      "k-means clustering is applied to the 3-dimensional embedding.\n",
      "nine clusters are\n",
      "identiﬁed per concept and the resulting plots show the closest-to-center image\n",
      "per cluster as a visual representation of the concept.\n",
      "2. we sort all failures by the classiﬁer conﬁdence and\n",
      "by default show the images associated with the top-two most conﬁdent failures.\n",
      "for corruption shifts, we further allow investigating the predictions on a ﬁxed\n",
      "input image over varying intensity levels.\n",
      "3\n",
      "experimental setup\n",
      "evaluating silent failure prevention: we follow jaeger et al.\n",
      "the area\n",
      "404\n",
      "t. j. bungert et al.\n",
      "under the risk-coverage curve aurc reﬂects this task, since it considers both the\n",
      "classiﬁer’s accuracy as well as the csf’s ability to detect failures by assigning\n",
      "low conﬁdence scores.\n",
      "thus, it can be interpreted as a silent failure rate or the\n",
      "error rate averaged over steps of ﬁltering cases one by one according to their rank\n",
      "of conﬁdence score (low to high).\n",
      "exemplary risk-coverage curves are shown in\n",
      "appendix fig.\n",
      "however, the method is not\n",
      "reliable across all settings, falling short on manifestation shifts and corruptions\n",
      "on the lung nodule ct dataset.\n",
      "silent failures in medical image classiﬁcation\n",
      "405\n",
      "table 1.\n",
      "all values denote an average of three runs.\n",
      "“cor” denotes the average over all corruption types and intensities levels.\n",
      "similarly,\n",
      "“acq”/“man” denote averages over all acquisition/manifestation shifts per dataset.\n",
      "results with further metrics are\n",
      "reported in appendix table 2\n",
      "dataset\n",
      "chest x-ray\n",
      "dermoscopy\n",
      "fc-microscopy\n",
      "lung nodule ct\n",
      "study\n",
      "iid\n",
      "cor\n",
      "acq\n",
      "iid\n",
      "cor\n",
      "acq\n",
      "man\n",
      "iid\n",
      "cor\n",
      "acq\n",
      "iid\n",
      "cor\n",
      "man\n",
      "msr\n",
      "15.3\n",
      "when looking beyond the averages displayed in table 1 and ana-\n",
      "lyzing the results of individual clinical centers, corruptions and manifestation\n",
      "shifts, one remarkable pattern can be observed: in various cases, the same csf\n",
      "showed opposing behavior between two variants of the same shift on the same\n",
      "dataset.\n",
      "on the chest x-ray dataset, mcd worsens the performance for darkening cor-\n",
      "ruptions across all csfs and intensity levels, whereas the opposite is observed\n",
      "for brightening corruptions.\n",
      "further, on the lung nodule ct dataset, dg-mcd-\n",
      "res performs best on bright/dark corruptions and the spiculation manifestation\n",
      "shift, but worst on noise corruption and falls behind on the texture manifestation\n",
      "shift.\n",
      "figure 1c provides a concept\n",
      "cluster plot that visually conﬁrms how some of these lesions (purple dot) share\n",
      "characteristics of the benign cluster of the source domain (turquoise dot), such\n",
      "silent failures in medical image classiﬁcation\n",
      "407\n",
      "as being smaller, brighter, and rounder compared to malignant source-lesions\n",
      "(blue dot).\n",
      "further towards the\n",
      "cluster boundary, the ambiguity in images seems to increase, as the csf is able\n",
      "to detect the failures (light blue layer of dots).\n",
      "in both examples, the\n",
      "brightening of the image leads to a malignant lesion taking on benign character-\n",
      "istics (brighter and smoother skin on the dermoscopy data, decreased contrast\n",
      "between lesion and background on the lung nodule ct data).\n",
      "this example shows how the tool allows the\n",
      "comparison of csfs and can help to identify failure modes speciﬁc to each csf.\n",
      "manifestation shift: on the dermoscopy data (fig.\n",
      "2g), we see how a mani-\n",
      "festation shift can cause silent failures.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf:\n",
      "the medical imaging community generates a wealth of data-\n",
      "sets, many of which are openly accessible and annotated for speciﬁc\n",
      "diseases and tasks such as multi-organ or lesion segmentation.\n",
      "we propose multitalent, a method\n",
      "that leverages multiple ct datasets with diverse and conﬂicting class\n",
      "deﬁnitions to train a single model for a comprehensive structure seg-\n",
      "mentation.\n",
      "our results demonstrate improved segmentation performance\n",
      "compared to previous related approaches, systematically, also compared\n",
      "to single-dataset training using state-of-the-art methods, especially for\n",
      "lesion segmentation and other challenging structures.\n",
      "we show that mul-\n",
      "titalent also represents a powerful foundation model that oﬀers a supe-\n",
      "rior pre-training for various segmentation tasks compared to commonly\n",
      "used supervised or unsupervised pre-training baselines.\n",
      "our ﬁndings oﬀer\n",
      "a new direction for the medical imaging community to eﬀectively uti-\n",
      "lize the wealth of available data for improved segmentation performance.\n",
      "keywords: medical image segmentation ·\n",
      "multitask learning ·\n",
      "transfer learning · foundation model · partially labeled datasets\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1_62.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43898-1_62\n",
      "multitalent: a multi-dataset approach to medical image segmentation\n",
      "649\n",
      "1\n",
      "introduction\n",
      "the success of deep neural networks heavily relies on the availability of large\n",
      "and diverse annotated datasets across a range of computer vision tasks.\n",
      "to learn\n",
      "a strong data representation for robust and performant medical image segmen-\n",
      "tation, huge datasets with either many thousands of annotated data structures\n",
      "or less speciﬁc self-supervised pretraining objectives with unlabeled data are\n",
      "needed\n",
      "the annotation of 3d medical images is a diﬃcult and labo-\n",
      "rious task.\n",
      "thus, depending on the task, only a bare minimum of images and\n",
      "target structures is usually annotated.\n",
      "recent eﬀorts have\n",
      "resulted in a large dataset of >1000 ct images with >100 annotated classes\n",
      "each, thus providing more than 100,000 manual annotations which can be used\n",
      "for pre-training [30].\n",
      "focusing on such a dataset prevents leveraging the poten-\n",
      "tially precious additional information of the above mentioned other datasets that\n",
      "are only partially annotated.\n",
      "integrating information across diﬀerent datasets\n",
      "potentially yields a higher variety in image acquisition protocols, more anatom-\n",
      "ical target structures or details about them as well as information on diﬀerent\n",
      "kinds of pathologies.\n",
      "in\n",
      "contrast to dataset 11, in dataset 7 the aorta is also annotated in the lower abdomen.\n",
      "[5,27] and penalizing overlapping predictions\n",
      "by taking advantage of the fact that organs are mutually exclusive [7,28].\n",
      "some\n",
      "other methods only predicted one structure of interest for each forward pass by\n",
      "incorporating the class information at diﬀerent stages of the network [4,22,31].\n",
      "however,\n",
      "most approaches are primarily geared towards multi-organ segmentation as they\n",
      "do not support overlapping target structures, like vessels or cancer classes within\n",
      "an organ [6,8,12,23].\n",
      "so far, all previous methods do not convincingly leverage\n",
      "cross-dataset synergies.\n",
      "as liu et al. pointed out, one common caveat is that\n",
      "many methods force the resulting model to average between distinct annota-\n",
      "tion protocol characteristics [22] by combining labels from diﬀerent datasets for\n",
      "the same target structure (visualized in fig.\n",
      "hence, they all fail to reach\n",
      "segmentation performance on par with cutting-edge single dataset segmentation\n",
      "methods.\n",
      "multitalent can be used in two scenarios: first, in a combined multi-dataset\n",
      "(md) training to generate one foundation segmentation model that is able to\n",
      "predict all classes that are present in any of the utilized partially annotated\n",
      "datasets, and second, for pre-training to leverage the learned representation of\n",
      "this foundation model for a new task.\n",
      "in experiments with a large collection\n",
      "of abdominal ct datasets, the proposed model outperformed state-of-the-art\n",
      "segmentation networks that were trained on each dataset individually as well\n",
      "as all previous methods that incorporated multiple datasets for training.\n",
      "additionally, at the example of three challenging datasets, we demonstrate\n",
      "that ﬁne-tuning multitalent yields higher segmentation performance than train-\n",
      "ing from scratch or initializing the model parameters using unsupervised pre-\n",
      "training strategies [29,33].\n",
      "2\n",
      "methods\n",
      "we introduce multitalent, a multi dataset learning and pre-training method,\n",
      "to train a foundation medical image segmentation model.\n",
      "it comes with a novel\n",
      "multitalent: a multi-dataset approach to medical image segmentation\n",
      "651\n",
      "dataset and class adaptive loss function.\n",
      "[1, k], with n (k)\n",
      "image and label pairs d(k) = {(x, y)(k)\n",
      "1 , ..., (x, y)(k)\n",
      "n (k)}.\n",
      "in these datasets, every\n",
      "image voxel x(k)\n",
      "i\n",
      ", i\n",
      "to solve the label contradiction problem we decou-\n",
      "ple the segmentation outputs for each class by applying a sigmoid activation\n",
      "function instead of the commonly used softmax activation function across the\n",
      "dataset.\n",
      "but it has indepen-\n",
      "dent segmentation head parameters θc for each class.\n",
      "consequently, the\n",
      "segmentation of each class can be thought of as a binary segmentation task.\n",
      "based on the well established\n",
      "combination of a cross-entropy and dice loss for single dataset medical image\n",
      "segmentation, we employ the binary binary cross-entropy loss (bce) and a\n",
      "modiﬁed dice loss for each class over all b, b ∈\n",
      "[1, b], images in a batch:\n",
      "i,b,c) −\n",
      "2 \u0003\n",
      "b,i ˆy(k)\n",
      "i,b,c y(k)\n",
      "i,b,c\n",
      "\u0003\n",
      "b,i ˆy(k)\n",
      "i,b,c + \u0003\n",
      "b,i y(k)\n",
      "i,b,c\n",
      "(1)\n",
      "while the regular dice loss is calculated for each image within a batch, we cal-\n",
      "culate the dice loss jointly for all images of the input batch.\n",
      "this regularizes the\n",
      "loss if only a few voxels of one class are present in one image and a larger area\n",
      "is present in another image of the same batch.\n",
      "thus, an inaccurate prediction of\n",
      "a few pixels in the ﬁrst image has a limited eﬀect on the loss.\n",
      "in the following,\n",
      "652\n",
      "c. ulrich et al.\n",
      "we unite the sum over the image voxels i and the batch b to \u0003\n",
      "z.\n",
      "to demonstrate the general applicability of this app-\n",
      "roach, we applied it to three segmentation networks.\n",
      "we employed a 3d u-net\n",
      "[24], an extension with additional residual blocks in the encoder (resenc u-net),\n",
      "that demonstrated highly competitive results in previous medical image segmen-\n",
      "tation challenges [14,15] and a recently proposed transformer based architecture\n",
      "(swinunetr [29]).\n",
      "we implemented our approach in the nnu-net framework\n",
      "[13].\n",
      "however, the automatic pipeline conﬁguration from nnu-net was not used\n",
      "in favor of a manually deﬁned conﬁguration that aims to reﬂect the peculiarities\n",
      "of each of the datasets, irrespective of the number of training cases they contain.\n",
      "we manually selected a patch size of [96, 192, 192] and image spacing of 1mm in\n",
      "plane and 1.5mm for the axial slice thickness, which nnu-net used to automati-\n",
      "cally create the two cnn network topologies.\n",
      "we trained multitalent with 13 public\n",
      "abdominal ct datasets with a total of 1477 3d images, including 47 classes\n",
      "(multi-dataset (md) collection)\n",
      "we increased the batch size to 4 and\n",
      "the number of training epochs to 2000 to account for the high number of train-\n",
      "ing images.\n",
      "to compensate for the varying number of training images in each\n",
      "dataset, we choose a sampling probability per case that is inversely proportional\n",
      "to √n, where n is the number of training cases in the corresponding source\n",
      "dataset.\n",
      "naturally, the target datasets were excluded\n",
      "multitalent: a multi-dataset approach to medical image segmentation\n",
      "653\n",
      "from the respective pre-training.\n",
      "first, the\n",
      "segmentation heads were warmed up over 10 epochs with linearly increasing\n",
      "learning rate, followed by a whole-network warm-up over 50 epochs.\n",
      "2.3\n",
      "baselines\n",
      "as a baseline for the multitalent, we applied the 3d u-net generated by the\n",
      "nnu-net without manual intervention to each dataset individually.\n",
      "all\n",
      "baseline networks were also implemented within the nnu-net framework and\n",
      "follow the default training procedure.\n",
      "as supervised baseline,\n",
      "we used the weights resulting from training the three model architectures on\n",
      "the totalsegmentator dataset, which consists of 1204 images and 104 classes\n",
      "we used the same patch\n",
      "size, image spacing, batch size and number of epochs as for the multitalent\n",
      "training.\n",
      "finally, for the swinunetr architecture, we compared the utility\n",
      "of the weights from our multitalent with the ones provided by tan et al. who\n",
      "performed self-supervised pre-training on 5050 ct images.\n",
      "implementation of swinunetr because the rec-\n",
      "ommended settings for ﬁne tuning were used.\n",
      "multitalent improves the performance of the purely convolutional architectures\n",
      "(u-net and resenc u-net) and outperforms the corresponding baseline models\n",
      "that were trained on each dataset individually.\n",
      "since a simple average over all\n",
      "classes would introduce a biased perception due to the highly varying numbers\n",
      "of images and classes, we additionally report an average over all datasets.\n",
      "for\n",
      "example, dataset 7 consists of only 30 training images but has 13 classes, whereas\n",
      "654\n",
      "c. ulrich et al.\n",
      "fig.\n",
      "dataset 6 has 126 training images but only 1 class.\n",
      "averaged over all datasets, the multitalent gains\n",
      "1.26 dice points for the resenc u-net architecture and 1.05 dice points for the u-\n",
      "net architecture.\n",
      "compared to the default nnu-net, conﬁgured without manual\n",
      "intervention for each dataset, the improvements are 1.56 and 0.84 dice points.\n",
      "both class groups, but espe-\n",
      "cially the cancer classes, experience notable performance improvements from\n",
      "multitalent.\n",
      "the\n",
      "advantages of multitalent include not only better segmentation results, but also\n",
      "considerable time savings for training and inference due to the simultaneous pre-\n",
      "diction of all classes.\n",
      "although multitalent was trained with a substantially lower amount of manually\n",
      "multitalent: a multi-dataset approach to medical image segmentation\n",
      "655\n",
      "annotated structures (˜3600 vs. ˜105 annotations), it also exceeds the supervised\n",
      "pre-training baseline.\n",
      "especially for the small multi-organ dataset, which only\n",
      "has 30 training images (btcv), and for the kidney tumor (kits19), the multi-\n",
      "talent pre-training boosts the segmentation results.\n",
      "in general, the results show\n",
      "that supervised pre-training can be beneﬁcial for the swinunetr as well, but\n",
      "pre-training on the large totalsegmentator dataset works better than the md\n",
      "pre-training.\n",
      "for the amos dataset, no pre-training scheme has a substantial\n",
      "impact on the performance.\n",
      "* indicates usage of multiple datasets.\n",
      "[32]\n",
      "single model\n",
      "84.97\n",
      "18.47\n",
      "multitalent resenc u-net*\n",
      "single model\n",
      "88.82\n",
      "16.35\n",
      "multitalent resenc u-net*\n",
      "5-fold ensemble 88.91\n",
      "14.68\n",
      "resenc u-net (pre-trained multitalent*) 5-fold ensemble 89.07\n",
      "15.01\n",
      "4\n",
      "discussion\n",
      "multitalent demonstrates the remarkable potential of utilizing multiple pub-\n",
      "licly available partially labeled datasets to train a foundation medical segmen-\n",
      "tation network, that is highly beneﬁcial for pre-training and ﬁnetuning various\n",
      "segmentation tasks.\n",
      "furthermore, multitalent takes less time for training and inference, sav-\n",
      "ing resources compared to training many single dataset models.\n",
      "in the transfer\n",
      "learning setting, the feature representations learned by multitalent boost seg-\n",
      "mentation performance and set a new state-of-the-art on the btcv leaderboard.\n",
      "swinunetr implementation and the provided\n",
      "self-supervised weights as additional baseline\n",
      "implement self-supervised [29]\n",
      "74.71\n",
      "0.30\n",
      "86.11\n",
      "0.20\n",
      "87.62\n",
      "0.55 43.64\n",
      "0.97\n",
      "swinunetr\n",
      "from scratch\n",
      "81.44\n",
      "87.59\n",
      "95.97\n",
      "76.52\n",
      "supervised (˜105 annot.)\n",
      "84.92\n",
      "0.03\n",
      "89.81\n",
      "0.16\n",
      "96.89\n",
      "0.04 84.01\n",
      "0.12\n",
      "allows including any publicly available datasets (e.g. amos and totalsegmen-\n",
      "tator).\n",
      "this paves the way towards holistic whole body segmentation model that\n",
      "is even capable of handling pathologies.\n",
      "acknowledgements.\n",
      "references\n",
      "1. antonelli, m., et al.: the medical segmentation decathlon.\n",
      "med3d: transfer learning for 3d medical image anal-\n",
      "ysis.\n",
      "imaging 26, 1045–1057 (2013)\n",
      "4. dmitriev, k., kaufman, a.e.: learning multi-class segmentations from single-class\n",
      "datasets.\n",
      "multi-organ segmentation over partially labeled datasets with\n",
      "multi-scale feature abstraction.\n",
      "ms-kd: multi-organ segmen-\n",
      "tation with multiple binary-labeled datasets.\n",
      "learning\n",
      "from partially overlapping labels: image segmentation under annotation shift.\n",
      "https://doi.org/10.1007/978-3-030-87722-4_12\n",
      "multitalent: a multi-dataset approach to medical image segmentation\n",
      "657\n",
      "9.\n",
      "gibson, e., et al.: automatic multi-organ segmentation on abdominal ct with\n",
      "dense v-networks.\n",
      "unetr: transformers for 3d medical image segmentation.\n",
      "heller, n., et al.: the kits19 challenge data: 300 kidney tumor cases with clini-\n",
      "cal context, ct semantic segmentations, and surgical outcomes.\n",
      "huang, r., zheng, y., hu, z., zhang, s., li, h.: multi-organ segmentation via co-\n",
      "training weight-averaged models from few-organ datasets.\n",
      "nnu-net:\n",
      "a self-conﬁguring method for deep learning-based biomedical image segmentation.\n",
      "ji, y., et al.: amos: a large-scale abdominal multi-organ benchmark for versatile\n",
      "medical image segmentation.\n",
      "segthor: segmentation of tho-\n",
      "racic organs at risk in ct images.\n",
      "landman, b., xu, z., igelsias, j.e., styner, m., langerak, t., klein, a.: mic-\n",
      "li, h., zhou, j., deng, j., chen, m.: automatic structure segmentation for radio-\n",
      "therapy planning challenge (2019).\n",
      "li, s., wang, h., meng, y., zhang, c., song, z.: multi-organ segmentation: a\n",
      "progressive exploration of learning paradigms under scarce annotation (2023)\n",
      "22.\n",
      "liu, j., et al.: clip-driven universal model for organ segmentation and tumor detec-\n",
      "tion.\n",
      "liu, p., zheng, g.: context-aware voxel-wise contrastive learning for label eﬃcient\n",
      "multi-organ segmentation.\n",
      "medical image computing and computer assisted intervention – miccai\n",
      "2022.\n",
      "ronneberger, o., fischer, p., brox, t.: u-net: convolutional networks for biomed-\n",
      "ical image segmentation.\n",
      "roth, h.r., et al.: deeporgan: multi-level deep convolutional networks for auto-\n",
      "mated pancreas segmentation.\n",
      "roth, h.r., et al.: deeporgan: multi-level deep convolutional networks for auto-\n",
      "mated pancreas segmentation.\n",
      "roulet, n., slezak, d.f., ferrante, e.: joint learning of brain lesion and anatomy\n",
      "segmentation from heterogeneous datasets.\n",
      "in: proceedings of the 2nd interna-\n",
      "tional conference on medical imaging with deep learning (2019)\n",
      "28. shi, g., xiao, l., chen, y., zhou, s.k.: marginal loss and exclusion loss for partially\n",
      "supervised multi-organ segmentation.\n",
      "image anal.\n",
      "tang, y., et al.: self-supervised pre-training of swin transformers for 3d medi-\n",
      "cal image analysis.\n",
      "30. wasserthal, j., meyer, m., breit, h.c., cyriac, j., yang, s., segeroth, m.:\n",
      "totalsegmentator: robust segmentation of 104 anatomical structures in ct images.\n",
      "arxiv:2208.05868 (2022)\n",
      "31.\n",
      "dodnet: learning to segment multi-organ and\n",
      "tumors from multiple partially labeled datasets.\n",
      "prior-aware neural network for partially-supervised multi-organ\n",
      "segmentation.\n",
      "image\n",
      "anal.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf:\n",
      "interactive segmentation reduces the annotation time of\n",
      "medical images and allows annotators to iteratively reﬁne labels with\n",
      "corrective interactions, such as clicks.\n",
      "while existing interactive mod-\n",
      "els transform clicks into user guidance signals, which are combined with\n",
      "images to form (image, guidance) pairs, the question of how to best\n",
      "represent the guidance has not been fully explored.\n",
      "we conduct\n",
      "our study on the msd spleen and the autopet datasets to explore the\n",
      "segmentation of both anatomy (spleen) and pathology (tumor lesions).\n",
      "our results show that choosing the guidance signal is crucial for inter-\n",
      "active segmentation as we improve the performance by 14% dice with\n",
      "our adaptive heatmaps on the challenging autopet dataset when com-\n",
      "pared to non-interactive models.\n",
      "this brings interactive models one step\n",
      "closer to deployment in clinical workﬂows.\n",
      "code: https://github.com/\n",
      "zrrr1997/guiding-the-guidance/.\n",
      "keywords: interactive segmentation · comparative study · click\n",
      "guidance\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1 61.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14222, pp.\n",
      "https://doi.org/10.1007/978-3-031-43898-1_61\n",
      "638\n",
      "z. marinov et al.\n",
      "1\n",
      "introduction\n",
      "deep learning models have achieved remarkable success in segmenting anatomy\n",
      "and lesions from medical images but often rely on large-scale manually annotated\n",
      "datasets [1–3].\n",
      "interactive segmen-\n",
      "tation models address this issue by utilizing weak labels, such as clicks, instead\n",
      "of voxelwise annotations [5–7].\n",
      "the clicks are transformed into guidance sig-\n",
      "nals, e.g., gaussian heatmaps or euclidean/geodesic distance maps, and used\n",
      "together with the image as a joint input for the interactive model.\n",
      "annotators\n",
      "can make additional clicks in missegmented areas to iteratively reﬁne the seg-\n",
      "mentation mask, which often signiﬁcantly improves the prediction compared to\n",
      "non-interactive models [4,13].\n",
      "2. we introduce 5 guidance evaluation metrics (m1)-(m5), which evaluate the\n",
      "performance, eﬃciency, and ability to improve with new clicks.\n",
      "our adaptive heatmaps mitigate the weaknesses of the 5 guidances\n",
      "and achieve the best performance on autopet [1] and msd spleen [2].\n",
      "however, neither of them explore diﬀerent parameter settings for each\n",
      "guidance and both work with natural 2d images.\n",
      "however, they use only initial clicks and\n",
      "do not add iterative corrective clicks to reﬁne the segmentation.\n",
      "σi = ⌊ae−bx⌋, where x =\n",
      "1\n",
      "|nci|\n",
      "\u0003\n",
      "v∈nci\n",
      "gdt(v, c)\n",
      "(6)\n",
      "here, nci is the 9-neighborhood of ci, a = 13 limits the maximum radius to\n",
      "13, and b = 0.15 is set empirically1 (details in supplementary).\n",
      "for each volume, n clicks\n",
      "are iteratively sampled from over- and undersegmented predictions of the model\n",
      "as in [16] and represented as foreground and background guidance signals.\n",
      "we\n",
      "implemented our experiments with monai label [23] and will release our code.\n",
      "msd spleen [2] contains 41 ct vol-\n",
      "umes with voxel size 0.79×0.79×5.00mm3 and average resolution of 512×512×89\n",
      "voxels with dense annotations of the spleen.\n",
      "we also only use pet data for our experiments.\n",
      "the pet volumes have a voxel\n",
      "size of 2.0 × 2.0 × 2.0mm3 and an average resolution of 400 × 400 × 352 voxels.\n",
      "2.3\n",
      "hyperparameters: experiments\n",
      "we keep these parameters constant for all models: learning rate = 10−5, #clicks\n",
      "n = 10, dice cross-entropy loss [24], and a ﬁxed 80–20 training-validation split\n",
      "(dtrain/dval).\n",
      "we apply the same data augmentation transforms to all models\n",
      "and simulate clicks as proposed in sakinis et al.\n",
      "(1) and (2)\n",
      "and also explore how this parameter inﬂuences the performance of the distance-\n",
      "based signals in eq.\n",
      "(3)–(5) aﬀects the performance.\n",
      "unlike mideepseg [5], we compute the θ threshold for\n",
      "each image individually, as ﬁxed thresholds may not be suitable for all images.\n",
      "guiding the guidance: a comparative analysis of user guidance signals\n",
      "641\n",
      "table 1. variation of hyperparameters (h1) – (h4) in our experiments.\n",
      "we randomly decide for each volume\n",
      "whether to add the n clicks or not, with a probability of p, in order to make the\n",
      "model more independent of interactions and improve its initial segmentation.\n",
      "2.4\n",
      "additional evaluation metrics\n",
      "we use 5 metrics (m1)–(m5) (table 2) to evaluate the validation performance.\n",
      "a higher initial\n",
      "dice indicates less work for the annotator\n",
      "(m3) eﬃciency\n",
      "inverted∗ time measurement (1 − t) in seconds needed to compute the\n",
      "guidance.\n",
      "∗our maximum measurement tmax is shorter than 1 second\n",
      "(m4) consistent\n",
      "improve-\n",
      "ment\n",
      "ratio of clicks c+ that improve the dice score to the total number of\n",
      "validation clicks:\n",
      "|c+|\n",
      "n·|dval|, where n = 10 and dval is the validation dataset\n",
      "(m5) ground-\n",
      "truth\n",
      "overlap\n",
      "overlap of the guidance g with the ground-truth mask m: |m∩g|\n",
      "|g|\n",
      ".\n",
      "1b) show that on msd spleen [2], the highest\n",
      "dice scores are at σ = 5, with a slight improvement for two samples at σ = 1,\n",
      "but performance decreases for higher values σ > 5.\n",
      "on autopet [1], σ = 5 and\n",
      "two samples with σ = 0 show the best performance, while higher values again\n",
      "demonstrate a signiﬁcant performance drop.\n",
      "geodesic\n",
      "maps exhibit lower dice scores for small σ < 5 and achieve the best performance\n",
      "for σ = 5 on both datasets.\n",
      "figure 1a) shows that\n",
      "the highest ﬁnal dice scores are achieved with θ = 10 for msd spleen [2].\n",
      "on autopet [1], the scores are relatively similar when varying θ with a slight\n",
      "improvement at θ\n",
      "that not truncating values on msd\n",
      "spleen [2], i.e. θ = 0, leads to a sharp drop in performance.\n",
      "for our next experiments, we ﬁx the optimal (σ, θ) pair for each of the ﬁve\n",
      "guidances (see table 3) and train a deepedit\n",
      "1e) indicate that the best performance is achieved by\n",
      "simply concatenating the guidance signal with the input volume.\n",
      "this holds true\n",
      "for both datasets and the diﬀerence in performance is substantial.\n",
      "figure 1e) shows that p ∈ {75%, 100%}\n",
      "results in the best performance on msd spleen [2], with a faster convergence\n",
      "rate for p = 75%.\n",
      "however, with p = 50%, the performance is worse than the\n",
      "non-interactive baseline (p = 0%).\n",
      "for the rest of our\n",
      "experiments, we use the optimal hyperparameters for each guidance in table 1.\n",
      "3.2\n",
      "additional evaluation metrics: results\n",
      "the comparison of the guidance signals using our ﬁve metrics (m1)–(m5)\n",
      "can be seen in fig.\n",
      "although the concrete values for msd spleen [2] and\n",
      "644\n",
      "z. marinov et al.\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "initial\n",
      "dice\n",
      "final\n",
      "dice\n",
      "efficiency\n",
      "consistent\n",
      "improvement\n",
      "overlap\n",
      "with gt\n",
      "exp.\n",
      "geodesic maps\n",
      "geodesic maps\n",
      "euclidean maps\n",
      "disks\n",
      "heatmaps\n",
      "adaptive heatmaps (ours)\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.80\n",
      "0.65\n",
      "0.52\n",
      "0.42\n",
      "0.39\n",
      "0.37\n",
      "0.81\n",
      "0.73\n",
      "0.73\n",
      "0.72\n",
      "0.66\n",
      "0.63\n",
      "0.96\n",
      "0.95\n",
      "0.89\n",
      "0.81\n",
      "0.22\n",
      "0.21\n",
      "0.90\n",
      "0.89\n",
      "0.86\n",
      "0.85\n",
      "0.83\n",
      "0.80\n",
      "0.31\n",
      "0.25\n",
      "0.20\n",
      "0.17\n",
      "0.04\n",
      "0.01\n",
      "0.64\n",
      "0.61\n",
      "0.46\n",
      "0.43\n",
      "0.42\n",
      "0.20\n",
      "msd spleen\n",
      "autopet\n",
      "0.2\n",
      "0.4\n",
      "0.8\n",
      "initial\n",
      "dice\n",
      "overlap\n",
      "with gt\n",
      "consistent\n",
      "improvement\n",
      "0.6\n",
      "final\n",
      "dice\n",
      "efficiency\n",
      "0.61\n",
      "0.60\n",
      "0.56\n",
      "0.47\n",
      "0.47\n",
      "0.43\n",
      "0.78\n",
      "0.68\n",
      "0.62\n",
      "0.60\n",
      "0.60\n",
      "0.59\n",
      "0.79\n",
      "0.78\n",
      "0.78\n",
      "0.75\n",
      "0.73\n",
      "0.73\n",
      "fig.\n",
      "(m3) consistent improvement.\n",
      "the consistent improvement is ≈ 65%\n",
      "for both datasets, but it is slightly worse for autopet [1] as it is more chal-\n",
      "lenging.\n",
      "heatmaps and disks achieve the most consistent improvement, which\n",
      "means they are more precise in correcting errors.\n",
      "these changes may confuse the model and lead to inconsistent improvement.\n",
      "this results in substantially\n",
      "higher consistent improvement and overlap with ground truth and the best initial\n",
      "and ﬁnal dice (table 3).\n",
      "thus, our comparative study has led to the creation\n",
      "of a more consistent and ﬂexible signal with a slight performance boost, albeit\n",
      "with an eﬃciency cost due to the need to compute both gdt and heatmaps.\n",
      "4\n",
      "conclusion\n",
      "our comparative experiments yield insights into tuning existing guiding signals\n",
      "and designing new ones.\n",
      "weaknesses in existing signals include overly large\n",
      "radiuses near edges and inconsistent improvement for geodesic-based signals that\n",
      "change with each click.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf:\n",
      "breast lesion segmentation in ultrasound (us) videos is essen-\n",
      "tial for diagnosing and treating axillary lymph node metastasis.\n",
      "to overcome this issue, we meticulously curated a\n",
      "us video breast lesion segmentation dataset comprising 572 videos and\n",
      "34,300 annotated frames, covering a wide range of realistic clinical sce-\n",
      "narios.\n",
      "furthermore, we propose a novel frequency and localization fea-\n",
      "ture aggregation network (fla-net) that learns temporal features from\n",
      "the frequency domain and predicts additional lesion location positions to\n",
      "assist with breast lesion segmentation.\n",
      "our experiments on\n",
      "our annotated dataset and two public video polyp segmentation datasets\n",
      "demonstrate that our proposed fla-net achieves state-of-the-art perfor-\n",
      "mance in breast lesion segmentation in us videos and video polyp segmen-\n",
      "tation while signiﬁcantly reducing time and space complexity.\n",
      "keywords: ultrasound video · breast lesion · segmentation\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1_48.\n",
      "early detection and timely treatment are crucial for improving outcomes and\n",
      "reducing the risk of recurrence.\n",
      "in breast cancer diagnosis, accurately segmenting\n",
      "breast lesions in ultrasound (us) videos is an essential step for computer-aided\n",
      "diagnosis systems, as well as breast cancer diagnosis and treatment.\n",
      "bbox: whether provide segmentation mask\n",
      "annotation.\n",
      "[12] 2022 188\n",
      "25,272\n",
      "✓\n",
      "×\n",
      "✓\n",
      "×\n",
      "ours\n",
      "2023 572\n",
      "34,300 ✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "the work presented in [10] proposed the ﬁrst pixel-wise annotated benchmark\n",
      "dataset for breast lesion segmentation in us videos, but it has some limitations.\n",
      "although their eﬀorts were commendable, this dataset is private and contains\n",
      "only 63 videos with 4,619 annotated frames.\n",
      "in this work, we\n",
      "collected a larger-scale us video breast lesion segmentation dataset\n",
      "with 572 videos and 34,300 annotated frames, of which 222 videos contain aln\n",
      "metastasis, covering a wide range of realistic clinical scenarios.\n",
      "although the existing benchmark method dpstt [10] has shown promis-\n",
      "ing results for breast lesion segmentation in us videos, it only uses the ultra-\n",
      "sound image to read memory for learning temporal features.\n",
      "however, ultrasound\n",
      "images suﬀer from speckle noise, weak boundaries, and low image quality.\n",
      "thus,\n",
      "there is still considerable room for improvement in ultrasound video breast lesion\n",
      "segmentation.\n",
      "to address this, we propose a novel network called fre-\n",
      "quency and localization feature aggregation network (fla-net) to\n",
      "improve breast lesion segmentation in ultrasound videos.\n",
      "our fla-net learns\n",
      "frequency-based temporal features and then uses them to predict auxiliary breast\n",
      "lesion location maps to assist the segmentation of breast lesions in video frames.\n",
      "additionally, we devise a contrastive loss to enhance the breast lesion location\n",
      "shifting more attention to breast lesion segmentation in ultrasound videos\n",
      "499\n",
      "fig.\n",
      "1. examples of our ultrasound video dataset for breast lesion segmentation.\n",
      "the experimental results unequiv-\n",
      "ocally showcase that our network surpasses state-of-the-art techniques in the\n",
      "realm of both breast lesion segmentation in us videos and two video polyp\n",
      "segmentation benchmark datasets (fig. 1).\n",
      "2\n",
      "ultrasound video breast lesion segmentation dataset\n",
      "to support advancements in breast lesion segmentation and aln metastasis\n",
      "prediction, we collected a dataset containing 572 breast lesion ultrasound videos\n",
      "with 34,300 annotated frames.\n",
      "nine experienced pathologists were invited to manually annotate breast lesions\n",
      "at each video frame.\n",
      "moreover, apart from the segmentation annotation, our\n",
      "dataset also includes lesion bounding box labels, which enables benchmarking\n",
      "breast lesion detection in ultrasound videos.\n",
      "more dataset statistics are available\n",
      "in the supplementary.\n",
      "then frequency-based\n",
      "feature aggregation module is then used to aggregate these features and the aggre-\n",
      "gated feature map is then passed into our two-branch decoder to predict the breast\n",
      "lesion segmentation mask of it, and a lesion localization heatmap.\n",
      "features ot of the ffa module into two decoder branches (similar to the unet\n",
      "decoder [14]): one is the localization branch to predict the localization map of\n",
      "the breast lesions, while another segmentation branch integrates the features of\n",
      "the localization branch to fuse localization feature for segmenting breast lesions.\n",
      "note that the current spectral fea-\n",
      "tures ( ˆft, ˆft−1, and ˆft−2) are complex numbers and incompatible with the neural\n",
      "shifting more attention to breast lesion segmentation in ultrasound videos\n",
      "501\n",
      "layers.\n",
      "therefore we concatenate the real and imaginary parts of these com-\n",
      "plex numbers along the channel dimension respectively and thus obtain three\n",
      "new tensors (xt ∈ r2c×h×w, xt−1 ∈ r2c×h×w, and xt−2 ∈ r2c×h×w) with dou-\n",
      "ble channels.\n",
      "then, we element-wise multiply the obtained atten-\n",
      "tion map from each group with the input features, and the multiplication results\n",
      "(see y1 and y2) are then transformed into complex numbers by splitting them\n",
      "into real and imaginary parts along the channel dimension.\n",
      "finally, we further element-wisely add z1 and z2 and then pass it\n",
      "into a “bconv” layer to obtain the output feature ot of our ffa module.\n",
      "math-\n",
      "ematically, ot is computed by ot = bconv(z1 + z2), where “bconv” contains a\n",
      "3 × 3 convolution layer, a group normalization, and a relu activation function.\n",
      "3.2\n",
      "two-branch decoder\n",
      "after obtaining the frequency features, we introduce a two-branch decoder con-\n",
      "sisting of a segmentation branch and a localization branch to incorporate tem-\n",
      "poral features from nearby frames into the current frame.\n",
      "let d1\n",
      "s and d2\n",
      "s\n",
      "denote the features at the last two layers of the segmentation decoder branch,\n",
      "and d1\n",
      "l and d2\n",
      "l denote the features at the last two layers of the localization decoder\n",
      "branch.\n",
      "then, we element-wisely add d1\n",
      "l and d1\n",
      "s, and element-\n",
      "wisely add d2\n",
      "l and d2\n",
      "s, and pass the addition result into a “bconv” convolution\n",
      "layer to predict the segmentation map st of the input video frame it.\n",
      "to do so, we compute\n",
      "a bounding box of the annotated breast lesion segmentation result, and then take\n",
      "the center coordinates of the bounding box.\n",
      "quantitative comparisons between our fla-net and the state-of-the-art\n",
      "methods on our test set in terms of breast lesion segmentation in ultrasound videos.\n",
      "method\n",
      "image/video dice ↑ jaccard ↑ f1-score ↑ mae ↓\n",
      "unet\n",
      "[14]\n",
      "image\n",
      "0.745\n",
      "0.636\n",
      "0.777\n",
      "0.043\n",
      "unet++\n",
      "[19]\n",
      "image\n",
      "0.749\n",
      "0.633\n",
      "0.780\n",
      "0.039\n",
      "transunet [4]\n",
      "image\n",
      "0.733\n",
      "0.637\n",
      "0.784\n",
      "0.042\n",
      "setr\n",
      "[18]\n",
      "image\n",
      "0.709\n",
      "0.588\n",
      "0.748\n",
      "0.045\n",
      "stm\n",
      "λ3liou(st, gs\n",
      "t ),\n",
      "(2)\n",
      "where gh\n",
      "t\n",
      "and gs\n",
      "t denote the ground truth of the breast lesion segmentation\n",
      "and the breast lesion localization.\n",
      "we empirically set weights λ1 = λ2 = λ3 = 1.\n",
      "4\n",
      "experiments and results\n",
      "implementation details.\n",
      "[6] on the imagenet dataset, while the remaining components\n",
      "of our network were trained from scratch.\n",
      "prior to inputting the training video\n",
      "frames into the network, we resize them to 352×352 dimensions.\n",
      "our network is\n",
      "implemented in pytorch and employs the adam optimizer with a learning rate\n",
      "of 5 × 10−5, trained over 100 epochs, and a batch size of 24.\n",
      "shifting more attention to breast lesion segmentation in ultrasound videos\n",
      "503\n",
      "fig.\n",
      "3. visual comparisons of breast lesion segmentation results produced by our net-\n",
      "work and state-of-the-art methods.\n",
      "for more visual-\n",
      "ization results, please refer to the supplementary material.\n",
      "quantitative comparison results of ablation study experiments.\n",
      "our method\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "0.789\n",
      "0.687\n",
      "0.815\n",
      "0.033\n",
      "4.1\n",
      "comparisons with state-of-the-arts\n",
      "we conduct a comparative analysis between our network and nine state-of-the-\n",
      "art methods, comprising four image-based methods and ﬁve video-based meth-\n",
      "ods.\n",
      "four image-based methods are unet\n",
      "to ensure a fair and equitable compar-\n",
      "ison, we acquire the segmentation results of all nine compared methods by utiliz-\n",
      "ing either their publicly available implementations or by implementing them our-\n",
      "selves.\n",
      "additionally, we retrain these networks on our dataset and ﬁne-tune their\n",
      "network parameters to attain their optimal segmentation performance, enabling\n",
      "accurate and meaningful comparisons.\n",
      "the quantitative results of our network and the\n",
      "nine compared breast lesion segmentation methods are summarized in table 2.\n",
      "analysis of the results reveals that, in terms of quantitative metrics, video-based\n",
      "methods generally outperform image-based methods.\n",
      "[16] in terms of dice, jaccard,\n",
      "and f1-score metrics, and has a superior mae performance over pns+ [9] and\n",
      "dpstt [10].\n",
      "quantitative comparison results on diﬀerent video polyp segmentation\n",
      "datasets.\n",
      "for more quantitative results please refer to the supplementary material.\n",
      "figure 3 visually presents a comparison of breast\n",
      "lesion segmentation results obtained from our network and three other methods\n",
      "across various input video frames.\n",
      "apparently, our method accurately segments\n",
      "breast lesions of the input ultrasound video frames, although these target breast\n",
      "lesions have varied sizes and diverse shapes in the input video frames.\n",
      "the supe-\n",
      "rior metric performance of “basic+fla” and “basic+lb” compared to “basic”\n",
      "clearly indicates that our fla module and the localization encoder branch eﬀec-\n",
      "tively enhance the breast lesion segmentation performance in ultrasound videos.\n",
      "then, the superior performance of “basic+fla+lb” over “basic+fla” and\n",
      "“basic+lb” demonstrate that combining our fla module and the localization\n",
      "encoder branch can incur a more accurate segmentation result.\n",
      "moreover, our\n",
      "method has larger dice, jaccard, f1-score results and a smaller mae result than\n",
      "“basic+fla+lb”, which shows that our location-based contrastive loss has its\n",
      "contribution to the success of our video breast lesion segmentation method.\n",
      "4.3\n",
      "generalizability of our network\n",
      "to further evaluate the eﬀectiveness of our fla-net, we extend its application\n",
      "to the task of video polyp segmentation.\n",
      "following the experimental protocol\n",
      "shifting more attention to breast lesion segmentation in ultrasound videos\n",
      "505\n",
      "employed in a recent study on video polyp segmentation [8], we retrain our net-\n",
      "work and present quantitative results on two benchmark datasets, namely cvc-\n",
      "300-tv\n",
      "similarly, for the cvc-612-v dataset, our method achieves improve-\n",
      "ments of 0.012, 0.014, 0.019, and 0 in dice, iou, eφ, and mae scores, respec-\n",
      "tively.\n",
      "although our sα results (0.907 on cvc-300-tv and 0.920 on cvc-612-v)\n",
      "take the 2nd rank, they are very close to the best sα results, which are 0.909\n",
      "on cvc-300-tv and 0.923 on cvc-612-v. hence, the superior metric results\n",
      "obtained by our network clearly demonstrate its ability to accurately segment\n",
      "polyp regions more eﬀectively than state-of-the-art video polyp segmentation\n",
      "methods.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf:\n",
      "accurate automated segmentation of infected regions in ct\n",
      "images is crucial for predicting covid-19’s pathological stage and treat-\n",
      "ment response.\n",
      "although deep learning has shown promise in medical\n",
      "image segmentation, the scarcity of pixel-level annotations due to their\n",
      "expense and time-consuming nature limits its application in covid-19\n",
      "segmentation.\n",
      "in this paper, we propose utilizing large-scale unpaired chest\n",
      "x-rays with classiﬁcation labels as a means of compensating for the lim-\n",
      "ited availability of densely annotated ct scans, aiming to learn robust\n",
      "representations for accurate covid-19 segmentation.\n",
      "the encoder is built to capture optimal feature represen-\n",
      "tations for both ct and x-ray images.\n",
      "to facilitate information interaction\n",
      "between unpaired cross-modal data, we propose the kc that introduces\n",
      "a momentum-updated prototype learning strategy to condense modality-\n",
      "speciﬁc knowledge.\n",
      "the condensed knowledge is fed into the ki module\n",
      "for interaction learning, enabling the uci to capture critical features and\n",
      "relationships across modalities and enhance its representation ability for\n",
      "covid-19 segmentation.\n",
      "the results on the public covid-19 segmenta-\n",
      "tion benchmark show that our uci with the inclusion of chest x-rays can\n",
      "signiﬁcantly improve segmentation performance, outperforming advanced\n",
      "segmentation approaches including nnunet, cotr, nnformer, and swin\n",
      "unetr.\n",
      "keywords: covid-19 segmentation · unpaired data · cross-modal\n",
      "q. guan and y. xie—contributed equally to this work.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1 58.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43898-1_58\n",
      "604\n",
      "q. guan et al.\n",
      "1\n",
      "introduction\n",
      "the covid-19 pneumonia pandemic has posed an unprecedented global health\n",
      "crisis, with lung imaging as a crucial tool for identifying and managing aﬀected\n",
      "individuals [16].\n",
      "the latter has been\n",
      "the preferred method for detecting acute lung manifestations of the virus due to its\n",
      "exceptional imaging quality and ability to produce a 3d view of the lungs.\n",
      "eﬀec-\n",
      "tive segmentation of covid-19 infections using ct can provide valuable insights\n",
      "into the disease’s development, prediction of the pathological stage, and treatment\n",
      "response beyond just screening for covid-19 cases.\n",
      "however, the current method\n",
      "of visual inspection by radiologists for segmentation is time-consuming, requires\n",
      "specialized skills, and is unsuitable for large-scale screening.\n",
      "automated segmen-\n",
      "tation is crucial, but it is also challenging due to three factors: the infected regions\n",
      "often vary in shape, size, and location, appear similar to surrounding tissues, and\n",
      "can disperse within the lung cavity.\n",
      "the success of deep convolutional neural net-\n",
      "works (dcnns) in image segmentation has led researchers to apply this approach\n",
      "to covid-19 segmentation using ct scans\n",
      "this\n",
      "limited data scale currently constrains the potential of dcnns for covid-19 seg-\n",
      "mentation using ct scans.\n",
      "in comparison to ct scans, 2d chest x-rays are a more accessible and cost-\n",
      "eﬀective option due to their fast imaging speed, low radiation, and low cost,\n",
      "especially during the early stages of the pandemic [21].\n",
      "we advocate using chest x-ray datasets such as chestx-\n",
      "ray and chestxr may beneﬁt covid-19 segmentation using ct scans because\n",
      "of three reasons: (1) supplement limited ct data and contribute to training\n",
      "a more accurate segmentation model; (2) provide large-scale chest x-rays with\n",
      "labeled features, including pneumonia, thus can help the segmentation model\n",
      "to recognize patterns and features speciﬁc to covid-19 infections; and (3) help\n",
      "improve the generalization of the segmentation model by enabling it to learn from\n",
      "diﬀerent populations and imaging facilities.\n",
      "inspired by this, in this study, we\n",
      "propose a new learning paradigm for covid-19 segmentation using ct scans,\n",
      "involving training the segmentation model using limited ct scans with pixel-\n",
      "wise annotations and unpaired chest x-ray images with image-level labels.\n",
      "although “chilopod”-shaped multi-modal learning [6] has been\n",
      "unpaired cross-modal interaction learning for covid-19 segmentation\n",
      "605\n",
      "proposed to share all cnn kernels across modalities, it is still limited when\n",
      "the diﬀerent modalities have a signiﬁcant dimension gap.\n",
      "second, the presence\n",
      "of unpaired data, speciﬁcally ct and x-ray data, in the feature fusion/cross-\n",
      "attention interaction can potentially cause the model to learn incorrect or irrel-\n",
      "evant information due to the possible diﬀerences in their image distributions\n",
      "and objectives, leading to reduced covid-19 segmentation accuracy.\n",
      "it’s worth\n",
      "noting that the method using paired multimodal data [2] is not suitable for our\n",
      "application scenario, and the latest unpaired cross-modal [3] requires pixel-level\n",
      "annotations for both modalities, while our method can use x-ray images with\n",
      "image-level labels for training.\n",
      "this paper proposes a novel unpaired cross-modal interaction (uci) learn-\n",
      "ing framework for covid-19 segmentation, which aims to learn strong represen-\n",
      "tations from limited dense annotated ct scans and abundant image-level anno-\n",
      "tated x-ray images.\n",
      "the uci framework learns representations from both seg-\n",
      "mentation and classiﬁcation tasks.\n",
      "it includes three main components: a multi-\n",
      "modal encoder for image representations, a knowledge condensation and inter-\n",
      "action module for unpaired cross-modal data, and task-speciﬁc networks.\n",
      "this design enables the network to capture optimal feature represen-\n",
      "tations for both ct and x-ray images while maintaining the ability to learn\n",
      "shared representations between the two modalities despite dimensional diﬀer-\n",
      "ences.\n",
      "to address the challenge of information interaction between unpaired\n",
      "cross-modal data, we introduce a momentum-updated prototype learning strat-\n",
      "egy to condense modality-speciﬁc knowledge.\n",
      "this strategy groups similar repre-\n",
      "sentations into the same prototype and iteratively updates the prototypes with a\n",
      "momentum term to capture essential information in each modality.\n",
      "finally, the task-speciﬁc networks, including\n",
      "the segmentation decoder and classiﬁcation head, are presented to learn from all\n",
      "available labels.\n",
      "the proposed uci framework has signiﬁcantly improved per-\n",
      "formance on the public covid-19 segmentation benchmark [15], thanks to the\n",
      "inclusion of chest x-rays.\n",
      "the main contributions of this paper are three-fold: (1) we are the ﬁrst to\n",
      "employ abundant x-ray images with image-level annotations to improve covid-\n",
      "19 segmentation on limited ct scans, where the ct and x-ray data are unpaired\n",
      "and have potential distributional diﬀerences; (2) we introduce the knowledge con-\n",
      "densation and interaction module, in which the momentum-updated prototype\n",
      "learning is oﬀered to concentrate modality-speciﬁc knowledge, and a knowledge-\n",
      "guided interaction module is proposed to harness the learned knowledge for\n",
      "boosting the representations of each modality; and (3) our experimental results\n",
      "demonstrate our uci learning method’s eﬀectiveness and strong generalizability\n",
      "in covid-19 segmentation and the potential for related disease screening.\n",
      "2\n",
      "approach\n",
      "the proposed uci aims to explore eﬀective representations for covid-19 seg-\n",
      "mentation by leveraging both limited dense annotated ct scans and abundant\n",
      "image-level annotated x-rays.\n",
      "figure 1 illustrates the three primary components\n",
      "of the uci framework: a multi-modal encoder used to extract features from each\n",
      "modality, the knowledge condensation and interaction module used to model\n",
      "unpaired cross-modal dependencies, and task-speciﬁc heads designed for seg-\n",
      "mentation and classiﬁcation purposes.\n",
      "2.1\n",
      "multi-modal encoder\n",
      "the multi-modal encoder f(·) consists of three stages of blocks, with modality-\n",
      "speciﬁc patch embedding layers and shared transformer layers in each block,\n",
      "capturing modality-speciﬁc and shared patterns, which can be more robust and\n",
      "discriminative across modalities.\n",
      "notice that due to the dimensional gap between\n",
      "ct and x-ray, we use the 2d convolution block as patch embedding for x-rays\n",
      "and the 3d convolution block as patch embedding for cts.\n",
      "in each stage, the\n",
      "patch embedding layers down-sample the inputs and generate the sequence of\n",
      "modality-speciﬁc embedded tokens.\n",
      "given a ct volume xct, and a chest x-ray image xcxr, we denote the output\n",
      "feature sequence of the multi-modal encoder as\n",
      "f ct = f(xct; 3d)\n",
      "∈ rccxr×n cxr\n",
      "(1)\n",
      "unpaired cross-modal interaction learning for covid-19 segmentation\n",
      "607\n",
      "where cct and ccxr represent the channels of ct and x-ray feature sequence.\n",
      "1(a), we design a knowledge condensation\n",
      "(kc) module by introducing a momentum-updated prototype learning strategy\n",
      "to condensate valuable knowledge in each modality from the learned features.\n",
      "then we introduce a momentum learning function to\n",
      "update the prototypes with ccxr\n",
      "i\n",
      ", which means that the updates at each iter-\n",
      "ation not only depend on the current ccxr\n",
      "i\n",
      "but also consider the direction and\n",
      "magnitude of the previous updates, deﬁned as\n",
      "pcxr\n",
      "i\n",
      "← λpcxr\n",
      "i\n",
      "+ (1 − λ)\n",
      "1\n",
      "ccxr\n",
      "i\n",
      "\u0005\n",
      "m∈ccxr\n",
      "i\n",
      "m,\n",
      "(3)\n",
      "where λ is the momentum factor, which controls the inﬂuence of the previous\n",
      "update on the current update.\n",
      "the momentum term allows prototypes to move more smoothly and\n",
      "consistently towards the optimal position, even in the presence of noise or other\n",
      "factors that might cause the prototypes to ﬂuctuate.\n",
      "inspired\n",
      "by the knowledge prototypes, ki modules boost the interaction between the two\n",
      "modalities and allow for the learning of strong representations for covid-19\n",
      "segmentation and x-ray classiﬁcation tasks.\n",
      "2.3\n",
      "task-speciﬁc networks\n",
      "the outputs of the ki module are fed into two multi-task heads - one decoder\n",
      "for segmentation and one prediction head for classiﬁcation respectively.\n",
      "the seg-\n",
      "mentation decoder has a symmetric structure with the encoder, consisting of\n",
      "three stages.\n",
      "in each stage, the input feature map is ﬁrst up-sampled by the\n",
      "3d patch embedding layer, and then reﬁned by the stacked transformer layers.\n",
      "the decoder includes a segmen-\n",
      "tation head for ﬁnal prediction.\n",
      "we use the deep supervision strategy by adding auxiliary segmentation losses\n",
      "(i.e., the sum of the dice loss and cross-entropy loss) to the decoder at diﬀerent\n",
      "scales.\n",
      "3\n",
      "experiment\n",
      "3.1\n",
      "materials\n",
      "we used the public covid-19 segmentation benchmark\n",
      "it is collected from two public resources [5,8] on chest ct images\n",
      "available on the cancer imaging archive (tcia)\n",
      "all ct images were\n",
      "acquired without intravenous contrast enhancement from patients with posi-\n",
      "tive reverse transcription polymerase chain reaction (rt-pcr) for sars-\n",
      "cov-2.\n",
      "in total, we used 199 ct images including 149 training images and 50\n",
      "test images.\n",
      "the chestx-\n",
      "ray14 dataset comprises 112,120 x-ray images showing positive cases from 30,805\n",
      "patients, encompassing 14 disease image labels pertaining to thoracic and lung\n",
      "ailments.\n",
      "an image may contain multiple or no labels.\n",
      "unpaired cross-modal interaction learning for covid-19 segmentation\n",
      "609\n",
      "3.2\n",
      "implementation details\n",
      "for ct data, we ﬁrst truncated the hu values of each scan using the range\n",
      "of [−958, 327] to ﬁlter irrelevant regions, and then normalized truncated voxel\n",
      "values by subtracting 82.92 and dividing by 136.97.\n",
      "we randomly cropped sub-\n",
      "volumes of size 32 × 256 × 256 as the input and employed the online data aug-\n",
      "mentation like [10] to diversify the ct training set.\n",
      "we employ the online data argumen-\n",
      "tation, including random cropping and zooming, random rotation, and horizon-\n",
      "tal/vertical ﬂip, to enlarge the x-ray training dataset.\n",
      "we follow the extension of\n",
      "[20] for weight initialization and use the adamw optimizer [11] and empirically\n",
      "set the initial learning rate to 0.0001, batch size to 2 and 32 for segmentation\n",
      "and classiﬁcation, maximum iterations to 25w, momentum factor λ to 0.99, and\n",
      "the number of prototypes k to 256.\n",
      "to evaluate the covid-19 segmentation performance, we utilized six met-\n",
      "rics, including the dice similarity coeﬃcient (dsc), intersection over union\n",
      "(iou), sensitivity (sen), speciﬁcity (spe), hausdorﬀ distance (hd), and aver-\n",
      "age surface distance (asd).\n",
      "these metrics provide a comprehensive assessment\n",
      "of the segmentation quality.\n",
      "the overlap-based metrics, namely dsc, iou, sen,\n",
      "and spe, range from 0 to 1, with a higher score indicating better performance.\n",
      "on the other hand, hd and asd are shape distance-based metrics that mea-\n",
      "sure the dissimilarity between the surfaces or boundaries of the segmentation\n",
      "output and the ground truth.\n",
      "for hd and asd, a lower value indicates better\n",
      "segmentation results.\n",
      "3.3\n",
      "compared with advanced segmentation approaches\n",
      "table 1 gives the performance of our models and four advanced competing\n",
      "ones, including nnunet [10], cotr\n",
      "[24], and swin unetr [9]\n",
      "in covid-19 lesion segmentation.\n",
      "this suggests that the\n",
      "segmentation outcomes generated by our models are in good agreement with\n",
      "the ground truth.\n",
      "notably, despite chestxr being more focused on covid-19\n",
      "recognition, the uci model aided by the chestx-ray14 dataset containing 80k\n",
      "images performs better than the uci model using the chestxr dataset with only\n",
      "16k images.\n",
      "this suggests that having a larger auxiliary dataset can improve the\n",
      "segmentation performance even if it is not directly related to the target task.\n",
      "the results also further prove the eﬀectiveness of using a wealth of chest x-rays\n",
      "to assist the covid-19 segmentation under limited cts.\n",
      "this\n",
      "reduction demonstrates that our segmentation results provide highly accurate\n",
      "boundaries that closely match the ground-truth boundaries.\n",
      "quantitative results of advanced segmentation approaches on the test set.\n",
      "2. we set the maximum iterations to 8w and\n",
      "use chestx-ray14 as auxiliary data for all ablation experiments.\n",
      "we compare ﬁve\n",
      "variants of our uci: (1) baseline: trained solely on densely annotated ct images;\n",
      "(2) w/o shared encoder: replacing the multi-modal encoder with two indepen-\n",
      "dent encoders, each designed to learn features from a separate modality; (3) w/o\n",
      "kc: removing the prototype and using the features before kc for interaction;\n",
      "(4) w/o kc & ki: only with encoder to share multi-modal information; and (5)\n",
      "w/o warm-up: removing the prototype warm-up in ki.\n",
      "firstly, our uci model, which jointly uses chest x-rays,\n",
      "outperforms the baseline segmentation results by up to 1.69%, highlighting the\n",
      "eﬀectiveness of using cheap large-scale auxiliary images.\n",
      "secondly, using only a\n",
      "shared encoder for multi-modal learning (uci w/o kc & ki) can still bring a\n",
      "segmentation gain of 0.96%, and the multi-modal encoder outperforms building\n",
      "independent modality-speciﬁc networks (uci w/o shared encoder), underscoring\n",
      "the importance of shared networks.\n",
      "to evaluate the impact of hyper-parameter set-\n",
      "tings on covid-19 segmentation, we conducted an investigation of the number\n",
      "of prototypes (k) and the number of momentum factors (λ).\n",
      "figure 3 illustrates\n",
      "unpaired cross-modal interaction learning for covid-19 segmentation\n",
      "611\n",
      "fig.\n",
      "3. dice scores of uci versus left: the number of prototypes k and right the\n",
      "number of momentum factors λ.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf:\n",
      "however, com-\n",
      "pared to the convolutional neural network (cnn) models, it has been\n",
      "observed that the vit models struggle to capture high-frequency compo-\n",
      "nents of images, which can limit their ability to detect local textures and\n",
      "edge information.\n",
      "as abnormalities in human tissue, such as tumors and\n",
      "lesions, may greatly vary in structure, texture, and shape, high-frequency\n",
      "information such as texture is crucial for eﬀective semantic segmentation\n",
      "tasks.\n",
      "furthermore, we introduce a novel\n",
      "eﬃcient enhancement multi-scale bridge that eﬀectively transfers spatial\n",
      "information from the encoder to the decoder while preserving the fun-\n",
      "damental features.\n",
      "we demonstrate the eﬃcacy of laplacian-former on\n",
      "multi-organ and skin lesion segmentation tasks with +1.87% and +0.76%\n",
      "dice scores compared to sota approaches, respectively.\n",
      "our implemen-\n",
      "tation is publically available at github.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1 70.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43898-1_70\n",
      "laplacian-former\n",
      "737\n",
      "keywords: deep learning · texture · segmentation · laplacian\n",
      "transformer\n",
      "1\n",
      "introduction\n",
      "the recent advancements in transformer-based models have revolutionized the\n",
      "ﬁeld of natural language processing and have also shown great promise in a wide\n",
      "range of computer vision tasks [5].\n",
      "as a notable example, the vision trans-\n",
      "former (vit) model utilizes multi-head self-attention (msa) blocks to globally\n",
      "model the interactions between semantic tokens created by treating local image\n",
      "patches as individual elements [7].\n",
      "this approach stands in contrast to cnns,\n",
      "which hierarchically increase their receptive ﬁeld from local to global to capture\n",
      "a global semantic representation.\n",
      "nevertheless, recent studies [3,20] have shown\n",
      "that vit models struggle to capture high-frequency components of images, which\n",
      "can limit their ability to detect local textures and it is vital for many diagnostic\n",
      "and prognostic tasks.\n",
      "this weakness in local representation can be attributed\n",
      "to the way in which vit models process images.\n",
      "vit models split an image\n",
      "into a sequence of patches and model their dependencies using a self-attention\n",
      "mechanism, which may not be as eﬀective as the convolution operation used in\n",
      "cnn models in extracting local features within receptive ﬁelds.\n",
      "this diﬀerence\n",
      "in how vit and cnn process images may explain the superior performance of\n",
      "cnn models in local feature extraction [1,8]. innovative approaches have been\n",
      "proposed in recent years to address the insuﬃcient local texture representation\n",
      "within transformer models.\n",
      "one such approach is the integration of cnn and\n",
      "vit features through complementary methods, aimed at seamlessly blending the\n",
      "strengths of both in order to compensate for any shortcomings in local represen-\n",
      "tation [5].\n",
      "transformers as a complement to cnns: transunet\n",
      "[11] proposed a novel solution called hiformer, which\n",
      "leverages a swin transformer module and a cnn-based encoder to generate two\n",
      "multi-scale feature representations, which are then integrated via a double-level\n",
      "fusion module.\n",
      "[10] used a transformer to create a powerful encoder\n",
      "with a cnn decoder for 3d medical image segmentation.\n",
      "[22],\n",
      "the segmentation performance in low-resolution stages was improved.\n",
      "despite\n",
      "these advances, there remain some limitations in these methods such as com-\n",
      "putationally ineﬃciency (e.g., transunet model), the requirement of a heavy\n",
      "cnn backbone (e.g., hiformer), and the lack of consideration for multi-scale\n",
      "information.\n",
      "these limitations have resulted in less eﬀective network learning\n",
      "results in the ﬁeld of medical image segmentation.\n",
      "new attention models: the redesign of the self-attention mechanism within\n",
      "pure transformer models is another method aiming to augment feature repre-\n",
      "738\n",
      "r. azad et al.\n",
      "sentation to enhance the local feature representation ultimately.\n",
      "former [24] is a pure transformer-based pipeline that comprises a double atten-\n",
      "tion module to capture locally ﬁne-grained attention and interaction with dif-\n",
      "ferent units in a dilated manner through its mechanism.\n",
      "drawbacks of transformers: recent research has revealed that traditional\n",
      "self-attention mechanisms, while eﬀective in addressing local feature discrepan-\n",
      "cies, have a tendency to overlook important high-frequency information such\n",
      "as texture and edge details [21].\n",
      "this is especially problematic for tasks like\n",
      "tumor detection, cancer-type identiﬁcation through radiomics analysis, as well\n",
      "as treatment response assessment, where abnormalities often manifest in texture.\n",
      "➋ we also introduce a novel eﬃcient\n",
      "enhancement multi-scale bridge that eﬀectively transfers spatial information\n",
      "from the encoder to the decoder while preserving the fundamental features.\n",
      "➌ our method not only alleviates the problem of the traditional self-attention\n",
      "mechanism mentioned above, but also it surpasses all its counterparts in terms\n",
      "of diﬀerent evaluation metrics for the tasks of medical image segmentation.\n",
      "1, taking an input image x ∈\n",
      "rh×w ×c with spatial dimensions h and w, and c channels, it is ﬁrst passed\n",
      "through a patch embedding module to obtain overlapping patch tokens of size\n",
      "4 × 4 from the input image.\n",
      "the proposed model comprises four encoder blocks,\n",
      "each containing two eﬃcient enhancement transformer layers and a patch merg-\n",
      "ing layer that downsamples the features by merging 2 × 2 patch tokens and\n",
      "increasing the channel dimension.\n",
      "the decoder is composed of three eﬃcient\n",
      "enhancement transformer blocks and four patch-expanding blocks, followed by\n",
      "a segmentation head to retrieve the ﬁnal segmentation map.\n",
      "laplacian-former\n",
      "then employs a novel eﬃcient enhancement multi-scale bridge to capture local\n",
      "laplacian-former\n",
      "739\n",
      "fig.\n",
      "2.1\n",
      "eﬃcient enhancement transformer block\n",
      "in medical imaging, it is important to distinguish diﬀerent structures and tis-\n",
      "sues, especially when tissue boundaries are ill-deﬁned.\n",
      "this is often the case for\n",
      "accurate segmentation of small abnormalities, where high-frequency information\n",
      "plays a critical role in deﬁning boundaries by capturing both textures and edges.\n",
      "inspired by this, we propose an eﬃcient enhancement transformer block that\n",
      "incorporates an eﬃcient frequency attention (ef-att) mechanism to capture\n",
      "contextual information of an image while recalibrating the representation space\n",
      "within an attention mechanism and recovering high-frequency details.\n",
      "our eﬃcient enhancement transformer block ﬁrst takes a layernorm (ln)\n",
      "from the input x.\n",
      "it is proved in [19]\n",
      "that as transformers become deeper, their features become less varied, which\n",
      "restrains their representation capacity and prevents them from attaining optimal\n",
      "performance.\n",
      "to address this issue, we have implemented an augmented short-\n",
      "740\n",
      "r. azad et al.\n",
      "fig.\n",
      "the structure of our frequency enhancement transformer block.\n",
      "cut method from [9], a diversity-enhanced shortcut (des), employing a kro-\n",
      "necker decomposition-based projection.\n",
      "this approach involves inserting addi-\n",
      "tional paths with trainable parameters alongside the original shortcut x, which\n",
      "enhances feature diversity and improves performance while requiring minimal\n",
      "hardware resources.\n",
      "this ﬁnal step completes\n",
      "our eﬃcient enhancement transformer block, as illustrated in fig.\n",
      "2.\n",
      "2.2\n",
      "eﬃcient frequency attention (ef-att)\n",
      "the traditional self-attention block computes the attention score s using query\n",
      "(q) and key (k) values, normalizes the result using softmax, and then multiplies\n",
      "the normalized attention map with value (v):\n",
      "s(q, k, v) = softmax\n",
      "\u0002\n",
      "qkt\n",
      "√dk\n",
      "\u0003\n",
      "v,\n",
      "(1)\n",
      "where dk is the embedding dimension.\n",
      "2.3\n",
      "eﬃcient enhancement multi-scale bridge\n",
      "it is widely known that eﬀectively integrating multi-scale information can lead\n",
      "to improved performance\n",
      "thus, we introduce the eﬃcient enhancement\n",
      "multi-scale bridge as an alternative to simply concatenating the features from\n",
      "the encoder and decoder layers.\n",
      "1, deliv-\n",
      "ers spatial information to each decoder layer, enabling the recovery of intricate\n",
      "details while generating output segmentation masks.\n",
      "in this approach, we aim\n",
      "742\n",
      "r. azad et al.\n",
      "to calculate the eﬃcient attention mechanism for each level and fuse the multi-\n",
      "scale information in their context; thus, it is important that all levels’ embedding\n",
      "dimension is of the same size.\n",
      "we then use a summation module to\n",
      "aggregate the global context of all levels and reshape the query for matrix mul-\n",
      "tiplication with the augmented global context.\n",
      "taking the second level with the\n",
      "dimension of h\n",
      "8 × w\n",
      "8 ×2c, the key and value are mapped to ( h\n",
      "8\n",
      "w\n",
      "8 )×c, and the\n",
      "query to (2 h\n",
      "8\n",
      "w\n",
      "8 ) ×\n",
      "c. the augmented global context with the shape of c × c\n",
      "is then multiplied by the query, resulting in an enriched feature map with the\n",
      "shape of (2 h\n",
      "8\n",
      "w\n",
      "8 ) × c. we reshape the obtained feature map into h\n",
      "8 × w\n",
      "8 × 2c\n",
      "and feed it through an ln and mix-ffn module with a skip connection to\n",
      "empower the feature representations.\n",
      "a batch size of 24 and a stochastic gradient descent\n",
      "algorithm with a base learning rate of 0.05, a momentum of 0.9, and a weight\n",
      "decay of 0.0001 was utilized during the training process, which was carried out\n",
      "for 400 epochs.\n",
      "we also followed [2] experiments to\n",
      "evaluate our method on the isic 2018 skin lesion dataset\n",
      "[6] with 2,694 images.\n",
      "3. segmentation results of the proposed method on the synapse dataset.\n",
      "synapse multi-organ segmentation: table 1 presents a comparison of our\n",
      "proposal with previous sota methods using the dsc and hd metrics across\n",
      "eight abdominal organs.\n",
      "figure 3 illustrates a qualitative result\n",
      "of our method for diﬀerent organ segmentation, speciﬁcally we can observe that\n",
      "the lalacianformer produces a precise boundary segmentation on gallbladder,\n",
      "liver, and stomach organs.\n",
      "it is noteworthy to mention that our pipeline, as\n",
      "a pure transformer-based architecture trained from scratch without pretraining\n",
      "weights, outperforms all previously presented network architectures.\n",
      "skin lesion segmentation: table 2a shows the comparison results of our pro-\n",
      "posed method, laplacian-former, against leading methods on the skin lesion seg-\n",
      "mentation benchmark.\n",
      "our method achieves superior performance by utilizing the frequency attention\n",
      "in a pyramid scale to model local textures.\n",
      "speciﬁcally, our frequency attention\n",
      "emphasizes the ﬁne details and texture characteristics that are indicative of skin\n",
      "lesion structures and ampliﬁes regions with signiﬁcant intensity variations, thus\n",
      "accentuating the texture patterns present in the image and resulting in better\n",
      "performance.\n",
      "it is evident standard\n",
      "design frequency response in deep layers of structure attenuates more than the\n",
      "laplacianformer, which is a visual endorsement of the capability of laplacian-\n",
      "744\n",
      "r. azad et al.\n",
      "table\n",
      "2.\n",
      "(a) performance comparison of laplacian-former against the sota\n",
      "approaches on isic 2018 skin lesion datset.\n",
      "the supplementary\n",
      "provides more visualization results.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf:\n",
      "pulmonary nodules and masses are crucial imaging features\n",
      "in lung cancer screening that require careful management in clinical diag-\n",
      "nosis.\n",
      "despite the success of deep learning-based medical image segmen-\n",
      "tation, the robust performance on various sizes of lesions of nodule and\n",
      "mass is still challenging.\n",
      "speciﬁcally, we introduce an adaptive scale-aware test-time click adap-\n",
      "tation method based on eﬀortlessly obtainable lesion clicks as test-time\n",
      "cues to enhance segmentation performance, particularly for large lesions.\n",
      "extensive experiments on both open-source and in-house datasets\n",
      "consistently demonstrate the eﬀectiveness of the proposed method over\n",
      "some cnn and transformer-based segmentation methods.\n",
      "keywords: pulmonary lesion segmentation · pulmonary mass\n",
      "segmentation · test-time adaptation · multi-scale\n",
      "1\n",
      "introduction\n",
      "lung cancer is the main cause of cancer death worldwide [18]. pulmonary nod-\n",
      "ules and masses are both features present in computed tomography images that\n",
      "aid in the diagnosis of lung cancer.\n",
      "1. (a): visualization on results of four large-scale mass segmentation given by nnu-\n",
      "net baseline\n",
      "compared with the ground-truth segmentation, the recall rate for these\n",
      "four samples is 46.29%, 58.34%, 79.51%, and 68.51%, respectively.\n",
      "existing methods have low\n",
      "recall rates for the segmentation of large scale nodules and masses.\n",
      "[27] and determining follow-up treatment.\n",
      "lesion\n",
      "segmentation can be utilized to evaluate two important factors: the volume of\n",
      "the lesion and its growth rate [5,6,8,12].\n",
      "[14,17].\n",
      "segmenting nodules is a tedious task that requires signiﬁcant human labor.\n",
      "however, the accuracy of the 3d nodule segmentation model is prone\n",
      "to signiﬁcantly decline in the application, regardless of whether its structure is\n",
      "based on cnn or transformer [2].\n",
      "1(a–c), the recall rate of the\n",
      "large-scale nodule and mass is usually lower than the average level.\n",
      "this makes\n",
      "the pulmonary nodule and mass segmentation task resemble a long-tail problem\n",
      "rather than a mere large scale span problem.\n",
      "this leads to unsatisfactory results\n",
      "when segmenting large lesions that require more accurate delineation [26].\n",
      "[4], where the input images are resized to diﬀerent resolu-\n",
      "test-time click adaptation for pulmonary lesion segmentation\n",
      "683\n",
      "tion ratios.\n",
      "some other methods leverage multi-scale feature maps to capture\n",
      "information from diﬀerent scales, such as cross-scale feature fusion [19] or using\n",
      "multi-scale convolutional ﬁlters\n",
      "furthermore, the attention mechanisms [23]\n",
      "has also been utilized to emphasize the features that are more relevant for seg-\n",
      "mentation.\n",
      "though these methods have achieved impressive performance, they\n",
      "still struggle to accurately segment the extremely imbalanced multi-scale lesions.\n",
      "recently, some click-based lesion segmentation methods [19–21] introduce the\n",
      "click at the input or feature level and modify the network accordingly, result-\n",
      "ing in higher accuracy results.\n",
      "this helps to improve the segmentation\n",
      "performance of large-scale nodules and masses.\n",
      "experimental results on two public datasets and one in-house dataset\n",
      "demonstrate that the proposed method outperforms existing methods with dif-\n",
      "ferent backbones.\n",
      "2\n",
      "method\n",
      "2.1\n",
      "restatement of image segmentation based on click\n",
      "for pulmonary nodule and mass segmentation, existing methods mostly rely on\n",
      "regions of interest (roi) obtained by lesion detection networks.\n",
      "a set of 3d roi\n",
      "inputs i can be represented as i ∈ rd×h×w with size (d, h, w), along with\n",
      "its corresponding segmentation ground truth of nodules and masses represented\n",
      "by s ∈ (0, 1)d×h×w .\n",
      "for each roi input, the center point c of the lesion, which is represented\n",
      "as pc = ( d\n",
      "2 , h\n",
      "2 , w\n",
      "2 ) in cartesian coordinate system, can be used as a reference\n",
      "point to assist the network in improving segmentation performance.\n",
      "we ﬁrst get the predicted segmentation ˆsi and compute its minimum\n",
      "3d bounding box b from the trained model.\n",
      "we also adopt a multi-\n",
      "scale input encoder to further improve the segmentation performance of nodules and\n",
      "masses with diﬀerent scales.\n",
      "to achieve this, we employ a clipping strategy to adjust the proportion of fore-\n",
      "ground and background in the input image, producing a group of input images\n",
      "with dimensions of 64×96×96, 32×48×48, and 16×24×24.\n",
      "these images are\n",
      "then passed through three convolution paths.\n",
      "due to diﬀerences in the statistical distribution of pulmonary\n",
      "nodule scale in image data from diﬀerent medical centers, the segmentation\n",
      "results of some images, especially for large nodules, are worse than expected.\n",
      "for such scenarios, we propose the scale-aware test-time click adaptation\n",
      "method, which can improve the performance of segmentation results for large-\n",
      "scale nodules and masses by adjusting some of the network parameters during\n",
      "testing.\n",
      "first, we use\n",
      "the pre-trained network to pre-segment the input ct from the test set, get-\n",
      "ting ˆsi = θ (ii) (i = 1, 2, · · · , n) where n is the number of samples in the test\n",
      "set.\n",
      "then we make a projection on the main connected region of ˆsi along three\n",
      "test-time click adaptation for pulmonary lesion segmentation\n",
      "685\n",
      "coordinate axes to obtain the size of the bounding box bi = (d, w, h) of the\n",
      "pre-segmentation result, and generate an ellipsoid mi with three axes length\n",
      "proportional to the corresponding side length of the bounding box bi.\n",
      "more\n",
      "formally, the coordinates of any foreground voxel point v : (x, y, z) in mi meets\n",
      "the following requirement:\n",
      "(x − d\n",
      "2 )2\n",
      "r (d)2\n",
      "+ (y − h\n",
      "2 )2\n",
      "r (h)2\n",
      "+ (z − w\n",
      "2 )2\n",
      "r (w)2\n",
      "= 1,\n",
      "(1)\n",
      "where r represents the mapping function between the axis length of the ellipsoid\n",
      "and the side length of the bounding box bi.\n",
      "ing adaptive click adjustment\n",
      "2.4\n",
      "training objective of sattca\n",
      "we use the foreground range of adaptively adjusted ellipsoid mi to mask ˆsi to\n",
      "obtain a masked segmentation ˆsm\n",
      "i .\n",
      "formally, ltt is given by:\n",
      "ltt = lbce + σldice + γlent,\n",
      "(3)\n",
      "where σ and γ are hyper-parameters set to 0.5 and 1 in all experiments,\n",
      "respectively.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "datasets and evaluation protocols\n",
      "we experiment on two public datasets and one in-house dataset.\n",
      "686\n",
      "z. li et al.\n",
      "table 1. performance of diﬀerent backbones with or without the proposed sattca\n",
      "and other click-based methods.\n",
      "experiments are conducted with various pulmonary\n",
      "nodule segmentation datasets using 3d nnunet [7], transbts [25] and nnunet with\n",
      "multi-scale input encoder (ms) as the backbone.\n",
      "comparative experiments are carried\n",
      "out with the click-based methods in [19,20] and simple test-time click adaptation on\n",
      "ms-unet.\n",
      "[1]: the lidc dataset is a publicly available lung ct image database\n",
      "containing 1018 scans, developed by the lung image database consortium\n",
      "(lidc).\n",
      "to generate the ground truth for each nodule and mass,\n",
      "we combined the segmentation annotations from diﬀerent raters.\n",
      "overall, we\n",
      "selected a total of 1625 nodules and masses that were annotated by more than\n",
      "three raters from the lidc dataset for the experiment.\n",
      "each ct scan in the dataset has been segmented\n",
      "by at least one radiologist.\n",
      "we exclude nodules and masses with diameters larger\n",
      "than 64 mm or smaller than 2 mm, as the diameter of the largest mass in the\n",
      "public dataset is no more than 64 mm.\n",
      "evaluation metrics: the performance of the nodule segmentation is evaluated\n",
      "by three metrics: volume-based dice similarity coeﬃcient (dsc), surface-based\n",
      "normalized surface dice (nsd)\n",
      "[13], and recall rate, which calculates the shape\n",
      "similarity between predictions and ground truth.\n",
      "test-time click adaptation for pulmonary lesion segmentation\n",
      "687\n",
      "table 2.\n",
      "the performance variation (%) of using test-time click adaptation (ttca)\n",
      "and scale-aware ttca (sattca) on the nnunet\n",
      "δ nsd δ recall\n",
      "w/test-time click adaptation\n",
      "micro\n",
      "–0.034\n",
      "–0.072\n",
      "0.440\n",
      "–0.078\n",
      "–0.017\n",
      "–0.089\n",
      "–0.063\n",
      "–0.042\n",
      "0.334\n",
      "small\n",
      "0.042\n",
      "–0.087\n",
      "0.824\n",
      "0.038\n",
      "0.066\n",
      "0.218\n",
      "–0.016\n",
      "–0.046\n",
      "0.399\n",
      "medium 0.147\n",
      "0.114\n",
      "0.795\n",
      "0.136\n",
      "0.195\n",
      "0.217\n",
      "0.040\n",
      "0.023\n",
      "0.431\n",
      "mass\n",
      "0.530\n",
      "0.465\n",
      "0.994\n",
      "–0.007\n",
      "–0.006\n",
      "–0.002\n",
      "0.107\n",
      "0.099\n",
      "0.378\n",
      "w/scale-aware test-time click adaptation\n",
      "micro\n",
      "0.593\n",
      "0.642\n",
      "1.256\n",
      "0.783\n",
      "0.277\n",
      "0.253\n",
      "0.900\n",
      "0.892\n",
      "0.518\n",
      "small\n",
      "0.831\n",
      "0.944\n",
      "1.547\n",
      "0.740\n",
      "0.296\n",
      "0.278\n",
      "1.109\n",
      "1.045\n",
      "0.881\n",
      "medium 1.337\n",
      "1.892\n",
      "1.693\n",
      "0.930\n",
      "1.014\n",
      "0.964\n",
      "1.324\n",
      "1.219\n",
      "1.646\n",
      "mass\n",
      "2.963\n",
      "3.182\n",
      "2.701\n",
      "2.590\n",
      "3.558\n",
      "3.093\n",
      "1.710\n",
      "1.656\n",
      "2.676\n",
      "3.2\n",
      "implementation details\n",
      "the roi of the lesion is a patch cropped around nodules or masses from the\n",
      "original ct scans with shape 64 × 96 × 96.\n",
      "all experiments are\n",
      "conducted on 4 nvidia rtx 3090 gpus with pytorch 1.11.0\n",
      "[7] and transbts [25] as the backbone to evaluate the pro-\n",
      "posed method on pulmonary nodule and mass segmentation.\n",
      "its adaptive framework makes it well-\n",
      "suited for pulmonary nodule segmentation.\n",
      "transbts is a 3d medical image seg-\n",
      "mentation network with a hybrid architecture of transformer and cnn.\n",
      "the experimental results presented in table 1, consistently\n",
      "demonstrate that the cnn-based network can achieve better results in multi-\n",
      "scale pulmonary nodule and mass segmentation tasks across all three datasets.\n",
      "this is mainly due to the fact that large receptive ﬁelds may involve background\n",
      "features that are not conducive to segmentation inference for micro or small\n",
      "nodules.\n",
      "in datasets such as lidc and in-house, where the number imbalance\n",
      "of multi-scale lesion phenomena is more notable, the multi-input method consis-\n",
      "tently outperforms the other two baselines.\n",
      "we also implemented comparative experiments with other click methods [20]\n",
      "and [19].\n",
      "as depicted in table 1, the experimental results show that using a point\n",
      "688\n",
      "z. li et al.\n",
      "fig.\n",
      "3. (a): visualization of some segmentation results predicted by the baseline with-\n",
      "out test-time adaptation (tta), with test-time click adaptation (ttca), and scale-\n",
      "aware test-time click adaptation (sattca).\n",
      "the recall rates show that sattca sig-\n",
      "niﬁcantly improves the segmentation performance for large nodules and masses.\n",
      "and a ﬁxed range of gaussian intensity expansion in the case of large ﬂuctua-\n",
      "tions in the size of the nodules does not take eﬀect in improving the segmentation\n",
      "performance.\n",
      "the inferior segmentation results of [19] can be attributed to the\n",
      "fact that when it fuses features of diﬀerent depths and scales, the number of\n",
      "channels in the feature map remains the same, and some of the up-sampling or\n",
      "down-sampling strides are too large, leading to redundancy in shallow features\n",
      "and a lack of deep features.\n",
      "moreover, the sattca improves the dice coeﬃ-\n",
      "cient and surface-based normalized surface dice of segmentation results in both\n",
      "networks.\n",
      "3, the recall rate of large nodule\n",
      "segmentation is signiﬁcantly improved.\n",
      "we further analyze the performance of sattca.\n",
      "firstly, we present the\n",
      "quantitative comparison in table 2, where we group the nodules and masses in\n",
      "each dataset at 10 mm intervals and calculate the average segmentation perfor-\n",
      "mance diﬀerences of the nodules in each scale group.\n",
      "the statistical results show\n",
      "that the proposed sattca signiﬁcantly improves the recall rate of the segmen-\n",
      "tation on large nodules and masses.\n",
      "3(a), for nodules smaller\n",
      "than 20 mm, both ttca and sattca eﬀectively increase the recall rate of\n",
      "predicted segmentation.\n",
      "for the medium nodule and mass, our sattca proves\n",
      "to be more eﬀective in improving segmentation performance.\n",
      "the diﬀerence between the two scatter\n",
      "diagrams indicates that the proposed sattca eﬀectively alleviates the issue of\n",
      "test-time click adaptation for pulmonary lesion segmentation\n",
      "689\n",
      "extremely imbalanced lesion scales, and improves the segmentation performance\n",
      "for large lesions.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_72.pdf:\n",
      "limited by expensive pixel-level labels, polyp segmentation\n",
      "models are plagued by data shortage and suﬀer from impaired general-\n",
      "ization.\n",
      "thus, to reduce labeling cost, we propose to learn\n",
      "a weakly supervised polyp segmentation model (i.e.,weakpolyp) com-\n",
      "pletely based on bounding box annotations.\n",
      "by explicitly aligning predictions across the same image\n",
      "at diﬀerent scales, the sc loss largely reduces the variation of predic-\n",
      "tions.\n",
      "extensive experiments demonstrate the eﬀectiveness of our\n",
      "proposed weakpolyp, which surprisingly achieves a comparable perfor-\n",
      "mance with a fully supervised model, requiring no mask annotations at\n",
      "all.\n",
      "keywords: polyp segmentation · weak supervision · colorectal\n",
      "cancer\n",
      "1\n",
      "introduction\n",
      "colorectal cancer (crc) has become a major threat to health worldwide.\n",
      "given its signiﬁcance, automatic polyp segmentation models [5,8,16,18]\n",
      "j. wei and y. hu—equal contributions.\n",
      "[3] introduce the transformer [4]\n",
      "backbone to extract global contexts, achieving a signiﬁcant performance gain.\n",
      "however, pixel-by-pixel labeling is time-consuming and expensive, which\n",
      "hampers practical clinical usage.\n",
      "besides, many polyps do not have well-\n",
      "deﬁned boundaries.\n",
      "pixel-level labeling inevitably introduces subjective noise.\n",
      "to address the above limitations, a generalized polyp segmentation model is\n",
      "urgently needed.\n",
      "in this paper, we achieve this goal by a weakly supervised\n",
      "polyp segmentation model (named weakpolyp) that only uses coarse bound-\n",
      "ing box annotations.\n",
      "more meaningfully, weakpolyp can take existing large-scale polyp\n",
      "detection datasets to assist the polyp segmentation task.\n",
      "all these advantages make weakpolyp more clinically practical.\n",
      "sur-\n",
      "prisingly, just by redesigning the supervision loss without any changes to the\n",
      "weakpolyp: you only look bounding box for polyp segmentation\n",
      "759\n",
      "model structure, weakpolyp achieves comparable performance to its fully super-\n",
      "vised counterpart.\n",
      "this indirect supervision avoids the misleading of box-shape bias of\n",
      "annotations.\n",
      "however, many regions in the predicted mask are lost in the projec-\n",
      "tion and therefore get no supervision.\n",
      "speciﬁcally, the sc loss explicitly reduces the distance between predictions\n",
      "of the same image at diﬀerent scales.\n",
      "by forcing feature alignment, it inhibits\n",
      "the excessive diversity of predictions, thus improving the model generalization.\n",
      "in summary, our contributions are three-fold: (1) we build the weakpolyp\n",
      "model completely based on bounding box annotations, which largely reduces the\n",
      "labeling cost and achieves a comparable performance to full supervision.\n",
      "(3) our proposed weakpolyp is\n",
      "a plug-and-play option, which can boost the performances of polyp segmentation\n",
      "models under diﬀerent backbones.\n",
      "2\n",
      "method\n",
      "model components.\n",
      "2 depicts the components of weakpolyp, including\n",
      "the segmentation phase and the supervision phase.\n",
      "for the segmentation phase,\n",
      "we adopt res2net\n",
      "for input image\n",
      "instead of the segmentation phase,\n",
      "our contributions primarily lie in the supervision phase, including mask-to-box\n",
      "(m2b) transformation and scale consistency (sc) loss.\n",
      "for each input image i, we ﬁrst resize it into two diﬀerent\n",
      "scales: i1 ∈ rs1×s1 and i2 ∈ rs2×s2.\n",
      "then, i1 and i2 are sent to the segmentation\n",
      "model and get two predicted masks p1 and p2, both of which have been resized\n",
      "to the same size.\n",
      "2. the framework of our proposed weakpolyp model, which consists of the seg-\n",
      "mentation phase and the supervision phase.\n",
      "the segmentation phase predicts the polyp\n",
      "mask for each input ﬁrstly, and the supervision phase uses the coarse box annotation to\n",
      "guide previous predicted mask.\n",
      "2.1\n",
      "mask-to-box (m2b) transformation\n",
      "one naive method to achieve the weakly supervised polyp segmentation is to\n",
      "use the bounding box annotation b to supervise the predicted mask p1/p2.\n",
      "because\n",
      "there is a strong box-shape bias in b. training with this bias, the model is forced\n",
      "to predict the box-shape mask, unable to maintain the polyp’s contours.\n",
      "this indirect supervision separates p1/p2\n",
      "from b so that p1/p2 is not aﬀected by the shape bias of b while obtaining\n",
      "the position and extent of polyps.\n",
      "but how to implement the transformation\n",
      "from p1/p2 to t1/t2?\n",
      "[0, 1]h×1\n",
      "(1)\n",
      "weakpolyp: you only look bounding box for polyp segmentation\n",
      "761\n",
      "fig.\n",
      "then, we element-wisely take the\n",
      "minimum of p\n",
      "′\n",
      "w and p\n",
      "′\n",
      "h to achieve the bounding box mask t. as shown in\n",
      "fig.\n",
      "because both t1/t2 and b are box-like masks, we directly calculate the super-\n",
      "vision loss between them without worrying about the misguidance of box-shape\n",
      "bias.\n",
      "notably, m2b is diﬀerentiable, which can be easily implemented with\n",
      "pytorch and plugged into the model to participate in gradient backpropagation.\n",
      "2.2\n",
      "scale consistency (sc) loss\n",
      "in m2b, most pixels in p are ignored in the projection, thus only a few pix-\n",
      "els with high response values are involved in the supervision loss.\n",
      "the gt row is the performance upper bound.\n",
      "the box row\n",
      "is the performance lower bound.\n",
      ".734 .611 .824 .705\n",
      "ours\n",
      ".853\n",
      ".781\n",
      ".854\n",
      ".777\n",
      ".907 .839 .792 .707 .922 .859\n",
      "p1 and p2 come from the same image i1.\n",
      "lt otal = lsum + lsc\n",
      "(5)\n",
      "3\n",
      "experiments\n",
      "datasets.\n",
      "two large polyp datasets are adopted to evaluate the model per-\n",
      "formance, including sun-seg [9] and polyp-seg.\n",
      "sun-seg originates\n",
      "weakpolyp: you only look bounding box for polyp segmentation\n",
      "763\n",
      "fig.\n",
      "modules\n",
      "res2net-50\n",
      "pvtv2-b2\n",
      "easy testing hard testing easy testing hard testing\n",
      "dice iou\n",
      "dice iou\n",
      "dice iou\n",
      "dice iou\n",
      "base\n",
      ".715\n",
      ".601\n",
      ".718\n",
      ".599\n",
      ".769\n",
      ".652\n",
      ".770\n",
      ".648\n",
      "base+m2b\n",
      ".748\n",
      ".654\n",
      ".768\n",
      ".673\n",
      ".822\n",
      ".738\n",
      ".822\n",
      ".735\n",
      "base+m2b+sc .792\n",
      ".715\n",
      ".807\n",
      ".727\n",
      ".853\n",
      ".781\n",
      ".854\n",
      ".777\n",
      "from [7,10], which consists of 19,544 training images, 17,070 easy tesing images,\n",
      "and 12,522 hard testing images.\n",
      "polyp-seg is our private polyp segmenta-\n",
      "tion dataset, which contains 15,916 training images and 4,040 testing images.\n",
      "weakpolyp is implemented using pytorch.\n",
      "all input images\n",
      "are uniformly resized to 352×352.\n",
      "for data augmentation, random ﬂip, random\n",
      "rotation, and multi-scale training are adopted.\n",
      "1 compares the model performance under\n",
      "diﬀerent supervisions, backbones, and datasets.\n",
      "the overall performance order\n",
      "is gt>weakpolyp>box>grabcut.\n",
      "[13] masks\n",
      "performs the worst, because the foreground and background of polyp images\n",
      "are similar.\n",
      "our weakpolyp predictably outperforms the model supervised by box\n",
      "masks because it is not aﬀected by the box-shape bias of the annotations.\n",
      "performance comparison with previous fully supervised models on sun-seg.\n",
      "combining all these modules, our model achieves the highest\n",
      "performance.\n",
      "3 shows our weakpolyp\n",
      "is even superior to many previous fully supervised methods: pranet\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf:\n",
      "computed tomography (ct) based precise prostate segmen-\n",
      "tation for treatment planning is challenging due to (1) the unclear bound-\n",
      "ary of the prostate derived from ct’s poor soft tissue contrast and (2)\n",
      "the limitation of convolutional neural network-based models in capturing\n",
      "long-range global context.\n",
      "here we propose a novel focal transformer-based\n",
      "image segmentation architecture to eﬀectively and eﬃciently extract local\n",
      "visual features and global context from ct images.\n",
      "additionally, we design\n",
      "an auxiliary boundary-induced label regression task coupled with the main\n",
      "prostate segmentation task to address the unclear boundary issue in ct\n",
      "images.\n",
      "we demonstrate that this design signiﬁcantly improves the quality\n",
      "of the ct-based prostate segmentation task over other competing meth-\n",
      "ods, resulting in substantially improved performance, i.e., higher dice sim-\n",
      "ilarity coeﬃcient, lower hausdorﬀ distance, and average symmetric sur-\n",
      "face distance, on both private and public ct image datasets.\n",
      "keywords: focal transformer · prostate segmentation · computed\n",
      "tomography · boundary-aware\n",
      "1\n",
      "introduction\n",
      "prostate cancer is a leading cause of cancer-related deaths in adult males, as\n",
      "reported in studies, such as [17].\n",
      "a common treatment option for prostate can-\n",
      "cer is external beam radiation therapy (ebrt) [4], where ct scanning is a\n",
      "cost-eﬀective tool for the treatment planning process compared with the more\n",
      "expensive magnetic resonance imaging (mri).\n",
      "as a result, precise prostate seg-\n",
      "mentation in ct images becomes a crucial step, as it helps to ensure that the\n",
      "radiation doses are delivered eﬀectively to the tumor tissues while minimizing\n",
      "harm to the surrounding healthy tissues.\n",
      "due to the relatively low spatial resolution and soft tissue contrast in ct\n",
      "images compared to mri images, manual prostate segmentation in ct images\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1 57.\n",
      "several automated segmentation methods have been proposed to alleviate\n",
      "these issues, especially the fully convolutional networks (fcn) based u-net\n",
      "despite good progress,\n",
      "these methods often have limitations in capturing long-range relationships and\n",
      "global context information [2] due to the inherent bias of convolutional opera-\n",
      "tions.\n",
      "[2] adapts vit to medical image segmentation\n",
      "tasks by connecting several layers of the transformer module (multi-head sa)\n",
      "to the fcn-based encoder for better capturing the global context information\n",
      "from the high-level feature maps.\n",
      "in spite of the improved performance for the aforementioned vit-based net-\n",
      "works, these methods utilize the standard or shifted-window-based sa, which\n",
      "is the ﬁne-grained local sa and may overlook the local and global interactions\n",
      "as reported by [20], even pre-trained with a massive amount of medical\n",
      "data using self-supervised learning, the performance of prostate segmentation\n",
      "task using high-resolution and better soft tissue contrast mri images has not\n",
      "been completely satisfactory, not to mention the lower-quality ct images.\n",
      "addi-\n",
      "tionally, the unclear boundary of the prostate in ct images derived from the\n",
      "low soft tissue contrast is not properly addressed [7,22].\n",
      "recently, focal transformer [24] is proposed for general computer vision\n",
      "tasks, in which focal self-attention is leveraged to incorporate both ﬁne-grained\n",
      "local and coarse-grained global interactions.\n",
      "each token attends its closest sur-\n",
      "rounding tokens with ﬁne granularity, and the tokens far away with coarse granu-\n",
      "larity; thus, focal sa can capture both short- and long-range visual dependencies\n",
      "eﬃciently and eﬀectively. inspired by this work, we propose the focalunetr\n",
      "(focal u-net transformers), a novel focal transformer architecture for ct-\n",
      "based medical image segmentation (fig. 1a).\n",
      "[15] incorporates additional decoders to enhance boundary detection\n",
      "and distance map estimation, they either lack the capacity for eﬀective global\n",
      "context capture through fcn-based techniques or overlook the signiﬁcance of\n",
      "considering the randomness of the boundary, particularly in poor soft tissue con-\n",
      "trast ct images for prostate segmentation.\n",
      "in contrast, our approach utilizes a\n",
      "multi-task learning strategy that leverages a gaussian kernel over the boundary\n",
      "of the ground truth segmentation mask\n",
      "this serves as a regularization term for the main\n",
      "task of generating the segmentation mask.\n",
      "and the auxiliary task enhances the\n",
      "model’s generalizability by addressing the challenge of unclear boundaries in\n",
      "low-contrast ct images.\n",
      "the architecture of focalunetr as (a) the main task for prostate segmenta-\n",
      "tion and (b) a boundary-aware regression auxiliary task.\n",
      "first, we develop a novel\n",
      "focal transformer model (focalunetr) for ct-based prostate segmentation,\n",
      "which makes use of focal sa to hierarchically learn the feature maps accounting\n",
      "for both short- and long-range visual dependencies eﬃciently and eﬀectively.\n",
      "second, we also address the challenge of unclear boundaries speciﬁc to ct images\n",
      "by incorporating an auxiliary task of contour regression.\n",
      "third, our methodology\n",
      "advances state-of-the-art performance via extensive experiments on both real-\n",
      "world and benchmark datasets.\n",
      "2\n",
      "methods\n",
      "2.1\n",
      "focalunetr\n",
      "our focalunetr architecture (fig. 1) follows a multi-scale design similar to\n",
      "[6,20], enabling us to obtain hierarchical feature maps at diﬀerent stages.\n",
      "the\n",
      "input medical image x ∈ rc×h×w is ﬁrst split into a sequence of tokens with\n",
      "dimension ⌈ h\n",
      "h′ ⌉×⌈ w\n",
      "w ′ ⌉, where h, w represent spatial height and width, respec-\n",
      "tively, and c represents the number of channels.\n",
      "these tokens are then projected\n",
      "into an embedding space of dimension d using a patch of resolution (h′, w ′).\n",
      "2. (a) the focal sa mechanism, and (b) an example of perfect boundary matching\n",
      "using focal sa for ct-based prostate segmentation task (lower panel), in which focal\n",
      "sa performs query-key interactions and query-value aggregations in both ﬁne- and\n",
      "coarse-grained levels (upper panel).\n",
      "of two focal levels (ﬁne and coarse) for capturing the interaction of local and\n",
      "global context for optimal boundary-matching between the prediction and the\n",
      "ground truth for prostate segmentation.\n",
      "finally, a relative position\n",
      "bias is added to compute the focal sa for qi by\n",
      "attention(qi, ki, vi) = softmax(qikt\n",
      "i\n",
      "√\n",
      "d\n",
      "+ b)vi,\n",
      "where b = {bl}l\n",
      "1 is the learnable relative position bias [24].\n",
      "the encoder utilizes a patch size of 2×2 with a feature dimension of 2×2×1 =\n",
      "4 (i.e., a single input channel ct) and a d-dimensional embedding space.\n",
      "the\n",
      "596\n",
      "c. li et al.\n",
      "overall architecture of the encoder comprises four stages of focal transformer\n",
      "blocks, with a patch merging layer applied between each stage to reduce the\n",
      "resolution by a factor of 2.\n",
      "we utilize an fcn-based decoder (fig. 1a) with\n",
      "skip connections to connect to the encoder at each resolution to construct a “u-\n",
      "shaped” architecture for our ct-based prostate segmentation task.\n",
      "the objective function\n",
      "for the segmentation head is given by: lseg = ldice(ˆpi, g) + lce(ˆpi, g), where ˆpi\n",
      "represents the predicted probabilities from the main task and g represents the\n",
      "ground truth mask, both given an input image i. the predicted probabilities,\n",
      "ˆpi, are derived from the main task through the application of the focalunetr\n",
      "model to the input ct image.\n",
      "to address the challenge of unclear boundaries in ct-based prostate segmen-\n",
      "tation, an auxiliary task is introduced for the purpose of predicting boundary-\n",
      "aware contours to assist the main prostate segmentation task.\n",
      "this auxiliary\n",
      "task is achieved by attaching another convolution head after the extracted fea-\n",
      "ture maps at the ﬁnal stage (see fig.\n",
      "we pre-\n",
      "dict this heatmap with a regression task trained by minimizing mean-squared\n",
      "error instead of treating it as a single-pixel boundary segmentation problem.\n",
      "given the ground truth of contour gc\n",
      "i , induced from the segmentation mask\n",
      "for input image i, and the reconstructed output probability ˆpc\n",
      "i , we use the fol-\n",
      "lowing loss function: lreg =\n",
      "1\n",
      "n\n",
      "\u0002\n",
      "i ||ˆpc\n",
      "i − gc\n",
      "i ||2 where n is the total number of\n",
      "images for each batch.\n",
      "this auxiliary task is trained concurrently with the main\n",
      "segmentation task.\n",
      "a multi-task learning approach is adopted to regularize the main segmen-\n",
      "tation task through the auxiliary boundary prediction task.\n",
      "focalunetr\n",
      "597\n",
      "3\n",
      "experiments and results\n",
      "3.1\n",
      "datasets and implementation details\n",
      "to evaluate our method, we use a large private dataset with 400 ct scans\n",
      "and a large public dataset with 300 ct scans (amos\n",
      "although the amos dataset includes the prostate class,\n",
      "it mixes the prostate (in males) and the uterus (in females) into one single class\n",
      "labeled pro/ute.\n",
      "we ﬁlter out ct scans missing the pro/ute ground-truth\n",
      "segmentation.\n",
      "[4, 8, 16, 32] for each of the four stages.\n",
      "for the implementation, we utilize a server equipped with 8 nvidia a100\n",
      "gpus, each with 40 gb of memory.\n",
      "all experiments are conducted in pytorch,\n",
      "and each model is trained on a single gpu.\n",
      "for 2d models, we ﬁrst slice each voxel\n",
      "patch in the axial direction into 64 slices of 128 × 128 images for training and\n",
      "stack them back for evaluation.\n",
      "we\n",
      "use random ﬂip, rotation, and intensity scaling as augmentation transforms with\n",
      "probabilities of 0.1, 0.1, and 0.2, respectively.\n",
      "however, we did not get\n",
      "improved performance compared with directly applying the training parameters\n",
      "learned from tuning the private dataset.\n",
      "we report the dice similarity coef-\n",
      "ﬁcient (dsc, %), 95% percentile hausdorﬀ distance (hd, mm), and average\n",
      "symmetric surface distance (assd, mm) metrics.\n",
      "3.2\n",
      "experiments\n",
      "comparison with state-of-the-art methods.\n",
      "to demonstrate the eﬀec-\n",
      "tiveness of focalunetr, we compare the ct-based prostate segmentation per-\n",
      "formance with three 2d u-net-based methods: u-net\n",
      "quantitative performance comparison on the private and amos datasets\n",
      "with a mean (standard deviation) for 3 runs with diﬀerent seeds.\n",
      "[16], two 2d transformer-based segmentation meth-\n",
      "ods: transunet\n",
      "transunet and swin-unet are the\n",
      "only methods that are pre-trained on imagenet.\n",
      "detailed information regarding\n",
      "the number of parameters, flops, and average inference time can be found in\n",
      "the supplementary materials.\n",
      "the\n",
      "amos dataset mixes the prostate(males)/uterus(females, a relatively small por-\n",
      "tion).\n",
      "thus, the overall performance of focalunetr is\n",
      "overshadowed by this challenge, resulting in only moderate improvement over\n",
      "the baselines on the amos dataset.\n",
      "however, the performance margin signiﬁ-\n",
      "cantly improves when using the real-world (private) dataset.\n",
      "when co-trained\n",
      "with the auxiliary contour regression task using the multi-task training strat-\n",
      "egy, the performance of focalunetrs is further improved.\n",
      "3. qualitative results on sample test ct images from the private (ﬁrst two rows)\n",
      "and amos (last two rows)\n",
      "datasets\n",
      "with an auxiliary contour regression task can improve the challenging ct-based\n",
      "prostate segmentation performance.\n",
      "3.\n",
      "the ﬁgure shows that our focalunetr-b and focalunetr-b* generate more\n",
      "accurate segmentation results that are more consistent with the ground truth\n",
      "than the results of the baseline models.\n",
      "additionally, the focalunetrs are less likely to produce false posi-\n",
      "tives (see more in supplementary materials) for ct images without a foreground\n",
      "ground truth, due to the focal sa mechanism that enables the model to cap-\n",
      "ture global context and helps to identify the correct boundary and shape of the\n",
      "prostate.\n",
      "overall, the focalunetrs demonstrate improved segmentation capa-\n",
      "bilities while preserving shapes more precisely, making them promising tools for\n",
      "clinical applications.\n",
      "the results (table 2) indicate that as the\n",
      "value of λ2 is gradually increased and that of λ1 is correspondingly decreased\n",
      "(thereby increasing the relative importance of the auxiliary contour regression\n",
      "task), segmentation performance initially improves.\n",
      "however, as the ratio of con-\n",
      "tour information to segmentation mask information becomes too unbalanced,\n",
      "performance begins to decline.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf:\n",
      "this paper presents a new robust loss function, the t-loss,\n",
      "for medical image segmentation.\n",
      "our experiments show that the t-\n",
      "loss outperforms traditional loss functions in terms of dice scores on two\n",
      "public medical datasets for skin lesion and lung segmentation.\n",
      "we also\n",
      "demonstrate the ability of t-loss to handle diﬀerent types of simulated\n",
      "label noise, resembling human error.\n",
      "our results provide strong evidence\n",
      "that the t-loss is a promising alternative for medical image segmenta-\n",
      "tion where high levels of noise or outliers in the dataset are a typical\n",
      "phenomenon in practice.\n",
      "keywords: robust loss · medical image segmentation ·\n",
      "noisy labels\n",
      "1\n",
      "introduction\n",
      "convolutional neural networks (cnns) and visual transformers (vits) have\n",
      "become the standard in semantic segmentation, achieving state-of-the-art results\n",
      "in many applications [1,16,24].\n",
      "however, supervised training of cnns and vits\n",
      "requires large amounts of annotated data, where each pixel in the image is labeled\n",
      "with the category it belongs to.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1 68.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43898-1_68\n",
      "robust t-loss for medical image segmentation\n",
      "715\n",
      "knowledge that is often scarcely available [6].\n",
      "in addition, medical image anno-\n",
      "tations can be aﬀected by human bias and poor inter-annotator agreement [23],\n",
      "further complicating the process.\n",
      "for instance, the fitzpatrick 17k dataset, commonly used in dermatology\n",
      "research, contains non-skin images and noisy annotations.\n",
      "in a random sample\n",
      "of 504 images, 5.4% were labeled incorrectly or as other classes [10].\n",
      "previous literature has explored many methods to mitigate the problem of\n",
      "noisy labels in deep learning.\n",
      "despite these advances, semantic segmenta-\n",
      "tion with noisy labels is relatively understudied.\n",
      "existing research in this area\n",
      "has focused on the development of noise-resistant network architectures [15], the\n",
      "incorporation of domain-speciﬁc prior knowledge [29], or more recent strategies\n",
      "that update the noisy masks before memorization\n",
      "although previous methods have shown robustness in semantic segmentation,\n",
      "they often have limitations, such as more hyper-parameters, modiﬁcations to the\n",
      "network architecture, or complex training procedures.\n",
      "the t-loss, whose simplest formulation\n",
      "features a single parameter, can adaptively learn an optimal tolerance level to\n",
      "label noise directly during backpropagation, eliminating the need for additional\n",
      "computations such as the expectation maximization (em) steps.\n",
      "to evaluate the eﬀectiveness of the t-loss as a robust loss function for medi-\n",
      "cal semantic segmentation, we conducted experiments on two widely-used bench-\n",
      "mark datasets in the ﬁeld: one for skin lesion segmentation and the other for lung\n",
      "segmentation.\n",
      "we injected diﬀerent levels of noise into these datasets that simu-\n",
      "late typical human labeling errors and trained deep learning models using various\n",
      "robust loss functions.\n",
      "our experiments demonstrate that the t-loss outperforms\n",
      "716\n",
      "a. gonzalez-jimenez et al.\n",
      "these robust state-of-the-art loss functions in terms of segmentation accuracy and\n",
      "robustness, particularly under conditions of high noise contamination.\n",
      "section 3 covers the\n",
      "datasets used in our experiments, the implementation and training details of\n",
      "t-loss, and the metrics used for comparison.\n",
      "2\n",
      "methodology\n",
      "let xi ∈ rc×w×h be an input image and yi ∈ {0, 1}w×h be its noisy annotated\n",
      "binary segmentation mask, where c represents the number of channels, w the\n",
      "image’s width, and h its height.\n",
      "given a set of images {x1, . . .\n",
      ", yn}, our goal is to train a model fw with parameters\n",
      "w such that fw(x) approximates the accurate binary segmentation mask for any\n",
      "given image x.\n",
      "to this end we note that, heuristically, assuming error terms to follow a\n",
      "student-t distribution (as suggested e.g. in [19]) allows for signiﬁcantly larger\n",
      "noise tolerance with respect to the usual gaussian form.\n",
      "recall that the student-t\n",
      "distribution for a d-dimensional variable y is deﬁned by the probability density\n",
      "function (pdf)\n",
      "p(y|µ, σ; ν) = γ\n",
      "\u0002 ν+d\n",
      "2\n",
      "\u0003\n",
      "γ\n",
      "\u0002 ν\n",
      "2\n",
      "\u0003\n",
      "|σ|−1/2\n",
      "(πν)d/2\n",
      "\u0004\n",
      "1\n",
      "(2)\n",
      "robust t-loss for medical image segmentation\n",
      "717\n",
      "the functional form of our loss function for one image is then obtained with the\n",
      "identiﬁcation y = yi and the approximation µ = fw(xi), and aggregated with\n",
      "lt = 1\n",
      "n\n",
      "n\n",
      "\b\n",
      "i=1\n",
      "− log p(yi|fw(xi), σ; ν).\n",
      "in the case of images, this can easily be in the\n",
      "order of 104 or larger, which makes a general computation highly non-trivial and\n",
      "may deteriorate the generalization capabilities of the model.\n",
      "for these reasons,\n",
      "we take σ to be the identity matrix id, despite knowing that pixel annotations\n",
      "in an image are not independent.\n",
      "the loss term for one image simpliﬁes to\n",
      "− log p(y|µ, id; ν)\n",
      "loss functions with similar dynamic tolerance parameters were also studied in\n",
      "[2] in the context of regression, where using the student-t distribution is only\n",
      "mentioned in passing.\n",
      "3\n",
      "experiments\n",
      "in this section, we demonstrate the robustness of the t-loss for segmentation\n",
      "tasks on two public image collections from diﬀerent medical modalities, namely\n",
      "isic [5] and shenzhen\n",
      "[15].\n",
      "3.1\n",
      "datasets\n",
      "the isic 2017 dataset [5] is a well-known public benchmark of dermoscopy\n",
      "images for skin cancer detection.\n",
      "it contains 2000 training and 600 test images\n",
      "with corresponding lesion boundary masks.\n",
      "the images are annotated with lesion\n",
      "type, diagnosis, and anatomical location metadata.\n",
      "we resized the images to\n",
      "256 × 256 pixels for our experiments.\n",
      "718\n",
      "a. gonzalez-jimenez et al.\n",
      "shenzhen [4,13,25] is a public dataset containing 566 frontal chest radio-\n",
      "graphs with corresponding lung segmentation masks for tuberculosis detection.\n",
      "since there is not a predeﬁned split for shenzhen as in isic, to ensure represen-\n",
      "tative training and testing sets, we stratiﬁed the images by their tuberculosis and\n",
      "normal lung labels, with 70% of the data for training and the remaining 30% for\n",
      "testing.\n",
      "resulting in 296 training images and 170 test images.\n",
      "all images were\n",
      "resized to 256 × 256 pixels.\n",
      "without a public benchmark with real noisy and clean segmentation masks,\n",
      "we artiﬁcially inject additional mask noise in these two datasets to test the\n",
      "model’s robustness to low annotation quality.\n",
      "this simulates the real risk of\n",
      "errors due to factors like annotator fatigue and diﬃculty in annotating certain\n",
      "images.\n",
      "the morphological transfor-\n",
      "mations included erosion, dilation, and aﬃne transformations, which respectively\n",
      "reduced, enlarged, and displaced the annotated area.\n",
      "3.2\n",
      "setup\n",
      "we train a nnu–net [12] as a segmentation network from scratch.\n",
      "to increase\n",
      "variations in the training data, we augment them with random mirroring, ﬂip-\n",
      "ping, and gamma transformations.\n",
      "in addition, if the diﬀerence is signiﬁcant, we\n",
      "1 https://github.com/gaozhitong/sp guided noisy label seg.\n",
      "robust t-loss for medical image segmentation\n",
      "719\n",
      "perform the tukey post-hoc test [14] to determine which means are diﬀerent.\n",
      "we\n",
      "assume statistical signiﬁcance for p-values of less than p = 0.05 and denote this\n",
      "with a ⋆.\n",
      "4\n",
      "results\n",
      "4.1\n",
      "results on the isic dataset\n",
      "we present experimental results for the skin lesion segmentation task on the isic\n",
      "dataset in table 1.\n",
      "our results show that conventional losses perform well with\n",
      "no noise or under low noise levels, but their performance decreases signiﬁcantly\n",
      "with increasing noise levels due to the memorization of noisy labels.\n",
      "1, where traditional robust\n",
      "losses overﬁt data in later stages of learning while metrics for the t-loss do not\n",
      "deteriorate.\n",
      "examples of the obtained masks can be seen\n",
      "in the supplementary material.\n",
      "0.761(6)⋆\n",
      "720\n",
      "a. gonzalez-jimenez et al.\n",
      "fig.\n",
      "2. the behavior of ˜ν in the skin lesion segmentation task.\n",
      "4.2\n",
      "results on the shenzhen dataset\n",
      "the results of lung segmentation for the shenzhen test set are reported in table\n",
      "2. similar to the isic dataset, all considered robust losses perform well at low\n",
      "noise levels.\n",
      "4.3\n",
      "dynamic tolerance to noise\n",
      "the value of ˜ν is crucial for the model’s performance, as it controls the sensitivity\n",
      "to label noise.\n",
      "as seen in fig. 2, ˜ν dynamically adjusts annotation noise tolerance in the early\n",
      "stages of training, independently of its initial value.\n",
      "the plots demonstrate that\n",
      "robust t-loss for medical image segmentation\n",
      "721\n",
      "table 2. dice score on the shenzhen dataset with diﬀerent noise ratios.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf:\n",
      "accurately segmenting the liver into anatomical segments\n",
      "is crucial for surgical planning and lesion monitoring in ct imaging.\n",
      "however, this is a challenging task as it is deﬁned based on vessel\n",
      "structures, and there is no intensity contrast between adjacent seg-\n",
      "ments in ct images.\n",
      "speciﬁcally, we ﬁrst seg-\n",
      "ment the liver and vessels from the ct image, and generate 3d\n",
      "liver point clouds and voxel grids embedded with vessel structure\n",
      "prior.\n",
      "then, we design a multi-scale point-voxel fusion network to cap-\n",
      "ture the anatomical structure and semantic information of the liver\n",
      "and vessels, respectively, while also increasing important data access\n",
      "through vessel structure prior.\n",
      "finally, the network outputs the clas-\n",
      "siﬁcation of couinaud segments in the continuous liver space, producing\n",
      "a more accurate and smooth 3d couinaud segmentation mask.\n",
      "our pro-\n",
      "posed method outperforms several state-of-the-art methods, both point-\n",
      "based and voxel-based, as demonstrated by our experimental results on\n",
      "two public liver datasets.\n",
      "code, datasets, and models are released at\n",
      "https://github.com/xukun-zhang/couinaud-segmentation.\n",
      "keywords: couinaud segmentation · point-voxel network · liver ct\n",
      "x. zhang and y. liu—contributed equally.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1 45.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43898-1_45\n",
      "466\n",
      "x. zhang et al.\n",
      "1\n",
      "introduction\n",
      "primary liver cancer is one of the most common and deadly cancer diseases\n",
      "in the world, and liver resection is a highly eﬀective treatment\n",
      "the\n",
      "couinaud segmentation [7] based on ct images divides the liver into eight func-\n",
      "tionally independent regions, which intuitively display the positional relation-\n",
      "ship between couinaud segments and intrahepatic lesions, and helps surgeons\n",
      "for make surgical planning [3,13].\n",
      "in clinics, couinaud segments obtained from\n",
      "manual annotation are tedious and time-consuming, based on the vasculature\n",
      "used as rough guide (fig. 1).\n",
      "thus, designing an automatic method to accu-\n",
      "rately segment couinaud segments from ct images is greatly demanded and\n",
      "has attracted tremendous research attention.\n",
      "however, automatic and accurate couinaud segmentation from ct images\n",
      "is a challenging task.\n",
      "since it is deﬁned based on the anatomical structure of live\n",
      "vessels, even no intensity contrast (fig. 1.(b)) can be observed between diﬀerent\n",
      "couinaud segments, and the uncertainty of boundary (fig. 1.(d)) often greatly\n",
      "aﬀect the segmentation performance.\n",
      "[4,8,15,19] mainly rely\n",
      "on handcrafted features or atlas-based models, and often fail to robustly han-\n",
      "dle those regions with limited features, such as the boundary between adjacent\n",
      "couinaud segments.\n",
      "recently, with the advancement of deep learning [5,10,18],\n",
      "many cnn-based algorithms perform supervised training through pixel-level\n",
      "couinaud annotations to automatically obtain segmentation results [1,9,21].\n",
      "unfortunately, the cnn models treat all voxel-wise features in the ct image\n",
      "equally, cannot eﬀectively capture key anatomical regions useful for couinaud\n",
      "segmentation.\n",
      "in addition, all these methods deal with the 3d voxels of the liver\n",
      "directly without considering the spatial relationship of the diﬀerent couinaud\n",
      "segments, even if this relationship is very important in couinaud segmentation.\n",
      "it can supplement the cnn-based method and improve the segmentation per-\n",
      "formance in regions without intensity contrast.\n",
      "in this paper, to tackle the aforementioned challenges, we propose a point-\n",
      "voxel fusion framework that represents the liver ct in continuous points to\n",
      "better learn the spatial structure, while performing the convolutions in voxels\n",
      "to obtain the complementary semantic information of the couinaud segments.\n",
      "fig.\n",
      "1. couinaud segments (denoted as roman numbers) in relation to the liver vessel\n",
      "structure.\n",
      "(a) and (b) brieﬂy several couinaud segments separated by the hepatic vein.\n",
      "(c) and (d) show several segments surrounded by the portal vein, which are divided by\n",
      "the course of the hepatic vein.\n",
      "anatomical-aware point-voxel network\n",
      "467\n",
      "speciﬁcally, the liver mask and vessel attention maps are ﬁrst extracted from the\n",
      "ct images, which allows us to randomly sample points embedded with vessel\n",
      "structure prior in the liver space and voxelize them into a voxel grid.\n",
      "the voxel-based branch is composed of\n",
      "a series of convolutions to learn semantic features, followed by de-voxelization\n",
      "to convert them back to points.\n",
      "through the operation of voxelization and de-\n",
      "voxelization at diﬀerent resolutions, the features extracted by these two branches\n",
      "can achieve multi-scale fusion on point-based representation, and ﬁnally output\n",
      "the couinaud segment category of each point.\n",
      "extensive experiments on two\n",
      "publicly available datasets named 3dircadb [20] and lits\n",
      "[2] demonstrate that\n",
      "our proposed framework achieves state-of-the-art (sota) performance, outper-\n",
      "forming cutting-edge methods quantitatively and qualitatively.\n",
      "fig.\n",
      "2. overall framework of our proposed method for couinaud segmentation.\n",
      "2\n",
      "method\n",
      "the overview of our framework to segment couinaud segments from ct images\n",
      "is shown in fig.\n",
      "2, including the liver segmentation, vessel attention map gener-\n",
      "ation, point data sampling and multi-scale point-voxel fusion network.\n",
      "2.1\n",
      "liver mask and vessel attention map generation\n",
      "liver segmentation is a fundamental step in couinaud segmentation task.\n",
      "con-\n",
      "sidering that the liver is large and easy to identify in the abdominal organs, we\n",
      "468\n",
      "x. zhang et al.\n",
      "extracted the liver mask through a trained 3d unet [6]. diﬀerent from liver seg-\n",
      "mentation, since we aim to use the vessel structure as a rough guide to improving\n",
      "the performance of the couinaud segmentation, we employ another 3d unet\n",
      "speciﬁcally, given a 3d ct\n",
      "image containing only the area covered by the liver mask (l), the 3d unet\n",
      "[6] output a binary vessel mask (m).\n",
      "2.2\n",
      "couinaud segmentation\n",
      "based on the above work, we ﬁrst use the m ′ and the l to sample get point data,\n",
      "which can convert into a voxel grid through re-voxelization.\n",
      "the converted voxel\n",
      "grid embeds the vessel prior and also dilutes the liver parenchyma information.\n",
      "inspired by [12], a novel multi-scale point-voxel fusion network then is proposed\n",
      "to simultaneously process point and voxel data through point-based branch and\n",
      "voxel-based branch, respectively, aiming to accurately perform couinaud seg-\n",
      "mentation.\n",
      "in order to obtain the topological relationship between couinaud seg-\n",
      "ments, a direct strategy is to sample the coordinate point data with 3d spatial\n",
      "information from liver ct and perform point-wise classiﬁcation.\n",
      "hence, we ﬁrst\n",
      "convert the image coordinate points i =\n",
      "\u0002\n",
      "i1, i2, ..., it, it ∈ r3\u0003\n",
      "in liver ct into\n",
      "the world coordinate points p =\n",
      "\u0002\n",
      "p1, p2, ..., pt, pt ∈ r3\u0003\n",
      ":\n",
      "p = i ∗ spacing ∗ direction + origin,\n",
      "(1)\n",
      "where spacing represents the voxel spacing in the ct images, direction repre-\n",
      "sents the direction of the scan, and origin represents the world coordinates\n",
      "of the image origin.\n",
      "however,\n",
      "directly feeding the transformed point data as input into the point-based branch\n",
      "undoubtedly ignores the vessel structure, which is crucial for couinaud segmen-\n",
      "tation.\n",
      "in the training stage of the\n",
      "network, the label of the point pt = (xt + δx, yt + δy, zt + δz) is generated by:\n",
      "ot = ot(r(xt + δx), r(yt + δy), r(zt + δz))\n",
      "it is not enough to extract the topological information and\n",
      "ﬁne-grained information of independent points only by point-based branch for\n",
      "accurate couinaud segmentation.\n",
      "to this end, we transform the point data\n",
      "{(pt, ft)} into voxel grid {vu,v,w} by re-voxelization, where ft ∈ rc is the feature\n",
      "corresponding to point pt, aiming to voxel-based convolution to extract comple-\n",
      "mentary semantic information in the grid.\n",
      "moreover, due to the previously mentioned point sampling\n",
      "strategy, the converted voxel grid also inherits the vessel structure from the point\n",
      "data and dilutes the unimportant information in the ct images.\n",
      "intuitively, due to the image\n",
      "intensity between diﬀerent couinaud segments being similar, the voxel-based\n",
      "cnn model is diﬃcult to achieve good segmentation performance.\n",
      "we propose\n",
      "a multi-scale point-voxel fusion network for accurate couinaud segmentation,\n",
      "take advantage of the topological relationship of coordinate points in 3d space,\n",
      "and leverage the semantic information of voxel grids.\n",
      "the features extracted\n",
      "by these two branches on multiple scales are fused to provide more accurate\n",
      "and robust couinaud segmentation performance.\n",
      "at the\n",
      "same time, the voxel grid {vu,v,w} passes the voxel branch based on convolution,\n",
      "denoted as ev, which can aggregate the features of surrounding points and learn\n",
      "the semantic information in the liver 3d space.\n",
      "we re-transform the features\n",
      "extracted from the voxel-based branch to point representation through trilinear\n",
      "interpolation, to combine them with ﬁne-grained features extracted from the\n",
      "point-based branch, which provide complementary information:\n",
      "(pt, f 1\n",
      "t ) = ep(p(pt, ft))\n",
      "after three rounds of point-\n",
      "voxel operations, we concatenate the original point feature ft and the features\n",
      "\u0002\n",
      "f 1\n",
      "t , f 2\n",
      "t , f 3\n",
      "t\n",
      "\u0003\n",
      "with multiple scales, then send them into a point-wise decoder d,\n",
      "parameterized by a fully connected network, to predict the corresponding couin-\n",
      "aud segment category:\n",
      "ˆ\n",
      "ot = d(cat\n",
      "\u0002\n",
      "ft, f 1\n",
      "t , f 2\n",
      "t , f 3\n",
      "t\n",
      "\u0003\n",
      ") ∈ {0, 1, ..., 7},\n",
      "(5)\n",
      "where {0, 1, ..., 7} denotes the couinaud segmentation category predicted by our\n",
      "model for the point pt.\n",
      "more method details are shown in the supplementary materials.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "datasets and evaluation metrics\n",
      "we evaluated the proposed framework on two publicly available datasets,\n",
      "3dircadb\n",
      "the 3dircadb dataset [20] contains 20 ct images\n",
      "with spacing ranging from 0.56 mm to 0.87 mm, and slice thickness ranging from\n",
      "1 mm to 4 mm with liver and liver vessel segmentation labels.\n",
      "the lits dataset\n",
      "[2] consists of 200 ct images, with a spacing of 0.56 mm to 1.0 mm and slice\n",
      "thickness of 0.45 mm to 6.0 mm, and has liver and liver tumour labels, but with-\n",
      "out vessels.\n",
      "we annotated the 20 subjects of the 3dircadb dataset [20] with the\n",
      "couinaud segments and randomly divided 10 subjects for training and another\n",
      "10 subjects for testing.\n",
      "[2], we observed the vessel structure\n",
      "on ct images, annotated the couinaud segments of 131 subjects, and randomly\n",
      "selected 66 subjects for training and 65 for testing.\n",
      "we have used three widely used metrics, i.e., accuracy (acc, in %), dice\n",
      "similarity metric (dice, in %), and average surface distance (asd, in mm) to\n",
      "evaluate the performance of the couinaud segmentation.\n",
      "3.2\n",
      "implementation details\n",
      "the proposed framework was implemented on an rtx8000 gpu using pytorch.\n",
      "we perform scaling within the range of\n",
      "0.9 to 1.1, arbitrary axis ﬂipping, and rotation in the range of 0 to 5 ◦c on the\n",
      "input point data as an augmentation strategy.\n",
      "all our experiments were trained 400\n",
      "epochs, with a random seed was 2023, and then we used the model with the best\n",
      "performance on the training set to testing.\n",
      "quantitative comparison with diﬀerent segmentation methods.\n",
      "acc and\n",
      "dice are the averages of all testing subjects, while asd is the average of the average\n",
      "performance of all segments in all testing subjects.\n",
      "[17] with dual attention to focus on\n",
      "the boundary of the couinaud segments and is speciﬁcally used for the couinaud\n",
      "segmentation task.\n",
      "we use pytorch to implement this model and maintain the\n",
      "same implementation details as other methods.\n",
      "[6] have achieved close performance in the lits\n",
      "dataset\n",
      "[2], which demonstrates the potential of the point-based methods in\n",
      "the couinaud segmentation task.\n",
      "finally, our pro-\n",
      "posed point-voxel fusion segmentation framework achieves the best performance.\n",
      "the ﬁrst two rows show\n",
      "that the vessel structure is used as the boundary guidance for couinaud segmen-\n",
      "tation, but voxel-based 3d unet\n",
      "[6] fails to accurately capture this key structural\n",
      "relationship, resulting in inaccurate boundary segmentation.\n",
      "besides, compared with the 3d view, it is obvious that the voxel-based cnn\n",
      "methods are easy to pay attention to the local area and produce a large area\n",
      "of error segmentation, so the reconstructed surface is uneven.\n",
      "3. comparison of segmentation results of diﬀerent methods.\n",
      "diﬀerent\n",
      "colours represent diﬀerent couinaud segments.\n",
      "in the ﬁrst column, we show the vessel attention map in 2d and 3d views\n",
      "as an additional reference for the segmentation results.\n",
      "fig.\n",
      "diﬀerent table 1, here acc\n",
      "and dice are the average performance of each couinaud segment.\n",
      "segmentation blur in boundary areas with high uncertainty.\n",
      "our method com-\n",
      "bines the advantages of point-based and voxel-based methods, and remedies their\n",
      "respective defects, resulting in smooth and accurate couinaud segmentation.\n",
      "3.4\n",
      "ablation study\n",
      "to further study the eﬀectiveness of our proposed framework, we compared two\n",
      "ablation experiments: 1) random sampling of t points in the liver space, with-\n",
      "anatomical-aware point-voxel network\n",
      "473\n",
      "out considering the guidance of vascular structure, and 2) considering only the\n",
      "voxel-based branch, where the couinaud segments mask is output by a cnn\n",
      "decoder.\n",
      "figure 4 shows the ablation experimental results obtained on all the\n",
      "couinaud segments of two datasets, under the dice and the asd metrics.\n",
      "it\n",
      "can be seen that our full method is signiﬁcantly better than the cnn branch\n",
      "joint decoder method on both metrics of two datasets, which demonstrates the\n",
      "performance gain by the combined point-based branch.\n",
      "in addition, compared\n",
      "with the strategy of random sampling, our full-method reduces the average asd\n",
      "by more than 2mm on eight couinaud segments.\n",
      "this is because to the ves-\n",
      "sel structure-guided sampling strategy can increase the important data access\n",
      "between the boundaries of the couinaud segments.\n",
      "besides, perturbations are\n",
      "applied to the points in the coverage area of the vessel attention map, so that our\n",
      "full method performs arbitrary point sampling in the continuous space near the\n",
      "vessel, and is encouraged to implicitly learn the couinaud boundary in countless\n",
      "points.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf:\n",
      "objects with complex structures pose signiﬁcant challenges\n",
      "to existing instance segmentation methods that rely on boundary or aﬃn-\n",
      "ity maps, which are vulnerable to small errors around contacting pixels\n",
      "that cause noticeable connectivity change.\n",
      "while the distance transform\n",
      "(dt) makes instance interiors and boundaries more distinguishable, it\n",
      "tends to overlook the intra-object connectivity for instances with varying\n",
      "width and result in over-segmentation.\n",
      "to address these challenges, we\n",
      "propose a skeleton-aware distance transform (sdt) that combines the\n",
      "merits of object skeleton in preserving connectivity and dt in model-\n",
      "ing geometric arrangement to represent instances with arbitrary struc-\n",
      "tures.\n",
      "comprehensive experiments on histopathology image segmenta-\n",
      "tion demonstrate that sdt achieves state-of-the-art performance.\n",
      "1\n",
      "introduction\n",
      "instances with complex shapes arise in many biomedical domains, and their\n",
      "morphology carries critical information.\n",
      "for example, the structure of gland\n",
      "tissues in microscopy images is essential in accessing the pathological stages for\n",
      "cancer diagnosis and treatment.\n",
      "these instances, however, are usually closely\n",
      "in touch with each other and have non-convex structures with parts of varying\n",
      "widths (fig. 1a), posing signiﬁcant challenges for existing segmentation methods.\n",
      "in the biomedical domain, most methods [3,4,13,14,22] ﬁrst learns interme-\n",
      "diate representations and then convert them into masks with standard segmen-\n",
      "tation algorithms like connected-component labeling and watershed transform.\n",
      "however, for objects with non-convex morphology,\n",
      "the boundary-based distance transform produces multiple local optima in the\n",
      "energy landscape (fig. 1c), which tends to break the intra-instance connectivity\n",
      "when applying thresholding and results in over-segmentation.\n",
      "in quantitative evaluations,\n",
      "we show that our proposed sdt achieves leading performance on histopathology\n",
      "image segmentation for instances with various sizes and complex structures.\n",
      "1.1\n",
      "related work\n",
      "instance segmentation.\n",
      "bottom-up instance segmentation approaches have\n",
      "become de facto for many biomedical applications due to the advantage in seg-\n",
      "menting objects with arbitrary geometry.\n",
      "however, for com-\n",
      "plex structure with parts of varying width, the boundary-based dt tends to\n",
      "produce relatively low values for thin connections and consequently causes over-\n",
      "segmentation.\n",
      "the vision community has been working on direct\n",
      "object skeletonization from images [7,9,16,19].\n",
      "[16] shows the application of the skeleton on segmenting single-object images.\n",
      "we instead focus on the more challenging instance segmentation task with multi-\n",
      "ple objects closely touching each other.\n",
      "object skeletons are also used to correct\n",
      "errors in pre-computed segmentation masks\n",
      "our sdt framework instead\n",
      "use the skeleton in the direct segmentation from images.\n",
      "2\n",
      "skeleton-aware distance transform\n",
      "2.1\n",
      "sdt energy function\n",
      "given an image, we aim to design a new representation e for a model to learn,\n",
      "which is later decoded into instances with simple post-processing.\n",
      "the boundary\n",
      "(or aﬃnity) map is a binary representation where e|γb = 0 and e|ω\\γb = 1.\n",
      "taking the merits of dt in modeling the geometric arrangement and skeleton\n",
      "in preserving connectivity, we propose a new representation e that satisﬁes:\n",
      "0\n",
      "for the realization of e, let x be a pixel in the input image, and d be the\n",
      "metric, e.g., euclidean distance.\n",
      "in the regression mode, the output is a single-channel image.\n",
      "we test both learning strategies\n",
      "in the experiments to illustrate the optimal setting for sdt.\n",
      "a fcn maps the\n",
      "image into the energy space to minimize the loss.\n",
      "(b) inference phase: we threshold the\n",
      "sdt to generate skeleton segments, which is processed into seeds with the connected\n",
      "component labeling.\n",
      "speciﬁcally, in all\n",
      "the experiments, we use a deeplabv3 model\n",
      "we also add a coordconv [10] layer before the 3rd stage in the backbone\n",
      "network to introduce spatial information into the segmentation model.\n",
      "some objects may touch the image border\n",
      "due to either a restricted ﬁeld of view (fov) of the imaging devices or spatial data\n",
      "augmentation like the random crop.\n",
      "if pre-computing the skeleton, we will get\n",
      "local skeleton (fig. 4c) for objects with missing masks due to imaging restrictions,\n",
      "and partial skeleton (fig. 4b) due to spatial data augmentation, which causes\n",
      "ambiguity.\n",
      "in\n",
      "534\n",
      "z. lin et al.\n",
      "inference, we always run predictions on the whole images to avoid inconsistent\n",
      "predictions.\n",
      "in the sdt energy map, all boundary pix-\n",
      "els share the same energy value and can be processed into segments by direct\n",
      "thresholding and connected component labeling, similar to dwt [1].\n",
      "we ﬁrst perform\n",
      "connected component labeling of the skeleton pixels to generate seeds and run the\n",
      "watershed algorithm on the reversed energy map using the seeds as basins (local\n",
      "optima) to generate the ﬁnal segmentation.\n",
      "we also follow previous works [4,22]\n",
      "and reﬁne the segmentation by hole-ﬁlling and removing small spurious objects.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "histopathology instance segmentation\n",
      "accurate instance segmentation of gland tissues in histopathology images is\n",
      "essential for clinical analysis, especially cancer diagnosis.\n",
      "we use the gland segmentation challenge\n",
      "dataset\n",
      "[17] that contains colored light microscopy images of tissues with a\n",
      "wide range of histological levels from benign to malignant.\n",
      "there are 85 and\n",
      "80 images in the training and test set, respectively, with ground truth annota-\n",
      "tions provided by pathologists.\n",
      "according to the challenge protocol, the test set\n",
      "is further divided into two splits with 60 images of normal and 20 images of\n",
      "abnormal tissues for evaluation.\n",
      "three evaluation criteria used in the challenge\n",
      "include instance-level f1 score, dice index, and hausdorﬀ distance, which mea-\n",
      "sure the performance of object detection, segmentation, and shape similarity,\n",
      "respectively.\n",
      "we compare sdt with previous state-of-the-art\n",
      "segmentation methods, including dcan\n",
      "with the same training settings as our sdt, we also report the performance of\n",
      "skeleton with scales (ss) and traditional distance transform (dt).\n",
      "5. visual comparison on histopathology image segmentation.\n",
      "since the training data is relatively limited due to\n",
      "the challenges in collecting medical images, we apply pixel-level and spatial-level\n",
      "augmentations, including random brightness, contrast, rotation, crop, and elastic\n",
      "transformation, to alleviate overﬁtting.\n",
      "2.\n",
      "we use the classiﬁcation learning strategy and optimize a model with 11 output\n",
      "channels (10 channels for energy quantized into ten bins and one channel for\n",
      "536\n",
      "z. lin et al.\n",
      "table 1. comparison with existing\n",
      "methods on the gland segmentation.\n",
      "the results suggest that the\n",
      "model trained with cross-entropy loss\n",
      "with α = 0.8 and local skeleton gener-\n",
      "ation achieves the best performance.\n",
      "we train the model for 20k iterations with an initial learning rate\n",
      "of 5 × 10−4 and a momentum of 0.9.\n",
      "our sdt framework achieves state-of-the-art performance on 5 out of\n",
      "6 evaluation metrics on the gland segmentation dataset (table 1).\n",
      "besides, under\n",
      "the hausdorﬀ distance for evaluating shape-similarity between ground-truth and\n",
      "predicted masks, our sdt reports an average score of 44.82 across two test splits,\n",
      "which improves the previous state-of-the-art approach (i.e., fullnet with an aver-\n",
      "age score of 50.15) by 10.6%.\n",
      "we set\n",
      "b = 0.1 for the experiments.\n",
      "table 2 shows that α = 0.8 achieves the best overall\n",
      "performance, which is slightly better than α = 1.0.\n",
      "the results show that\n",
      "pre-computed sdt signiﬁcantly degrades performance (table 2).\n",
      "we argue this\n",
      "is because pre-computed energy not only introduces inconsistency for instances\n",
      "touching the image border but also restricts the diversity of sdt energy maps.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf:\n",
      "deep learning-based medical image enhancement methods\n",
      "(e.g., denoising and super-resolution) mainly rely on paired data and\n",
      "correspondingly the well-trained models can only handle one type of\n",
      "task.\n",
      "in this paper, we address the limitation with a diﬀusion model-\n",
      "based framework that mitigates the requirement of paired data and can\n",
      "simultaneously handle multiple enhancement tasks by one pre-trained\n",
      "diﬀusion model without ﬁne-tuning.\n",
      "experiments on low-dose ct and\n",
      "heart mr datasets demonstrate that the proposed method is versatile\n",
      "and robust for image denoising and super-resolution.\n",
      "we believe our work\n",
      "constitutes a practical and versatile solution to scalable and generalizable\n",
      "image enhancement.\n",
      "1\n",
      "introduction\n",
      "computed tomography (ct) and magnetic resonance (mr) are two widely\n",
      "used imaging techniques in clinical practice.\n",
      "ct imaging uses x-rays to pro-\n",
      "duce detailed, cross-sectional images of the body, which is particularly useful for\n",
      "imaging bones and detecting certain types of cancers with fast imaging speed.\n",
      "low-dose ct techniques have been developed to\n",
      "address this concern by using lower doses of radiation, but the image quality is\n",
      "degraded with increased noise, which may compromise diagnostic accuracy\n",
      "[9].\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1_1.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14222, pp.\n",
      "mr imaging, on the other hand, uses a strong magnetic ﬁeld and radio waves\n",
      "to create detailed images of the body’s internal structures, which can produce\n",
      "high-contrast images for soft tissues and does not involve ionizing radiation.\n",
      "motivated by the aforementioned, there is a pressing need to improve the\n",
      "quality of low-dose ct images and low-resolution mr images to ensure that\n",
      "they provide the necessary diagnostic information.\n",
      "numerous algorithms have\n",
      "been developed for ct and mr image enhancement, with deep learning-based\n",
      "methods emerging as a prominent trend [5,14], such as using the conditional\n",
      "generative adversarial network for ct image denoising [32] and convolutional\n",
      "neural network for mr image super-resolution (sr)\n",
      "these algorithms are capable of improving image quality, but they have\n",
      "two signiﬁcant limitations.\n",
      "first, paired images are required for training,\n",
      "e.g., low-dose and full-dose ct images; low-resolution and high-resolution mr\n",
      "images).\n",
      "although it is possible to simulate low-quality images from high-quality\n",
      "images, the models derived from such data may have limited generalization abil-\n",
      "ity when applied to real data [9,14].\n",
      "recently, pre-trained diﬀusion models [8,11,21] have shown great promise\n",
      "in the context of unsupervised natural image reconstruction [6,7,12,28].\n",
      "how-\n",
      "ever, their applicability to medical images has not been fully explored due to the\n",
      "absence of publicly available pre-trained diﬀusion models tailored for the med-\n",
      "ical imaging community.\n",
      "the training of diﬀusion models requires a signiﬁcant\n",
      "amount of computational resources and training images.\n",
      "for example, openai’s\n",
      "improved diﬀusion models [21] took 1600–16000 a100 hours to be trained on\n",
      "the imagenet dataset with one million images, which is prohibitively expensive.\n",
      "several studies have used diﬀusion models for low-dose ct denoising [30] and\n",
      "mr image reconstruction\n",
      "[22,31], but they still rely on paired images.\n",
      "in this paper, we aim at addressing the limitations of existing image enhance-\n",
      "ment methods and the scarcity of pre-trained diﬀusion models for medical\n",
      "images.\n",
      "speciﬁcally, we provide two well-trained diﬀusion models on full-dose ct\n",
      "images and high-resolution heart mr images, suitable for a range of applications\n",
      "including image generation, denoising, and super-resolution.\n",
      "motivated by the\n",
      "existing plug-and-play image restoration methods [26,34,35] and denoising dif-\n",
      "fusion restoration and null-space models (ddnm)\n",
      "[12,28], we further introduce\n",
      "a paradigm for plug-and-play ct and mr image denoising and super-resolution\n",
      "as shown in fig.\n",
      "notably, it eliminates the need for paired data, enabling\n",
      "greater scalability and wider applicability than existing paired-image dependent\n",
      "pre-trained diﬀusion models\n",
      "5\n",
      "fig.\n",
      "1. comparison of (a) the common paired-image dependent paradigm and (b) the\n",
      "plug-and-play paradigm for medical image enhancement.\n",
      "the former needs to build\n",
      "customized models for diﬀerent tasks based on paired low/high-quality images, while\n",
      "the latter can share one pre-trained diﬀusion model for all tasks and only high-quality\n",
      "images are required as training data.\n",
      "the pre-trained model can handle unseen images\n",
      "as demonstrated in experiments.\n",
      "methods.\n",
      "our method does not need additional training on speciﬁc tasks\n",
      "and can directly use the single pre-trained diﬀusion model on multiple medical\n",
      "image enhancement tasks.\n",
      "2\n",
      "method\n",
      "this section begins with a brief overview of diﬀusion models for image generation\n",
      "and the mathematical model and algorithm for general image enhancement.\n",
      "we\n",
      "then introduce a plug-and-play framework that harnesses the strengths of both\n",
      "approaches to enable unsupervised medical image enhancement.\n",
      "2.1\n",
      "denoising diﬀusion probabilistic models (ddpm)\n",
      "for unconditional image generation\n",
      "image generation models aim to capture the intrinsic data distribution from\n",
      "a set of training images and generate new images from the model itself.\n",
      "[11] for unconditional medical image generation, which contains a\n",
      "diﬀusion (or forward) process and a sampling (or reverse) process.\n",
      "the diﬀusion\n",
      "process gradually adds random gaussian noise to the input image x0, following\n",
      "a markov chain with transition kernel q(xt|xt−1)\n",
      "= n(xt; √1 − βtxt−1, βti),\n",
      "6\n",
      "j. ma et al.\n",
      "where t ∈ {1, · · · , t} represents the current timestep, xt and xt−1 are adjacent\n",
      "image status, and βt ∈ {β1, · · · , βt } is a predeﬁned noise schedule.\n",
      "this property enables simple\n",
      "model training where the input is the noisy image xt and the timestep t and the\n",
      "output is the predicted noise\n",
      "the\n",
      "sampling process aims to generate a clean image from gaussian noise xt ∼\n",
      "n(0, i), and each reverse step is deﬁned by:\n",
      "xt−1 =\n",
      "1\n",
      "√αt\n",
      "\u0003\n",
      "xt − 1 − αt\n",
      "√1 − ¯αt\n",
      "ϵθ(xt, t)\n",
      "\u0004\n",
      "+ βtz; z ∼ n(0, i).\n",
      "(2)\n",
      "2.2\n",
      "image enhancement with denoising algorithm\n",
      "in general, image enhancement tasks can be formulated by:\n",
      "y = hx + n,\n",
      "(3)\n",
      "where y is the degraded image, h is a degradation matrix, x is the unknown\n",
      "original image, and n is the independent random noise.\n",
      "this model can represent\n",
      "various image restoration tasks.\n",
      "for instance, in the image denoising task, h is\n",
      "the identity matrix, and in the image super-resolution task, h is the downsam-\n",
      "pling operator.\n",
      "(6)\n",
      "intuitively, idbp iteratively estimates the original image from the current\n",
      "degraded image and makes a projection by constraining it with prior knowledge.\n",
      "although idbp oﬀers a ﬂexible way to solve image enhancement problems, it\n",
      "still requires paired images to train the denoising operator [26].\n",
      "pre-trained diﬀusion models\n",
      "7\n",
      "2.3\n",
      "pre-trained diﬀusion models for plug-and-play medical image\n",
      "enhancement\n",
      "we introduce a plug-and-play framework by leveraging the beneﬁts of the diﬀu-\n",
      "sion model and idbp algorithm.\n",
      "here we highlight two beneﬁts: (1) it removes\n",
      "the need for paired images; and (2) it can simply apply the single pre-trained\n",
      "diﬀusion model across multiple medical image enhancement tasks.\n",
      "the ﬁrst step estimates the\n",
      "denoised image x0|t based on the current noisy image xt and the trained denoising\n",
      "network ϵθ(xt, t).\n",
      "the second step generates a rectiﬁed image xt−1 by taking a\n",
      "weighted sum of x0|t and xt and adding a gaussian noise perturbation.\n",
      "as mentioned in eq.\n",
      "(3), our goal is to restore an unknown original image\n",
      "x0 from a degraded image y. thus, the degraded image y needs to be involved\n",
      "in the sampling process.\n",
      "(8), we have:\n",
      "xt−1 =\n",
      "√¯αt−1βt\n",
      "1 − ¯αt\n",
      "ˆx0|t +\n",
      "√αt(1 − ¯αt−1)\n",
      "1 − αt\n",
      "xt + βtz.\n",
      "(10)\n",
      "algorithm 1 shows the complete steps for image enhancement, which inherit the\n",
      "denoising operator from ddpm and the projection operator from idbp.\n",
      "the\n",
      "former employs the strong denoising capability in the diﬀusion model and the\n",
      "latter can make sure that the generated results match the input image.\n",
      "8\n",
      "j. ma et al.\n",
      "algorithm 1. pre-trained ddpm for plug-and-play medical image enhancement\n",
      "require: pre-trained ddpm ϵθ, low-quality image y, degradation operator h\n",
      "1: initialize xt ∼ n(0, i).\n",
      "project x0|t on the hyperplane y = hx\n",
      "5:\n",
      "xt−1 =\n",
      "√\n",
      "¯αt−1βt\n",
      "1−¯αt\n",
      "ˆx0|t +\n",
      "√αt(1−¯αt−1)\n",
      "1−¯αt\n",
      "xt + βtz, z ∼ n(0, i) // sampling\n",
      "6: end for\n",
      "7: return enhanced image x0\n",
      "3\n",
      "experiments\n",
      "dataset.\n",
      "we conducted experiments on two common image enhancement tasks:\n",
      "denoising and sr.\n",
      "to mimic the real-world setting, the diﬀusion models were\n",
      "trained on a diverse dataset, including images from diﬀerent centers and scan-\n",
      "ners.\n",
      "the testing set (e.g., mr images) is from a new medical center that has\n",
      "not appeared in the training set.\n",
      "experiments show that our model can gen-\n",
      "eralize to these unseen images.\n",
      "notably, the presented framework elimi-\n",
      "nates the requirement of paired data.\n",
      "for the ct image enhancement task, we\n",
      "trained a diﬀusion model\n",
      "[21] based on the full-dose dataset that contains 5351\n",
      "images, and the hold-out quarter-dose images were used for testing.\n",
      "for the mr\n",
      "enhancement task, we used the whole acdc\n",
      "the testing\n",
      "images were downsampled by operator h with factors of 4× and 8× to produce\n",
      "low-resolution images, and the original images served as the ground truth.\n",
      "evaluation metrics.\n",
      "the image quality was quantitatively evaluated by the\n",
      "peak signal-to-noise ratio (psnr), structural similarity index (ssim)\n",
      "[29],\n",
      "and visual information fidelity (vif) [24], which are widely used measures in\n",
      "medical image enhancement tasks [9,17].\n",
      "implementation details.\n",
      "the input image size was normalized to 256 × 256 and the 2d\n",
      "u-net\n",
      "[13] with a batch size of 16 and a learning\n",
      "rate of 10−4, and an exponential moving average (ema) over model parame-\n",
      "ters with a rate of 0.9999.\n",
      "all the models were trained on a100 gpu and the\n",
      "total training time was 16 d. the implementation was based on ddnm\n",
      "speciﬁcally, we used the\n",
      "identity matrix i as the degradation operator for the denoising task and scaled\n",
      "pre-trained diﬀusion models\n",
      "9\n",
      "the projection diﬀerence h†(hx0|t−y) with coeﬃcient σ to balance the informa-\n",
      "tion from measurement y and denoising output x0|t.\n",
      "the downsampling opera-\n",
      "tor implemented with torch.nn.adaptiveavgpool2d for the super-resolution task.\n",
      "we also compared the present\n",
      "method with one commonly used image enhancement method dip [10] and two\n",
      "recent diﬀusion model-based methods: ivlr\n",
      "[6], which adopted low-frequency\n",
      "information from measurement y to guide the generation process towards a nar-\n",
      "row data manifold, and dps\n",
      "notably, dps used 1000 sampling\n",
      "steps while we only used 100 sampling steps.\n",
      "table 1. performance (mean±standard deviation) on ct denoising task.\n",
      "the arrows\n",
      "indicate directions of better performance.\n",
      "methods psnr ↑\n",
      "ssim ↑\n",
      "vif ↑\n",
      "baseline\n",
      "24.9 ± 2.4\n",
      "0.778 ± 0.07\n",
      "0.451 ± 0.07\n",
      "[7]\n",
      "26.5 ± 2.3\n",
      "0.791 ± 0.08\n",
      "0.475 ± 0.09\n",
      "ours\n",
      "28.3 ± 2.8 0.803 ± 0.11 0.510 ± 0.10\n",
      "4\n",
      "results and discussion\n",
      "low-dose ct image enhancement.\n",
      "the presented method outperformed\n",
      "all other methods on the denoising task in all metrics, as shown in table 1, with\n",
      "average psnr, ssim, and vif of 28.3, 0.803, and 0.510, respectively.\n",
      "supple-\n",
      "mentary fig.\n",
      "our method still achieves\n",
      "better performance across all metrics as shown in table 2.\n",
      "by visually comparing\n",
      "the enhancement results in fig. 1 (b) and (c), our results can reconstruct more\n",
      "anatomical details even in the challenging noisy 8× sr task.\n",
      "10\n",
      "j. ma et al.\n",
      "table 2. evaluation results of joint denoising and super-resolution for ct images.\n",
      "[7]\n",
      "25.1±2.8\n",
      "0.172±0.13\n",
      "0.416±0.12\n",
      "24.7±3.1\n",
      "0.705±0.14\n",
      "0.398±0.12\n",
      "ours\n",
      "26.2±2.6 0.743±0.10 0.446±0.10 25.9±3.1 0.731±0.13 0.431±0.12\n",
      "table 3. evaluation results of heart mr image super-resolution tasks.\n",
      "[7]\n",
      "25.6±0.8\n",
      "0.741±0.03\n",
      "0.335±0.03\n",
      "21.2±0.8\n",
      "0.635±0.04\n",
      "0.177±0.02\n",
      "ours\n",
      "26.9±0.8\n",
      "0.805±0.025 0.416±0.03 22.4±0.8\n",
      "0.700±0.03\n",
      "0.226±0.02\n",
      "mr image enhancement.\n",
      "to demonstrate the generality of the presented\n",
      "method, we also applied it for the heart mr image 4× and 8× sr tasks, and the\n",
      "quantitative results are presented in table 3.\n",
      "dip obtains slightly better scores in psnr for the 4× sr\n",
      "task and psnr and ssim for the 8× sr tasks, but visualized image quality is\n",
      "signiﬁcantly worse than our results as shown in supplementary fig.\n",
      "2, e.g., many\n",
      "anatomical structures are smoothed.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_53.pdf:\n",
      "pulmonary vessel segmentation in computerized tomogra-\n",
      "phy (ct) images is essential for pulmonary vascular disease and surgical\n",
      "navigation.\n",
      "however, the existing methods were generally designed for\n",
      "contrast-enhanced images, their performance is limited by the low con-\n",
      "trast and the non-uniformity of hounsﬁeld unit (hu) in non-contrast\n",
      "ct images, meanwhile, the varying size of the vessel structures are\n",
      "not well considered in current pulmonary vessel segmentation meth-\n",
      "ods.\n",
      "to address this issue, we propose a hierarchical enhancement net-\n",
      "work (henet) for better image- and feature-level vascular representa-\n",
      "tion learning in the pulmonary vessel segmentation task.\n",
      "speciﬁcally, we\n",
      "ﬁrst design an auto contrast enhancement (ace) module to adjust\n",
      "the vessel contrast dynamically.\n",
      "then, we propose a cross-scale non-\n",
      "local block (csnb) to eﬀectively fuse multi-scale features by utilizing\n",
      "both local and global semantic information.\n",
      "experimental results show\n",
      "that our approach achieves better pulmonary vessel segmentation out-\n",
      "comes compared to other state-of-the-art methods, demonstrating the\n",
      "eﬃcacy of the proposed ace and csnb module.\n",
      "keywords: pulmonary vessel segmentation · non-contrast ct ·\n",
      "hierarchical enhancement\n",
      "1\n",
      "introduction\n",
      "segmentation of the pulmonary vessels is the foundation for the clinical diagnosis\n",
      "of pulmonary vascular diseases such as pulmonary embolism (pe), pulmonary\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14222, pp.\n",
      "accurate vascular quantitative analysis is\n",
      "crucial for physicians to study and apply in treatment planning, as well as mak-\n",
      "ing surgical plans.\n",
      "although contrast-enhanced ct images have better contrast\n",
      "for pulmonary vessels compared to non-contrast ct images, the acquisition of\n",
      "contrast-enhanced ct images needs to inject a certain amount of contrast agent\n",
      "to the patients.\n",
      "in the literature, several conventional methods [5,16] have been proposed for\n",
      "the segmentation of pulmonary vessels in contrast-enhanced ct images.\n",
      "most of\n",
      "these methods employed manual features to segment peripheral intrapulmonary\n",
      "vessels.\n",
      "in recent years, deep learning-based methods have emerged as promising\n",
      "approaches to solving challenging medical image analysis problems and have\n",
      "demonstrated exciting performance in segmenting various biological structures\n",
      "however, for vessel segmentation, the widely used models, such as\n",
      "u-net and its variants, limit their segmentation accuracy on low-contrast small\n",
      "vessels due to the loss of detailed information caused by the multiple down-\n",
      "sampling operations.\n",
      "[17] proposed a nested structure\n",
      "unet++ to redesign the skip connections for aggregating multi-scale features\n",
      "and improve the segmentation quality of varying-size objects.\n",
      "[1] also proposed an orthogonal\n",
      "fused u-net++ for pulmonary peripheral vessel segmentation.\n",
      "however, all these\n",
      "methods ignored the signiﬁcant variability in hu values of pulmonary vessels at\n",
      "diﬀerent regions.\n",
      "to summarize, there exist several challenges for pulmonary vessel segmen-\n",
      "tation in non-contrast ct images: (1) the contrast between pulmonary vessels\n",
      "and background voxels is extremely low (fig. 1(c)); (2) pulmonary vessels have\n",
      "a complex structure and signiﬁcant variability in vessel appearance, with diﬀer-\n",
      "ent scales in diﬀerent areas.\n",
      "to address the above challenges, we propose a h ierarchical enhancement\n",
      "n etwork (henet) for pulmonary vessel segmentation in non-contrast ct images\n",
      "by enhancing the representation of vessels at both image- and feature-level.\n",
      "for\n",
      "the input ct images, we propose an auto contrast enhancement (ace) module\n",
      "to automatically adjust the range of hu values in diﬀerent areas of ct images.\n",
      "it mimics the radiologist in setting the window level (wl) and window width\n",
      "hierarchical enhancement network\n",
      "553\n",
      "fig.\n",
      "the challenges of accurate pulmonary vessel segmentation.\n",
      "(c) hard to distinguish vessels in non-contrast ct images.\n",
      "2. our proposed hier-\n",
      "archical enhancement network (henet) consists of two main modules: (1) auto\n",
      "contrast enhancement (ace) module, and (2) cross-scale non-local block\n",
      "(csnb) as the skip connection bridge between encoders and decoders.\n",
      "first, the ace module\n",
      "is developed to enhance the contrast of vessels in the original ct images for the\n",
      "following vessel segmentation network.\n",
      "2.1\n",
      "auto contrast enhancement\n",
      "in non-contrast ct images, the contrast between pulmonary vessels and the\n",
      "surrounding voxels is pretty low.\n",
      "normally, radiol-\n",
      "ogists have to manually set the suitable window level (wl) and window width\n",
      "(ww) for diﬀerent regions in images to enhance vessels according to the hu\n",
      "value range of surrounding voxels, just as diﬀerent settings to better visualize\n",
      "the extrapulmonary and intrapulmonary vessels (fig. 1(d) and (e)).\n",
      "the ace module leverages convolution operations to generate dynamic wl\n",
      "and ww for the input ct images according to the hu values covered by the\n",
      "kernel.\n",
      "it consists of two components: (1) auto\n",
      "contrast enhancement module (bottom).\n",
      "normalization to linearly transform the hu values of the original image x to the\n",
      "range (−1, 1).\n",
      "here, the learned shift map and scale map act as the window\n",
      "level and window width settings of the “width/level” scaling in ct images.\n",
      "it\n",
      "matches the requirement of the positive integer for ww.\n",
      "the upsampled shift map and scale map are denoted\n",
      "as mshift and mscale, respectively, and then the contrast enhancement image\n",
      "xace can be generated through:\n",
      "(1)\n",
      "hierarchical enhancement network\n",
      "555\n",
      "it can be observed that the intensity values of input x are re-centered and\n",
      "re-scaled by mshift and mscale (fig. 3(c)).\n",
      "the clip operation (clip(·)) truncates\n",
      "the ﬁnal output into the range [−1, 1], which sets the intensity value above 1\n",
      "to 1, and below –1 to –1. in our experiments, we ﬁnd that a large kernel size\n",
      "for learning of mshift and mscale could deliver better performance, which can\n",
      "capture more information on hu values from the ct images.\n",
      "2.2\n",
      "cross-scale non-local block\n",
      "there are studies\n",
      "[14,18] showing that non-local operations could capture long-\n",
      "range dependency to improve network performance.\n",
      "to segment pulmonary ves-\n",
      "sels with signiﬁcant variability in scale and shape, we design a cross-scale non-\n",
      "local block (csnb) to fuse the local features extracted by cnn backbone from\n",
      "diﬀerent scales, and to accentuate the cross-scale dependency to address the\n",
      "complex scale variations of pulmonary vessels.\n",
      "inspired by [18], our csnb incorporates 6 modiﬁed asymmetric non-local\n",
      "blocks (anbs), which integrate pyramid sampling modules into the non-local\n",
      "blocks to largely reduce the computation and memory consumption.\n",
      "2, the csnb works as the information bridge between encoders\n",
      "and decoders while also ensuring the feasibility of experiments involving large\n",
      "3d data.\n",
      "the speciﬁc computation of anb proceeds\n",
      "as follows: first, three 1×1×1 convolutions (denoted as conv(·)) are applied\n",
      "to transform fh and fl into diﬀerent embeddings q, k, and v ; then, spatial\n",
      "pyramid pooling operations (denoted as p(·)) are implemented on k and v .\n",
      "et al.\n",
      "where the ﬁnal convolution is used as a weighting parameter to adjust the impor-\n",
      "tance of this non-local operation and recover the channel dimension to ch. anb-\n",
      "p has the same structure as anb-h, but the inputs fh and fl here are the same,\n",
      "which is the output of anb-h at the same level.\n",
      "thereby, the response of multi-scale vessels can be enhanced.\n",
      "3\n",
      "experiments and results\n",
      "dataset and evaluation metrics.\n",
      "we use a total of 160 non-contrast ct\n",
      "images with the inplane size of 512 × 512, where the slice number varies from\n",
      "217 to 622.\n",
      "the annotations of pulmonary\n",
      "vessels are semi-automatically segmented in 3d by two radiologists using the\n",
      "3d slicer software.\n",
      "the quantitative\n",
      "results are reported by dice similarity coeﬃcient (dice), mean intersection over\n",
      "union (miou), false positive rate (fpr), average surface distance (asd), and\n",
      "hausdorﬀ distance (hd).\n",
      "implementation details.\n",
      "our experiments are implemented using pytorch\n",
      "framework and trained using a single nvidia-a100 gpu.\n",
      "in the training stage, we randomly crop sub-volumes with\n",
      "the size of 192 × 192 × 64 near the lung ﬁeld, and then the cropped sub-volumes\n",
      "are augmented by random horizontal and vertical ﬂipping with a probability of\n",
      "0.5.\n",
      "in the testing phase, we perform the sliding window average prediction with\n",
      "strides of (64, 64, 32) to cover the entire ct images.\n",
      "for a fair comparison,\n",
      "we use the same hyper-parameter settings and dice similarity coeﬃcient loss\n",
      "across all experiments.\n",
      "in particular, we use the same data augmentation, no\n",
      "post-processing scheme, adam optimizer with an initial learning rate of 10−4,\n",
      "and train for 800 epochs with a batch size of 4.\n",
      "in our experiments, we use a\n",
      "two-step optimization strategy: 1) ﬁrst, train the ace module with the basic\n",
      "u-net; 2) integrate the trained ace module and a new csnb module into the\n",
      "u-net, and ﬁx the parameters of ace module when training this network.\n",
      "hierarchical enhancement network\n",
      "557\n",
      "fig.\n",
      "3. the eﬀectiveness of the proposed components, with the images in red circles\n",
      "generated from our method.\n",
      "(a–c) the contrast of vessels is signiﬁcantly enhanced in\n",
      "the ct images processed by ace module.\n",
      "compared to the baseline,\n",
      "both ace module and csnb lead to better performance.\n",
      "with the two compo-\n",
      "nents, our henet has signiﬁcant improvements over baseline on all the metrics.\n",
      ", the ace module eﬀectively enhances the contrast\n",
      "of pulmonary vessels at the image-level.\n",
      "∗ denotes signiﬁcant improvement\n",
      "compared to the baseline u-net (p<0.05).\n",
      "we also compare our method with state-of-the-art\n",
      "deep learning-based vessel segmentation methods, including cldice [12], cs2-\n",
      "net [7], and of-net [1].\n",
      "4. qualitative segmentation results.\n",
      "(color ﬁgure online)\n",
      "but our method has better dice and miou than cs2-net (increasing 0.56% and\n",
      "0.43%, respectively), indicating the under-segmentation of cs2-net and more\n",
      "vessels being correctly segmented by our method.\n",
      "in the ﬁrst row of qualitative\n",
      "results (fig. 4), the competing methods can produce satisfactory results for the\n",
      "overall structure but generate many false positives.\n",
      "furthermore, due to low con-\n",
      "trast between small intrapulmonary vessels and the surrounding voxels, results\n",
      "of competing methods exist many discontinuities (the second row), while our\n",
      "method obtains more connective segmentation for these small vessels.\n",
      "also, for\n",
      "the segmentation of large extrapulmonary vessels (the last row), our method can\n",
      "produce more accurate results.\n",
      "this implies\n",
      "that cldice tends to over-segment vessels, and it cannot obtain precise segmen-\n",
      "tation for the large extrapulmonary vessels.\n",
      "hierarchical enhancement network\n",
      "559\n",
      "4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf:\n",
      "despite its improved comprehensive diagnostic\n",
      "performance, skin disease diagnosis should aim for personalized diag-\n",
      "nosis rather than centralized and generalized diagnosis, due to personal\n",
      "diversities and variability, such as skin color, wrinkles, and aging.\n",
      "to this\n",
      "end, we propose a novel deep learning network for personalized diagno-\n",
      "sis in an adaptive manner, utilizing personal characteristics in diagnos-\n",
      "ing dermatitis in a mobile- and fl-based environment.\n",
      "apd-net incorporates a novel architectural design that lever-\n",
      "ages personalized and centralized parameters, along with a ﬁne-tuning\n",
      "method based on a modiﬁed ga to identify personal characteristics.\n",
      "we validated apd-net on clinical datasets and demonstrated its supe-\n",
      "rior performance, compared with state-of-the-art approaches.\n",
      "our exper-\n",
      "imental results showed that apd-net markedly improved personalized\n",
      "diagnostic accuracy by 9.9% in dermatitis diagnosis, making it a promis-\n",
      "ing tool for clinical practice.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1_37.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43898-1_37\n",
      "fine-tuning network in federated learning for personalized skin diagnosis\n",
      "379\n",
      "1\n",
      "introduction\n",
      "for the past several years, in skin disease diagnosis, deep learning (dl) tech-\n",
      "niques have been extensively studied, due to its eﬀectiveness and outstanding\n",
      "diagnostic performance [8,16,19].\n",
      "for example, wu et al. used a custom dataset\n",
      "to develop an eﬃcientnet-b4-based dl model and successfully diagnosed skin\n",
      "diseases [19]. srinivasu et al. designed an advanced dl model, by combining\n",
      "long short-term memory (lstm) with mobilenet and achieved improved per-\n",
      "formance as well as fast prediction time in skin disease diagnosis\n",
      "for the\n",
      "development of dl models, a large number of datasets are needed for accurate\n",
      "model ﬁtting at the training stage.\n",
      "as such, the performance of dl\n",
      "models is often limited, due to a small number of datasets [2,16,20].\n",
      "fig.\n",
      "1. pipeline of adp-net framework for a personalized diagnosis.\n",
      "to overcome the limitations mentioned above, federated learning (fl) can be\n",
      "a viable solution in the context of digital healthcare, especially in the covid-\n",
      "19 pandemic era [15].\n",
      "moreover, fl allows to acquire many heterogeneous\n",
      "images from edge devices at multiple medical sites [1,6,7,10,15].\n",
      "for example,\n",
      "b. mcmahan et al. developed a protocol to average the gradients from decen-\n",
      "tralized clients, without data sharing [10].\n",
      "k. bonawitz et al. built high-level fl\n",
      "systems and architectures in a mobile environment\n",
      "however, dl models in\n",
      "the fl environment were optimized to deal with datasets from multiple clients;\n",
      "therefore, while the dl models yielded a generalized prediction capability across\n",
      "all of the domains involved, the dl models cannot eﬃciently perform personal-\n",
      "ized diagnosis, which is deemed a weakness of fl.\n",
      "[4], human against machine (ham)\n",
      "380\n",
      "k. lee et al.\n",
      "experimental results demonstrated that the apd-net yielded outstanding per-\n",
      "formance, compared with other comparison methods, and achieved adaptively\n",
      "personalized diagnosis.\n",
      "the contributions of this paper are three-fold:\n",
      "• we developed a mobile- and fl-based learning (apd-net) for skin disease\n",
      "diagnosis and achieved superior performance on skin disease diagnosis for\n",
      "public and custom datasets.\n",
      "• we introduce a customized ga for apd-net, combined with a corresponding\n",
      "network architecture, resulting in improved personalized diagnostic perfor-\n",
      "mance as well as faster prediction time.\n",
      "• we provide a new ﬂuorescence dataset for skin disease diagnosis containing\n",
      "2,490 images for four classes, including eczema, dermatitis, rosacea, and\n",
      "normal.\n",
      "2.1\n",
      "dual-pipeline (dp) architecture for apd-net\n",
      "since the proposed fl system is implemented in a mobile-based environment,\n",
      "mobilenetv3 is employed as a baseline network of apd-net.\n",
      "2.2\n",
      "customized genetic algorithm\n",
      "in the fl environment, data privacy is achieved by transferring gradients with-\n",
      "out sharing data.\n",
      "the ga is the optimal solution for adaptively personalized\n",
      "diagnosis in the fl environment, since it heuristically searches for another local\n",
      "minimum point regardless of the domain gaps.\n",
      "here, p (i)\n",
      "f |k is the average value of pg|k and p ∗\n",
      "p |k if g(i)\n",
      "k\n",
      "= 0.\n",
      "in addition, since we experimentally demonstrated that lk ≥ 0.15 provides\n",
      "a much longer time to ﬁne-tune apd-net, we constrained lk ≤ 0.15, and the\n",
      "ﬁxed values of lk are randomly determined for each experiment.\n",
      "while training the dl model, we experimentally veriﬁed that the convo-\n",
      "lution weights are changed within the range of the maximum 0.2%.\n",
      "let c(i; p)\n",
      "be the output of part i using the parameter p with an input image (i).\n",
      "therefore, the apd-net with our ga oﬀers\n",
      "high accuracy in both the conventional diagnosis for overall patients and the\n",
      "personalized diagnosis for each patient at a speciﬁc client.\n",
      "fine-tuning network in federated learning for personalized skin diagnosis\n",
      "383\n",
      "2.3\n",
      "training and fine-tuning apd-net\n",
      "to summarize, (1) apd-net is initially trained in the fl server using gradi-\n",
      "ents from many clients.\n",
      "conﬁgurations of datasets (left) and experiment i (right).\n",
      "datasets\n",
      "nevus\n",
      "melanoma\n",
      "others\n",
      "total\n",
      "7pt\n",
      "575\n",
      "268\n",
      "168\n",
      "1011\n",
      "isic\n",
      "5193\n",
      "284\n",
      "27349\n",
      "32826\n",
      "ham\n",
      "6705\n",
      "1113\n",
      "2197\n",
      "10015\n",
      "testset (25%)\n",
      "nevus\n",
      "melanoma\n",
      "others\n",
      "total\n",
      "7pt\n",
      "144\n",
      "67\n",
      "42\n",
      "253\n",
      "isic\n",
      "1299\n",
      "71\n",
      "6838\n",
      "8207\n",
      "ham\n",
      "1677\n",
      "279\n",
      "550\n",
      "2504\n",
      "# images\n",
      "(# for testset)\n",
      "group\n",
      "nevus\n",
      "melanoma\n",
      "others\n",
      "7pt\n",
      "g-00\n",
      "575 (144)\n",
      "268 (67)\n",
      "168 (42)\n",
      "isic\n",
      "g-01\n",
      "180 (45)\n",
      "80 (20)\n",
      "17070 (4268)\n",
      "g-02\n",
      "685 (172)\n",
      "363 (91)\n",
      "7915 (1979)\n",
      "g-03\n",
      "4328 (1082)\n",
      "(left) the number of skin images in the customized dataset for experiment\n",
      "ii.\n",
      "[14]\n",
      "✓\n",
      "ours\n",
      "fl + da\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "3\n",
      "experimental results\n",
      "3.1\n",
      "experimental setup\n",
      "dataset.\n",
      "to evaluate the performance and feasibility of apd-net, we used\n",
      "three public datasets, including 7pt, isic, and ham, and detailed descriptions\n",
      "for datasets are illustrated in table 1.\n",
      "furthermore, in this work, we collected\n",
      "skin images through the tertiary referral hospital under the approval of the\n",
      "institutional review board (irb no. 1908-161-1059) and obtained images with\n",
      "the consent of the subjects according to the principles of the declaration of\n",
      "helsinki from 51 patients and subjects.\n",
      "3. re-sampling the distribution of images in each client for experiment i\n",
      "table 3.\n",
      "4. classiﬁcation accuracy of apd-nets in each fl client\n",
      "including eczema, dermatitis, rosacea, and normal skin, with 258, 294, 738, and\n",
      "1,200 images, respectively, as illustrated in table 2(left).\n",
      "to compensate for the limited number of images in the test set, a 4-fold\n",
      "cross-validation approach was employed.\n",
      "to assess the performance of the pro-\n",
      "posed network as well as compared networks, two distinct fl environments were\n",
      "considered: (1) an fl simulation environment to evaluate the performance of\n",
      "apd-net and (2) a realistic fl environment to analyze the feasibility of apd-\n",
      "net.\n",
      "for the fl simulation environment in experiment i, public datasets were\n",
      "employed, and the distribution of samples was re-sampled using t-distributed\n",
      "stochastic neighbor embedding (t-sne)\n",
      "the images in all skin datasets\n",
      "were subsequently re-grouped, as illustrated in fig.\n",
      "3. in contrast, for experi-\n",
      "ment ii, we utilized a custom dataset for a realistic fl environment.\n",
      "six dl\n",
      "models that have shown exceptional performances in da, fl, and skin disease\n",
      "diagnosis were used as comparison methods to evaluate the fl and dl perfor-\n",
      "mance of apd-net.\n",
      "fine-tuning network in federated learning for personalized skin diagnosis\n",
      "385\n",
      "3.2\n",
      "experiment i.\n",
      "an ablation study was conducted to evaluate the impact of\n",
      "ga and dp on diagnostic performance.\n",
      "as illus-\n",
      "trated in table 3(left), the apd-net with ga and dp yielded the best perfor-\n",
      "mance.\n",
      "here, it is important to note that the dp architecture also improved the\n",
      "performance of the models for adaptively personalized diagnosis, similar to ga,\n",
      "by jointly using personalized and generalized parameters in the dp architecture.\n",
      "performance of apd-net was compared against those\n",
      "of the other dl models for adaptively personalized diagnosis.\n",
      "table 3(right)\n",
      "shows the performances of the dl models in every client (group).\n",
      "furthermore, fig. 4 demonstrates that the\n",
      "prediction with images from other clients yielded lower accuracy.\n",
      "fig.\n",
      "(right) comparison analysis of apd-net and the other dl models in a\n",
      "desirable fl environment.\n",
      "386\n",
      "k. lee et al.\n",
      "3.3\n",
      "experiment ii.\n",
      "fl environment\n",
      "to verify the feasibility of apd-net, the performance of apd-net was evaluated\n",
      "using the customized datasets acquired from our devices for adaptively person-\n",
      "alized diagnosis.\n",
      "the performance of apd-nets was compared against the other\n",
      "dl models.\n",
      "since the prediction time is critical in the mobile-based environ-\n",
      "ment, the prediction times of the dl models were compared in addition to the\n",
      "accuracy.\n",
      ", apd-net (ours) yielded an outstanding\n",
      "performance as well as a shorter prediction time compared with the other dl\n",
      "models for adaptively personalized diagnosis.\n",
      "5(right) shows\n",
      "the performances of apd-nets for adaptively personalized diagnosis in every\n",
      "client.\n",
      "6(left) illustrates\n",
      "the performance of apd-net.\n",
      "since the number of images is relatively small, the\n",
      "images were divided into three clients.\n",
      "the results showed that our apd-net has the\n",
      "potential to be used in the fl environment with an accuracy of 88.51%.\n",
      "the ﬁtness score\n",
      "and accuracy were calculated corresponding to many input images and various\n",
      "versions of the ﬁne-tuned parameters.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf:\n",
      "we investigate performance disparities in deep classiﬁers.\n",
      "we\n",
      "ﬁnd that the ability of classiﬁers to separate individuals into subgroups\n",
      "varies substantially across medical imaging modalities and protected char-\n",
      "acteristics; crucially, we show that this property is predictive of algorith-\n",
      "mic bias.\n",
      "through theoretical analysis and extensive empirical evalua-\n",
      "tion (code is available at https://github.com/biomedia-mira/subgroup-\n",
      "separability), we ﬁnd a relationship between subgroup separability, sub-\n",
      "group disparities, and performance degradation when models are trained\n",
      "on data with systematic bias such as underdiagnosis.\n",
      "our ﬁndings shed new\n",
      "light on the question of how models become biased, providing important\n",
      "insights for the development of fair medical imaging ai.\n",
      "1\n",
      "introduction\n",
      "medical image computing has seen great progress with the development of deep\n",
      "image classiﬁers, which can be trained to perform diagnostic tasks to the level of\n",
      "skilled professionals [19].\n",
      "recently, it was shown that these models might rely on\n",
      "sensitive information when making their predictions [7,8] and that they exhibit\n",
      "performance disparities across protected population subgroups [20].\n",
      "although\n",
      "many methods exist for mitigating bias in image classiﬁers, they often fail unex-\n",
      "pectedly and may even be harmful in some situations [26].\n",
      "today, no bias miti-\n",
      "gation methods consistently outperform the baseline approach of empirical risk\n",
      "minimisation (erm)\n",
      "[22,27], and none are suitable for real-world deployment.\n",
      "if we wish to deploy appropriate and fair automated systems, we must ﬁrst\n",
      "understand the underlying mechanisms causing erm models to become biased.\n",
      "some medical\n",
      "images encode sensitive information that models may leverage to classify indi-\n",
      "viduals into subgroups [7].\n",
      "we may expect\n",
      "groups with intrinsic physiological diﬀerences to be highly separable for deep\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43898-1 18.\n",
      "https://doi.org/10.1007/978-3-031-43898-1_18\n",
      "180\n",
      "c. jones et al.\n",
      "image classiﬁers (e.g. biological sex from chest x-ray can be predicted with\n",
      "> 0.98 auc).\n",
      "this is especially relevant in\n",
      "medical imaging, where attributes such as age, biological sex, self-reported race,\n",
      "socioeconomic status, and geographic location are often considered sensitive for\n",
      "various clinical, ethical, and societal reasons.\n",
      "we show that the ability of\n",
      "models to detect which group an individual belongs to varies across modalities\n",
      "and groups in medical imaging and that this property has profound consequences\n",
      "for the performance and fairness of deep classiﬁers.\n",
      "to the best of our knowledge,\n",
      "ours is the ﬁrst work which analyses group-fair image classiﬁcation through the\n",
      "lens of subgroup separability.\n",
      "– we show theoretically that such diﬀerences in subgroup separability aﬀect\n",
      "model bias in learned classiﬁers and that group fairness metrics may be inap-\n",
      "propriate for datasets with low subgroup separability.\n",
      "– we corroborate our analysis with extensive testing on real-world medical\n",
      "datasets, ﬁnding that performance degradation and subgroup disparities are\n",
      "functions of subgroup separability when data is biased.\n",
      "2\n",
      "related work\n",
      "group-fair image analysis seeks to mitigate performance disparities caused by\n",
      "models exploiting sensitive information.\n",
      "follow-up work has addi-\n",
      "tionally shown that these models may use sensitive information to bias their\n",
      "predictions [7,8].\n",
      "unfortunately, standard bias mitigation methods from com-\n",
      "puter vision, such as adversarial training [1,14] and domain-independent training\n",
      "[24], are unlikely to be suitable solutions.\n",
      "on\n",
      "natural images, zietlow et al.\n",
      "[26] showed that bias mitigation methods worsen\n",
      "performance for all groups compared to erm, giving a stark warning that blindly\n",
      "applying methods and metrics leads to a dangerous ‘levelling down’ eﬀect [16].\n",
      "one step towards overcoming these challenges and developing fair and perfor-\n",
      "mant methods is understanding the circumstances under which deep classiﬁers\n",
      "learn to exploit sensitive information inappropriately.\n",
      "closely related to our work is oakden-rayner et al., who\n",
      "consider how ‘hidden stratiﬁcation’ may aﬀect learned classiﬁers [18]; similarly,\n",
      "jabbour et al. use preprocessing ﬁlters to inject spurious correlations into chest\n",
      "x-ray data, ﬁnding that erm-trained models are more biased when the corre-\n",
      "lations are easier to learn [12]. outside of fairness, our work may have broader\n",
      "impact in the ﬁelds of distribution shift and shortcut learning [6,25], where many\n",
      "examples exist of models learning to exploit inappropriate spurious correlations\n",
      "[3,5,17], yet tools for detecting and mitigating the problem remain immature.\n",
      "subgroup separability in medical image classiﬁcation\n",
      "181\n",
      "3\n",
      "the role of subgroup separability\n",
      "consider a binary disease classiﬁcation problem where, for each image x ∈ x, we\n",
      "wish to predict a class label\n",
      "→ [0, 1] the\n",
      "underlying mapping between images and class labels.\n",
      "suppose we have access to a\n",
      "(biased) training dataset, where ptr is the conditional distribution between train-\n",
      "ing images and training labels; we say that such a dataset is biased if ptr ̸= p.\n",
      "we focus on group fairness, where each individual belongs to a subgroup a ∈ a\n",
      "and aim to learn a fair model that maximises performance for all groups when\n",
      "deployed on an unbiased test dataset drawn from p. we assume that the groups\n",
      "are consistent across both datasets.\n",
      "the bias we consider in this work is under-\n",
      "diagnosis, a form of label noise [4] where some truly positive individuals x+ are\n",
      "mislabeled as negative.\n",
      "we are particularly concerned with cases where under-\n",
      "diagnosis manifests in speciﬁc subgroups due to historic disparities in healthcare\n",
      "provision or discriminatory diagnosis policy.\n",
      "≤ p(y|x+, a∗) and ∀a ̸= a∗, ptr(y|x+, a) = p(y|x+, a)\n",
      "(1)\n",
      "we may now use the law of total probability to express the overall mapping\n",
      "from image to label in terms of the subgroup-wise mappings in eq.\n",
      "(3) – the probability of a truly positive individual\n",
      "being assigned a positive label is lower in the biased training dataset than for\n",
      "the unbiased test set.\n",
      "ptr(y|x) =\n",
      "\u0002\n",
      "a∈a\n",
      "ptr(y|x, a)ptr(a|x)\n",
      "(2)\n",
      "ptr(y|x+) ≤ p(y|x+)\n",
      "(3)\n",
      "at training time, supervised learning with empirical risk minimisation aims\n",
      "to obtain a model ˆp, mapping images to predicted labels ˆy = argmaxy∈y ˆp(y|x)\n",
      "such that ˆp(y|x)\n",
      "since this model approximates the biased\n",
      "training distribution, we may expect underdiagnosis from the training data to be\n",
      "reﬂected by the learned model when evaluated on the unbiased test set.\n",
      "this model underdiagnoses group\n",
      "a = a∗ whilst recovering the unbiased mapping for other groups.\n",
      "(5) show that, at test-time, our model will demonstrate\n",
      "worse performance for the underdiagnosed subgroup than the other subgroups.\n",
      "indeed, consider true positive rate (tpr) as a performance metric.\n",
      "the group-\n",
      "wise tpr of an unbiased model, tpr(u)\n",
      "a , is expressed in eq.\n",
      "remember, in practice, we must train our model on the biased train-\n",
      "ing distribution ptr.\n",
      "(8) demonstrate that\n",
      "tpr of the underdiagnosed group is directly aﬀected by bias from the training\n",
      "set while other groups are mainly unaﬀected.\n",
      "given this diﬀerence across groups,\n",
      "an appropriately selected group fairness metric may be able to identify the bias,\n",
      "in some cases even without access to an unbiased test set [23].\n",
      "ˆp(y|x+, a) ≈ ptr(y|x+), ∀a ∈ a\n",
      "(9)\n",
      "equations (3) and (9) imply that the performance of the trained model\n",
      "degrades for all groups.\n",
      "(10) represents\n",
      "performance degradation for all groups when separability is poor.\n",
      "in such sit-\n",
      "uations, we expect performance degradation to be uniform across groups and\n",
      "thus not be detected by group fairness metrics.\n",
      "tpr(b)\n",
      "a\n",
      "≈ |ptr(y|x+, a) > 0.5|\n",
      "n+,a\n",
      "≤ |p(y|x+, a) > 0.5|\n",
      "n+,a\n",
      "≈ tpr(u)\n",
      "a , ∀a ∈ a\n",
      "(10)\n",
      "we have derived the eﬀect of underdiagnosis bias on classiﬁer performance\n",
      "for the two extreme cases of high and low subgroup separability.\n",
      "4, we empirically investigate (i) how subgroup separa-\n",
      "bility varies in the wild, (ii) how separability impacts performance for each group\n",
      "when underdiagnosis bias is added to the datasets, (iii) how models encode sen-\n",
      "sitive information in their representations.\n",
      "subgroup separability in medical image classiﬁcation\n",
      "183\n",
      "4\n",
      "experiments and results\n",
      "we support our analysis with experiments on ﬁve datasets adapted from a subset\n",
      "of the medfair benchmark [27].\n",
      "[9,10,21], fundus images [15], and chest x-ray\n",
      "we record\n",
      "summary statistics for the datasets used in the supplementary material (table\n",
      "a1), where we also provide access links (table a2).\n",
      "our architecture and hyper-\n",
      "parameters are listed in table a3, adapted from the experiments in medfair.\n",
      "mean and standard deviation are\n",
      "reported over ten random seeds, with results sorted by ascending mean auc.\n",
      "dataset-attribute\n",
      "modality\n",
      "subgroups\n",
      "auc\n",
      "group 0 group 1\n",
      "μ\n",
      "σ\n",
      "papila-sex\n",
      "fundus image\n",
      "male\n",
      "female\n",
      "0.642 0.057\n",
      "ham10000-sex\n",
      "skin dermatology male\n",
      "female\n",
      "0.723 0.015\n",
      "ham10000-age\n",
      "skin dermatology <60\n",
      "≥60\n",
      "0.803 0.020\n",
      "papila-age\n",
      "fundus image\n",
      "<60\n",
      "≥60\n",
      "0.812 0.046\n",
      "fitzpatrick17k-skin skin dermatology i–iii\n",
      "iv–vi\n",
      "0.891 0.010\n",
      "chexpert-age\n",
      "chest x-ray\n",
      "<60\n",
      "≥60\n",
      "0.920 0.003\n",
      "mimic-age\n",
      "chest x-ray\n",
      "<60\n",
      "≥60\n",
      "0.930 0.002\n",
      "chexpert-race\n",
      "chest x-ray\n",
      "white\n",
      "non-white 0.936 0.005\n",
      "mimic-race\n",
      "chest x-ray\n",
      "white\n",
      "non-white 0.951 0.004\n",
      "chexpert-sex\n",
      "chest x-ray\n",
      "male\n",
      "female\n",
      "0.980 0.020\n",
      "mimic-sex\n",
      "chest x-ray\n",
      "male\n",
      "female\n",
      "0.986 0.008\n",
      "age is consistently\n",
      "well predicted across all modalities, whereas separability of biological sex varies,\n",
      "184\n",
      "c. jones et al.\n",
      "with prediction of sex from fundus images being especially weak.\n",
      "performance degradation under label bias\n",
      "we now test our theoretical ﬁnding: models are aﬀected by underdiagnosis dif-\n",
      "ferently depending on subgroup separability.\n",
      "we inject underdiagnosis bias into\n",
      "each training dataset by randomly mislabelling 25% of positive individuals in\n",
      "group 1 (see table 1) as negative.\n",
      "for each dataset-attribute combination, we\n",
      "train ten disease classiﬁcation models with the biased training data and ten mod-\n",
      "els with the original clean labels; we test all models on clean data.\n",
      "we assess\n",
      "how the test-time performance of the models trained on biased data degrades\n",
      "relative to models trained on clean data.\n",
      "we illustrate the mean percentage point\n",
      "accuracy degradation for each group in fig.\n",
      "and use the mann-whitney u test\n",
      "(with the holm-bonferroni adjustment for multiple hypothesis testing) to deter-\n",
      "mine if the performance degradation is statistically signiﬁcant at pcritical = 0.05.\n",
      "we include an ablation experiment over varying label noise intensity in fig.\n",
      "1. percentage-point degradation in accuracy for disease classiﬁers trained on\n",
      "biased data, compared to training on clean data.\n",
      "lower values indicate worse per-\n",
      "formance for the biased model when tested on a clean dataset.\n",
      "3. we report\n",
      "no statistically signiﬁcant performance degradation for dataset-attribute combi-\n",
      "nations with low subgroup separability (<0.9 auc).\n",
      "in these experiments, the\n",
      "proportion of mislabelled images is small relative to the total population; thus,\n",
      "the underdiagnosed subgroups mostly recover from label bias by sharing the\n",
      "subgroup separability in medical image classiﬁcation\n",
      "185\n",
      "correct mapping with the uncorrupted group.\n",
      "while we see surprising improve-\n",
      "ments in performance for papila, note that this is the smallest dataset, and\n",
      "these improvements are not signiﬁcant at pcritical = 0.05.\n",
      "as subgroup separabil-\n",
      "ity increases, performance degrades more for the underdiagnosed group (group\n",
      "1), whilst performance for the uncorrupted group (group 0) remains somewhat\n",
      "unharmed.\n",
      "we see a statistically signiﬁcant performance drop for group 0 in the\n",
      "mimic-sex experiment – we believe this is because the model learns separate\n",
      "group-wise mappings, shrinking the eﬀective size of the dataset for group 0.\n",
      "use of sensitive information in biased models\n",
      "finally, we investigate how biased models use sensitive information.\n",
      "[7,8] to all\n",
      "models trained for the previous experiment, involving freezing the trained back-\n",
      "bone and re-training the ﬁnal layer to predict the sensitive attribute.\n",
      "we ﬁnd that models trained on biased data\n",
      "learn to encode sensitive information in their representations and see a statisti-\n",
      "cally signiﬁcant association between the amount of information available and the\n",
      "amount encoded in the representations.\n",
      "models trained on unbiased data have\n",
      "no signiﬁcant association, so do not appear to exploit sensitive information.\n",
      "fig.\n",
      "along the maximum sensitive information\n",
      "line, models trained for predicting the disease encode as much sensitive information in\n",
      "their representations as the images do themselves.\n",
      "186\n",
      "c. jones et al.\n",
      "5\n",
      "discussion\n",
      "we investigated how subgroup separability aﬀects the performance of deep neural\n",
      "networks for disease classiﬁcation.\n",
      "in fairness liter-\n",
      "ature, data is often assumed to contain suﬃcient information to identify indi-\n",
      "viduals as subgroup members.\n",
      "our results are not exhaustive – there are\n",
      "many modalities and sensitive attributes we did not consider – however, by\n",
      "demonstrating a wide range of separability results across diﬀerent attributes and\n",
      "modalities, we highlight a rarely considered property of medical image datasets.\n",
      "performance degradation is a function of subgroup separability.\n",
      "we showed,\n",
      "theoretically and empirically, that the performance and fairness of models trained\n",
      "on biased data depends on subgroup separability.\n",
      "when separability is high,\n",
      "models learn to exploit the sensitive information and the bias is reﬂected by stark\n",
      "subgroup diﬀerences.\n",
      "this indicates that group\n",
      "fairness metrics may be insuﬃcient for detecting bias when separability is low.\n",
      "our analysis centred on bias in classiﬁers trained with the standard approach\n",
      "of empirical risk minimisation – future work may wish to investigate whether\n",
      "subgroup separability is a factor in the failure of bias mitigation methods and\n",
      "whether it remains relevant in further image analysis tasks (e.g. segmentation).\n",
      "sources of bias matter.\n",
      "in our experiments, we injected underdiagnosis bias\n",
      "into the training set and treated the uncorrupted test set as an unbiased ground\n",
      "truth.\n",
      "however, this is not an endorsement of the quality of the data.\n",
      "at least\n",
      "some of the datasets may already contain an unknown amount of underdiagnosis\n",
      "bias (among other sources of bias)\n",
      "this pre-existing bias will likely have\n",
      "a smaller eﬀect size than our artiﬁcial bias, so it should not play a signiﬁcant\n",
      "role in our results.\n",
      "still, the unmeasured bias may explain some variation in\n",
      "results across datasets.\n",
      "future work should investigate how subgroup separability\n",
      "interacts with other sources of bias.\n",
      "we renew the call for future datasets to be\n",
      "released with patient metadata and multiple annotations to enable analysis of\n",
      "diﬀerent sources and causes of bias.\n",
      "reproducibility and impact.\n",
      "we provide a complete implementation\n",
      "of our preprocessing, experimentation, and analysis of results at https://github.\n",
      "com/biomedia-mira/subgroup-separability.\n",
      "subgroup separability in medical image classiﬁcation\n",
      "187\n",
      "acknowledgements.\n",
      "references\n",
      "1. alvi, m., zisserman, a., nell˚aker, c.: turning a blind eye: explicit removal of biases\n",
      "and variation from deep neural network embeddings.\n",
      "potential sources of dataset bias complicate\n",
      "investigation of underdiagnosis by machine learning algorithms.\n",
      "https://doi.org/10.1038/s42256-020-00257-z\n",
      "7. gichoya, j.w., et al.: ai recognition of patient race in medical imaging: a mod-\n",
      "elling study.\n",
      "https://doi.org/10.1016/j.ebiom.2023.104467\n",
      "9. groh, m., harris, c., daneshjou, r., badri, o., koochek, a.: towards transparency\n",
      "in dermatology image datasets with skin tone annotations by experts, crowds,\n",
      "and an algorithm.\n",
      "groh, m., et al.: evaluating deep neural networks trained on clinical images in\n",
      "dermatology with the ﬁtzpatrick 17k dataset.\n",
      "kim, b., kim, h., kim, k., kim, s., kim, j.: learning not to learn: training deep\n",
      "neural networks with biased data.\n",
      "kovalyk, o., morales-s´anchez, j., verd´u-monedero, r., sell´es-navarro, i., palaz´on-\n",
      "cabanes, a., sancho-g´omez, j.l.: papila: dataset with fundus images and clini-\n",
      "cal data of both eyes of the same patient for glaucoma assessment.\n",
      "https://doi.org/10.1038/s41597-022-01388-1\n",
      "16. mittelstadt, b., wachter, s., russell, c.: the unfairness of fair machine learning:\n",
      "levelling down and strict egalitarianism by default, january 2023\n",
      "17.\n",
      "https://doi.org/10.1145/3368555.3384468\n",
      "19. rajpurkar, p., et al.: chexnet: radiologist-level pneumonia detection on chest x-\n",
      "rays with deep learning, november 2017\n",
      "20. seyyed-kalantari, l., zhang, h., mcdermott, m.b., chen, i.y., ghassemi, m.:\n",
      "underdiagnosis bias of artiﬁcial intelligence algorithms applied to chest radiographs\n",
      "in under-served patient populations.\n",
      "tschandl, p., rosendahl, c., kittler, h.: the ham10000 dataset, a large collection\n",
      "of multi-source dermatoscopic images of common pigmented skin lesions.\n",
      "wachter, s., mittelstadt, b., russell, c.: bias preservation in machine learning:\n",
      "the legality of fairness metrics under eu non-discrimination law.\n",
      "wang, z., et al.: towards fairness in visual recognition: eﬀective strategies for bias\n",
      "mitigation.\n",
      "zong, y., yang, y., hospedales, t.: medfair: benchmarking fairness for medical\n",
      "imaging.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf:\n",
      "since mhsi\n",
      "is a 3d hypercube, building a 3d segmentation network is the most\n",
      "intuitive way for mhsi segmentation.\n",
      "but, high spatiospectral dimen-\n",
      "sions make it diﬃcult to perform eﬃcient and eﬀective segmentation.\n",
      "by plugging our dual-stream strategy into 2d backbones,\n",
      "we can achieve state-of-the-art mhsi segmentation performances with 3–\n",
      "13 times faster compared with existing 3d networks in terms of inference\n",
      "speed.\n",
      "experiments show our strategy leads to remarkable performance\n",
      "gains in diﬀerent 2d architectures, reporting an improvement up to 7.7%\n",
      "compared with its 2d counterpart in terms of dsc on a public multi-\n",
      "dimensional choledoch dataset.\n",
      "keywords: medical hyperspectral images · mhsi segmentation\n",
      "1\n",
      "introduction\n",
      "medical hyperspectral imaging (mhsi) is an emerging imaging modality which\n",
      "acquires two-dimensional medical images across a wide range of electromag-\n",
      "netic spectrum.\n",
      "typically, an mhsi is presented as a hypercube, with hundreds\n",
      "of narrow and contiguous spectral bands in spectral dimension, and thousands\n",
      "of pixels in spatial dimension (fig. 1(a)).\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43901-8_15.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43901-8_15\n",
      "factor space and spectrum\n",
      "153\n",
      "(b)\n",
      "posi\u0002ve\n",
      "nega\u0002ve\n",
      "spatial dimension \n",
      "spatial dimension \n",
      "(a)\n",
      "2d-encode\n",
      "2d-decoder\n",
      "pca\n",
      "mask\n",
      "(d) pca-2d-net\n",
      "2d-encoder\n",
      "2d-decoder\n",
      "mask\n",
      "(c) 2d-net\n",
      "3d-encoder\n",
      "3d-decoder\n",
      "mask\n",
      "spectral-\n",
      "pooling\n",
      "(e) 3d-net\n",
      "2d-encoder\n",
      "2d-decoder\n",
      "1d-encoder\n",
      "spectral\n",
      "mask\n",
      "(f) dual-stream\n",
      "550      700  \n",
      "850    1000  \n",
      "spectral correlation\n",
      "550      700  \n",
      "850    1000  \n",
      "0   10  20\n",
      "30   40   50   60  \n",
      "spectral bands\n",
      "0.5  0.6\n",
      "0.7   0.8   0.9 \n",
      "values\n",
      "spatial correlation\n",
      "fig.\n",
      "(c)–(f): hsi classiﬁcation and segmentation backbones.\n",
      "due to the success of 2-dimensional (2d) deep neural network in natural\n",
      "images, the simplest way to classify/segment an mhsi is to treat its two spa-\n",
      "tial dimensions as input spatial dimension, and treat its spectral dimension as\n",
      "input channel dimension [25] (fig. 1(c)).\n",
      "dimensionality reduction [12] and recur-\n",
      "rent approaches\n",
      "these methods are not suitable\n",
      "for high spatial resolution mhsi, and they may bring noises in spatial features\n",
      "while reducing spectral dimension.\n",
      "high spatiospectral dimensions make it diﬃcult to perform a thorough anal-\n",
      "ysis of mhsi.\n",
      "exploring mhsi’s\n",
      "low-rank prior can promote the segmentation performance.\n",
      "in this paper, we consider treating spatiospectral dimensions separately and\n",
      "propose an eﬀective and eﬃcient dual-stream strategy to “factor” the archi-\n",
      "tecture, by exploiting the correlation information of mhsis.\n",
      "for\n",
      "the spatial feature extraction stream, inspired from spatial redundancy between\n",
      "adjacent bands, we group adjacent bands into a spectral agent.\n",
      "diﬀerent spec-\n",
      "tral agents are fed into a 2d cnn backbone as a batch.\n",
      "we also show that with our proposed strategy,\n",
      "u-net model using resnet-34 can achieve state-of-the-art mhsi segmentation\n",
      "with 3–13 faster than other 3d architectures.\n",
      "the goal of mhsi segmentation is to predict the per-pixel annotation mask\n",
      "ˆy ∈ {0, 1}h×w .\n",
      "it represents a spatial stream, which focuses on extracting spa-\n",
      "tial features from spectral agents (sect. 2.1).\n",
      "the lightweight spectral stream\n",
      "learns multi-granular spectral features, and it consists of three key modules:\n",
      "depthwise convolution (dwconv), spectral matrix decomposition (smd) and\n",
      "feed forward network, where smd module eﬀectively leverages low-rank prior\n",
      "from spectral features (sect. 2.1).\n",
      "the\n",
      "input mhsi z is decomposed into a spatial input zspa ∈ rg×(s/g)×h×w and\n",
      "a spectral input zspe ∈ rs×cspe\n",
      "0\n",
      "×h×w , where g indicates evenly dividing spec-\n",
      "tral bands into g groups, i.e., spectral agents.\n",
      "s/g and cspe\n",
      "0\n",
      "= 1 are the input\n",
      "feature dimensions for two streams respectively.\n",
      "2.1\n",
      "dual-stream architecture with spatiospectral representation\n",
      "as mentioned above, for the spatial stream, we ﬁrst reshape mhsi z ∈ rs×h×w\n",
      "into zspa ∈ rg×(s/g)×h×w , which has g spectral agents.\n",
      "each spectral agent is\n",
      "factor space and spectrum\n",
      "155\n",
      "spectral encoder block\n",
      "encoder block\n",
      "decoder block\n",
      "low-rank decomposition\n",
      "g\n",
      "h\n",
      "w\n",
      "s × c0\n",
      "spe\n",
      "an mhsi goes through a spectral\n",
      "stream with the proposed spectral encoder block which consists of three key mod-\n",
      "ules, i.e, dwconv, spectral matrix decomposition (smd) and feed forward network\n",
      "(ffn), and it goes through a spatial stream after dividing into g spectral agents.\n",
      "our\n",
      "spectral encoder block can be formulated by:\n",
      "x = dwconv(zin), x′ = smd(norm(x))+ x, zout = ffn(norm(x′))+x′,\n",
      "(1)\n",
      "where zin ∈ rs×cspe×h×w indicates the input spectral token tensor, and cspe\n",
      "is the spectral feature dimension.\n",
      "2, spectral information is integrated from\n",
      "channel dimensions by performing concatenation, after the second and fourth\n",
      "encoder blocks, to aggregate the spatiospectral features.\n",
      "the reason for this\n",
      "design is that spectral features are simpler and lack hierarchical structures com-\n",
      "pared to spatial features, we will discuss more in the experimental section.\n",
      "finally, we aggregate all rank-1 tensors (from a1 to ar) into the attention\n",
      "map along the channel dimension, followed by a linear layer used to reduce the\n",
      "feature dimension to obtain the low-rank feature ulow:\n",
      "ulow = u ⊙ linear(concate(a1, a2, ..., ar)),\n",
      "(3)\n",
      "where ⊙ is the element-wise product, and ulow ∈ rc′×h′×w ′. we employ a\n",
      "straightforward non-parametric ensemble approach for grouping spectral agents.\n",
      "this approach involves multiple agents combining their features by averaging\n",
      "the vote.\n",
      "the encoders in the spatial stream produce 2d feature maps with g\n",
      "spectral agents, deﬁned as fi ∈ rg×ci×h/2i×w/2i for the ith encoder, where g,\n",
      "ci, h/2i, and w/2i represent the spectral, channel, and two spatial dimensions,\n",
      "respectively.\n",
      "f g\n",
      "i ), where\n",
      "f g\n",
      "i\n",
      "∈ rci×h/2i×w/2i represents the 2d feature map of the gth agent.\n",
      "the\n",
      "ensemble operation aggregates spectral agents to produce a 2d feature map\n",
      "factor space and spectrum\n",
      "157\n",
      "table\n",
      "sa denote the spectral agent.\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "×\n",
      "55.01 (15.54)\n",
      "69.58 (14.10)\n",
      "86.25 (35.40)\n",
      "✓\n",
      "×\n",
      "✓\n",
      "×\n",
      "✓\n",
      "✓\n",
      "56.90 (17.38) 70.88 (15.05) 82.72 (31.77)\n",
      "✓\n",
      "×\n",
      "conv ×\n",
      "conv ✓\n",
      "55.19 (18.58)\n",
      "69.15 (16.63)\n",
      "83.13 (31.08)\n",
      "✓\n",
      "×\n",
      "tr\n",
      "×\n",
      "tr\n",
      "✓\n",
      "55.72 (17.16)\n",
      "69.89 (15.26)\n",
      "84.48 (32.13)\n",
      "with enhanced information interactions learned from the multi-spectral agents.\n",
      "the feature maps obtained from the ensemble can be decoded using lightweight\n",
      "2d decoders to generate segmentation masks.\n",
      "3\n",
      "experimental results\n",
      "3.1\n",
      "experimental setup\n",
      "we conducted experiments on the public multi-dimensional choledoch (mdc)\n",
      "dataset\n",
      "[31] with 538 scenes and hyperspectral gastric carcinoma (hgc)\n",
      "dataset [33] (data provided by the author) with 414 scenes, both with high-\n",
      "quality labels for binary mhsi segmentation tasks.\n",
      "the size of a single band image in mdc and\n",
      "hgc are both resized to 256 × 320.\n",
      "we use data augmentation techniques such as rotation and ﬂipping, and train\n",
      "with an adam optimizer using a combination of dice loss and cross-entropy\n",
      "loss for 8 batch size and 100 epochs.\n",
      "the segmentation performance is evalu-\n",
      "ated using dice-sørensen coeﬃcient (dsc), intersection of union (iou), and\n",
      "hausdorﬀ distance (hd) metrics, and throughput (images/s) is reported for\n",
      "158\n",
      "b. yun et al.\n",
      "fig.\n",
      "left ﬁgures plot each\n",
      "feature embedding in ascending order of the number of times they are dominant in the\n",
      "population (y-axis) and feature dimension (x-axis) on the statistical results of the test\n",
      "set.\n",
      "pytorch framework and four nvidia geforce rtx 3090 are used\n",
      "for implementation.\n",
      "3.2\n",
      "evaluation of the proposed dual-stream strategy\n",
      "ablation study.\n",
      "as shown in table 1,\n",
      "our ablation study shows that spectral agent strategy improves segmentation\n",
      "performance by more than 2.5% (63.11 vs. 66.05).\n",
      "if we utilize spectral infor-\n",
      "mation from the spectral stream to assist in the spatial stream, we ﬁnd that\n",
      "inserting spectral information at l2 and l4 yields a signiﬁcant improvement of\n",
      "3.7% (69.73 vs. 66.05), while inserting at l4 alone also results in a signiﬁcant\n",
      "increase of 1.9% in dsc (67.95 vs. 66.05).\n",
      "a slight improvement is observed when\n",
      "inserting at l2, possibly due to the coarse features of shallow spectral informa-\n",
      "tion.\n",
      "our proposed ld module is crucial, resulting in a 1.12% performance drop in\n",
      "terms of dsc without it.\n",
      "performance comparison in “mean (std)” in mdc and hcg dataset.\n",
      "+ours\n",
      "fpn\n",
      "73.01 (13.84) 78.37 (30.26) 81.88 (14.75) 68.60 (45.63)\n",
      "table 3. performance comparison with sota methods with throughput(images/s)\n",
      "on mdc dataset and hgc dataset.\n",
      "[28]\n",
      "73.66\n",
      "76.92 1.40\n",
      "84.74\n",
      "66.90\n",
      "2.44\n",
      "smd and ld modules can reduce feature redundancy and lead to an increase\n",
      "in segmentation performance.\n",
      "comparisons between w/ and w/o dual-stream strategy on diﬀerent\n",
      "backbones.\n",
      "to show the eﬀectiveness of our dual-stream strategy in improv-\n",
      "ing mhsi segmentation performance in various architectures, we plug it into\n",
      "diﬀerent segmentation methods, i.e., u-net\n",
      "the results\n",
      "obtained with the proposed dual-stream strategy can consistently boost the seg-\n",
      "mentation performance by a large margin.\n",
      "160\n",
      "b. yun et al.\n",
      "comparisons with state-of-the-art mhsi segmentation methods.\n",
      "experimental\n",
      "results show that 2d methods are generally faster than 3d methods in inference\n",
      "speed, but 3d methods have an advantage in segmentation performance (dsc\n",
      "& hd).\n",
      "however, our approach outperforms other methods in both inference\n",
      "speed and segmentation accuracy.\n",
      "it is also plug-and-play, with the potential\n",
      "to achieve better segmentation performance by selecting more powerful back-\n",
      "bones.\n",
      "the complete table (including iou and variance) and qualitative results\n",
      "are shown in the supplementary material.\n",
      "4\n",
      "conclusion\n",
      "in this paper, we present to factor space and spectrum for accurate and fast\n",
      "medical hyperspectral image segmentation.\n",
      "experiments show signiﬁcant performance improvements on\n",
      "diﬀerent evaluation metrics, e.g., with our proposed strategy, we can obtain over\n",
      "7.7% improvement in dsc compared with its 2d counterpart.\n",
      "after plugging\n",
      "our strategy into resnet-34 backbone, we can achieve state-of-the-art mhsi\n",
      "segmentation accuracy with 3–13 times faster in terms of inference speed than\n",
      "existing 3d networks.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf:\n",
      "recent researches on cancer segmentation in dynamic con-\n",
      "trast enhanced magnetic resonance imaging (dce-mri) usually resort to\n",
      "the combination of temporal kinetic characteristics and deep learning to\n",
      "improve segmentation performance.\n",
      "however, the diﬃculty in accessing\n",
      "complete temporal sequences, especially post-contrast images, hinders\n",
      "segmentation performance, generalization ability and clinical application\n",
      "of existing methods.\n",
      "in this work, we propose a diﬀusion kinetic model\n",
      "(dkm) that implicitly exploits hemodynamic priors in dce-mri and\n",
      "eﬀectively generates high-quality segmentation maps only requiring pre-\n",
      "contrast images.\n",
      "we speciﬁcally consider the underlying relation between\n",
      "hemodynamic response function (hrf) and denoising diﬀusion process\n",
      "(ddp), which displays remarkable results for realistic image generation.\n",
      "our proposed dkm consists of a diﬀusion module (dm) and segmen-\n",
      "tation module (sm) so that dkm is able to learn cancer hemodynamic\n",
      "information and provide a latent kinetic code to facilitate segmenta-\n",
      "tion performance.\n",
      "once the dm is pretrained, the latent code estimated\n",
      "from the dm is simply incorporated into the sm, which enables dkm to\n",
      "automatically and accurately annotate cancers with pre-contrast images.\n",
      "to our best knowledge, this is the ﬁrst work exploring the relationship\n",
      "between hrf and ddp for dynamic mri segmentation.\n",
      "we evaluate\n",
      "the proposed method for tumor segmentation on public breast cancer\n",
      "dce-mri dataset.\n",
      "compared to the existing state-of-the-art approaches\n",
      "with complete sequences, our method yields higher segmentation perfor-\n",
      "mance even with pre-contrast images.\n",
      "keywords: deep learning · kinetic representation · dce-mri ·\n",
      "cancer segmentation · denoising diﬀusion model\n",
      "1\n",
      "introduction\n",
      "dynamic contrast-enhanced magnetic resonance imaging (dce-mri) reveal-\n",
      "ing tumor hemodynamics information is often applied to early diagnosis and\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43901-8_10\n",
      "diﬀusion kinetic model for cancer segmentation\n",
      "101\n",
      "fig.\n",
      "x0 and xk\n",
      "represent pre-contrast images and post-contrast images in dce-mri, respectively.\n",
      "treatment of breast cancer\n",
      "in particular, automatically and accurately seg-\n",
      "menting tumor regions in dce-mri is vital for computer-aided diagnosis (cad)\n",
      "and various clinical tasks such as surgical planning.\n",
      "for the sake of promoting\n",
      "segmentation performance, recent methods utilize the dynamic mr sequence and\n",
      "exploit its temporal correlations to acquire powerful representations [2–4].\n",
      "more\n",
      "recently, a handful of approaches take advantage of hemodynamic knowledge\n",
      "and time intensity curve (tic) to improve segmentation accuracy\n",
      "[5,6]. how-\n",
      "ever, the aforementioned methods require the complete dce-mri sequences and\n",
      "overlook the diﬃculty in assessing complete temporal sequences and the missing\n",
      "time point problem, especially post-contrast phase, due to the privacy protection\n",
      "and patient conditions.\n",
      "hence, these breast cancer segmentation models cannot\n",
      "be deployed directly in clinical practice.\n",
      "[7,8] has produced\n",
      "a tremendous impact on image generation ﬁeld due to its impressive performance.\n",
      "diﬀusion model is composed of a forward diﬀusion process that add noise to\n",
      "images, along with a reverse generation process that generates realistic images\n",
      "from the noisy input [8].\n",
      "based on this, several methods investigate the potential\n",
      "of ddpm for natural image segmentation [9] and medical image segmentation\n",
      "[9] explores the intermediate activations\n",
      "from the networks that perform the markov step of the reverse diﬀusion process\n",
      "and ﬁnd these activations can capture semantic information for segmentation.\n",
      "however, the applicability of ddpm to medical image segmentation are still\n",
      "limited.\n",
      "in addition, existing ddpm-based segmentation networks are generic\n",
      "and are not optimized for speciﬁc applications.\n",
      "in particular, a core question for\n",
      "dce-mri segmentation is how to optimally exploit hemodynamic priors.\n",
      "by designing a net-\n",
      "work architecture to eﬀectively transmute pre-contrast images into post-contrast\n",
      "images, the network should acquire hemodynamic inherent in hrf that can be\n",
      "used to improve segmentation performance.\n",
      "inspired by the fact that ddpm\n",
      "generates images from noisy input provided by the parameterized gaussian pro-\n",
      "cess, this work aims to exploit implicit hemodynamic information by a diﬀusion\n",
      "process that predict post-contrast images from noisy pre-contrast images.\n",
      "specif-\n",
      "ically, given the pre-contrast and post-contrast images, the latent kinetic code is\n",
      "learned using a score function of ddpm, which contains suﬃcient hemodynamic\n",
      "characteristics to facilitate segmentation performance.\n",
      "once the diﬀusion module is pretrained, the latent kinetic code can be easily\n",
      "generated with only pre-contrast images, which is fed into a segmentation module\n",
      "to annotate cancers.\n",
      "to verify the eﬀectiveness of the latent kinetic code, the\n",
      "sm adopts a simple u-net-like structure, with an encoder to simultaneously\n",
      "conduct semantic feature encoding and kinetic code fusion, along with a decoder\n",
      "to obtain voxel-level classiﬁcation.\n",
      "in this manner, our latent kinetic code can\n",
      "be interpreted to provide tic information and hemodynamic characteristics for\n",
      "accurate cancer segmentation.\n",
      "we verify the eﬀectiveness of our proposed diﬀusion kinetic model (dkm)\n",
      "on dce-mri-based breast cancer segmentation using breast-mri-nact-pilot\n",
      "dataset\n",
      "compared to the existing state-of-the-art approaches with complete\n",
      "sequences, our method yields higher segmentation performance even with pre-\n",
      "contrast images.\n",
      "in summary, the main contributions of this work are listed as\n",
      "follows:\n",
      "• we propose a diﬀusion kinetic model that implicitly exploits hemodynamic\n",
      "priors in dce-mri and eﬀectively generates high-quality segmentation maps\n",
      "only requiring pre-contrast images.\n",
      "• compared to the existing approaches with complete sequences, the proposed\n",
      "method yields higher cancer segmentation performance even with pre-contrast\n",
      "images.\n",
      "2. it can be observed that the devised model consists of a diﬀusion module\n",
      "(dm) and a segmentation module (sm).\n",
      "let {xk, k = 0, 1, ..., k} be a sequence\n",
      "of images representing the dce-mri protocol, in which x0 represents the pre-\n",
      "contrast image and xk represents the late post-contrast image.\n",
      "the dm takes\n",
      "a noisy pre-contrast image xt\n",
      "as input and generates post-contrast images to\n",
      "estimate the latent kinetic code.\n",
      "once the dm is trained, the learned kinetic code\n",
      "diﬀusion kinetic model for cancer segmentation\n",
      "103\n",
      "fig.\n",
      "2. illustration of our method for implicitly exploiting hemodynamic information\n",
      "from pre-contrast images.\n",
      "the combination of learned kinetic code is an example.\n",
      "is incorporated into the sm as hemodynamic priors to guide the segmentation\n",
      "process.\n",
      "= n(xt;\n",
      "\u0002\n",
      "1 − βtxt−1, βti)\n",
      "(1)\n",
      "particularly, a noisy image xt can be directly obtained from the data x0:\n",
      "q(xt|x0) := n(xt; √¯αtx0, (1 − ¯αt)i)\n",
      "(2)\n",
      "where αt := 1 − βt and ¯αt := \u0003t\n",
      "s=1 αs.\n",
      "104\n",
      "t. lv et al.\n",
      "inspired by the property of ddpm [8], we devise the diﬀusion module by\n",
      "considering the pre-contrast images x0 as source and regarding the post-contrast\n",
      "images xk as target.\n",
      "as thus, the\n",
      "dm gradually exploits the latent kinetic code by comparing the pre-contrast and\n",
      "post-contrast images, which contains hemodynamic knowledge for segmentation.\n",
      "2.2\n",
      "segmentation module\n",
      "once pretrained, the dm outputs multi-scale latent kinetic code fdm from inter-\n",
      "mediate layers, which is fed into the sm to guide cancer segmentation.\n",
      "in this way, the hemodynamic knowledge can be\n",
      "incorporated into the sm to capture more expressive representations to improve\n",
      "segmentation performance.\n",
      "in the ﬁrst step, the dm is trained to transform\n",
      "pre-contrast images into post-contrast images for a latent space where hemo-\n",
      "dynamic priors are exploited.\n",
      "in particular, the diﬀusion loss for the reverse\n",
      "diﬀusion process can be formulated as follows:\n",
      "ldm = et,ϵ,x||ϵθ(xt, t; x0, xk) − ϵ||2\n",
      "(6)\n",
      "where ϵθ represents the denoising model that employs an u-net structure, x0\n",
      "and xk are the pre-contrast and post-contrast images, respectively, ϵ is gaussian\n",
      "distribution data ∼ n(0, i), and t is a timestep.\n",
      "for a second step, we train the sm that integrates the previously learned\n",
      "latent kinetic code to provide tumor hemodynamic information for voxel-level\n",
      "diﬀusion kinetic model for cancer segmentation\n",
      "105\n",
      "prediction.\n",
      "considering the varying sizes, shapes and appearances of tumors\n",
      "that results from intratumor heterogeneity and results in diﬃculties of accurate\n",
      "cancer annotation, we design the segmentation loss as follows:\n",
      "(7)\n",
      "where lssim is used to evaluate tumor structural characteristics, s and g rep-\n",
      "resents segmentation map and ground truth, respectively; μs is the mean of s\n",
      "and μg is the mean of g; ϕs represents the variance of s and ϕg represents\n",
      "the variance of g; c1 and c2 denote the constant to hold training stable\n",
      "3\n",
      "experiments\n",
      "dataset: to demonstrate the eﬀectiveness of our proposed dkm, we evaluate\n",
      "our method on 4d dce-mri breast cancer segmentation using the breast-mri-\n",
      "nact-pilot dataset\n",
      "ground truth segmentations of the data are provided in the dataset for tumor\n",
      "annotation.\n",
      "no data augmentation techniques are used to ensure fairness.\n",
      "competing methods and evaluation metrics: to comprehensively evalu-\n",
      "ate the proposed method, we compare it with 3d segmentation methods, includ-\n",
      "ing dual attention net (danet)\n",
      "[19], and 4d segmentation methods, including lnet\n",
      "implementation details:\n",
      "we implement our proposed framework with\n",
      "pytorch using two nvidia rtx 2080ti gpus to accelerate model training.\n",
      "[8], we set 128, 256, 256, 256 channels for each stage in the dm\n",
      "and set the noise level from 10−4 to 10−2 using a linear schedule with t = 1000.\n",
      "once the dm is trained, we extract intermediate feature maps from four res-\n",
      "olutions for further segmentation task.\n",
      "table 1. cancer segmentation comparison between our method and previous models\n",
      "(mean ± std).\n",
      "[5]\n",
      "complete sequence 64.4 ± 2.4\n",
      "51.5 ± 2.6\n",
      "dkm (ours)\n",
      "pre-contrast\n",
      "71.5 ± 2.5 58.5 ± 2.6\n",
      "512, 1024 channels for each stage in the sm to capture expressive and suﬃ-\n",
      "cient semantic information.\n",
      "no data augmentation techniques are used to ensure fairness.\n",
      "imental results demonstrate that the proposed method comprehensively other\n",
      "models with less scans (i.e., pre-contrast) in testing.\n",
      "we attribute it to the abil-\n",
      "ity of diﬀusion module to exploit hemodynamic priors to guide the segmentation\n",
      "task.\n",
      "speciﬁcally, in comparison with 3d segmentation models (e.g. mtln),\n",
      "our method yields higher segmentation scores.\n",
      "besides, we can observe that our method\n",
      "achieves improvements when compared to 4d segmentation models using com-\n",
      "plete sequence.\n",
      "it probably due to two aspects: 1) the hemodynamic\n",
      "knowledge is implicitly exploited by diﬀusion module from pre-contrast images,\n",
      "which is useful for cancer segmentation.\n",
      "2) the intermediate activations from\n",
      "diﬀusion kinetic model for cancer segmentation\n",
      "107\n",
      "fig.\n",
      "4. visual comparison of segmentation performance.\n",
      "the baseline is implemented\n",
      "without the incorporation of kinetic code.\n",
      "ji (%) ↑\n",
      "✓\n",
      "67.9 ± 2.4\n",
      "54.4 ± 2.4\n",
      "✓\n",
      "68.6 ± 2.3\n",
      "55.0 ± 2.2\n",
      "✓\n",
      "68.3 ± 2.4\n",
      "54.8 ± 2.5\n",
      "✓\n",
      "69.3 ± 2.0\n",
      "55.5 ± 2.1\n",
      "✓\n",
      "✓\n",
      "67.6 ± 2.3\n",
      "53.2 ± 2.4\n",
      "✓\n",
      "✓\n",
      "70.1 ± 2.1\n",
      "56.2 ± 2.3\n",
      "✓\n",
      "✓\n",
      "71.5 ± 2.5 58.5 ± 2.6\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "70.2 ± 2.3\n",
      "56.4 ± 2.3\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "69.5 ± 2.1\n",
      "55.9 ± 2.4\n",
      "diﬀusion models eﬀectively capture the semantic information and are excellent\n",
      "pixel-level representations for the segmentation problem [9].\n",
      "thus, combining\n",
      "the intermediate features can further promote the segmentation performance.\n",
      "in a word, the proposed framework can produce accurate prediction masks only\n",
      "requiring pre-contrast images.\n",
      "we denote the intermedi-\n",
      "ate features extracted from each stage in the dm as f1, f2, f3, and f4, respec-\n",
      "tively, where fi represents the feature map of i-th stage.\n",
      "table 2 reports the\n",
      "segmentation performance with diﬀerent incorporations of intermediate kinetic\n",
      "codes.\n",
      "it can be observed that the latent kinetic code is able to guide the network\n",
      "training for better segmentation results.\n",
      "figure 4 shows visual comparison of segmen-\n",
      "tation performance.\n",
      "4\n",
      "conclusion\n",
      "we propose a diﬀusion kinetic model by exploiting hemodynamic priors in dce-\n",
      "mri to eﬀectively generate high-quality segmentation results only requiring pre-\n",
      "contrast images.\n",
      "our models learns the hemodynamic response function based on\n",
      "the denoising diﬀusion process and estimates the latent kinetic code to guide the\n",
      "segmentation task.\n",
      "experiments demonstrate that our proposed framework has\n",
      "the potential to be a promising tool in clinical applications to annotate cancers.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf:\n",
      "radiotherapy (rt) is a standard treatment modality for\n",
      "head and neck (han) cancer that requires accurate segmentation of tar-\n",
      "get volumes and nearby healthy organs-at-risk (oars) to optimize radi-\n",
      "ation dose distribution.\n",
      "however, computed tomography (ct) imaging\n",
      "has low image contrast for soft tissues, making accurate segmentation\n",
      "of soft tissue oars challenging.\n",
      "therefore, magnetic resonance (mr)\n",
      "imaging has been recommended to enhance the segmentation of soft tis-\n",
      "sue oars in the han region.\n",
      "based on our two empirical observations\n",
      "that deformable registration of ct and mr images of the same patient\n",
      "is inherently imperfect and that concatenating such images at the input\n",
      "layer of a deep learning network cannot optimally exploit the information\n",
      "provided by the mr modality, we propose a novel modality fusion mod-\n",
      "ule (mfm) that learns to spatially align mr-based feature maps before\n",
      "fusing them with ct-based feature maps.\n",
      "the proposed mfm can be\n",
      "easily implemented into any existing multimodal backbone network.\n",
      "our\n",
      "implementation within the nnu-net framework shows promising results\n",
      "on a dataset of ct and mr image pairs from the same patients.\n",
      "keywords: multimodal segmentation · head and neck ·\n",
      "organs-at-risk · computed tomography · magnetic resonance ·\n",
      "nnu-net\n",
      "1\n",
      "introduction\n",
      "head and neck (han) cancer is a prevalent type of cancer [3] with a yearly inci-\n",
      "dence of above 1 million cases and prevalence of above 4 million cases worldwide,\n",
      "accounting for around 5% of all cancer sites [17]. radiotherapy (rt) is a standard\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43901-8 71.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43901-8_71\n",
      "746\n",
      "g. podobnik et al.\n",
      "treatment modality for han cancer, which aims to deliver high doses of radiation\n",
      "to cancerous cells while sparing nearby healthy organs-at-risk (oars)\n",
      "[21]. to\n",
      "optimize radiation dose distribution, accurate three-dimensional (3d) segmen-\n",
      "tation of target volumes and oars is required.\n",
      "computed tomography (ct) is\n",
      "the primary imaging modality used for rt planning due to its ability to pro-\n",
      "vide information about electron density, however, its low image contrast for soft\n",
      "tissues, including tumors, makes accurate segmentation of soft tissue oars chal-\n",
      "lenging.\n",
      "therefore, the integration of complementary imaging modalities, such as\n",
      "magnetic resonance (mr), has been strongly recommended in clinical practice to\n",
      "enhance the segmentation of several soft tissue oars in the han region\n",
      "this\n",
      "naturally poses a question of whether automatic oar segmentation can beneﬁt\n",
      "from the mr image modality.\n",
      "our study therefore aims to evaluate the impact of\n",
      "mr integration on the quality and robustness of automatic oar segmentation\n",
      "in the han region, therefore contributing to the growing body of research on\n",
      "multimodal methods for medical image analysis.\n",
      "[24] divides deep learning\n",
      "(dl)-based multimodal segmentation methods into three fusion strategy groups:\n",
      "early, late and hybrid (also named layer) fusion.\n",
      "the ﬁrst two groups of meth-\n",
      "ods are most commonly applied; early fusion comprises simple concatenation\n",
      "of modalities along the channel dimension before feeding them into the deep\n",
      "neural network.\n",
      "[23]\n",
      "proposed an attention mechanism to fuse fms from two separate u-nets that\n",
      "accepted contrast-enhanced arterial and venous phase ct images.\n",
      "the third\n",
      "group, hybrid fusion, aims to combine the strengths of early and late fusion [24]\n",
      "by employing two or more separate encoders (i.e. one for each modality) and a\n",
      "single decoder, where features from diﬀerent resolution levels of the encoder are\n",
      "fused and fed into the decoder that produces the ﬁnal full-resolution segmenta-\n",
      "tion.\n",
      "similar conclusions were reached in a review of multimodal seg-\n",
      "mentation methods in the medical imaging community by zhou et al.\n",
      "most\n",
      "methods implement either early or late fusion, however, the layer fusion strat-\n",
      "egy was identiﬁed as a better choice, since dense connections among layers can\n",
      "exploit more complex and complementary information to enhance training.\n",
      "[4]\n",
      "that employs dense connections between two convolutional paths, and achieves\n",
      "multimodal ct and mr segmentation of han oars\n",
      "747\n",
      "improvements compared to other fusion strategies and single modality variants.\n",
      "relevant to the ﬁeld of multimodal segmentation are\n",
      "also developments on unpaired multimodal segmentation, where cross-modality\n",
      "learning is employed to take advantage of diﬀerent image modalities covering\n",
      "the same anatomy, but without the constraint to collect images from the same\n",
      "patients [5,10,19].\n",
      "although the methodologies comprising cyclegans and/or\n",
      "multiple segmentation networks [10,19] seem promising, they can be excessively\n",
      "complex for the task of han oar segmentation where both ct and mr image\n",
      "modalities from the same patient are often available.\n",
      "consequently, our primary\n",
      "focus is the paired multimodal segmentation problem, including the missing\n",
      "modality scenario.\n",
      "motivation.\n",
      "when segmenting oars in the han region for the purpose of rt\n",
      "planning, a multimodal segmentation model that can leverage the information\n",
      "from ct and mr images of the same patient might be beneﬁcial compared to\n",
      "separate single-modal models.\n",
      "firstly, as intuition suggests, such a model would\n",
      "rely on the ct image for bone structures and on the mr image for soft tissues,\n",
      "and therefore improve the overall segmentation quality by exploiting the comple-\n",
      "mentary information from both modalities.\n",
      "secondly, a multimodal model would\n",
      "facilitate cross-modality learning by extracting knowledge from one and applying\n",
      "that knowledge to the other modality, potentially improving the segmentation\n",
      "accuracy.\n",
      "several studies indicated that such an approach is feasible, for exam-\n",
      "ple, for improving video classiﬁcation by training a model on an auxiliary audio\n",
      "reconstruction task [12], or for audio-based detection by using the multimodal\n",
      "knowledge distillation concept, where teacher networks trained on rgb, depth\n",
      "and thermal images improve a student network trained only on audio data [20].\n",
      "finally, from the dl infrastructure maintenance perspective, it is easier to main-\n",
      "tain a single model that can handle both modalities than two separate models\n",
      "for each modality.\n",
      "firstly,\n",
      "although mr image acquisition is recommended, it is not always feasible due to\n",
      "time constraints, scanner occupancy and ﬁnancial aspects.\n",
      "consequently, auto-\n",
      "matic oar multimodal segmentation is required to handle the missing modality\n",
      "scenario, and provide a similar segmentation quality as a single-modality system.\n",
      "secondly, because ct and mr images are not acquired simultaneously and with\n",
      "the same acquisition parameters (e.g. resolution), there is an inherent misalign-\n",
      "ment between both modalities.\n",
      "this can be mitigated with image registration,\n",
      "but not completely, mainly due to diﬀerent patient positioning that especially\n",
      "aﬀects the deformation of soft tissues, and various modality-speciﬁc artifacts\n",
      "(e.g. motion, implants, partial volume eﬀect, etc.).\n",
      "to tackle these considerations, we propose a mechanism named\n",
      "modality fusion module (mfm) that can generally be applied to any network\n",
      "architecture that learns features from multiple modalities, and shows promising\n",
      "performance also in the missing modality scenario.\n",
      "the advantages of the pro-\n",
      "posed mfm are the following: 1) it enables the spatial alignment of fms from\n",
      "one with fms from the other modality to further reduce errors that persist after\n",
      "deformable registration of input images, and enrich the fms to improve the ﬁnal\n",
      "oar segmentation, 2) it signiﬁcantly improves the performance of the missing\n",
      "modality scenario compared to other baseline fusion approaches, and 3) it per-\n",
      "forms well also on single modality out-of-distribution data, therefore facilitating\n",
      "cross-modality learning and contributing to better model generalizability.\n",
      "our chosen backbone network is based on nnu-net,\n",
      "a publicly available framework for dl-based segmentation [8] that builds on\n",
      "the u-net architecture [16], adds self-conﬁgurable pre-processing, augmentation\n",
      "and post-processing, and employs eﬃcient training strategies.\n",
      "however, nnu-net,\n",
      "which uses an early fusion strategy by concatenating input images or patches\n",
      "before feeding them to the ﬁrst network layer, may not be the optimal strategy\n",
      "for multimodal segmentation.\n",
      "this is particularly problematic when fusing\n",
      "ct and mr images, which diﬀer in several aspects, such as the type and loca-\n",
      "tion of artifacts, acquisition parameters, and visibility of soft tissues and bone\n",
      "structures.\n",
      "while mr images can help to improve the delineations of oars that\n",
      "are poorly visible in ct images, the primary delineation is always performed\n",
      "on ct images with the help of registered mr images.\n",
      "an important repercus-\n",
      "sion is that image registration errors propagate into oar delineations, which\n",
      "is particularly salient in the han region.\n",
      "[9], who introduced a spatial transformer network (stn)\n",
      "that learns to infer transformation parameters in a single forward pass, and then\n",
      "uses them to transform images and/or fms.\n",
      "the fundamental idea is that stn\n",
      "can learn meaningful features that are spatially invariant to characteristics of\n",
      "the input data, without the need for extra supervision, thereby enhancing task\n",
      "multimodal ct and mr segmentation of han oars\n",
      "749\n",
      "fig.\n",
      "the proposed backbone architecture is based on nnu-net but with separate\n",
      "encoders for the computed tomography (ct) and magnetic resonance (mr) image,\n",
      "and with the proposed modality fusion module.\n",
      "performance.\n",
      "while it was demonstrated that complete spatial invariance can-\n",
      "not be achieved with stns [6], the work of jaderberg et al. is crucial in showing\n",
      "that stns can be implemented as diﬀerentiable modules, enabling the loss to be\n",
      "propagated through the sampling (interpolation) mechanism.\n",
      "the same under-\n",
      "lying principle of stns has also been leveraged in optical ﬂow and its derivative\n",
      "work semantic ﬂow, where the ﬂow alignment module was proposed to resample\n",
      "low-resolution fms and align them with high-resolution fms\n",
      "we capital-\n",
      "ize on the same principle to register fms from mr images to those from ct\n",
      "images.\n",
      "notably, mfm is diﬀerent from semantic ﬂow, as it takes two fms of\n",
      "the same resolution but from diﬀerent modalities, and aligns fms from the aux-\n",
      "iliary modality to fms of the primary modality.\n",
      "the localization network is a regressor network that accepts concate-\n",
      "nated fms from both encoders and applies four blocks of strided convolutions\n",
      "followed by the relu activation to reduce their spatial dimensions.\n",
      "both\n",
      "the grid generator and the sampler and readily implemented in the pytorch\n",
      "library [9], and because they are both diﬀerentiable, no special optimization is\n",
      "needed for the localization network, allowing localization parameters to be opti-\n",
      "mized with the main (segmentation) loss function.\n",
      "the purpose of this architecture is to align fms from both\n",
      "modalities and improve their fusion, leading to better segmentation results.\n",
      "we evaluate the performance of the proposed mfm\n",
      "nnu-net against three baseline networks: 1) a single modality nnu-net trained\n",
      "only on ct images, 2) a nnu-net trained on concatenated ct and mr image\n",
      "pairs, and 3) a model with separate encoders for both modalities, but with a\n",
      "simple concatenation along the channel axis instead of the proposed mfm.\n",
      "[23].\n",
      "3\n",
      "experiments and results\n",
      "image datasets.\n",
      "the han-seg dataset comprises ct and t1-weighted mr images of\n",
      "56 patients, which were deformably registered with the simpleelastix registra-\n",
      "tion tool, and corresponding curated manual delineations of 30 oars (for details,\n",
      "please refer to [14]).\n",
      "although only a subset of images is publicly available1 due\n",
      "to the ongoing han-seg challenge2, both the publicly available training as well\n",
      "as the privately withheld test images were used in our 4-fold cross-validation\n",
      "experiments.\n",
      "on the other hand, to evaluate the generalization ability of our\n",
      "method, we also conducted experiments on the ct-only pddca dataset (for\n",
      "details, please refer to [15]), from which we collected 15 images from the oﬀ-\n",
      "and on-site test sets of the corresponding challenge for our evaluation.\n",
      "as this\n",
      "dataset is widely used for evaluating the performance of automatic han oar\n",
      "segmentation methods, it serves as a valuable benchmark for comparison with\n",
      "other state-of-the-art methods.\n",
      "note that none of the images from the ct-only\n",
      "pddca dataset were used for training, and as our model expects two inputs,\n",
      "we substituted the missing mr modality with an empty matrix (i.e. zeros).\n",
      "implementation details.\n",
      "all models were trained for all oars using the\n",
      "3d fullres conﬁguration of nnu-net, with the only modiﬁcation that we\n",
      "reduced rotation around the axial axis and disabled image ﬂipping along\n",
      "the sagittal plane, which eliminated segmentation errors that were previously\n",
      "observed for the paired (left and right) oars.\n",
      "multimodal ct and mr segmentation of han oars\n",
      "751\n",
      "fig.\n",
      "to address the challenge of a rela-\n",
      "tively small dataset, we adopted a 4-fold cross-validation strategy without using\n",
      "any external training images.\n",
      "the quality of the obtained oar segmentation masks was evaluated\n",
      "by computing the dice similarity coeﬃcient (dsc) and the 95th-percentile haus-\n",
      "dorﬀ distance (hd95) against reference manual delineations, and the results for\n",
      "all oars are presented in figs.\n",
      "since not all images con-\n",
      "tain all 30 oars (due to a diﬀerent ﬁeld-of-view), we ﬁrst calculated the mean\n",
      "metric for each oar and then the overall mean across all oars to ensure that\n",
      "752\n",
      "g. podobnik et al.\n",
      "the contributions were equally weighted.\n",
      "4\n",
      "discussion\n",
      "in this study, we evaluated the impact on the quality and robustness of auto-\n",
      "matic oar segmentation in the han region caused by the incorporation of the\n",
      "mr modality into the segmentation framework.\n",
      "we devised a mechanism named\n",
      "mfm and combined it with nnu-net as our backbone segmentation network.\n",
      "segmentation results.\n",
      "however,\n",
      "dsc has been identiﬁed not to be the most appropriate metric for evaluating the\n",
      "clinical adequacy of segmentations, especially when the results are close to the\n",
      "interrater variability\n",
      "on the other hand, distance-based metrics, such as hd95 (fig. 3),\n",
      "are preferred as they better measure the shape consistency between the refer-\n",
      "ence and predicted segmentations.\n",
      "although maml achieved the best results\n",
      "in terms of hd95, indicating that late fusion can eﬃciently merge the informa-\n",
      "tion from both modalities, it should be noted that maml has a considerate\n",
      "advantage due to having two decoders and an additional attention fusion block\n",
      "compared to the baseline nnu-net with separate encoders and a single decoder.\n",
      "an\n",
      "approximate 10% improvement in hd95 suggests that mfm allows the network\n",
      "to learn more informative fms that lead to a better overall performance.\n",
      "the overall good performance on the han-seg\n",
      "dataset suggests that all models are close to the maximal performance, which is\n",
      "bounded by the quality of reference segmentations.\n",
      "however, the performance\n",
      "on the pddca dataset that consists only of ct images allows us to test how\n",
      "the models handle the missing modality scenario and perform on an out-of-\n",
      "distribution dataset, as images from this dataset were not used for training.\n",
      "as\n",
      "multimodal ct and mr segmentation of han oars\n",
      "753\n",
      "expected, the ct-only model performed best in its regular operating scenario,\n",
      "with a mean dsc of 74.7% (fig. 2) and hd95 of 6.02 mm (fig. 3).\n",
      "it should be noted that we did not\n",
      "employ any training strategies to improve handling of missing modalities, such\n",
      "as swapping input images or intensity augmentations.\n",
      "a possible explanation is\n",
      "that the proposed mfm facilitates cross-modality learning, enabling nnu-net\n",
      "to extract better fms from ct images even in such extreme scenarios.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf:\n",
      "recently, deep learning methods have been widely used\n",
      "for tumor segmentation of multimodal medical images with promising\n",
      "results.\n",
      "in this paper, we propose a hybrid densely connected net-\n",
      "work for tumor segmentation, named h-denseformer, which combines\n",
      "the representational power of the convolutional neural network (cnn)\n",
      "and the transformer structures.\n",
      "we con-\n",
      "duct extensive experiments on two public multimodal datasets, heck-\n",
      "tor21 and pi-cai22.\n",
      "the experimental results show that our proposed\n",
      "method outperforms the existing state-of-the-art methods while having\n",
      "lower computational complexity.\n",
      "keywords: tumor segmentation · multimodal medical image ·\n",
      "transformer · deep learning\n",
      "1\n",
      "introduction\n",
      "accurate tumor segmentation from medical images is essential for quantita-\n",
      "tive assessment of cancer progression and preoperative treatment planning [3].\n",
      "this study was supported by the fundamental\n",
      "research funds for the central universities (no. yd2150002001).\n",
      "in clinical practice, multimodal registered images, such as\n",
      "pet-ct images and magnetic resonance (mr) images with diﬀerent sequences,\n",
      "are often utilized to delineate tumors to improve accuracy.\n",
      "however, manual\n",
      "delineation is time-consuming and error-prone, with a low inter-professional\n",
      "agreement [12].\n",
      "these have prompted the demand for intelligent applications\n",
      "that can automatically segment tumors from multimodal images to optimize\n",
      "clinical procedures.\n",
      "recently, multimodal tumor segmentation has attracted the interest of many\n",
      "researchers.\n",
      "with the emergence of multimodal datasets (e.g., brats [25] and\n",
      "hecktor [1]), various deep-learning-based multimodal image segmentation\n",
      "methods have been proposed [3,10,13,27,29,31]. overall, large eﬀorts have been\n",
      "made on eﬀectively fusing image features of diﬀerent modalities to improve seg-\n",
      "mentation accuracy.\n",
      "as a typical approach, input-level fusion\n",
      "[8,20,26,31,34] refers to concatenating multimodal images in the channel dimen-\n",
      "sion as network input during the data processing or augmentation stage.\n",
      "however, the shallow fusion entangles the low-level fea-\n",
      "tures from diﬀerent modalities, preventing the eﬀective extraction of high-level\n",
      "semantics and resulting in limited performance gains.\n",
      "the core idea is to train an\n",
      "independent segmentation network for each data modality and fuse the results\n",
      "in a speciﬁc way.\n",
      "in addition to the progress on the fusion of multimodal features, improving\n",
      "the model representation ability is also an eﬀective way to boost segmentation\n",
      "performance.\n",
      "in the past few years, transformer structure [11,24,30], centered on\n",
      "the multi-head attention mechanism, has been introduced to multimodal image\n",
      "segmentation tasks.\n",
      "extensive studies [2,4,14,16] have shown that the trans-\n",
      "former can eﬀectively model global context to enhance semantic representations\n",
      "and facilitate pixel-level prediction.\n",
      "[31] proposed transbts, a form\n",
      "of input-level fusion with a u-like structure, to segment brain tumors from mul-\n",
      "timodal mr images.\n",
      "[29] adopted a similar structure in which the\n",
      "transformer serves as the encoder rather than a wrapper, also achieving promis-\n",
      "ing performance.\n",
      "[33], which combine the transformer\n",
      "with the multimodal feature fusion approaches mentioned above, further demon-\n",
      "strate the potential of this idea for multimodal tumor segmentation.\n",
      "694\n",
      "j. shi et al.\n",
      "although remarkable performance has been accomplished with these eﬀorts,\n",
      "there still exist several challenges to be resolved.\n",
      "to this end, we propose an eﬃcient multimodal tumor segmentation solu-\n",
      "tion named hybrid densely connected network (h-denseformer).\n",
      "first, our\n",
      "method leverages transformer to enhance the global contextual information\n",
      "of diﬀerent modalities.\n",
      "second, h-denseformer integrates a transformer-based\n",
      "multi-path parallel embedding (mpe) module, which can extract and fuse mul-\n",
      "timodal image features as a complement to naive input-level fusion structure.\n",
      "speciﬁcally, mpe assigns an independent encoding path to each modality, then\n",
      "merges the semantic features of all paths and feeds them to the encoder of the\n",
      "segmentation network.\n",
      "finally, we design a lightweight, densely connected transformer (dct)\n",
      "module to replace the standard transformer to ensure performance and com-\n",
      "putational eﬃciency.\n",
      "extensive experimental results on two publicly available\n",
      "datasets demonstrate the eﬀectiveness of our proposed method.\n",
      "h-denseformer com-\n",
      "prises a multi-path parallel embedding (mpe) module and a u-shaped seg-\n",
      "mentation backbone network in form of input-level fusion.\n",
      "speciﬁcally, given a multimodal image input\n",
      "x3d ∈ rc×h×w ×d or x2d ∈ rc×h×w with a spatial resolution of h ×w, the\n",
      "depth dimension of d (number of slices) and c channels (number of modalities),\n",
      "we ﬁrst utilize mpe to extract and fuse multimodal image features.\n",
      "then, the\n",
      "obtained features are progressively upsampled and delivered to the encoder of\n",
      "the segmentation network to enhance the semantic representation.\n",
      "finally, the\n",
      "segmentation network generates multi-scale outputs, which are used to calculate\n",
      "deep supervision loss as the optimization target.\n",
      "2.2\n",
      "multi-path parallel embedding\n",
      "many methods [5,10,15] have proved that decoupling the feature representa-\n",
      "tion of diﬀerent modalities facilitates the extraction of high-quality multimodal\n",
      "features.\n",
      "then, interpolation upsampling is performed to obtain\n",
      "the multimodal fusion feature fout ∈ rk× h\n",
      "8 × w\n",
      "8 × d\n",
      "8 , where k = 128 refers to the\n",
      "channel dimension.\n",
      "finally, fout is progressively upsampled to multiple scales\n",
      "and delivered to diﬀerent encoder stages to enhance the learned representation.\n",
      "[11] typically consist of dense linear layers with\n",
      "a computational complexity proportional to the feature dimension.\n",
      "therefore,\n",
      "integrating the transformer could lead to a mass of additional computation\n",
      "and memory requirements.\n",
      "each transformer layer has a lin-\n",
      "ear projection layer that reduces the input feature dimension to g = 32 to save\n",
      "computation.\n",
      "diﬀerent transformer layers are connected densely to preserve rep-\n",
      "resentational power with lower feature dimensions.\n",
      "feature dimension resolution transformer\n",
      "stacked dct (×3)\n",
      "gflops ↓ params ↓ gflops ↓ params ↓\n",
      "256\n",
      "(512,512)\n",
      "6.837\n",
      "6.382m\n",
      "2.671\n",
      "1.435m\n",
      "512\n",
      "(512,512)\n",
      "26.256\n",
      "25.347m\n",
      "3.544\n",
      "2.290m\n",
      "2.4\n",
      "segmentation backbone network\n",
      "the h-denseformer adopts a u-shaped encoder-decoder structure as its back-\n",
      "bone.\n",
      "the multi-level multimodal features from mpe are fused in a bit-\n",
      "wise addition way to enrich the semantic information.\n",
      "during training, the decoder has four outputs; for\n",
      "example, the i-th output of 2d h-denseformer is oi ∈ rc× h\n",
      "2i × w\n",
      "2i , where\n",
      "i ∈ [0, 1, 2, 3], and c = 2 (tumor and background) represents the number of\n",
      "segmentation classes.\n",
      "this approach can improve the convergence speed\n",
      "and performance of the network.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "dataset and metrics\n",
      "to validate the eﬀectiveness of our proposed method, we performed extensive\n",
      "experiments on hecktor21\n",
      "[1] and pi-cai221. hecktor21 is a dual-\n",
      "modality dataset for head and neck tumor segmentation, containing 224 pet-\n",
      "ct image pairs.\n",
      "pi-cai22 provides multimodal mr images of 220 patients\n",
      "with prostate cancer, including t2-weighted imaging (t2w), high b-value\n",
      "diﬀusion-weighted imaging (dwi), and apparent diﬀusion coeﬃcient (adc)\n",
      "maps.\n",
      "after standard resampling and center cropping, all images have a size of\n",
      "(24,384,384).\n",
      "for quantitative analysis, we use the dice\n",
      "similarity coeﬃcient (dsc), the jaccard index (ji), and the 95% hausdorﬀ\n",
      "distance (hd95) as evaluation metrics for segmentation performance.\n",
      "a better\n",
      "segmentation will have a smaller hd95 and larger values for dsc and ji.\n",
      "we\n",
      "also conduct holistic t-tests of the overall performance for our method and all\n",
      "baseline models with the two-tailed p < 0.05.\n",
      "3.2\n",
      "implementation details\n",
      "we use pytorch to implement our proposed method and the baselines.\n",
      "for a\n",
      "fair comparison, all models are trained from scratch using two nvidia a100\n",
      "gpus and all comparison methods are implemented with open-source codes,\n",
      "following their original conﬁgurations.\n",
      "online data augmentation, including random rotation and ﬂipping, is performed\n",
      "to alleviate the overﬁtting problem.\n",
      "1 https://pi-cai.grand-challenge.org/.\n",
      "698\n",
      "j. shi et al.\n",
      "3.3\n",
      "overall performance\n",
      "table 2. comparison with existing methods on independent test set.\n",
      "we show the\n",
      "mean ± std (standard deviation) scores of averaged over the 5 folds.\n",
      "methods (year)\n",
      "params↓ gflops↓ dsc(%) ↑\n",
      "hd95(mm) ↓ ji(%) ↑\n",
      "hecktor21, two modalities (ct and pet)\n",
      "3d u-net (2016)\n",
      "[4]\n",
      "93.23m\n",
      "72.62\n",
      "44.8 ± 3.0\n",
      "59.3 ± 14.8\n",
      "33.2 ± 2.5\n",
      "2d h-denseformer\n",
      "4.25m\n",
      "31.46\n",
      "49.9 ± 1.2 35.9 ± 8.2\n",
      "37.1 ± 1.2\n",
      "table 2 compares the performance and computational complexity of our pro-\n",
      "posed method with the existing state-of-the-art methods on the independent\n",
      "test sets.\n",
      "for hecktor21, 3d h-denseformer achieves a dsc of 73.9%, hd95\n",
      "of 8.1mm, and ji of 62.5%, which is a signiﬁcant improvement (p < 0.01) over\n",
      "3d u-net [7], unetr [16], and transbts [31].\n",
      "it is worth noting that the per-\n",
      "formance of hybrid models such as unetr is not as good as expected, even\n",
      "worse than 3d u-net, perhaps due to the small size of the dataset.\n",
      "overall, h-denseformer\n",
      "reaches an eﬀective balance of performance and computational cost compared to\n",
      "existing cnns and hybrid structures.\n",
      "2 that our approach\n",
      "can describe tumor contours more accurately while providing better segmenta-\n",
      "tion accuracy for small-volume targets.\n",
      "these results further demonstrate the\n",
      "eﬀectiveness of our proposed method in multimodal tumor segmentation tasks.\n",
      "as illustrated in table 3, the network performance\n",
      "varies with the change in dct depth.\n",
      "h-denseformer achieves the best perfor-\n",
      "mance at the dct depth of 6.\n",
      "ji (%) ↑\n",
      "3\n",
      "3.25m\n",
      "242.38\n",
      "73.5 ± 1.4\n",
      "8.4 ± 0.7\n",
      "62.2 ± 1.6\n",
      "6\n",
      "3.64m\n",
      "242.96\n",
      "73.9 ± 0.5 8.1 ± 0.6\n",
      "62.5 ± 0.5\n",
      "9\n",
      "4.03m\n",
      "243.55\n",
      "72.7 ± 1.2\n",
      "8.7 ± 0.6\n",
      "61.2 ± 1.3\n",
      "of the dct has increased from 3 to 9, the performance does not improve or even\n",
      "worsen.\n",
      "the above results demonstrate the superiority\n",
      "of our method, but it is unclear which module plays a more critical role in perfor-\n",
      "mance improvement.\n",
      "therefore, we perform ablation experiments on mpe, dct\n",
      "and ds loss.\n",
      "as shown in table 4, the performance\n",
      "decreases with varying degrees when removing them separately, which means all\n",
      "the modules are critical for h-denseformer.\n",
      "we can observe that dct has a\n",
      "greater impact on overall performance than the others, further demonstrating its\n",
      "eﬀectiveness.\n",
      "hd95 (mm) ↓ ji (%) ↑\n",
      "3d h-denseformer w/o mpe\n",
      "72.1 ± 0.8\n",
      "10.8 ± 1.1\n",
      "60.4 ± 0.8\n",
      "3d h-denseformer w/o dct\n",
      "70.7 ± 1.8\n",
      "11.9 ± 1.9\n",
      "58.6 ± 2.1\n",
      "3d h-denseformer w/o ds loss 72.2 ± 0.9\n",
      "10.2 ± 1.0\n",
      "60.1 ± 1.2\n",
      "3d h-denseformer\n",
      "73.9 ± 0.5 8.1 ± 0.6\n",
      "62.5 ± 0.5\n",
      "700\n",
      "j. shi et al.\n",
      "ﬁrms that decoupling the feature expression of diﬀerent modalities helps obtain\n",
      "higher-quality multimodal features and improve segmentation performance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf:\n",
      "despite its clinical utility, medical image segmentation (mis)\n",
      "remains a daunting task due to images’ inherent complexity and variabil-\n",
      "ity.\n",
      "to overcome this obstacle, data-eﬃcient\n",
      "vits were proposed, but they are typically trained using a single source\n",
      "of data, which overlooks the valuable knowledge that could be leveraged\n",
      "from other available datasets.\n",
      "na¨ıvly combining datasets from diﬀerent\n",
      "domains can result in negative knowledge transfer (nkt), i.e., a decrease\n",
      "in model performance on some domains with non-negligible inter-domain\n",
      "heterogeneity.\n",
      "experiments on 4 skin lesion seg-\n",
      "mentation datasets show that mdvit outperforms state-of-the-art algo-\n",
      "rithms, with superior segmentation performance and a ﬁxed model size, at\n",
      "inference time, even as more domains are added.\n",
      "keywords: vision transformer · data-eﬃciency · multi-domain\n",
      "learning · medical image segmentation · dermatology\n",
      "1\n",
      "introduction\n",
      "medical image segmentation (mis) is a crucial component in medical image\n",
      "analysis, which aims to partition an image into distinct regions (or segments)\n",
      "that are semantically related and/or visually similar.\n",
      "this process is essential\n",
      "for clinicians to, among others, perform qualitative and quantitative assessments\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43901-8 43.\n",
      "https://doi.org/10.1007/978-3-031-43901-8_43\n",
      "mdvit\n",
      "449\n",
      "of various anatomical structures or pathological conditions and perform image-\n",
      "guided treatments or treatment planning\n",
      "they process images as sequences\n",
      "of patches, with each patch having a global view of the entire image.\n",
      "this enables\n",
      "a vit to achieve improved segmentation performance compared to traditional\n",
      "convolutional neural networks (cnns) on plenty of segmentation tasks [16].\n",
      "how-\n",
      "ever, due to the lack of inductive biases, such as weight sharing and locality, vits\n",
      "are more data-hungry than cnns, i.e., require more data to train [31].\n",
      "mean-\n",
      "while, it is common to have access to multiple, diverse, yet small-sized datasets\n",
      "(100 s to 1000 ss of images per dataset) for the same mis task, e.g., ph2\n",
      "as each dataset alone is too small\n",
      "to properly train a vit, the challenge becomes how to eﬀectively leverage the\n",
      "diﬀerent datasets.\n",
      "method\n",
      "vit mitigate vits’ data-hunger\n",
      "u f\n",
      "[7,22,39] √\n",
      "√ by adding inductive bias\n",
      "× –\n",
      "[32]\n",
      "√\n",
      "×\n",
      "√ ×\n",
      "mdvit\n",
      "√\n",
      "√ by multi-domain learning\n",
      "√ √\n",
      "various strategies have been proposed to address vits’ data-hunger\n",
      "(table 1), mainly: adding inductive bias by constructing a hybrid network that\n",
      "fuses a cnn with a vit\n",
      "sharing knowl-\n",
      "edge by transferring knowledge from a cnn [31] or pertaining vits on multiple\n",
      "related tasks and then ﬁne-tuning on a down-stream task [37]; increasing data via\n",
      "augmentation [34]; and non-supervised pre-training [8].\n",
      "multi-domain learning, which trains\n",
      "a single universal model to tackle all the datasets simultaneously, has been found\n",
      "450\n",
      "s. du et al.\n",
      "promising for reducing computational demands while still leveraging information\n",
      "from multiple domains\n",
      "given the inter-domain heterogeneity resulting from variations in imaging\n",
      "protocols, scanner manufacturers, etc.\n",
      "[4,21], directly mixing all the datasets\n",
      "for training, i.e., joint training, may improve a model’s performance on one\n",
      "dataset while degrading performance on other datasets with non-negligible unre-\n",
      "lated domain-speciﬁc information, a phenomenon referred to as negative knowl-\n",
      "edge transfer (nkt)\n",
      "to address vits’ data-hunger, in this work, we propose mdvit, a novel ﬁxed-\n",
      "size multi-domain vit trained to adaptively aggregate valuable knowledge from\n",
      "multiple datasets (domains) for improved segmentation.\n",
      "(2) we propose a multi-domain vit,\n",
      "mdvit, for medical image segmentation with a novel domain adapter to coun-\n",
      "teract negative knowledge transfer and with mutual knowledge distillation to\n",
      "enhance representation learning.\n",
      "(3) the experiments on 4 skin lesion segmenta-\n",
      "tion datasets show that our multi-domain adaptive training outperforms separate\n",
      "and joint training (st and jt), especially a 10.16% improvement in iou on the\n",
      "skin cancer detection dataset compared to st and that mdvit outperforms\n",
      "state-of-the-art data-eﬃcient vits and multi-domain learning strategies.\n",
      "2\n",
      "methodology\n",
      "let x ∈ rh×w ×3 be an input rgb image and y ∈ {0, 1}h×w be its ground-\n",
      "truth segmentation mask.\n",
      "i ≤ 4 is the number\n",
      "of patches and ci is the channel dimension.\n",
      "the h parallel heads of mhsa mimic\n",
      "how humans examine the same object from diﬀerent perspectives [10].\n",
      "rather than manually designate each\n",
      "head to one of the domains, guided by a domain label, mdvit learns to focus on\n",
      "the corresponding features from diﬀerent heads when encountering a domain.\n",
      "k is the channel dimension of features from\n",
      "the heads.\n",
      "[uh\n",
      "1, uh\n",
      "2, ..., uh\n",
      "k] ∈ rn×k from the hth head, we utilize\n",
      "ah to calibrate the information along the channel dimension: ˜uh\n",
      "k = ah\n",
      "k · uh\n",
      "k.\n",
      "mutual knowledge distillation (mkd): distilling knowledge from domain-\n",
      "speciﬁc networks has been found beneﬁcial for universal networks to learn more\n",
      "robust representations [21,40].\n",
      "1-a) go through an mlp layer and an up-sample oper-\n",
      "ation to unify the channel dimension and resolution to h\n",
      "4 × w\n",
      "4 , which are then\n",
      "concatenated with the feature involving domain-shared information from the\n",
      "mdvit\n",
      "453\n",
      "table 2. segmentation results comparing base, mdvit, and sota methods.\n",
      "(millions) (m) t\n",
      "segmentation results in test sets (%)\n",
      "dice ↑\n",
      "iou ↑\n",
      "isic\n",
      "dmf\n",
      "scd\n",
      "ph2\n",
      "avg ± std\n",
      "isic\n",
      "dmf\n",
      "scd\n",
      "ph2\n",
      "avg ± std\n",
      "(a) base\n",
      "base\n",
      "27.8×\n",
      "st\n",
      "90.18\n",
      "90.68\n",
      "86.82\n",
      "93.41\n",
      "90.27 ± 1.16\n",
      "82.82\n",
      "83.22\n",
      "77.64\n",
      "87.84\n",
      "82.88 ± 1.67\n",
      "base\n",
      "27.8\n",
      "jt\n",
      "89.42\n",
      "89.89\n",
      "92.96\n",
      "94.24\n",
      "91.63 ± 0.42\n",
      "81.68\n",
      "82.07\n",
      "87.03\n",
      "89.36\n",
      "85.04 ± 0.64\n",
      "(b) our method\n",
      "mdvit\n",
      "28.5\n",
      "mat 90.29\n",
      "90.78\n",
      "93.22 95.53 92.45 ± 0.65 82.99\n",
      "83.41\n",
      "87.80 91.57 86.44 ± 0.94\n",
      "(c) other data-eﬃcient mis vits\n",
      "swinunet\n",
      "41.4×\n",
      "st\n",
      "89.25\n",
      "90.69\n",
      "88.58\n",
      "94.13\n",
      "90.66 ± 0.87\n",
      "81.51\n",
      "83.25\n",
      "80.40\n",
      "89.00\n",
      "83.54 ± 1.27\n",
      "swinunet\n",
      "41.4\n",
      "jt\n",
      "89.64\n",
      "90.40\n",
      "92.98\n",
      "94.86\n",
      "91.97 ± 0.30\n",
      "81.98\n",
      "82.80\n",
      "87.08\n",
      "90.33\n",
      "85.55 ± 0.50\n",
      "utnet\n",
      "10.0×\n",
      "st\n",
      "89.74\n",
      "90.01\n",
      "88.13\n",
      "93.23\n",
      "90.28 ± 0.62\n",
      "82.16\n",
      "82.13\n",
      "79.87\n",
      "87.60\n",
      "82.94 ± 0.82\n",
      "utnet\n",
      "10.0\n",
      "jt\n",
      "90.24\n",
      "89.85\n",
      "92.06\n",
      "94.75\n",
      "91.72 ± 0.63\n",
      "82.92\n",
      "82.00\n",
      "85.66\n",
      "90.17\n",
      "85.19 ± 0.96\n",
      "bat\n",
      "32.2×\n",
      "st\n",
      "90.45 90.56\n",
      "90.78\n",
      "94.72\n",
      "91.63 ± 0.68\n",
      "83.04\n",
      "82.97\n",
      "83.66\n",
      "90.03\n",
      "84.92 ± 1.01\n",
      "bat\n",
      "32.2\n",
      "jt\n",
      "90.06\n",
      "90.06\n",
      "92.66\n",
      "93.53\n",
      "91.58 ± 0.33\n",
      "82.44\n",
      "82.18\n",
      "86.48\n",
      "88.11\n",
      "84.80 ± 0.53\n",
      "transfuse\n",
      "26.3×\n",
      "st\n",
      "90.43\n",
      "91.04 91.37\n",
      "94.93\n",
      "91.94 ± 0.67\n",
      "83.18 83.86 84.91\n",
      "90.44\n",
      "85.60 ± 0.95\n",
      "transfuse\n",
      "26.3\n",
      "jt\n",
      "90.03\n",
      "90.48\n",
      "92.54\n",
      "95.14\n",
      "92.05 ± 0.36\n",
      "82.56\n",
      "82.97\n",
      "86.50\n",
      "90.85\n",
      "85.72 ± 0.56\n",
      "swin unetr 25.1×\n",
      "st\n",
      "90.29\n",
      "90.95\n",
      "91.10\n",
      "94.45\n",
      "91.70 ± 0.51\n",
      "82.93\n",
      "83.69\n",
      "84.16\n",
      "89.59\n",
      "85.09 ± 0.79\n",
      "swin unetr 25.1\n",
      "jt\n",
      "89.81\n",
      "90.87\n",
      "92.29\n",
      "94.73\n",
      "91.93 ± 0.29\n",
      "82.21\n",
      "83.58\n",
      "86.10\n",
      "90.11\n",
      "85.50 ± 0.44\n",
      "(d) other multi-domain learning methods\n",
      "rundo et al.\n",
      "28.2\n",
      "mat 89.43\n",
      "89.46\n",
      "92.62\n",
      "94.68\n",
      "91.55 ± 0.64\n",
      "81.73\n",
      "81.40\n",
      "86.71\n",
      "90.12\n",
      "84.99 ± 0.90\n",
      "wang et al.\n",
      "28.1\n",
      "mat 89.46\n",
      "89.62\n",
      "92.62\n",
      "94.47\n",
      "91.55 ± 0.54\n",
      "81.79\n",
      "81.59\n",
      "86.71\n",
      "89.76\n",
      "84.96 ± 0.74\n",
      "base†\n",
      "27.8(.02×)\n",
      "mat 90.22\n",
      "90.61\n",
      "93.69 95.55\n",
      "92.52 ± 0.45\n",
      "82.91\n",
      "83.14\n",
      "88.28 91.58\n",
      "86.48 ± 0.74\n",
      "mdvit†\n",
      "28.6(.02×)\n",
      "mat 90.24 90.71 93.38\n",
      "95.90 92.56 ± 0.52 82.97 83.31 88.06\n",
      "92.19 86.64 ± 0.76\n",
      "universal network’s last transformer block.\n",
      "finally, we pass the fused feature to\n",
      "an mlp layer and do an up-sample to obtain a segmentation map.\n",
      "2.2\n",
      "objective function\n",
      "similar to combo loss [29], base’s segmentation loss combines dice and binary\n",
      "cross entropy loss: lseg = ldice+lbce.\n",
      "in mdvit, we use the same segmentation\n",
      "loss for the universal network and auxiliary peers, denoted as lu\n",
      "seg and la\n",
      "seg,\n",
      "respectively.\n",
      "after training, we discard the auxiliary peers\n",
      "and only utilize the universal network for inference.\n",
      "3\n",
      "experiments\n",
      "datasets and evaluation metrics: we study 4 skin lesion segmentation data-\n",
      "sets collected from varied sources: isic 2018 (isic)\n",
      "[11], dermoﬁt image library\n",
      "(dmf)\n",
      "the green and red contours present\n",
      "the ground truth and segmentation results, respectively.\n",
      "to facilitate a fairer performance\n",
      "comparison across datasets, as in [4], we only use the 1212 images from dmf\n",
      "that exhibited similar lesion conditions as those in other datasets.\n",
      "implementation details: we conduct 3 training paradigms: separate (st),\n",
      "joint (jt), and multi-domain adaptive training (mat), described in sect.\n",
      "images are resized\n",
      "to 256 × 256 and then augmented through random scaling, shifting, rotation,\n",
      "ﬂipping, gaussian noise, and brightness and contrast changes.\n",
      "the encoding\n",
      "transformer blocks’ channel dimensions are [64, 128, 320, 512] (fig.\n",
      "the hidden dimensions of the cnn bridge and auxiliary peers\n",
      "are 1024 and 512.\n",
      "we deploy models on a single titan v gpu and train them\n",
      "for 200 epochs with the adamw [23] optimizer, a batch size of 16, ensuring 4\n",
      "samples from each dataset, and an initial learning rate of 1×10−4, which changes\n",
      "through a linear decay scheduler whose step size is 50 and decay factor γ = 0.5.\n",
      "comparing against base: in table 2-a,b, compared with base in st,\n",
      "base in jt improves the segmentation performance on small datasets (ph2\n",
      "and scd)\n",
      "but at the expense of diminished performance on larger datasets (isic\n",
      "and dmf).\n",
      "the above results\n",
      "demonstrate that shared knowledge in related domains facilitates training a vit\n",
      "on small datasets while, without a well-designed multi-domain algorithm, caus-\n",
      "ing negative knowledge transfer (nkt) due to inter-domain heterogeneity, i.e.,\n",
      "the model’s performance decreases on other datasets.\n",
      "additionally, mdvit outperforms base in jt across all the domains,\n",
      "with average improvements of 0.82% on dice and 1.4% on iou.\n",
      "mdvit\n",
      "455\n",
      "table 3. ablation studies of mdvit and experiments of da’s plug-in capability.\n",
      ": we conduct\n",
      "experiments on sota data-eﬃcient mis vits and multi-domain learning meth-\n",
      "ods.\n",
      "previous mis vits mitigated the data-hunger in one dataset by adding\n",
      "inductive bias, e.g., swinunet\n",
      "we implement resnet-34 as the backbone of bat for fair\n",
      "comparison (similar model size).\n",
      "this is expected since they are designed\n",
      "to reduce data requirements.\n",
      "finally, mdvit achieves the best segmentation\n",
      "performance in average dice and iou without nkt and has the best results\n",
      "on scd and ph2.\n",
      "figure 2 shows mdvit’s excellent performance on isic and\n",
      "dmf and that it achieves the closest results to ground truth on scd and ph2.\n",
      "more segmentation results are presented in the supplementary material.\n",
      "though\n",
      "bat and transfuse in st have better results on some datasets like isic, they\n",
      "require extra compute resources to train m models as well as an m-fold increase\n",
      "in memory requirements.\n",
      "in table 2-d, base† confronts nkt, which\n",
      "lowers the performance on dmf compared with base in st, whereas mdvit†\n",
      "not only addresses nkt but also outperforms base† on average dice and iou.\n",
      "456\n",
      "s. du et al.\n",
      "ablation studies and plug-in capability of da: we conduct ablation\n",
      "studies to demonstrate the eﬃcacy of da, mkd, and auxiliary peers.\n",
      "as shown\n",
      "in table 3-a,b, da can be used in various vits but is more advantageous in\n",
      "mdvit with more transformer blocks in the encoding and decoding process.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf:\n",
      "transformers for medical image segmentation have attracted broad\n",
      "interest.\n",
      "unlike convolutional networks (cnns), transformers use self-attentions\n",
      "that do not have a strong inductive bias.\n",
      "although they,\n",
      "e.g. swinunetr, achieve state-of-the-art (sota) results on some benchmarks,\n",
      "the lack of inductive bias makes transformers harder to train, requires much\n",
      "more training data, and are sensitive to training recipes.\n",
      "in many clinical scenar-\n",
      "ios and challenges, transformers can still have inferior performances than sota\n",
      "cnns like nnunet.\n",
      "a transformer backbone and corresponding training recipe,\n",
      "which can achieve top performances under different medical image segmenta-\n",
      "tion scenarios, still needs to be developed.\n",
      "in this paper, we enhance the swi-\n",
      "nunetr with convolutions, which results in a surprisingly stronger backbone,\n",
      "the swinunetr-v2, for 3d medical image segmentation.\n",
      "it achieves top per-\n",
      "formance on a variety of benchmarks of different sizes and modalities, including\n",
      "the whole abdominal organ dataset (word), miccai flare2021 dataset,\n",
      "msd pancreas dataset, msd prostate dataset, and msd lung cancer dataset,\n",
      "all using the same training recipe (https://github.com/project-monai/research-\n",
      "contributions/tree/main/swinunetr/btcv, our training recipe is the same as\n",
      "that by swinunetr) with minimum changes across tasks.\n",
      "keywords: swin transformer · convolution · hybrid model · medical image\n",
      "segmentation\n",
      "1\n",
      "introduction\n",
      "medical image segmentation is a core step for quantitative and precision medicine.\n",
      "in\n",
      "the past decade, convolutional neural networks (cnns) became the sota method to\n",
      "achieve accurate and fast medical image segmentation\n",
      "[21], has achieved top performances on over 20 medical segmenta-\n",
      "tion challenges.\n",
      "parallel to manually created networks such as nn-unet, dints [10],\n",
      "a cnn designed by automated neural network search, also achieved top performances\n",
      "in medical segmentation decathlon (msd)\n",
      "the convolution operation\n",
      "in cnn provides a strong inductive bias which is translational equivalent and efﬁ-\n",
      "cient in capturing local features like boundary and texture.\n",
      "however, this inductive\n",
      "bias limits the representation power of cnn models which means a potentially lower\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43901-8_40\n",
      "swinunetr-v2: stronger swin backbone\n",
      "417\n",
      "performance ceiling on more challenging tasks [7].\n",
      "recently, vision transformers have been proposed, which adopt the transformers\n",
      "in natural language processing by splitting images into patches (tokens)\n",
      "this is intriguing and numerous works\n",
      "have been proposed to incorporate transformer attentions into medical image segmen-\n",
      "tation\n",
      "[23] has achieved the new\n",
      "top performance in the msd challenge and beyond the cranial vault (btcv) segmen-\n",
      "tation challenge by pretraining on large datasets.\n",
      "although transformers have achieved certain success in medical imaging, the lack\n",
      "of inductive bias makes them harder to be trained and requires much more training\n",
      "data to avoid overﬁtting.\n",
      "unlike natural image segmentation benchmarks, e.g. ade20k\n",
      "[34],\n",
      "where the challenge is in learning complex relationships and scene understanding from\n",
      "a large amount of labeled training images, many medical image segmentation networks\n",
      "need to be extremely focused on local boundary details while less in need of high-\n",
      "level relationships.\n",
      "besides lacking\n",
      "inductive bias and enough training data, one extra reason could be that transformers are\n",
      "computationally much expensive and harder to tune.\n",
      "more improvements and empirical\n",
      "evidence are needed before we say transformers are ready to replace cnns for medical\n",
      "image segmentation.\n",
      "in this paper, we try to develop a new “to-go” transformer for 3d medical image\n",
      "segmentation, which is expected to exhibit strong performance under different data sit-\n",
      "uations and does not require extensive hyperparameter tuning.\n",
      "swinunetr reaches\n",
      "top performances on several large benchmarks, making itself the current sota, but\n",
      "without effective pretraining and excessive tuning, its performance on new datasets and\n",
      "challenges is not as high-performing as expected.\n",
      "many methods have been proposed and most\n",
      "of them fall into two directions: 1) a new self-attention scheme to have convolution-\n",
      "like properties [5,7,16,25,26,29].\n",
      "it uses a local window instead of the whole image to perform self-attention.\n",
      "[7] uses\n",
      "gated positional self-attention which is equipped with a soft convolutional inductive\n",
      "bias.\n",
      "our swinunetr-v2 adds a convolu-\n",
      "tion block at the beginning of each resolution stage.\n",
      "although those works showed strong performances, which works best and can\n",
      "be the “to go” transformer for 3d medical image segmentation is still unknown.\n",
      "for\n",
      "this purpose, we design the swinunetr-v2, which improves the current sota swi-\n",
      "nunetr by introducing stage-wise convolutions into the backbone.\n",
      "differently, our work only adds a resconv block at the beginning of each stage, which\n",
      "is a macro-network level design.\n",
      "although simple, we found it surprisingly effective for 3d medical image\n",
      "segmentation.\n",
      "the network is evaluated extensively on a variety of benchmarks and\n",
      "achieved top performances on the word [17], flare2021\n",
      "we also experimented with four design varia-\n",
      "tions inspired by existing works to justify the swinunetr-v2 design.\n",
      "four stages of swin transformer block\n",
      "followed by patch merging are used to encode the input patches.\n",
      "a patch merging layer is applied after every swin transformer block to\n",
      "reduce each spatial dimension by half.\n",
      "420\n",
      "y. he et al.\n",
      "stage-wise convolution.\n",
      "although swin-transformer uses local window attention to\n",
      "introduce inductive bias like convolutions, self-attentions can still mess up with the\n",
      "local details.\n",
      "we experimented with multiple designs as in fig.\n",
      "and found that inter-\n",
      "leaved stage-wise convolution is the most effective for swin: convolution followed by\n",
      "swin blocks, then convolution goes on.\n",
      "at the beginning of each resolution level (stage),\n",
      "the input tokens are reshaped back to the original 3d volumes.\n",
      "there are in total 4 resconv blocks at 4 stages.\n",
      "we also tried\n",
      "inverted convolution blocks with depth-wise convolution like moat [31] or with orig-\n",
      "inal 3d convolution, they improve the performance but are worse than the resconv\n",
      "block.\n",
      "decoder.\n",
      "a ﬁnal convolution with 1×1×1 kernel\n",
      "is used to map features to segmentation maps.\n",
      "3\n",
      "experiments\n",
      "we use extensive experiments to show its effectiveness and justify its design for 3d\n",
      "medical image segmentation.\n",
      "we use this split for our experiments.\n",
      "2) the miccai flare 2021 dataset [18].\n",
      "it provides 361 training scans with man-\n",
      "ual labels from 11 medical centers.\n",
      "each scan is an abdominal 3d ct image with 4\n",
      "organ annotations.\n",
      "the medi-\n",
      "cal segmentation decathlon (msd)\n",
      "the lung tumor dataset contains 63 lung ct images with tumor annotations.\n",
      "the challenge comes from segmenting small tumors from large full 3d ct images.\n",
      "implementation details\n",
      "the training pipeline is based on the publicly available swinunetr codebase\n",
      "(https://github.com/project-monai/research-contributions/tree/main/swinunetr/bt\n",
      "cv, our training recipe is the same as that by swinunetr).\n",
      "random gaussian smooth, gaussian noise, and ran-\n",
      "dom gamma correction are also added as additional data augmentation.\n",
      "msd data are resampled to 1 × 1x1 mm\n",
      "resolution and normalized to zero mean and standard deviation (ct images are ﬁrstly\n",
      "clipped by .5% and 99.5% foreground intensity percentile).\n",
      "swin-var-bot changes the top 2 stages of the encoder with resconv\n",
      "blocks and only keeps transformer blocks in the higher stages.\n",
      "to make a fair com-\n",
      "parison, we didn’t use any test-time augmentation or model ensemble.\n",
      "flare 2021 5-fold cross-validation average test dice scores (on held-out test scans)\n",
      "and standard deviation in brackets.\n",
      "baseline results from 3d ux-net paper [14].\n",
      "3d u-net segresnet rap-net nn-unet transbts unetr nnformer swinunetr 3d ux-net swinunetr-v2\n",
      "spleen\n",
      "0.911\n",
      "0.963\n",
      "0.946\n",
      "0.971\n",
      "0.964\n",
      "0.927\n",
      "0.973\n",
      "0.979\n",
      "0.981\n",
      "0.980 (0.018)\n",
      "kidney\n",
      "0.962\n",
      "0.934\n",
      "0.967\n",
      "0.966\n",
      "0.959\n",
      "0.947\n",
      "0.960\n",
      "0.965\n",
      "0.969\n",
      "0.973 (0.013)\n",
      "liver\n",
      "0.905\n",
      "0.965\n",
      "0.940\n",
      "0.976\n",
      "0.974\n",
      "0.960\n",
      "0.975\n",
      "0.980\n",
      "0.982\n",
      "0.983 (0.008)\n",
      "pancreas 0.789\n",
      "0.745\n",
      "0.799\n",
      "0.792\n",
      "0.711\n",
      "0.710\n",
      "0.717\n",
      "0.788\n",
      "0.801\n",
      "0.851 (0.037)\n",
      "mean\n",
      "0.892\n",
      "0.902\n",
      "0.913\n",
      "0.926\n",
      "0.902\n",
      "0.886\n",
      "0.906\n",
      "0.929\n",
      "0.934\n",
      "0.947 (0.019)\n",
      "swinunetr-v2: stronger swin backbone\n",
      "423\n",
      "table 4. msd prostate, lung, and pancreas 5-fold cross-validation average test dice scores and\n",
      "standard deviation in brackets.\n",
      "following [14], the ﬁve trained models are evaluated on 20 held-out test\n",
      "scans, and the average dice scores (not model ensemble) are shown in table 3.\n",
      "for msd datasets, we perform 5-fold cross-validation and ran the base-\n",
      "line experiments with our codebase using exactly the same hyperparameters as men-\n",
      "tioned.\n",
      "nnunet2d/3d baseline experiments are performed using nnunet’s original code-\n",
      "base2 since it has its own automatic hyperparameter selection.\n",
      "the test dice score and\n",
      "standard deviation (averaged over 5 fold) are shown in table 4.\n",
      "we didn’t compare with leaderboard results because\n",
      "the purpose of the experiments is to make fair comparisons, while not resorting to addi-\n",
      "tional training data/pretraining, postprocessing, or model ensembling.\n",
      "variations of swinunetr-v2 in this section, we investigate other variations of adding\n",
      "convolutions into swin transformer.\n",
      "as for the (2.a) of parallel\n",
      "branches, it increases the gpu memory usage for 3d medical image too much and\n",
      "2 https://github.com/mic-dkfz/nnunet.\n",
      "424\n",
      "y. he et al.\n",
      "3, we investigate 1) swin-var-\n",
      "bot (2.b scheme): replacing the top 2 stages of swin transformer with resconv block,\n",
      "and keeping the bottom two stages using swin blocks.\n",
      "3) swin-\n",
      "var-res (2.c scheme): instead of only adding resconv blocks at the beginning of each\n",
      "stage, we create a new swin transformer block which all starts with this resconv block,\n",
      "like the moat [31] work.\n",
      "we can see that adding\n",
      "convolution at different places does affect the performances, and the swinunetr-v2\n",
      "design is the optimal on word test set.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_54.pdf:\n",
      "breast\n",
      "dynamic\n",
      "contrast-enhanced\n",
      "magnetic\n",
      "resonance\n",
      "imaging (dce-mri) plays an important role in the screening and prog-\n",
      "nosis assessment of high-risk breast cancer.\n",
      "the segmentation of can-\n",
      "cerous regions is essential useful for the subsequent analysis of breast\n",
      "mri.\n",
      "to alleviate the annotation eﬀort to train the segmentation net-\n",
      "works, we propose a weakly-supervised strategy using extreme points as\n",
      "annotations for breast cancer segmentation.\n",
      "the network ﬁrst utilizes the pseudo-masks generated using the extreme\n",
      "points to train itself, by minimizing a contrastive loss, which encourages\n",
      "the network to learn more representative features for cancerous voxels.\n",
      "then the trained network ﬁne-tunes itself by using a similarity-aware\n",
      "propagation learning (simple) strategy, which leverages feature similar-\n",
      "ity between unlabeled and positive voxels to propagate labels.\n",
      "experimental results demonstrate our method eﬀectively\n",
      "ﬁne-tunes the network by using the simple strategy, and achieves a\n",
      "mean dice value of 81%.\n",
      "keywords: breast cancer · weakly-supervised learning · medical\n",
      "image segmentation · contrastive learning · dce-mri\n",
      "1\n",
      "introduction\n",
      "breast cancer is the most common cause of cancer-related deaths among women\n",
      "all around the world [8].\n",
      "early diagnosis and treatment is beneﬁcial to improve\n",
      "the survival rate and prognosis of breast cancer patients.\n",
      "1. breast mri and diﬀerent annotations: (a) t1-weighted images, (b) correspond-\n",
      "ing contrast-enhanced images, (c) the cancer annotation with full segmentation masks,\n",
      "and (d) the cancer annotation using extreme points (note that to facilitate the visual-\n",
      "ization, here we show the extreme points in 2d images, our method is based on 3d).\n",
      "t1-weighted acquisition depicts enhancing\n",
      "abnormalities after contrast material administration, that is, the cancer screen-\n",
      "ing is performed by using the post-contrast images.\n",
      "radiologists will analyze\n",
      "features such as texture, morphology, and then make the treatment plan or\n",
      "prognosis assessment.\n",
      "computer-aided feature quantiﬁcation and diagnosis algo-\n",
      "rithms have recently been exploited to facilitate radiologists analyze breast dce-\n",
      "mri [12,22], in which automatic cancer segmentation is the very ﬁrst and impor-\n",
      "tant step.\n",
      "to better support the radiologists with breast cancer diagnosis, various seg-\n",
      "mentation algorithms have been developed [20].\n",
      "early studies focused on image\n",
      "processing based approaches by conducting graph-cut segmentation [29] or ana-\n",
      "lyzing low-level hand-crafted features [1,11,19].\n",
      "these methods may encounter\n",
      "the issue of high computational complexity when analyzing volumetric data,\n",
      "and most of them require manual interactions.\n",
      "[28] proposed a\n",
      "mask-guided hierarchical learning framework for breast tumor segmentation via\n",
      "convolutional neural networks (cnns), in which breast masks were also required\n",
      "to train one of cnns.\n",
      "[30]\n",
      "employed a 3d aﬃnity learning based multi-branch ensemble network for the\n",
      "simple for weakly-supervised segmentation\n",
      "569\n",
      "segmentation reﬁnement and generated 78% dice on 90 testing subjects.\n",
      "[25] proposed\n",
      "a tumor-sensitive synthesis module to reduce false segmentation and obtained\n",
      "78% dice value.\n",
      "to reduce the huge annotation burden for the segmentation task,\n",
      "zeng et al.\n",
      "[27] presented a semi-supervised strategy to segment the manually\n",
      "cropped dce-mri scans, and attained a dice value of 78%.\n",
      "although [27] has been proposed to alleviate the annotation eﬀort, to acquire\n",
      "the voxel-level segmentation masks is still time-consuming and laborious, see\n",
      "fig. 1(c).\n",
      "[21]\n",
      "utilized extreme points to generate scribbles to supervise the training of the\n",
      "segmentation network.\n",
      "[5] introduced a regular-\n",
      "ized loss [4] derived from a conditional random field (crf) formulation to\n",
      "encourage the prediction consistency over homogeneous regions.\n",
      "[6]\n",
      "employed bounding boxes to train the segmentation network for organs.\n",
      "how-\n",
      "ever, the geometric prior used in [6] can not be an appropriate strategy for the\n",
      "segmentation of lesions with various shapes.\n",
      "to our knowledge, currently only\n",
      "one weakly-supervised work [18] has been proposed for breast mass segmentation\n",
      "in dce-mri.\n",
      "this method employed three partial annotation methods including\n",
      "single-slice, orthogonal-slice (i.e., 3 slices) and interval-slice (∼6 slices) to allevi-\n",
      "ate the annotation cost, and then constrained segmentation by estimated volume\n",
      "using the partial annotation.\n",
      "in this study, we propose a simple yet eﬀective weakly-supervised strategy,\n",
      "by using extreme points as annotations (see fig. 1(d)) to segment breast cancer.\n",
      "speciﬁcally, we attempt to optimize the segmentation network via the conven-\n",
      "tional train - ﬁne-tune - retrain process.\n",
      "experimental\n",
      "results show our method achieves competitive performance compared with fully\n",
      "supervision, demonstrating the eﬃcacy of the proposed simple strategy.\n",
      "the segmentation network is ﬁrstly trained based on\n",
      "the initial pseudo-masks.\n",
      "2. the schematic illustration of the proposed similarity-aware propagation learn-\n",
      "ing (simple) and the train - ﬁne-tune - retrain procedure for the breast cancer seg-\n",
      "mentation in dce-mri.\n",
      "2.1\n",
      "generate initial pseudo-masks\n",
      "we use the extreme points to generate pseudo-masks based on random walker\n",
      "algorithm\n",
      "[9]. to improve the performance of random walker, according to [21],\n",
      "we ﬁrst generate scribbles by searching the shortest path on gradient magnitude\n",
      "map between each extreme point pair via the dijkstra algorithm\n",
      "a simple training approach is to minimize the partial\n",
      "cross entropy loss lpce, which is formulated as:\n",
      "simple for weakly-supervised segmentation\n",
      "571\n",
      "lpce = −\n",
      "\u0003\n",
      "yinit(k)=0\n",
      "log(1 − f(x; θ)(k))\n",
      "(1)\n",
      "moreover, supervised contrastive learning is employed to encourage voxels\n",
      "of the same label to gather around in feature space.\n",
      "(3)\n",
      "2.3\n",
      "simple-based fine-tune and retrain\n",
      "the performance of the network trained by the incomplete initial pseudo-masks\n",
      "is still limited.\n",
      "then the network is ﬁne-tuned using the partial cross\n",
      "entropy loss same as in the initial train stage.\n",
      "finally the network is retrained from random initial-\n",
      "ization by minimizing the cross entropy loss with the binary pseudo-masks.\n",
      "3\n",
      "experiments\n",
      "dataset.\n",
      "the dce-mri sequences\n",
      "(tr/te = 4.43 ms/1.50 ms, and ﬂip angle = 10◦) using gadolinium-based con-\n",
      "trast agent were performed with the t1-weighted gradient echo technique,\n",
      "and injected 0.2 ml/kg intravenously at 2.0 ml/s followed by 20 ml saline.\n",
      "all cancerous regions and extreme points were manually annotated by an\n",
      "experienced radiologist via itk-snap [26] and further conﬁrmed by another\n",
      "radiologist.\n",
      "implementation details.\n",
      "the framework was implemented in pytorch, using\n",
      "a nvidia geforce gtx 1080 ti with 11gb of memory.\n",
      "the batch size was 2,\n",
      "consisting of a random foreground patch and a random background patch\n",
      "located via initial segmentation yinit.\n",
      "for the simple strategy, we set n = 100, λ = 0.96, α = 0.96, w = 0.1.\n",
      "1 we have tried diﬀerent amount of training data to investigate the segmentation\n",
      "performance of the fully-supervised network.\n",
      "simple for weakly-supervised segmentation\n",
      "573\n",
      "fig.\n",
      "the pseudo-masks (shown as boundaries) at diﬀerent training stages, including\n",
      "the initial pseudo-mask generated by the random walker (purple), the trained network’s\n",
      "output (green), the ﬁne-tuned pseudo-mask using simple (blue) and the ground-truth\n",
      "(red).\n",
      "note that all images here are the training images.\n",
      "the segmentation visualization in transversal slices.\n",
      "the blue and red contours\n",
      "are the segmented boundaries and the ground-truths, respectively.\n",
      "• retrain: the training strategy was the same as the initial train stage.\n",
      "we ﬁrst veriﬁed the eﬃcacy of our\n",
      "simple in the training stage.\n",
      "figure 3 illustrates the pseudo-masks at diﬀerent\n",
      "training stages.\n",
      "therefore, such fune-tuned\n",
      "pseudo-masks could be used to retrain the network for better performance.\n",
      "5. three cases of 3d visualization of the surface distance between segmented\n",
      "surface and ground-truth.\n",
      "the proposed simple consistently enhances the segmentation.\n",
      "the numerical results of diﬀerent methods for breast cancer segmentation\n",
      "methods\n",
      "dice [%]\n",
      "jaccard\n",
      "lctr+ simple\n",
      "81.20 ± 13.28 70.01 ± 15.02 0.69 ± 0.44 2.40 ± 1.69\n",
      "fully supervision\n",
      "81.52 ± 19.40 72.10 ± 20.45 0.68 ± 0.63 2.40 ± 2.76\n",
      "figure 4 visualizes our cancer segmentation results on the testing data.\n",
      "table 1 reports the quantitative dice, jaccard, average surface distance (asd),\n",
      "and hausdorﬀ distance (95hd) results of diﬀerent methods.\n",
      "in contrast, the proposed\n",
      "simple largely boosted the performance of the basically trained networks, by\n",
      "+14.74% dice and +15.16% jaccard (v.s. lcrf), +11.81% dice and +12.65%\n",
      "jaccard (v.s. lctr).\n",
      "note that the average anno-\n",
      "tation time for extreme points and full masks were 31 s and 95 s per scan, respec-\n",
      "tively.\n",
      "figure 5 visualizes the 3d distance map between the segmented surface\n",
      "and ground-truth.\n",
      "it can be observed that our simple consistently enhanced\n",
      "the segmentation.\n",
      "simple for weakly-supervised segmentation\n",
      "575\n",
      "4\n",
      "conclusion\n",
      "we introduce a simple yet eﬀective weakly-supervised learning method for breast\n",
      "cancer segmentation in dce-mri.\n",
      "the primary attribute is to fully exploit the\n",
      "simple train - ﬁne-tune - retrain process to optimize the segmentation network\n",
      "via only extreme point annotations.\n",
      "experimental results demonstrate the eﬃcacy of the proposed simple strat-\n",
      "egy for weakly-supervised segmentation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_4.pdf:\n",
      "classiﬁcation and segmentation are crucial in medical image\n",
      "analysis as they enable accurate diagnosis and disease monitoring.\n",
      "how-\n",
      "ever, current methods often prioritize the mutual learning features and\n",
      "shared model parameters, while neglecting the reliability of features and\n",
      "performances.\n",
      "in this paper, we propose a novel uncertainty-informed\n",
      "mutual learning (uml) framework for reliable and interpretable medi-\n",
      "cal image analysis.\n",
      "our uml introduces reliability to joint classiﬁcation\n",
      "and segmentation tasks, leveraging mutual learning with uncertainty to\n",
      "improve performance.\n",
      "to achieve this, we ﬁrst use evidential deep learn-\n",
      "ing to provide image-level and pixel-wise conﬁdences.\n",
      "then, an uncer-\n",
      "tainty navigator is constructed for better using mutual features and gen-\n",
      "erating segmentation results.\n",
      "overall, uml could\n",
      "produce conﬁdence estimation in features and performance for each link\n",
      "(classiﬁcation and segmentation).\n",
      "the experiments on the public datasets\n",
      "demonstrate that our uml outperforms existing methods in terms of\n",
      "both accuracy and robustness.\n",
      "our uml has the potential to explore\n",
      "the development of more reliable and explainable medical image analysis\n",
      "models.\n",
      "keywords: mutual learning · medical image classiﬁcation and\n",
      "segmentation · uncertainty estimation\n",
      "k. ren and k. zou—denotes equal contribution.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43901-8 4.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43901-8_4\n",
      "36\n",
      "k. ren et al.\n",
      "1\n",
      "introduction\n",
      "accurate and robust classiﬁcation and segmentation of the medical image are\n",
      "powerful tools to inform diagnostic schemes.\n",
      "in clinical practice, the image-level\n",
      "classiﬁcation and pixel-wise segmentation tasks are not independent [8,27].\n",
      "joint\n",
      "classiﬁcation and segmentation can not only provide clinicians with results for\n",
      "both tasks simultaneously, but also extract valuable information and improve\n",
      "performance.\n",
      "however, improving the reliability and interpretability of medical\n",
      "image analysis is still reaching.\n",
      "considering the close correlation between the classiﬁcation and segmenta-\n",
      "tion, many researchers [6,8,20,22,24,27,28] proposed to collaboratively analyze\n",
      "the two tasks with the help of sharing model parameters or task interacting.\n",
      "most of the methods are based on sharing model parameters, which improves\n",
      "the performance by fully utilizing the supervision from multiple tasks [8,27].\n",
      "[20] combined whole image classiﬁcation and segmenta-\n",
      "tion of skin cancer using a shared encoder.\n",
      "however, there has been relatively\n",
      "little research on introducing reliability into joint classiﬁcation and segmenta-\n",
      "tion.\n",
      "the reliability and interpretability of the model are particularly important\n",
      "for clinical tasks, a single result of the most likely hypothesis without any clues\n",
      "about how to make the decision might lead to misdiagnoses and sub-optimal\n",
      "treatment [10,22].\n",
      "one potential way of improving reliability is to introduce\n",
      "uncertainty for the medical image analysis model.\n",
      "all of these methods are widely utilized in\n",
      "classiﬁcation and segmentation applications for medical image analysis.\n",
      "[1] employed three uncertainty quantiﬁcation methods (monte carlo dropout,\n",
      "ensemble mc dropout, and deep ensemble) simultaneously to deal with uncer-\n",
      "tainty estimation during skin cancer image classiﬁcation.\n",
      "[31] pro-\n",
      "posed tbrats based on evidential deep learning to generate robust segmentation\n",
      "results for brain tumor and reliable uncertainty estimations.\n",
      "unlike the afore-\n",
      "mentioned methods, which only focus on uncertainty in either medical image\n",
      "classiﬁcation or segmentation.\n",
      "furthermore, none of the existing methods have\n",
      "considered how pixel-wise and image-level uncertainty can help improve perfor-\n",
      "mance and reliability in mutual learning.\n",
      "based on the analysis presented above, we design a novel uncertainty-\n",
      "informed mutual learning (uml) network for medical image analysis in this\n",
      "study.\n",
      "our uml not only enhances the image-level and pixel-wise reliability of\n",
      "medical image classiﬁcation and segmentation, but also leverages mutual learning\n",
      "under uncertainty to improve performance.\n",
      "[16,31] to simultaneously estimate the uncertainty of both to estimate\n",
      "image-level and pixel-wise uncertainty.\n",
      "we introduce an uncertainty navigator\n",
      "for segmentation (un) to generate preliminary segmentation results, taking into\n",
      "account the uncertainty of mutual learning features.\n",
      "we also propose an uncer-\n",
      "uncertainty-informed mutual learning\n",
      "37\n",
      "             uncertainty navigator (un)\n",
      "             uncertainty instructor (ui)\n",
      "lcls\n",
      "c\n",
      "c\n",
      "label\n",
      "cls \n",
      "feature\n",
      "encoder\n",
      "un\n",
      "feature\n",
      "mixer\n",
      "seg\n",
      "feature\n",
      "encoder\n",
      "f\n",
      "c\n",
      "cls \n",
      "evidence\n",
      "lseg\n",
      "u s\n",
      "m\n",
      "gt\n",
      "s\n",
      "(a)\n",
      "(b)\n",
      "mutual\n",
      "feature\n",
      "decoder\n",
      "0\n",
      "1\n",
      "ui\n",
      "gt\n",
      "lmut\n",
      "u c\n",
      "b1\n",
      "b0\n",
      "mutual\n",
      "evidence\n",
      "mutual\n",
      "dirichlet\n",
      "cls \n",
      "dirichlet \n",
      "flow of \n",
      "feature\n",
      "softplus\n",
      "global average \n",
      "pooling\n",
      "flow of \n",
      "uncertainty\n",
      "c\n",
      "concat\n",
      "deep \n",
      "supervision\n",
      "conv\n",
      "reliable\n",
      "mask\n",
      "fig.\n",
      "tainty instructor for classiﬁcation (ui) to screen reliable masks for classiﬁcation\n",
      "based on the preliminary segmentation results.\n",
      "our uml represents pioneering\n",
      "work in introducing reliability and interpretability to joint classiﬁcation and seg-\n",
      "mentation, which has the potential to the development of more trusted medical\n",
      "analysis tools1.\n",
      "2\n",
      "method\n",
      "the overall architecture of the proposed uml, which leverages mutual learn-\n",
      "ing under uncertainty, is illustrated in fig.\n",
      "firstly, uncertainty estimation\n",
      "for classiﬁcation and segmentation adapts evidential deep learning to provide\n",
      "image-level and pixel-wise uncertainty.\n",
      "then, trusted mutual learning not only\n",
      "utilizes the proposed un to fully exploit pixel-wise uncertainty as the guidance\n",
      "for segmentation but also introduces the ui to ﬁlter the feature ﬂow between\n",
      "task interaction.\n",
      "given an input medical image\n",
      "i, i ∈ rh,w , where h, w are the height and\n",
      "width of the image, separately.\n",
      "to maximize the extraction of speciﬁc infor-\n",
      "mation required for two diﬀerent tasks while adequately mingling the common\n",
      "feature which is helpful for both classiﬁcation and segmentation, i is ﬁrstly fed\n",
      "into the dual backbone network that outputs the classiﬁcation feature maps\n",
      "f c\n",
      "i , i ∈ 1, ..., 4 and segmentation feature maps f s\n",
      "i , i ∈ 1, ..., 4, where i denotes the\n",
      "ith layer of the backbone.\n",
      "2.1\n",
      "uncertainty estimation for classiﬁcation and segmentation\n",
      "classiﬁcation uncertainty estimation.\n",
      "for the k classiﬁcation problems,\n",
      "we utilize subjective logic [7] to produce the belief mass of each class and the\n",
      "uncertainty mass of the whole image based on evidence.\n",
      ", αc\n",
      "k], which associated with the cls evidence ec\n",
      "k, i.e. αc\n",
      "k =\n",
      "ec\n",
      "k + 1. in the end, the image-level belief mass and the uncertainty mass of the\n",
      "classiﬁcation can be calculated by\n",
      "bc\n",
      "k = ec\n",
      "k\n",
      "t c\n",
      "ally, eq. 2 describes such a phenomenon that the higher the probability assigned\n",
      "to the kth class, the more evidence observed for kth category should be.\n",
      "segmentation uncertainty estimation.\n",
      "essentially, segmentation is the\n",
      "classiﬁcation for each pixel of a medical image.\n",
      "given a pixel-wise segmenta-\n",
      "tion result, following [31] the seg dirichlet distribution can be parameterized\n",
      "by αs(h,w) =\n",
      "we can compute the belief\n",
      "mass and uncertainty mass of the input image by\n",
      "bs(h,w)\n",
      "q\n",
      "= es(h,w)\n",
      "q\n",
      "t s(h,w) = αs(h,w)\n",
      "q\n",
      "− 1\n",
      "t s(h,w)\n",
      ",\n",
      "and us(h,w) =\n",
      "q\n",
      "t s(h,w) ,\n",
      "(3)\n",
      "where bs(h,w)\n",
      "q\n",
      "≥ 0 and us(h,w) ≥ 0 denote the probability of the pixel at coordi-\n",
      "nate (h, w) for the qth class and the overall uncertainty value respectively.\n",
      "we\n",
      "also deﬁne u s = {us(h,w), (h, w) ∈ (h, w)} as the pixel-wise uncertainty of the\n",
      "segmentation result.\n",
      "2.2\n",
      "uncertainty-informed mutual learning\n",
      "uncertainty navigator for segmentation.\n",
      "actually, we have already\n",
      "obtained an initial segmentation mask m = αs, m ∈ (q, h, w) through esti-\n",
      "mating segmentation uncertainty, and achieved lots of valuable features such as\n",
      "uncertainty-informed mutual learning\n",
      "39\n",
      "fig.\n",
      "in our method, appropriate uncertainty guided decoding on the\n",
      "feature list can obtain more reliable information and improve the performance\n",
      "of segmentation [3,9,26].\n",
      "so we introduce uncertainty navigator for segmen-\n",
      "tation(un) as a feature decoder, which incorporates the pixel-wise uncertainty\n",
      "in u sand lesion location information in m with the segmentation feature maps\n",
      "to generate the segmentation result and reliable features.\n",
      "having a unet-like\n",
      "architecture [15], un computes segmentation si, i ∈ 1, .., 4 at each layer, as well\n",
      "as introduces the uncertainty in the bottom and top layer by the same way.\n",
      ", un calculates the reliable\n",
      "mask m r by:\n",
      "m r = (s1 ⊕ m) ⊗ e−u s,\n",
      "(4)\n",
      "then, the reliable segmentation feature rs, which combines the trusted informa-\n",
      "tion in m r with the original features, is generated by:\n",
      "rs = cat(conv(m r), cat(f s\n",
      "1, f b\n",
      "2)),\n",
      "(5)\n",
      "where f s\n",
      "1 derives from jump connecting and f b\n",
      "2 is the feature of the s2 with one\n",
      "up-sample operation.\n",
      "the rs is calculated from the segmentation result\n",
      "s1 and contains uncertainty navigated information not found in s1.\n",
      "in order to mine the comple-\n",
      "mentary knowledge of segmentation as the instruction for the classiﬁcation and\n",
      "eliminate intrusive features, we devise an uncertainty instructor for classiﬁcation\n",
      "(ui) following [22]. figure 2(b) shows the architecture of ui.\n",
      "⊗ f c\n",
      "4),\n",
      "(6)\n",
      "where dn(·) denotes that the frequency of down-sampling operations is n. then\n",
      "the produced features are transformed into a semantic feature vector by the\n",
      "40\n",
      "k. ren et al.\n",
      "global average pooling.\n",
      "2.3\n",
      "mutual learning process\n",
      "in a word, to obtain the ﬁnal results of classiﬁcation and segmentation, we con-\n",
      "struct an end-to-end mutual learning process, which is supervised by a joint loss\n",
      "function.\n",
      "to obtain an initial segmentation result m and a pixel-wise uncertainty\n",
      "estimation u s, following [31], a mutual loss is used as:\n",
      "lm(αs, ys) = lice(αs, ys) + λm\n",
      "1 lkl(αs)\n",
      "+ λm\n",
      "2 ldice(αs, ys),\n",
      "(7)\n",
      "where ys is the ground truth (gt) of the segmentation.\n",
      "similarly, in order to estimate the image-\n",
      "level uncertainty and classiﬁcation results.\n",
      "8)\n",
      "where yc is the true class of the input image.\n",
      "the hyperparameter λc serves as\n",
      "a crucial hyperparameter governing the kl, aligning with previous work [5]. to\n",
      "obtain reliable segmentation results, we also adopt deep supervision for the ﬁnal\n",
      "segmentation result s = {si, i = 1, ..., 4}, which can be denoted as:\n",
      "ls =\n",
      "\u00034\n",
      "i=1 ldice(υi−1(si), ys)\n",
      "4\n",
      ",\n",
      "(9)\n",
      "where υn indicates the number of up-sampling is 2n.\n",
      "+ wclc(αc, yc) + wsls,\n",
      "(10)\n",
      "where wm, wc, ws denote the weights and are set 0.1, 0.5, 0.4, separately.\n",
      "3\n",
      "experiments\n",
      "dataset and implementation.\n",
      "[13]. refuge contains two tasks, classiﬁ-\n",
      "cation of glaucoma and segmentation of optic disc/cup in fundus images.\n",
      "the\n",
      "overall 1200 images were equally divided for training, validation, and testing.\n",
      "all images are uniformly adjusted to 256 × 256 px.\n",
      "the tasks of ispy-1 are\n",
      "the pcr prediction and the breast tumor segmentation.\n",
      "for each case, we cut out the slices in the 3d image and totally got 1,570 2d\n",
      "images, which are randomly divided into the train, validation, and test datasets\n",
      "with 1,230, 170, and 170 slices, respectively.\n",
      "uncertainty-informed mutual learning\n",
      "41\n",
      "table 1. evaluation of the classiﬁcation and segmentation performance.\n",
      "3. the visual result of segmentation and classiﬁcation in refuge and ispy-1.\n",
      "top is the original image, and bottom is the input with gaussian noise (σ = 0.05).\n",
      "from left to right, input (with gt), the result of classiﬁcation (belief and image-level\n",
      "uncertainty), the result of segmentation, pixel-wise uncertainty.\n",
      "we implement the proposed method via pytorch and train it on nvidia\n",
      "geforce rtx 2080ti.\n",
      "we choose vgg-16 and res2net as the encoders\n",
      "for classiﬁcation and segmentation, separately.\n",
      "tbrats then extended ec to medical image seg-\n",
      "mentation.\n",
      "meriting both transformers and u-net, transunet is a strong model\n",
      "for medical image segmentation.\n",
      "the baseline of the joint classiﬁcation and segmentation framework\n",
      "(bcs) is a simple but useful way to share model parameters, which utilize two\n",
      "diﬀerent encoders and decoders for learning respectively.\n",
      "the deep synergistic\n",
      "interaction network (dsi) has demonstrated superior performance in joint task.\n",
      "dice score (di) and average symmetric surface\n",
      "distance (assd) are chosen for the segmentation task.\n",
      "as shown in table 1,\n",
      "we report the performance on the two datasets of the proposed uml and other\n",
      "methods.\n",
      "by comparison, we can observe the fact that the accuracy of the model\n",
      "results is low if either classiﬁcation or segmentation is done in isolation, the\n",
      "acc has only just broken 0.5 in ec.\n",
      "but joint classiﬁcation and segmentation\n",
      "changes this situation, the performance of bcs and dsi improves considerably,\n",
      "especially the acc and the dice score of optic cup.\n",
      "excitingly, our uml not only\n",
      "achieves the best classiﬁcation performance in acc (85.3%) and f1 (0.875) with\n",
      "signiﬁcant increments of 1.8%, 4.9%, but also obtains the superior segmentation\n",
      "performance with increments of 6.6% in didisc and 3.2% in dicup.\n",
      "a similar\n",
      "improvement can be observed in the experimental results in ispy-1.\n",
      "comparison under noisy data.\n",
      "to further valid the reliability of our model,\n",
      "we introduce gaussian noise with various levels of standard deviations (σ) to\n",
      "the input medical images.\n",
      "as\n",
      "can be observed that, the accuracy of classiﬁcation and segmentation signiﬁ-\n",
      "cantly decreases after adding noise to the raw data.\n",
      "it is obvious that both\n",
      "the image-level uncertainty and the pixel-wise uncertainty respond reasonably\n",
      "well to noise.\n",
      "these experimental results can verify the reliability and interpre of\n",
      "the uncertainty guided interaction between the classiﬁcation and segmentation\n",
      "in the proposed uml.\n",
      "the results of more qualitative comparisons can be found\n",
      "in the supplementary material.\n",
      "ablation study.\n",
      "it is clear that the performance\n",
      "of classiﬁcation and segmentation is signiﬁcantly improved when we introduce\n",
      "supervision of mutual features.\n",
      "4\n",
      "conclusion\n",
      "in this paper, we propose a novel deep learning approach, uml, for joint classiﬁ-\n",
      "cation and segmentation of medical images.\n",
      "our approach is designed to improve\n",
      "the reliability and interpretability of medical image classiﬁcation and segmenta-\n",
      "tion, by enhancing image-level and pixel-wise reliability estimated by evidential\n",
      "deep learning, and by leveraging mutual learning with the proposed un and\n",
      "ui modules.\n",
      "our extensive experiments demonstrate that uml outperforms\n",
      "baselines and introduces signiﬁcant improvements in both classiﬁcation and seg-\n",
      "mentation.\n",
      "overall, our results highlight the potential of uml for enhancing the\n",
      "performance and interpretability of medical image analysis.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf:\n",
      "skin lesion segmentation in dermoscopy images has seen\n",
      "recent success due to advancements in multi-scale boundary attention\n",
      "and feature-enhanced modules.\n",
      "however, existing methods that rely on\n",
      "end-to-end learning paradigms, which directly input images and output\n",
      "segmentation maps, often struggle with extremely hard boundaries, such\n",
      "as those found in lesions of particularly small or large sizes.\n",
      "motivated by the impressive advances of diﬀusion models that\n",
      "regard image synthesis as a parameterized chain process, we introduce\n",
      "a novel approach that formulates skin lesion segmentation as a bound-\n",
      "ary evolution process to thoroughly investigate the boundary knowledge.\n",
      "speciﬁcally, we propose the medical boundary diﬀusion model (mb-\n",
      "diﬀ), which starts with a randomly sampled gaussian noise, and the\n",
      "boundary evolves within ﬁnite times to obtain a clear segmentation map.\n",
      "first, we propose an eﬃcient multi-scale image guidance module to con-\n",
      "strain the boundary evolution, which makes the evolution direction suit\n",
      "our desired lesions.\n",
      "we evaluate the performance of our model on two popular\n",
      "skin lesion segmentation datasets and compare our model to the latest\n",
      "cnn and transformer models.\n",
      "our results demonstrate that our model\n",
      "outperforms existing methods in all metrics and achieves superior per-\n",
      "formance on extremely challenging skin lesions.\n",
      "the proposed approach\n",
      "has the potential to signiﬁcantly enhance the accuracy and reliability\n",
      "of skin lesion segmentation, providing critical information for diagnosis\n",
      "and treatment.\n",
      "all resources will be publicly available at https://github.\n",
      "com/jcwang123/mbdiﬀ.\n",
      "keywords: skin lesion segmentation · diﬀusion model\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43901-8_41.\n",
      "https://doi.org/10.1007/978-3-031-43901-8_41\n",
      "428\n",
      "j. wang et al.\n",
      "1\n",
      "introduction\n",
      "segmentation of skin lesions from dermoscopy images is a critical task in\n",
      "disease diagnosis and treatment planning of skin cancers [17].\n",
      "manual lesion\n",
      "segmentation is time-consuming and prone to inter- and intra-observer vari-\n",
      "ability.\n",
      "to improve the eﬃciency and accuracy of clinical workﬂows, numer-\n",
      "ous automated skin lesion segmentation models have been developed over the\n",
      "years\n",
      "[2,18,19], resulting in signiﬁcant improvements in skin lesion segmenta-\n",
      "tion performance.\n",
      "despite these advances, the segmentation of skin lesions with\n",
      "ambiguous boundaries, particularly at extremely challenging scales, remains a\n",
      "bottleneck issue that needs to be addressed.\n",
      "in such cases, even state-of-the-art\n",
      "segmentation models struggle to achieve accurate and consistent results.\n",
      "boundary evolution\n",
      "image\n",
      "small\n",
      "large\n",
      "fig.\n",
      "it could be seen that various lesions can be\n",
      "accurately segmented by splitting the segmentation into sequential timesteps (t), named\n",
      "as boundary evolution in this work.\n",
      "the small one covers\n",
      "1.03% in the image space and the large one covers 72.96%.\n",
      "as studied prior,\n",
      "solving the segmentation problems of such two types of lesions have diﬀerent\n",
      "strategies.\n",
      "the latest transformer,\n",
      "xbound-former, comprehensively addresses the multi-scale boundary problem\n",
      "through cross-scale boundary learning and exactly reaches higher performance\n",
      "on whatever small or large lesions.\n",
      ", current models for skin lesion segmentation are still struggling with\n",
      "extremely challenging cases, which are often encountered in clinical practice.\n",
      "while some approaches aim to optimize the model architecture by incorporating\n",
      "local and global contexts and multi-task supervision, and others seek to improve\n",
      "performance by collecting more labeled data and building larger models, both\n",
      "strategies are costly and can be limited by the inherent complexity of skin lesion\n",
      "boundaries.\n",
      "therefore, we propose a novel approach that shifts the focus from\n",
      "merely segmenting lesion boundaries to predicting their evolution.\n",
      "our approach\n",
      "is inspired by recent advances in image synthesis achieved by diﬀusion proba-\n",
      "bilistic models [6,9,14,15], which generate synthetic samples from a randomly\n",
      "sampled gaussian distribution in a series of ﬁnite steps.\n",
      "we adapt this process\n",
      "to model the evolution of skin lesion boundaries as a parameterized chain pro-\n",
      "cess, starting from gaussian noise and progressing through a series of denoising\n",
      "steps to yield a clear segmentation map with well-deﬁned lesion boundaries.\n",
      "by\n",
      "predicting the next step in the chain process rather than the ﬁnal segmenta-\n",
      "tion map, our approach enables the more accurate segmentation of challenging\n",
      "lesions than previous models.\n",
      "1, where each row corresponds to a diﬀerent step in the evolution process,\n",
      "culminating in a clear segmentation map with well-deﬁned boundaries.\n",
      "in this paper, we propose a medical boundary diﬀusion model (mb-diﬀ)\n",
      "to improve the skin lesion segmentation, particularly in cases where the lesion\n",
      "boundaries are ambiguous and have extremely large or small sizes.\n",
      "however, it also includes two\n",
      "key innovations: firstly, we have developed an eﬃcient multi-scale image guid-\n",
      "ance module, which uses a pretrained transformer encoder to extract multi-scale\n",
      "features from prior images.\n",
      "secondly, we have implemented an\n",
      "evolution uncertainty-based fusion strategy, which takes into account the uncer-\n",
      "tainty of diﬀerent initializations to reﬁne the evolution results and obtain more\n",
      "precise lesion boundaries.\n",
      "we evaluate our model on two popular skin lesion\n",
      "segmentation datasets, isic-2016 and ph2 datasets, and ﬁnd that it performs\n",
      "signiﬁcantly better than existing models.\n",
      "speciﬁcally, given the image and boundary mask distributions as (x, y), assum-\n",
      "ing that the evolution consists of t steps in total, the boundary at t-th step\n",
      "(yt ) is the randomly initialized noise and the boundary at 0-th (y0) step denotes\n",
      "the accurate result.\n",
      "(2)\n",
      "note that the prediction function takes the input image as a condition, enabling\n",
      "the evolving boundary to ﬁt the corresponding lesion accurately.\n",
      "by modeling\n",
      "boundary evolution as a step-by-step denoising process, mb-diﬀ can eﬀectively\n",
      "capture the complex structures of skin lesions with ambiguous boundaries, lead-\n",
      "ing to superior performance in lesion segmentation.\n",
      "to optimize the model parameters θ, we use the evolution target as an approx-\n",
      "imation of the posterior at each evolution step.\n",
      "given the segmentation label y\n",
      "as y0, the label is gradually added by a gaussian noise as:\n",
      "q(y1:t |y0) := \u0003t\n",
      "t=1 q(yt|yt−1) :\n",
      "2.2\n",
      "paramterized architecture with image prior\n",
      "the proposed model is a parameterized chain process that predicts the μ∗\n",
      "t−1 and\n",
      "\u0004∗\n",
      "t−1 at each evolution step t under the prior conditions of the image x\n",
      "to capture the deep semantics of these conditions and\n",
      "perform eﬃcient fusion, we adopt a basic u-net\n",
      "[16] architecture inspired by the\n",
      "plain dpm and introduce novel designs for condition fusion, that is the eﬃcient\n",
      "multi-scale image guidance module.\n",
      "at the bottleneck\n",
      "layer, we fuse the evolution features with the image guidance to constrain the\n",
      "evolution and ensure that the ﬁnal boundary suits the conditional image.\n",
      "to achieve this, priors train a segmentation model concurrently with the\n",
      "evolution model and use an attention-based parser to translate the image features\n",
      "medical boundary diﬀusion model\n",
      "431\n",
      "in the segmentation branch into the evolution branch [22].\n",
      "since the segmentation\n",
      "model is trained much faster than the evolution model, we adopt a pretrained\n",
      "pyramid vision transformer (pvt)\n",
      "[20] as the image feature extractor to obtain\n",
      "the multi-scale image features.\n",
      "after that, the four features are concatenated and fed into a full-\n",
      "connection layer to map the image feature space into the evolution space.\n",
      "we\n",
      "then perform a simple yet eﬀective addition of the mapped image feature and\n",
      "the encoded prior evolution feature, similar to the fusion of time embeddings, to\n",
      "avoid redundant computation.\n",
      "2.3\n",
      "evolution uncertainty\n",
      "similar to typical evolutionary algorithms, the ﬁnal results of boundary evolu-\n",
      "tion are heavily inﬂuenced by the initialized population.\n",
      "the\n",
      "reason is that the image features in such ambiguous regions may not provide\n",
      "discriminative guidance for the evolution, resulting in signiﬁcant variations in\n",
      "diﬀerent evolution times.\n",
      "instead of reducing the diﬀerences, we surprisingly\n",
      "ﬁnd that these diﬀerences can represent segmentation uncertainty.\n",
      "based on the\n",
      "evolution-based uncertainty estimation, the segmentation results become more\n",
      "accurate and trustworthy in practice [4,5,12].\n",
      "unlike traditional segmentation\n",
      "models that typically scale the prediction into the range of 0 to 1, the evolved\n",
      "maps generated by mb-diﬀ have unﬁxed distributions due to random sampling.\n",
      "therefore,\n",
      "432\n",
      "j. wang et al.\n",
      "table 1. comparison of skin lesion segmentation with diﬀerent approaches on the isic-\n",
      "2016 and ph2 datasets.\n",
      "the averaged scores of both sets are presented respectively.\n",
      "[22]\n",
      "83.39\n",
      "89.85\n",
      "12.38\n",
      "31.23\n",
      "82.21\n",
      "89.73\n",
      "13.53\n",
      "36.59\n",
      "mb-diﬀ (ours)\n",
      "88.87 93.78 7.19\n",
      "18.90\n",
      "87.12 92.85 9.16\n",
      "22.95\n",
      "we employ the max vote algorithm to obtain the ﬁnal segmentation map.\n",
      "finally, the segmentation map is\n",
      "generated as y∗ = (\u0004n\n",
      "i=1 y∗,i) ≥ τ.\n",
      "3\n",
      "experiment\n",
      "3.1\n",
      "datasets and evaluation metrics\n",
      "datasets: we use two publicly available skin lesion segmentation datasets from\n",
      "diﬀerent institutions in our experiments: the isic-2016 dataset and the ph2\n",
      "dataset.\n",
      "[13],\n",
      "which contains 200 labeled samples and is used to evaluate the generalization\n",
      "performance of our methods.\n",
      "evaluation metrics: to comprehensively compare the segmentation results,\n",
      "particularly the boundary delineations, we employ four commonly used metrics\n",
      "to quantitatively evaluate the performance of our segmentation methods.\n",
      "these\n",
      "metrics include the dice score, the iou score, average symmetric surface dis-\n",
      "tance (assd), and hausdorﬀ distance of boundaries (95−th percentile; hd95).\n",
      "to ensure fair comparison, all labels and predictions are resized to (512×512)\n",
      "before computing these scores, following the approach of a previous study [18].\n",
      "3.2\n",
      "implementation details\n",
      "for the diﬀusion model hyper-parameters, we use the default settings of the plain\n",
      "diﬀusion model, which can be found in the supplementary materials.\n",
      "regarding\n",
      "medical boundary diﬀusion model\n",
      "433\n",
      "false negatives\n",
      "true positves\n",
      "false postives\n",
      "image\n",
      "u-net++\n",
      "ca-net\n",
      "transunet\n",
      "mb-diff\n",
      "(ours)\n",
      "gt\n",
      "transfuse\n",
      "xbound-\n",
      "former\n",
      "medseg\n",
      "diff\n",
      "uncertainty\n",
      "(ours)\n",
      "fig.\n",
      "the training parameters, we resize all images to (256 × 256) for eﬃcient memory\n",
      "utilization and computation.\n",
      "we use a set of random augmentations, including\n",
      "vertical ﬂipping, horizontal ﬂipping, and random scale change (limited to 0.9 ∼\n",
      "1.1), to augment the training data.\n",
      "3.3\n",
      "comparison with state-of-the-arts\n",
      "we majorly compare our method to the latest skin lesion segmentation models,\n",
      "including the cnn-based and transformer-based models, i.e., u-net++\n",
      "though the parameters of cnns and transformers are\n",
      "selected with the best performance on isic-2016 validation set and the param-\n",
      "eters of our method are selected by completing the 200,000 iterations, mb-diﬀ\n",
      "still achieves the 1.18% iou improvement and 0.7% dice improvement.\n",
      "moreover, our method shows a larger improvement in generalization\n",
      "performance on the ph2 dataset, indicating its better ability to handle new data.\n",
      "our visual comparison reveals several key ﬁndings: (1) mb-diﬀ consistently\n",
      "achieves better segmentation performance on small and large lesions due to its\n",
      "thorough learning of boundary evolution, as seen in rows 3, 5, and 6.\n",
      "(2) mb-\n",
      "diﬀ is able to produce correct boundaries even in cases where they are nearly\n",
      "indistinguishable in human perception, eliminating the need for further manual\n",
      "adjustments and demonstrating signiﬁcant practical value.\n",
      "(3) mb-diﬀ generates\n",
      "fewer false positive segmentation, resulting in cleaner predictions that enhance\n",
      "the user experience.\n",
      "this information can be\n",
      "used to guide human reﬁnement of the segmentation in practical applications,\n",
      "ultimately increasing the ai’s trustworthiness.\n",
      "3.4\n",
      "detailed analysis of the evolution\n",
      "in this subsection, we make a comprehensive analysis to investigate the perfor-\n",
      "mance of each component in our method and compare it to the diﬀusion-based\n",
      "model, medsegdiﬀ. the results of our ablation study are presented in fig.\n",
      "3(a),\n",
      "where “w/o evo” refers to using image features to directly train a segmentation\n",
      "model with fpn [11] architecture and “w/o fusion” means no evolution fusion is\n",
      "used.\n",
      "to ensure a fair comparison, we average the scores of multiple evolutions\n",
      "to represent the performance of “w/o fusion”.\n",
      "the results demonstrate that our\n",
      "evolutionary approach can signiﬁcantly improve performance, and the evolution\n",
      "uncertainty-based fusion strategy further enhances performance.\n",
      "3(b) shows that our method\n",
      "converges faster and achieves smaller losses, indicating that our multi-scale image\n",
      "guidance is more eﬀective than that of medsegdiﬀ. furthermore, we evaluate our\n",
      "medical boundary diﬀusion model\n",
      "435\n",
      "method’s performance using parameters saved at diﬀerent iterations, as shown\n",
      "in fig.\n",
      "our results demonstrate that our method has competitive perfor-\n",
      "mance at 50k iterations versus medsegdiﬀ at 200k iterations and our method\n",
      "at 100k iterations has already outperformed well-trained medsegdiﬀ.\n",
      "4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf:\n",
      "the use of functional imaging such as pet in radiother-\n",
      "apy (rt) is rapidly expanding with new cancer treatment techniques.\n",
      "a fundamental step in rt planning is the accurate segmentation of\n",
      "tumours based on clinical diagnosis.\n",
      "furthermore, recent tumour con-\n",
      "trol techniques such as intensity modulated radiation therapy (imrt)\n",
      "dose painting requires the accurate calculation of multiple nested con-\n",
      "tours of intensity values to optimise dose distribution across the tumour.\n",
      "recently, convolutional neural networks (cnns) have achieved tremen-\n",
      "dous success in image segmentation tasks, most of which present the\n",
      "output map at a pixel-wise level.\n",
      "in addition, for the dose painting\n",
      "strategy, there is a need to develop image segmentation approaches that\n",
      "reproducibly and accurately identify the high recurrent-risk contours.\n",
      "to\n",
      "address these issues, we propose a novel hybrid-cnn that integrates a\n",
      "kernel smoothing-based probability contour approach (kspc) to produce\n",
      "contour-based segmentation maps, which mimic expert behaviours and\n",
      "provide accurate probability contours designed to optimise dose paint-\n",
      "ing/imrt strategies.\n",
      "instead of user-supplied tuning parameters, our\n",
      "ﬁnal model, named kspc-net, applies a cnn backbone to automatically\n",
      "learn the parameters and leverages the advantage of kspc to simultane-\n",
      "ously identify object boundaries and provide probability contour accord-\n",
      "ingly.\n",
      "the proposed model demonstrated promising performance in com-\n",
      "parison to state-of-the-art models on the miccai 2021 challenge dataset\n",
      "(hecktor).\n",
      "keywords: image segmentation · pet imaging · probability\n",
      "contour · dose painting · deep learning\n",
      "1\n",
      "introduction\n",
      "fluorodeoxyglucose positron emission tomography (pet) is widely recognized\n",
      "as an essential tool in oncology\n",
      "[10], playing an important role in the stag-\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43901-8 51.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "it enables the extrac-\n",
      "tion of semi-quantitative metrics such as standardized uptake values (suvs),\n",
      "which normalize pixel intensities based on patient weight and radiotracer dose\n",
      "[20].\n",
      "manual delineation is a time-consuming and laborious task that is prone to\n",
      "poor reproducibility in medical imaging, and this is particularly true for pet,\n",
      "due to its low signal-to-noise ratio and limited spatial resolution [10].\n",
      "in addition,\n",
      "manual delineation depends heavily on the expert’s prior knowledge, which often\n",
      "leads to large inter-observer and intra-observer variations [8].\n",
      "therefore, there\n",
      "is an urgent need for developing accurate automatic segmentation algorithms in\n",
      "pet images which will reduce expert workload, speed up rt planning while\n",
      "reducing intra-observer variability.\n",
      "in the last decade, cnns have demonstrated remarkable achievements in\n",
      "medical image segmentation tasks.\n",
      "despite the\n",
      "headway made in using cnns, their applications have been restricted to the\n",
      "generation of pixel-wise segmentation maps instead of smooth contour.\n",
      "although\n",
      "cnns may yield satisfactory segmentation results, low values of the loss function\n",
      "may not always indicate a meaningful segmentation.\n",
      "the kspc provides a surface\n",
      "over images that naturally produces contour-based results rather than pixel-wise\n",
      "results, thus mimicking experts’ hand segmentation.\n",
      "however, the performance\n",
      "of kspc depends heavily on the tuning parameters of bandwidth and threshold\n",
      "in the model, and it lacks information from other patients.\n",
      "beyond tumour delineation, another important use of functional images, such\n",
      "as pet images is their use for designing imrt dose painting (dp).\n",
      "in partic-\n",
      "ular, dose painting uses functional images to paint optimised dose prescriptions\n",
      "based on the spatially varying radiation sensitivities of tumours, thus enhanc-\n",
      "ing the eﬃcacy of tumour control\n",
      "however, there is an urgent need to\n",
      "develop image segmentation approaches that reproducibly and accurately iden-\n",
      "tify the high recurrent-risk contours [18].\n",
      "our previously proposed kspc pro-\n",
      "vides a clear framework to calculate the probability contours of the suv values\n",
      "and can readily be used to deﬁne an objective strategy for segmenting tumours\n",
      "into subregions based on metabolic activities, which in turn can be used to design\n",
      "the imrt dp strategy.\n",
      "in the proposed\n",
      "kspc-net, a cnn is employed to learn directly from the data to produce the\n",
      "pixel-wise bandwidth feature map and initial segmentation map, which are used\n",
      "to deﬁne the tuning parameters in the kspc module.\n",
      "more speciﬁcally, we use the classic unet\n",
      "[17] as the cnn backbone and evaluate our kspc-net on the publicly avail-\n",
      "able miccai hecktor (head and neck tumor segmentation) challenge\n",
      "2021 dataset.\n",
      "moreover, it can produce contour-based segmentation results which provide a\n",
      "more accurate delineation of object edges and provide probability contours as a\n",
      "byproduct, which can readily be used for dp planning.\n",
      "2\n",
      "methods\n",
      "2.1\n",
      "kernel smoothing based probability contour\n",
      "kernel-based method and follow up approach of modal clustering [13,16] have\n",
      "been used to cluster high-dimensional random variables and natural-scene image\n",
      "segmentation.\n",
      "in particular, let y = (y1, y2, ..., yn)\n",
      "denote n pixel’s suv in a 2d pet image sequentially, and xi = (xi1, xi2), i =\n",
      "1, ..., n denote position vector with xi1 and xi2 being the position in 2d respec-\n",
      "tively.\n",
      "by placing a threshold plane, a contour-based segmen-\n",
      "tation map can naturally be obtained.\n",
      "note that one can obtain a pixel-based\n",
      "segmentation map, by thresholding the surface at the observed grid points.\n",
      "a visualization example of how kspc works: (a) an example of a pet image\n",
      "(b) grid-level intensity values as observations (c) the resulting smoothed surface built\n",
      "by kspc with a threshold plane.\n",
      "the estimated probability contour level fω can be computed as the ω-th\n",
      "quantile of ˆfω of ˆf(x1; h), ..., ˆf(xn; h) (proof in supplementary materials).\n",
      "the primary advantage of utilizing probability contours is their ability to\n",
      "assign a clear probabilistic interpretation on the deﬁned contours, which are\n",
      "scale-invariant [5].\n",
      "4.2.\n",
      "2.2\n",
      "the kspc-net architecture\n",
      "in the kspc module, the model performance heavily depends on the bandwidth\n",
      "matrix h and it is often assumed that each kernel shares the same scalar band-\n",
      "width parameter.\n",
      "additionally, we obtain the optimal thresh-\n",
      "old for constructing the kspc contour from the initial segmentation map.\n",
      "2 the proposed kspc-net integrates the kspc approach with a\n",
      "cnn backbone (unet) in an end-to-end diﬀerentiable manner.\n",
      "first, the ini-\n",
      "tial segmentation map and pixel-level bandwidth parameter map h(xi1, xi2) of\n",
      "kspc are learned from data by the cnn backbone.\n",
      "then the kspc module\n",
      "obtains the quantile threshold value for each image by identifying the quantile\n",
      "corresponding to the minimum suv of the tumour class in the initial segmen-\n",
      "tation map.\n",
      "the next step involves transmitting the bandwidth map, quantile\n",
      "threshold, and raw image to kspc module to generate the segmentation map\n",
      "and its corresponding probability contours.\n",
      "additionally, the initial unet segmentation can produce another\n",
      "loss function, called cnn loss, which serves as an auxiliary supervision for the\n",
      "cnn backbone.\n",
      "deep probability contour framework\n",
      "539\n",
      "2.3\n",
      "loss function\n",
      "the dice similarity coeﬃcient is widely employed to evaluate segmentation mod-\n",
      "els.\n",
      "we utilize the dice loss function to optimize the model performance during\n",
      "training, which is deﬁned as:\n",
      "ldice(y, ˆy) = 1 −\n",
      "2 \u0005n\n",
      "i yiˆyi\n",
      "\u0005n\n",
      "i yi + \u0005n\n",
      "i ˆyi + ϵ\n",
      ",\n",
      "where yi is the label from experts and ˆyi is the predicted label of i-th pixel.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "dataset\n",
      "the dataset is from the hecktor challenge in miccai 2021 (head and neck\n",
      "tumor segmentation challenge).\n",
      "for each patient, fdg-\n",
      "pet input images and corresponding labels in binary description (0 s and 1 s)\n",
      "for the primary gross tumour volume are provided and co-registered to a size\n",
      "of 144 × 144 × 144 using bounding box information encompassing the tumour.\n",
      "five-fold cross-validation is used to generalize the performance of models.\n",
      "3.2\n",
      "implementation details\n",
      "we used python and a trained network on a nvidia dual quadro rtx machine\n",
      "with 64 gb ram using the pytorch package.\n",
      "4\n",
      "results\n",
      "4.1\n",
      "results on hecktor 2021 dataset\n",
      "to evaluate the performance of our kspc-net, we compared it with the results\n",
      "of 5-fold cross-validation against three widely-used models namely, the standard\n",
      "2d unet, the 2d residual unet and the 3d unet.\n",
      "additionally, we compare our\n",
      "540\n",
      "w. zhang and s. ray\n",
      "performance against newly developed approaches msa-net\n",
      "to quantify the\n",
      "performance, we report several metrics including dice similarity scores, preci-\n",
      "sion, recall, and hausdorﬀ distance.\n",
      "it is worth mentioning that since\n",
      "our kspc-net is in a 2d unet structure, the hausdorﬀ distance here was calcu-\n",
      "lated on slice averages to use a uniform metric across all 2d and 3d segmentation\n",
      "models.\n",
      "mean segmentation results of diﬀerent models and our proposed model.\n",
      "the\n",
      "model with best performance for each metric is indicated in bold*.\n",
      "method\n",
      "dice score hausdorﬀ dist precision recall\n",
      "2d-unet\n",
      "0.740\n",
      "0.561\n",
      "0.797\n",
      "0.873\n",
      "res-unet\n",
      "0.680\n",
      "0.611\n",
      "0.740\n",
      "0.841\n",
      "3d-unet\n",
      "0.764\n",
      "0.546\n",
      "0.839*\n",
      "0.797\n",
      "msa-net\n",
      "0.757\n",
      "-\n",
      "0.788\n",
      "0.785\n",
      "ccut-net\n",
      "0.750\n",
      "-\n",
      "0.776\n",
      "0.804\n",
      "kspc-net(ours) 0.768*\n",
      "0.521*\n",
      "0.793\n",
      "0.911*\n",
      "the results clearly demonstrate that the proposed kspc-net is eﬀective in\n",
      "segmenting h&n tumours, achieving a mean dice score of 0.768.\n",
      "this repre-\n",
      "sents a substantial improvement over alternative approaches, including 2d-unet\n",
      "(0.740), 3d u-net (0.764), residual-unet (0.680), msa-net (0.757) and ccut-\n",
      "net (0.750).\n",
      "while we acknowledge that there was no statistically signiﬁcant\n",
      "improvement compared to other sota models, it is important to note that our\n",
      "main goal is to showcase the ability to obtain probability contours as a nat-\n",
      "ural byproduct while preserving state-of-the-art accuracy levels.\n",
      "on the other\n",
      "hand, in comparison to the baseline 2d-unet model, kspc-net yields a higher\n",
      "recall (0.911) with a signiﬁcant improvement (4.35%), indicating that kspc-\n",
      "net generates fewer false negatives (fn).\n",
      "in addition, the proposed kspc-net achieves the best\n",
      "performance on hausdorﬀ distance among the three commonly used unet mod-\n",
      "els (2d-unet, res-unet and 3d-unet), which indicates that kspc-net exhibits\n",
      "a stronger capacity for accurately localizing the boundaries of objects.\n",
      "this is\n",
      "consistent with the mechanisms of kspc, which leverages neighbouring weights\n",
      "to yield outputs with enhanced smoothness.\n",
      "for exam-\n",
      "ple, fig. 3 provides two examples of pet image segmentation maps by kspc-\n",
      "net and their corresponding probability contours in the last column.\n",
      "3. illustrations of the segmentation results and probability contours on two exam-\n",
      "ples.\n",
      "the four columns are original pet images, ground truth provided by experts,\n",
      "segmentation maps from kspc-net and its probability contours (in 10%, 30%, 50%,\n",
      "70%, 90% respectively).\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf:\n",
      "segmenting prostate from mri is crucial for diagnosis and\n",
      "treatment planning of prostate cancer.\n",
      "however, if the local center has limited image collection capability,\n",
      "there may also not be enough unlabeled data for semi-supervised learn-\n",
      "ing to be eﬀective.\n",
      "to overcome this issue, other partner centers can be\n",
      "consulted to help enrich the pool of unlabeled images, but this can result\n",
      "in data heterogeneity, which could hinder ssl that functions under the\n",
      "assumption of consistent data distribution.\n",
      "tailoring for this important\n",
      "yet under-explored scenario, this work presents a novel category-level\n",
      "regularized unlabeled-to-labeled (cu2l) learning framework for semi-\n",
      "supervised prostate segmentation with multi-site unlabeled mri data.\n",
      "our method is evaluated\n",
      "on prostate mri data from six diﬀerent clinical centers and shows supe-\n",
      "rior performance compared to other semi-supervised methods.\n",
      "keywords: prostate segmentation · semi-supervised · heterogeneity\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43901-8_1\n",
      "4\n",
      "z. xu et al.\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "suponly\n",
      "mt\n",
      "ua-mt\n",
      "ict\n",
      "cpcl\n",
      "cct\n",
      "cps\n",
      "ssnet\n",
      "local labeled \n",
      "local unlabeled\n",
      "external multi-site unlabeled support data \n",
      "…\n",
      ",\n",
      ",\n",
      ",\n",
      "typical semi-supervised learning (ssl)\n",
      "effectively work under the assumption that the \n",
      "centralized local i.i.d. unlabeled data is abundant\n",
      "\u0002\n",
      "limited performance gain or even fail when the local\n",
      "unlabeled data is also limited (e.g., due to restricted \n",
      "image collection capabilities or a scarcity of patients)\n",
      "\u0003\n",
      "multi-site semi-supervised learning (ms-ssl)\n",
      "the unlabeled image pool can be quickly enriched\n",
      "via the support from partner clinical centers with low \n",
      "barriers of entry (only unlabeled images are required)\n",
      "\u0002\n",
      "data heterogeneity due to different scanners, \n",
      "scanning protocols and subject groups, which violate \n",
      "the typical ssl assumption of i.i.d. data\n",
      "local: c1.\n",
      "each center supplied t2-weighted mr images of the prostate.\n",
      "center source\n",
      "#scans field strength (t) resolution (in-plane/through-plane in mm) coil\n",
      "scanner\n",
      "c1\n",
      "runmc [1]\n",
      "30\n",
      "3\n",
      "0.6–0.625/3.6–4\n",
      "surface\n",
      "siemens\n",
      "c2\n",
      "bmc\n",
      "[1]\n",
      "30\n",
      "1.5\n",
      "0.4/3\n",
      "endorectal philips\n",
      "c3\n",
      "hcrudb [4] 19\n",
      "3\n",
      "0.67–0.79/1.25\n",
      "–\n",
      "siemens\n",
      "c4\n",
      "ucl\n",
      "[5]\n",
      "13\n",
      "1.5 and 3\n",
      "0.325–0.625/3–3.6\n",
      "–\n",
      "siemens\n",
      "c5\n",
      "bidmc [5]\n",
      "12\n",
      "3\n",
      "0.25/2.2–3\n",
      "endorectal ge\n",
      "c6\n",
      "hk\n",
      "[5]\n",
      "12\n",
      "1.5\n",
      "0.625/3.6\n",
      "endorectal siemens\n",
      "1\n",
      "introduction\n",
      "prostate segmentation from magnetic resonance imaging (mri) is a crucial step\n",
      "for diagnosis and treatment planning of prostate cancer.\n",
      "recently, deep learning-\n",
      "based approaches have greatly improved the accuracy and eﬃciency of automatic\n",
      "prostate mri segmentation [7,8].\n",
      "yet, their success usually requires a large\n",
      "amount of labeled medical data, which is expensive and expertise-demanding\n",
      "in practice.\n",
      "in this regard, semi-supervised learning (ssl) has emerged as an\n",
      "attractive option as it can leverage both limited labeled data and abundant\n",
      "unlabeled data [3,9–11,15,16,21–26,28].\n",
      "unfor-\n",
      "tunately, such “abundance” may be unobtainable in practice, i.e., the local unla-\n",
      "beled pool is also limited due to restricted image collection capabilities or scarce\n",
      "patient samples.\n",
      "taking c1 as a case study, if the amount of\n",
      "local unlabeled data is limited, existing ssl methods may still suﬀer from inferior\n",
      "performance when generalizing to unseen test data (fig. 1).\n",
      "yet, due to diﬀerences in imaging protocols and variations in\n",
      "patient demographics, this solution usually introduces data heterogeneity, lead-\n",
      "category-level regularized unlabeled-to-labeled learning\n",
      "5\n",
      "ing to a quality problem.\n",
      "such heterogeneity may impede the performance of\n",
      "ssl which typically assumes that the distributions of labeled data and unla-\n",
      "beled data are independent and identically distributed (i.i.d.)\n",
      "here, we deﬁne this new ssl scenario as multi-site semi-supervised learn-\n",
      "ing (ms-ssl), allowing to enrich the unlabeled pool with multi-site heteroge-\n",
      "neous images.\n",
      "thus, it intuitively utilizes image-level mapping to minimize\n",
      "dual-distribution discrepancy.\n",
      "yet, their adversarial min-max optimization often\n",
      "leads to instability and it is diﬃcult to align multiple external sources with the\n",
      "local source using a single image mapping network.\n",
      "2,\n",
      "to achieve robust ms-ssl for prostate mri segmentation.\n",
      ", the local unlabeled data is involved into pseudo-\n",
      "label supervised-like learning to reinforce ﬁtting of the local data distribution;\n",
      "(ii) considering that intra-class variance hinders eﬀective ms-ssl, we introduce\n",
      "a non-parametric unlabeled-to-labeled learning scheme, which takes advantage of\n",
      "the scarce expert labels to explicitly constrain the prototype-propagated predic-\n",
      "tions, to help the model exploit discriminative and domain-insensitive features\n",
      "from heterogeneous multi-site data to support the local center.\n",
      "yet, observing\n",
      "that such scheme is challenging when signiﬁcant shifts and various distributions\n",
      "are present, we further propose category-level regularization, which advocates\n",
      "prototype alignment, to regularize the distribution of intra-class features from\n",
      "arbitrary external data to be closer to the local distribution; (iii) based on the\n",
      "fact that perturbations (e.g., gaussian noises [15]) can be regarded as a simu-\n",
      "lation of heterogeneity, perturbed stability learning is incorporated to enhance\n",
      "the robustness of the model.\n",
      "our method is evaluated on prostate mri data\n",
      "from six diﬀerent clinical centers and shows promising performance on tackling\n",
      "ms-ssl compared to other semi-supervised methods.\n",
      "ema: exponential moving average.\n",
      "xl\n",
      "local(i), xu\n",
      "local(i) ∈ rh×w ×d denote the scans with height h, width w and\n",
      "depth d, and y l\n",
      "local(i) ∈ {0, 1}h×w ×d denotes the label of xl\n",
      "local(i) (we focus\n",
      "on binary segmentation).\n",
      "considering the\n",
      "large variance on slice thickness among diﬀerent centers [7,8], our experiments\n",
      "are performed in 2d.\n",
      "speciﬁcally, the student f s\n",
      "θ is an in-training model optimized by loss\n",
      "back-propagation as usual while the teacher model f t\n",
      "˜θ is slowly updated with\n",
      "a momentum term that averages previous weights with the current weights,\n",
      "where θ denotes the student’s weights and ˜θ the teacher’s weights.\n",
      "˜θ is updated\n",
      "by ˜θt = α˜θt−1 + (1 − α)θt at iteration t, where α is the exponential moving\n",
      "average (ema) coeﬃcient and empirically set to 0.99\n",
      "thus, the teacher model is\n",
      "suitable for handling the heterogeneous external images and producing relatively\n",
      "stable pseudo labels (will be used later).\n",
      "category-level regularized unlabeled-to-labeled learning\n",
      "7\n",
      "2.2\n",
      "pseudo labeling for local distribution fitting\n",
      "as mentioned above, supervised-like learning is advocated for local unlabeled\n",
      "data to help the model ﬁt local distribution better.\n",
      "the dice loss is calculated for each\n",
      "of the k equally-sized regions of the image, and the ﬁnal loss is obtained by\n",
      "taking their mean.\n",
      "inherently, the challenge of ms-ssl stems\n",
      "from intra-class variation, which results from diﬀerent imaging protocols, disease\n",
      "progress and patient demographics. inspired by prototypical networks [13,19,25]\n",
      "that compare class prototypes with pixel features to perform segmentation,\n",
      "here, we introduce a non-parametric unlabeled-to-labeled (u2l) learning scheme\n",
      "that utilizes expert labels to explicitly constrain the prototype-propagated pre-\n",
      "dictions.\n",
      "such design is based on two considerations: (i) a good prototype-\n",
      "propagated prediction requires both compact feature and discriminative pro-\n",
      "totypes, thus enhancing this prediction can encourage the model to learn in a\n",
      "variation-insensitive manner and focus on the most informative clues; (ii) using\n",
      "expert labels as ﬁnal guidance can prevent error propagation from pseudo labels.\n",
      "speciﬁcally, we denote the feature map of the external unlabeled image xu\n",
      "e before\n",
      "the penultimate convolution in the teacher model as f u,t\n",
      "e\n",
      ".\n",
      "with the argmax pseudo label ˆy u,t\n",
      "e\n",
      "and the predicted probability map\n",
      "p u,t\n",
      "e\n",
      ", the object prototype from the external unlabeled data can be computed via\n",
      "conﬁdence-weighted masked average pooling: cu(obj)\n",
      "e\n",
      "=\n",
      "\u0002\n",
      "v\n",
      "\u0003\n",
      "ˆy u,t,obj\n",
      "e(v)\n",
      "·p u,t,obj\n",
      "e(v)\n",
      "·f u,t\n",
      "e(v)\n",
      "\u0004\n",
      "\u0002\n",
      "v\n",
      "\u0003\n",
      "ˆy u,t,obj\n",
      "e(v)\n",
      "·p u,t,obj\n",
      "e(v)\n",
      "\u0004\n",
      ".\n",
      "2, given the feature map f l\n",
      "local of the local labeled image xl\n",
      "local\n",
      "from the in-training student model, we can compare {cu(obj)\n",
      "e\n",
      ", cu(bg)\n",
      "speciﬁcally, we introduce category-level regularization, which\n",
      "advocates class prototype alignment between local and external data, to regu-\n",
      "larize the distribution of intra-class features from arbitrary external data to be\n",
      "closer to the local one, thus reducing the diﬃculty of u2l learning.\n",
      "the weight\n",
      "of background prototype alignment is smaller due to less relevant contexts.\n",
      "speciﬁcally, for the same unlabeled input xu ∈ {du\n",
      "local ∪ du\n",
      "e }\n",
      "with diﬀerent perturbations ξ and ξ′ (using the same gaussian noises as in [26]),\n",
      "we encourage consistent pre-softmax predictions between the teacher and stu-\n",
      "dent models, formulated as lu\n",
      "sta = d\n",
      "\u0006\n",
      "f t\n",
      "˜θ(xu + ξ), f s\n",
      "θ (xu + ξ′)\n",
      "\u0007\n",
      ", where mean\n",
      "squared error is also adopted as the distance function d(·, ·).\n",
      "(4)\n",
      "3\n",
      "experiments and results\n",
      "materials.\n",
      "we utilize prostate t2-weighted mr images from six diﬀerent clini-\n",
      "cal centers (c1–6)\n",
      "cu2l (ours)\n",
      "8\n",
      "10\n",
      "86\n",
      "86.46 (6.72)\n",
      "76.74 (9.97)\n",
      "82.30 (9.93)\n",
      "70.71 (12.94)\n",
      "supervised (upper bound) 18\n",
      "0\n",
      "0\n",
      "89.19 (4.33)\n",
      "80.76 (6.71)\n",
      "85.01 (4.35)\n",
      "74.15 (6.44)\n",
      "rizes the characteristics of the six data sources, following [7,8], where [7,8] also\n",
      "reveal the severity of inter-center heterogeneity here through extensive experi-\n",
      "ments.\n",
      "compared to c1 and\n",
      "c2, scans from c3 to c6 are taken from patients with prostate cancer, either for\n",
      "detection or staging purposes, which can cause inherent semantic diﬀerences in\n",
      "the prostate region to further aggravate heterogeneity.\n",
      "we take c1 or c2 as the local target center and randomly divide their 30 scans\n",
      "into 18, 3, and 9 samples as training, validation, and test sets, respectively.\n",
      "implementation and evaluation metrics.\n",
      "the framework is implemented\n",
      "on pytorch using an nvidia geforce rtx 3090 gpu.\n",
      "cu2l-1\n",
      "cu2l-2\n",
      "cu2l-3\n",
      "cu2l-4\n",
      "cu2l\n",
      "c1\n",
      "80.24\n",
      "82.75\n",
      "83.31\n",
      "84.77\n",
      "85.93\n",
      "c2\n",
      "77.24\n",
      "77.52\n",
      "79.43\n",
      "80.01\n",
      "80.29\n",
      "71\n",
      "73\n",
      "75\n",
      "77\n",
      "79\n",
      "81\n",
      "83\n",
      "85\n",
      "87\n",
      "dsc (%)\n",
      "c1\n",
      "c2\n",
      "image\n",
      "cu2l (ours)\n",
      "mt\n",
      "ssnet\n",
      "ahdc\n",
      "local: c1\n",
      "local: c2\n",
      "93.61%\n",
      "86.41%\n",
      "89.95%\n",
      "88.81%\n",
      "82.76%\n",
      "61.02%\n",
      "59.12%\n",
      "58.99%\n",
      "(a)\n",
      "(b)\n",
      "fig.\n",
      "data augmentation is applied, including random ﬂip and rotation.\n",
      "we adopt the\n",
      "dice similarity coeﬃcient (dsc) and jaccard as the evaluation metrics and the\n",
      "results are the average over three runs with diﬀerent seeds.\n",
      "all methods are imple-\n",
      "mented with the same backbone and training protocols to ensure fairness.\n",
      "as\n",
      "observed, compared to the supervised-only baselines, our cu2l with {6, 8} local\n",
      "labeled scans achieves {19.15%, 17.42%} and {9.1%, 6.44%} dsc improvements\n",
      "in {c1, c2}, showing its eﬀectiveness in leveraging multi-site unlabeled data.\n",
      "however, due to the lack of proper\n",
      "mechanisms for learning from heterogeneous data, limited improvement can be\n",
      "achieved by them, especially for cps [3] and fixmatch\n",
      "[2] is mediocre in ms-ssl, mainly due to the instabil-\n",
      "ity of adversarial training and the diﬃculty of aligning multiple distributions\n",
      "to the local distribution via a single image-mapping network.\n",
      "in contrast, with\n",
      "specialized mechanisms for simultaneously learning informative representations\n",
      "category-level regularized unlabeled-to-labeled learning\n",
      "11\n",
      "from multi-site data and handling heterogeneity, our cu2l obtains the best\n",
      "performance over the recent ssl methods.\n",
      "firstly, when we remove lu\n",
      "p l (cu2l-1), the performance drops by\n",
      "{5.69% (c1), 3.05%(c2)} in dsc, showing that reinforcing conﬁrmation on local\n",
      "distribution is critical.\n",
      "if we\n",
      "remove lcr which accompanies with lu2l (cu2l-3), the performance degrades,\n",
      "which justiﬁes the necessity of this regularization to reduce the diﬃculty of\n",
      "unlabeled-to-labeled learning process.\n",
      "as\n",
      "observed, such a typical stability loss [15] can further improve the performance\n",
      "by introducing hand-crafted noises to enhance the robustness to real-world het-\n",
      "erogeneity.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf:\n",
      "transformer and its variants have been widely used for\n",
      "medical image segmentation.\n",
      "we incorporate a group multi-axis hadamard product attention module\n",
      "(ghpa) and a group aggregation bridge module (gab) in a lightweight\n",
      "manner.\n",
      "the gab eﬀectively fuses multi-\n",
      "scale information by grouping low-level features, high-level features, and\n",
      "a mask generated by the decoder at each stage.\n",
      "comprehensive experi-\n",
      "ments on the isic2017 and isic2018 datasets demonstrate that ege-\n",
      "unet outperforms existing state-of-the-art methods.\n",
      "in short, compared\n",
      "to the transfuse, our model achieves superior segmentation performance\n",
      "while reducing parameter and computation costs by 494x and 160x,\n",
      "respectively.\n",
      "keywords: medical image segmentation · light-weight model ·\n",
      "mobile health\n",
      "1\n",
      "introduction\n",
      "malignant melanoma is one of the most rapidly growing cancers in the world.\n",
      "thus, an automated skin lesion\n",
      "segmentation system is imperative, as it can assist medical professionals in\n",
      "swiftly identifying lesion areas and facilitating subsequent treatment processes.\n",
      "to enhance the segmentation performance, recent studies tend to employ mod-\n",
      "ules with larger parameter and computational complexity, such as incorporat-\n",
      "ing self-attention mechanisms of vision transformer (vit)\n",
      "[4], based on the swin transformer [11], leverages the feature extrac-\n",
      "tion ability of self-attention mechanisms to improve segmentation performance.\n",
      "1. (a) and (b) respectively show the visualization of comparative experimental\n",
      "results on the isic2017 and isic2018 datasets.\n",
      "[5] has pioneered a serial fusion of cnn and vit for medical image\n",
      "segmentation.\n",
      "[8] utilizes\n",
      "a hybrid hierarchical architecture, eﬃcient bidirectional attention, and semantic\n",
      "maps to achieve global multi-scale feature fusion, combining the strengths of\n",
      "cnn and vit.\n",
      "transbts [23] introduces self-attention into brain tumor seg-\n",
      "mentation tasks and uses it to aggregate high-level information.\n",
      "prior works have enhanced performance by introducing intricate modules,\n",
      "but neglected the constraint of computational resources in real medical settings.\n",
      "hence, there is an urgent need to design a low-parameter and low-computational\n",
      "load model for segmentation tasks in mobile healthcare.\n",
      "[21] to develop a lightweight model that\n",
      "attains superior performance, while diminishing parameter and computation.\n",
      "furthermore, malunet [19] has reduced the model size by declining the num-\n",
      "ber of model channels and introducing multiple attention modules, resulting in\n",
      "better performance for skin lesion segmentation than unext.\n",
      "however, while\n",
      "malunet greatly reduces the number of parameter and computation, its seg-\n",
      "mentation performance is still lower than some large models, such as trans-\n",
      "fuse.\n",
      "therefore, in this study, we propose ege-unet, a lightweight skin lesion\n",
      "segmentation model that achieves state-of-the-art while signiﬁcantly reducing\n",
      "parameter and computation costs.\n",
      "to be speciﬁc, ege-unet leverages two key modules: the group multi-axis\n",
      "hadamard product attention module (ghpa) and group aggregation bridge\n",
      "ege-unet: an eﬃcient group enhanced unet\n",
      "483\n",
      "module (gab).\n",
      "mhsa\n",
      "divides the input into multiple heads and calculates self-attention in each head,\n",
      "which allows the model to obtain information from diverse perspectives, integrate\n",
      "diﬀerent knowledge, and improve performance.\n",
      "on the other hand, for gab, since the size\n",
      "and shape of segmentation targets in medical images are inconsistent, it is essen-\n",
      "tial to obtain multi-scale information [19].\n",
      "via combining the\n",
      "above two modules with unet, we propose ege-unet, which achieves excel-\n",
      "lent segmentation performance with extremely low parameter and computation.\n",
      "unlike previous approaches that focus solely on improving performance, our\n",
      "model also prioritizes usability in real-world environments.\n",
      "(2) we propose ege-\n",
      "unet, an extremely lightweight model designed for skin lesion segmentation.\n",
      "(3) we conduct extensive experiments, which demonstrate the eﬀectiveness of\n",
      "our methods in achieving state-of-the-art performance with signiﬁcantly lower\n",
      "resource requirements.\n",
      "the encoder is composed of six stages,\n",
      "each with channel numbers of {8, 16, 24, 32, 48, 64}.\n",
      "while the ﬁrst three\n",
      "stages employ plain convolutions with a kernel size of 3, the last three stages\n",
      "utilize the proposed ghpa to extract representation information from diverse\n",
      "perspectives.\n",
      "in contrast to the simple skip connections in unet, ege-unet\n",
      "incorporates gab for each stage between the encoder and decoder.\n",
      "furthermore,\n",
      "our model leverages deep supervision\n",
      "via the integration of these advanced modules, ege-unet signiﬁcantly\n",
      "reduces the parameter and computational load while enhancing the segmentation\n",
      "performance compared to prior approaches.\n",
      "we divide the input into four groups equally along the channel dimension\n",
      "and perform hpa on the height-width, channel-height, and channel-width axes\n",
      "for the ﬁrst three groups, respectively.\n",
      "finally, we concatenate the four groups along the channel\n",
      "dimension and apply another dw to integrate the information from diﬀerent\n",
      "perspectives.\n",
      "the acquisition of multi-scale informa-\n",
      "tion is deemed pivotal for dense prediction tasks, such as medical image segmen-\n",
      "tation.\n",
      "secondly, we\n",
      "partition both feature maps into four groups along the channel dimension, and\n",
      "concatenate one group from the low-level features with one from the high-level\n",
      "features to obtain four groups of fused features.\n",
      "finally, the four groups are con-\n",
      "catenated along the channel dimension, followed by the application of a plain\n",
      "convolution with the kernel size of 1 to enable interaction among features at\n",
      "diﬀerent scales.\n",
      "[27] is employed to calculate the loss function\n",
      "for diﬀerent stages, in order to generate more accurate mask information.\n",
      "λi is the weight\n",
      "for diﬀerent stage.\n",
      "3\n",
      "experiments\n",
      "datasets and implementation details.\n",
      "to assess the eﬃcacy of our model,\n",
      "we select two public skin lesion segmentation datasets, namely isic2017\n",
      "[2,6], containing 2150 and 2694 dermoscopy images, respectively.\n",
      "all experiments are per-\n",
      "formed on a single nvidia rtx a6000 gpu.\n",
      "the images are normalized and\n",
      "resized to 256 × 256.\n",
      "we apply various data augmentation, including horizontal\n",
      "ﬂipping, vertical ﬂipping, and random rotation.\n",
      "the comparative experimental results presented in\n",
      "table 1 reveal that our ege-unet exhibits a comprehensive state-of-the-art per-\n",
      "formance on the isic2017 dataset.\n",
      "speciﬁcally, in contrast to larger models,\n",
      "such as transfuse, our model not only demonstrates superior performance, but\n",
      "also signiﬁcantly curtails the number of parameter and computation by 494x\n",
      "and 160x, respectively.\n",
      "in comparison to other lightweight models, ege-unet\n",
      "surpasses unext-s with a miou improvement of 1.55% and a dsc improvement\n",
      "of 0.97%, while exhibiting parameter and computation reductions of 17% and\n",
      "72% of unext-s. furthermore, ege-unet outperforms malunet with a miou\n",
      "improvement of 1.03% and a dsc improvement of 0.64%, while reducing param-\n",
      "eter and computation to 30% and 85% of malunet.\n",
      "for the isic2018 dataset,\n",
      "the performance of our model also outperforms that of the best-performing\n",
      "model.\n",
      "comparative experimental results on the isic2017 and isic2018 dataset.\n",
      "(c) the micro ablation on gab.\n",
      "type model\n",
      "params(m)↓ gflops↓ miou(%)↑ dsc(%)↑\n",
      "(a)\n",
      "baseline\n",
      "0.107\n",
      "0.076\n",
      "76.30\n",
      "86.56\n",
      "baseline + ghpa\n",
      "0.034\n",
      "0.058\n",
      "78.82\n",
      "88.16\n",
      "baseline + gab\n",
      "0.126\n",
      "0.086\n",
      "78.78\n",
      "88.13\n",
      "(b)\n",
      "w/o multi-axis grouping\n",
      "0.074\n",
      "0.074\n",
      "79.13\n",
      "88.35\n",
      "w/o dw for initialized tensor 0.050\n",
      "0.072\n",
      "79.03\n",
      "88.29\n",
      "(c)\n",
      "w/o mask information\n",
      "0.052\n",
      "0.070\n",
      "78.97\n",
      "88.25\n",
      "w/o dilation rate of conv2d\n",
      "0.053\n",
      "0.072\n",
      "79.11\n",
      "88.34\n",
      "reducing parameter to about 50kb with excellent segmentation performance.\n",
      "figure 1 presents a more clear visualization of the experimental ﬁndings and\n",
      "fig.\n",
      "4 shows some segmentation results.\n",
      "we conduct extensive ablation experiments to demonstrate\n",
      "the eﬀectiveness of our proposed modules.\n",
      "the baseline utilized in our work is\n",
      "referenced from malunet [19], which employs a six-stage u-shaped architecture\n",
      "488\n",
      "j. ruan et al.\n",
      "fig.\n",
      "each stage includes a plain\n",
      "convolution operation with a kernel size of 3, and the number of channels at\n",
      "each stage is set to {8, 16, 24, 32, 48, 64}.\n",
      "secondly, we substitute the\n",
      "skip-connection operation in baseline with gab, resulting in further improved\n",
      "performance.\n",
      "furthermore, we substitute the dilated convolutions in gab with plain\n",
      "convolutions, which also leads to a reduction in performance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf:\n",
      "modern deep learning methods for semantic segmentation\n",
      "require labor-intensive labeling for large-scale datasets with dense pixel-\n",
      "level annotations.\n",
      "recent data augmentation methods such as dropping,\n",
      "mixing image patches, and adding random noises suggest eﬀective ways to\n",
      "address the labeling issues for natural images.\n",
      "however, they can only be\n",
      "restrictively applied to medical image segmentation as they carry risks of\n",
      "distorting or ignoring the underlying clinical information of local regions\n",
      "of interest in an image.\n",
      "in this paper, we propose a novel data augmen-\n",
      "tation method for medical image segmentation without losing the seman-\n",
      "tics of the key objects (e.g., polyps).\n",
      "our\n",
      "proposed method signiﬁcantly outperforms various existing methods with\n",
      "high sensitivity and dice scores and extensive experiment results with mul-\n",
      "tiple backbones on two datasets validate its eﬀectiveness.\n",
      "keywords: adversarial attack and defense · data augmentation ·\n",
      "semantic segmentation\n",
      "1\n",
      "introduction\n",
      "semantic segmentation aims to segment objects in an image by classifying each\n",
      "pixel into an object class.\n",
      "training a deep neural network (dnn) for such a task\n",
      "is known to be data-hungry, as labeling dense pixel-level annotations requires\n",
      "laborious and expensive human eﬀorts in practice [23,32].\n",
      "furthermore, semantic\n",
      "segmentation in medical imaging suﬀers from privacy and data sharing issues [13,\n",
      "35] and a lack of experts to secure accurate and clinically meaningful regions of\n",
      "interest (rois).\n",
      "this data shortage problem causes overﬁtting for training dnns,\n",
      "resulting in the networks being biased by outliers and ignorant of unseen data.\n",
      "to alleviate the sample size and overﬁtting issues, diverse data augmentations\n",
      "have been recently developed.\n",
      "for example, cutmix [31] and cutout [4] aug-\n",
      "ment images by dropping random-sized image patches or replacing the removed\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14223, pp. 555–566, 2023.\n",
      "https://doi.org/10.1007/978-3-031-43901-8_53\n",
      "556\n",
      "h. cho et al.\n",
      "regions with a patch from another image.\n",
      "geometric transforma-\n",
      "tions such as elastic transformation [26] warp images and deform the original\n",
      "shape of objects.\n",
      "alternatively, feature perturbation methods augment data by\n",
      "perturbing data in feature space [7,22] and logit space [9].\n",
      "although these augmentation approaches have been successful for natural\n",
      "images, their usage for medical image semantic segmentation is quite restricted\n",
      "as objects in medical images contain non-rigid morphological characteristics that\n",
      "should be sensitively preserved.\n",
      "for example, basalioma (e.g., pigmented basal\n",
      "cell carcinoma) may look similar to malignant melanoma or mole in terms of color\n",
      "and texture [6,20], and early-stage colon polyps are mostly small and indistin-\n",
      "guishable from background entrail surfaces\n",
      "in these cases, the underlying\n",
      "clinical features of target rois (e.g., polyp, tumor and cancer) can be distorted if\n",
      "regional colors and textures are modiﬁed with blur-based augmentations or geo-\n",
      "metric transformations.\n",
      "considering the rois are usually small and under-\n",
      "represented compared to the backgrounds, the loss of information may cause a\n",
      "fatal class imbalance problem in semantic segmentation tasks.\n",
      "in these regards, we tackle these issues with a novel augmentation method\n",
      "without distorting the semantics of objects in image space.\n",
      "we ﬁrst augment hard samples with adversarial attacks [18]\n",
      "that deceive a network and defend against such attacks with anti-adversaries.\n",
      "from active learning\n",
      "perspective [12,19], as vague samples near the decision boundary are augmented\n",
      "and trained, improvement on a downstream prediction task is highly expected.\n",
      "we summarize our main contributions as follows: 1) we propose a novel\n",
      "online data augmentation method for semantic segmentation by imposing object-\n",
      "speciﬁc consistency regularization between anti-adversarial and adversarial data.\n",
      "2) our method provides a ﬂexible regularization between diﬀerently perturbed\n",
      "data such that a vulnerable network is eﬀectively trained on challenging sam-\n",
      "ples considering their ambiguities.\n",
      "3) our method preserves underlying mor-\n",
      "phological characteristics of medical images by augmenting data with quasi-\n",
      "imperceptible perturbation.\n",
      "as a result, our method signiﬁcantly improves sen-\n",
      "sitivity and dice scores over existing augmentation methods on kvasir-seg [11]\n",
      "and etis-larib polyp db [25] benchmarks for medical image segmentation.\n",
      "(color ﬁgure online)\n",
      "2\n",
      "preliminary: adversarial attack and anti-adversary\n",
      "adversarial\n",
      "attack\n",
      "is\n",
      "an\n",
      "input\n",
      "perturbation\n",
      "method\n",
      "that\n",
      "adds\n",
      "quasi-\n",
      "imperceptible noises into images to deceive a dnn.\n",
      "given an image x, let μ\n",
      "be a noise bounded by l∞-norm.\n",
      "while the diﬀerence between x and the per-\n",
      "turbed sample x′ = x + μ is hardly noticeable to human perception, a network\n",
      "fθ(·) can be easily fooled (i.e., fθ(x)\n",
      "3\n",
      "method\n",
      "let {xi}n\n",
      "i=1 be an image set with n samples each paired with correspond-\n",
      "ing ground truth pixel-level annotations yi.\n",
      "our proposed method aims to 1)\n",
      "generate realistic images with adversarial attacks and 2) train a segmentation\n",
      "model fθ(xi)\n",
      "= yi for robust semantic segmentation with anti-adversarial con-\n",
      "sistency regularization (aac).\n",
      "figure 2 shows the overall training scheme with\n",
      "three phases: 1) online data augmentation, 2) computing adaptive aac between\n",
      "558\n",
      "h. cho et al.\n",
      "fig.\n",
      "adversarial and anti-adversarial perturba-\n",
      "tions are iteratively performed for the objects of a given image xi. adversarial noise\n",
      "µ−\n",
      "i,k moves xi across the decision boundary, whereas anti-adversarial noise µ+\n",
      "i,k pushes\n",
      "xi away from the boundary.\n",
      "downstream consistency regularization loss rcon mini-\n",
      "mizes the gap between adversaries {x−\n",
      "i,k}k\n",
      "k=1 and anti-adversary x+\n",
      "i,k.\n",
      "diﬀerently perturbed samples, and 3) updating the segmentation model using\n",
      "the loss from the augmented and original data.\n",
      "first, we generate plausible\n",
      "images with iterative adversarial and anti-adversarial perturbations.\n",
      "data augmentation with object-targeted adversarial attack.\n",
      "in many\n",
      "medical applications, false negatives (i.e., failing to diagnose a critical disease)\n",
      "are much more fatal than false positives.\n",
      "to do so, we ﬁrst exclude the\n",
      "background and perturb only the objects in the given image.\n",
      "given ˆxi,k as a perturbed sample at k-th step (k = 1, ..., k), the adversarial and\n",
      "anti-adversarial perturbations use the same initial image as x−\n",
      "i,0 = ˆ\n",
      "xi and x+\n",
      "(2)\n",
      "in contrast to the adversarial attack in eq. 1, the anti-adversarial noise μ+\n",
      "i,k =\n",
      "argminμl(fθ(x+\n",
      "i,k+1), yi) manipulates samples to increase the classiﬁcation\n",
      "score.\n",
      "note that, generating noises and images are online and training-free as the\n",
      "loss derivatives are calculated with freezed network parameters.\n",
      "speciﬁcally, if\n",
      "x′\n",
      "i is a harder sample to predict than x+\n",
      "i,k, i.e., l(p ′\n",
      "i, yi)>l(p +\n",
      "i,k, yi), the weight\n",
      "gets larger, and thus consistency regularization is intensiﬁed between the images.\n",
      "training a segmentation network.\n",
      "let ˆy +\n",
      "i,k be a segmentation outcome, i.e.,\n",
      "one-hot encoded pseudo-label from the network output p +\n",
      "i,k of anti-adversary\n",
      "x+\n",
      "i,k. given xi and {x−\n",
      "i,k}k\n",
      "k=1 as training data, the supervised segmentation\n",
      "loss lsup and the consistency regularization rcon are deﬁned as\n",
      "lsup = 1\n",
      "n\n",
      "n\n",
      "\u0003\n",
      "i=1\n",
      "l(pi, yi) +\n",
      "1\n",
      "nk\n",
      "n\n",
      "\u0003\n",
      "i=1\n",
      "k\n",
      "\u0003\n",
      "k=1\n",
      "l(p −\n",
      "i,k, yi)\n",
      "and\n",
      "(4)\n",
      "rcon = 1\n",
      "n\n",
      "n\n",
      "\u0003\n",
      "i=1\n",
      "w(xi, x+\n",
      "i,k)l(pi, ˆy +\n",
      "i,k) +\n",
      "1\n",
      "nk\n",
      "n\n",
      "\u0003\n",
      "i=1\n",
      "k\n",
      "\u0003\n",
      "k=1\n",
      "w(x−\n",
      "i,k, x+\n",
      "i,k)l(p −\n",
      "i,k, ˆy +\n",
      "i,k).\n",
      "with a hyperparame-\n",
      "ter α, the whole training loss l = lsup+αrcon is minimized via backpropagation\n",
      "to optimize the network parameters for semantic segmentation.\n",
      "4\n",
      "experiments\n",
      "4.1\n",
      "experimental setup\n",
      "dataset.\n",
      "we conducted experiments on two representative public polyp segmen-\n",
      "tation datasets: kvasir-seg [11] and etis-larib polyp db [25] (etis).\n",
      "the images of kvasir-seg were\n",
      "resized to 512 × 608 (h × w) and that of etis was set with 966 × 1255 resolu-\n",
      "tion.\n",
      "implementation.\n",
      "we implemented our method on pytorch framework with 4\n",
      "nvidia rtx a6000 gpus.\n",
      "along with conventional augmentation methods (i.e., random hori-\n",
      "zontal and vertical ﬂipping denoted as ‘basic’ in table 1), recent methods such\n",
      "as cutmix [31], cutout [4], elastic transform [26], random erase [33], drop-\n",
      "block [7], gaussian noise training (gnt)\n",
      "the basic augmentation was used in\n",
      "all methods including ours by default.\n",
      "for the training, we used k augmented\n",
      "images with the given images for all baselines as in ours for a fair comparison.\n",
      "evaluation.\n",
      "as the evaluation metric, mean intersection\n",
      "over union (miou) and mean dice coeﬃcient (mdice) are used for all experi-\n",
      "ments on test sets.\n",
      "additionally, we provide recall and precision scores to oﬀer\n",
      "a detailed analysis of class-speciﬁc misclassiﬁcation performance.\n",
      "note\n",
      "that, all baselines showed improvements in most cases.\n",
      "performance comparison with existing data augmentation methods.\n",
      "3. comparisons of precision and recall on the test set of kvasir-seg with u-net.\n",
      "performed better even compared with the tumorcp which uses seven diﬀer-\n",
      "ent augmentations methods together for tumor segmentation.\n",
      "this is because\n",
      "our method preserves the semantics of the key rois with small but eﬀective\n",
      "noises unlike geometric transformations [26,30], drop and cut-and-paste-based\n",
      "methods [4,7,30,31,33].\n",
      "also, as we augment uncertain samples that deliber-\n",
      "ately deceive a network as in active learning [12,16], our method is able to\n",
      "sensitively include the challenging (but roi-relevant) features into prediction,\n",
      "unlike existing noise-based methods that extract noises from known distributions\n",
      "[9,22,30].\n",
      "562\n",
      "h. cho et al.\n",
      "fig.\n",
      "(color ﬁgure\n",
      "online)\n",
      "4.3\n",
      "analysis on anti-adversaries and adversaries\n",
      "in fig. 4, we visualize data augmentation results with (anti-) adversarial pertur-\n",
      "bations on kvasir-seg dataset.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_20.pdf:\n",
      "nuclei appear small in size, yet, in real clinical practice, the\n",
      "global spatial information and correlation of the color or brightness con-\n",
      "trast between nuclei and background, have been considered a crucial com-\n",
      "ponent for accurate nuclei segmentation.\n",
      "however, the ﬁeld of automatic\n",
      "nuclei segmentation is dominated by convolutional neural networks\n",
      "(cnns), meanwhile, the potential of the recently prevalent transformers\n",
      "has not been fully explored, which is powerful in capturing local-global\n",
      "correlations.\n",
      "to this end, we make the ﬁrst attempt at a pure trans-\n",
      "former framework for nuclei segmentation, called transnuseg.\n",
      "diﬀerent\n",
      "from prior work, we decouple the challenging nuclei segmentation task\n",
      "into an intrinsic multi-task learning task, where a tri-decoder structure\n",
      "is employed for nuclei instance, nuclei edge, and clustered edge segmen-\n",
      "tation respectively.\n",
      "experi-\n",
      "ments on two datasets of diﬀerent modalities, including monuseg have\n",
      "shown that our methods can outperform state-of-the-art counterparts\n",
      "such as ca2.5-net by 2–3% dice with 30% fewer parameters.\n",
      "in con-\n",
      "clusion, transnuseg conﬁrms the strength of transformer in the context\n",
      "of nuclei segmentation, which thus can serve as an eﬃcient solution for\n",
      "real clinical practice.\n",
      "keywords: lightweight multi-task framework · shared attention\n",
      "heads · nuclei · edge and clustered edge segmentation\n",
      "1\n",
      "introduction\n",
      "accurate cancer diagnosis, grading, and treatment decisions from medical images\n",
      "heavily rely on the analysis of underlying complex nuclei structures [7].\n",
      "1. semantic illustrations of the nuclei segmentation networks with diﬀerent num-\n",
      "bers of decoders.\n",
      "(a) sole-decoder to perform a single task of nuclei segmentation.\n",
      "(b)\n",
      "bi-decoder to segment nuclei and locate nuclei edges simultaneously.\n",
      "to the numerous nuclei contained in a digitized whole-slide image (wsi), or even\n",
      "in an image patch of deep learning input, dense annotation of nuclei contour-\n",
      "ing is extremely time-consuming and labor-expensive [11].\n",
      "consequently, auto-\n",
      "mated nuclei segmentation approaches have emerged to satisfy a broad range\n",
      "of computer-aided diagnostic systems, where the deep learning methods, partic-\n",
      "ularly the convolutional neural networks\n",
      "[3] with bi-decoder structure achieves improved instance segmentation\n",
      "performance by adopting multi-task learning, in which one decoder learns to\n",
      "segment the nuclei and the other recognizes edges as described in fig.\n",
      "[20] extends dcan with an extra information aggregator to fuse\n",
      "the features from two decoders for more precise segmentation.\n",
      "much recently,\n",
      "ca2.5-net [6] shows identifying the clustered edges in a multiple-task learning\n",
      "manner can achieve higher performance, and thereby proposes an extra out-\n",
      "put path to learn the segmentation of clustered edges explicitly.\n",
      "a signiﬁcant\n",
      "drawback of the aforementioned multi-decoder networks is the ignorance of the\n",
      "prediction consistency between branches, resulting in sub-optimal performance\n",
      "and missing correlations between the learned branches.\n",
      "speciﬁcally, a prediction\n",
      "mismatch between the nuclei and edge branches is observed in previous work [8],\n",
      "implying a direction for performance improvement.\n",
      "he et al.\n",
      "segmentation tasks.\n",
      "inspired by the capability in long-range global context cap-\n",
      "turing by transformers [17], we make the ﬁrst attempt to construct a tri-decoder\n",
      "based transformer model to segment nuclei.\n",
      "in short, our major contributions\n",
      "are three-fold: (1) we propose a novel multi-task framework for nuclei segmenta-\n",
      "tion, namely transnuseg, as the ﬁrst attempt at a fully swin-transformer driven\n",
      "architecture for nuclei segmentation.\n",
      "furthermore, the incorporation of a light-weighted mlp bottleneck\n",
      "leads to a sharp reduction of parameters at no cost of performance decline.\n",
      "fig.\n",
      "[13] as\n",
      "the building blocks to capture the long-range feature correlations in the nuclei\n",
      "segmentation context.\n",
      "our network consists of three individual output decoder\n",
      "paths for nuclei segmentation, normal edges segmentation, and clustered edges\n",
      "segmentation.\n",
      "to capture the strong correlation between nuclei\n",
      "segmentation and contour segmentation between multiple decoders [15], we intro-\n",
      "duce a novel attention sharing scheme that is designed as an enhancement to\n",
      "the multi-headed self-attention (msa) module in the plain transformer [17].\n",
      "[·] writes for the concatenation, sa(·) denotes the self-attention head whose\n",
      "output dimension is dh, and uu\n",
      "msa ∈ r(m+n)·dh×d is a learnable matrix.\n",
      "to reduce the complexity of the model, we leverage a\n",
      "token mlp bottleneck as a light-weight alternative for the swin transformer bot-\n",
      "tleneck.\n",
      "to alleviate the inconsistency between the con-\n",
      "tour generated from the nuclei segmentation prediction and the predicted edge,\n",
      "210\n",
      "z. he et al.\n",
      "we propose a novel consistency self distillation loss, denoted as lsd.\n",
      "we employ a multi-task learning paradigm\n",
      "to train the tri-decoder network, aiming to improve model performance by\n",
      "leveraging the additional supervision signal from edges.\n",
      "particularly, the nuclei\n",
      "semantic segmentation is considered the primary task, while the normal edge\n",
      "and clustered edge semantic segmentation are viewed as auxiliary tasks.\n",
      "subsequently, the overall loss l is calculated as a\n",
      "weighted summation of semantic nuclei mask loss (ln), normal edge loss (le),\n",
      "and clustered edge loss (lc), and the self distillation loss (lsd) i. e.\n",
      "l = γn · ln + γe · le + γc · lc + γsd · lsd, where coeﬃcients γn, γe and γc are\n",
      "set to 0.30, 0.35, 0.35 respectively, and γsd is initially set to 1 with a 0.3 decrease\n",
      "for every 10 epochs until it reaches 0.4.\n",
      "3\n",
      "experiments\n",
      "dataset.\n",
      "(1) fluores-\n",
      "cence microscopy image dataset: this set combines three diﬀerent data sources\n",
      "to simulate the heterogeneous nature of medical images [9].\n",
      "it consists of 524\n",
      "ﬂuorescence images, each with a resolution of 512 × 512 pixels.\n",
      "(2) histology\n",
      "image dataset: this set is the combination of the open dataset monuseg [10]\n",
      "and another private histology dataset\n",
      "[8] of 462 images.\n",
      "we crop each image\n",
      "in the monuseg dataset into four partially overlapping 512 × 512 images.\n",
      "fig.\n",
      "the best performance with\n",
      "respect to each metric is highlighted in boldface.\n",
      "the private dataset contains 300 images sized at 512 × 512 tessellated from 50\n",
      "wsis scanned at 20×, and meticulously labeled by ﬁve pathologists according\n",
      "to the labeling guidelines of the monuseg [10].\n",
      "table 2. comparison of the model complexity in terms of the number of parameters,\n",
      "flops, as well as the training cost in the form of the averaged training time per epoch.\n",
      "the average training time is computed using the same batch size for both datasets, with\n",
      "the ﬁrst number indicating the averaged time on the fluorescence microscopy image\n",
      "dataset and the second on the histology image dataset.\n",
      "4. exemplary samples and their segmentation results using diﬀerent methods.\n",
      "transnuseg demonstrates superior segmentation performance compared to its counter-\n",
      "parts, which can successfully distinguish severely clustered nuclei from normal edges.\n",
      "implementations.\n",
      "all experiments are performed on one nvidia rtx 3090\n",
      "gpu with 24 gb memory.\n",
      "table 1 shows the quantitative comparisons for the nuclei segmen-\n",
      "tation.\n",
      "the large margin between the swinunet and the other cnn-based or\n",
      "hybrid networks also conﬁrms the superiority of the transformer in ﬁne-grained\n",
      "nuclei segmentation.\n",
      "for example, in the histology image\n",
      "dataset, transnuseg improves the dice score, f1 score, accuracy, and iou by\n",
      "2.08%, 3.41%, 1.25%, and 2.70% respectively, over the second-best models.\n",
      "simi-\n",
      "larly, in the ﬂuorescence microscopy image dataset, our proposed model improves\n",
      "dsc by 0.96%, while also leading to 1.65%, 1.03% and 1.91% increment in\n",
      "f1 score, accuracy, and iou to the second-best performance.\n",
      "for better visu-\n",
      "alization, representative samples and their segmentation results using diﬀerent\n",
      "methods are demonstrated in fig.\n",
      "furthermore, table 2 compares the model\n",
      "complexity in terms of the number of parameters, ﬂoating point operations per\n",
      "second (flops), and the training computational cost, where our approach can\n",
      "signiﬁcantly reduce around 28% of the training time compared to the state-of-\n",
      "the-art cnn multi-task method ca2.5-net, while also boosting performance.\n",
      "our ablation study yields that token mlp bottleneck and attention\n",
      "sharing schemes can complementarily reduce the training cost while increasing\n",
      "eﬃciency, as shown in table 2 (the last 4 rows).\n",
      "as described in table 3, each component\n",
      "proportionally contributes to the improvement to reach the overall performance\n",
      "fig.\n",
      "(a) raw input image.\n",
      "segmentation results by transnuseg trained (b) w/o\n",
      "self distillation, and (c) w/ self distillation.\n",
      "accordingly, the numbers below images indicate the proportion of the\n",
      "pixels belonging to the three parts.\n",
      "compared to the results without self distillation,\n",
      "the outputs with self distillation exhibit reduced mismatches, resulting in improved\n",
      "segmentation performance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf:\n",
      "one task in diagnosing\n",
      "lyme disease is lesion segmentation, i.e., separating benign skin from\n",
      "lesions, which can not only help clinicians to focus on lesions but also\n",
      "improve downstream tasks such as disease classiﬁcation.\n",
      "however, it is\n",
      "challenging to segment lyme disease lesions due to the lack of well-\n",
      "segmented, labeled lyme datasets and the nature of lyme, e.g., the\n",
      "typical bull’s eye lesion and its closeness to normal skin.\n",
      "in this paper,\n",
      "we design a simple yet novel data preprocessing and alteration method,\n",
      "called edgemixup, to help segment lyme lesions on imbalanced training\n",
      "datasets.\n",
      "the key insight is to deploy a linear combination of lesion edge,\n",
      "either detected or computed, and the source image highlights the aﬀected\n",
      "lesion area so that a learning model focuses more on the preserved lesion\n",
      "structure instead of skin tone, thus iteratively improving segmentation\n",
      "performance.\n",
      "additionally, the improved edge from lesion segmentation\n",
      "can be further used for lyme disease classiﬁcation—e.g., in diﬀerentiat-\n",
      "ing lyme from other similar lesions including tinea corporis and herpes\n",
      "zoster—with improved model fairness on diﬀerent subpopulations.\n",
      "1\n",
      "introduction\n",
      "medical image analysis has greatly beneﬁted from advances in ai\n",
      "[1] yet some\n",
      "improvements still remain to be addressed, importantly in areas that allow both\n",
      "algorithmic performance and fairness [2], and in certain medical applications that\n",
      "promise to signiﬁcantly lessen morbidity and mortality.\n",
      "early detection of skin\n",
      "lesions is such an endeavor as it can aid in identifying infectious diseases with\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43901-8_36.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43901-8_36\n",
      "edgemixup\n",
      "375\n",
      "cutaneous manifestations.\n",
      "the earliest\n",
      "and most treatable phase of lyme disease is manifested via a red concentric\n",
      "lesion at the site of a tick bite, called erythema migrans (em) [5].\n",
      "[7–9].\n",
      "one important diagnosis task is to segment lyme lesion, particularly the\n",
      "em pattern, from benign skins.\n",
      "such dl-assisted segmentation not only helps\n",
      "clinicians in pre-screening patients but also improves downstream tasks such as\n",
      "lesion classiﬁcation.\n",
      "however, while lyme disease lesion segmentation is intu-\n",
      "itively simple, it is challenging due to the following reasons.\n",
      "first, there lacks\n",
      "of a well-segmented dataset with manual labels on lyme disease.\n",
      "on one hand,\n",
      "some datasets—such as ham10000 [10] and isbi challenges [11]—have manual\n",
      "annotated segmentations for diseases like melanoma, but they do not have lyme\n",
      "disease lesions.\n",
      "[12]—have\n",
      "lyme disease and skin tone and classiﬁcation labels, but not segmentation.\n",
      "second, the segmentation of lyme lesion is itself challenging due to the nature\n",
      "of em pattern.\n",
      "therefore, existing skin disease\n",
      "segmentation\n",
      "[13] as well as existing general segmentation works, such as u-\n",
      "net [14], polar training\n",
      "[17], usually suﬀer\n",
      "from relatively low performance and reduced fairness [2,18,19].\n",
      "in this paper, we present the ﬁrst lyme disease dataset that contains labeled\n",
      "segmentation and skin tones.\n",
      "our lyme disease dataset contains two parts: (i)\n",
      "a classiﬁcation dataset, composed of more than 3,000 diseased skin images that\n",
      "are either obtained from public resources or clinicians with patient-informed con-\n",
      "sent, and (ii) a segmentation dataset containing 185 samples that are manually\n",
      "annotated for three regions—i.e., background, skin (light vs. dark), and lesion—\n",
      "conducted under clinician supervision and institutional review boards (irb)\n",
      "approval.\n",
      "our dataset with manual labels is available at this url\n",
      "secondly, we design a simple yet novel data preprocessing and alternation\n",
      "method, called edgemixup, to improve lyme disease segmentation and diagno-\n",
      "sis fairness on samples with diﬀerent skin-tones.\n",
      "the key insight is to alter a skin\n",
      "image with a linear combination of the source image and a detected lesion bound-\n",
      "ary so that the lesion structure is preserved while minimizing skin tone informa-\n",
      "tion.\n",
      "such an improvement is an iterative process that gradually improves lesion\n",
      "edge detection and segmentation fairness until convergence.\n",
      "a motivating example to illustrate why edgemixup improves model perfor-\n",
      "mance and reduces biases via mixing up lesion boundary with original image (heatmap\n",
      "is generated via grad-cam).\n",
      "(color ﬁgure online)\n",
      "converged edge in the ﬁrst step also helps classiﬁcation of lyme diseases via mixup\n",
      "with improved fairness.\n",
      "we evaluate edgemixup for skin disease segmentation and classiﬁcation\n",
      "tasks.\n",
      "our results show that edgemixup is able to increase segmentation utility\n",
      "and improve fairness.\n",
      "we also show that the improved segmentation further\n",
      "improves classiﬁcation fairness as well as joint fairness-utility metrics compared\n",
      "to existing debiasing methods, e.g., ad\n",
      "[21] and st-debias\n",
      "note that not all skin disease datasets are\n",
      "carefully processed either due to the large amount of work required or the scarcity\n",
      "of data samples collected, e.g., sd-198 [23] contains samples that are taken under\n",
      "variant environments.\n",
      "we\n",
      "keep all hyper-parameters exactly the same for two models, and only augment the\n",
      "same image with and without mixing lesion boundary up with the original image.\n",
      "figure 1 shows the original image (fig. 1a) as well as two models’\n",
      "attention as heat-maps where red color represents the highest attention, yellow a\n",
      "higher attention, and purple the least attention.\n",
      "the reason is that a legacy\n",
      "diagnosis has no information about lesion and does not know where to locate its\n",
      "focus, thus easily gets distracted by ﬁngers instead of the lesion pattern.\n",
      "3\n",
      "method\n",
      "in this section, we ﬁrst give the deﬁnition for model fairness, and we then\n",
      "describe the design of edgemixup for the purpose of de-biasing in fig.\n",
      "we consider any model f, either a classiﬁcation model fclass or\n",
      "a segmentation model fseg, to be biased against certain skin-tone st2 if given\n",
      "metrics m and samples xst1 and xst2 from class y, where st1 and st2 are diﬀer-\n",
      "ent skin-tones according to their ita scores, m(f(xst1), y) > m(f(xst2), y).\n",
      "edgemixup improves model fairness on light and dark skin samples in both\n",
      "segmentation and classiﬁcation tasks, and it has two major components: (i) edge\n",
      "detection using mixup, and (ii) data preprocessing and alteration for downstream\n",
      "tasks.\n",
      "more speciﬁcally, our proposed edge detection has two parts: initial edge\n",
      "detection and iterative improvement.\n",
      "initial edge detection: the purpose of initial detection, which is documented\n",
      "in the initial_edge_detection function of algorithm 1, is to provide a start-\n",
      "ing point, i.e., a rough boundary, for the next step of iterative improvement.\n",
      "first, edgemixup trains\n",
      "a classiﬁcation model based on a mixup of the ground-truth segmentation under\n",
      "clinician supervision and the original image (line 7).\n",
      "second, edgemixup gen-\n",
      "erates many edge candidates.\n",
      "note that the initial\n",
      "edge detection is irrelevant to the sample size of a particular subpopulation, thus\n",
      "improving the fairness.\n",
      "iterative edge improvement: edgemixup includes iterative edge improve-\n",
      "ment in the training phase of our segmentation model to further improve model\n",
      "utility.\n",
      "pseudo-code of edgemixup\n",
      "require: a labelled sample (x, y) ∈ d, mixup weights α, ground-truth edged training set dtrain\n",
      "edge_gt\n",
      "ensure: dataset dfinal_edge in which each sample has it lesion edge highlighted (xedge, y)\n",
      "1: function main( )\n",
      "2:\n",
      "dinitial_edge = initial_edge_detection(d, α)\n",
      "3:\n",
      "dfinal_edge = iterative_edge_improvement(dinitial_edge, α)\n",
      "4:\n",
      "return dfinal_edge\n",
      "5: end function\n",
      "6: function initial_edge_detection(d, α)\n",
      "7:\n",
      "train classiﬁcation model mclass using dtrain\n",
      "edge_gt\n",
      "8:\n",
      "for each sample x ∈ d do\n",
      "9:\n",
      "get all edge candidates {edge1, edge2, .., edgen} for each sample x\n",
      "10:\n",
      "mixup each edge candidate with x\n",
      "11:\n",
      "query mclass using all mixed-up {xedge1, ...xedgen} and choose the optimal edge edgeopt\n",
      "12:\n",
      "generate edged sample xedge\n",
      "= mixup(x, edgeopt, α)\n",
      "13:\n",
      "end for\n",
      "14:\n",
      "return dedge\n",
      "15: end function\n",
      "16: function iterative_edge_improvement(dedge, α)\n",
      "17:\n",
      "train the ﬁrst model miter using edged dataset dtrain\n",
      "edge\n",
      "18:\n",
      "evaluate miter using dtest\n",
      "edge and get current_jaccard\n",
      "19:\n",
      "best_jaccard\n",
      "3. illustration of iterative edge improvement on diﬀerent iterations with train loss\n",
      "mixup of detected edge and original image, given the lesion boundary feature\n",
      "detected in the previous iteration, the next-iteration segmentation model can\n",
      "converge better and the lesion boundary predicted by it is ﬁne-grained.\n",
      "speciﬁ-\n",
      "cally, edgemixup iteratively trains segmentation models from scratch, and we\n",
      "let the model trained in the previous iteration to predict lesion edge, which is\n",
      "then mixed-up with original training samples as the new training set for next-\n",
      "iteration model.\n",
      "annotated segmentation and classiﬁcation dataset characteristics, broken\n",
      "down by ita-based skin tones (light skin/ dark skin) and disease types.\n",
      "besides, edgemixup calculates a linear combination of original image and lesion\n",
      "boundary, i.e., by assigning the weight of original image as α and lesion bound-\n",
      "ary as 1 − α. figure 3 shows the edge-mixed-up images for diﬀerent iterations.\n",
      "first,\n",
      "we collect and annotate a dataset with 3,027 images containing three types of\n",
      "disease/lesions, i.e., tinea corporis (tc), herpes zoster (hz), and erythema\n",
      "migrans (em).\n",
      "all skin images are either collected from publicly available sources\n",
      "or from clinicians with patient informed consent.\n",
      "then, a medical technician\n",
      "and a clinician in our team manually annotate each image.\n",
      "for the segmentation\n",
      "task, we annotate skin images into three classes: background, skin, and lesion;\n",
      "then, for the classiﬁcation task, we annotate skin images by classifying them\n",
      "into four classes: no disease (no), tc, hz, and em.\n",
      "[23], a benchmark\n",
      "dataset for skin disease classiﬁcation, as another dataset for both segmentation\n",
      "and classiﬁcation tasks.\n",
      "note that due to the amount of manual work involved\n",
      "in annotation, we select those classes based on the number of samples in each\n",
      "class.\n",
      "we\n",
      "choose 30 samples in each class for segmentation task, and we split them into\n",
      "0.7, 0.1, and 0.2 ratio for training, validation, and testing, respectively.\n",
      "table 1 show the characteristics of these two datasets for both classiﬁcation\n",
      "and segmentation tasks broken down by the disease type and skin tone, as cal-\n",
      "culated by the individual typology angle (ita)\n",
      "one prominent observation is\n",
      "that ls images are more abundant than ds images due to a disparity in the avail-\n",
      "ability of ds imagery found from either public sources or from clinicians with\n",
      "patient consent.\n",
      "segmentation: performance and fairness (margin of error reported in paren-\n",
      "thesis)\n",
      "method\n",
      "unet\n",
      "polar\n",
      "mfsnet\n",
      "vit-adapter\n",
      "edgemixup\n",
      "skin\n",
      "jaccard 0.7053(0.0035) 0.7126(0.0033) 0.5877(0.0080) 0.7027(0.0057) 0.7807(0.0031)\n",
      "jgap\n",
      "0.0809(0.0001) 0.0813(0.0001) 0.1291(0.0076) 0.2346(0.0035) 0.0379(0.0001)\n",
      "sd-seg jaccard 0.7134(0.0031) 0.6527(0.0036) 0.6170(0.0052) 0.5088(0.0042) 0.7799(0.0031)\n",
      "jgap\n",
      "0.0753(0.0001) 0.1210(0.0003) 0.0636(0.0033) 0.2530(0.0021) 0.0528(0.0001)\n",
      "5\n",
      "evaluation\n",
      "we implement edgemixup using python 3.8 and pytorch, and all experiments\n",
      "are performed using one geforce rtx 3090 graphics card (nvidia).\n",
      "segmentation evaluation.\n",
      "our segmentation evaluation adopts four base-\n",
      "lines, (i) a u-net trained to segment skin lesions, (ii) a polar training [15]\n",
      "transforming images from cartesian coordinates to polar coordinates, (iii) vit-\n",
      "adapter\n",
      "[16], a state-of-the-art semantic segmentation using a ﬁne-tuned vit\n",
      "model, (iv) mfsnet [17], a segmentation model with diﬀerently scaled feature\n",
      "maps to compute the ﬁnal segmentation mask.\n",
      "our evaluation metrics include (i) jaccard index (iou\n",
      "score), which measures the similarity between a predicted mask and the manu-\n",
      "ally annotated ground truth, and (ii) the gap between jaccard values (jgap) to\n",
      "measure fairness.\n",
      "table 2 shows the performance and fairness of edgemixup and diﬀerent\n",
      "baselines.\n",
      "we compare predicted masks with the manually-annotated ground\n",
      "truth by calculating the jaccard index, and computing the gap for subpopula-\n",
      "tions with ls and ds (based on ita).\n",
      "edgemixup, a data preprocessing method,\n",
      "improves the utility of lesion segmentation in terms of jaccard index compared\n",
      "with all existing baselines.\n",
      "one reason is that edgemixup preserves skin lesion\n",
      "information, thus improving the segmentation quality, while attenuating markers\n",
      "for protected factors.\n",
      "note that edgemixup iteratively improves the segmen-\n",
      "tation results.\n",
      "our classiﬁcation evaluation involves: (i) adversar-\n",
      "ial debiasing (ad)\n",
      "[21], (ii) dexined-avg, the average version of dexined [26]\n",
      "as an boundary detector used by edgemixup, and (iii) st-debias\n",
      "[22], a debi-\n",
      "asing method augmenting data with conﬂicting shape and texture information.\n",
      "table 3 shows utility performance (acc and auc) and fairness results (gaps\n",
      "of acc and auc between ls and ds subpopulations).\n",
      "skin disease classiﬁcation and associated bias.\n",
      "(margin of error reported in parentheses, subpopulation reported\n",
      "in brackets)\n",
      "metrics\n",
      "resnet34\n",
      "baselines\n",
      "edgemixup (ours)\n",
      "ad\n",
      "dexined-avg st-debias\n",
      "u-net\n",
      "mask-based\n",
      "skin\n",
      "acc\n",
      "88.08(3.66) 81.79(4.35)\n",
      "69.87(5.17)\n",
      "76.52(5.23)\n",
      "86.75(3.82)\n",
      "[ds]\n",
      "70.59[ls]\n",
      "72.11[ls]\n",
      "75.00[ls]\n",
      "cai0.5\n",
      "–\n",
      "0.525\n",
      "−4.780\n",
      "2.795\n",
      "4.090\n",
      "6.995\n",
      "cai0.75\n",
      "–\n",
      "1.822\n",
      "−0.934\n",
      "6.127\n",
      "6.850\n",
      "10.06\n",
      "auc\n",
      "0.922(0.10)\n",
      "0.962(0.06)\n",
      "0.824(0.13)\n",
      "0.941(0.08)\n",
      "0.953(0.11)\n",
      "0.970(0.06)\n",
      "aucgap\n",
      "0.429(0.35)\n",
      "0.319(0.36)\n",
      "0.175(0.29)\n",
      "0.255(0.31)\n",
      "0.178(0.29)\n",
      "0.170(0.29)\n",
      "aucmin\n",
      "0.500[ds]\n",
      "0.650[ds]\n",
      "0.650[ds]\n",
      "0.711[ds]\n",
      "0.784[ds]\n",
      "0.800[ds]\n",
      "cauci0.5\n",
      "–\n",
      "0.075\n",
      "0.078\n",
      "0.097\n",
      "0.140\n",
      "0.153\n",
      "cauci0.75 –\n",
      "0.092\n",
      "0.166\n",
      "0.135\n",
      "0.196\n",
      "0.206\n",
      "the baseline unet model while “mask-based” implements deep-learning model\n",
      "involved methodology introduced in sect.\n",
      "3. by adding the “unet” variant, we\n",
      "demonstrate here that simply applying lesion edge predicetd by the baseline unet\n",
      "model, while not optimal, eﬃciently reduces model bias on diﬀerent skin-tone\n",
      "samples.\n",
      "edgemixup outperforms sota approaches in balancing the model’s\n",
      "performance and fairness, i.e., the caiα and cauciα values of edgemixup are\n",
      "the highest compared with the vanilla resnet34 and other baselines.\n",
      "6\n",
      "related work\n",
      "skin disease classiﬁcation and segmentation: previous researches mainly\n",
      "work on improving model utility for both medical image\n",
      "as for skin lesion segmentation tasks, few works has been proposed\n",
      "due to the lack of datasets with ground-truth segmentation masks.\n",
      "[11] to encourage researches studying lesion seg-\n",
      "mentation, feature detection, and image classiﬁcation.\n",
      "[10] only contains melanoma samples and all of the\n",
      "samples are with light skins according to our inspection using ita scores.\n",
      "382\n",
      "h. yuan et al.\n",
      "bias mitigation: researchers have addressed bias and heterogeneity in deep\n",
      "learning models [18,29].\n",
      "first, masking sensitive factors in imagery is shown\n",
      "to improve fairness in object detection and action recognition\n",
      "second,\n",
      "adversarial debiasing operates on the principle of simultaneously training two\n",
      "networks with diﬀerent objectives [31].\n",
      "as a comparison,\n",
      "edgemixup is an eﬀective preprocessing approach to debiasing when applied to\n",
      "skin disease particularly for lyme-focused classiﬁcation and segmentation tasks.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf:\n",
      "accurate segmentation of brain tumors in mri images\n",
      "requires precise detection of the edges.\n",
      "in this paper, we introduce the\n",
      "edge-oriented transformer (eoformer) which speciﬁcally captures and\n",
      "enhances edge information for brain tumor segmentation.\n",
      "the cnn structure captures low-\n",
      "level local features in the image, while the transformer structure estab-\n",
      "lishes long-range dependencies between features to generate high-level\n",
      "global features.\n",
      "experimental results on the brats 2020 dataset\n",
      "and a private medulloblastoma dataset demonstrate the superiority of\n",
      "our approach compared with existing state-of-the-art methods.\n",
      "keywords: brain tumor segmentation · edge-oriented module ·\n",
      "transformer\n",
      "1\n",
      "introduction\n",
      "accurate segmentation of brain tumors from mri images is of great signiﬁcance\n",
      "as it enables more accurate assessment of tumor morphology, size, location, and\n",
      "distribution range, thereby providing clinicians with a reliable basis for diagnosis\n",
      "and treatment [16].\n",
      "physicians manually delineate the tumor regions based on\n",
      "the varying signal intensities between diseased and normal tissues.\n",
      "this signal\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43901-8 32.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "al.\n",
      "disparity constitutes the edge information in the images, making it essential for\n",
      "accurate tumor segmentation.\n",
      "[8], have\n",
      "made signiﬁcant progress in the ﬁeld of medical image segmentation, including\n",
      "brain tumor segmentation.\n",
      "with the emergence of transformer [19], which is\n",
      "capable of modeling long-range dependencies that cnns struggle with, a num-\n",
      "ber of cnn-transformer hybrid networks have been proposed, such as transbts\n",
      "[21], unetr [7], swin-unetr [6] and nestedformer [23], leading to further\n",
      "improvements in brain tumor segmentation.\n",
      "however, the performance of exist-\n",
      "ing brain tumor segmentation methods are still unsatisfactory, especially for the\n",
      "segmentation of edges between tumor lesion and normal tissues.\n",
      "considerable advancement has been achieved in the ﬁeld of natural image\n",
      "segmentation by focusing on the edge information [3,11,18,25], and this idea\n",
      "is also being applied to medical image segmentation.\n",
      "other methods [1,12,20,22] involve post-processing uncertain regions\n",
      "to more accurately segment pixels near edges.\n",
      "[22] use the conﬁdence map to evaluate the uncertainty of each pixel to enhance\n",
      "the segmentation of the ambiguous edges of ultrasound images.\n",
      "however, the\n",
      "methods mentioned above are not suitable for brain tumor segmentation for two\n",
      "main reasons.\n",
      "[9] require the calculation\n",
      "of the hd at each iteration, which is both time-consuming and computationally\n",
      "demanding.\n",
      "moreover, processing every slice of large volumes of mri images at\n",
      "the pixel-level is impractical.\n",
      "unlike many other medical\n",
      "image segmentation tasks that involve the segmentation of a single roi, brain\n",
      "tumor segmentation requires the simultaneous segmentation of three regions:\n",
      "the whole tumor (wt), the tumor core (tc), and the enhancing tumor (et)\n",
      "regions.\n",
      "therefore, in addition to focusing on the edge between the tumor lesion\n",
      "and normal tissue to segment the wt, it is also necessary to consider the edges\n",
      "within the tumor in order to segment the tc and et regions.\n",
      "in this paper, we propose an edge-oriented transformer (eoformer), for\n",
      "eﬃcient and accurate brain tumor segmentation.\n",
      "speciﬁcally, the input image is ﬁrst processed by the cnn\n",
      "blocks to extract low-level local features.\n",
      "then, the extracted features are fed into\n",
      "the transformer blocks to create long-range dependencies, resulting in the forma-\n",
      "tion of high-level semantic features.\n",
      "in order to reduce the computational and\n",
      "eoformer: edge-oriented transformer for brain tumor segmentation\n",
      "335\n",
      "fig.\n",
      "our model has been evaluated on both the publicly brats 2020 dataset\n",
      "and a private medulloblastoma segmentation dataset.\n",
      "the results demonstrate\n",
      "that eoformer clearly outperforms the state-of-the-art methods with limited\n",
      "model parameters and lower flops (see more in supplementary material).\n",
      ", comprises four stages, each of which consists\n",
      "of a feature extraction module and a downsampling module.\n",
      "in the ﬁrst two stages of ehe, we use depth-wise convolution\n",
      "(dwconv) to instantiate the token mixer, called the convformer block.\n",
      "in the\n",
      "third stage and bottleneck, we use the multi-head self-attention (msa) to instan-\n",
      "tiate the token mixer, which is the typical transformer block.\n",
      "for each stage i,\n",
      "given an input feature map x, the output of the ith block x\n",
      "′′ is computed as\n",
      "follows:\n",
      "x\n",
      "′ = tokenmixeri(norm(x))\n",
      "assuming the size of the input feature is n and the dimensionality is d, the\n",
      "input feature x ∈ rn×d pass through three linear layers to generate the queries\n",
      "q ∈ rn×dk, keys k ∈ rn×dk and values v ∈ rn×dv.\n",
      "moreover, to boost the segmentation performance without sacriﬁcing eﬃciency,\n",
      "we incorporate the re-parameterization technique in the decoder.\n",
      "× 1 × 1 × 1 convolution to enhance the interaction between channel\n",
      "eoformer: edge-oriented transformer for brain tumor segmentation\n",
      "337\n",
      "features of x, then utilizes a learnable scaled sobel ﬁlter to extract the 1st-\n",
      "order diﬀerentiation edge information from x. this ﬁlter is capable of detecting\n",
      "edges in three directions (i.e. horizontal, vertical, and orthogonal directions), so\n",
      "it comprises three ﬁlters mx, my, and mz, each of which is represented by a 3\n",
      "× 3 × 3 array.\n",
      "we then apply a learnable scaling matrix s ∈ rc×1×1×1 to mx, which allows for\n",
      "dynamic adjustment of the scaling factor in each channel.\n",
      "we introduce the\n",
      "re-parameterization [4,5] into the edge-oriented modules to boost the segmenta-\n",
      "tion performance while maintaining high eﬃciency.\n",
      "+ bconvlap\n",
      "(7)\n",
      "where ‘∗’ represents the convolution operation, wconv means the weights of the\n",
      "convolution and bconv denotes the bias, and up(·) is the spatial broadcasting\n",
      "operation ,which upgrades the bias b ∈ r1×c×1×1×1 into up(b) ∈ r1×c×3×3×3.\n",
      "in the inference stage, the output feature f is produced by a normal 3 × 3 × 3\n",
      "convolution as follows:\n",
      "[15]\n",
      "76.76\n",
      "55.60\n",
      "66.18\n",
      "7.810\n",
      "9.411\n",
      "8.611\n",
      "transbts [21]\n",
      "72.35\n",
      "55.56\n",
      "63.96\n",
      "11.09\n",
      "11.19\n",
      "11.14\n",
      "unetr [7]\n",
      "73.38\n",
      "56.02\n",
      "64.70\n",
      "9.112\n",
      "12.70\n",
      "10.90\n",
      "swin-unetr [6]\n",
      "70.10\n",
      "60.79 65.44\n",
      "9.766\n",
      "9.339\n",
      "9.552\n",
      "nestedformer [23] 79.89 55.76\n",
      "67.83\n",
      "7.099\n",
      "12.08\n",
      "9.587\n",
      "eoformer\n",
      "79.74\n",
      "59.10\n",
      "69.42 6.978 7.104 7.041\n",
      "3\n",
      "experiment\n",
      "3.1\n",
      "dataset and evaluation metric\n",
      "in order to validate the performance of eoformer, we conduct extensive experi-\n",
      "ments on both the publicly available brats 2020 dataset and a private medul-\n",
      "loblastoma segmentation dataset (medseg).\n",
      "the brats 2020 dataset [14] consists of mri image data from 369 patients,\n",
      "with each patient having four modalities (t1, t1ce, t2 and t2-flair) of\n",
      "skull-striped mri, which are aligned to a standard brain template.\n",
      "the medseg dataset includes mri images of t1, t1ce, t2, and t2 flair\n",
      "modalities from 255 patients with medulloblastoma.\n",
      "the dataset includes manual\n",
      "annotations of the wt and et regions.\n",
      "the\n",
      "images are registered to the size of 24 × 256 × 256.\n",
      "eoformer: edge-oriented transformer for brain tumor segmentation\n",
      "339\n",
      "fig.\n",
      "2. qualitative comparison of segmentation results on brats and medseg.\n",
      "3.2\n",
      "implementation details\n",
      "we implement eoformer in pytorch 1.11.\n",
      "for data\n",
      "augmentation, we apply image croping, ﬂipping, identity scaling and shifting.\n",
      "table 1 displays the performance comparison of eoformer against other\n",
      "methods on the brats 2020 dataset.\n",
      "eoformer achieves the highest dice scores\n",
      "on tc, et, and the average.\n",
      "in addition, eoformer attains the best hd95\n",
      "scores on tc, et, and the average.\n",
      "table 2 illus-\n",
      "trates the performance of eoformer and other methods on medseg.\n",
      "eoformer\n",
      "outperforms the second-ranked nestedformer by an average of 1.59% on dice\n",
      "and achieves the top performance for both wt and et on hd95.\n",
      "index encoder/decoder dice (%) ↑\n",
      "hd95 (mm) ↓\n",
      "wt\n",
      "tc\n",
      "et\n",
      "ave\n",
      "wt\n",
      "tc\n",
      "et\n",
      "ave\n",
      "enc1\n",
      "unet encoder\n",
      "83.29\n",
      "83.43\n",
      "79.01\n",
      "84.00\n",
      "6.232\n",
      "8.018\n",
      "6.697\n",
      "6.983\n",
      "enc2\n",
      "cf×4\n",
      "88.51\n",
      "83.42\n",
      "82.56\n",
      "84.83\n",
      "7.131\n",
      "8.772\n",
      "5.641\n",
      "7.181\n",
      "enc3\n",
      "cf×2+tf×2\n",
      "90.92 86.63 81.05\n",
      "86.20\n",
      "3.906 5.227\n",
      "5.637\n",
      "4.923\n",
      "enc4\n",
      "cf×2+etf×2\n",
      "90.84\n",
      "86.38\n",
      "83.22\n",
      "86.81 3.974\n",
      "4.500 3.432\n",
      "3.968\n",
      "enc5\n",
      "etf×4\n",
      "90.24\n",
      "84.15\n",
      "85.40 86.59\n",
      "3.951\n",
      "5.651\n",
      "3.405 4.336\n",
      "dec1\n",
      "unet decoder\n",
      "89.75\n",
      "84.09\n",
      "80.12\n",
      "84.66\n",
      "7.562\n",
      "7.332\n",
      "6.427\n",
      "7.107\n",
      "dec2\n",
      "if×3\n",
      "90.27\n",
      "86.07\n",
      "80.09\n",
      "85.48\n",
      "5.448\n",
      "6.069\n",
      "4.929\n",
      "5.482\n",
      "dec3\n",
      "cf×3\n",
      "90.63\n",
      "85.63\n",
      "81.61\n",
      "85.96\n",
      "4.098\n",
      "4.467 3.023 4.842\n",
      "dec4\n",
      "eof×3\n",
      "90.84 86.38 83.22 86.81 3.974 4.500\n",
      "3.432\n",
      "3.968\n",
      "average hd95 improvement of 1.57 mm, highlighting its superior performance\n",
      "in tumor boundary prediction.\n",
      "it is worth mentioning that eoformer has the\n",
      "lowest flops and limited model size.\n",
      "2 represents the segmentation results\n",
      "on the brats 2020 and medseg datasets.\n",
      "the visualisation demonstrates that\n",
      "the eoformer achieves the closest segmentation results to the ground truth.\n",
      "speciﬁcally, eoformer accurately segments both tc and et region boundaries.\n",
      "3.4\n",
      "ablation\n",
      "we evaluate the eﬀectiveness of our proposed eoformer framework by con-\n",
      "ducting ablation experiments on the brats 2020.\n",
      "our results show that ehe outperforms other methods, with high average\n",
      "dice and low average hd95.\n",
      "our proposed ehe balances the\n",
      "strengths of both and achieves the best segmentation performance.\n",
      "addition-\n",
      "ally, our extended eﬃcient attention achieves better performance compared with\n",
      "enc3 because it has a better ability to capture the periphery of objects [17].\n",
      "decoder design.\n",
      "we compare the performance of various decoders in table 3.\n",
      "dec1 - 4 share the same ehe encoder, but employ diﬀerent decoders: dec1\n",
      "uses the unet decoder, dec2 has three identityformer blocks, dec3 replaces\n",
      "eoformer: edge-oriented transformer for brain tumor segmentation\n",
      "341\n",
      "the identityformer blocks with convformer blocks, and dec4 is our proposed\n",
      "eoformer decoder.\n",
      "our results show that the eoformer decoder achieves the\n",
      "highest dice scores, and achieves the lowest average hd95 score due to the\n",
      "incorporation of the eos and eol modules within the eoformer block.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf:\n",
      "capturing global contextual information plays a critical\n",
      "role in breast ultrasound (bus) image classiﬁcation.\n",
      "although con-\n",
      "volutional neural networks (cnns) have demonstrated reliable perfor-\n",
      "mance in tumor classiﬁcation, they have inherent limitations for mod-\n",
      "eling global and long-range dependencies due to the localized nature\n",
      "of convolution operations.\n",
      "vision transformers have an improved capa-\n",
      "bility of capturing global contextual information but may distort the\n",
      "local image patterns due to the tokenization operations.\n",
      "in this study,\n",
      "we proposed a hybrid multitask deep neural network called hybrid-mt-\n",
      "estan, designed to perform bus tumor classiﬁcation and segmentation\n",
      "using a hybrid architecture composed of cnns and swin transformer\n",
      "components.\n",
      "the proposed approach was compared to nine bus clas-\n",
      "siﬁcation methods and evaluated using seven quantitative metrics on\n",
      "a dataset of 3,320 bus images.\n",
      "multitask learning ·\n",
      "hybrid cnn-transformer\n",
      "1\n",
      "introduction\n",
      "breast cancer is the leading cause of cancer-related fatalities among women.\n",
      "currently, it holds the highest incidence rate of cancer among women in the u.s.,\n",
      "and in 2022 it accounted for 31% of all newly diagnosed cancer cases [1].\n",
      "due\n",
      "to the high incidence rate, early breast cancer detection is essential for reducing\n",
      "mortality rates and expanding treatment options.\n",
      "however, bus image analysis is also challenging due to the large variations in\n",
      "tumor shape and appearance, speckle noise, low contrast, weak boundaries, and\n",
      "occurrence of artifacts.\n",
      "in the past decade, deep learning-based approaches achieved remarkable\n",
      "advancements in bus tumor classiﬁcation [2,3].\n",
      "the progress has been driven\n",
      "by the capability of cnn-based models to learn hierarchies of structured image\n",
      "representations as semantics.\n",
      "nevertheless, one disadvantage of such architectural\n",
      "choice is that the feature representations in the deeper layers become increasingly\n",
      "abstract, leading to a loss of spatial and contextual information.\n",
      "the intrinsic\n",
      "locality of convolutional operations hinders the ability of cnns to model long-\n",
      "range dependencies while preserving spatial information in images eﬀectively.\n",
      "[5] and its variants recently demonstrated supe-\n",
      "rior performance in image classiﬁcation tasks.\n",
      "these models convert input images\n",
      "into smaller patches and utilize the self-attention mechanism to model the rela-\n",
      "tionships between the patches.\n",
      "self-attention enables vits to capture long-range\n",
      "dependencies and model complex relationships between diﬀerent regions of the\n",
      "image.\n",
      "however, the eﬀectiveness of vit-based approaches heavily relies on\n",
      "access to large datasets for learning meaningful representations of input images.\n",
      "this is primarily because the architectural design of vits does not rely on the\n",
      "same inductive biases in feature extraction which allow cnns to learn spatially\n",
      "invariant features.\n",
      "accordingly, numerous prior studies introduced modiﬁcations to the origi-\n",
      "nal vit network speciﬁcally designed for bus image classiﬁcation [13,14,23].\n",
      "[4] designed\n",
      "two hybrid cnn-transformer networks intended either for classiﬁcation or seg-\n",
      "mentation of multi-modal breast cancer images.\n",
      "despite the promising results\n",
      "of such hybrid approaches, eﬀectively capturing the local patterns and global\n",
      "long-range dependencies in bus images remains challenging [4,5,24].\n",
      "multitask learning leverages shared information across related tasks by\n",
      "jointly training the model.\n",
      "moreover,\n",
      "multitask learning acts as a regularizer by introducing inductive bias and pre-\n",
      "vents overﬁtting [25] (particularly with vits), and with that, can mitigate the\n",
      "challenges posed by small bus dataset sizes.\n",
      "in this study, we introduce a hybrid multitask approach, hybrid-mt-estan,\n",
      "which encompasses tumor classiﬁcation as a primary task and tumor segmenta-\n",
      "tion as a secondary task.\n",
      "hybrid-mt-estan combines the advantages of cnns\n",
      "and transformers in a framework incorporating anatomical tissue information in\n",
      "bus images.\n",
      "the anatomy of the human breast\n",
      "is categorized into four primary layers: the skin, premammary (subcutaneous\n",
      "fat), mammary, and retromammary layers, where each layer has a distinct tex-\n",
      "ture and generates diﬀerent echo patterns.\n",
      "the primary layers in bus images\n",
      "are arranged in a vertical stack, with similar echo patterns appearing horizon-\n",
      "tally across the images.\n",
      "[4], in which the authors used hybrid single-task cnn-transformer\n",
      "networks for either classiﬁcation or segmentation of bus images.\n",
      "the main contributions of this work are summarized as:\n",
      "• the proposed architecture eﬀectively integrates the advantages of cnns for\n",
      "extracting hierarchical and local patterns in bus images and swin trans-\n",
      "formers for leveraging long-range dependencies.\n",
      "• the multitask learning approach leverages the shared representations across\n",
      "the classiﬁcation and segmentation tasks to improve the model performance.\n",
      "fig.\n",
      "1. hybrid-mt-estan consists of mt-estan and aaa encoders, a segmentation\n",
      "branch, and a classiﬁcation branch.\n",
      "[3], and a swin transformer-based encoder with anatomy-\n",
      "aware attention (aaa) blocks, (2) a decoder branch for the segmentation task,\n",
      "and (3) a branch with fully-connected layers for the classiﬁcation task.\n",
      "mt-\n",
      "estan [3] is a cnn-based multitask learning network that simultaneously per-\n",
      "forms bus classiﬁcation and segmentation.\n",
      "[17], which employs row-column-wise kernels to learn and\n",
      "fuse context information in bus images at diﬀerent context scales (see fig.\n",
      "these specialized convolutional kernels eﬀectively extract contextual\n",
      "information of small tumors in bus images.\n",
      "refer to [17,22], and [3] for the\n",
      "implementation details of estan and mt-estan.\n",
      "swin transformer parti-\n",
      "tions an input image into non-overlapping patches of size 4×4, where each patch\n",
      "is treated as a “token”.\n",
      "a linear layer receives the patches and projects them into\n",
      "an arbitrary dimension.\n",
      "(7)\n",
      "concretely, we ﬁrst reconstruct the i-th feature map (yi) by merging (m) all\n",
      "patches, and afterward, we applied average pooling (avg-p) and max pooling\n",
      "348\n",
      "b. shareef et al.\n",
      "(max-p) layers with size (2, 2).\n",
      "3. anatomy-aware attention (aaa) block.\n",
      "2.3\n",
      "segmentation and classiﬁcation branches/tasks\n",
      "the segmentation branch in fig.\n",
      "2.4\n",
      "loss function\n",
      "we applied a multitask loss function (lmt) that aggregates two terms: a focal loss\n",
      "lf ocal for the classiﬁcation task and dice loss ldice for the segmentation task.\n",
      "since in medical image diagnosis achieving high sensitivity\n",
      "places emphasis on the detection of malignant lesions, we employed the focal loss\n",
      "for the classiﬁcation task to trade oﬀ between sensitivity and speciﬁcity.\n",
      "because\n",
      "malignant tumors are more challenging to detect due to greater diﬀerences in\n",
      "margin, shape, and appearance in bus images, focal loss forces the model to\n",
      "focus more on diﬃcult predictions.\n",
      "in the formulation, α\n",
      "bus tumor classiﬁcation using multitask cnn-transformer network\n",
      "349\n",
      "is a weighting coeﬃcient, n denotes the number of image samples, ti is the target\n",
      "label of the ith training sample, and pi denotes the prediction.\n",
      "the segmentation\n",
      "loss is calculated using the commonly-employed dice loss (ldice) function.\n",
      "3\n",
      "experimental results\n",
      "3.1\n",
      "datasets\n",
      "we evaluated the performance of hybrid-mt-estan using four public datasets,\n",
      "hmss\n",
      "we combined all four\n",
      "datasets to build a large and diverse dataset with a total of 3,320 b-mode bus\n",
      "images, of which 1,664 contain benign tumors and 1,656 have malignant tumors.\n",
      "hmss dataset does not\n",
      "provide the segmentation ground-truth masks, and for this study we arranged\n",
      "with a group of experienced radiologists to prepare the masks for hmss.\n",
      "bus dataset no. of images distribution\n",
      "source\n",
      "hmss\n",
      "1,948\n",
      "b:812, m:1136\n",
      "netherlands\n",
      "busi\n",
      "647\n",
      "b:437, m:210\n",
      "egypt\n",
      "busis\n",
      "562\n",
      "b:306, m:256\n",
      "china\n",
      "dataset b\n",
      "163\n",
      "b:109, m:54\n",
      "spain\n",
      "total\n",
      "3,320\n",
      "b: 1,664, m: 1,656\n",
      "3.2\n",
      "evaluation metrics\n",
      "for performance evaluation of the classiﬁcation task, we used the following met-\n",
      "rics: accuracy (acc), sensitivity (sens), speciﬁcity (spec), f1 score, area under\n",
      "the curve of receiver operating characteristic (auc), false positive rate (fpr),\n",
      "and false negative rate (fnr).\n",
      "to evaluate the segmentation performance, we\n",
      "used dice similarity coeﬃcient (dsc) and jaccard index (ji).\n",
      "3.3\n",
      "implementation details\n",
      "the proposed approach was implemented with keras and tensorflow libraries.\n",
      "all experiments were performed on a machine with nvidia quadro rtx 8000\n",
      "gpus and two intel xeon silver 4210r cpus (2.40ghz) with 512 gb of ram.\n",
      "all bus images in the dataset were zero-padded and reshaped to form square\n",
      "images.\n",
      "to avoid data leakage and bias, we selected the train, test, and vali-\n",
      "dation sets based on the cases, i.e., the images from one case (patient) were\n",
      "350\n",
      "b. shareef et al.\n",
      "table 2. performance metrics of the compared methods for bus image classiﬁcation\n",
      "and segmentation.\n",
      "classiﬁcation\n",
      "segmentation\n",
      "methods\n",
      "acc↑ sens.↑ spec.↑ f1↑\n",
      "auc↑ fnr↓ fpr↓ dsc↑\n",
      "ji↑\n",
      "sha-mtl [8]\n",
      "69.6\n",
      "48.1\n",
      "90.8\n",
      "0.58\n",
      "69.5\n",
      "51.9\n",
      "9.2\n",
      "72.2\n",
      "60.7\n",
      "mobilenet\n",
      "[5]\n",
      "72.1\n",
      "74.1\n",
      "69.3\n",
      "0.73\n",
      "71.7\n",
      "25.9\n",
      "30.7\n",
      "-\n",
      "-\n",
      "chowdery [10]\n",
      "77.4\n",
      "77.3\n",
      "77.3\n",
      "0.77\n",
      "77.3\n",
      "22.7\n",
      "22.7\n",
      "77.0\n",
      "67.9\n",
      "swin transformer 77.4\n",
      "72.6\n",
      "82.5\n",
      "0.74\n",
      "77.6\n",
      "27.4\n",
      "17.5\n",
      "-\n",
      "-\n",
      "mt-estan\n",
      "78.6\n",
      "83.7\n",
      "72.6\n",
      "0.83\n",
      "78.2\n",
      "16.3\n",
      "27.4\n",
      "78.2\n",
      "69.3\n",
      "ours\n",
      "82.8\n",
      "86.4\n",
      "79.2\n",
      "0.86 82.8\n",
      "13.6\n",
      "20.8\n",
      "84.1\n",
      "75.7\n",
      "note: a dash ‘-’ in the segmentation column indicates that the model uses single-task\n",
      "learning.\n",
      "assigned to only one of the training, validation, and test sets.\n",
      "furthermore,\n",
      "we employed horizontal ﬂip, height shift (20%), width shift (20%), and rota-\n",
      "tion (20◦c) for data augmentation.\n",
      "the proposed approach utilizes the building\n",
      "blocks of resnet50 and swin-transformer-v2, pretrained on imagenet dataset.\n",
      "namely, mt-estan uses pretrained resnet50 as a base model for the ﬁve\n",
      "encoder blocks (the implementation details of mt-estan can be found in\n",
      "[3]).\n",
      "for model\n",
      "training we utilized adam optimizer with a learning rate of 10−5 and mini batch\n",
      "size of 4 images.\n",
      "3.4\n",
      "performance evaluation and comparative analysis\n",
      "we compared the performance of hybrid-mt-estan for bus classiﬁcation to\n",
      "nine deep learning approaches commonly used for medical image analysis.\n",
      "the values of the performance metrics are shown in table 2, indicating that the\n",
      "proposed hybrid-mt-estan outperformed all nine approaches by achieving\n",
      "the best accuracy, sensitivity, f1 score, and auc with 82.8%, 86.4%, 86.0%,\n",
      "and 82.8%, respectively.\n",
      "classiﬁcation\n",
      "segmentation\n",
      "methods\n",
      "acc↑ sens.↑ spec.↑ f1↑\n",
      "auc↑ fnr↓ fpr↓ dsc↑\n",
      "ji↑\n",
      "mt-estan [10]\n",
      "78.6\n",
      "83.7\n",
      "72.6\n",
      "0.83\n",
      "78.2\n",
      "16.3\n",
      "27.4\n",
      "78.2\n",
      "69.3\n",
      "swin trans.\n",
      "77.4\n",
      "72.6\n",
      "82.5\n",
      "0.74\n",
      "77.6\n",
      "27.4\n",
      "17.5\n",
      "-\n",
      "-\n",
      "mt-estan + swin trans.\n",
      "80.3\n",
      "84.2\n",
      "76.3\n",
      "0.83\n",
      "80.2\n",
      "15.8\n",
      "23.7\n",
      "82.3\n",
      "73.6\n",
      "ours\n",
      "82.8\n",
      "86.4\n",
      "79.2\n",
      "0.86 82.8\n",
      "13.6\n",
      "20.8\n",
      "84.1\n",
      "75.7\n",
      "the preferred trade-oﬀ in medical image analysis typically is high sensitivity\n",
      "without signiﬁcant degradation in speciﬁcity.\n",
      "we evaluated the segmentation performance of hybrid mt-estan and com-\n",
      "pared the results to ﬁve multitask approaches, including sha-mtl\n",
      "as shown in\n",
      "table 2,the proposed hybrid mt-estan achieved the highest performance and\n",
      "increased dsc and ji by 5.9% and 6.4%, respectively compared to mt-estan.\n",
      "note that results of single-task models in table 2 are not provided.\n",
      "3.5\n",
      "eﬀectiveness of the anatomy-aware attention (aaa) block\n",
      "to verify the eﬀectiveness of the anatomy-aware attention (aaa) block, we\n",
      "conducted an ablation study that quantiﬁed the impact of the diﬀerent com-\n",
      "ponents in hybrid-mt-estan on the classiﬁcation and segmentation perfor-\n",
      "mance.\n",
      "table 3 presents the values of the performance metrics for mt-estan\n",
      "(pure cnn-based approach), swin transformer (pure transformer network), a\n",
      "hybrid architecture of mt-estan and swin transformer, and our proposed\n",
      "hybrid-mt-estan with aaa block.\n",
      "the hybrid architectures of mt-estan with swin\n",
      "transformer improved the classiﬁcation performance and has higher accuracy,\n",
      "sensitivity, f1 score, and auc with 80.3%, 84.2%, 83%, and 80.2%, compared\n",
      "to mt-estan and swin transformer individually.\n",
      "the proposed approach,\n",
      "hybrid-mt-estan with aaa block, further improved accuracy, sensitivity, f1\n",
      "score, and auc by 2.5%, 2.2%, 3%, and 2.6%, respectively, relative to the hybrid\n",
      "model without the aaa block.\n",
      "to evaluate the segmentation performance, we compared the proposed app-\n",
      "roach with and without the aaa block and swin transformer.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf:\n",
      "designing deep learning algorithms for gland segmentation\n",
      "is crucial for automatic cancer diagnosis and prognosis.\n",
      "however, the\n",
      "expensive annotation cost hinders the development and application of\n",
      "this technology.\n",
      "in this paper, we make a ﬁrst attempt to explore a deep\n",
      "learning method for unsupervised gland segmentation, where no man-\n",
      "ual annotations are required.\n",
      "existing unsupervised semantic segmenta-\n",
      "tion methods encounter a huge challenge on gland images.\n",
      "they either\n",
      "over-segment a gland into many fractions or under-segment the\n",
      "gland regions by confusing many of them with the background.\n",
      "to overcome this challenge, our key insight is to introduce an empirical\n",
      "cue about gland morphology as extra knowledge to guide the segmen-\n",
      "tation process.\n",
      "to this end, we propose a novel morphology-inspired\n",
      "method via selective semantic grouping.\n",
      "we ﬁrst leverage the empirical\n",
      "cue to selectively mine out proposals for gland sub-regions with variant\n",
      "appearances.\n",
      "then, a morphology-aware semantic grouping module is\n",
      "employed to summarize the overall information about glands by explic-\n",
      "itly grouping the semantics of their sub-region proposals.\n",
      "in this way,\n",
      "the ﬁnal segmentation network could learn comprehensive knowledge\n",
      "about glands and produce well-delineated and complete predictions.\n",
      "we\n",
      "conduct experiments on the glas dataset and the crag dataset.\n",
      "keywords: whole slide image · unsupervised gland segmentation ·\n",
      "morphology-inspired learning · semantic grouping\n",
      "1\n",
      "introduction\n",
      "accurate gland segmentation from whole slide images (wsis) plays a crucial role\n",
      "in the diagnosis and prognosis of cancer, as the morphological features of glands\n",
      "can provide valuable information regarding tumor aggressiveness\n",
      "with the\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43901-8 27.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "(b)\n",
      "prior uss methods in medical image research [2] and natural image research\n",
      "(color ﬁgure online)\n",
      "emergence of deep learning (dl), there has been a growing interest in developing\n",
      "dl-based methods for semantic-level [9,12,36] and instance-level [5,13,25,27,\n",
      "32,35] gland segmentation.\n",
      "however, such methods typically rely on large-scale\n",
      "annotated image datasets, which usually require signiﬁcant eﬀort and expertise\n",
      "from pathologists and can be prohibitively expensive [28].\n",
      "to reduce the annotation cost, developing annotation-eﬃcient methods for\n",
      "semantic-level gland segmentation has attracted much attention [10,18,23,37].\n",
      "recently, some researchers have explored weakly supervised semantic segmen-\n",
      "tation methods which use weak annotations (e.g., bound box\n",
      "[18]) instead of pixel-level annotations to train a gland segmentation net-\n",
      "work.\n",
      "[10,29] to design annotation-free\n",
      "methods for gland segmentation.\n",
      "however, the performance of these methods\n",
      "can vary widely, especially in cases of malignancy.\n",
      "this paper focuses on unsu-\n",
      "pervised gland segmentation, where no annotations are required during\n",
      "training and inference.\n",
      "one potential solution is to adopt unsupervised semantic segmentation (uss)\n",
      "methods which have been successfully applied to medical image research and nat-\n",
      "ural image research.\n",
      "on the one hand, existing uss methods have shown promis-\n",
      "ing results in various medical modalities, e.g., magnetic resonance images [19],\n",
      "x-ray images [1,15] and dermoscopic images [2].\n",
      "however, directly utilizing these\n",
      "methods to segment glands could lead to over-segment results where a gland is\n",
      "segmented into many fractions rather than being considered as one target (see\n",
      "fig.\n",
      "this is because these methods are usually designed to be extremely\n",
      "sensitive to color [2], while gland images present a unique challenge due to\n",
      "their highly dense and complex tissues with intricate color distribution [18].\n",
      "on the other hand, prior uss methods for natural images can be broadly cate-\n",
      "gorized into coarse-to-ﬁne-grained\n",
      "[4,14,16,21,31] and end-to-end (e2e) cluster-\n",
      "morphology-inspired ugs via selective semantic grouping\n",
      "[31], and self-attention maps [4,14,21])\n",
      "as prior, which is not always feasible on gland images.\n",
      "the e2e clustering meth-\n",
      "ods, however, produce under-segment results on gland images by confusing many\n",
      "gland regions with the background; see fig. 1(b).\n",
      "as such, the e2e clustering\n",
      "methods tend to blindly cluster pixels with similar properties and confuse many\n",
      "gland regions with the background, leading to under-segment results.\n",
      "to tackle the above challenges, our solution is to incorporate an empir-\n",
      "ical cue about gland morphology as additional knowledge to guide\n",
      "gland segmentation.\n",
      "to this end, we propose a novel morphology-inspired method via selective\n",
      "semantic grouping, abbreviated as mssg.\n",
      "to begin, we leverage the empirical\n",
      "cue to selectively mine out proposals for the two gland sub-regions with vari-\n",
      "ant appearances.\n",
      "then, considering that our segmentation target is the gland,\n",
      "we employ a morphology-aware semantic grouping module to summarize the\n",
      "semantic information about glands by explicitly grouping the semantics of the\n",
      "sub-region proposals.\n",
      "in this way, we not only prioritize and dedicate extra atten-\n",
      "tion to the target gland regions, thus avoiding under-segmentation; but also\n",
      "exploit the valuable morphology information hidden in the empirical cue, and\n",
      "force the segmentation network to recognize entire glands despite the excessive\n",
      "variance among the sub-regions, thus preventing over-segmentation.\n",
      "our contributions are as follows: (1) we identify the major challenge encoun-\n",
      "tered by prior unsupervised semantic segmentation (uss) methods when dealing\n",
      "with gland images, and propose a novel mssg for unsupervised gland segmen-\n",
      "tation.\n",
      "(2) we propose to leverage an empirical cue to select gland sub-regions\n",
      "and explicitly group their semantics into a complete gland region, thus avoid-\n",
      "ing over-segmentation and under-segmentation in the segmentation results.\n",
      "[13]), and the experiment results\n",
      "demonstrate the eﬀectiveness of our mssg in unsupervised gland segmentation.\n",
      "2. overview of our morphology-inspired unsupervised gland segmentation via\n",
      "selective semantic grouping.\n",
      "we leverage an\n",
      "empirical cue to select proposals for gland sub-regions from the prediction of a shal-\n",
      "low encoder f(·) which emphasizes low-level appearance features rather than high-\n",
      "level semantic features.\n",
      "(b) morphology-aware semantic grouping (msg) pipeline.\n",
      "a segmentation network.\n",
      "meantime, a morphology-aware semantic grouping\n",
      "(msg) module is used to summarize the overall information about glands from\n",
      "their sub-region proposals.\n",
      "2.1\n",
      "selective proposal mining\n",
      "instead of generating pseudo-labels for the gland region directly from all the\n",
      "pixels of the gland images as previous works typically do, which could lead to\n",
      "over-segmentation and under-segmentation results, we propose using the empir-\n",
      "ical cue as extra hints to guide the proposal generation process.\n",
      "speciﬁcally, let the ith input image be denoted as xi ∈ rc×h×w , where h,\n",
      "w, and c refer to the height, width, and number of channels respectively.\n",
      "we train the\n",
      "encoder in a self-supervised manner, and the loss function l consists of a typical\n",
      "self-supervised loss lss, which is the cross-entropy loss between the feature map\n",
      "fi and the one-hot cluster label ci = arg max (fi), and a spatial continuity loss\n",
      "lsc, which regularizes the vertical and horizontal variance among pixels within\n",
      "a certain area s to assure the continuity and completeness of the gland border\n",
      "regions (see fig.\n",
      "1 in the supplementary material).\n",
      "the expressions for\n",
      "lss and lsc are given below:\n",
      "lss(fi[:, h, w], ci[:, h, w]) = −\n",
      "d\n",
      "\u0002\n",
      "d\n",
      "ci[d, h, w] · ln fi[d, h, w]\n",
      "(1)\n",
      "morphology-inspired ugs via selective semantic grouping\n",
      "285\n",
      "lsc (fi) =\n",
      "s,h−s,w −s\n",
      "\u0002\n",
      "s,h,w\n",
      "(fi[:, h + s, w] − fi\n",
      "+ n5 equals the total number of pixels in the input image (h × w).\n",
      "the aforemen-\n",
      "tioned empirical cue is used to select proposals for the gland border and interior\n",
      "epithelial tissues from the candidate regions yi.\n",
      "particularly, we select the region\n",
      "with the highest average gray level as the proposal for the gland border.\n",
      "then,\n",
      "we ﬁll the areas surrounded by the gland border proposal and consider them as\n",
      "the proposal for the interior epithelial tissues, while the rest areas of the gland\n",
      "image are regarded as the background (i.e., non-glandular region).\n",
      "finally, we\n",
      "obtain the proposal map pi ∈ r3×h×w , which contains the two proposals for\n",
      "two gland sub-regions and one background proposal.\n",
      "2.2\n",
      "morphology-aware semantic grouping\n",
      "a direct merge of the two sub-region proposals to train a fully-supervised seg-\n",
      "mentation network may not be optimal for our case.\n",
      "firstly, the two gland sub-\n",
      "regions exhibit signiﬁcant variation in appearance, which can impede the seg-\n",
      "mentation network’s ability to recognize them as integral parts of the same\n",
      "object.\n",
      "secondly, the spm module may produce proposals with inadequate\n",
      "highlighting of many gland regions, particularly the interior epithelial tissues,\n",
      "as shown in fig.\n",
      "consequently,\n",
      "applying pixel-level cross-entropy loss between the gland image and the merged\n",
      "proposal map could introduce undesired noise into the segmentation network,\n",
      "thus leading to under-segment predictions with confusion between the glands and\n",
      "the background.\n",
      "as such, we propose two types of morphology-aware semantic\n",
      "grouping (msg) modules (i.e., msg for variation and msg for omission) to\n",
      "respectively reduce the confusion caused by the two challenges mentioned above\n",
      "and improve the overall accuracy and comprehensiveness of the segmentation\n",
      "results.\n",
      "here, we ﬁrst slice the gland image and its proposal map into patches as\n",
      "inputs.\n",
      "we can obtain the feature embedding map\n",
      "ˆf which is derived as ˆf = ffeat( ˆx) and the prediction map \u0004\n",
      "x as \u0004\n",
      "x = fcls( ˆf),\n",
      "where ffeat and fcls refers to the feature extractor and pixel-wise classiﬁer of\n",
      "the segmentation network respectively.\n",
      "msg for variation is designed to mitigate the adverse impact of appear-\n",
      "ance variation between the gland sub-regions.\n",
      "then,\n",
      "we use the average of the pixel embeddings in gland border set g as the align-\n",
      "ment anchor and pull all pixels of i towards the anchor:\n",
      "lmsgv = 1\n",
      "i\n",
      "\u0002\n",
      "i∈i\n",
      "\u0003\n",
      "i − 1\n",
      "g\n",
      "\u0002\n",
      "g∈g\n",
      "g\n",
      "\u00042\n",
      ".\n",
      "it identiﬁes and relabels the overlooked gland regions in\n",
      "the proposal map and groups them back into the gland semantic category.\n",
      "finally, we impose a pixel-level cross-entropy loss\n",
      "on the prediction and reﬁned proposal rp to train the segmentation network:\n",
      "lmsgo = −\n",
      "ˆ\n",
      "h, ˆ\n",
      "w\n",
      "\u0002\n",
      "ˆh, ˆ\n",
      "w\n",
      "rp[:, ˆh, ˆw] · ln \u0005\n",
      "x[:, ˆh, ˆw],\n",
      "(5)\n",
      "the total objective function l for training the segmentation network can be\n",
      "summarized as follows:\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "datasets\n",
      "we evaluate our mssg on the gland segmentation challenge (glas) dataset\n",
      "[27] and the colorectal adenocarcinoma gland (crag) dataset\n",
      "the crag dataset has more irregular malignant\n",
      "glands, which makes it more diﬃcult than glas, and we would like to empha-\n",
      "size that the results on crag are from the model trained on glas without\n",
      "retraining.\n",
      "morphology-inspired ugs via selective semantic grouping\n",
      "287\n",
      "fig.\n",
      "black\n",
      "denotes glandular tissues and white denotes non-glandular tissues (more in the sup-\n",
      "plementary material).\n",
      "[2]\n",
      "pspnet\n",
      "none\n",
      "69.29%\n",
      "67.88%\n",
      "55.31%\n",
      "mssg\n",
      "pspnet\n",
      "none\n",
      "77.43% 77.26% 65.89%\n",
      "3.2\n",
      "implementation details\n",
      "the experiments are conducted on four rtx 3090 gpus.\n",
      "for the msg, mmsegmentation\n",
      "[7] is used\n",
      "to construct a pspnet [38] with a resnet-50 backbone as the segmentation\n",
      "network.\n",
      "without msg, the performance is not good\n",
      "enough, due to signiﬁcant sub-region variation and gland omission.\n",
      "with msg mod-\n",
      "ules, the performance of the network is progressively improved (more in the sup-\n",
      "plementary material).\n",
      "table 2. performance gains with msg modules.\n",
      "the segmentation performance is\n",
      "progressively improved as the involvement of msg for variation & msg for omission.\n",
      "msg for variation\n",
      "msg for omission\n",
      "miou\n",
      "improvement(δ)\n",
      "×\n",
      "×\n",
      "48.42%\n",
      "-\n",
      "√\n",
      "×\n",
      "56.12%\n",
      "on the glas dataset, the end-to-end clustering methods\n",
      "(denoted by “∗”) end up with limited improvement over a randomly initialized\n",
      "network.\n",
      "addi-\n",
      "tionally, we visualize the segmentation results of mssg and its counterpart (i.e.,\n",
      "sgscn [2]) in fig.\n",
      "morphology-inspired ugs via selective semantic grouping\n",
      "289\n",
      "3.4\n",
      "ablation study\n",
      "table 2 presents the ablation test results of the two msg modules.\n",
      "it can be\n",
      "observed that the segmentation performance without the msg modules is not\n",
      "satisfactory due to the signiﬁcant sub-region variation and\n",
      "moreover, with both msg modules\n",
      "incorporated, the performance signiﬁcantly improves to 62.72% (+14.30%).\n",
      "more ablation tests on the spm (tab. 1 & 2) and\n",
      "hyper-parameters (tab. 3) are in the supplementary material.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_15.pdf:\n",
      "gastric cancer is the third leading cause of cancer-related\n",
      "mortality worldwide, but no guideline-recommended screening test\n",
      "exists.\n",
      "existing methods can be invasive, expensive, and lack sensitiv-\n",
      "ity to identify early-stage gastric cancer.\n",
      "we propose a novel cluster-induced mask\n",
      "transformer that jointly segments the tumor and classiﬁes abnormal-\n",
      "ity in a multi-task manner.\n",
      "in\n",
      "our experiments, the proposed method achieves a sensitivity of 85.0%\n",
      "and speciﬁcity of 92.6% for detecting gastric tumors on a hold-out test\n",
      "set consisting of 100 patients with cancer and\n",
      "in compari-\n",
      "son, two radiologists have an average sensitivity of 73.5% and speciﬁcity\n",
      "of 84.3%.\n",
      "our approach performs comparably to estab-\n",
      "lished state-of-the-art gastric cancer screening tools like blood testing\n",
      "and endoscopy, while also being more sensitive in detecting early-stage\n",
      "cancer.\n",
      "[16], which is\n",
      "mainly attributed to patients being diagnosed with advanced-stage disease har-\n",
      "boring unresectable tumors.\n",
      "this is often due to the latent and nonspeciﬁc signs\n",
      "and symptoms of early-stage gc.\n",
      "however, patients with early-stage disease\n",
      "have a substantially higher ﬁve-year survival rate of around 72% [16].\n",
      "unfortunately, current guidelines do not recommend any screening tests for gc\n",
      "[22].\n",
      "this is because early-stage gastric tumors may only invade\n",
      "the mucosal and muscularis layers, which are diﬃcult to identify without the\n",
      "help of stomach preparation and contrast injection.\n",
      "unlike the conventional “segmentation for classiﬁcation”\n",
      "methods that directly employ segmentation networks, we developed a cluster-\n",
      "induced mask transformer that performs segmentation and global classiﬁcation\n",
      "simultaneously.\n",
      "by incorporating self-attention layers for global context modeling,\n",
      "our model can leverage both local and global cues for accurate detection.\n",
      "in\n",
      "our experiments, the proposed approach outperforms nnunet\n",
      "researchers have explored automated tumor\n",
      "detection techniques on endoscopic [13,14], pathological images [20], and the\n",
      "prediction of cancer prognosis [12].\n",
      "recent developments in deep learning have\n",
      "signiﬁcantly improved the segmentation of gastric tumors [11], which is critical\n",
      "for their detection.\n",
      "while previous\n",
      "studies have successfully detected pancreatic [25] and esophageal [26] cancers on\n",
      "non-contrast ct, identifying gastric cancer presents a unique challenge due to\n",
      "its subtle texture changes, various shape alterations, and complex background,\n",
      "e.g., irregular gastric wall; liquid and contents in the stomach.\n",
      "recent studies have used transformers for natural and\n",
      "medical image segmentation [21].\n",
      "[1] as memory-encoded queries for segmentation.\n",
      "mask transformers are locally sen-\n",
      "sitive to image textures for precise segmentation and globally aware of organ-\n",
      "tumor morphology for recognition.\n",
      "there-\n",
      "fore, mask transformers are an ideal choice for an end-to-end joint segmentation\n",
      "and classiﬁcation system for detecting gastric cancer.\n",
      "pi ∈ l is the class label of the image, conﬁrmed by pathology, radiology, or\n",
      "clinical records.\n",
      "any misaligned\n",
      "ones are revised manually.\n",
      "in this manner (fig.\n",
      ", a relatively coarse yet highly\n",
      "reliable tumor mask can be obtained for the non-contrast ct image.\n",
      "(a) the non-contrast ct image is ﬁrst forwarded into a u-\n",
      "net [8,18] to extract a feature map.\n",
      "the cluster assignment (a.k.a. mask prediction) is further used to generate the ﬁnal\n",
      "segmentation output and the classiﬁcation probability.\n",
      "segmentation\n",
      "for classiﬁcation is widely used in tumor detection [25,26,32].\n",
      "we ﬁrst train a\n",
      "unet [8,18] to segment the stomach and tumor regions using the masks from the\n",
      "previous step.\n",
      "speciﬁcally, given image x ∈ rh×w ×d, annotation y ∈ rk×hw d, and\n",
      "patient class p ∈ l, our model consists of three components: 1) a cnn back-\n",
      "bone to extract its pixel-wise features f ∈ rc×hw d (fig. 1a), 2) a transformer\n",
      "module (fig. 1b), and 3) a multi-task cluster inference module (fig. 1c).\n",
      "the assignment of clusters (a.k.a. mask\n",
      "prediction) m\n",
      "(2)\n",
      "the ﬁnal segmentation logits\n",
      "the aggregation of pixels is achieved by\n",
      "z = ckm, where the cluster-wise classiﬁcation ck is represented by an mlp\n",
      "that projects the cluster centers c to k channels (the number of segmentation\n",
      "classes).\n",
      "the learned cluster centers possess high-level semantics with both inter-\n",
      "cluster discrepancy and intra-cluster similarity for eﬀective classiﬁcation.\n",
      "rather\n",
      "than directly classifying the ﬁnal feature map, we ﬁrst generate the cluster-\n",
      "path feature vector by taking the channel-wise average of cluster centers ¯c =\n",
      "1\n",
      "n\n",
      "\u0002\n",
      "i=1 ci ∈ rc.\n",
      "additionally, to enhance the consistency between the seg-\n",
      "mentation and classiﬁcation outputs, we apply global max pooling to cluster\n",
      "assignments r to obtain the pixel-path feature vector ¯r ∈ rn.\n",
      "this establishes\n",
      "a direct connection between classiﬁcation features and segmentation predictions.\n",
      "the overall training objective is formulated as,\n",
      "l = lseg(z, y) + lcls(ˆp, p),\n",
      "(3)\n",
      "where the segmentation loss lseg(·, ·) is a combination of dice and cross entropy\n",
      "losses, and the classiﬁcation loss lcls(·, ·) is cross entropy loss.\n",
      "4\n",
      "experiments\n",
      "4.1\n",
      "experimental setup\n",
      "dataset and ground truth.\n",
      "all patients underwent multi-phase cts with a median spac-\n",
      "ing of 0.75 × 0.75 × 5.0 mm and an average size of (512, 512, 108) voxel.\n",
      "this\n",
      "early-stage gc case is miss-detected by both radiologists and nnunet\n",
      "[23], while the stomach was automatically\n",
      "annotated using a self-learning model [31].\n",
      "implementation details.\n",
      "we set the num-\n",
      "ber of object queries n to 8, with each having a dimension of 128, and included\n",
      "an eight-head self-attention layer in each block.\n",
      "we followed [8] to augment data.\n",
      "to enhance performance, we added\n",
      "deep supervision by aligning the cross-attention map with the ﬁnal segmentation\n",
      "map, as per kmax-deeplab [27].\n",
      "the hidden layer dimension in the two-layer\n",
      "mlp is 128.\n",
      "[8,18] to localize the stomach\n",
      "region in the entire image in the testing phase.\n",
      "evaluation metrics and reader study.\n",
      "for the binary classiﬁcation, model\n",
      "performance is evaluated using area under roc curve (auc), sensitivity (sens.),\n",
      "and speciﬁcity (spec.).\n",
      "and successful localization of the tumors is considered\n",
      "when the overlap between the segmentation mask generated by the model and\n",
      "152\n",
      "m. yuan et al.\n",
      "table 1.\n",
      "(78.1, 91.1) (88.0, 96.5) (96.7, 98.7)\n",
      "table 2. patient-level detection and tumor-level localization results (%) over gas-\n",
      "tric cancer across diﬀerent t-stages.\n",
      "tumor-level localization evaluates how segmented\n",
      "masks overlap with the ground-truth cancer (dice > 0.01 for correct detection).\n",
      "miss-t:\n",
      "missing of t stage information.\n",
      "[10],\n",
      "ugis and endoscopy screening performance in large population [4], and early stage\n",
      "gastric cancer detection rate of senior radiologists on narrow-band imaging with mag-\n",
      "nifying endoscopy (me-nbi)\n",
      "†: we also merely consider early-stage gastric\n",
      "cancer cases, including tumor in situ, t1, and t2 stages, among whom we successfully\n",
      "detect 17 of 19 cases.)\n",
      "[10]\n",
      "99.5\n",
      "66.7\n",
      "69.4∗\n",
      "upper-gastrointestinal series [4] 96.1\n",
      "36.7\n",
      "85.0\n",
      "endoscopy screening [4]\n",
      "96.0\n",
      "69.0\n",
      "85.0\n",
      "me-nbi (early-stage)\n",
      "the ﬁrst two approaches belong to “segmentation\n",
      "for classiﬁcation” (s4c)\n",
      "a case\n",
      "is classiﬁed as positive if the segmented tumor volume exceeds a threshold that\n",
      "maximizes the sum of sensitivity and speciﬁcity on the validation set.\n",
      "the advantage\n",
      "of our approach is that it captures the local and global information simultane-\n",
      "ously in virtue of the unique architecture of mask transformer.\n",
      "it also extracts\n",
      "high-level semantics from cluster representations, making it suitable for classiﬁ-\n",
      "cation and facilitating a holistic decision-making process.\n",
      "moreover, our method\n",
      "reaches a considerable speciﬁcity of 97.7% on the external test set, which is cru-\n",
      "cial in opportunistic screening for less false positives and unnecessary human\n",
      "workload.\n",
      "the model achieves a sensitivity of 85.0% in\n",
      "detecting gastric cancer, which signiﬁcantly exceeds the mean performance of\n",
      "doctors (73.5%) and also surpasses the best performing doctor (r2: 75.0%),\n",
      "while maintaining a high speciﬁcity.\n",
      "this early-stage cancer (t1) is miss-detected by both radiologists, whereas clas-\n",
      "siﬁed and localized precisely by our model.\n",
      "154\n",
      "m. yuan et\n",
      "in table 2, we report the performance of patient-level\n",
      "detection and tumor-level localization stratiﬁed by tumor (t) stage.\n",
      "we compare\n",
      "our model’s performance with that of both radiologists.\n",
      "the results show that\n",
      "our model performs better in detecting early stage tumors (t1, t2) and provides\n",
      "more precise tumor localization.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf:\n",
      "the success of large-scale pre-trained vision-language models\n",
      "(vlm) has provided a promising direction of transferring natural image\n",
      "representations to the medical domain by providing a well-designed\n",
      "prompt with medical expert-level knowledge.\n",
      "besides, the models pre-trained with natu-\n",
      "ral images fail to detect lesions precisely.\n",
      "to solve this problem, fusing\n",
      "multiple prompts is vital to assist the vlm in learning a more compre-\n",
      "hensive alignment between textual and visual modalities.\n",
      "in this paper,\n",
      "we propose an ensemble guided fusion approach to leverage multiple\n",
      "statements when tackling the phrase grounding task for zero-shot lesion\n",
      "detection.\n",
      "extensive experiments are conducted on three public medical\n",
      "image datasets across diﬀerent modalities and the detection accuracy\n",
      "improvement demonstrates the superiority of our method.\n",
      "keywords: vision-language models · lesion detection · multiple\n",
      "prompts · prompt fusion · ensemble learning\n",
      "1\n",
      "introduction\n",
      "medical lesion detection plays an important role in assisting doctors with the\n",
      "interpretation of medical images for disease diagnosing, cancer staging, etc.,\n",
      "which can improve eﬃciency and reduce human errors [9,19].\n",
      "current object\n",
      "detection approaches are mainly based on supervised learning with abundant\n",
      "well-paired image-level annotations, which heavily rely on expert-level knowl-\n",
      "edge.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9_28.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43904-9_28\n",
      "284\n",
      "m. guo et al.\n",
      "recently, large-scale pre-trained vision-language models (vlms), by learning\n",
      "the visual concepts in the images through the weak labels from text, have pre-\n",
      "vailed in natural object detection or visual grounding and shown extraordinary\n",
      "performance.\n",
      "however, current existing vlms are mostly based on a single prompt to\n",
      "establish textual and visual alignment.\n",
      "apparently, even a well-designed\n",
      "prompt is not always able to combine all expressive attributes into one sentence\n",
      "without semantic and syntactic ambiguity, e.g., the prompt design for melanoma\n",
      "detection should include numerous kinds of information describing attributes\n",
      "complementing each other, such as shape, color, size, etc\n",
      "in this work, instead of striving to design a single satisfying prompt, we\n",
      "aim to take advantage of pre-trained vlms in a more ﬂexible way with the\n",
      "form of multiple prompts, where each prompt can elicit respective knowledge\n",
      "from the model which can then be fused for better lesion detection performance.\n",
      "in addition, we\n",
      "also examine the language syntax based prompt fusion approach as a compar-\n",
      "ison, and explore several fusion strategies by ﬁrst grouping the prompts either\n",
      "with described attributes or categories and then repeating the fusion process.\n",
      "we evaluate the proposed approach on a broad range of public medical\n",
      "datasets across diﬀerent modalities including photography images for skin lesion\n",
      "detection isic 2016\n",
      "[2], endoscopy images for polyp detection cvc-300\n",
      "[21],\n",
      "and cytology images for blood cell detection bccd.\n",
      "2\n",
      "related work\n",
      "object detection and vision-language models.\n",
      "in the vision-language\n",
      "ﬁeld, phrase grounding can be regarded as another solution for object detec-\n",
      "tion apart from conventional r-cnns\n",
      "recently, vision-language models\n",
      "multiple prompt fusion for zero-shot lesion detection\n",
      "285\n",
      "have achieved exciting performance in the zero-shot and few-shot visual recog-\n",
      "nition [4,16].\n",
      "in addition, vild [7] is proposed\n",
      "for open-vocabulary object detection taking advantage of the rich knowledge\n",
      "learned from clip [4] and text input.\n",
      "as pointed out by a review [3], ensemble learning meth-\n",
      "ods achieve better performance by producing predictions based on extracted fea-\n",
      "tures and fusing via various voting mechanisms.\n",
      "for example, a selective ensem-\n",
      "ble of classiﬁer chains [13] is proposed to reduce the computational cost and the\n",
      "storage cost arose in multi-label learning [12] by decreasing the ensemble size.\n",
      "3\n",
      "method\n",
      "in this section, we ﬁrst brieﬂy introduce the vision-language model for unifying\n",
      "object detection as phrase grounding, e.g., glip\n",
      "then we present\n",
      "a simple language syntax based prompt fusion approach in sect.\n",
      "3.3 to improve the zero-shot lesion detection.\n",
      "3.1\n",
      "preliminaries\n",
      "phrase grounding is the task of identifying the ﬁne-grained correspondence\n",
      "between phrases in a sentence and objects in an image.\n",
      "the glip model takes\n",
      "as input an image i and a text prompt p that describes all the m candidate\n",
      "categories for the target objects.\n",
      "then, glip uses a ground-\n",
      "ing module to align image boxes with corresponding phrases in the text prompt.\n",
      "the whole process can be formulated as follows:\n",
      "o = enci(i), p = enct(p), sground = op ⊤, lcls = loss(sground; t),\n",
      "(1)\n",
      "where o ∈ rn×d, p ∈ rm×d denote the image and text features respectively for\n",
      "n candidate region proposals and m target objects, sground ∈ rn×m represents\n",
      "the cross-modal alignment scores, and t ∈ {0, 1}n×m is the target matrix.\n",
      "3.2\n",
      "language syntax based prompt fusion\n",
      "as mentioned above, it is diﬃcult for a single prompt input structure such as\n",
      "glip to cover all necessary descriptions even through careful designation of\n",
      "the prompt.\n",
      "however, it is challenging to\n",
      "combine the grounding results from multiple prompts since manual integration is\n",
      "subjective, ineﬀective, and lacks uniform standards.\n",
      "we achieve this by extracting and\n",
      "fusing the preﬁxes and suﬃxes of each prompt based on language conventions\n",
      "and grammar rules.\n",
      "moreover, the fused prompts are normally too long that the\n",
      "model could lose proper attention to the key information, resulting in extremely\n",
      "unstable performance (results shown in sect.\n",
      "more speciﬁcally, the vlm outputs a set of candidate\n",
      "region proposals ci for each prompt pi, and these candidates carry more multi-\n",
      "dimensional information than prompts.\n",
      "we ﬁnd in our preliminary experiments\n",
      "that direct concatenation of the candidates is not satisfactory and eﬀective, since\n",
      "simply integration hardly screens out the bad predictions.\n",
      "therefore, we consider step-wise clustering mechanisms\n",
      "using the above information to screen out the implausible candidates based on\n",
      "clustering ensemble learning [3].\n",
      "another observation in our preliminary experiments is that most of the can-\n",
      "didates distribute near the target if the prompt description matches better with\n",
      "the object.\n",
      "4\n",
      "experiments and results\n",
      "4.1\n",
      "experimental settings\n",
      "we collect three public medical image datasets across various modalities includ-\n",
      "ing skin lesion detection dataset isic 2016\n",
      "for the experiments, we use the\n",
      "glip-t variant [11] as our base pre-trained model and adopt two metrics for\n",
      "the grounding evaluation, including average precision (ap) and ap50.\n",
      "more\n",
      "details on the dataset and implementation are described in the appendix.\n",
      "[20]\n",
      "1.16\n",
      "5.37\n",
      "3.27\n",
      "9.40\n",
      "1.22\n",
      "4.75\n",
      "concatenation\n",
      "16.9\n",
      "27.4\n",
      "21.5\n",
      "27.8\n",
      "11.6\n",
      "20.6\n",
      "syntax based fusion 13.9\n",
      "24.1\n",
      "10.0\n",
      "16.4\n",
      "12.8\n",
      "25.4\n",
      "ours\n",
      "19.8 30.9\n",
      "36.1 47.9\n",
      "15.8 32.6\n",
      "4.2\n",
      "results\n",
      "this section demonstrates that our proposed ensemble guided fusion approach\n",
      "can eﬀectively beneﬁt the model’s performance.\n",
      "the proposed approach achieves the best performance in zero-\n",
      "shot lesion detection compared to baselines.\n",
      "to conﬁrm the validity of\n",
      "our method, we conduct extensive experiments under the zero-shot setting and\n",
      "include a series of fusion baselines: concatenation, non-maximum suppression\n",
      "(nms)\n",
      "these\n",
      "prompts give comparable performance to our single prompt and can be still be\n",
      "improved by our fusion method.\n",
      "here we present part of the single prompts used in the experiments\n",
      "for illustration.\n",
      "fine-tuned models can further improve the detection performance.\n",
      "we conduct 10-shot ﬁne-tuning experiments as a complement, and ﬁnd the per-\n",
      "formance greatly improved.\n",
      "therefore, we can conclude that the pre-trained glip\n",
      "model has the ability to learn a reasonable alignment between textual and visual\n",
      "modalities in medical domains.\n",
      "visualizations.\n",
      "syntax based fusion sometimes fails to ﬁlter out unreasonable\n",
      "predictions because these regions are generated directly by the vlm without\n",
      "further processing and eventually resulting in unstable detection performance.\n",
      "the step-wise clustering mechanism based on\n",
      "ensemble learning enables our method to exploit multi-dimensional information\n",
      "besides visual features.\n",
      "the results show that when combining the above three components,\n",
      "the proposed approach gives the best lesion detection performance, suggesting\n",
      "that all components are necessary and eﬀective in the proposed approach.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf:\n",
      "however, it is not clear how sensitive mil is to\n",
      "clinically realistic domain shifts, i.e., diﬀerences in data distribution that\n",
      "could negatively aﬀect performance, and if already existing metrics for\n",
      "detecting domain shifts work well with these algorithms.\n",
      "we trained an\n",
      "attention-based mil algorithm to classify whether a whole-slide image\n",
      "of a lymph node contains breast tumour metastases.\n",
      "our\n",
      "contributions include showing that mil for digital pathology is aﬀected\n",
      "by clinically realistic diﬀerences in data, evaluating which features from\n",
      "a mil model are most suitable for detecting changes in performance,\n",
      "and proposing an unsupervised metric named fr´echet domain distance\n",
      "(fdd) for quantiﬁcation of domain shifts.\n",
      "shift measure performance\n",
      "was evaluated through the mean pearson correlation to change in classiﬁ-\n",
      "cation performance, where fdd achieved 0.70 on 10-fold cross-validation\n",
      "models.\n",
      "fdd could be a valuable tool for care\n",
      "providers and vendors who need to verify if a mil system is likely to\n",
      "perform reliably when implemented at a new site, without requiring any\n",
      "additional annotations from pathologists.\n",
      "supported by swedish e-science research center, vinnova, the ceniit career\n",
      "development program at link¨oping university, and wallenberg ai, wasp funded by\n",
      "the knut and alice wallenberg foundation.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9 16.\n",
      "et al.\n",
      "keywords: domain shift detection · attention-based mil · digital\n",
      "pathology\n",
      "1\n",
      "introduction\n",
      "the spreading digitalisation of pathology labs has enabled the development of\n",
      "deep learning (dl) tools that can assist pathologists in their daily tasks.\n",
      "how-\n",
      "ever, supervised dl methods require detailed annotations in whole-slide images\n",
      "(wsis) which is time-consuming, expensive and prone to inter-observer disagree-\n",
      "ments\n",
      "this remains a signiﬁcant obstacle to the deployment of dl\n",
      "applications in clinical practice [7].\n",
      "hence, it is important\n",
      "to provide indications of the expected performance on a target dataset without\n",
      "requiring annotations [5,25].\n",
      "[33] which aims to detect individual samples that are ood, in contrast\n",
      "to our objective of estimating a diﬀerence of expected performances between\n",
      "some datasets.\n",
      "alternatively,\n",
      "a drop in performance can be estimated by comparing the model’s softmax out-\n",
      "puts [8] or some hidden features [24,28] acquired on in-domain and domain shift\n",
      "datasets.\n",
      "we show that our proposed unsupervised metric for quantifying\n",
      "domain shift correlates best with the changes in performance, in comparison to\n",
      "multiple baselines.\n",
      "the novel contributions of our work can be summarised as:\n",
      "1. proposing an unsupervised metric named fr´echet domain distance (fdd)\n",
      "for quantifying the eﬀects of domain shift in attention-based mil;\n",
      "2. showing how fdd can help to identify subsets of patient cases for which mil\n",
      "performance is worse than reported on the in-domain test data;\n",
      "3.\n",
      "comparing the eﬀectiveness of using uncertainty estimation versus learnt rep-\n",
      "resentations for domain shift detection in mil.\n",
      "2\n",
      "methods\n",
      "our experiments center on an mil algorithm with attention developed for clas-\n",
      "siﬁcation in digital pathology.\n",
      "[17] because it well represents an architecture of mil\n",
      "with attention, meaning that our approach can equally be applied to many other\n",
      "such methods.\n",
      "2.2\n",
      "mil features\n",
      "we explored several diﬀerent feature sets that can be extracted from the\n",
      "attention-based mil framework: learnt embedding of the instances (referred to\n",
      "as patch features), and penultimate layer features (penultimate features).\n",
      "inspired by fid, we propose a\n",
      "metric named fr´echet domain distance (fdd) for evaluating if a model is expe-\n",
      "riencing a drop in performance on some new dataset.\n",
      "141 wsis from axillary nodes dissection cases (57 wsis with metastases):\n",
      "potentially large shift as some patients have already started neoadjuvant\n",
      "treatment as well as the tissue may be aﬀected from the procedure of sentinel\n",
      "lymph node removal.\n",
      "2a.\n",
      "the sentinel/axillary division\n",
      "is motivated by the diﬀering dl prediction performance on such subsets, as\n",
      "detecting domain shift in mil\n",
      "161\n",
      "observed by jarkman et al.\n",
      "moreover, discussions with pathologists led to\n",
      "the conclusion that it is clinically relevant to evaluate the performance diﬀerence\n",
      "between ductal and lobular carcinoma.\n",
      "4\n",
      "experiments\n",
      "the goal of the study is to evaluate how well fddk and the baseline methods\n",
      "correlate with the drop in classiﬁcation performance of attention-based mil\n",
      "caused by several potential sources of domain shifts.\n",
      "in this section, we describe\n",
      "the experiments we conducted.\n",
      "the classiﬁcation perfor-\n",
      "mance is evaluated using the area under receiver operating characteristic curve\n",
      "(roc-auc) and matthews correlation coeﬃcient (mcc) [2].\n",
      "following the con-\n",
      "clusions of [2] that mcc well represents the full confusion matrix and the fact\n",
      "that in clinical practice a threshold needs to be set for a classiﬁcation decision,\n",
      "mcc is used as a primary metric of performance for domain shift analysis while\n",
      "roc-auc is reported for completeness.\n",
      "a large diﬀerence\n",
      "indicates a potential drop in performance.\n",
      "however, it is not trivial\n",
      "which hidden features of mil that are most suitable for this task, and we\n",
      "evaluate several options (see sect. 2.2) with both methods.\n",
      "162\n",
      "m. poceviˇci¯ut˙e et al.\n",
      "for all possible pairs of camelyon and the other test datasets, and for the\n",
      "10 cv models, we compute the domain shift measures and compare them to\n",
      "the observed drop in performance.\n",
      "all results\n",
      "are reported as mean and standard deviation over the 10-fold cv.\n",
      "5\n",
      "results\n",
      "the ﬁrst part of our results is the performance of the wsi classiﬁcation task\n",
      "across the subsets, summarized in table 1.\n",
      "overall, the performance is in\n",
      "line with previously published work [17,29].\n",
      "table\n",
      "1.\n",
      "classiﬁcation\n",
      "performance\n",
      "reported in mean (standard deviation) of\n",
      "mcc and roc-auc metrics, computed\n",
      "over the 10-fold cv models.\n",
      "clam models achieved\n",
      "better performance on ductal carcinoma compared to the in-domain camelyon\n",
      "test data.\n",
      "table 2 summarises the pearson correlation between the change in perfor-\n",
      "mance, i.e., the mcc diﬀerence between camelyon and other test datasets, and\n",
      "the domain shift measures for the same pairs.\n",
      "figure 1 shows how\n",
      "individual drop in performance of model-dataset combinations are related to the\n",
      "fdd64 metric.\n",
      "for most models detecting larger drop in performance (> 0.05)\n",
      "is easier on axillary lymph nodes data than on any other analysed dataset.\n",
      "pearson correlations between domain shift measure and diﬀerence in per-\n",
      "formance (mcc metric) of camelyon test set and the other datasets.\n",
      "however, the results\n",
      "were among the worst with any other number of k. both combined and positive\n",
      "evidence achieved peak performance of 0.68 (0.17) and 0.70 (0.13), respectively,\n",
      "when fd and k = 64 were used.\n",
      "we conclude that in our setup the best and\n",
      "most reliable performance of domain shift quantiﬁcation is achieved by positive\n",
      "evidence with fd and k = 64, i.e. fdd64.\n",
      "164\n",
      "m. poceviˇci¯ut˙e et al.\n",
      "6\n",
      "discussion and conclusion\n",
      "mil is aﬀected by domain shift.\n",
      "however, as clinically relevant subsets of brln\n",
      "data are analysed, stark diﬀerences in performance and reliability (indicated\n",
      "by the standard deviation) are revealed.\n",
      "the evaluated baselines that use uncer-\n",
      "tainty and conﬁdence aggregation for domain shift detection, i.e., de and doc,\n",
      "showed poor ability to estimate the experienced drop in performance (see table\n",
      "2).\n",
      "this could be a potential cause for the observed poor results by de and doc\n",
      "in our experiments.\n",
      "the highest\n",
      "pearson correlation between change in performance and a distance metric is\n",
      "achieved by fr´echet distance with 64 positive evidence features, fdd64 (see\n",
      "table 2).\n",
      "thus, it seems a critical component in domain\n",
      "shift measurement in attention-based mil is to correctly make use of the atten-\n",
      "tion scores.\n",
      "from fig. 1 we can see that if we further investigated all model-\n",
      "dataset combinations that resulted in fdd64 above 0.5, we would detect many\n",
      "cases with a drop in performance larger than 0.05.\n",
      "an interesting\n",
      "aspect is that the performance was better for the out-of-domain ductal subset\n",
      "compared to in-domain camelyon wsis.\n",
      "in practical applications, it may be a\n",
      "problem when the domain shift quantiﬁcation cannot separate between shifts\n",
      "having positive or negative eﬀect on performance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf:\n",
      "in this\n",
      "paper, we present a new method, prime+, for breast cancer risk pre-\n",
      "diction that leverages prior mammograms using a transformer decoder,\n",
      "outperforming a state-of-the-art risk prediction method that only uses\n",
      "mammograms from a single time point.\n",
      "experimental results show that our model achieves\n",
      "a statistically signiﬁcant improvement in performance over the state-of-\n",
      "the-art based model, with a c-index increase from 0.68 to 0.73 (p < 0.05)\n",
      "on held-out test sets.\n",
      "keywords: breast cancer · mammogram · risk prediction\n",
      "1\n",
      "introduction\n",
      "breast cancer impacts women globally [15] and mammographic screening for\n",
      "women over a certain age has been shown to reduce mortality\n",
      "to\n",
      "mitigate this, supplemental screening like mri or a tailored screening inter-\n",
      "val have been explored to add to the screening protocol\n",
      "recently, several studies [8,32,33] revealed the potential of artiﬁcial intelligence\n",
      "(ai) to develop a better risk assessment model to identify women who may ben-\n",
      "eﬁt from supplemental screening or a personalized screening interval and these\n",
      "may lead to improved screening outcomes.\n",
      "in clinical practice, breast density and traditional statistical methods for pre-\n",
      "dicting breast cancer risks such as the gail\n",
      "several\n",
      "studies have shown that utilizing past mammograms can improve the classi-\n",
      "ﬁcation performance of radiologists in the classiﬁcation of benign and malig-\n",
      "nant masses\n",
      "[11,25,26,29], especially for the detection of subtle abnormalities\n",
      "[25]. more recently, deep learning models trained on both prior and current\n",
      "mammograms have shown improved performance in breast cancer classiﬁcation\n",
      "tasks [24].\n",
      "the data comprises three main elements: features x, time of the event\n",
      "t, and the occurrence of the event e\n",
      "for medical applications, x typically\n",
      "represents patient information like age, family history, genetic makeup, and diag-\n",
      "nostic test results (e.g., a mammogram).\n",
      "δt\n",
      "(1)\n",
      "enhanced risk prediction with prior images\n",
      "391\n",
      "fig.\n",
      "a common backbone network extracts features from\n",
      "the prior and current images, resulting in xprior and xcurr.\n",
      "we use an imagenet pre-\n",
      "trained resnet-34\n",
      "[12] as the image feature backbone.\n",
      "the goal of training the\n",
      "model is to minimize this loss function, which encourages the model to make\n",
      "accurate predictions of the risk of developing breast cancer over time.\n",
      "2.3\n",
      "incorporating prior mammograms\n",
      "to improve the performance of the breast cancer risk prediction model, we incor-\n",
      "porate information from prior mammograms taken with the same view, using a\n",
      "transformer decoder structure\n",
      "= xcp c ⊕ xcurr, which is\n",
      "then used by the base hazard network and time-dependent hazard network to\n",
      "predict the cumulative hazard function ˆh.\n",
      "enhanced risk prediction with prior images\n",
      "393\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "dataset\n",
      "we compiled an in-house mammography dataset comprising 16,113 exams\n",
      "(64,452 images) from 9,113 patients across institutions from the united states,\n",
      "gathered between 2010 and 2021.\n",
      "mammograms were captured\n",
      "using hologic (72.3%) and siemens (27.7%) devices.\n",
      "the c-\n",
      "index measures the performance of a model by evaluating how well it correctly\n",
      "predicts the relative order of survival times for pairs of individuals in the dataset.\n",
      "time-dependent roc\n",
      "analysis generates an roc curve and the area under the curve (auc) for each\n",
      "speciﬁc time point in the follow-up period, enabling evaluation of the model’s\n",
      "performance over time.\n",
      "we evaluate the eﬀectiveness of prime+ by comparing it with two other\n",
      "models: (1) baseline based on mirai, a state-of-the art risk prediction method\n",
      "from [33], and (2) prime, a model that uses prior images by simply summing\n",
      "xcurr and xprior without the use of the transformer decoder.\n",
      "3.3\n",
      "implementation details\n",
      "our model is implemented in pytorch and trained on four v100 gpus.\n",
      "we\n",
      "trained the model using stochastic gradient descent (sgd) for 20k iterations\n",
      "with a learning rate of 0.005, weight decay of 0.0001, and momentum of 0.9.\n",
      "we resize the images to 960 × 640 pixels and use a batch size of 96.\n",
      "to\n",
      "augment the training data, we apply geometric transformations such as vertical\n",
      "ﬂipping, rotation and photometric transformations such as brightness/contrast\n",
      "adjustment, gaussian noise, sharpen, clahe, and solarize.\n",
      "empirically, we ﬁnd\n",
      "that strong photometric augmentations improved the risk prediction model’s\n",
      "394\n",
      "h. lee et al.\n",
      "table 1. ablation analysis on the eﬀectiveness of prior information and transformer\n",
      "decoder.\n",
      "all cases\n",
      "prior decoder c-index\n",
      "time-dependent auc\n",
      "1-year\n",
      "2-year\n",
      "3-year\n",
      "4-year\n",
      "✗\n",
      "✗\n",
      "0.68±0.03 0.70±0.05 0.71±0.04 0.70±0.04 0.71±0.09\n",
      "✓\n",
      "✗\n",
      "0.70±0.03 0.72±0.05 0.73±0.05 0.74±0.04 0.75±0.07\n",
      "✓\n",
      "✓\n",
      "0.73±0.03 0.75±0.05 0.75±0.04 0.77±0.04 0.76±0.08\n",
      "excluding cancer cases with event time < 180 days\n",
      "prior decoder c-index\n",
      "time-dependent auc\n",
      "1-year\n",
      "2-year\n",
      "3-year\n",
      "4-year\n",
      "✗\n",
      "✗\n",
      "0.63±0.04 0.64±0.10 0.66±0.08 0.64±0.06 0.64±0.11\n",
      "✓\n",
      "✗\n",
      "0.68±0.05 0.64±0.14 0.73±0.08 0.70±0.05 0.71±0.09\n",
      "✓\n",
      "✓\n",
      "0.70±0.04 0.68±0.13 0.76±0.07 0.73±0.05 0.71±0.10\n",
      "performance, while strong geometric transformations had a negative impact.\n",
      "by using the transformer decoder to jointly model prior images,\n",
      "we observed improved c-index from 0.70 (0.67 to 0.73) to 0.73 (0.70 to 0.76).\n",
      "we observe similar performance improvements when evaluating cases with\n",
      "at least 180 days to cancer diagnosis.\n",
      "the model must learn patterns of risk, not\n",
      "enhanced risk prediction with prior images\n",
      "395\n",
      "table 2. to better understand why the addition of prior images works, we split our\n",
      "test set into two groups based on the mammographic density: change and no change.\n",
      "the ﬁrst and second row corresponds to performance of the baseline and prime+\n",
      "model, respectively.\n",
      "our results support this intuition as the performance improvements over the\n",
      "baseline are much more pronounced for longer term risk (3, 4-year auc) than\n",
      "short term risk (1 year).\n",
      "the prime and prime+ models, which incorporate\n",
      "prior mammograms, show high performance for long-term risk prediction (3,\n",
      "4-year auc), indicating that considering changes in breast over time contain\n",
      "useful information for breast cancer risk prediction.\n",
      "lastly, we empirically conﬁrm that a transformer decoder eﬀectively models\n",
      "spatial relations between prior and current mammograms by demonstrating con-\n",
      "sistent performance improvements of prime+ across both short-term and long-\n",
      "term risk prediction settings.\n",
      "our results suggest that incorporating changes\n",
      "in patients using prior mammograms and a transformer decoder improves the\n",
      "performance of breast cancer risk prediction models.\n",
      "analysis based on density.\n",
      "to better understand why adding prior\n",
      "images improves performance, we divided our test set into subgroups to examine\n",
      "the performance of the baseline model and the prime+ model on each of these\n",
      "groups.\n",
      "women with dense breasts have a four-to six-fold\n",
      "higher risk of breast cancer\n",
      "the addition of mammographic breast density\n",
      "has improved the performance traditional breast cancer risk models\n",
      "[4] and can\n",
      "therefore help us understand why the addition of prior images works.\n",
      "density change was deﬁned according to whether\n",
      "the bi-rads category changed in the current image as compared to the prior\n",
      "396\n",
      "h. lee et al.\n",
      "table 3.\n",
      "in order to assess the performance of the models on varying levels of breast\n",
      "density, a critical risk factor, we divided our test set into two groups based on mam-\n",
      "mographic density: fatty and dense.\n",
      "0.78±0.05 0.76±0.08\n",
      "dense\n",
      "0.68±0.06 0.66±0.09 0.68±0.09 0.71±0.08 0.65±0.21\n",
      "0.71±0.05 0.72±0.08 0.73±0.08 0.72±0.08 0.72±0.25\n",
      "image.\n",
      "we suspect this is because deep neural networks generally work better on\n",
      "low density images given that visual cues of cancer in images with lower breast\n",
      "density are more clearly visible.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf:\n",
      "experimental results on\n",
      "lung (83 cts from 19 patients) and liver (77 cects from 18 patients) datasets\n",
      "with more than two scans per patient yielded an individual lesion change class\n",
      "accuracy of 98% and 85%, and identiﬁcation of patterns of lesion change with\n",
      "an accuracy of 96% and 76%, respectively.\n",
      "keywords: longitudinal follow-up · lesion matching · lesion change analysis\n",
      "1\n",
      "introduction\n",
      "the periodic acquisition and analysis of volumetric ct and mri scans of oncology\n",
      "patients is essential for the evaluation of the disease status, the selection of the treat-\n",
      "ment, and the response to treatment.\n",
      "currently, scans are acquired every 2–12 months\n",
      "according to the patient’s characteristics, disease stage, and treatment regime.\n",
      "the scan\n",
      "interpretation consists of identifying lesions (primary tumors, metastases) in the affected\n",
      "supplementary information the online version contains supplementary material available at\n",
      "https://doi.org/10.1007/978-3-031-43904-9_11.\n",
      "as\n",
      "treatments improve and patients live longer, the number of scans in longitudinal studies\n",
      "increases and their interpretation is more challenging and time-consuming.\n",
      "the recist 1.1 guidelines call for ﬁnding new lesions (if any), identify-\n",
      "ing up to the ﬁve largest lesions in each scan in the ct slice where they appear largest,\n",
      "manually measuring their diameters, and comparing their difference [1].\n",
      "while volu-\n",
      "metric measures of individual lesions and of all lesions (tumor burden) have long been\n",
      "established as more accurate and reliable than partial linear measurements, they are not\n",
      "used clinically because they require manual lesion delineation and lesion matching in\n",
      "unregistered scans, which is usually time-consuming and subject to variability [2].\n",
      "in a previous paper, we presented an automatic pipeline for the detection and quantiﬁ-\n",
      "cation of lesion changes in pairs of ct liver scans\n",
      "complex lesion changes include merged lesions, which occurs when at least two lesions\n",
      "grow and merge into one (possible disease progression), split lesions, which occurs\n",
      "when a lesion shrinks and cleaves into several parts (possible response to treatment) and\n",
      "conglomeration of lesions, which occurs when clusters of lesions coalesce.\n",
      "com-\n",
      "prehensive quantitative analysis of lesion changes and patterns is of clinical importance,\n",
      "since response to treatment may vary among lesions, so the analysis of a few lesions\n",
      "may not be representative.\n",
      "experimental results on lung (83 cts, 19 patients) and liver (77 cects, 18 patients)\n",
      "datasets show that our method yields high classiﬁcation accuracy.\n",
      "although many methods exist for object tracking in optical images\n",
      "and videos [15–17], they are unsuited for analyzing lesion changes since they assume\n",
      "many consecutive 2d images where objects have very similar appearance and undergo\n",
      "small changes between images.\n",
      "overlap-based methods pair two lesions in registered\n",
      "scans when their segmentations overlap, with a reported accuracy of 66–98% [3, 5–11,\n",
      "108\n",
      "b. di veroli et al.\n",
      "18].\n",
      "they are limited to 2d images, assume registration between images, and do not handle\n",
      "conglomerate changes.\n",
      "lesion matchings are computed with an overlap-based\n",
      "lesion pairing method after establishing a common reference frame by deformable regis-\n",
      "tration of the scans and organ segmentations.\n",
      "graph-theoretic automatic lesion tracking and detection\n",
      "109\n",
      "the method inputs the scans and the organ and lesion segmentations in each scan.\n",
      "the method is a pipeline of four steps: 1) pairwise\n",
      "deformable registration of each prior scan, organ and lesion segmentations, with the\n",
      "most recent (current) scan as in [3]; 2) overlap-based lesion matching; 3) construction of\n",
      "the lesion change graph from the individual lesion segmentations and lesion matches; 4)\n",
      "detection of changes in individual lesions and patterns of lesion changes from the graph\n",
      "properties and from analysis of its connected components.\n",
      "2.1\n",
      "problem formalization\n",
      "let s =\n",
      "\u0002\n",
      "s1, . . .\n",
      ", vi\n",
      "ni\n",
      "\u0005\n",
      "is a set of vertices vi\n",
      "j corresponding to the lesions associated\n",
      "with the lesion segmentation masks li =\n",
      "\u0004\n",
      "li\n",
      "1, li\n",
      "2, . .\n",
      "the lesion matching rule is lesion voxel overlap: when\n",
      "the lesion segmentation voxels li\n",
      "j, lk\n",
      "l of vertices vi\n",
      "j, vk\n",
      "l overlap, 1 ≤\n",
      "consecutivelesionmatchingonscans(si, si+1)isperformedwithaniterativegreedy\n",
      "strategy whose aim is to compensate for registration errors: 1) the lesion segmentations\n",
      "in li and li+1 are isotropically dilated in 3d by d millimeters; 2) for all pairs of lesions\n",
      "\u0007\n",
      "vi\n",
      "j, vi+1\n",
      "l\n",
      "\b\n",
      ", compute the intersection % of their corresponding lesion a segmentations\n",
      "(li\n",
      "j, li+1\n",
      "l\n",
      ") as max(\n",
      "\n",
      "\n",
      "\n",
      "li\n",
      "j ∩ li+1\n",
      "l\n",
      "\n",
      "\n",
      "\n",
      "/\n",
      "\n",
      "\n",
      "\n",
      "li\n",
      "j\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "li\n",
      "j ∩ li+1\n",
      "l\n",
      "\n",
      "\n",
      "\n",
      "/\n",
      "\n",
      "\n",
      "\n",
      "li+1\n",
      "l\n",
      "\n",
      "\n",
      "\n",
      "); ; 3) if the % intersection is ≥ p, then\n",
      "edge ei,i+1\n",
      "j,l\n",
      "=\n",
      "\u0007\n",
      "vi\n",
      "j, vi+1\n",
      "l\n",
      "\b\n",
      "is added to ec; ; 4) remove the lesion segmentations li\n",
      "j, li+1\n",
      "l\n",
      "from li, li+1, respectively.\n",
      "3\n",
      "experimental results\n",
      "we evaluated our method with two studies on retrospectively collected patient datasets\n",
      "that were manually annotated by an expert radiologist.\n",
      "we ran our method on the dlungs and dliver lesion segmentations.\n",
      "the\n",
      "settings of the parameters were: dilation distance d = 1 mm, overlap percentage p =\n",
      "10%, number of iterations r = 5 and 7, and centroid maximum distance δ = 17 and\n",
      "23 mm for the lungs and liver lesions, respectively.\n",
      "these patterns are hard\n",
      "to detect manually but their correct classiﬁcation and tracking are crucial for the proper\n",
      "application of the recist 1.1 follow-up protocol\n",
      "see the supplemental\n",
      "material for examples of these scenarios.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf:\n",
      "experimen-\n",
      "however, recognizing important diagnostic features from ceus videos\n",
      "to diagnose lesions as benign or malignant is non-trivial and requires lots of\n",
      "experience.\n",
      "to improve diagnostic eﬃciency and accuracy, many computational methods\n",
      "were proposed to analyze renal us images and could assist radiologists in making\n",
      "clinical decisions [6].\n",
      "however, most of these methods only focused on conven-\n",
      "tional b-mode images.\n",
      "in recent years, there has been increasing interest in\n",
      "multi-modal medical image fusion [1].\n",
      "weight-based fusion\n",
      "methods generally used an importance prediction module to learn the weight\n",
      "of each modality and then performed sum, replacement, or exchange based on\n",
      "the weights [7,16,17,19].\n",
      "nevertheless, we prove in our experiments that these attention-\n",
      "based methods may have the potential risks of entangling features of diﬀerent\n",
      "modalities.\n",
      "previous researches\n",
      "have proved that temporal information is eﬀective in improving the performance\n",
      "of deep learning models.\n",
      "lin et al.[11] proposed a network for breast lesion\n",
      "detection in us videos by aggregating temporal features, which outperformed\n",
      "other image-based methods.\n",
      "[2] showed that ceus videos can provide\n",
      "more detailed blood supply information of tumors allowing a more accurate\n",
      "breast lesion diagnosis than static us images.\n",
      "experimental results\n",
      "show that the proposed framework outperforms single-modal, single-frame, and\n",
      "other state-of-the-art methods in renal tumor diagnosis.\n",
      "it can be divided\n",
      "into two stages: single-frame detection stage and video-based diagnosis stage.\n",
      "(1)\n",
      "in the single-frame detection stage, the network predicts the tumor bounding box\n",
      "and category on each frame in the multi-modal ceus video clips.\n",
      "during the diagnostic process, experi-\n",
      "enced radiologists usually take the global features of us images into considera-\n",
      "tion [20].\n",
      "(2) in the video-based diagnosis stage, the network\n",
      "automatically chooses high-conﬁdence region features of each frame according to\n",
      "the single-frame detection results and performs temporal aggregation to output\n",
      "a more accurate diagnosis.\n",
      "the above two stages are trained successively.\n",
      "we\n",
      "ﬁrst perform a strong data augmentation to train the network for tumor detec-\n",
      "tion and classiﬁcation on individual frames.\n",
      "after that, the ﬁrst stage model is\n",
      "switched to the evaluation mode and predicts the label of each frame in the video\n",
      "clip.\n",
      "2.2\n",
      "dual-attention strategy for multimodal fusion\n",
      "using complementary information between multi-modal data can greatly\n",
      "improve the precision of detection.\n",
      "the process\n",
      "mentioned above can be formulated as follows:\n",
      "finvar = softmax(qbkt\n",
      "c\n",
      "√\n",
      "d\n",
      ")vc + softmax(qckt\n",
      "b\n",
      "√\n",
      "d\n",
      ")\n",
      "therefore, we design an ota module that\n",
      "aggregates single-frame renal tumor detection results in temporal dimension for\n",
      "diagnosing tumors as benign and malignant.\n",
      "after feature selection, we aggregate the features in the temporal\n",
      "dimension by time attention.\n",
      "3\n",
      "experimental results\n",
      "3.1\n",
      "materials and implementations\n",
      "we collect a renal tumor us dataset of 179 cases from two medical centers,\n",
      "which is split into the training and validation sets.\n",
      "we further collect 36 cases\n",
      "from the two medical centers mentioned above (14 benign cases) and another\n",
      "center (fujian provincial hospital, 22 malignant cases) to form the test set.\n",
      "some\n",
      "examples of the images are shown in fig.\n",
      "2. there is an obvious visual diﬀerence\n",
      "between the images from the fujian provincial hospital (last column in fig.\n",
      "more than two radiologists with ten\n",
      "years of experience manually annotate the tumor bounding box and class label at\n",
      "the frame level using the pair annotation software package (https://www.aipair.\n",
      "the number of cases and annotated frames is summarized in table 1.\n",
      "weights pre-trained from imagenet are used to initialize the swin-\n",
      "transformer backbone.\n",
      "data augmentation strategies are applied synchronously\n",
      "to b-mode and ceus-mode images for all experiments, including random rota-\n",
      "tion, mosaic, mixup, and so on.\n",
      "the\n",
      "weight decay is set to 0.0005 and the momentum is set to 0.9.\n",
      "all\n",
      "experiments are implemented in pytorch with an nvidia rtx a6000 gpu.\n",
      "ap50 and ap75 are used to assess the performance of single-frame detection.\n",
      "2. examples of the annotated b-mode and ceus-mode us images.\n",
      "3.2\n",
      "ablation study\n",
      "single-frame detection.\n",
      "as shown in table 2, using\n",
      "swin-transformer as the backbone in yolox achieves better performance than\n",
      "the original backbone while reducing half of the parameters.\n",
      "the improvement\n",
      "may stem from the fact that swin-transformer has a better ability to characterize\n",
      "global features, which is critical in us image diagnosis.\n",
      "however, “ca+sa” (row 6 in table 2) obtains inferior\n",
      "performance than “ca” (row 5 in table 2).\n",
      "we conjecture that connecting the two\n",
      "attention modules in series leads to the entanglement of modality-speciﬁc and\n",
      "modality-invariant information, which would disrupt the model training.\n",
      "therefore, the proposed method\n",
      "achieves the best performance.\n",
      "we investigate the performance of the ota module\n",
      "for renal tumor diagnosis in multi-modal videos.\n",
      "this\n",
      "suggests that the multi-frame model can provide a more comprehensive char-\n",
      "acterization of the tumor and thus achieves better performance.\n",
      "meanwhile,\n",
      "increasing the sampling interval tends to decrease the performance (row 4 and\n",
      "row 5 in table 3).\n",
      "this proves that complementary informa-\n",
      "tion exists among diﬀerent modalities.\n",
      "for a fair comparison with other fusion\n",
      "methods, we embed their fusion modules into our framework so that diﬀerent\n",
      "approaches can be validated in the same environment.\n",
      "this may be because\n",
      "the generated weights are biased to make similar decisions to the source domain,\n",
      "thereby reducing model generalization in the external data.\n",
      "tmm focuses on both modality-speciﬁc and\n",
      "modality-invariant information, but the chaotic confusion of the two types of\n",
      "information deteriorates the model performance.\n",
      "on the contrary, our amf\n",
      "module prevents information entanglement by conducting cross-attention and\n",
      "self-attention blocks in parallel.\n",
      "meanwhile,\n",
      "the improvement of the detection performance is beneﬁcial to our ota mod-\n",
      "ule to obtain lesion features from more precise locations, thereby improving the\n",
      "accuracy of benign and malignant renal tumor diagnosis.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf:\n",
      "accurate segmentation of polyps is a crucial step in the\n",
      "eﬃcient diagnosis of colorectal cancer during screening procedures.\n",
      "to address these\n",
      "limitations, we propose a novel feature enhancement network that lever-\n",
      "ages feature propagation enhancement and feature aggregation enhance-\n",
      "ment modules for more eﬃcient feature fusion and multi-scale feature\n",
      "propagation.\n",
      "speciﬁcally, the feature propagation enhancement module\n",
      "transmits all encoder-extracted feature maps from the encoder to the\n",
      "decoder, while the feature aggregation enhancement module performs\n",
      "feature fusion with gate mechanisms, allowing for more eﬀective infor-\n",
      "mation ﬁltering.\n",
      "the multi-scale feature aggregation module provides\n",
      "rich multi-scale semantic information to the decoder, further enhanc-\n",
      "ing the network’s performance.\n",
      "keywords: polyp segmentation · feature propagation · feature\n",
      "aggregation\n",
      "1\n",
      "introduction\n",
      "colorectal cancer is a life-threatening disease that results in the loss of millions\n",
      "of lives each year.\n",
      "hence, regular bowel screenings are recommended, where\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43904-9_61\n",
      "revisiting feature propagation and aggregation in polyp segmentation\n",
      "633\n",
      "endoscopy is the gold standard.\n",
      "to reduce\n",
      "the workload on physicians and enhance diagnostic accuracy, computer vision\n",
      "technologies, such as deep neural networks, are involved to assist in the pre-\n",
      "segmentation of endoscopic images.\n",
      "the unet-like model uses skip\n",
      "connections that transmit only single-stage features.\n",
      "in contrast, our approach utilizes\n",
      "fpe to propagate features from all stages, incorporating a gate mechanism to regulate\n",
      "the ﬂow of valuable information.\n",
      "deep learning-based image segmentation methods have gained popularity in\n",
      "recent years, dominated by unet\n",
      "[11] in the ﬁeld of medical image segmenta-\n",
      "tion.\n",
      "unet’s success has led to the development of several other methods that\n",
      "use a similar encoder-decoder architecture to tackle polyp segmentation, includ-\n",
      "ing resunet++\n",
      "however, these\n",
      "methods are prone to ineﬃcient feature fusion at the decoder due to the trans-\n",
      "mission of multi-stage features without ﬁltering out irrelevant information.\n",
      "to address these limitations, we propose a novel feature enhancement net-\n",
      "work for polyp segmentation that employs feature propagation enhancement\n",
      "(fpe) modules to transmit multi-scale features from all stages to the decoder.\n",
      "figure 1 illustrates a semantic comparison of our feature propagation scheme\n",
      "with the unet-like model.\n",
      "while the existing unet-like models use skip connec-\n",
      "tions to propagate a single-scale feature, our method utilizes fpe to propagate\n",
      "multi-scale features from all stages in encoder.\n",
      "more importantly, this research\n",
      "highlights the usage of fpe can eﬀectively replace skip connections by providing\n",
      "more comprehensive multi-scale characteristics from full stages in encoder.\n",
      "to\n",
      "further address the issue of high-level semantics being overwhelmed in the pro-\n",
      "gressive feature fusion process, we also integrate a feature aggregation enhance-\n",
      "ment (fae) module that aggregates the outputs of fpe from previous stages at\n",
      "634\n",
      "y. su et al.\n",
      "decoder.\n",
      "finally, we propose a multi-scale aggregation (msa) module appended\n",
      "to the output of the encoder to capture multi-scale features and provide the\n",
      "decoder with rich multi-scale semantic information.\n",
      "the msa incorporates a\n",
      "cross-stage multi-scale feature aggregation scheme to facilitate the aggregation\n",
      "of multi-scale features.\n",
      "overall, our proposed method improves upon existing\n",
      "unet-like encoder-decoder architectures by addressing the limitations in feature\n",
      "propagation and feature aggregation, leading to improved polyp segmentation\n",
      "performance.\n",
      "our major contributions to accurate polyp segmentation are summarized as\n",
      "follows.\n",
      "(1) the method addresses the limitations of the unet-like encoder-decoder\n",
      "architecture by introducing three modules: feature propagation enhance-\n",
      "ment (fpe), feature aggregation enhancement (fae), and multi-scale\n",
      "aggregation (msa).\n",
      "(2) fpe transmits all encoder-extracted feature maps to the decoder, and fae\n",
      "combines the output of the last stage at the decoder and multiple outputs\n",
      "from fpe.\n",
      "(3) the proposed method achieves state-of-the-art results in ﬁve polyp segmen-\n",
      "tation datasets and outperforms the previous cutting-edge approach by a\n",
      "large margin (3%) on cvc-colondb and etis datasets.\n",
      "our proposed feature enhancement network illustrated in fig.\n",
      "[16] pretrained on imagenet as the encoder.\n",
      "the decoder consists of\n",
      "three feature aggregation enhancement modules (fae) and a multi-scale aggre-\n",
      "gation module (msa).\n",
      "given an input image i, we ﬁrst extract the pyramidal\n",
      "features using the encoder, which is deﬁned as follows,\n",
      "p1, p2, p3, p4 = pvt(i)\n",
      "(1)\n",
      "where, {p1, p2, p3, p4} is the set of pyramidal features from four stages with the\n",
      "spatial size of 1/4, 1/8, 1/16, 1/32 of the input respectively.\n",
      "features with lower\n",
      "spatial resolution usually contain richer high-level semantics.\n",
      "then, these fea-\n",
      "tures are transmitted by the feature propagation enhancement module (fpe) to\n",
      "yield the feature set {c1, c2, c3, c4}, which provides multi-scale information from\n",
      "all the stages.\n",
      "this is diﬀerent from the skip connection which only transmits the\n",
      "single-scale features at the present stage.\n",
      "afterwards, feature fusion is performed by\n",
      "revisiting feature propagation and aggregation in polyp segmentation\n",
      "635\n",
      "(a) overall architecture\n",
      "(b) fpe\n",
      "(c) fae\n",
      "(d) msa\n",
      "fig.\n",
      "(a) overall architecture; (b) fpe:\n",
      "feature propagation enhancement module; (c) fae: feature aggregation enhancement\n",
      "module; (d) msa: multi-scale aggregation module\n",
      "fae in the decoder, whereby it progressively integrates the outputs from fpe\n",
      "and previous stages.\n",
      "the higher-level semantic features of the fpe output are\n",
      "capable of eﬀectively compensating for the semantics that may have been over-\n",
      "whelmed during the upsampling process.\n",
      "feature propagation enhancement module.\n",
      "in contrast to the traditional\n",
      "encoder-decoder architecture with skip connections, the fpe aims to transmit\n",
      "multi-scale information from full stage at the encoder to the decoder, rather than\n",
      "single-scale features at the current stage.\n",
      "the input of the fpe includes the features from the other three stages,\n",
      "in addition to the feature of the current stage, which delivers richer spatial and\n",
      "semantic information to the decoder.\n",
      "however, these multi-stage inputs need to be downsampled or upsampled to\n",
      "match the spatial resolution of the features at the present stage.\n",
      "the features from the other three stages, denoted as p1, p2, and p3, are\n",
      "downsampled or upsampled to generate p\n",
      "′\n",
      "1, p\n",
      "′\n",
      "2, and p\n",
      "′\n",
      "3.\n",
      "fpe\n",
      "leverages such gate mechanism to obtain informative features in p and passes\n",
      "them through a cu respectively.\n",
      "feature aggregation enhancement module.\n",
      "the fae is a novel approach\n",
      "that integrates the outputs of the last stages at the decoder with the fpe’s\n",
      "outputs at both the current and deeper stages to compensate for the high-level\n",
      "semantics that may be lost in the process of progressive feature fusion.\n",
      "in contrast\n",
      "to the traditional encoder-decoder architecture with skip connections, the fae\n",
      "assimilates the output of the present and higher-stage fpes, delivering richer\n",
      "spatial and semantic information to the decoder.\n",
      "the fae, depicted in fig. 2(c), integrates the outputs of the current and\n",
      "deeper fpe stages with high-level semantics.\n",
      "as an example, the last fae takes\n",
      "as inputs o2 (output of the penultimate fae), c1 (output of the current fpe\n",
      "stage), and c2 and c3 (outputs of fpe from deeper stages).\n",
      "multiple outputs\n",
      "from deeper fpe stages are introduced to compensate for high-level seman-\n",
      "tics.\n",
      "furthermore, gate mechanisms are utilized to ﬁlter out valueless features\n",
      "revisiting feature propagation and aggregation in polyp segmentation\n",
      "637\n",
      "for fusion, and the resulting enhanced feature is generated by a cu after con-\n",
      "catenating the ﬁltered features.\n",
      "finally, o2 is merged with the output feature\n",
      "through element-wise summation, followed by a cu to produce the ﬁnal output\n",
      "feature o1.\n",
      "[8], and\n",
      "etc. by aiding in forming a coarse location of the polyp and contributing to\n",
      "improved accuracy and performance.\n",
      "3\n",
      "experiments\n",
      "datasets.\n",
      "we conduct extensive experiments on ﬁve polyp segmentation\n",
      "datasets, including kvasir [6], cvc-clinicdb\n",
      "following the setting in [2,3,5,10,10,17,19], the model\n",
      "is trained using a fraction of the images from cvc-clinicdb and kvasir, and its\n",
      "performance is evaluated by the remaining images as well as those from cvc-t,\n",
      "cvc-colondb, and etis.\n",
      "in particular, there are 1450 images in the training\n",
      "set, of which 900 are from kvasir and 550 from cvc-clinicdb.\n",
      "the test set\n",
      "contains all of the images from cvc-t, cvc-colondb, and etis, which have\n",
      "60, 380, and 196 images, respectively, along with the remaining 100 images from\n",
      "kvasir and the remaining 62 images from cvc-clinicdb.\n",
      "implementations.\n",
      "we utilize pytorch 1.10 to run experiments on an nvidia\n",
      "rtx3090 gpu.\n",
      "we adopt the same data augmentation techniques as uacanet [8],\n",
      "including random ﬂip, random rotation, and color jittering.\n",
      "in evaluation phase,\n",
      "we mainly focus on mdice, miou, the two most common metrics in medical image\n",
      "638\n",
      "y. su et al.\n",
      "segmentation, to evaluate the performance of the model.\n",
      "according to the experimental\n",
      "settings, the results on cvc-clinicdb and kvasir demonstrate the learning abil-\n",
      "ity of the proposed model, while the results on cvc-t, cvc-colondb, and etis\n",
      "demonstrate the model’s ability for cross-dataset generalization.\n",
      "the experimen-\n",
      "tal results are listed in table.1.\n",
      "furthermore, our proposed method demonstrates strong cross-dataset gener-\n",
      "alization capability on cvc-t, cvc-colondb, and etis datasets, with partic-\n",
      "ularly good performance on the latter two due to their larger and more represen-\n",
      "tative datasets.\n",
      "these results validate the eﬀectiveness of feature-level enhancement and high-\n",
      "light the superior performance of our method.\n",
      "we carried out ablation experiments to verify the eﬀective-\n",
      "ness of the proposed fpe, fae, and msa.\n",
      "for our baseline, we use the sim-\n",
      "ple encoder-decoder structure with skip connections for feature fusion and per-\n",
      "form element-wise summation at the decoder.\n",
      "table 2 presents the results of our\n",
      "ablation experiments.\n",
      "following the ablation study conducted on our proposed\n",
      "revisiting feature propagation and aggregation in polyp segmentation\n",
      "639\n",
      "approach, it is with conﬁdence that we assert the signiﬁcant contribution of\n",
      "each module to the overall performance enhancement compared to the baseline.\n",
      "our results indicate that the impact of each module on the ﬁnal performance\n",
      "is considerable, and their combination yields the optimal overall performance.\n",
      "for\n",
      "miou, the improvements are 1.5% and 3.5% on the corresponding datasets.\n",
      "3. exemplary images and results that are segmented by diﬀerent approaches.\n",
      "4\n",
      "conclusion\n",
      "we introduce a new approach to polyp segmentation that addresses ineﬃcient\n",
      "feature propagation in existing unet-like encoder-decoder networks.\n",
      "speciﬁcally,\n",
      "640\n",
      "y. su et al.\n",
      "a feature propagation enhancement module is introduced to propagate multi-\n",
      "scale information over full stages in the encoder, while a feature aggregation\n",
      "enhancement module is attended at the decoder side to prevent the loss of\n",
      "high-level semantics during progressive feature fusion.\n",
      "experimental results on ﬁve popular polyp\n",
      "datasets demonstrate the eﬀectiveness and superiority of our proposed method.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf:\n",
      "to reduce false posi-\n",
      "tives, we identify three challenges: (1) unlike natural images, a malignant\n",
      "mammogram typically contains only one malignant ﬁnding; (2) mam-\n",
      "mography exams contain two views of each breast, and both views ought\n",
      "to be considered to make a correct assessment; (3) most mammograms\n",
      "are negative and do not contain any ﬁndings.\n",
      "in this work, we tackle the\n",
      "three aforementioned challenges by: (1) leveraging sparse r-cnn and\n",
      "showing that sparse detectors are more appropriate than dense detectors\n",
      "for mammography; (2) including a multi-view cross-attention module\n",
      "to synthesize information from diﬀerent views; (3) incorporating multi-\n",
      "instance learning (mil) to train with unannotated images and perform\n",
      "breast-level classiﬁcation.\n",
      "we validate m&m’s detec-\n",
      "tion and classiﬁcation performance using ﬁve mammography datasets.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9_75.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "(b) quantitative detection evaluation with\n",
      "and without negative images on optimam.\n",
      "retina\n",
      "net\n",
      "fcos\n",
      "faster\n",
      "r-cnn\n",
      "cascade\n",
      "r-cnn\n",
      "m&m\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "-26.9\n",
      "-24.3\n",
      "-25.4\n",
      "-23.0\n",
      "-3.5\n",
      "average precision (ap)\n",
      "without negative\n",
      "with negative\n",
      "fig.\n",
      "(a) few\n",
      "works report detailed performance in the clinically relevant region of less than 1\n",
      "fp/image.\n",
      "across all dense mod-\n",
      "els, there is a large performance drop in the clinically representative setting that includes\n",
      "negative images.\n",
      "this means that the dense models are producing too many fps on neg-\n",
      "ative images.\n",
      "our model, m&m, successfully tackles this performance gap.\n",
      "rate of false positive (fp) predictions of cad can cause a signiﬁcant reduc-\n",
      "tion in radiologists’ speciﬁcity\n",
      "1a, most works focus on reporting\n",
      "recalls outside the clinically relevant region of less than 1 fp/image.\n",
      "to tackle the high rate of false positives in mammography, we identify three\n",
      "challenges: (1) a malignant mammogram typically contains only one malignant\n",
      "ﬁnding.\n",
      "this is diﬀerent from natural images: for example, an image in coco\n",
      "contains on average 7.7 objects [11].\n",
      "this calls into question the usage of dense\n",
      "detectors for mammography; (2) a standard screening exam consists of two views\n",
      "per breast.\n",
      "however, excluding negative\n",
      "images from training and evaluation leads to a distribution shift since negative\n",
      "images are abundant in clinical practice.\n",
      "to achieve these goals,\n",
      "m&m leverages three components: (1) sparse r-cnn to replace dense anchors\n",
      "with a set of sparse proposals; (2) multi-view cross-attention to synthesize\n",
      "780\n",
      "y. n. truong vu et al.\n",
      "information from two views and iteratively reﬁne the predictions, and (3) multi-\n",
      "instance learning (mil) to include negative images during training.\n",
      "with sparse r-cnn,\n",
      "m&m generalizes better to clinically-representative data, where the majority\n",
      "of images are negative, i.e., have no ﬁndings (table 2);\n",
      "with multi-view reasoning, m&m\n",
      "improves the recall at 0.1 fp/image by 8.6%, as shown in fig.\n",
      "we leverage mil to include images without bounding boxes during training\n",
      "(sec. 2.3).\n",
      "accordingly, m&m sees seven times more images during training.\n",
      "with mil, m&m improves the recall at 0.1 fp/image by 12.6% (fig. 4).\n",
      "1b, dense detectors generalize poorly to negative images as they\n",
      "produce too many false positives.\n",
      "m&m tackles false positives through (1, blue, dotted arrows) leveraging the\n",
      "sparse r-cnn cascade architecture to iteratively reﬁne sparse learnable proposals into\n",
      "predictions, (2, red, solid arrows) incorporating a cross-attention module to reason\n",
      "about relations between objects across two views, and (3, green, dashed arrows) uti-\n",
      "lizing image and breast mil pooling to train with images that do not have lesion\n",
      "annotations.\n",
      "yet, a model\n",
      "generalizes poorly if these negative images are dropped during training (fig. 1b).\n",
      "since image- and breast-level labels are available, we adopt an mil module\n",
      "to include images without bounding boxes during training.\n",
      "to compute image-\n",
      "and breast-level scores, we leverage the proposal malignancy logits mi (eq.\n",
      "since an image is malignant if it contains a malignant lesion, we obtain image-\n",
      "level scores by applying the noisyor function f(x)\n",
      "next, as cc and mlo views\n",
      "oﬀer complimentary information on a breast, we obtain breast-level malignancy\n",
      "score by averaging the image-level scores across these views.\n",
      "we apply cross-entropy losses limage and lbreast at the image and breast\n",
      "level for all training samples.\n",
      "we thus obtain the following total training loss for m&m:\n",
      "l = 1annotated lesionllesion + 0.5limage + 0.5lbreast.\n",
      "(5)\n",
      "3\n",
      "experiments\n",
      "implementation details.\n",
      "we resize the images’ shorter edges to 2560 with the larger edges no longer than\n",
      "3328.\n",
      "each batch contains 16 breasts (32 images).\n",
      "we employ a 1:1\n",
      "sampling ratio between unannotated and annotated images.\n",
      "we utilize three 2d digital mammography datasets: (1) optimam :\n",
      "a development dataset derived from the optimam database\n",
      "δ denotes the ap gap\n",
      "between evaluating with and without negative images.\n",
      "we report average precision with intersection over union from 0.25\n",
      "to 0.75.\n",
      "apmb denotes average precision on the set of annotated malignant and\n",
      "benign images.\n",
      "ap denotes average precision when all data is included.\n",
      "we\n",
      "report free response operating characteristic (froc) curves and recalls at var-\n",
      "ious fp/image (r@t).\n",
      "[31]\n",
      "0.735\n",
      "phresnet50 [14]\n",
      "0.739\n",
      "cross-view transformer [27]\n",
      "0.803∗\n",
      "m&m (ours)\n",
      "0.883\n",
      "23 points (pt) between excluding and including negative images.\n",
      "large δ means\n",
      "the models are producing too many fps on negative images.\n",
      "with this performance gap closed, m&m is\n",
      "able to achieve a high recall of 87.7% at just 0.1 fp/image.\n",
      "for all\n",
      "models, the breast-level score is the average of the cc score and mlo score,\n",
      "while the exam-level score is the max of the left breast score and right breast\n",
      "score.\n",
      "4. eﬀect of m&m’s components on classiﬁcation and detection performance.\n",
      "0.08–0.12 exam auc when evaluated on inhouse-a and inhouse-b. in compar-\n",
      "ison, m&m has smaller performance gaps of 0.02 on inhouse-a and 0.04 on\n",
      "inhouse-b. similar observations for other classiﬁers, such as eﬃcientnet, are\n",
      "reported in the appendix.\n",
      "on the left, we demonstrate how each component of m&m con-\n",
      "tributes to closing the gap δ between evaluating with and without negative\n",
      "images.\n",
      "mil allows the model to train with\n",
      "signiﬁcantly more negative images, reducing δ to −3.6pt (row 4).\n",
      "on the right\n",
      "of fig. 4, the froc curves show how each component of m&m improves recall\n",
      "signiﬁcantly at low fp/image.\n",
      "in particular, m&m’s recall at 0.1fp/image is\n",
      "86.3%, +21.2% over vanilla sparse r-cnn.\n",
      "further studies.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf:\n",
      "liver tumor segmentation and classiﬁcation are important\n",
      "tasks in computer aided diagnosis.\n",
      "it uses a mask transformer to jointly segment\n",
      "and classify each lesion with improved anchor queries and a foreground-\n",
      "enhanced sampling loss.\n",
      "it also has an image-wise classiﬁer to eﬀectively\n",
      "aggregate global information and predict patient-level diagnosis.\n",
      "on contrast-\n",
      "enhanced ct, our lesion-level detection precision, recall, and classiﬁca-\n",
      "tion accuracy are 92%, 89%, and 86%, outperforming widely used cnn\n",
      "and transformers for lesion segmentation.\n",
      "plan is on par with a senior human radi-\n",
      "ologist, showing the clinical signiﬁcance of our results.\n",
      "keywords: liver tumor · lesion segmentation and classiﬁcation · ct\n",
      "1\n",
      "introduction\n",
      "liver cancer is the third leading cause of cancer death world-wide in 2020\n",
      "early detection and accurate diagnosis of liver tumors may improve overall\n",
      "partially supported by the national natural science foundation of china (grant\n",
      "82071885), basic research projects of liaoning provincial department of education\n",
      "(ljkmz20221160), the national youth talent support program of china, and science\n",
      "and technology innovation talent project in shenyang (rc210265).\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9 8.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "[18] and\n",
      "esophagus [20] have shown that latest deep learning techniques can detect subtle\n",
      "texture and shape changes in nc ct that even human eyes may miss.\n",
      "thus,\n",
      "we aim to investigate the performance of liver tumor segmentation and classiﬁ-\n",
      "cation in nc cts.\n",
      "after an incidental\n",
      "tumor is found, the patient may undergo further imaging examination such as\n",
      "a multi-phase dce ct for diﬀerential diagnosis [11], which can provide useful\n",
      "discriminative information such as the vascularity of lesions and the pattern of\n",
      "contrast agent enhancement [19].\n",
      "liver is largest solid organ in body and is the\n",
      "site of many tumor types [11].\n",
      "therefore, accurate tumor type classiﬁcation is\n",
      "important for the decision of treatment plans and prognosis.\n",
      "many researchers have developed algorithms to automatically segment [1,9,\n",
      "13,15,23] or classify [19,21,25] liver tumors in ct to help radiologists improve\n",
      "their accuracy and eﬃciency.\n",
      "for example, public datasets such as the liver\n",
      "tumor segmentation benchmark (lits)\n",
      "[1] fostered a series of works aiming to\n",
      "segment liver tumors with improved convolutional neural network (cnn) back-\n",
      "bones\n",
      "besides\n",
      "lesion segmentation, cnn-based lesion classiﬁcation algorithms have been stud-\n",
      "ied to distinguish common lesion types [19,21,25].\n",
      "in this paper, we build a comprehensive framework to address both tumor\n",
      "screening and diagnosis.\n",
      "most existing works in tumor\n",
      "segmentation and detection did not explicitly consider it since their training and\n",
      "testing images are all tumor patients.\n",
      "such models may generate false positives\n",
      "in real-world screening scenario when facing diverse tumor-free images.\n",
      "(2) most works studied liver tumor segmentation alone without\n",
      "diﬀerentiating tumor types, while a few works classify liver tumors on cropped\n",
      "tumor patches\n",
      "meanwhile, we learn tumor segmentation and classiﬁ-\n",
      "cation with one network using an instance segmentation framework [3].\n",
      "(3) for evaluation,\n",
      "previous segmentation works typically use pixel-level metrics such as dice coef-\n",
      "ﬁcient.\n",
      "such metrics cannot reﬂect the lesion-level accuracy (how many lesion\n",
      "instances are correctly detected and classiﬁed) and may bias to large lesions when\n",
      "a patient has multiple tumors.\n",
      "patient-level metrics (e.g. classifying whether a\n",
      "subject has malignant tumors) are also useful for treatment recommendation\n",
      "in clinical practice [18,20].\n",
      "algorithms for liver tumor segmentation have focused on improving the fea-\n",
      "ture extraction backbone of a fully-convolutional cnn [9,13,15,23].\n",
      "the pixel-\n",
      "wise segmentation architectures may not be optimal for lesion and patient-level\n",
      "evaluation metrics since they cannot consider a lesion or an image holistically.\n",
      "[3,4,17] have emerged in the\n",
      "computer vision community and achieved the state-of-the-art performance in\n",
      "instance segmentation tasks.\n",
      "in brief, they use object queries to interact with\n",
      "image feature maps and with each other to produce mask and class predictions\n",
      "for each instance.\n",
      "inspired by them, we propose a novel end-to-end framework\n",
      "named pixel-lesion-patient network (plan) for lesion segmentation and classi-\n",
      "ﬁcation, as well as patient classiﬁcation.\n",
      "it contains three branches with bottom-\n",
      "up cooperation: the segmentation map from the pixel branch helps to initialize\n",
      "the lesion branch, which is an improved mask transformer aiming to segment and\n",
      "classify each lesion; the patient branch aggregates information from the whole\n",
      "image and predicts image-level labels of each lesion type, with regularization\n",
      "terms to encourage consistency with the lesion branch.\n",
      "on the non-contrast tumor screening\n",
      "and diagnosis task, plan achieves 95.0%, 96.4%, and 0.965 in patient-level sen-\n",
      "sitivity, speciﬁcity, and average auc for malignant and benign patients, in con-\n",
      "trast to 94.4%, 93.7%, and 0.889 for the widely-used nnu-net [8].\n",
      "2\n",
      "method\n",
      "2.1\n",
      "preliminary on mask transformer\n",
      "mask transformers are a series of latest works achieving superior accuracy\n",
      "on various segmentation tasks\n",
      "[3,4,17,22]. diﬀerent from traditional fully-\n",
      "convolutional segmentators [8] that predict a class label for each pixel, mask\n",
      "transformers predict a class label and a binary mask for each object.\n",
      "the image, where m is the embedding dimension, d × h × w is the shape\n",
      "of the 3d image.\n",
      "they are processed by a transformer\n",
      "decoder to interact with multi-scale image features and each other using cross\n",
      "and self-attention operations.\n",
      "multiplying qi with p gives the\n",
      "binary mask mi ∈ rd×h×w of object i. during inference, the class and mask\n",
      "predictions of all queries can be merged by matrix multiplication to obtain the\n",
      "ﬁnal semantic segmentation result ˆy ∈ rc×d×h×w .\n",
      "mask transformers have various advantages when applied to our task.\n",
      "therefore, we pioneer mask transformers’ adaptation\n",
      "for lesion segmentation and classiﬁcation in 3d medical images.\n",
      "given a ground-\n",
      "truth or a predicted lesion mask image, we perform connected component (cc)\n",
      "analysis and treat each cc as a lesion instance for training and evaluation.\n",
      "76\n",
      "k. yan et al.\n",
      "2.2\n",
      "pixel-lesion-patient network (plan)\n",
      "our goal is to segment the mask and classify the type of each tumor in a liver ct.\n",
      "[3] with three key improvements: (1) a pixel branch is added to\n",
      "provide anchor queries to the lesion branch.\n",
      "(2) the lesion branch is composed\n",
      "of the transformer decoder in mask2former, and we improve its segmentation\n",
      "loss to enhance recall of small lesions.\n",
      "(3) a patient branch is attached to make\n",
      "dedicated image-level predictions with a proposed lesion-patient consistency loss.\n",
      "the pixel branch is a convolutional\n",
      "layer after the pixel decoder and learns to predict pixel-wise segmentation maps\n",
      "similar to traditional segmentators.\n",
      "we do cc analysis to the predicted mask\n",
      "to extract lesion instances, and then average the pixel embeddings inside each\n",
      "predicted lesion to obtain a feature vector.\n",
      "compared to the random queries in the original mask2former, the\n",
      "anchor queries contain prior information of the lesions to be segmented, helping\n",
      "the lesion branch to match with the lesion targets more easily\n",
      "1. mask2former calculates its segmentation loss on k sam-\n",
      "pled pixels instead of on the whole image, which is shown to both improve\n",
      "accuracy and reduce gpu memory usage [3].\n",
      "however, in lesion segmentation,\n",
      "some tumors are very small compared to the whole 3d image.\n",
      "the importance\n",
      "sampling strategy [3] can hardly select any foreground pixels in such cases, so\n",
      "the loss only contains background pixels, degrading the segmentation recall of\n",
      "small lesions.\n",
      "a patient-level diagnosis is useful for triage.\n",
      "for example,\n",
      "diagnosing the subject as normal, benign, or malignant will result in completely\n",
      "diﬀerent treatments [24].\n",
      "intuitively, we can also infer patient-level labels from\n",
      "segmentation results by checking if there is any lesion in the predicted mask.\n",
      "since one patient can have\n",
      "multiple liver tumors of diﬀerent types, in our problem, we give each image\n",
      "several hierarchical binary labels.\n",
      "the ﬁrst label classiﬁes normal and tumor\n",
      "subjects (whether the image contains any tumor); the second and third labels\n",
      "indicate the existence of respectively benign and malignant tumors; the rest\n",
      "c\n",
      "we employ\n",
      "the dual-path transformer block [17] to fuse multi-scale features from the pixel\n",
      "encoder and decoder to generate a feature map, followed by global average pool-\n",
      "ing and a linear classiﬁcation layer to predict the c + 3 labels.\n",
      "liver tumor screening and diagnosis in ct\n",
      "77\n",
      "a lesion-patient consistency loss is further proposed to encourage coher-\n",
      "ence of the lesion and patient-level predictions.\n",
      "the overall loss of plan is listed in eq. 1, where lpixel is the combined cross-\n",
      "entropy (ce) and dice loss for the pixel branch as in nnu-net [8]; llesion-class\n",
      "is the ce loss [3] for lesion classiﬁcation in the lesion branch; llesion-mask is\n",
      "the combined ce and dice loss [3] for binary lesion segmentation in the lesion\n",
      "branch with the foreground-enhanced sampling strategy; lpatient is the binary\n",
      "ce loss for the multi-label classiﬁcation task in the patient branch.\n",
      "(1)\n",
      "3\n",
      "experiments\n",
      "data.\n",
      "eight tumor types are considered in our study: hepa-\n",
      "tocellular carcinoma (hcc), intrahepatic cholangiocarcinoma (icc), metastasis\n",
      "(meta), hepatoblastoma (hepato), hemangioma (heman), focal nodular hyper-\n",
      "plasia (fnh), cyst, and others (all other tumor types).\n",
      "if a lesion’s type cannot\n",
      "be determined according to image signs [11] and pathology, it will be marked\n",
      "as “unknown” and ignored in training and evaluation.\n",
      "detailed\n",
      "statistics and examples of the lesions are shown in the supplementary material.\n",
      "implementation details.\n",
      "we ﬁrst train an nnu-net on public datasets to segment liver and surround-\n",
      "ing organs (gallbladder, hepatic vein, spleen, stomach, and pancreas), and then\n",
      "crop the liver region to train plan.\n",
      "to help plan diﬀerentiate liver tumors\n",
      "and other organs, we train the network to segment both tumors and organs\n",
      "patient-level performance on the test set of 500 cases.\n",
      "2\n",
      "malignant\n",
      "benign\n",
      "8-class average\n",
      "nnu-net [8]\n",
      "94.4\n",
      "95.1\n",
      "91.0\n",
      "0.948\n",
      "0.829\n",
      "0.863\n",
      "mask2former\n",
      "the number of random queries is\n",
      "q = 20; the embedding dimension is m = 64; the number of sampled pixels is\n",
      "k = 12544\n",
      "for dce ct, the three phases form a 3-channel image as\n",
      "the network input.\n",
      "extensive data augmentation is applied including random\n",
      "cropping, scaling, ﬂipping, elastic deformation, and brightness adjustment\n",
      "among the\n",
      "8 tumor types, hcc, icc, meta, and hepato are malignant; heman, fnh, and\n",
      "cyst are benign.\n",
      "the nc test set contains 198 tumor cases, 202\n",
      "completely normal cases, and 100 “hard” non-tumor cases which may have larger\n",
      "image noise, artifact, ascites, diﬀuse liver diseases such as hepatitis and steatosis.\n",
      "these cases are used to test the robustness of the model in real-world screening\n",
      "scenario with diverse tumor-free images.\n",
      "as displayed in\n",
      "table 1, plan achieves the best accuracy on all tasks, especially in nc pre-\n",
      "liminary diagnosis tasks, which demonstrates the eﬀectiveness of its dedicated\n",
      "patient branch that can explicitly aggregate features from the whole image.\n",
      "lesion and pixel-level results.\n",
      "liver tumor screening and diagnosis in ct\n",
      "79\n",
      "table 2. lesion-level performance (precision, recall, recall of lesions with diﬀerent\n",
      "radius, classiﬁcation accuracy of 8 tumor types), and pixel-level performance (dice per\n",
      "case).\n",
      "2. roc curve of our method versus\n",
      "2 radiologists’ performance.\n",
      "lesions smaller than 3 mm in radius are ignored.\n",
      "although nc images have low contrast, they\n",
      "can still be used to segment and classify lesions with ∼ 80% precision, recall, and\n",
      "classiﬁcation accuracy.\n",
      "it can be seen that\n",
      "our proposed anchor queries produced by the pixel branch, fes loss, and lesion-\n",
      "patient consistency loss are useful for the ﬁnal performance.\n",
      "due to space limit, we will show the accuracy for each\n",
      "tumor type and more qualitative examples in the supplementary material.\n",
      "comparison with literature.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf:\n",
      "generic detectors perform below expectations on gld\n",
      "tasks for 2 reasons: 1) the scale of labeled data of gld datasets is far\n",
      "smaller than that of natural-image object detection datasets.\n",
      "2) gastro-\n",
      "scopic images exhibit distinct diﬀerences from natural images, which are\n",
      "usually of high similarity in global but high diversity in local.\n",
      "such char-\n",
      "acteristic of gastroscopic images also degrades the performance of using\n",
      "generic self-supervised or semi-supervised methods to solve the labeled\n",
      "data shortage problem using massive unlabeled data.\n",
      "in this paper, we\n",
      "propose self- and semi-supervised learning (ssl) for gld tailored for\n",
      "using massive unlabeled gastroscopic images to enhance gld tasks per-\n",
      "formance, which consists of a hybrid self-supervised learning (hsl)\n",
      "method for backbone pre-training and a prototype-based pseudo-label\n",
      "generation (ppg) method for semi-supervised detector training.\n",
      "the\n",
      "hsl combines patch reconstruction with dense contrastive learning to\n",
      "boost their advantages in feature learning from massive unlabeled data.\n",
      "moreover, we contribute the ﬁrst large-scale\n",
      "gld datasets (lgldd), which contains 10,083 gastroscopic images\n",
      "with 12,292 well-annotated boxes for four-category lesions.\n",
      "experiments\n",
      "on lgldd demonstrate that ssl can bring signiﬁcant improvement.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9_9.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "although deep neural network-based object detectors\n",
      "achieve tremendous success within the domain of natural images, directly train-\n",
      "ing generic object detectors on gld datasets performs below expectations for\n",
      "two reasons: 1) the scale of labeled data in gld datasets is limited in compar-\n",
      "ison to natural images due to the annotation costs.\n",
      "though gastroscopic images\n",
      "are abundant, those containing lesions are rare, which necessitates extensive\n",
      "image review for lesion annotation.\n",
      "2) the characteristic of gastroscopic images\n",
      "exhibits distinct diﬀerences from the natural images [18,19,21] and is often of\n",
      "high similarity in global but high diversity in local.\n",
      "speciﬁcally, each type of\n",
      "lesion may have diverse appearances though gastroscopic images look quite sim-\n",
      "ilar.\n",
      "generic self-supervised backbone pre-training or semi-supervised\n",
      "detector training methods can solve the ﬁrst challenge for natural images but its\n",
      "eﬀectiveness is undermined for gastroscopic images due to the second challenge.\n",
      "self-supervised backbone pre-training methods enhance object detection\n",
      "performance by learning high-quality feature representations from massive unla-\n",
      "belled data for the backbone.\n",
      "self- and semi-supervised learning for gastroscopic lesion detection\n",
      "85\n",
      "image modeling\n",
      "on the other hand, masked image modeling is expert in\n",
      "extracting local detailed information but is weak in preserving the discriminabil-\n",
      "ity of feature representation.\n",
      "semi-supervised object detection methods [12,14,16,17,20,22,23] ﬁrst use\n",
      "detectors trained with labeled data to generate pseudo-labels for unlabeled data\n",
      "and then enhance object detection performance by regarding these unlabeled\n",
      "data with pseudo-labels as labeled data to train the detector.\n",
      "the motivation of this paper is to explore how to enhance gld perfor-\n",
      "mance using massive unlabeled gastroscopic images to overcome the labeled\n",
      "data shortage problem.\n",
      "enlightened by this, we propose the self- and semi-supervised\n",
      "learning (ssl) framework tailored to address challenges in daily clinical prac-\n",
      "tice and use massive unlabeled data to enhance gld performance.\n",
      "ssl over-\n",
      "comes the challenges of gld by leveraging a large volume of unlabeled gastro-\n",
      "scopic images using self-supervised learning for improved feature representations\n",
      "and semi-supervised learning to discover and utilize potential lesions to enhance\n",
      "performance.\n",
      "[10] with the patch recon-\n",
      "struction to inherit the advantages of discriminative feature learning and grasp\n",
      "the detailed information that is important for gld tasks.\n",
      "more-\n",
      "over, we propose the ﬁrst large-scale gld datasets (lgldd), which contains\n",
      "10,083 gastroscopic images with 12,292 well-annotated lesion bounding boxes\n",
      "of four categories of lesions (polyp, ulcer, cancer, and sub-mucosal tumor).\n",
      "we\n",
      "evaluate ssl with multiple detectors on lgldd and ssl brings signiﬁcant\n",
      "improvement compared with baseline methods (centernet [6]: +2.7ap, faster\n",
      "rcnn\n",
      "in summary, our contributions include:\n",
      "– a self- and semi-supervise learning (ssl) framework to leverage massive\n",
      "unlabeled data to enhance gld performance.\n",
      "– a large-scale gastroscopic lesion detection datasets (lgldd)\n",
      "86\n",
      "x. zhang et al.\n",
      "– experiments on lgldd demonstrate that ssl can bring signiﬁcant enhance-\n",
      "ment compared with baseline methods.\n",
      "1.\n",
      "2.1\n",
      "hybrid self-supervised learning\n",
      "the motivation of hybrid self-supervised learning (hsl) is to learn the local\n",
      "feature representations of high discriminability meanwhile contain detailed infor-\n",
      "mation for the backbone from massive unlabeled gastroscopic images.\n",
      "among\n",
      "existing backbone pre-training methods, dense contrastive learning can preserve\n",
      "local discriminability and masked image modeling can grasp local detailed infor-\n",
      "mation.\n",
      "therefore, to leverage the advantages of both types of methods, we\n",
      "propose hybrid self-supervised learning (hsl), which combines patch recon-\n",
      "struction with dense contrastive learning to achieve the goal.\n",
      "structure.\n",
      "hsl heritages the structure of the densecl\n",
      "the global projection head\n",
      "and the dense projection head heritages from the densecl\n",
      "[10], and the pro-\n",
      "posed reconstruction projection head is inspired by the masked image modeling.\n",
      "like other self-supervised contrastive learning methods,\n",
      "hsl randomly generates 2 diﬀerent “views” of the input image, uses the backbone\n",
      "to extract the dense feature maps f1, f2 ∈ rh×w ×c, and then feeds them to\n",
      "the following projection heads.\n",
      "the global contrastive learning uses the global feature vector fg\n",
      "of an image as query q and feature vectors from the alternate view of the query\n",
      "image and the other images within the batch as keys k = {k1, k2, ..., }.\n",
      "for each\n",
      "self- and semi-supervised learning for gastroscopic lesion detection\n",
      "87\n",
      "query q, the only positive key k+ is the diﬀerent views of the same images and\n",
      "the others are all negative keys (k−) like moco [9].\n",
      "the negative keys t− here are the feature vectors of\n",
      "diﬀerent images while the positive key t+ is the correspondence feature vector\n",
      "of r in another view of the images.\n",
      "lh = lg + λdld + λrlr\n",
      "where λd and λr are the weights of ld and lr and are set to 1 and 2.\n",
      "2.2\n",
      "prototype-based pseudo-label generation method\n",
      "we propose the prototype-based pseudo-label generation method (ppg) to\n",
      "discover potential lesions from unlabeled gastroscopic data meanwhile avoid\n",
      "introducing much noise to further enhance gld performance.\n",
      "ssl can actually enhance the\n",
      "gld performance for some challenging cases.\n",
      "lgmdd collects about 1m+ gastroscopic images from 2 hospi-\n",
      "tals of about 500 patients and their diagnosis reports.\n",
      "we invite 10 senior doctors to annotate them from the unlabeled\n",
      "endoscopic images.\n",
      "finally, they annotates 12,292 lesion boxes in 10,083 images after going\n",
      "through about 120,000 images.\n",
      "evaluation metrics : we use standard object detection metrics to evaluate the\n",
      "gld performance, which computes the average precision (ap) under multiple\n",
      "intersection-of-union (iou) thresholds and then evaluate the performance using\n",
      "the mean of aps (map) and the ap of some speciﬁc iou threshold.\n",
      "we also report ap under some speciﬁc iou threshold (ap50 for .5, ap75 for .75)\n",
      "and ap of diﬀerent scale lesions (aps, apm, apl) like coco [11].\n",
      "4\n",
      "experiments\n",
      "please kindly refer to the supplemental materials for implementation details\n",
      "and training setups.\n",
      "both components of ssl (hsl\n",
      "& ppg) can bring signiﬁcant performance enhancement for gld tasks.\n",
      "parameters analysis experiment results.\n",
      "(d)\n",
      "extension experiment on endo21.\n",
      "when compared\n",
      "with the supervised pre-training (imagenet [5] weights) baseline, ssl can boost\n",
      "more ap enhancement (centernet: +5.3ap, fasterrcnn: +3.2ap).\n",
      "2. it can be noticed, ssl can actually enhance the gld\n",
      "performance for both types of detectors, especially for some challenging cases.\n",
      "ablation studies.\n",
      "hsl can bring 1.8 ap and 1.1 ap enhancement for centernet and fasterrcnn\n",
      "respectively compared with densecl.\n",
      "ppg can bring extra 0.9ap and 0.9ap\n",
      "enhancement for centernet and fasterrcnn respectively.\n",
      "we conduct extra experiments based on faster rcnn\n",
      "to further analyze the eﬀect of diﬀerent parameter settings on lgldd.\n",
      "1) reconstruction loss weight λr is designed to balance the losses of con-\n",
      "trastive learning and the reconstruction, which is to balance the discriminability\n",
      "and the detailed information volume of local feature representations.\n",
      "as illus-\n",
      "trated in table 2.a, only suitable λr can fully boost the detection performance.\n",
      "2) objectiveness score threshold τu: we compare ppg with objective-\n",
      "ness score-based pseudo-label generation methods with diﬀerent τu (table 2.b).\n",
      "a) a\n",
      "self- and semi-supervised learning for gastroscopic lesion detection\n",
      "91\n",
      "low threshold generates noisy pseudo-labels, leading to reduced performance (-\n",
      "0.6/-0.2 ap at thresholds 0.5/0.6).\n",
      "b) a high threshold produces high-quality\n",
      "pseudo-labels but may miss potential lesions, resulting in only slight performance\n",
      "improvement (+0.3 ap at threshold 0.7).\n",
      "c) ppg approach uses a low threshold\n",
      "(0.5) to identify potential lesions, which are then ﬁltered using prototype feature\n",
      "vectors, resulting in the most signiﬁcant performance enhancement (+0.9 ap).\n",
      "3) memory update strategy inﬂuences the representativeness of memory\n",
      "and the prototype feature vectors.\n",
      "exper-\n",
      "iment results (table 2.c) show our memory update strategy performs better.\n",
      "4) endo21: to further evaluate the eﬀectiveness of ssl, we conduct experi-\n",
      "ments on endo21\n",
      "experi-\n",
      "mental results in table 2.d show that ssl can bring signiﬁcant improvements to\n",
      "publicly available datasets.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf:\n",
      "there is a growing body of\n",
      "evidence that about 30% of colorectal cancer patients do not respond to\n",
      "radiotherapy and will need alternative treatment.\n",
      "by jointly predict-\n",
      "ing a patient’s response to radiotherapy, the presence of cms4, and the\n",
      "epithelial tissue map from morphological features extracted from stan-\n",
      "dard h&e slides we provide a comprehensive clinically relevant assess-\n",
      "ment of a biopsy.\n",
      "a graph neural network is trained to achieve this joint\n",
      "prediction task, which subsequently provides novel interpretability maps\n",
      "to aid clinicians in their cancer treatment decision making process.\n",
      "interpretability\n",
      "supported by cancer research uk.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9_73.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp.\n",
      "a common form of treatment for such patients is neoadjuvant therapy,\n",
      "including chemotherapy and radiotherapy, which can be given to patients with\n",
      "locally advanced rectal cancer to shrink the tumour prior to surgery.\n",
      "recent\n",
      "evidence suggests that 10–20% of patients will have a complete pathological\n",
      "response to neoadjuvant therapy and can therefore avoid surgery altogether [2,5].\n",
      "however, one third of patients do not beneﬁt from radiotherapy treatment prior\n",
      "to surgery [8], hence it is important to determine how a patient will respond to\n",
      "radiotherapy with a personalized approach in order to avoid overtreatment.\n",
      "histology-based digital biomarkers enable the possibility to predict a\n",
      "patient’s response to therapy.\n",
      "it has been shown that\n",
      "these four cms classes can be predicted directly from the standard haematoxylin\n",
      "and eosin (h&e) stained slide images using deep learning\n",
      "interactions between the epithelial tissue\n",
      "(cellular tissue lining) and other prevalent tissue types in the tumour microen-\n",
      "vironment are also indicators of prognosis [15], since progression of colorectal\n",
      "cancer is dependent on both the epithelial and stromal tissues [20].\n",
      "other work\n",
      "has looked at predicting chemoradiotherapy response in rectal cancer patients\n",
      "from h&e images using diﬀerent approaches, but without providing contextual\n",
      "interpretations [19,22].\n",
      "input to our model is a standard h&e whole slide\n",
      "image (wsi) which is split into smaller patches to overcome the memory limita-\n",
      "tions of existing gpus.\n",
      "pathologists and\n",
      "oncologists can use this information to inspect the validity of the prediction result\n",
      "and interrogate key aspects of the spatial biology that is critical for patient man-\n",
      "agement.\n",
      "2\n",
      "methods\n",
      "in this section we present the patch-level feature extraction, provide the detail\n",
      "of the superpixel segmentation of the wsi, and illustrate the resulting graph\n",
      "presentation.\n",
      "for computational reasons, all images are split into patches of size\n",
      "256 × 256 pixels.\n",
      "the dino framework [4] uses a self-distillation\n",
      "training approach, using data augmentation to locally crop the patches and train\n",
      "with a local-global student-teacher approach.\n",
      "we use the dino framework to\n",
      "train a vit in a self-supervised manner on our h&e slides\n",
      "we use only the training set to train this model, and\n",
      "use the image patches at 20x magniﬁcation.\n",
      "the slic superpixel algorithm seg-\n",
      "ments the entire slide into smaller regions [1].\n",
      "[1] on the wsis at 5x magniﬁcation to segment the tissue to capture cellular\n",
      "neighbourhoods that are roughly between 80–100 µm2/pixels in size.\n",
      "it can be\n",
      "seen that the superpixel boundaries consistently align with the boundaries of\n",
      "tissue compartments.\n",
      "the pre-treatment biopsy slides were all sectioned and\n",
      "stained in the same laboratory, and scanned at 20x magniﬁcation (0.5 µm2/pixel)\n",
      "on an aperio scanner.\n",
      "pathological complete response, which we use as a tar-\n",
      "get outcome here, was derived from histopathological assessment from post-\n",
      "treatment resections.\n",
      "these epithelial segmentation masks were generated at 10x mag-\n",
      "niﬁcation (1 µm2/pixel) with a u-net [17] which was trained and validated on\n",
      "666 full tissue sections belonging to 362 patients from the focus cohort [18].\n",
      "we use these masks in our analysis to ﬁlter out background and irrelevant\n",
      "tissue from the images.\n",
      "we address this imbalance\n",
      "in the supplementary materials.\n",
      "3\n",
      "experiments\n",
      "implementation.\n",
      "[1] with compactness of\n",
      "20, setting the number of segments for each wsi as half the mean size of the\n",
      "wsi.\n",
      "[21] with dimensions 64, 32 and 16 respectively.\n",
      "we apply\n",
      "dropout of 0.5 in-between graph layers, use minimum aggregation for message\n",
      "passing between nodes and use maximum pooling for concatenating the node\n",
      "activations.\n",
      "despite the noise in our reference data used for training, our model\n",
      "achieves good performance in terms of mean auc scores on all three prediction\n",
      "branches of our model, predicting complete response to radiotherapy (rt) with\n",
      "0.819 auc, cms4 with 0.819 auc and epithelial tissue at the node level with\n",
      "0.760 auc across folds.\n",
      "the prediction\n",
      "performance of the model could be improved by utilising a larger training dataset\n",
      "and performing more exhaustive parameter searches, however the current per-\n",
      "formance of the model is suﬃcient to demonstrate the impact of this approach.\n",
      "2. additional samples are presented in the\n",
      "supplementary materials.\n",
      "2, with further slides in the supplementary materials.\n",
      "changing the dropout, loss weights, loss func-\n",
      "tion, and message passing aggregation methods only changes prediction auc\n",
      "scores by absolute values up to 0.03.\n",
      "the results can be found in the supplementary materials.\n",
      "these small changes indicate that the noise in our data does not degrade the\n",
      "performance of our classiﬁer, reinforcing it as a robust and accurate model.\n",
      "4\n",
      "conclusion\n",
      "by setting the prediction of response to therapy in context with disease biol-\n",
      "ogy and spatial organisation of the tissue we are providing a novel approach\n",
      "for enhancing the interpretablity of complex prediction tasks.\n",
      "extending the amount of training data and improving model training will\n",
      "improve model performance, which is already impressive.\n",
      "the prediction branches only diverge\n",
      "at the ﬁnal stage of translating these graph features into outcome predictions\n",
      "for our three clinically relevant outcomes.\n",
      "importantly, this level of visualisation\n",
      "is not only accessible to pathologists, this joint prediction model also enhances\n",
      "the communication between pathologists and oncologists which is critical for\n",
      "patient management.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf:\n",
      "curricu-\n",
      "lum learning is inspired by the way humans learn, starting with simple\n",
      "examples and gradually progressing to more challenging ones.\n",
      "this new approach has been evaluated on\n",
      "two medical image datasets, and the results show that it outperforms\n",
      "other curriculum learning methods.\n",
      "keywords: medical image classiﬁcation · curriculum learning ·\n",
      "uncertainty estimation\n",
      "1\n",
      "introduction\n",
      "curriculum learning methods in deep learning are inspired by human educa-\n",
      "tion and involve structuring the training data from easy to hard to teach net-\n",
      "works progressively.\n",
      "the ﬁrst is that human experts\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9 72.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp.\n",
      "these metrics not only require suﬃcient domain knowledge, but they also run\n",
      "the risk that metrics of diﬃculty from a human perspective may not be applica-\n",
      "ble to a learning model due to diﬀerent decision boundaries between the model\n",
      "and the human [25].\n",
      "another popular approach is diﬃculty measurers based on the network,\n",
      "including transfer learning\n",
      "however, this type of approach suﬀers from the\n",
      "uncertainty of quantifying the diﬃculty of data due to insuﬃcient training in\n",
      "the early stages.\n",
      "in addition, while deep learning models have achieved impres-\n",
      "sive performance in the medical image analysis ﬁeld, there remain challenges in\n",
      "measuring and developing a model with low in-domain uncertainty.\n",
      "our approach is motivated by two key observations: 1) sample diﬃ-\n",
      "culty is inﬂuenced by both the complexity of the data and the model’s inability\n",
      "dclu for medical image classiﬁcation\n",
      "749\n",
      "to explain data, related to in-domain uncertainty, and 2) reducing in-domain\n",
      "uncertainty by improving the learning process can boost model performance.\n",
      "in particular, our dynamic diﬃculty measurer (ddm) generates uncertain-\n",
      "ties and predictions for each image simultaneously.\n",
      "uncertainties reﬂect the dif-\n",
      "ﬁculty and we use these as the criteria for data rearrangement.\n",
      "we evaluate our method on two medical\n",
      "image datasets isic 2018 task 3 and chest-xray8 (covid-19).\n",
      "as our proposal exploits in-domain uncer-\n",
      "tainties from the current network at every iteration, we want to avoid using\n",
      "additional modules and metrics to obtain similar uncertainty estimates from\n",
      "the above methods and thereby reduce computational resource requirements.\n",
      "dclu for medical image classiﬁcation\n",
      "751\n",
      "additionally, the dirichlet distribution of ddm can be deﬁned with parameters\n",
      "α\n",
      "≤ p1, ..., pk ≤ 1. b(α) is a k-dimensional multinomial\n",
      "beta function [13].\n",
      "[6]. inspired by this phenomenon, we employ in-domain uncertainty to measure\n",
      "the diﬃculty of data at each iteration.\n",
      "however, some existing pacing functions attempt to partition the\n",
      "dataset into multiple subsets and gradually feed them into the network during\n",
      "training - this fails to satisfy the requirement of our diﬃculty measurer.\n",
      "in our work, we implement uas through two approaches.\n",
      "firstly, uas (expo-\n",
      "nential) incorporates both the reorder and sampling modules, prompting the net-\n",
      "work to prioritize learning easier examples during the initial stages of training and\n",
      "then gradually learn more diﬃcult examples.\n",
      "4\n",
      "experiment\n",
      "4.1\n",
      "dataset and experimental setup\n",
      "dataset.\n",
      "we evaluated our method on two public medical image datasets includ-\n",
      "ing isic 2018 task 3\n",
      "[18]. isic 2018 task\n",
      "3 dataset has 10,015 training images and 194 images for validation.\n",
      "chest-xray 8 (covid-19) contains 1125 x-ray images of the chest of the\n",
      "individuals studied, including 125 images labeled covid-19 taken from [3], 500\n",
      "images labeled pneumonia and 500 images labeled no ﬁndings were randomly\n",
      "dclu for medical image classiﬁcation\n",
      "753\n",
      "taken from the chestx-ray8\n",
      "chest-xray 8 (covid-\n",
      "19) dataset is randomly divided into two parts, with 80% of the images being\n",
      "the training set and 20% of the images being the test set.\n",
      "evaluation metrics.\n",
      "for both datasets, the performance of diagnosis is eval-\n",
      "uated with both accuracy and f1 score.\n",
      "moreover, for the assessment of uncer-\n",
      "tainty estimation, we apply expected calibration error (ece) as the metric.\n",
      "implementation details.\n",
      "in our experiments, we used the tensorflow framework and trained on\n",
      "an nvidia 3090 gpu with 32g of ram.\n",
      "the total epoch is 50.\n",
      "4.2\n",
      "experimental results\n",
      "comparison with state-of-the-art.\n",
      "our method with uas (exponential) performs better than other\n",
      "methods on chest-xray 8 (covid-19) and has the second best performance\n",
      "on isic 2018 task 3.\n",
      "the performance gap between our two methods may be\n",
      "table 1. comparison with state-of-the-art curriculum learning methods on isic 2018\n",
      "task 3 and chest-xray 8 (covid-19)\n",
      "method\n",
      "isic 2018 task 3\n",
      "chest-xray8 (covid-19)\n",
      "accuracy f1 score accuracy f1 score\n",
      "vanilla\n",
      "81.73\n",
      "38.8\n",
      "71.88\n",
      "57.51\n",
      "fcl\n",
      "when the\n",
      "samples are selected by uas (exponential) at the early training stage, samples\n",
      "from smaller numbered classes may not be chosen which leads to the optimiza-\n",
      "tion may tend to fall into local extrema.\n",
      "compared with loss\n",
      "function-based curriculum learning (spl, spcl and adaptive cl), our meth-\n",
      "ods obtain better performance, showing the importance of using uncertainty\n",
      "estimates as the criterion for diﬃculty measurement of data.\n",
      "furthermore, our\n",
      "method is more eﬀective than spl at an early training stage.\n",
      "first of all,\n",
      "dclu for medical image classiﬁcation\n",
      "755\n",
      "our method performs 8.93% better than fcl using the same exponential pacing\n",
      "functions.\n",
      "moreover, in order to demonstrate\n",
      "that the order generated by ddm is robust, we have conducted experiments\n",
      "on diﬀerent backbones, details of which can be found in the appendix.\n",
      "finally, we compare the eﬀect of using the loss function with and without l2\n",
      "regular term on the model performance, ﬁnding that using the loss function with\n",
      "l2 can eliminate the eﬀect of overﬁtting.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf:\n",
      "yet, many downstream tasks including polyp\n",
      "characterization (cadx), quality metrics, automatic reporting, require\n",
      "aggregating polyp data from multiple frames.\n",
      "our solution uses an attention-based self-supervised\n",
      "ml model, speciﬁcally designed to leverage the temporal nature of video\n",
      "input.\n",
      "we quantitatively evaluate method’s performance and demon-\n",
      "strate its value for the cadx task.\n",
      "it is well known\n",
      "that many polyps go unnoticed during colonoscopy [22].\n",
      "the success of polyp detector sparkled the\n",
      "development of new cad tools for colonoscopy, including polyp characterization\n",
      "(cadx, or optical biopsy), extraction of various quality metrics, and automatic\n",
      "reporting.\n",
      "many of those new cad applications require aggregation of all avail-\n",
      "able data on a polyp into a single uniﬁed entity.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9_57.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "while this is\n",
      "true, there are a few factors speciﬁc to the colonoscopy setup: (a) due to abrupt\n",
      "endoscope camera movements, targets (polyps) often go out of the ﬁeld of view,\n",
      "(b) because of heavy imaging conditions (liquids, debris, low illumination) and\n",
      "non-rigid nature of the colon, targets may change their appearance signiﬁcantly,\n",
      "(c) many targets (polyps) are quite similar in appearance.\n",
      "those factors limit the\n",
      "scope and accuracy of existing frame-by-frame spatio-temporal tracking meth-\n",
      "ods, which typically yield an over-fragmented result.\n",
      "that is, the track is often\n",
      "lost, resulting in relatively short tracklets (temporal sequences of same target\n",
      "detections in multiple near-consecutive frames), see supplementary fig.1.\n",
      "a recently published method\n",
      "the tracklets are built incrementally, by adding a single frame detection to the\n",
      "matched tracklet, one-by-one.\n",
      "to avoid manual data annotation, which is extremely ineﬀective in our case,\n",
      "we turn to self-supervision and adapt the widely used contrastive learning app-\n",
      "roach\n",
      "as tracklet re-identiﬁcation is a sequence-to-sequence matching problem, the\n",
      "standard solution is comparing sequences element-wise and then aggregating\n",
      "the per-element comparisons, e.g. by averaging or max/min pooling [21] - the\n",
      "so-called late fusion technique.\n",
      "[23] to leverage the attention paradigm for non-\n",
      "uniform weighing and “knowledge exchange” between tracklet frames.\n",
      "– the application of polyp reid to boost the polyp cadx performance.\n",
      "as brieﬂy mentioned above, the proposed approach starts with an initial\n",
      "grouping of polyp detections using an oﬀ-the-shelf multiple object tracking algo-\n",
      "rithm.\n",
      "2.1\n",
      "single-frame representation for reid\n",
      "to generate a single frame representation we train an embedding model that\n",
      "maps a polyp image into a latent space, s.t.\n",
      "a straightforward approach to train such model is supervised learning, which\n",
      "requires forming a large collection of polyp image pairs, manually labeled as\n",
      "same/not same polyp [1].\n",
      "in addition, ﬁnding hard negative pairs is especially challenging, as\n",
      "images of two randomly sampled polyps are usually very dissimilar.\n",
      "moreover,\n",
      "self-supervised techniques using extensive unannotated datasets has exhibited\n",
      "substantial advantages within the medical domain [12].\n",
      "hence, we turn to simclr [5], a contrastive self-supervised learning tech-\n",
      "nique, which requires no manual labeling.\n",
      "in simclr the loss is calculated\n",
      "over the whole batch where all input samples serve as negatives of each other\n",
      "and positive samples are generated via image augmentations.\n",
      "combined with\n",
      "the temperature mechanism this allows for hard negative mining by prioritizing\n",
      "hard-to-distinguish pairs, resulting in a more eﬀective loss weighting scheme.\n",
      "one caveat of simclr is the diﬃculty to generate augmentations beneﬁcial\n",
      "for the learning process [5].\n",
      "speciﬁcally for colonoscopy, the standard image\n",
      "augmentations do not capture the diversity of polyp appearances in diﬀerent\n",
      "views (see fig. 1(c)).\n",
      "identiﬁcation in colonoscopy\n",
      "593\n",
      "instead of customizing the augmentations to ﬁt the colonoscopy setup, we\n",
      "leverage the temporal nature of videos, and take diﬀerent polyp views from the\n",
      "same tracklet as positive samples (see fig. 1(b)).\n",
      "fig.\n",
      "1. (a) a polyp image, (b) two additional views of the polyp in (a) taken from the\n",
      "same tracklet, (c) two typical augmentations of the polyp in (a).\n",
      "images in (b) oﬀer\n",
      "more realistic variations, such as diﬀerent texture, tools, etc.\n",
      "formally, a batch is formed by sampling one tracklet from n diﬀerent pro-\n",
      "cedures to ensure the tracklets belong to diﬀerent polyps.\n",
      "an example of similarities between frames\n",
      "can be seen in supplementary fig.\n",
      "we postulate\n",
      "that learning a joint embedding of multiple views in an end-to-end manner will\n",
      "produce a better representation of the visual properties of a polyp, by allowing\n",
      "“knowledge exchange” between the tracklet frames.\n",
      "we artiﬁcially split a\n",
      "tracklet into 3 disjoint segments, where the middle segment is discarded, and the\n",
      "ﬁrst and the last segments are used as a positive pair, thus providing suﬃciently\n",
      "diﬀerent appearances of the same polyp as would happen in real procedures.\n",
      "in addition, this type of sampling approach, which eﬀectively discards highly\n",
      "correlated samples from training, has been shown to improve model performance\n",
      "in [17].\n",
      "3\n",
      "experiments\n",
      "this section includes two parts.\n",
      "the\n",
      "average length of the recorded procedures is 15 min, with a median duration of\n",
      "self-supervised polyp re-identiﬁcation in colonoscopy\n",
      "595\n",
      "13 min. for training, we automatically generated polyp tracklets using automatic\n",
      "polyp detection and tracking as described in sect.\n",
      "for evaluation, the test set polyp tracklets were\n",
      "manually annotated (timestamps and bounding boxes) by certiﬁed physicians.\n",
      "in addition, tracklet pairs from the same procedure were manually labeled as\n",
      "either belonging to the same polyp or not.\n",
      "[9] as the single frame encoder, with an mlp\n",
      "head projecting the representation into a 128-dimensional embedding vector.\n",
      "we initialize the model using pre-trained imagenet\n",
      "due to memory limitations, we\n",
      "use 8 views per tracklet during training, resulting in 1024 ∗ 8 = 8192 images per\n",
      "training step.\n",
      "we evaluate the\n",
      "performance using auc of the roc and precision-recall curve (prc) for track-\n",
      "let similarity scores over the test set (see table 1 and supplementary fig. 3).\n",
      "in addition, we evaluate the eﬀectiveness of reid by measuring the aver-\n",
      "age polyp fragmentation rate (fr), deﬁned as the average number of tracklets\n",
      "polyps are split into.\n",
      "obviously, lower fragmentation rate means better result\n",
      "(with the best fragmentation of 1), but it may come at the expense of wrong\n",
      "tracklet matching (false positive).\n",
      "we measure the fragmentation rate at the\n",
      "operating point of 5% false positive rate.\n",
      "the number of polyp fragments is\n",
      "determined by matching tracklets to manually annotated polyps and counting\n",
      "596\n",
      "y. intrator et al.\n",
      "table 1.\n",
      "results presented in table 2 demonstrate that reid can reduce\n",
      "the fragmentation rate by over 50%, compared to a tracking only solution [27].\n",
      "table 2. fragmentation rate (fr) statistics before and after the reid.\n",
      "fragmented polyps ratio is the percentage of polyps divided\n",
      "into more than one tracklet.\n",
      "fr\n",
      "fr std fragmented polyps ratio\n",
      "tracking\n",
      "3.3\n",
      "3.3\n",
      "0.64\n",
      "tracking+reid 1.86 1.49\n",
      "0.45\n",
      "3.2\n",
      "reid for cadx\n",
      "in this section, we investigate the potential beneﬁts of using polyp reid as part\n",
      "of a cadx system.\n",
      "the tracklets are then manually\n",
      "grouped together to build a single sequence for every polyp.\n",
      "we trained a simple image classiﬁcation cnn, composed of a mobilenet\n",
      "[20] backbone, followed by an mlp layer with a sigmoid activation, to predict the\n",
      "non-adenoma/adenoma score in [0, 1], for each frame.\n",
      "the 3 evaluated\n",
      "methods are: (1) manual annotation (2) grouping by tracking, and (3) grouping\n",
      "by reid.\n",
      "the manually annotated tracklets - the ground truth (gt) - are the\n",
      "longest sequences, containing all frames of each polyp in the test set.\n",
      "by construction, tracklets generated by methods (2) and\n",
      "(3) are subsets of the corresponding manually annotated gt tracklet, and are\n",
      "assigned its polyp classiﬁcation label.\n",
      "a visualization of the resulting tracklets\n",
      "using diﬀerent grouping methods is provided in supplementary fig.\n",
      "cadx test data distribution and fragmentation rate (fr).\n",
      "the result on the manually annotated data is the accu-\n",
      "racy upper-bound and is brought as a reference point.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf:\n",
      "breast ultrasound videos contain richer information than\n",
      "ultrasound images, therefore it is more meaningful to develop video\n",
      "models for this diagnosis task.\n",
      "in this paper, we explore the feasibil-\n",
      "ity of enhancing the performance of ultrasound video classiﬁcation using\n",
      "the static image dataset.\n",
      "the kga-net adopts both video clips and static images to\n",
      "train the network.\n",
      "the coherence loss uses the feature centers generated\n",
      "by the static images to guide the frame attention in the video model.\n",
      "our kga-net boosts the performance on the public busv dataset by\n",
      "a large margin.\n",
      "[15] and static images from\n",
      "busi\n",
      "we use a 2d resnet trained on ultrasound images to get the features.\n",
      "while ultrasound videos oﬀer more information, prior studies have primarily\n",
      "focused on static image classiﬁcation [2,11,27]. obtaining ultrasound video data\n",
      "with pathology gold standard results poses a major challenge.\n",
      "sonographers typ-\n",
      "ically record keyframe images during general ultrasound examinations, not entire\n",
      "videos.\n",
      "consequently, while there are many breast ultrasound image\n",
      "datasets [1,28], breast ultrasound video datasets remain scarce, with only one\n",
      "relatively small dataset [15] containing 188 videos available currently.\n",
      "given the diﬃculties in collecting ultrasound video data, we investigate\n",
      "the feasibility of enhancing the performance of ultrasound video classiﬁcation\n",
      "using a static image dataset.\n",
      "to achieve this, we ﬁrst analyze the relationship\n",
      "between ultrasound videos and images.\n",
      "the images in the ultrasound dataset\n",
      "are keyframes of a lesion that exhibit the clearest appearance and most typical\n",
      "symptoms, making them more discriminative for diagnosis.\n",
      "1, the feature points of static images are more\n",
      "concentrated, while the feature of video frames sometimes are away from the\n",
      "class centers.\n",
      "therefore, it is a\n",
      "promising approach to guide the video model to pay more attention to important\n",
      "frames close to the class center with the assistance of static keyframe images.\n",
      "our approach leverages both\n",
      "image (keyframes) and video datasets to train the network.\n",
      "during training, we construct category fea-\n",
      "ture centers for malignant and benign examples respectively using center loss [26]\n",
      "on static image inputs and use the centers to guide the training of video frame\n",
      "attention.\n",
      "due to the feature centers being generated by the\n",
      "larger scale image dataset, it provides more accurate and discriminative feature\n",
      "centers which can guide the video frame attention to focus on important frames,\n",
      "and ﬁnally leads to better video classiﬁcation.\n",
      "our experimental results on the public busv dataset [15] show that our\n",
      "kga-net signiﬁcantly outperforms other video classiﬁcation models by using an\n",
      "external ultrasound image dataset.\n",
      "this phenomenon makes our method more\n",
      "explainable and provides a new perspective for selecting keyframes from video.\n",
      "we analyze the relationship between ultrasound video data and image data,\n",
      "and propose the coherence loss to use image feature centers to guide the\n",
      "training of frame attention.\n",
      "2. we propose kga-net, which adopts a static image dataset to boost the\n",
      "performance of ultrasound video classiﬁcation.\n",
      "[19,23,27] utilize multi-task learn-\n",
      "ing to improve the model performance.\n",
      "however, all of them are based on image\n",
      "datasets, such as busi\n",
      "2. overview of our proposed keyframe-guided attention network.\n",
      "performance.\n",
      "our proposed\n",
      "kga-net is a simple framework that leverages the frame attention module to\n",
      "aggregate multi-frame features eﬃciently.\n",
      "2, our kga-net takes the video inputs and static image inputs\n",
      "simultaneously to train the network.\n",
      "the coherence loss is proposed to guide the\n",
      "frame attention by using the feature centers generated by the images.\n",
      "we will\n",
      "then elaborate on each component in the following sections.\n",
      "3.1\n",
      "video and image classiﬁcation network\n",
      "the video classiﬁcation network is illustrated in fig.\n",
      "the image classiﬁcation network is used to assist in training the video\n",
      "model.\n",
      "[26] to the image model besides the cross-entropy loss.\n",
      "(4)\n",
      "446\n",
      "a. sun et al.\n",
      "lv\n",
      "ce and li\n",
      "ce denote the cross-entropy for video classiﬁcation and image and\n",
      "frame classiﬁcation.\n",
      "empirically, we set λ = 1 in our experiments.\n",
      "4\n",
      "experiments\n",
      "4.1\n",
      "implementation details\n",
      "datasets.\n",
      "[1] as the image dataset.\n",
      "busi contains 445 images of benign lesions and 210 images\n",
      "of malignant lesions.\n",
      "all images of the busi dataset are adopted to train our kga-net.\n",
      "[12] pretrained on imagenet [4] is used as backbone.\n",
      "for\n",
      "each batch, the video clips and static images are both sampled and sent to the\n",
      "network.\n",
      "we use a total batchsize of 16 and the sample probability of video clips\n",
      "and images is 1:1.\n",
      "we implement the model based on pytorch and train it with\n",
      "nvidia titan rtx gpu cards.\n",
      "in order\n",
      "to satisfy the ﬁxed video length requirement of mvit\n",
      "[7], we sample up to 128\n",
      "frames of each video to form a video clip and predict its classiﬁcation result\n",
      "using all the models in experiments.\n",
      "therefore, we compare our method with strong\n",
      "video baselines on natural images.\n",
      "for fairness comparison, we train these models using both video\n",
      "and image data, treating images as static videos.\n",
      "evaluation metrics are reported\n",
      "on the busv test set for performance assessment.\n",
      "as shown in table 1, by leveraging the guidance of the image dataset, our\n",
      "kga-net signiﬁcantly surpasses all other models on all of the metrics.\n",
      "therefore, the success of our\n",
      "kga-net lies in the correct usage of the image guidance.\n",
      "[7]\n",
      "90.53\n",
      "82.05\n",
      "80.77\n",
      "84.62\n",
      "kga-net (our) 94.67\n",
      "89.74\n",
      "88.46\n",
      "92.31\n",
      "formed by the image dataset with larger data size and clear appearance eﬀec-\n",
      "tively improve the accuracy of frame attention hence boosting the video classi-\n",
      "ﬁcation performance.\n",
      "we use the same training schedule for all of the experiments.\n",
      "image guidance is the main purpose of our method.\n",
      "to portray the eﬀect of\n",
      "using the image dataset, we train the kga-net using busv dataset alone in the\n",
      "ﬁrst row of table 2.\n",
      "without the image dataset, we generate the feature centers\n",
      "from the video frames.\n",
      "as a result, the performance signiﬁcantly drops due to the\n",
      "decrease in dataset scale.\n",
      "it also shows that the feature centers generated by the\n",
      "image dataset are more discriminative than that of the video dataset.\n",
      "it is not\n",
      "only because the lesion number of busi is larger than busv, but also because\n",
      "the images in busi are all the keyframes that contain typical characteristics of\n",
      "lesions.\n",
      "it can be seen that both of these two modules\n",
      "contribute to the overall performance according to auc and acc.\n",
      "this phenomenon certiﬁes the eﬀective-\n",
      "ness of our kga-net to prevent false negatives in diagnosis.\n",
      "w/o image guidance\n",
      "85.21\n",
      "76.92\n",
      "73.08\n",
      "84.62\n",
      "w/o coherence loss & attention 88.17\n",
      "74.36\n",
      "61.54\n",
      "100.0\n",
      "w/o coherence loss\n",
      "92.90\n",
      "87.18\n",
      "80.77\n",
      "100.0\n",
      "kga-net\n",
      "94.67\n",
      "89.74\n",
      "88.46\n",
      "92.31\n",
      "4.4\n",
      "visual analysis\n",
      "fig.\n",
      "overall speaking, the frames with high atten-\n",
      "tion weights do have clear image appearances for diagnosis.\n",
      "3(b) clearly demonstrate the edge micro-lobulation and\n",
      "irregular shapes, which lead to malignant judgment.\n",
      "the qualitative analysis proves the interpretability of our method, which will\n",
      "beneﬁt clinical usage.\n",
      "our kga-net takes as input both the video data and image data to\n",
      "train the network.\n",
      "we propose the coherence loss to guide the training of the\n",
      "video model by the guidance of feature centers of the images.\n",
      "our method signif-\n",
      "icantly exceeds the performance of other competitive video baselines.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf:\n",
      "in current pathology image classiﬁcation, methods mostly\n",
      "rely on patch-based multi-instance learning (mil), which only consid-\n",
      "ers the relationship between patches and slides.\n",
      "however, in clinical\n",
      "medicine, doctors use slide-level labels to summarize patient-level labels\n",
      "as a diagnostic result, indicating the involvement of three levels of patch,\n",
      "slide, and patient in actual pathology image analysis, which we refer to\n",
      "as the multi-level multi-instance learning (ml-mil) problem.\n",
      "this allows for interaction between patient-level and slide-level\n",
      "information and the correction of their respective features to achieve bet-\n",
      "ter classiﬁcation performance.\n",
      "the results show\n",
      "that our method improves the performance of the baselines on both slide\n",
      "and patient levels.\n",
      "keywords: multiple instance learning · multi-level labels ·\n",
      "pathology images · transformer\n",
      "f. li, m. wang and b. huang—contribute equally to this work.\n",
      "https://doi.org/10.1007/978-3-031-43904-9_7\n",
      "64\n",
      "f. li et al.\n",
      "1\n",
      "introduction\n",
      "pathological image analysis is a vital area of research within medical image\n",
      "analysis, focused on utilizing computer technology to aid doctors in diagnosing\n",
      "and treating diseases by analyzing pathological tissue slide images [5]. advance-\n",
      "ments in pathological image analysis have been made in early cancer diagnosis,\n",
      "tumor localization, and grading, and treatment planning [3,10].\n",
      "speciﬁcally, in clinical problems of pathological image analysis, doctors usu-\n",
      "ally summarize patient-level labels based on slide labels as the diagnostic results\n",
      "[1,6].\n",
      "similar situations exist in other tasks, such as the classiﬁcation of breast cancer\n",
      "metastases in lymph nodes, where slide categories may have diﬀerent classiﬁca-\n",
      "tions, and the corresponding diagnosis of the same patient is whether the cancer\n",
      "has spread to the regional lymph nodes (n-stage)\n",
      "1, actual pathological image analysis involves the relationships of patches,\n",
      "slides, and patients, which is called a multi-level multi-instance learning (ml-\n",
      "mil) problem.\n",
      "the ﬁrst\n",
      "method is to directly average the prediction values of slides or take the maxi-\n",
      "mum prediction value [9].\n",
      "this\n",
      "simple yet eﬀective method allows for interaction between patient-level and slide-\n",
      "level information to correct their respective features and improve classiﬁcation\n",
      "performance.\n",
      "our framework consists of two steps: ﬁrst, at the patch-slide level,\n",
      "a common mil framework is used to train a mil neural network and obtain\n",
      "p&sre: a ml-mil framework for pathological image analysis\n",
      "65\n",
      "fig.\n",
      "our method can eﬀectively solve the problem of diﬃcult train-\n",
      "ing due to the scarcity of samples at the highest level in ml-mil, and can be\n",
      "integrated into two state-of-the-art methods to further improve performance.\n",
      "we\n",
      "conducted rigorous experiments on two datasets and demonstrated the eﬀective-\n",
      "ness of our method.\n",
      "before this, no other frame-\n",
      "work had directly tackled this speciﬁc problem, making our proposal a\n",
      "ground-breaking step in the application of ml-mil in healthcare;\n",
      "2) proposing a simple yet highly eﬀective method that leverages self-attention\n",
      "mechanisms and transformer models to enhance the interaction between slide\n",
      "and patient information.\n",
      "this innovative approach not only improves the clas-\n",
      "siﬁcation performance at the patient level but also at the slide level, show-\n",
      "casing its eﬀectiveness and versatility;\n",
      "3) conducting extensive experiments on two separate datasets.\n",
      "the experiments resulted in improved per-\n",
      "formance, indicating that our method enhances the eﬃcacy of these existing\n",
      "approaches.\n",
      "[9] for the slide-patch stage.\n",
      "these matrices are element-\n",
      "wise multiplied and then passed through an fc layer to obtain the weight of each\n",
      "patch, ωk.\n",
      "for dsmil, the attention of each patch is based on the cosine distance\n",
      "between instances and key instances.\n",
      "therefore, the slide feature output by both methods can\n",
      "be generalized as:\n",
      "p&sre: a ml-mil framework for pathological image analysis\n",
      "67\n",
      "hj =\n",
      "mj\n",
      "\u0002\n",
      "k=1\n",
      "ωk ∗ pk/\n",
      "mj\n",
      "\u0002\n",
      "k=1\n",
      "ωk\n",
      "(1)\n",
      "finally, we obtain the feature vector set hi={hj|j=1 to ni} for all slides {sj}\n",
      "of patientxi through patch-slide mil.\n",
      "then, we perform a\n",
      "weighted average of the vectors based on this weight to obtain the patient fea-\n",
      "ture vi:\n",
      "αj = fc({hj|j\n",
      "speciﬁcally, we merge the slide feature set {hj} and\n",
      "the patient feature vi into the input tokens t in\n",
      "i\n",
      "= {h1, h2, ..., hni, vi} = {t},\n",
      "68\n",
      "f. li et al.\n",
      "and then input them into a multi-layer transformer through self-attention and\n",
      "feed-forward neural network layers to obtain the interaction information between\n",
      "slides and output tokens t out\n",
      "i\n",
      ":\n",
      "βk,l = softmax(w qtk\n",
      "t (w ktl)/\n",
      "√\n",
      "d)\n",
      "(4)\n",
      "tk =\n",
      "ni+1\n",
      "\u0002\n",
      "l=1\n",
      "βk,lw v tl\n",
      "(5)\n",
      "t′\n",
      "k = relu(tkw r + b1)w o + b2\n",
      "(6)\n",
      "where d is the dimension of the token, and tk and tl come from t in\n",
      "i .\n",
      "b1\n",
      "and b2 are bias vectors.\n",
      "[7] loss function.\n",
      "3\n",
      "experiments and results\n",
      "3.1\n",
      "dataset and evaluation\n",
      "cd-itb dataset.\n",
      "on average, there were 5 slides per patient.\n",
      "the dataset comprises an average of 2.3k instances per\n",
      "bag, with the largest bag containing over 16k instances.\n",
      "camelyon17 dataset.\n",
      "there\n",
      "were 5 slides per patient on average.\n",
      "the patients are divided into two groups\n",
      "p&sre: a ml-mil framework for pathological image analysis\n",
      "69\n",
      "based on their pn stage, namely lymph node positive and lymph node negative,\n",
      "in proportions of 24:76, respectively.\n",
      "the average number of instances per bag is approximately\n",
      "6.1k, and the largest bag contains over 23k instances.\n",
      "metrics.\n",
      "to avoid randomness, we run all experiments ﬁve times and\n",
      "report the averaged metrics.\n",
      "3.2\n",
      "implementation details\n",
      "we utilized resnet50, which was pre-trained on imagenet1k, to extract features\n",
      "from patches.\n",
      "all networks are\n",
      "implemented using pytorch and trained on a nvidia rtx titan gpu with\n",
      "24 gb memory.\n",
      "3.3\n",
      "comparisons and results\n",
      "we compared our strategy with two state-of-the-art mil methods to evaluate\n",
      "its performance.\n",
      "to investigate the impact of self-attention and transformers on\n",
      "slide-level and case-level results, we conducted ablation experiments: “abmil\n",
      "+ p&sre (with/without psfi)” and “dsmil + p&sre (with/without psfi)”,\n",
      "respectively.\n",
      "therefore, the ablation experiments demonstrate\n",
      "the eﬀectiveness of p&sre in enhancing the classiﬁcation performance at both\n",
      "the slide and patient levels.\n",
      "in the future, we plan to leverage clustering and active learning methods\n",
      "to reduce the number of patches and enable the interaction of all three levels\n",
      "with the transformer, which would further enhance the accuracy and eﬃciency\n",
      "of our proposed method.\n",
      "based on existing\n",
      "state-of-the-art mil methods, we then extend the framework to p&sre, which\n",
      "p&sre: a ml-mil framework for pathological image analysis\n",
      "71\n",
      "conducts feature extraction and interaction at the slide-patient level.\n",
      "by intro-\n",
      "ducing a transformer, the framework enables iterative interaction and correction\n",
      "of information between patients and slides, resulting in better performance at\n",
      "both the patient level and slide level compared to existing state-of-the-art algo-\n",
      "rithms on two validation datasets.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf:\n",
      "colonoscopy videos contain richer information than\n",
      "still images, making them a valuable resource for deep learning methods.\n",
      "for the background,\n",
      "yona conducts background dynamic alignment guided by inter-frame\n",
      "diﬀerence to eliminate the invalid features produced by drastic spatial\n",
      "jitters.\n",
      "quantitative and quali-\n",
      "tative experiments on three public challenging benchmarks demonstrate\n",
      "that our proposed yona outperforms previous state-of-the-art competi-\n",
      "tors by a large margin in both accuracy and speed.\n",
      "keywords: video polyp detection · colonoscopy · feature alignment ·\n",
      "contrastive learning\n",
      "y. jiang and z. zhang—equal contribution.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9_5.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "1(a), we show the target motion speed [26]1 on imagenetvid\n",
      "the motion speed in\n",
      "imagenetvid evenly distributes in three intervals.\n",
      "thus we conjecture that collaborating too many frames for polyp video detec-\n",
      "tion will increase the misalignment between adjacent frames and leads to poor\n",
      "detection performance.\n",
      "figure 1(b) shows the performance of fgfa\n",
      "(b) the performance of\n",
      "fgfa [26] using multiple reference frames increases on imagenetvid while decreasing\n",
      "on ldpolypvideo.\n",
      "(color ﬁgure online)\n",
      "1 averaged intersection-over-union scores of target in the nearby frames (±10 frames).\n",
      "speciﬁcally, we propose the foreground\n",
      "temporal alignment (fta) module to explicitly align the foreground channel\n",
      "activation patterns between adjacent features according to their foreground simi-\n",
      "larity.\n",
      "in addition, we design the background dynamic alignment (bda) module\n",
      "after fta that further learns the inter-frame background spatial dynamics to\n",
      "better eliminate the inﬂuence of motion speed and increase the training robust-\n",
      "ness.\n",
      "in summary, our contributions are in three-folds: (1) to the best of our knowl-\n",
      "edge, we are the ﬁrst to investigate the obstacles to the development of existing\n",
      "video polyp detectors and conclude that two-frame collaboration is enough for\n",
      "video polyp detection.\n",
      "it composes the foreground and background alignment modules\n",
      "to align the features under the fast-moving condition.\n",
      "(3) extensive experiments demonstrate that\n",
      "our yona achieves new state-of-the-art performance on three large-scale public\n",
      "video polyp detection datasets.\n",
      "2. we leverage the centernet\n",
      "then, multi-scale features are fused and up-sampled to the resolution of the\n",
      "ﬁrst stage as the intermediate features f a, f r. then, we conduct foreground\n",
      "temporal alignment (fig. 2(a)) on intermediate features to align their channel\n",
      "activation pattern.\n",
      "next, the enhanced anchor feature ˜f is further reﬁned by\n",
      "the background dynamic alignment module (fig.\n",
      "yona for accurate and fast video polyp detection\n",
      "47\n",
      "overall, the whole network is optimized with the combination loss function\n",
      "in an end-to-end manner.\n",
      "the\n",
      "ﬁnal output of (b) is used to predict the bounding box of the current frame.\n",
      "2.1\n",
      "foreground temporal alignment\n",
      "since the camera moves at a high speed, the changes in the frame are very\n",
      "drastic for both foreground and background targets.\n",
      "thus we propose to conduct tem-\n",
      "poral alignment between adjacent features by leveraging the foreground context\n",
      "of only one adjacent reference frame.\n",
      "speciﬁ-\n",
      "cally, given the intermediate features f a, f r and reference binary map m r, we\n",
      "ﬁrst pooling f r to 1d channel pattern f r by the binary map on the spatial\n",
      "dimension (rn×c×h×w → rn×c×1) and normalize it to [0, 1]:\n",
      "f r = norm\n",
      "[f r(x, y)]/sum[m r(x, y)]\n",
      "if m r(x, y) = 1\n",
      "(1)\n",
      "then, the foreground temporal alignment is implemented by channel attention\n",
      "mechanism, where the attention maps are computed by weighted dot-product.\n",
      "48\n",
      "y. jiang et al.\n",
      "at the training stage, the ground truth boxes of the reference frame are used\n",
      "to generate the binary map m r. during the inference stage, we conduct fta\n",
      "only if the validated bounding box of the reference frame exists, where “validated”\n",
      "denotes the conﬁdence scores of detected boxes are greater than 0.6.\n",
      "for eﬃciency, we use the cosine similar-\n",
      "ity metric [8] to measure the similarity, where f a is the 1d channel pattern of\n",
      "f a computed with eq. 1:\n",
      "α = exp\n",
      "\u0002 f r · f a\n",
      "|f r||f a|\n",
      "\u0003\n",
      "(3)\n",
      "2.2\n",
      "background dynamic alignment\n",
      "the traditional convolutional-based object detector can detect objects well when\n",
      "the background is stable.\n",
      "however, once it receives obvious interference, such as\n",
      "light or shadow, the background changes may cause the degradation of spa-\n",
      "tial correlation and lead to many false-positive predictions.\n",
      "in practice, given the enhanced anchor feature ˜f from fta and reference\n",
      "feature f r, the inter-frame diﬀerence is deﬁned as the element-wise subtraction\n",
      "of enhanced anchor and reference feature.\n",
      "lcontrast =\n",
      "1\n",
      "nt\n",
      "nt\n",
      "\u0005\n",
      "j=1\n",
      "lnce\n",
      "j\n",
      "(8)\n",
      "3\n",
      "experiments\n",
      "we evaluate the proposed method on three public video polyp detection bench-\n",
      "marks: sun colonoscopy video database [7,10] (train set: 19,544 frames, test\n",
      "set: 12,522 frames), ldpolypvideo\n",
      "for the fairness of the experiments, we keep the same dataset settings\n",
      "for yona and all other methods.\n",
      "performance comparison with other image/video-based detection models.\n",
      "detailed results are listed in the supple-\n",
      "ment.\n",
      "we randomly crop and resize the images to 512×512 and normalize them\n",
      "using imagenet settings.\n",
      "random rotation and ﬂip with probability p = 0.5 are\n",
      "used for data augmentation.\n",
      "besides,\n",
      "yona achieves the best trade-oﬀ between accuracy and speed compared with\n",
      "all other image-based sotas across all datasets.\n",
      "thanks to this one-adjacent-frame framework,\n",
      "our yona can not only prevent the false positive caused by part occlusion (1st\n",
      "and 2nd clips) but also capture useful information under severe image quality\n",
      "(2nd clip).\n",
      "moreover, our yona shows robust performance even for challenging\n",
      "scenarios like concealed polyps (3rd clip).\n",
      "due to the large\n",
      "variance of colonoscopy image content, the f1 score slightly decreases if directly\n",
      "adding fta without the adaptive re-weighting strategy.\n",
      "overall, by combining all the proposed\n",
      "methods, our model can achieve new state-of-the-art performance.\n",
      "fig.\n",
      "to address the problem of fast-moving polyps, we introduced\n",
      "the foreground temporal alignment module, which explicitly aligns the channel\n",
      "patterns of two frames according to their foreground similarity.\n",
      "for the complex\n",
      "background content, we designed the background dynamic alignment module to\n",
      "mitigate the large variances by exploiting the inter-frame diﬀerence.\n",
      "extensive\n",
      "experiment results conﬁrmed the eﬀectiveness of our method, demonstrating the\n",
      "potential for practical use in real clinical applications.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf:\n",
      "yet, there are many healthcare domains for which ssl has not been\n",
      "extensively explored.\n",
      "these strong image representations serve\n",
      "as a foundation for secondary training with limited annotated datasets,\n",
      "resulting in state-of-the-art performance in endoscopic benchmarks like\n",
      "surgical phase recognition during laparoscopy and colonoscopic polyp\n",
      "characterization.\n",
      "additionally, we achieve a 50% reduction in annotated\n",
      "data size without sacriﬁcing performance.\n",
      "over 250 million endoscopic\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9_55.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "a cardinal challenge in performing endoscopy is the limited ﬁeld of view which\n",
      "hinders navigation and proper visual assessment, potentially leading to high\n",
      "detection miss-rate, incorrect diagnosis or insuﬃcient treatment.\n",
      "these limita-\n",
      "tions have fostered the development of computer-aided systems based on artiﬁcial\n",
      "intelligence (ai), resulting in unprecedented performance over a broad range of\n",
      "clinical applications\n",
      "in the last few years, self-supervised\n",
      "learning (ssl [5–8]) has been shown to be a revolutionary strategy for unsu-\n",
      "pervised representation learning, eliminating the need to manually annotate vast\n",
      "quantities of data.\n",
      "we ﬁrst experiment solely on public datasets, cholec80\n",
      "demonstrating performance on-par with the top results reported\n",
      "in the literature.\n",
      "through extensive experiments, we\n",
      "ﬁnd that scaling the data size necessitates scaling the model architecture, lead-\n",
      "ing to state-of-the-art performance in surgical phase recognition of laparoscopic\n",
      "procedures, as well as in polyp characterization of colonoscopic videos.\n",
      ", the proposed approach exhibits robust generalization, yielding better\n",
      "performance with only 50% of the annotated data, compared with standard\n",
      "supervised learning using the complete labeled dataset.\n",
      "cholecystectomy is the surgical\n",
      "removal of the gallbladder using small incisions and specialized instruments.\n",
      "apart\n",
      "from measuring quality and monitoring adverse event, this task also serves in\n",
      "facilitating education, statistical analysis, and evaluating surgical performance.\n",
      "self-supervised learning for endoscopy\n",
      "571\n",
      "furthermore, the ability to recognize phases allows real-time monitoring and\n",
      "decision-making assistance during surgery, thus improving patient safety and\n",
      "outcomes.\n",
      "ai solutions have shown remarkable performance in recognizing sur-\n",
      "gical phases of cholecystectomy procedures\n",
      "as an alternative, ssl methods have\n",
      "been developed [12,28,30], however, these are early-days methods that based\n",
      "on heuristic, often require external information and leads to sub-optimal perfor-\n",
      "mance.\n",
      "this limitation has motivated the\n",
      "development of ai systems for automatic optical biopsy, allowing non-experts to\n",
      "also eﬀectively perform optical biopsy during polyp management.\n",
      "through extensive experiments in sect.\n",
      "[2] have set a new state-of-the-art among ssl methods on the imagenet\n",
      "benchmark [29], with a particular focus on the low data regime.\n",
      "during pretraining, on each image xi ∈ rn of a mini-batch of b ≥ 1 samples\n",
      "(e.g. laparoscopic images) we apply two sets of random augmentations to gen-\n",
      "erate anchor and target views, denoted by xa\n",
      "i and xt\n",
      "i respectively.\n",
      "the resultant anchor and target sequences are used\n",
      "as inputs to their respective image encoders fθa and fθt.\n",
      "both encoders share\n",
      "the same vision transformer (vit [16]) architecture where the parameters θt of\n",
      "the target encoder are updated via an exponential moving average of the anchor\n",
      "encoder parameters θa.\n",
      "furthermore, to prevent representation collapse and\n",
      "encourage the model to fully exploit the prototypes, a mean entropy maximiza-\n",
      "tion (me-max) regularizer [2,22] is added, aiming to maximize the entropy h(¯pa)\n",
      "of the average prediction across all the anchor views ¯pa ≜\n",
      "1\n",
      "mb\n",
      "\u0004b\n",
      "i=1\n",
      "\u0004m\n",
      "m=1 pa\n",
      "i,m.\n",
      "thus, the overall training objective to be minimized for both θa and q is where\n",
      "λ > 0 is an hyperparameter and the gradients are computed only with respect\n",
      "to the anchor predictions pa\n",
      "i,m (not the target predictions pt\n",
      "i).\n",
      "we compiled a dataset of laparoscopic procedures videos exclu-\n",
      "sively performed on patients aged 18 years or older.\n",
      "the recorded procedures have an average duration of 47 min, with\n",
      "a median duration of 40 min. each video recording was sampled at a rate of 1\n",
      "frame per second (fps), resulting in an extensive dataset containing 23.3 million\n",
      "images.\n",
      "further details are given in the supplementary materials.\n",
      "colonoscopy.\n",
      "we have curated a dataset comprising 13,979 colonoscopy videos\n",
      "of patients aged 18 years or older.\n",
      "the average duration of the recorded procedures is 15 min,\n",
      "with a median duration of 13 min. to identify and extract polyps from the videos,\n",
      "we employed a pretrained polyp detection model\n",
      "for each frame,\n",
      "we cropped the bounding boxes to generate individual images of the polyps.\n",
      "this\n",
      "process resulted in a comprehensive collection of 2.2 million polyp images.\n",
      "fig.\n",
      "bottom: colonoscopy.\n",
      "4\n",
      "experiments\n",
      "in this section, we empirically demonstrate the power of ssl in the context\n",
      "of endoscopy.\n",
      "our experimental protocol is the following: (i) ﬁrst, we perform\n",
      "ssl pretraining with msns over our unlabeled private dataset to learn infor-\n",
      "mative and generic representations, (ii) second we probe these representations\n",
      "by utilizing them for diﬀerent public downstream tasks.\n",
      "we speciﬁcally\n",
      "use multi-stage temporal convolution networks (ms-tcn) as used in [13,27].\n",
      "we perform an extensive\n",
      "hyperparameter grid search for all downstream experiments and report the test\n",
      "results for the models that exceed the best validation results.\n",
      "implementation details.\n",
      "for ssl we re-implemented msns in jax using\n",
      "scenic library [15].\n",
      "as our image encoders we train vision transformer (vit [16])\n",
      "of diﬀerent sizes, abbreviated as vit-s/b/l, using 16 tpus.\n",
      "downstream exper-\n",
      "iments are implemented in tensorflow where training is performed on 4 nvidia\n",
      "tesla v100 gpus.\n",
      "see the supplementary for further implementation details.1\n",
      "4.1\n",
      "results and discussion\n",
      "scaling laws of ssl.\n",
      "we pretrain the models with msn and then\n",
      "report their downstream performances.\n",
      "we ﬁnd that replacing resnet50 with vit-s, despite\n",
      "comparable number of parameters, yields sub-optimal performance.\n",
      "the performance in per-frame\n",
      "phase recognition is comparable with the baseline.\n",
      "importantly, we see that the performance gap\n",
      "becomes prominent when using the large scale private datasets for ssl pretrain-\n",
      "ing.\n",
      "here, per-frame and per-video phase recognition performances improve by\n",
      "6.7% and 8.2%, respectively.\n",
      "notice\n",
      "that the performance improves with scaling both model and private data sizes,\n",
      "demonstrating that both factors are crucial to achieve optimal performance.\n",
      "next, we examine the beneﬁts of using msns to improve\n",
      "downstream performance in a low-shot regime with few annotated samples.\n",
      "1 for reproducibility purposes, code and model checkpoints are available at https://\n",
      "github.com/royhirsch/endossl.\n",
      "self-supervised learning for endoscopy\n",
      "575\n",
      "table 1.\n",
      "comparing the downstream f1 performances of: (i) models trained on the\n",
      "private (pri) and public (pub) datasets using ssl.\n",
      "(ii) fully supervised baselines pre-\n",
      "trained on imagenet-1k (in1k).\n",
      "figure 3 shows the low-shot\n",
      "performance for the two endoscopic tasks.\n",
      "each experiment is repeated three times with a random sample of train\n",
      "videos, and we report the mean and standard deviation (shaded area).\n",
      "when examining the cholecystectomy phase recognition task, it is evident\n",
      "that we can achieve comparable frame-level performance by using only 12% of\n",
      "the annotated videos.\n",
      "we see that 50% random masking (i.e. we keep 98 tokens out of\n",
      "196 for the global view) and using 4 local views gives the best of performance.\n",
      "we study the eﬀect of data augmentation.\n",
      "ssl augmentation pipelines\n",
      "have been developed on imagenet-1k\n",
      "[7], hence, it is important to re-evaluate\n",
      "these choices for medical images.\n",
      "surprisingly, we see that augmentations primar-\n",
      "ily found to work well on imagenet-1k are also eﬀective on laparoscopic videos\n",
      "(e.g. color jiterring and horizontal ﬂips).\n",
      "in table2e), we look at the eﬀect of\n",
      "the training length when starting from scratch or from a good ssl pretrained\n",
      "checkpoint on imagenet-1k.\n",
      "we observe that excellent performance is achieved\n",
      "with only 10 epochs of ﬁnetuning on medical data when starting from a strong\n",
      "dino checkpoint [6]. table 2g) shows that imagenet-1k dino is a solid starting\n",
      "point compared to other alternatives\n",
      "a) number of prototypes\n",
      "d) data augmentation\n",
      "f) avoiding collapse.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf:\n",
      "to solve\n",
      "this issue, we constructively introduce the segmentation of gv into the\n",
      "classiﬁcation framework and propose the region-constraint module and\n",
      "cross-region attention module for better feature localization and to learn\n",
      "the correlation of context information.\n",
      "we also collect a gv bleeding\n",
      "risks rating dataset (gvbleed) with 1678 gastroscopy images from 411\n",
      "patients that are jointly annotated in three levels of risks by senior clin-\n",
      "ical endoscopists.\n",
      "the experiments on our collected dataset show that\n",
      "our method can improve the rating accuracy by nearly 5% compared to\n",
      "the baseline.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9_1.\n",
      "https://doi.org/10.1007/978-3-031-43904-9_1\n",
      "4\n",
      "y. jiang et al.\n",
      "keywords: gastric varices · bleeding risk rating · cross-region\n",
      "attention\n",
      "1\n",
      "introduction\n",
      "esophagogastric varices are one of the common manifestations in patients with\n",
      "liver cirrhosis and portal hypertension and occur in about 50 percent of patients\n",
      "with liver cirrhosis [3,6].\n",
      "it is crucial to iden-\n",
      "tify high-risk patients and oﬀer prophylactic treatment at the appropriate time.\n",
      "regular endoscopy examinations have been proven an eﬀective clinical approach\n",
      "to promptly detect esophagogastric varices with a high risk of bleeding [7]. dif-\n",
      "ferent from the grading of esophageal varices (ev) that is relatively complete\n",
      "[1], the bleeding risk grading of gastric varices (gv) involves complex variables\n",
      "including the diameter, shapes, colors, and locations.\n",
      "although the existing rating systems tried to identify\n",
      "the risk from diﬀerent perspectives, they still lack clear quantiﬁcation standard\n",
      "and heavily rely on the endoscopists’ subjective judgment.\n",
      "intuitively we may regard the gv bleeding risk rating as an image\n",
      "classiﬁcation task and apply typical classiﬁcation architectures (e.g., resnet\n",
      "however, they may\n",
      "raise poor performance due to the large intra-class variation between gv with\n",
      "the same bleeding risk and small inter-class variation between gv and normal\n",
      "tissue or gv with diﬀerent bleeding risks.\n",
      "also, since the gv images are taken from diﬀerent\n",
      "distances and angles, the number of pixels of the gv area may not reﬂect its\n",
      "actual size.\n",
      "3. to encourage the model to learn more\n",
      "robust representations, we constructively introduce segmentation into the clas-\n",
      "siﬁcation framework.\n",
      "with the segmentation information, we further propose a\n",
      "region-constraint module (rcm) and a cross-region attention module (cram)\n",
      "for better feature localization and utilization.\n",
      "the segmentation results to constrain the cam heatmaps of the feature maps\n",
      "extracted by the classiﬁcation backbone, avoiding the model making predictions\n",
      "based on incorrect areas.\n",
      "in cram, the varices features are extracted using the\n",
      "segmentation results and combined with an attention mechanism to learn the\n",
      "intra-class correlation and cross-region correlation between the target area and\n",
      "the context.\n",
      "while most works and public datasets focus on colonoscopy\n",
      "[13,15] and esophagus [5,9], with a lack of study on gastroscopy images.\n",
      "in the\n",
      "public dataset of endocv challenge [2], the majority are colonoscopies while only\n",
      "few are gastroscopy images.\n",
      "in this work, we collect a gv bleeding risks rating\n",
      "dataset (gvbleed) that contains 1678 gastroscopy images from 411 patients\n",
      "with diﬀerent levels of gv bleeding risks.\n",
      "three senior clinical endoscopists are\n",
      "invited to grade the bleeding risk of the retrospective data in three levels and\n",
      "annotated the corresponding segmentation masks of gv areas.\n",
      "in sum, the contributions of this paper are: 1) a novel gv bleeding risk rating\n",
      "framework that constructively introduces segmentation to enhance the robust-\n",
      "ness of representation learning; 2) a region-constraint module for better feature\n",
      "localization and a cross-region attention module to learn the correlation of tar-\n",
      "get gv with its context; 3) a gv bleeding risk rating dataset (gvbleed) with\n",
      "high-quality annotation from multiple experienced endoscopists.\n",
      "experimental\n",
      "results demonstrate the eﬀectiveness of our proposed framework and modules,\n",
      "where we improve the accuracy by nearly 5% compared to the baseline model.\n",
      "6\n",
      "y. jiang et al.\n",
      "fig.\n",
      "the framework consists of a segmentation module, a cross-region attention module,\n",
      "and a region constraint module.\n",
      "2, which consists\n",
      "of a segmentation module (sm), a region constraint module (rcm), and a cross-\n",
      "region attention module (cram).\n",
      "given a gastroscopy image, the sm is ﬁrst\n",
      "applied to generate the varices mask of the image.\n",
      "then, the image together\n",
      "with the mask are fed into the cram to extract the cross-region attentive\n",
      "feature map, and a class activation map (cam) is calculated to represent the\n",
      "concentrated regions through rcm.\n",
      "finally, a simple classiﬁer is used to predict\n",
      "the bleeding risk using the extracted feature map.\n",
      "2.1\n",
      "segmentation module\n",
      "due to the large intra-class variation between gv with the same bleeding risk\n",
      "and small inter-class variation between gv and normal tissue or gv with diﬀer-\n",
      "ent bleeding risks, existing classiﬁcation models exhibit poor perform and tend\n",
      "to lose focus on the gv areas.\n",
      "to solve this issue, we ﬁrst embed a segmentation\n",
      "network into the classiﬁcation framework.\n",
      "[11] as the segmentation network, considering its great per-\n",
      "formance, and calculate the diceloss between the segmentaion result mp and\n",
      "ground truth mask of vaices region mgt for optimizing the network:\n",
      "lse = 1 −\n",
      "2σmp ∗ mgt\n",
      "σm 2p\n",
      "automatic bleeding risk rating system of gastric varices\n",
      "7\n",
      "a straightforward strategy to utilize the segmentation mask is directly using\n",
      "it as an input of the classiﬁcation model, such as concatenating the image with\n",
      "the mask as the input.\n",
      "although such strategy can improve the classiﬁcation\n",
      "performance, it may still lose focus in some hard cases where the gv area can\n",
      "hardly be distinguished.\n",
      "to further regularize the attention and fully utilize the\n",
      "context information around the gv area, on top of the segmentation framework\n",
      "we proposed the cross-region attention module and the region-constraint module.\n",
      "the cram\n",
      "consists of an image encoder fim, a varices local encoder fvl and a varices global\n",
      "encoder fve.\n",
      "given the image i and the predicted varices mask mp, a feature\n",
      "extraction step is ﬁrst performed to generate the image feature vm, the local\n",
      "varices feature vvl and global varices feature vvg:\n",
      "vm = fim(i),\n",
      "vvl = fvl(i ∗ mp),\n",
      "vvg = fvg(concat[i, mp]),\n",
      "(2)\n",
      "then, through similarity measuring, we can compute the attention with\n",
      "a = (vvl)t vvg,\n",
      "wij =\n",
      "exp(aij)\n",
      "σp(exp(apj)),\n",
      "(3)\n",
      "which composes of two correlations: self-attention over varices regions and cross-\n",
      "region attention between varices and background regions.\n",
      "the training process of the proposed network consists of three steps: 1) the\n",
      "segmentation network is trained ﬁrst; 2) the ground-truth segmentation masks\n",
      "and images are used as the inputs of the cram, the classiﬁcation network,\n",
      "including cram and rcm, are jointly trained; 3) the whole framework is\n",
      "jointly ﬁne-tuned.\n",
      "3\n",
      "gvbleed dataset\n",
      "data collection and annotation.\n",
      "the gvbleed dataset contains 1678 endo-\n",
      "scopic images with gastric varices from 527 cases.\n",
      "in the current version, images from patients with ages elder than 18 are\n",
      "retained1.\n",
      "the images are selected from the raw endoscopic videos and frames.\n",
      "to maximize the variations, non-consecutive frames with larger angle diﬀerences\n",
      "are selected.\n",
      "2) moderate: moderate risk of\n",
      "bleeding, and endoscopic treatment is necessary, with relatively low endoscopic\n",
      "treatment diﬃculty (usually with a diameter between 5 mm and 10 mm).\n",
      "3)\n",
      "severe: high risk of bleeding and endoscopic treatment is necessary, with high\n",
      "endoscopic treatment diﬃculty.\n",
      "note that\n",
      "the diameter is only one reference for the ﬁnal risk rating since the gv is with\n",
      "1 please refer to the supplementary material for more detailed information about our\n",
      "dataset.\n",
      "the gvbleed dataset is partitioned into training and testing sets for evalu-\n",
      "ation, where the training set contains 1337 images and the testing set has 341\n",
      "images.\n",
      "4\n",
      "experiments\n",
      "4.1\n",
      "implementation details\n",
      "in experiments, the weights ωs, ωco, and ωcl of the segmentation loss, region\n",
      "constraint loss, and classiﬁcation loss are set to 0.2, 1, and 1, respectively.\n",
      "the\n",
      "details of the three-step training are as follows: 1) segmentation module: we\n",
      "trained the segmentation network for 600 epochs, using adam as the optimizer,\n",
      "and the learning rate is initialized as 1e−3 and drops to 1e−4 after 300 epochs.\n",
      "2) cross-region attention module and region constraint module: we\n",
      "used the ground-truth varices masks and images as the inputs of the cram,\n",
      "and jointly trained the cram and rcm for 100 epochs.\n",
      "in addition, common data augmentation techniques\n",
      "such as rotation and ﬂipping were adopted here.\n",
      "however, the\n",
      "transformer-based models achieves much worse performances since they always\n",
      "require more training data, which is not available in our task.\n",
      "the simple cnn models as baselines since they achieve better performances.\n",
      "by introducing the segmentation of gv into the frame-\n",
      "work, concatenating the image with its segmentation mask as the inputs of the\n",
      "classiﬁer can improve the classiﬁcation accuracy by 1.2%.\n",
      "with the help of cram,\n",
      "the performance of the model can be further improved.\n",
      "although the model can\n",
      "extract more important context information at the varices regions, the perfor-\n",
      "mance improvement is not very large since the focus ability is not the best and\n",
      "the model may still make predictions based on the incorrect regions for some\n",
      "hard images.\n",
      "by adding the rcm to the cram, the focus ability of the model\n",
      "can be further improved, and thus the model has a signiﬁcant improvement in\n",
      "performance by 5% compared to the baseline model, this proves the eﬀective-\n",
      "ness of our proposed modules.\n",
      "note that, the baseline model tends to predict\n",
      "the images as severe, thus the f1-score of severe is high but the f1-scores of mild\n",
      "and moderate are signiﬁcantly lower than other models.\n",
      "more quantitative and\n",
      "visualization results are shown in supplementary material.\n",
      "in addition, given the\n",
      "input image with resolution 512 × 512, the parameters and computational cost\n",
      "of our framework are 40.2m, and 52.4g macs, and 29 ms inference time for a\n",
      "single image on gpu rtx2080.\n",
      "due to the large intra-class variation between gv with the same bleed-\n",
      "ing risk and small inter-class variation between gv and normal tissue or gv with\n",
      "diﬀerent bleeding risks, existing classiﬁcation models cannot correctly focus on\n",
      "the varices regions and always raise poor performance.\n",
      "to solve this issue, we\n",
      "constructively introduce segmentation to enhance the robustness of representa-\n",
      "tion learning.\n",
      "the experiments on\n",
      "our dataset demonstrated the eﬀectiveness and superiority of our framework.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf:\n",
      "bias in healthcare negatively impacts marginalized popula-\n",
      "tionswithlowersocioeconomicstatusandcontributestohealthcareinequal-\n",
      "ities.\n",
      "eliminating bias in ai models is crucial for fair and precise medical\n",
      "implementation.\n",
      "the development of a holistic approach to reducing bias\n",
      "aggregationinmultimodalmedicaldataandpromotingequityinhealthcare\n",
      "ishighlydemanded.racialdisparitiesexistinthepresentationanddevelop-\n",
      "mentofalgorithmsforpulmonaryembolism(pe),anddeepsurvivalpredic-\n",
      "tionmodelcanbede-biasedwithmultimodaldata.inthispaper,wepresent\n",
      "a novel survival prediction (sp) framework with demographic bias disen-\n",
      "tanglement for pe.\n",
      "the ctpa images and clinical reports are encoded by\n",
      "the state-of-the-art backbones pretrained with large-scale medical-related\n",
      "tasks.\n",
      "the proposed de-biased sp modules eﬀectively disentangle latent\n",
      "race-intrinsic attributes from the survival features, which provides a fair\n",
      "survival outcome through the survival prediction head.\n",
      "we evaluate our\n",
      "method using a multimodal pe dataset with time-to-event labels and race\n",
      "identiﬁcations.\n",
      "the comprehensive results show an eﬀective de-biased per-\n",
      "formance of our framework on outcome predictions.\n",
      "keywords: pulmonary embolism · deep survival prediction ·\n",
      "de-bias learning · multi-modal learning\n",
      "1\n",
      "introduction\n",
      "bias in medicine has demonstrated a notable challenge for providing comprehen-\n",
      "sive and equitable care.\n",
      "implicit biases can negatively aﬀect patient care, particu-\n",
      "larly for marginalized populations with lower socioeconomic status [30]. evidence\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9_50.\n",
      "https://doi.org/10.1007/978-3-031-43904-9_50\n",
      "516\n",
      "z. zhong et al.\n",
      "has demonstrated that implicit biases in healthcare providers could contribute\n",
      "to exacerbating these healthcare inequalities and create a more unfair system for\n",
      "people of lower socioeconomic status\n",
      "[30]. based on the data with racial bias, the\n",
      "unfairness presents in developing evaluative algorithms.\n",
      "using biased data for ai models\n",
      "reinforces racial inequities, worsening disparities among minorities in healthcare\n",
      "decision-making [22].\n",
      "within the radiology arm of ai research, there have been signiﬁcant advances\n",
      "in diagnostics and decision making [19].\n",
      "along these advancements, bias in\n",
      "healthcare and ai are exposing poignant gaps in the ﬁeld’s understanding of\n",
      "model implementation and their utility [25,26].\n",
      "ai model quality relies on input\n",
      "data and addressing bias is a crucial research area.\n",
      "systemic bias poses a greater\n",
      "threat to ai model’s applications, as these biases can be baked right into the\n",
      "model’s decision process\n",
      "[22].\n",
      "pulmonary embolism (pe) is an example of health disparities related to race.\n",
      "black patients exhibit a 50% higher age-standardized pe fatality rate and a\n",
      "twofold risk for pe hospitalization than white patients [18,24].\n",
      "the pulmonary embolism severity index (pesi) is a\n",
      "well-validated clinical tool based on 11 clinical variables and used for outcome pre-\n",
      "diction measurement [2].\n",
      "[7,12,14].\n",
      "however, one issue with traditional survival analysis is bias from single modal\n",
      "data that gets compounded when curating multimodal datasets, as diﬀerent\n",
      "combinations of modes and datasets create with a uniﬁed structure.\n",
      "multimodal\n",
      "data sets are useful for fair ai model development as the bias complementary\n",
      "from diﬀerent sources can make de-biased decisions and assessments.\n",
      "in that\n",
      "process, the biases of each individual data set will get pooled together, creating\n",
      "a multimodal data set that inherits multiple biases, such as racial bias [1,15,23].\n",
      "in addition, it has been found that creating multimodal datasets without any de-\n",
      "biasing techniques does not improve performance signiﬁcantly and does increase\n",
      "bias and reduce fairness [5]. overall, a holistic approach to model development\n",
      "would be beneﬁcial in reducing bias aggregation in multimodal datasets.\n",
      "[4] for bias disentanglement\n",
      "improves model generalization for fairness [3,6,27].\n",
      "we developed a pe outcome model that predicted mortality and detected\n",
      "bias in the output.\n",
      "we then implemented methods to remove racial bias in our\n",
      "dataset and model and output unbiased pe outcomes as a result.\n",
      "our contri-\n",
      "butions are as follows: (1) we identiﬁed bias diversity in multimodal informa-\n",
      "tion using a survival prediction fusion framework.\n",
      "(2) we proposed a de-biased\n",
      "survival prediction framework with demographic bias disentanglement.\n",
      "(3) the\n",
      "multimodal cph learning models improve fairness with unbiased features.\n",
      "de-biased outcome prediction model\n",
      "517\n",
      "fig.\n",
      "1. overview of the survival prediction (sp) framework and the proposed de-\n",
      "biased sp module (lower right).\n",
      "id branch (ei;ci) and survival branch (ec;cc) are\n",
      "trained to disentangle race-intrinsic attributes and survival attributes with the feature\n",
      "swapping augmentation, respectively.\n",
      "the survival head predicts the outcomes based\n",
      "on the de-biased survival attributes.\n",
      "2\n",
      "bias in survival prediction\n",
      "this section describes the detail of how we identify the varying degrees of bias\n",
      "in multimodal information and illustrates bias using the relative diﬀerence in\n",
      "survival outcomes.\n",
      "we will ﬁrst introduce our pulmonary embolism multimodal\n",
      "datasets, including survival and race labels.\n",
      "then, we evaluate the baseline sur-\n",
      "vival learning framework without de-biasing in the various racial groups.\n",
      "dataset.\n",
      "the pulmonary embolism dataset used in this study from 918 patients\n",
      "(163 deceased, median age 64 years, range 13–99 years, 52% female), including\n",
      "3978 ctpa images and 918 clinical reports, which were identiﬁed via retro-\n",
      "spective review across three institutions.\n",
      "for\n",
      "each patient, the race labels, survival time-to-event labels and pesi variables\n",
      "are collected from clinical data, and the 11 pesi variables are used to calcu-\n",
      "late the pesi scores, which include age, sex, comorbid illnesses (cancer, heart\n",
      "failure, chronic lung disease), pulse, systolic blood pressure, respiratory rate,\n",
      "temperature, altered mental status, and arterial oxygen saturation at the time\n",
      "of diagnosis\n",
      "[2].\n",
      "diverse bias of multimodal survival prediction model.\n",
      "the\n",
      "frameworks without de-basing are evaluated for risk prediction in the test set\n",
      "by performing survival prediction on ctpa images, clinical reports, and clini-\n",
      "cal variables, respectively.\n",
      "first, we use two large-scale data-trained models as\n",
      "backbones to respectively extract features from preprocessed images and cleaned\n",
      "clinical reports.\n",
      "to encode survival features zm\n",
      "sur from image,\n",
      "text and pesi variables, these modules are trained to distinguish critical disease\n",
      "from non-critical disease with cox partial log-likelihood loss (coxphloss)\n",
      "we evaluate the performance of each module with\n",
      "concordance probability (c-index), which measures the accuracy of prediction\n",
      "in terms of ranking the order of survival times\n",
      "when debiasing is not performed, signif-\n",
      "icant diﬀerences exist among the diﬀerent modalities, with the image modality\n",
      "exhibiting the most pronounced deviation, followed by text and pesi variables.\n",
      "the biased performance of the imaging-based module is likely caused by the rich-\n",
      "ness of redundant information in images, which includes implicit features such as\n",
      "body structure and posture that reﬂect the distribution of diﬀerent races.\n",
      "this\n",
      "redundancy leads to model overﬁtting on race, compromising the fairness of risk\n",
      "prediction across diﬀerent races.\n",
      "besides, clinical data in the form of text reports\n",
      "and pesi variables objectively reﬂect the patient’s physiological information and\n",
      "the physician’s diagnosis, exhibiting smaller race biases in correlation with sur-\n",
      "vival across diﬀerent races.\n",
      "3\n",
      "de-biased survival prediction model\n",
      "based on our sp baseline framework and multimodal ﬁndings from sect.\n",
      "2, we\n",
      "present a feature-level de-biased sp module that enhances fairness in survival\n",
      "de-biased outcome prediction model\n",
      "519\n",
      "outcomes by decoupling race attributes, as shown in the lower right of fig.\n",
      "1.\n",
      "in the de-biased sp module, ﬁrstly, two separate encoders em\n",
      "i\n",
      "and em\n",
      "c are for-\n",
      "mulated to embed features f m into disentangled latent vectors for race-intrinsic\n",
      "attributes zid or race-conﬂicting attributes zsur implied survival information [16].\n",
      "then, the linear classiﬁers cm\n",
      "i\n",
      "and cm\n",
      "c constructed to predict the race label yid\n",
      "with concatenated vector z =\n",
      "to disentangle survival features from\n",
      "the race identiﬁcation, we use the generalized cross-entropy (gce) loss\n",
      "[31] to\n",
      "train em\n",
      "c and cm\n",
      "c to overﬁt to race label while training em\n",
      "i\n",
      "and cm\n",
      "i\n",
      "with cross-\n",
      "entropy (ce) loss.\n",
      "the relative diﬃculty scores w as deﬁned in eq. 1 reweight\n",
      "and enhance the learning of the race-intrinsic attributes [20].\n",
      "the objective func-\n",
      "tion for disentanglement shown in eq. 2, but the parameters of id or survival\n",
      "branch are only updated by their respective losses:\n",
      "w(z) =\n",
      "ce (cc(z), yid)\n",
      "+ gce (cc(z), yid)\n",
      "(2)\n",
      "to promote race-intrinsic learning in em\n",
      "i\n",
      "and cm\n",
      "i , we apply diversify with\n",
      "latent vectors swapping.\n",
      "as the random combination are\n",
      "generated from diﬀerent samples, the swapping decreases the correlation of these\n",
      "feature vectors, thereby enhancing the race-intrinsic attributes.\n",
      "the loss func-\n",
      "tions of swapping augmentation added to train two neural networks is deﬁned\n",
      "as:\n",
      "lsw = w(z)ce (ci(zsw), yid)\n",
      "the weights λsw and λsur are assigned as 0.5 and 0.8, respectively,\n",
      "to balance the feature disentanglement and survival prediction.\n",
      "4\n",
      "experiment\n",
      "we validate the proposed de-biased survival prediction frameworks on the col-\n",
      "lected multi-modality pe data.\n",
      "performance comparison of the proposed de-biased sp framework and base-\n",
      "line using c-index values on multiple modal outcomes.\n",
      "the larger c-index value is\n",
      "better and the lower bias is fairer.\n",
      "method\n",
      "baseline\n",
      "de-biased sp model\n",
      "dataset\n",
      "overall white color bias\n",
      "overall white color bias\n",
      "imaging\n",
      "0.662\n",
      "0.736\n",
      "0.422\n",
      "0.314 0.646\n",
      "0.656\n",
      "0.622\n",
      "0.035\n",
      "text\n",
      "0.657\n",
      "0.642\n",
      "0.714\n",
      "0.071 0.719\n",
      "0.689\n",
      "0.746\n",
      "0.057\n",
      "variable\n",
      "0.668\n",
      "0.669\n",
      "0.741\n",
      "0.072 0.698\n",
      "0.683\n",
      "0.778\n",
      "0.095\n",
      "multimodal 0.709\n",
      "0.692\n",
      "we apply race-balanced resam-\n",
      "pling to the training and validation sets to eliminate training bias caused by\n",
      "minority groups.\n",
      "the lung region of cpta images is extracted with a slice thickness of 1.25 mm\n",
      "and scaled to n × 512 × 512 pixels [10].\n",
      "the penet is pre-trained on large-scale\n",
      "ctpa studies and shows excellent pe detection performance with an auroc\n",
      "of 0.85 on our entire dataset.\n",
      "the 2048 dimensional features from the last convo-\n",
      "lution with the highest probability of pe, are designated as the imaging features.\n",
      "we build the encoders of the baseline sp modules and de-biased sp modules\n",
      "with multi-layer perceptron (mlp) neural networks and relu activation.\n",
      "the\n",
      "mlps with 3 hidden layers are used to encode image and text features, and\n",
      "another mlps with 2 layers encodes the features of pesi variables.\n",
      "for training the biased and de-biased sp modules, we collect data from one\n",
      "modality as a batch with synchronized batch normalization.\n",
      "the sp modules\n",
      "are optimized using the adamw [17] optimizer with a momentum of 0.9, a\n",
      "weight decay of 0.0005, and a learning rate of 0.001.\n",
      "experiments are conducted\n",
      "on an nvidia gv100 gpu.\n",
      "de-biased outcome prediction model\n",
      "521\n",
      "fig.\n",
      "based on the com-\n",
      "parison between the id features and others, it is observed that the clusters containing\n",
      "race obtained from the same class are more compact.\n",
      "3. kaplan-meier survival curves of our 3 de-biased sp modules and the multi-\n",
      "modal coxph model.\n",
      "(color ﬁgure online)\n",
      "4.1\n",
      "results\n",
      "table 1 shows the quantitative comparisons of the baseline and de-biased frame-\n",
      "works with the c-indexes of the multimodal survival predictions.\n",
      "in general, our\n",
      "framework including de-biased sp modules shows signiﬁcantly better predictions\n",
      "in testing set than the pesi-based outcome estimation with c-indexes of 0.669,\n",
      "0.654, 0.697, 0.043 for the overall testset, white testset, color testset and race\n",
      "bias.\n",
      "the de-biased results outperform the baseline in overall survival c-index\n",
      "and show a lower race bias, especially in imaging- and fusion-based predictions.\n",
      "the results indicate the eﬀectiveness of the proposed de-biasing in mitigating\n",
      "race inequity.\n",
      "the results also prove the observations for the diﬀerent biases\n",
      "present in diﬀerent modalities, especially in the ctpa images containing more\n",
      "abundant race-related information.\n",
      "it also explains the limited eﬀectiveness of\n",
      "de-biasing the clinical results, which contain less racial identiﬁcation.\n",
      "every 2 columns (overall performance of testing\n",
      "and bias) represent a training setting.\n",
      "swapping\n",
      "×\n",
      "✓\n",
      "×\n",
      "✓\n",
      "resampling\n",
      "×\n",
      "×\n",
      "✓\n",
      "✓\n",
      "dataset\n",
      "testing bias\n",
      "testing bias\n",
      "testing bias\n",
      "testing bias\n",
      "imaging\n",
      "0.666\n",
      "0.062 0.641\n",
      "0.014 0.649\n",
      "0.050 0.622\n",
      "0.035\n",
      "text\n",
      "0.684\n",
      "0.090 0.711\n",
      "0.123 0.698\n",
      "0.102 0.709\n",
      "0.057\n",
      "variable\n",
      "0.702\n",
      "0.095 0.701\n",
      "0.052 0.697\n",
      "0.082 0.699\n",
      "0.095\n",
      "multimodal 0.716\n",
      "0.025 0.737\n",
      "0.041 0.741\n",
      "0.011 0.743\n",
      "0.012\n",
      "diction performance based on multiply modalities is signiﬁcantly better than the\n",
      "pesi-based outcome estimation.\n",
      "the disentangled representations, transformed\n",
      "from latent space to a 2d plane via tsne and color-coded by race [9], are shown\n",
      "in fig.\n",
      "2. we observe the disentanglement in the visualization of the id features\n",
      "zid, while the survival features zsur eliminate the race bias.\n",
      "the lack of appar-\n",
      "ent race bias observed in both the original features and those encoded in the\n",
      "baseline can be attributed to the subordinate role that id features play in the\n",
      "multimodal information.\n",
      "in addition,\n",
      "the predictions of the de-biased framework show favorable performance, and our\n",
      "multimodal fusion demonstrates a more pronounced discriminative ability in the\n",
      "k-m survival analysis compared to the single-modal results.\n",
      "we conducted ablation studies to examine the eﬀect of the two key compo-\n",
      "nents, including swapping feature augmentation and race-balance resampling.\n",
      "as shown in table 2, the diﬀerent training settings show signiﬁcant diﬀerences\n",
      "in survival prediction performance across modalities.\n",
      "the swapping augmenta-\n",
      "tion provides a strong bias correction eﬀect for image data with obvious bias.\n",
      "for clinical data, the resampling generally improves performance in most cases.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf:\n",
      "we propose a novel text-guided cross-position attention\n",
      "module which aims at applying a multi-modality of text and image to\n",
      "position attention in medical image segmentation.\n",
      "to match the dimen-\n",
      "sion of the text feature to that of the image feature map, we multi-\n",
      "ply learnable parameters by text features and combine the multi-modal\n",
      "semantics via cross-attention.\n",
      "it allows a model to learn the dependency\n",
      "between various characteristics of text and image.\n",
      "our proposed model\n",
      "demonstrates superior performance compared to other medical models\n",
      "using image-only data or image-text data.\n",
      "the rois obtained from the model contribute\n",
      "to improve the performance of classiﬁcation models.\n",
      "keywords: image segmentation · multi modal learning · cross\n",
      "position attention · text-guided attention · medical image\n",
      "1\n",
      "introduction\n",
      "advances in deep learning have been witnessed in many research areas over\n",
      "the past decade.\n",
      "in medical ﬁeld, automatic analysis of medical image data has\n",
      "actively been studied.\n",
      "in particular, segmentation which identify region of inter-\n",
      "est (roi) in an automatic way is an essential medical imaging process.\n",
      "thus,\n",
      "deep learning-based segmentation has been utilized in various medical domains\n",
      "such as brain, breast cancers, and colon polyps.\n",
      "among the popular architec-\n",
      "tures, variants of u-net have been widely adopted due to their eﬀective encoder-\n",
      "decoder structure, proﬁcient at capturing the characteristics of cells in images.\n",
      "recently, it has been demonstrated that the attention modules [4,17,20] enable\n",
      "deep learning networks to better extract robust features, which can be applied in\n",
      "medical image segmentation to learn subtle medical features and achieve higher\n",
      "performance [14,16,18,21].\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp.\n",
      "lee et al.\n",
      "however, as image-only training trains a model with pixels that constitute\n",
      "an image, there is a limit in extracting ﬁne-grained information about a target\n",
      "object even if transfer learning is applied through a pre-trained model.\n",
      "recently,\n",
      "to overcome this limitation, multi-modality studies have been conducted, aiming\n",
      "to enhance the expressive power of both text and image features.\n",
      "for instance,\n",
      "clip [12] used contrastive learning based on image-text pairs to learn the sim-\n",
      "ilarity between the image of an object and the text describing it, achieving\n",
      "signiﬁcant performance gains in a variety of computer vision problems.\n",
      "the trend of text-image multi-modality-based research on image processing\n",
      "has extended to the medical ﬁeld.\n",
      "[19] proposed a semantic matching loss that\n",
      "learns medical knowledge to supplement the disadvantages of clip that cannot\n",
      "capture uncertain medical semantic meaning.\n",
      "in [2], they trained to increase\n",
      "the similarity between the image and text by calculating their inﬂuence on each\n",
      "other as a weighted feature.\n",
      "for the segmentation task, lvit\n",
      "furthermore,\n",
      "it proposed a double u-shaped structure consisting of a u-shaped vit that\n",
      "combines image and text information and a u-shaped cnn that produces a\n",
      "segmentation mask.\n",
      "however, when combining medical images with non-ﬁne-\n",
      "grained text information, noise can aﬀect the outcome.\n",
      "in this paper, we propose a new text-guided cross-position attention mod-\n",
      "ule (cpam t g) that combines text and image.\n",
      "in a medical image, a position\n",
      "attention module (pam) eﬀectively learns subtle diﬀerences among pixels.\n",
      "we\n",
      "utilized pam which calculates the inﬂuence among pixels of an image to capture\n",
      "the association between text and image.\n",
      "to this end, we converted the global\n",
      "text representation generated from the text encoder into a form, such as an\n",
      "image feature map, to create keys and values.\n",
      "the image feature map generated\n",
      "from an image encoder was used as a query.\n",
      "learning the association between\n",
      "text and image enables us to learn positional information of targets in an image\n",
      "more eﬀectively than existing models that learned multi-modality from medical\n",
      "images.\n",
      "cpam t g showed an excellent segmentation performance in our com-\n",
      "prehensive experiments on various medical images, such as cell, chest x-ray, and\n",
      "magnetic resonance image (mri).\n",
      "our main contributions are as follows:\n",
      "– we devised a text-guided cross-position attention module (cpam t g) that\n",
      "eﬃciently combines text information with image feature maps.\n",
      "– we demonstrated the eﬀect of cpam t g on segmentation for various types\n",
      "of medical images.\n",
      "– for a practical computer-aided diagnosis system, we conﬁrm the eﬀectiveness\n",
      "of the proposed method in a deep learning-based sacroiliac arthritis diagnosis\n",
      "system.\n",
      "text-guided cross-position attention for segmentation\n",
      "539\n",
      "fig.\n",
      "1. overview of our proposed segmentation model.\n",
      "2\n",
      "methods\n",
      "in this section, we propose text-guided segmentation model that can eﬀectively\n",
      "learn the multi-modality of text and images.\n",
      "figure 1 shows the overall architec-\n",
      "ture of the proposed model, which consists of an image encoder for generating a\n",
      "feature map from an input image, a text encoder for embedding a text describing\n",
      "the image, and a cross-attention module.\n",
      "the cross-attention module allows the\n",
      "text to serve as a guide for image segmentation by using the correlation between\n",
      "the global text representation and the image feature map.\n",
      "to achieve robust text\n",
      "encoding, we adopt a transformer [17] structure which performs well in natural\n",
      "language processing (nlp).\n",
      "for image encoding and decoding, we employed\n",
      "u-net, widely used as a backbone in medical image segmentation.\n",
      "to train our\n",
      "proposed model, we utilize a dataset consisting of image and text pairs.\n",
      "2.1\n",
      "conﬁguration of text-image encoder and decoder\n",
      "as transformer has demonstrated its eﬀectiveness in handling the long-range\n",
      "dependency in sequential data through self-attention [1], it performs well in\n",
      "various ﬁelds requiring nlp or contextual information analysis of data.\n",
      "we\n",
      "used a transformer (encodert ) to encode the semantic information of the text\n",
      "describing a medical image into a global text representation vt ∈ r1×2c as\n",
      "vt = encodert (t).\n",
      "here, the text semantics (t) can be a sentence indicating\n",
      "the location or characteristics of an interested region in an image such as a lesion\n",
      "shown in fig.\n",
      "1.\n",
      "to create a segmentation mask from medical images (i), we used u-net\n",
      "[13]\n",
      "which has a relatively simple yet eﬀective structure for biomedical image segmen-\n",
      "540\n",
      "g.-e.\n",
      "[15] as the encoder (encoderi) to obtain the image feature\n",
      "fi ∈\n",
      "rc×h×w as fi = encoderi(i) and the decoder (decoderi) that will\n",
      "generate the segmented image from the enhanced encoding vector obtained by\n",
      "the cross-position attention which will be described in the following subsection.\n",
      "the weights of text and image encoders were initialized by the weights of\n",
      "clip’s pre-trained transformer and vgg16 pre-trained on imagenet, respec-\n",
      "tively, and ﬁne-tuned by a loss function for segmentation which will be described\n",
      "in sect.\n",
      "[5] to\n",
      "combine the semantic information of text and image.\n",
      "this module utilizes not\n",
      "only the image feature map from the image encoder but also the global text\n",
      "representation from the text encoder to learn the dependency between various\n",
      "characteristics of text and image.\n",
      "in particu-\n",
      "lar, this correlation analysis among pixels can eﬀectively analyze medical images\n",
      "in which objects are relatively ambiguous compared to other types of natural\n",
      "images.\n",
      "by the global\n",
      "text representation (vt ) to match the dimension of the text feature with that of\n",
      "the image feature map as ft = r(g(vt )⊺ × l), where g(·) is a fully connected\n",
      "layer that adjusts the 2c channel of the global text representation vt to the\n",
      "image feature map channel c. r(·) is a reshape operator to c × h × w.\n",
      "text-guided cross-position attention for segmentation\n",
      "541\n",
      "the text feature map ft is used as key and value, and the image feature\n",
      "map fi is used as a query to perform self-attention as\n",
      "q = hq(fi),\n",
      "k = hk(ft ),\n",
      "v = hv (ft ),\n",
      "(1)\n",
      "where hq, hk, and hv are convolution layers with a kernel size of 1, and q,\n",
      "k, and v are queries, keys, and values for self-attention.\n",
      "attention = softmax(q⊺k)\n",
      "(2)\n",
      "cpam t g = attention⊺v + fi\n",
      "(3)\n",
      "finally, by upsampling the low-dimensional cpam t g obtained through cross-\n",
      "attention of text and image together with skip-connection, more accurate seg-\n",
      "mentation prediction can express the detailed information of an object.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "setup\n",
      "medical datasets.\n",
      "[8] contains\n",
      "30 digital microscopic tissue images of several patients and qata-cov19 are\n",
      "covid-19 chest x-ray images.\n",
      "among all mri slices, we selected the gadolinium-\n",
      "enhanced fat-suppressed t1-weighted oblique coronal images, excluding the ﬁrst\n",
      "and last several slices in which the pelvic bones did not appear, and added the\n",
      "text annotations for the slices.\n",
      "training and metrics.\n",
      "for a better training, data augmentation was used.\n",
      "we\n",
      "randomly rotated images by −20◦ ∼ +20◦ and conducted a horizontal ﬂip with\n",
      "0.5 probability for only the monuseg and qata-cov19 datasets.\n",
      "the mdice and miou metrics, widely\n",
      "used to measure the performance of segmentation models, were used to evaluate\n",
      "the performance of object segmentation.\n",
      "for experiments, pytorch (v1.7.0) were\n",
      "used on a computer with nvidia-v100 32 gb gpu.\n",
      "3.2\n",
      "segmentation performance\n",
      "table 1 presents the comparison of image segmentation performance among\n",
      "the proposed model and the u-net\n",
      "analyzing the results in table 1, unlike nat-\n",
      "ural image segmentation, the attention module-based method (attention u-net)\n",
      "and transformer-based method (medt) did not achieve signiﬁcant performance\n",
      "gains compared to u-net based methods (u-net and u-net++).\n",
      "by contrast, lvit and cpam t g, which utilize both text and image informa-\n",
      "tion, signiﬁcantly improved image segmentation performance because of multi-\n",
      "modal complementarity, even for medical images with complex and ambiguous\n",
      "object boundaries.\n",
      "furthermore, cpam t g achieves a better performance by 1\n",
      "to 3% than lvit\n",
      "this means that the proposed cpam t g\n",
      "helps to improve segmentation performance by allowing text information to serve\n",
      "as a guide for feature extraction for segmentation.\n",
      "figure 3 shows the examples of segmentation masks obtained using each\n",
      "method.\n",
      "3 shows that cpam t g\n",
      "and lvit, which use text information together for image segmentation, create a\n",
      "segmentation mask with more distinctive borders than other methods.\n",
      "figure 3 also shows that even on the qata-cov19 and\n",
      "monuseg datasets, cpam t g predicted the most accurate segmentation masks\n",
      "(see the red box areas).\n",
      "from these results, we conjecture that the reasons for the\n",
      "performance improvement of cpam t g are as follows.\n",
      "cpam t g independently\n",
      "encodes the input text and image and then combines semantic information via\n",
      "a cross-attention module.\n",
      "consequently, the two types of information (text and\n",
      "image) do not act as noise from each other, and cpam t g achieves an improved\n",
      "performance compared to lvit.\n",
      "in addition, we investigated whether text\n",
      "information about images serves as a guide in the position attention process for\n",
      "image segmentation by comparing it with cpam t g. table 2 summarizes the\n",
      "result of each case.\n",
      "as can be observed in table 2, the performance of pam\n",
      "was higher than that of the backbone.\n",
      "this indicates that pam improves per-\n",
      "formance by learning associations between pixels for ambiguous targets, as in\n",
      "medical images.\n",
      "in addition, the best performance results of cpam t g show\n",
      "that text information provided helpful information in an image segmentation\n",
      "process using the proposed model.\n",
      "text-guided cross-position attention for segmentation\n",
      "543\n",
      "table 1.\n",
      "performance comparison of medical segmentation models with three datasets\n",
      "qata-cov19\n",
      "monuseg\n",
      "sij\n",
      "mdice\n",
      "miou\n",
      "mdice\n",
      "miou\n",
      "mdice\n",
      "miou\n",
      "u-net\n",
      "0.7902\n",
      "0.6946\n",
      "0.7645\n",
      "0.6286\n",
      "0.7395\n",
      "0.6082\n",
      "u-net++ 0.7962\n",
      "0.7025\n",
      "0.7701\n",
      "0.6304\n",
      "0.7481\n",
      "0.6124\n",
      "attunet\n",
      "0.7931\n",
      "3. qualitative results of segmentation models.\n",
      "3.4\n",
      "application: deep-learning based disease diagnosis\n",
      "in this section, we conﬁrm the eﬀectiveness of the proposed segmentation method\n",
      "through a practical bio-medical application as a deep learning-based active\n",
      "sacroiliitis diagnosis system.\n",
      "we segmented the pelvic bones in mri slices using the proposed method\n",
      "to construct a fully automatic deep learning-based active sacroiliitis diagnosis\n",
      "system, including roi settings from mri input images.\n",
      "“pam” means\n",
      "we used it instead of cpam t g for image-only training.\n",
      "as presented in table 3, compared to the case of using\n",
      "the original mri image without the roi setting, using the hand-crafted roi\n",
      "patch [9] showed an average of 7% higher performance in recall, precision, and\n",
      "f1.\n",
      "it is noticeable that the automatically set roi patch showed similar or better\n",
      "performance than the manual roi patch for each measurement.\n",
      "4\n",
      "conclusion\n",
      "in\n",
      "this\n",
      "study,\n",
      "we\n",
      "developed\n",
      "a\n",
      "new\n",
      "text-guided\n",
      "cross-attention\n",
      "module\n",
      "(cpam t g) that learns text and image information together.\n",
      "the proposed\n",
      "model has a composite structure of position attention and cross-attention in\n",
      "that the key and value are from text data, and the query is created from the\n",
      "image.\n",
      "we use a learnable parameter to convert text features into a tensor of\n",
      "the same dimension as the image feature map to combine text and image infor-\n",
      "mation eﬀectively.\n",
      "by calculating the association between the reshaped global\n",
      "text representation and each component of the image feature map, the proposed\n",
      "method outperformed image segmentation performance compared to previous\n",
      "studies using both text and image or image-only training method.\n",
      "we also con-\n",
      "ﬁrmed that it could be utilized for a deep-learning-based sacroiliac arthritis\n",
      "text-guided cross-position attention for segmentation\n",
      "545\n",
      "diagnosis system, one of the use cases for practical medical applications.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_3.pdf:\n",
      "the rapid identiﬁcation and accurate diagnosis of breast can-\n",
      "cer, known as the killer of women, have become greatly signiﬁcant for\n",
      "those patients.\n",
      "numerous breast cancer histopathological image classiﬁ-\n",
      "cation methods have been proposed.\n",
      "(1) these methods can only hand high-resolution (hr) images.\n",
      "however, the low-resolution (lr) images are often collected by the dig-\n",
      "ital slide scanner with limited hardware conditions.\n",
      "compared with hr\n",
      "images, lr images often lose some key features like texture, which\n",
      "deeply aﬀects the accuracy of diagnosis.\n",
      "(2) the existing methods have\n",
      "ﬁxed receptive ﬁelds, so they can not extract and fuse multi-scale fea-\n",
      "tures well for images with diﬀerent magniﬁcation factors.\n",
      "to ﬁll these\n",
      "gaps, we present a single histopathological image super-resolution\n",
      "classiﬁcation network (shisrcnet), which consists of two modules:\n",
      "super-resolution (sr) and classiﬁcation (cf) modules.\n",
      "sr module\n",
      "reconstructs lr images into sr ones.\n",
      "cf module extracts and fuses\n",
      "the multi-scale features of sr images for classiﬁcation.\n",
      "in the training\n",
      "stage, we introduce hr images into the cf module to enhance shis-\n",
      "rcnet’s performance.\n",
      "finally, through the joint training of these two\n",
      "modules, super-resolution and classiﬁed of lr images are integrated into\n",
      "our model.\n",
      "the experimental results demonstrate that the eﬀects of our\n",
      "method are close to the sota methods with taking hr images as inputs.\n",
      "keywords: breast cancer · histopathological image · super-resolution ·\n",
      "classiﬁcation · joint training\n",
      "1\n",
      "introduction\n",
      "breast cancer is one of the high-mortality cancers among women in the 21st\n",
      "century.\n",
      "every year, 1.2 million women around the world suﬀer from breast\n",
      "cancer and about 0.5 million die of it\n",
      "https://doi.org/10.1007/978-3-031-43904-9_3\n",
      "24\n",
      "l. xie et al.\n",
      "will make a correct assessment of the patient’s risk and improve the chances\n",
      "of survival.\n",
      "due to being collected by various devices, the resolution of histopathologi-\n",
      "cal images extracted may not always be high.\n",
      "low-resolution (lr) images lack\n",
      "of lots of details, which will have an important impact on doctors’ diagnosis.\n",
      "considering the improvement of histopathological images’ acquisition equipment\n",
      "will cost lots of money while signiﬁcantly increasing patients’ expense of detec-\n",
      "tion.\n",
      "the super-resolution (sr) algorithms that improve the resolution of lr\n",
      "images at a small cost can be a practical solution to assist doctors in diagno-\n",
      "sis.\n",
      "[9] and multi-scale reﬁned context to improve the eﬀect of\n",
      "reconstructing histopathological images.\n",
      "this limits its performance in the scenarios\n",
      "with various magniﬁcation factors.\n",
      "therefore, designing an appropriate feature\n",
      "extraction block for sr of the histopathological images is still a challenging task.\n",
      "in recent years, a series of deep learning methods have been proposed to\n",
      "solve the breast cancer histopathological image classiﬁcation issue by the high-\n",
      "resolution (hr) histopathological images.\n",
      "[12,21,22] improved the speciﬁc model\n",
      "structure to classify breast histopathology images, which showed a signiﬁcant\n",
      "improvement in recognition accuracy compared with the previous works [1,20].\n",
      "so it is still worth to explore the potential of extraction and fusion of multi-scale\n",
      "features for breast images classiﬁcation.\n",
      "to tackle the problem of lr breast cancer histopathological images recon-\n",
      "struction and diagnosis, we propose the single histopathological image super-\n",
      "resolution classiﬁcation network (shisrcnet) integrating super-resolution\n",
      "(sr) and classiﬁcation (cf) modules.\n",
      "these make mfeblock reconstruct lr images into sr images well.\n",
      "(2) the cf module completes the task of image classiﬁcation by utilizing the\n",
      "sr images.\n",
      "(3) through the joint training of these two designed modules, the super-\n",
      "resolution and classiﬁcation of low-resolution histopathological images are inte-\n",
      "grated into our model.\n",
      "for improving the performance of cf module and reduc-\n",
      "ing the error caused by the reconstructed sr images, we introduce hr images to\n",
      "cf module in the training stage.\n",
      "the experimental results demonstrate that the\n",
      "eﬀects of our method are close to those of sota methods that take hr breast\n",
      "cancer histopathological images as inputs.\n",
      "the sr module reconstructs the lr image into the sr image.\n",
      "the\n",
      "cf module utilize the reconstructed sr images to diagnose histopathological\n",
      "images.\n",
      "in the training stage, we introduce hr images to improve the perfor-\n",
      "mance of cf module and alleviate the error caused by sr images.\n",
      "[11], srmfenet takes\n",
      "a single low-resolution image as input and uses the pixelshuﬄe layer to get the\n",
      "restructured image.\n",
      "the diﬀerence between srmfenet and srresnet is that\n",
      "a multi-features extraction block (mfeblock) is proposed to extract and fuse\n",
      "multi-scale histopathological images’ features.\n",
      "i <= n\n",
      "where n is the number of atrous convolutions and is set to 4 by the experiments.\n",
      "it is beneﬁcial for the network to extract shallow local tex-\n",
      "ture information and global semantic information.\n",
      "firstly, we conduct global average pooling (gap)\n",
      "[14] on the multi-scale features to obtain their average channel-wise weights.\n",
      "next soft-\n",
      "max operation normalizes the same position of the obtained multi-scale average\n",
      "channel-wise weights.\n",
      "mfeblock is very applicable to process histopatho-\n",
      "logical images of diﬀerent magniﬁcation factors, as it employs convolution and\n",
      "attention operations to capture local and global image context information and\n",
      "fuse them well.\n",
      "2.2\n",
      "classiﬁcation module\n",
      "the task of the cf module is to classify the reconstructed sr images.\n",
      "in csfblock, the upsampling operations are performed on\n",
      "shisrcnet\n",
      "27\n",
      "the low-resolution features xl to realize consistency with xh dimension.\n",
      "xh and\n",
      "restructured xl are fused via an element-wise summation:\n",
      "u = xh + up(xl)\n",
      "then, using gap along the channel dimension to get the global information\n",
      "s. a fc layer generates a compact feature vector z which guides the feature\n",
      "selection procedure.\n",
      "and z is reconstructed into two weight vectors a, b of the\n",
      "same dimension as s through two fc layers, which can be deﬁned as:\n",
      "z = δ(wcs),\n",
      "a = waz,\n",
      "b = wbz\n",
      "where δ denotes relu and wa, wb, wc, means the weight of the fc layers.\n",
      "in the cf module,\n",
      "we introduce hr images to cf module in the training stage for improving the\n",
      "performance of cf module and reducing the error caused by the reconstructed\n",
      "sr images.\n",
      "[16] to alleviate the class imbalanced data problem\n",
      "of the hr and sr images’ classiﬁcation.\n",
      "[5], the hr and sr of the same images are similar to two\n",
      "diﬀerent views.\n",
      "in the inference stage, only sr images are taken as inputs\n",
      "by cf module.\n",
      "in our experiment, λ1, λ2 and λ3 are set to 0.6, 0.3 and 0.1,\n",
      "respectively.\n",
      "and the temperature parameter in nt − xent loss is set to 0.5.\n",
      "3\n",
      "experiment\n",
      "dataset: this work uses the breast cancer histopathological image database\n",
      "(breakhis)1 [20].\n",
      "the images in the dataset have four magniﬁcation factors\n",
      "1 https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-\n",
      "breakhis/.\n",
      "28\n",
      "l. xie et al.\n",
      "(40x, 100x, 200x, 400x) and eight breast cancer classes.\n",
      "implementation details: for all experiments, we conduct 5-fold cross valida-\n",
      "tion, and report the mean.\n",
      "we use lr histopathological images with size 48 × 48,\n",
      "96 × 96, 192 × 192 as input for diﬀerent single image sr tasks (x8, x4, x2) and\n",
      "set batch size to 8.\n",
      "for the corresponding lr and hr images in the training\n",
      "dataset, the same data augmentation is adopted, such as rotation, color jitter.\n",
      "[6] to evalu-\n",
      "ate the performance of the sr model.\n",
      "and the joint train-\n",
      "ing of srmfenet and cf module improves the performance of super-resolution.\n",
      "we compare our introduced cf module with ﬁve state-of-the-art breast can-\n",
      "cer histopathological image models and diagnosis network with mrc-net\n",
      "average psnr/ssim for x8, x4, x2 sr.\n",
      "2. qualitative comparison with sr methods on breast cancer histopathological\n",
      "images x8 and x4.\n",
      "table 2. compare results with state-of-the-art on image level (* means that inputs are\n",
      "hr images, # means that inputs are down sample to half resolution from hr images.).\n",
      "[6] 2021\n",
      "94.43\n",
      "94.45\n",
      "94.73\n",
      "93.92\n",
      "shisrcnet (ours)#\n",
      "-\n",
      "97.49 96.19 97.60 97.04\n",
      "performance in four diﬀerent magniﬁcation factors.\n",
      "shisrcnet, which uses down sample to half resolution (x2) from hr images,\n",
      "outperforms the ssca at 40x, 200x and 400x.\n",
      "meanwhile, compared with the diagnosis\n",
      "network that also uses lr images as input, shiscnet has remarked performance\n",
      "advantages.\n",
      "table 3 compares our results with the cf module using diﬀerent res-\n",
      "olution images.\n",
      "the performance of the cf module decreases signiﬁcantly with\n",
      "the reduction of resolution.\n",
      "in contrast, shisrcnet greatly improves the cf\n",
      "module performance of diﬀerent scale low-resolution images.\n",
      "30\n",
      "l. xie et al.\n",
      "table 3. comparison of accuracy under diﬀerent scales on the image level.\n",
      "4.2\n",
      "ablation study of the shisrcnet\n",
      "to verify the eﬀectiveness of the proposed components in shisrcnet, a com-\n",
      "parison between shisrcnet and its ﬁve components on x2 images is given in\n",
      "table 3.\n",
      "(3) w/o csfblock, w/o hr images and w/o nt-\n",
      "xent loss remove the corresponding operation, respectively.\n",
      "as shown in table 3,\n",
      "ﬁrstly, the performance of super-resolution in the shisrcnet is signiﬁcantly\n",
      "reduced when we remove msf.\n",
      "thirdly, compared with using fpn alone, the\n",
      "performance of shisrcnet is further improved by adding csfblock to fpn.\n",
      "finally, the introduction of hr images further promotes the performance of\n",
      "shisrcnet.\n",
      "because the training method of hr and sr images proposed by us\n",
      "helps to improve the generalization of the shisrcnet (table 4).\n",
      "table 4. ablation study of shisrcnet on x2 images.\n",
      "w/o fpn+csfblock 34.41\n",
      "0.9609\n",
      "93.98\n",
      "92.11\n",
      "91.35\n",
      "92.15\n",
      "w/o csfblock\n",
      "34.57\n",
      "0.9619\n",
      "94.37\n",
      "93.21\n",
      "93.99\n",
      "94.71\n",
      "w/o hr images\n",
      "34.43\n",
      "0.9611\n",
      "93.13\n",
      "94.28\n",
      "93.86\n",
      "94.17\n",
      "w/o nt-xent loss\n",
      "34.54\n",
      "0.9623\n",
      "95.53\n",
      "95.20\n",
      "95.01\n",
      "95.36\n",
      "shisrcnet\n",
      "31\n",
      "5\n",
      "conclusion\n",
      "this paper proposes shisrcnet for the low-resolution breast cancer histopatho-\n",
      "logical images’ super-resolution and classiﬁcation problem.\n",
      "the sr module\n",
      "employs mfeblock to extract and fuse multi-scale features for reconstruct-\n",
      "ing low-resolution histopathological images into high-resolution ones.\n",
      "we introduce high-resolution images\n",
      "into the cf module in the training stage to improve shisrcnet’s robustness.\n",
      "through the joint training of the two modules, the super-resolution and classiﬁ-\n",
      "cation of the low-resolution histopathological images are integrated in one model.\n",
      "our method’s results are close to the sota methods, which require using high-\n",
      "resolution breast cancer histopathological images instead of low-resolution ones.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf:\n",
      "in clinical practice,\n",
      "the contextual structure of nodules and the accumulated experience of\n",
      "radiologists are the two core elements related to the accuracy of identi-\n",
      "ﬁcation of benign and malignant nodules.\n",
      "the context parsing module ﬁrst\n",
      "segments the context structure of nodules and then aggregates contex-\n",
      "tual information for a more comprehensive understanding of the nodule.\n",
      "the prototype recalling module utilizes prototype-based learning to con-\n",
      "dense previously learned cases as prototypes for comparative analysis,\n",
      "which is updated online in a momentum way during training.\n",
      "building\n",
      "on the two modules, our method leverages both the intrinsic characteris-\n",
      "tics of the nodules and the external knowledge accumulated from other\n",
      "nodules to achieve a sound diagnosis.\n",
      "experiments on several\n",
      "datasets demonstrate that our method achieves advanced screening per-\n",
      "formance on both low-dose and noncontrast scenarios.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9 20.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "particularly, the evaluation of nodule (i.e., 8–30mm)\n",
      "malignancy is recommended in the guidelines [13].\n",
      "fig.\n",
      "it is char-\n",
      "acterized by a lack of standard-of-\n",
      "truth of labels for malignancy [16,\n",
      "27], and due to this limitation, many\n",
      "studies use radiologists’ subjective\n",
      "judgment on ct as labels, such as\n",
      "lidc-idri\n",
      "despite their advantages in rep-\n",
      "resentation learning, these methods do not take into account expert diagnostic\n",
      "knowledge and experience, which may lead to a bad consequence of poor general-\n",
      "ization.\n",
      "motivated by this, we ﬁrst segment the context\n",
      "structure, i.e., nodule and its surroundings, and then aggregate the context\n",
      "information to the nodule representation via the attention-based dependency\n",
      "modeling, allowing for a more comprehensive understanding of the nodule itself.\n",
      "experimental results on sev-\n",
      "eral datasets demonstrate that our method achieves outstanding performance on\n",
      "both ldct and ncct screening scenarios.\n",
      "(4) our method achieves advanced\n",
      "malignancy prediction performance in both screening scenarios (0.931 auc), and\n",
      "exhibits strong generalization in external validation, setting a new state of the\n",
      "art on lungx (0.801 auc).\n",
      "202\n",
      "j. zhang et al.\n",
      "2\n",
      "method\n",
      "figure 2 illustrates the overall architecture of pare, which consists of three\n",
      "stages: context segmentation, intra context parsing, and inter prototype recall-\n",
      "ing.\n",
      "we now delve into diﬀerent stages in detail in the following subsections.\n",
      "2.1\n",
      "context segmentation\n",
      "the nodule context information has an important eﬀect on the benign and malig-\n",
      "nant diagnosis.\n",
      "therefore, we use a u-like net-\n",
      "work (unet) to parse the semantic mask m for the input image patch x, thus\n",
      "allowing subsequent context modeling of both the nodule and its surrounding\n",
      "structures.\n",
      "this segmentation process allows pare to gather\n",
      "comprehensive context information that is crucial for an accurate diagnosis.\n",
      "for\n",
      "the diagnosis purpose, we extract the global feature from the bottleneck of unet\n",
      "as the nodule embedding q, which will be used in later diagnostic stages.\n",
      "2.2\n",
      "intra context parse\n",
      "in this stage, we attempt to enhance the discriminative representations of\n",
      "nodules by aggregating contextual information produced by the segmentation\n",
      "model.\n",
      "the input image is also split into patches and\n",
      "then embedded into the context tokens to keep the original image information.\n",
      "besides, positional encoding is added in a learnable manner to retain location\n",
      "information.\n",
      "here\n",
      "g is the number of context tokens, and d represents the embedding dimension.\n",
      "we believe that explicitly\n",
      "modeling the dependency between nodule embedding and its contextual struc-\n",
      "ture can lead to the evolution of more discriminative representations, thereby\n",
      "improving discrimination between benign and malignant nodules.\n",
      "2.3\n",
      "inter prototype recall\n",
      "deﬁnition of the prototype: to retain previously acquired knowledge, a\n",
      "more eﬃcient approach is needed instead of storing all learned nodules in mem-\n",
      "ory, which leads to a waste of storage and computing resources.\n",
      "cross prototype attention: in addition to parsing intra context, we also\n",
      "encourage the model to capture inter-level dependencies between nodules and\n",
      "external prototypes.\n",
      "updating prototype online: the prototypes are updated in an online man-\n",
      "ner, thereby allowing them to adjust quickly to changes in the nodule represen-\n",
      "tations.\n",
      "as for the nodule embedding q of the data (x, y), its nearest prototype\n",
      "is singled out and then updated by the following momentum rules,\n",
      "\u0003 p b\n",
      "arg minjd(q,p b\n",
      "j ) = λ · p b\n",
      "arg minjd(q,p b\n",
      "j ) + (1 − λ) · q\n",
      "if y = 0\n",
      "p m\n",
      "arg minjd(q,p m\n",
      "j\n",
      ")\n",
      "+ (1 − λ) · q otherwise\n",
      "(1)\n",
      "where λ is the momentum factor, set to 0.95 by default.\n",
      "the momentum updating\n",
      "can help accelerate the convergence and improve the generalization ability.\n",
      "2.4\n",
      "training process of pare\n",
      "the algorithm 1 outlines the training process of our pare model which is\n",
      "based on two objectives: segmentation and classiﬁcation.\n",
      "the dice and cross-\n",
      "entropy loss are combined for segmentation, while cross-entropy loss is used for\n",
      "classiﬁcation.\n",
      "additionally, deep classiﬁcation supervision is utilized to enhance\n",
      "the representation of nodule embedding in shallow layers like the output of the\n",
      "unet and sca modules.\n",
      "3\n",
      "experiment\n",
      "3.1\n",
      "datasets and implementation details\n",
      "data collection and curation: nlst is the ﬁrst large-scale ldct dataset\n",
      "for low-dose ct lung cancer screening purpose\n",
      "segmentation annotation: we provide the segmentation mask for our\n",
      "in-house data, but not for the nlst data considering its high cost of pixel-level\n",
      "labeling.\n",
      "the nodule mask of each in-house data was manually annotated with\n",
      "the assistance of ct labeler\n",
      "[20] by our radiologists, while other contextual masks\n",
      "such as lung, vessel, and trachea were generated using the totalsegmentator\n",
      "segmentation: we\n",
      "also evaluate the segmentation performance of our method on the public nod-\n",
      "ule segmentation dataset lidc-idri\n",
      "[3], which has 2,630 nodules with nodule\n",
      "segmentation mask.\n",
      "evaluation metrics: the area under the receiver operat-\n",
      "ing characteristic curve (auc) is used to evaluate the malignancy prediction\n",
      "performance.\n",
      "towards accurate lung nodule malignancy prediction like radiologists\n",
      "205\n",
      "implementation: all experiments in this work were implemented based on the\n",
      "nnunet framework [8], with the input size of 32 × 48 × 48, batch size of 64, and\n",
      "total training iterations of 10k.\n",
      "the hyper-parameters of pare\n",
      "are empirically set based on the ablation experiments on the validation set.\n",
      "due to the lack of manual annotation of nodule masks for the\n",
      "nlst dataset, we can only optimize the segmentation task using our in-house\n",
      "dataset, which has manual nodule masks.\n",
      "3.2\n",
      "experiment results\n",
      "ablation study: in table 1, we investigate the impact of diﬀerent conﬁgura-\n",
      "tions on the performance of pare on the validation set, including transformer\n",
      "layers, number of prototypes, embedding dimension, and deep supervision.\n",
      "we\n",
      "observe that a higher auc score can be obtained by increasing the number of\n",
      "transformer layers, increasing the number of prototypes, doubling the channel\n",
      "dimension of token embeddings, or using deep classiﬁcation supervision.\n",
      "based\n",
      "on the highest auc score of 0.931, we empirically set l=4, n=40, d=256, and\n",
      "ds=true in the following experiments.\n",
      "in table 2, we investigate the ablation\n",
      "study of diﬀerent methods/modules on the validation set and observe the follow-\n",
      "ing results: (1) the pure segmentation method performs better than the pure\n",
      "classiﬁcation method, primarily because it enables greater supervision at the\n",
      "pixel level, (2) the joint segmentation and classiﬁcation is superior to any sin-\n",
      "gle method, indicating the complementary eﬀect of both tasks, (3) both context\n",
      "parsing and prototype comparing contribute to improved performance on the\n",
      "strong baseline, demonstrating the eﬀectiveness of both modules, and (4) seg-\n",
      "menting more contextual structures such as vessels, lungs, and trachea provide\n",
      "a slight improvement, compared to solely segmenting nodules.\n",
      "table 1. ablation comparison of hyper-\n",
      "parameters (transformer layers (l), num-\n",
      "ber of prototypes (n), embedding dimen-\n",
      "sion (d), and deep supervision (ds))\n",
      "*: only nodule mask\n",
      "was used in the segmentation task.\n",
      "method\n",
      "auc\n",
      "pure classiﬁcation\n",
      "0.907\n",
      "pure segmentation\n",
      "0.915\n",
      "mt\n",
      "0.916\n",
      "mt+context*\n",
      "0.921\n",
      "mt+context\n",
      "0.924\n",
      "mt+context+prototype 0.931\n",
      "206\n",
      "j. zhang et al.\n",
      "table 3. comparison of diﬀerent methods on both nlst and in-house test sets.\n",
      "†:\n",
      "pure classiﬁcation; ‡: pure segmentation; ⋄: multi-task learning; *: ensemble of deep\n",
      "supervision heads.\n",
      "note that we add the segmentation task in ca-net.\n",
      "[25]†\n",
      "0.821\n",
      "0.755\n",
      "0.810\n",
      "0.908\n",
      "0.858\n",
      "0.784\n",
      "0.751\n",
      "0.904\n",
      "nnunet [8]‡\n",
      "0.815\n",
      "0.736\n",
      "0.815\n",
      "0.910\n",
      "0.863\n",
      "0.804\n",
      "0.750\n",
      "0.911\n",
      "ca-net [12]⋄ 0.833\n",
      "0.759\n",
      "0.807\n",
      "0.916\n",
      "0.878\n",
      "0.786\n",
      "0.779\n",
      "0.918\n",
      "pare⋄\n",
      "0.882\n",
      "0.770\n",
      "0.826\n",
      "0.928\n",
      "0.892\n",
      "0.817\n",
      "0.783\n",
      "0.927\n",
      "pare⋄*\n",
      "0.890\n",
      "0.781\n",
      "0.827\n",
      "0.931 0.899\n",
      "0.821\n",
      "0.780\n",
      "0.931\n",
      "comparison to other methods on both screening scenarios: table 3\n",
      "presents a comparison of pare with other advanced methods, including pure\n",
      "classiﬁcation-based, pure segmentation-based, and multi-task-based methods.\n",
      "stratiﬁcation assessments were made in both test sets based on the nodule size\n",
      "distribution.\n",
      "the results indicate that the segmentation-based method outper-\n",
      "forms pure classiﬁcation methods, mainly due to its superior ability to segment\n",
      "contextual structures.\n",
      "3 reveal that our method achieves\n",
      "performance comparable to that of radiologists.\n",
      "generalization on ldct and ncct: our model is trained on a mix of\n",
      "ldct and ncct datasets, which can perform robustly across low-dose and\n",
      "regular-dose applications.\n",
      "we compare the generalization performance of the\n",
      "models obtained under three training data conﬁgurations (ldct, ncct, and\n",
      "a combination of them).\n",
      "however, our mixed training approach performs best on both\n",
      "ldct and ncct with almost no performance degradation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_35.pdf:\n",
      "accurately classifying the histological subtype of non-small cell lung\n",
      "cancer (nsclc) using computed tomography (ct) images is critical for clinicians\n",
      "in determining the best treatment options for patients.\n",
      "although recent advances\n",
      "in multi-view approaches have shown promising results, discrepancies between\n",
      "ct images from different views introduce various representations in the feature\n",
      "space, hindering the effective integration of multiple views and thus impeding\n",
      "classiﬁcation performance.\n",
      "speciﬁcally, we introduce a cross-view representation alignment\n",
      "learning network which learns effective view-invariant representations in a com-\n",
      "mon subspace to reduce multi-view discrepancies in a discriminability-enforcing\n",
      "way.\n",
      "additionally, carl learns view-speciﬁc representations as a complement\n",
      "to provide a holistic and disentangled perspective of the multi-view ct images.\n",
      "experimentalresultsdemonstratethatcarlcaneffectivelyreducethemulti-view\n",
      "discrepancies and outperform other state-of-the-art nsclc histological subtype\n",
      "classiﬁcation methods.\n",
      "keywords: cross-view alignment · representation learning · multi-view ·\n",
      "histologic subtype classiﬁcation · non-small cell lung cancer\n",
      "1\n",
      "introduction\n",
      "lung cancer is currently the foremost cause of cancer-related mortalities globally, with\n",
      "non-small cell lung cancer (nsclc) being responsible for 85% of reported cases [25].\n",
      "since scc and adc differ in the\n",
      "effectiveness of chemotherapy and the risk of complications, accurate identiﬁcation of\n",
      "different subtypes is crucial for clinical treatment options [15].\n",
      "therefore, non-invasive meth-\n",
      "ods utilizing computed tomography (ct) images have garnered signiﬁcant attention over\n",
      "the last decade [15, 16].\n",
      "recently, several deep-learning methods have been put forward to differentiate\n",
      "between the nsclc histological subtypes using ct images [4, 11, 13, 22]. chaun-\n",
      "zwa et al.\n",
      "[13] both employ a convolutional neural network\n",
      "(cnn) model with axial view ct images to classify the tumor histology into scc and\n",
      "adc.\n",
      "albeit the good performance, the above 2d cnn-based models only take ct\n",
      "images from a single view as the input, limiting their ability to describe rich spatial\n",
      "properties of ct volumes\n",
      "[22] aggregate features from axial, coronal, and sagittal view\n",
      "ct images via a multi-view fusion model.\n",
      "additionally,\n",
      "images from certain views may inevitably contain some unique background information,\n",
      "e.g., the spine in the sagittal view [17].\n",
      "consequently, the discrepancies\n",
      "of distinct views will hamper the fusion of multi-view information, limiting further\n",
      "improvements in the classiﬁcation performance.\n",
      "to overcome the challenge mentioned above, we propose a novel cross-aligned repre-\n",
      "sentation learning (carl) method for the multi-view histologic subtype classiﬁcation of\n",
      "nsclc.\n",
      "carl offers a holistic and disentangled perspective of multi-view ct images\n",
      "by generating both view-invariant and -speciﬁc representations.\n",
      "speciﬁcally, carl\n",
      "incorporates a cross-view representation alignment learning network which targets the\n",
      "reduction of multi-view discrepancies by obtaining discriminative view-invariant repre-\n",
      "sentations.asharedencoderwithanoveldiscriminability-enforcingsimilarityconstraint\n",
      "is utilized to map all representations learned from multi-view ct images to a common\n",
      "subspace, enabling cross-view representation alignment.\n",
      "such aligned projections help\n",
      "to capture view-invariant features of cross-view ct images and meanwhile make full\n",
      "use of the discriminative information obtained from each view.\n",
      "additionally, carl\n",
      "learns view-speciﬁc representations as well which complement the view-invariant ones,\n",
      "providing a comprehensive picture of the ct volume data for histological subtype pre-\n",
      "diction.\n",
      "detailed experimental results demonstrate the\n",
      "effectiveness of carl in reducing multi-view discrepancies and improving nsclc\n",
      "360\n",
      "y. luo et al.\n",
      "histological subtype classiﬁcation performance.\n",
      "to reduce the discrepancies of multi-\n",
      "view ct images, carl incorporates a cross-view representation alignment learning\n",
      "network for discriminative view-invariant representations.\n",
      "– we employ a view-speciﬁc representation learning network to learn view-speciﬁc\n",
      "representations as a complement to the view-invariant representations.\n",
      "– we conduct experiments on a publicly available dataset and achieve superior\n",
      "performance compared to the most advanced methods currently available.\n",
      "fig.\n",
      "the cross-view representation align-\n",
      "mentlearningnetworkincludesasharedencoderwhichprojectspatchesofaxial,coronal,\n",
      "and sagittal views into a common subspace with a discriminability-enforcing similarity\n",
      "constraint to obtain discriminative view-invariant representations for multi-view dis-\n",
      "crepancy reduction.\n",
      "in addition, carl introduces a view-speciﬁc representation learn-\n",
      "ing network consisting of three unique encoders which focus on learning view-speciﬁc\n",
      "carl: cross-aligned representation learning\n",
      "361\n",
      "representations in respective private subspaces to yield complementary information to\n",
      "view-invariant representations.\n",
      "2.2\n",
      "cross-view representation alignment learning\n",
      "since the discrepancies of different views may result in divergent statistical properties in\n",
      "feature space, e.g., huge distributional disparities, aligning representations of different\n",
      "views is essential for multi-view fusion.\n",
      "with the aim to reduce multi-view discrepancies,\n",
      "carl introduces a cross-view representation alignment learning network for mapping\n",
      "the representations from distinct views into a common subspace, where view-invariant\n",
      "representations can be obtained by cross-view alignment.\n",
      "technically speaking, given the axial view image iav, coronal view image icv, and\n",
      "sagittal view image isv, the cross-view representation alignment learning network tries to\n",
      "generate view-invariant representations hc\n",
      "v, v ∈ {av, cv, sv} via a shared encoder based\n",
      "on a residual neural network\n",
      "however, the distributions of hc\n",
      "av, hc\n",
      "cv and hc\n",
      "sv are very complex due to the signif-\n",
      "icant variations between different views, which puts a burden on obtaining well-aligned\n",
      "view-invariant representations with merely an encoder.\n",
      "to address this issue, we design a discriminability-enforcing similarity loss ldsim to\n",
      "further enhance the alignment of cross-view representations in the common subspace.\n",
      "mathematically, we introduce a cross-view similarity loss lsim which\n",
      "calculates the central moment discrepancy (cmd) metric\n",
      "despite the fact that minimizing the lsim can efﬁciently mitigate\n",
      "the issue of distributional disparities, it may not guarantee that the alignment network\n",
      "will learn informative and discriminative representations.\n",
      "by minimizing ldsim, the\n",
      "cross-view representation alignment learning network pushes the representations of each\n",
      "sub-view to align with those of the main view in a discriminability-enforcing manner.\n",
      "notably, the beneﬁts of such cross-alignment are twofold.\n",
      "secondly, since the alignment between distinct views compels\n",
      "the representation distribution of the sub-views to match that of the discriminative main\n",
      "view, it can also enhance the discriminative power of the sub-view representations.\n",
      "in other words, the cross-alignment procedure spontaneously promotes the transfer of\n",
      "discriminative information learned by the representations of the main view to those of\n",
      "the sub-views.\n",
      "as a result, the introduced cross-view representation alignment learning\n",
      "network is able to generate consistent and discriminative view-invariant representations\n",
      "cross all views to effectively narrow the multi-view discrepancies.\n",
      "2.3\n",
      "view-speciﬁc representation learning\n",
      "on the basis of learning view-invariant representations, carl additionally learns\n",
      "view-speciﬁc representations in respective private subspaces, which provides supple-\n",
      "mentary information for the view-invariant representations and contribute to subtype\n",
      "classiﬁcation as well.\n",
      "a\n",
      "reconstruction module is also employed to calculate a reconstruction loss lrec between\n",
      "original image iv and reconstructed image ir\n",
      "v using the l1-norm, which ensures the\n",
      "hidden representations to capture details of the respective view.\n",
      "throughout the experiments, we set the values of α, β and γ to 0.6s, 0.4 and 0.6,\n",
      "respectively.\n",
      "3\n",
      "experiments and results\n",
      "3.1\n",
      "dataset\n",
      "our dataset nsclc-tcia for lung cancer histological subtype classiﬁcation is sourced\n",
      "from two online resources of the cancer imaging archive (tcia)\n",
      "we evaluate the\n",
      "performance of nsclc classiﬁcation in ﬁve-fold cross validation on the nsclc-tcia\n",
      "dataset, and measure accuracy (acc), sensitivity (sen), speciﬁcity (spe), and the area\n",
      "under the receiver operating characteristic (roc) curve (auc) as evaluation metrics.\n",
      "we\n",
      "also conduct analysis including standard deviations and 95% ci, and delong statistical\n",
      "test for further auc comparison.\n",
      "for preprocessing, given that the ct data from nsclc-tcia has an in-plane reso-\n",
      "lution of 1 mm × 1 mm and a slice thickness of 0.7–3.0 mm, we resample the ct images\n",
      "using trilinear interpolation to a common resolution of 1mm × 1mm × 1mm.\n",
      "finally following [7], we clip the intensities of the input patches to the interval\n",
      "(−1000, 400 hounsﬁeld unit) and normalize them to the range of [0, 1].\n",
      "3.2\n",
      "implementation details\n",
      "the implementation of carl is carried out using pytorch and run on a worksta-\n",
      "tion equipped with nvidia geforce rtx 2080ti gpus and intel xeon cpu 4110 @\n",
      "2.10ghz.\n",
      "we use pub-\n",
      "licly available codes of these comparison methods and implement models for methods\n",
      "without code.\n",
      "the experimental results are reported in table 1.\n",
      "2(a) is also closer to the upper-left corner, further indicating\n",
      "its superior performance.\n",
      "besides, carl-b3 and carl-b4 show better performance than\n",
      "carl-b0, illustrating view-speciﬁc representations as a complement which can also\n",
      "contribute to subtype classiﬁcation.\n",
      "though single loss already contributes to perfor-\n",
      "mance improvement, carl-b5 to carl-b7 demonstrate that the combinations of\n",
      "different losses can further enhance classiﬁcation results.\n",
      "more importantly, carl with\n",
      "all losses achieves the best performance among all methods, demonstrating that our pro-\n",
      "posed method effectively reduces multi-view discrepancies and signiﬁcantly improves\n",
      "the performance of histological subtype classiﬁcation by providing a holistic and disen-\n",
      "tangled perspective of the multi-view ct images.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf:\n",
      "the recent surge of foundation models in computer vision\n",
      "and natural language processing opens up perspectives in utilizing multi-\n",
      "modal clinical data to train large models with strong generalizability.\n",
      "yet pathological image datasets often lack biomedical text annotation\n",
      "and enrichment.\n",
      "guiding data-eﬃcient image diagnosis from the use of\n",
      "biomedical text knowledge becomes a substantial interest.\n",
      "in this paper,\n",
      "we propose to connect image and text embeddings (cite) to enhance\n",
      "pathological image classiﬁcation.\n",
      "cite injects text insights gained from\n",
      "language models pre-trained with a broad range of biomedical texts,\n",
      "leading to adapt foundation models towards pathological image under-\n",
      "standing.\n",
      "through extensive experiments on the patchgastric stomach\n",
      "tumor pathological image dataset, we demonstrate that cite achieves\n",
      "leading performance compared with various baselines especially when\n",
      "training data is scarce.\n",
      "cite oﬀers insights into leveraging in-domain\n",
      "text knowledge to reinforce data-eﬃcient pathological image classiﬁca-\n",
      "tion.\n",
      "keywords: foundation models · multi-modality · model adaptation ·\n",
      "pathological image classiﬁcation\n",
      "1\n",
      "introduction\n",
      "deep learning for medical imaging has achieved remarkable progress, leading to\n",
      "a growing body of parameter-tuning strategies [1–3].\n",
      "in parallel, foundation models [4] have surged in computer vision [5,6]\n",
      "and natural language processing [7,8] with growing model capacity and data\n",
      "size, opening up perspectives in utilizing foundation models and large-scale clin-\n",
      "ical data for diagnostic tasks.\n",
      "given\n",
      "the complex tissue characteristics of pathological whole slide images (wsi), it\n",
      "is crucial to develop adaptation strategies allowing (1) training data eﬃciency,\n",
      "and (2) data fusion ﬂexibility for pathological image analysis.\n",
      "https://doi.org/10.1007/978-3-031-43904-9_27\n",
      "text-guided foundation model adaptation for medical image classiﬁcation\n",
      "273\n",
      "fig.\n",
      "1. connecting image and text embeddings.\n",
      "an image with the visual prompt is processed through a\n",
      "vision encoder and a projection layer.\n",
      "classiﬁcation prediction is made\n",
      "by the similarity between image and text embeddings.\n",
      "although foundation models promise a strong generalization ability [4], there\n",
      "is an inherent domain shift between medical and natural concepts in both vision\n",
      "and language modalities.\n",
      "pre-trained biomedical language models are increas-\n",
      "ingly applied to medical context understanding [9–11].\n",
      "language models prove\n",
      "to be eﬀective in capturing semantic characteristics with a lower data acquisition\n",
      "and annotation cost in medical areas [12].\n",
      "in addition, vision-language models demonstrate the importance of joining\n",
      "multi-modal information for learning strong encoders\n",
      "thus, connecting\n",
      "visual representations with text information from biomedical language models\n",
      "becomes increasingly critical to adapting foundation models for medical image\n",
      "classiﬁcation, particularly in the challenging setting of data deﬁciency.\n",
      "in this study, we propose cite, a data-eﬃcient adaptation framework that\n",
      "connects image and text embeddings from foundation models to perform\n",
      "pathological image classiﬁcation with limited training samples (see fig. 1).\n",
      "to\n",
      "enable language comprehension, cite makes use of large language models pre-\n",
      "trained on biomedical text datasets [10,11] with rich and professional biomedical\n",
      "knowledge.\n",
      "we demonstrate the usefulness of injecting biomedical text knowledge into\n",
      "foundation model adaptation for improved pathological image classiﬁcation.\n",
      "3. cite is simple yet eﬀective that outperforms supervised learning, visual\n",
      "prompt tuning, and few-shot baselines by a remarkable margin, especially\n",
      "under the data deﬁciency with limited amounts of training image samples\n",
      "(e.g., using only 1 to 16 slides per class).\n",
      "2\n",
      "related work\n",
      "medical image classiﬁcation.\n",
      "deep learning for medical image classiﬁcation\n",
      "has long relied on training large models from scratch [1,15].\n",
      "also, ﬁne-tuning\n",
      "or linear-probing the pre-trained models obtained from natural images [16–18]\n",
      "is reasonable.\n",
      "in addition, task-speciﬁc models do\n",
      "not generalize well with diﬀerent image modalities [2].\n",
      "to tackle this issue, we\n",
      "emphasize the adaptation of foundation models in a data-eﬃcient manner.\n",
      "vision-language pre-training.\n",
      "recent work has made eﬀorts in pre-training\n",
      "vision-language models.\n",
      "clip [5] collects 400 million image-text pairs from the\n",
      "internet and trains aligned vision and text encoders from scratch.\n",
      "however, those methods establish vision-language alignment by pre-training on\n",
      "large-scale image-text pairs.\n",
      "prompt tuning proves to be an\n",
      "eﬃcient adaptation method for both vision and language models [22,23]. orig-\n",
      "inating from natural language processing, “prompting” refers to adding (man-\n",
      "ual) text instructions to model inputs, whose goal is to help the pre-trained\n",
      "model better understand the current task.\n",
      "for instance, coop [22] introduces\n",
      "learnable prompt parameters to the text branch of vision-language models.\n",
      "to address this challenge,\n",
      "we leverage large language models pre-trained with biomedical text to inject\n",
      "medical domain knowledge.\n",
      "biomedical language model utilization.\n",
      "leveraging language\n",
      "models pre-trained with biomedical text for medical language tasks is a common\n",
      "application.\n",
      "[9] pre-train a clinical text model with\n",
      "biobert [10] initialization and show a signiﬁcant improvement on ﬁve clinical\n",
      "text-guided foundation model adaptation for medical image classiﬁcation\n",
      "(a) the pathological images are cut into patches.\n",
      "(b)\n",
      "the class token, image tokens, and learnable prompt tokens are concatenated.\n",
      "(c) the\n",
      "tokens are processed by a pre-trained vision transformer to generate image embeddings.\n",
      "(d) the image is recognized as\n",
      "the class with maximum cosine similarity between image and text embeddings.\n",
      "(e) the\n",
      "class names are processed by a biomedical language model to generate text embeddings.\n",
      "language tasks.\n",
      "in our eﬀorts, we\n",
      "emphasize the importance of utilizing biomedical language models for adapting\n",
      "foundational vision models into cancer pathological analysis.\n",
      "3\n",
      "methodology\n",
      "figure 2 depicts an overview of our approach cite for data-eﬃcient pathological\n",
      "image classiﬁcation.\n",
      "cite jointly understands the image features extracted by\n",
      "vision encoders pre-trained with natural imaging, and text insights encoded in\n",
      "large language models pre-trained with biomedical text (e.g., biolinkbert [11]\n",
      "which captures rich text insights spanning across biomedical papers via cita-\n",
      "tions).\n",
      "we connect text and imaging by a projection and classify the images by\n",
      "comparing the cosine similarity between image and text embeddings.\n",
      "they are (1) prompt tokens in the input space to model task-speciﬁc\n",
      "information, and (2) a projection layer in the latent space to align image and text\n",
      "embeddings.\n",
      "276\n",
      "y. zhang et al.\n",
      "3.1\n",
      "connecting text and imaging\n",
      "an image i to be classiﬁed is processed through a pre-trained vision encoder to\n",
      "generate the image embedding xv with dimension dv, where v stands for “vision”:\n",
      "xv = visionencoder(i)\n",
      "xv ∈ rdv.\n",
      "[1, c]) with a\n",
      "pre-trained biomedical language model instead of training a classiﬁcation head\n",
      "(see fig.\n",
      "we tokenize and process tc through the language encoder to\n",
      "generate the text embedding xc\n",
      "l with dimension dl, where l stands for “language”:\n",
      "xc\n",
      "l = languageencoder(tokenizer(tc))\n",
      "(2)\n",
      "vision-language models like clip [5] contain both a vision encoder and a\n",
      "language encoder, which provide well-aligned embeddings in the same feature\n",
      "space.\n",
      "in this case, prediction ˆy is obtained by applying softmax on scaled cosine\n",
      "similarities between the image and text embeddings (see fig.\n",
      "):\n",
      "p(ˆy = c|i) =\n",
      "exp(sim(xc\n",
      "l , xv)/τ)\n",
      "\u0002c\n",
      "c′=1 exp(sim(xc′\n",
      "l , xv)/τ)\n",
      ",\n",
      "(3)\n",
      "where sim(·, ·) refers to cosine similarity and τ is the temperature parameter.\n",
      "for irrelevant vision and language encoders, we introduce an extra projection\n",
      "layer to the end of the vision encoder to map the image embeddings to the same\n",
      "latent space as the text embeddings.\n",
      "3.2\n",
      "learning visual prompt\n",
      "medical concepts exhibit a great visual distribution shift from natural images,\n",
      "which becomes impractical for a ﬁxed vision encoder to capture task-speciﬁc\n",
      "information in few-shot scenarios.\n",
      "visual prompt tuning (vpt [23]) is a\n",
      "lightweight adaptation method that can alleviate such an inherent diﬀerence\n",
      "by only tuning prompt tokens added to the visual inputs of a ﬁxed vision trans-\n",
      "former [24], showing impressive performance especially under data deﬁciency.\n",
      "a vision transformer ﬁrst cuts the image into a sequence of n patches and\n",
      "projects them to patch embeddings e0 ∈ rn×dv, where dv represents the visual\n",
      "embedding dimension.\n",
      "cls embedding\n",
      "of the last layer output is the image feature xv.\n",
      ", pp] ∈ rp×dv,\n",
      "text-guided foundation model adaptation for medical image classiﬁcation\n",
      "277\n",
      "where p is the prompt length, with cls token c0 and patch embeddings e0 before\n",
      "they are processed through the ﬁrst transformer layer:\n",
      ", k\n",
      "xv = ck\n",
      "xv ∈ rdv,\n",
      "(5)\n",
      "where [·, ·] refers to concatenation along the sequence length dimension, and\n",
      "zk ∈ rp×dv represents the output embeddings of the k-th transformer layer\n",
      "at the position of the prompts (see fig.\n",
      "3.1.\n",
      "4\n",
      "experimental settings\n",
      "dataset.\n",
      "we adopt the patchgastric [25] dataset, which includes histopatho-\n",
      "logical image patches extracted from h&e stained whole slide images (wsi)\n",
      "of stomach adenocarcinoma endoscopic biopsy specimens.\n",
      "we randomly split the wsis into train (20%) and validation (80%) sub-\n",
      "sets for measuring the model performance.\n",
      "the evaluation metric is patient-wise accuracy, where the prediction of a wsi\n",
      "is obtained by a soft vote over the patches, and accuracy is averaged class-wise.\n",
      "implementation.\n",
      "[5] as the visual backbone, with\n",
      "input image size 224 × 224, patch size 16 × 16, and embedding dimension dv =\n",
      "512.\n",
      "we adopt biolinkbert-large [11] as the biomedical language model, with\n",
      "embedding dimension dl\n",
      "= 1, 024. to show the extensibility of our approach, we\n",
      "additionally test on vision encoders including imagenet-21k vit-b/16\n",
      "[6], and biomedical language model biobert-large [10].\n",
      "our implementation is based on clip1, huggingface2 and mmclassiﬁcation3.\n",
      "we resize the images to 224×224\n",
      "to ﬁt the model and follow the original data pipeline in patchgastric [25].\n",
      "a\n",
      "class-balanced sampling strategy is adopted by choosing one image from each\n",
      "class in turn.\n",
      "all our experiment results\n",
      "are averaged on 3 random seeds unless otherwise speciﬁed.\n",
      "[27] backbone pre-trained on imagenet-21k\n",
      "[5] backbone. averaged results and standard deviation (error\n",
      "bars) of 3 runs are displayed.\n",
      "our cite consistently outperforms all baselines under\n",
      "all data fractions, showing a remarkable improvement under data deﬁciency.\n",
      "[27] backbone pre-trained on imagenet-21k\n",
      "[18]: apply an attention network on image features to predict pseudo\n",
      "labels and cluster the images.\n",
      "(5) zero-shot [5]: classify images to the nearest text\n",
      "embeddings obtained by class names, without training.\n",
      "(6) few-shot [28]: cluster\n",
      "image features of the training data and classify images to the nearest class cen-\n",
      "ter.\n",
      "our\n",
      "cite outperforms all baselines that require training classiﬁcation heads, as well\n",
      "as image feature clustering methods, demonstrating the key beneﬁt of leveraging\n",
      "additional biomedical text information for pathological image classiﬁcation.\n",
      "cite shows a favorable improvement when data is scarce.\n",
      "when only\n",
      "one training slide per class is available, cite achieves a remarkable performance,\n",
      "outperforming all baselines by a signiﬁcant margin (from 51.4% to 60.2%).\n",
      "together, our ﬁndings\n",
      "text-guided foundation model adaptation for medical image classiﬁcation\n",
      "279\n",
      "table 1. ablation study of cite with and without prompt and text.\n",
      "we\n",
      "report the average accuracy and standard deviation.\n",
      "each component improves the performance.\n",
      "[5], imagenet-21k vit-b/16\n",
      "[11] language models.\n",
      "the highest performance of each visual encoder is bolded.\n",
      "60.2±1.2 59.1±1.2 60.3±0.8\n",
      "66.4±0.7 67.9±0.4 69.7±0.1\n",
      "in-21k vit-b/16\n",
      "linear\n",
      "-\n",
      "46.7±0.7\n",
      "45.8±1.6\n",
      "53.4±1.2\n",
      "59.5±0.5\n",
      "60.6±0.6\n",
      "66.5±0.8\n",
      "fine-tune -\n",
      "48.0±0.3\n",
      "49.6±0.1\n",
      "50.8±0.1\n",
      "59.3±0.3\n",
      "62.2±0.4\n",
      "66.3±0.2\n",
      "cite\n",
      "bb\n",
      "51.4±1.4\n",
      "51.8±1.3\n",
      "56.6±1.9\n",
      "62.7±1.0\n",
      "64.0±0.5\n",
      "67.2±1.4\n",
      "cite\n",
      "blb\n",
      "52.4±1.5 52.7±0.8 57.0±0.9 62.8±1.2 64.5±1.1 67.4±0.7\n",
      "intern vit-b/16 linear\n",
      "-\n",
      "47.3±0.2\n",
      "47.2±0.2\n",
      "52.4±0.5\n",
      "59.7±0.3\n",
      "63.1±0.2\n",
      "66.8±0.7\n",
      "fine-tune -\n",
      "42.0±0.3\n",
      "46.0±0.3\n",
      "51.0±0.9\n",
      "60.4±0.1\n",
      "62.7±0.5\n",
      "68.2±0.4\n",
      "cite\n",
      "bb\n",
      "51.7±0.1 55.4±1.8 59.6±0.3 66.4±0.8 68.1±0.8 69.7±0.7\n",
      "cite\n",
      "blb\n",
      "48.4±5.2\n",
      "49.1±5.5\n",
      "57.9±0.8\n",
      "65.3±0.4\n",
      "67.9±0.8\n",
      "69.4±0.9\n",
      "demonstrate that adding domain-speciﬁc text information provides an eﬃcient\n",
      "means to guide foundation model adaptation for pathological image diagnosis.\n",
      "we evaluate our approach with additional\n",
      "backbones and biomedical language models to assess its potential extensibility.\n",
      "the text information encoded in biomedical language models allows vision\n",
      "280\n",
      "y. zhang et al.\n",
      "models pre-trained with natural imaging to bridge the domain gap without task-\n",
      "speciﬁc pre-training on medical imaging.\n",
      "importantly, when using both the vision\n",
      "and language encoders of clip vit-b/16, our approach still outperforms the\n",
      "baselines by a remarkable margin (47.7% to 60.1%), demonstrating the impor-\n",
      "tance of multi-modal information.\n",
      "while clip gains such modality matching\n",
      "through pre-training, our cite shows an appealing trait that irrelevant vision\n",
      "and language models can be combined to exhibit similar multi-modal insights\n",
      "on pathological tasks without a need of joint pre-training.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf:\n",
      "such a mil-based\n",
      "wvad method leverages video-level annotations to detect frame-level\n",
      "diseases and shows promising results.\n",
      "the former is used to\n",
      "learn atoms for representing normal features, and the latter is used to\n",
      "encourage our model to gain robust disease detection.\n",
      "we demonstrate\n",
      "that our cfd network is achieving new sota performance on the exist-\n",
      "ing polyp dataset and the introduced panda-mil dataset.\n",
      "with the recent success of deep learning, researchers are\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9 25.\n",
      "as a result, the\n",
      "diﬃculty of data collection restricts the development of the supervised cad.\n",
      "due to the diﬃculty of acquiring the abundant annotated training data, the\n",
      "current sota method, i.e., csm [14], proposes a mil-based wvad manner to\n",
      "speciﬁcally tackle one speciﬁc disease detection task, i.e., colorectal cancer diag-\n",
      "nosis via colonoscopy.\n",
      "with the decoupled snippet-level feature ingredi-\n",
      "ents, our cfd employs both the normal and abnormal feature ingredients via a\n",
      "contrastive learning paradigm to concurrently optimize video-level and snippet-\n",
      "level disease scores for pursuing more accurate detection.\n",
      "to assess the proposed contrastive feature decoupling network, we conduct\n",
      "experiments on two datasets, i.e., polyp and panda-mil.\n",
      "the ablation study shows the decoupled\n",
      "diseased ingredients enable accurate disease detection, and the accompanied\n",
      "contrastive learning paradigm provides further improvement.\n",
      "– we demonstrate the generalization of cfd by achieving new sota perfor-\n",
      "mance on two biomedical imaging datasets concerning diﬀerent pathological\n",
      "modalities, i.e., the colonoscopy videos in polyp and prostate tissue biopsies\n",
      "in panda-mil.\n",
      "254\n",
      "j.-c.\n",
      "[8] established a large-scale attention-\n",
      "based database and designed a specialized model using retinal fundus images for\n",
      "detecting glaucoma.\n",
      "one popular branch is contrastive learning which shows a remarkable ability to\n",
      "obtain the desired semantic representation from various perspectives.\n",
      "our model contains an oﬄine trained memory\n",
      "bank to store feature atoms before the cfd training procedure, which associates\n",
      "a contrastive loss to boost the model performance using decoupled features per\n",
      "instance.\n",
      ", t\n",
      "is the number of instances, and c represents the instance-level feature dimension.\n",
      ",\n",
      "(4)\n",
      "where φd, g, and ψ are linear projections, global average pooling, and the multi-\n",
      "scale temporal network [13], respectively.\n",
      "a complete objec-\n",
      "tive should consider the symmetric form by switching d1 and d0 in (8).\n",
      "4\n",
      "experiments\n",
      "4.1\n",
      "dataset and metric\n",
      "we evaluate our model against sotas on the existing polyp [14] dataset and\n",
      "the panda-mil dataset introduced in this work.\n",
      "please refer to the\n",
      "supplementary material for the statistics of the two datasets.\n",
      "the prostate cancer grade assessment (panda) challenge\n",
      "[2] comprises over 10k whole-slide images (wsis) of digitized hematoxylin\n",
      "and eosin-stained biopsies originating from radboud university medical cen-\n",
      "ter and karolinska institute.\n",
      "we follow the previous methods [4,13,14] to employ the instant-level\n",
      "area under curve (auc) and the average precision (ap) for a fair comparison.\n",
      "the larger values of both metrics mean better disease detection performance.\n",
      "[14]\n",
      "miccai’22 98.41\n",
      "86.63\n",
      "76.52\n",
      "73.12\n",
      "cfd\n",
      "99.51 88.13 87.28 80.78\n",
      "4.2\n",
      "implementation details\n",
      "all the evaluated methods in the experiment used the same feature encoder,\n",
      "i.e., i3d\n",
      "precisely, our model achieves the new sota by 1.1%\n",
      "auc and 1.5% ap improvements on the polyp dataset and 1.09% auc and\n",
      "2.45% ap improvements on the panda-mil dataset.\n",
      "please refer to the sup-\n",
      "plementary material for the completed results, including more wvad methods\n",
      "[12,16,20,24].\n",
      "figure 2 visualizes one disease detection result of our cfd model on the\n",
      "panda-mil dataset.\n",
      "3.3, each loss function shows its improvement\n",
      "in our model performance.\n",
      "the contrastive loss contributes the most to auc\n",
      "improvement, enabling our model to achieve the sota performance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf:\n",
      "pancreatic ductal adenocarcinoma (pdac) is a highly lethal\n",
      "cancer in which the tumor-vascular involvement greatly aﬀects the\n",
      "resectability and, thus, overall survival of patients.\n",
      "this paper proposes a novel learnable neural distance that describes the\n",
      "precise relationship between the tumor and vessels in ct images of dif-\n",
      "ferent patients, adopting it as a major feature for prognosis prediction.\n",
      "besides, diﬀerent from existing models that used cnns or lstms to\n",
      "exploit tumor enhancement patterns on dynamic contrast-enhanced ct\n",
      "imaging, we improved the extraction of dynamic tumor-related texture\n",
      "features in multi-phase contrast-enhanced ct by fusing local and global\n",
      "features using cnn and transformer modules, further enhancing the fea-\n",
      "tures extracted across multi-phase ct images.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43904-9 24.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "keywords: pancreatic ductal adenocarcinoma (pdac) · survival\n",
      "prediction · texture-aware transformer · cross-attention · nerual\n",
      "distance\n",
      "1\n",
      "introduction\n",
      "pancreatic ductal adenocarcinoma (pdac) is one of the deadliest forms of\n",
      "human cancer, with a 5-year survival rate of only 9% [16].\n",
      "how-\n",
      "ever, current clinical markers such as larger tumor size and high carbohydrate\n",
      "antigen (ca) 19-9 level may not be suﬃcient to accurately tailor neoadjuvant\n",
      "treatment for patients [19].\n",
      "therefore, multi-phase contrast-enhanced ct has a\n",
      "great potential to enable personalized prognostic prediction for pdac, lever-\n",
      "aging its ability to provide a wealth of texture information that can aid in the\n",
      "development of accurate and eﬀective prognostic models [2,10].\n",
      "previous studies have utilized image texture analysis with hand-crafted fea-\n",
      "tures to predict the survival of patients with pdacs [1], but the representational\n",
      "fig.\n",
      "we deﬁne the surface-to-surface distance based on point-to-surface dis-\n",
      "tance (weighted-average of red lines from ♦ to △) instead of point-to-point distance (blue\n",
      "lines) to better capture the relationship between the tumor and the perivascular tissue.\n",
      "it is necessary to incor-\n",
      "porate tumor-vascular involvement into the feature extraction process of the\n",
      "prognostic model.\n",
      "[21,22], these methods may not be suﬃciently capable of capturing the\n",
      "complex dynamics between the tumor and its environment.\n",
      "furthermore, to capture the tumor enhancement patterns across multi-phase ct\n",
      "images, we are the ﬁrst to combine convolutional neural networks (cnn) and\n",
      "transformer [4] modules for extracting the dynamic texture patterns of pdac\n",
      "and its surroundings.\n",
      "this approach takes advantage of the visual transformer’s\n",
      "adeptness in capturing long-distance information compared to the cnn-only-\n",
      "based framework in the original approach.\n",
      "our proposed model has the\n",
      "potential to be used in combination with clinical factors for risk stratiﬁcation\n",
      "and treatment decisions for patients with pdac.\n",
      "the second component proposes a neural dis-\n",
      "tance metric between pdac and important vessels to assess their involvements.\n",
      "these\n",
      "blocks encode the input feature of an image fi\n",
      "the 3 × 3 × 3 convolution captures local spatial\n",
      "information, while the 1 × 1 × 1 convolution maps the input tensor to a higher-\n",
      "dimensional space (i.e., cl\n",
      "the cross-modality output fcross\n",
      "and in-modality output ft\n",
      "o are then concatenated and passed through an average\n",
      "pooling layer to obtain the ﬁnal output feature of the texture branch, denoted\n",
      "as ft ∈ rct.\n",
      "2.2\n",
      "neural distance: positional and structural information\n",
      "between pdac and vessels\n",
      "the vascular involvement in patients with pdac aﬀects the resectability and\n",
      "treatment planning [5].\n",
      "we used a semi-supervised\n",
      "nnunet model to segment pdac and the surrounding vessels, following recent\n",
      "work [11,21].\n",
      "we deﬁne a general distance between the surface boundaries of\n",
      "pdac (p) and the aforementioned four types of vessels (v) as d(v, p), which\n",
      "can be derived as follows:\n",
      "d(v, p) = dss(v, p)\n",
      "(5)\n",
      "neural distance allows for the ﬂexible assignment of weights to diﬀerent points\n",
      "and is able to ﬁnd positional information that is more suitable for pdac prog-\n",
      "nosis prediction.\n",
      "finally, we concatenate the features extracted from the two components and\n",
      "apply a fully-connected layer to predict the survival outcome, denoted as oos,\n",
      "which is a value between 0 and 1. to optimize the proposed model, we use the\n",
      "negative log partial likelihood as the survival loss [9].\n",
      "3\n",
      "experiments\n",
      "dataset.\n",
      "pdac masks for 340 patients were manually labeled by a radiol-\n",
      "ogist from shengjing hospital with 18 years of experience in pancreatic cancer,\n",
      "while the rest were predicted using self-learning models [11,24] and checked by\n",
      "the same annotator.\n",
      "other vessel masks were generated using the same semi-\n",
      "supervised segmentation models.\n",
      "implementation details: we used nested 5-fold cross-validation and aug-\n",
      "mented the training data by rotating volumetric tumors in the axial direction\n",
      "and randomly selecting cropped regions with random shifts.\n",
      "we also set the out-\n",
      "put feature dimensions to ct = 64 for the texture-aware transformer, cs = 64\n",
      "for the structure extraction and k = 32 for the neural distance.\n",
      "the batch size\n",
      "was 16 and the maximum iteration was set to 1000 epochs, and we selected\n",
      "the model with the best performance on the validation set during training for\n",
      "testing.\n",
      "we implemented our experiments using pytorch 1.11 and trained the\n",
      "models on a single nvidia 32g-v100 gpu.\n",
      "ablation study.\n",
      "we ﬁrst evaluated the performance of our proposed texture-\n",
      "aware transformer (tat) by comparing it with the resnet18 cnn backbone\n",
      "and vit transformer backbone, as shown in table 1.\n",
      "our model leverages the\n",
      "strengths of both local and global information in the pancreas and achieved the\n",
      "best result.\n",
      "next, we compared diﬀerent methods for multi-phase stages, includ-\n",
      "ing lstm, early fusion (fusion), and cross-attention (cross) in our method.\n",
      "to further evaluate the performance of our proposed model, we\n",
      "compared it with recent deep prediction methods [17,21] and report the results\n",
      "in table 2.\n",
      "our proposed method, which\n",
      "uses the transformer and structure-aware blocks to capture tumor enhancement\n",
      "248\n",
      "h. dong et al.\n",
      "table 1. ablation tests with diﬀerent network backbones including resnet18 (res),\n",
      "vit and texture-aware transformer (tat) and methods for multi-phases including\n",
      "lstm, early fusion (fusion) and cross-attention (cross).\n",
      "nested 5-fold cv (n = 892)\n",
      "independent test (n = 178)\n",
      "c-index\n",
      "auc\n",
      "c-index auc\n",
      "3dcnn-p [12]\n",
      "0.630 ± 0.009\n",
      "0.668 ± 0.019\n",
      "0.674\n",
      "0.740\n",
      "early fusion [17]\n",
      "0.635 ± 0.011\n",
      "0.670 ± 0.024\n",
      "0.696\n",
      "0.779\n",
      "deepct-pdac [21] 0.640 ± 0.018\n",
      "0.680 ± 0.036\n",
      "0.697\n",
      "0.773\n",
      "ours\n",
      "0.656 ± 0.017 0.695 ± 0.023 0.710\n",
      "0.792\n",
      "patterns and tumor-vascular involvement, demonstrated its eﬀectiveness with\n",
      "better performance in both nested 5-fold cross-validation and the multi-center\n",
      "independent test set.\n",
      "in table 3, we used univariate and multivariate cox proportional-hazards\n",
      "models to evaluate our signature and other clinicopathologic factors in the inde-\n",
      "pendent test set.\n",
      "the proposed risk stratiﬁcation was a signiﬁcant prognostic\n",
      "factor, along with other factors like pathological tnm stages.\n",
      "to demonstrate the added value of our\n",
      "signature as a tool to select patients for neoadjuvant treatment before surgery,\n",
      "we plotted kaplan-meier survival curves in fig.\n",
      "3. we further stratify patients by\n",
      "our signature after grouping them by tumor size and ca19-9, two clinically used\n",
      "preoperative criteria for selection, and also age.\n",
      "our signature could signiﬁcantly\n",
      "stratify patients in all cases and those in the high-risk group had worse outcomes\n",
      "and might be considered as potential neoadjuvant treatment candidates (e.g. 33\n",
      "high-risk patients with larger tumor size and high ca19-9).\n",
      "independent test set (n = 178)\n",
      "univariate analysis\n",
      "multivariate analysis\n",
      "hr (95% ci)\n",
      "p-value\n",
      "hr (95% ci)\n",
      "p-value\n",
      "proposed (high vs low risk)\n",
      "2.42(1.64-3.58)\n",
      "<0.0001\n",
      "1.85(1.08-3.17)\n",
      "0.027\n",
      "age (> 60 vs = 60)\n",
      "1.49(1.01-2.20)\n",
      "0.043\n",
      "1.01(0.65-1.58)\n",
      "0.888\n",
      "sex (male vs female)\n",
      "1.28(0.86-1.90)\n",
      "0.221\n",
      "-\n",
      "-\n",
      "pt (pt3-pt4 vs pt1-pt2)\n",
      "3.17(2.10-4.77)\n",
      "<0.0001\n",
      "2.44(1.54-3.86)\n",
      "0.00015\n",
      "pn (positive ve negative)\n",
      "1.47(0.98-2.20)\n",
      "0.008\n",
      "1.34(0.85-2.12)\n",
      "0.210\n",
      "resection margin (r1 vs r0)\n",
      "2.84(1.64-4.93)\n",
      "<0.0001\n",
      "1.68(0.92-3.07)\n",
      "0.091\n",
      "ca19-9 (> 210 vs ≤ 210 u/ml)\n",
      "0.94(0.64-1.39)\n",
      "0.759\n",
      "-\n",
      "-\n",
      "tumor size (> 25 vs ≤ 25 mm)\n",
      "2.36(1.59-3.52)\n",
      "<0.0001\n",
      "0.99(0.52-1.85)\n",
      "0.963\n",
      "tumor location (head vs tail)\n",
      "1.06(0.63-1.79)\n",
      "0.819\n",
      "-\n",
      "-\n",
      "fig.\n",
      "high risk group indicated by the proposed method is the potential patient group\n",
      "that could beneﬁt from neoadjuvant treatment before surgery.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf:\n",
      "our\n",
      "exhaustive study of state-of-the-art (sota) cnn image classiﬁcation\n",
      "models for this problem reveals that they often fail to learn the gouty\n",
      "mskus features, including the double contour sign, tophus, and snow-\n",
      "storm, which are essential for sonographers’ decisions.\n",
      "to address this\n",
      "issue, we establish a framework to adjust cnns to “think like sonog-\n",
      "raphers” for gout diagnosis, which consists of three novel components:\n",
      "(1) where to adjust: modeling sonographers’ gaze map to emphasize\n",
      "the region that needs adjust; (2) what to adjust: classifying instances\n",
      "to systematically detect predictions made based on unreasonable/biased\n",
      "reasoning and adjust; (3) how to adjust: developing a training mecha-\n",
      "nism to balance gout prediction accuracy and attention reasonability for\n",
      "improved cnns.\n",
      "the experimental results on clinical mskus datasets\n",
      "demonstrate the superiority of our method over several sota cnns.\n",
      "keywords: musculoskeletal ultrasound · gout diagnosis · gaze\n",
      "tracking · reasonability\n",
      "1\n",
      "introduction\n",
      "gout is the most common inﬂammatory arthritis and musculoskeletal ultrasound\n",
      "(mskus) scanning is recommended to diagnose gout due to the non-ionizing\n",
      "radiation, fast imaging speed, and non-invasive characteristics of mskus\n",
      "although\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 16.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp. 159–168, 2023.\n",
      "https://doi.org/10.1007/978-3-031-43987-2_16\n",
      "160\n",
      "z. cao et al.\n",
      "convolutional neural networks (cnns) based ultrasound classiﬁcation models\n",
      "have been successfully used for diseases such as thyroid nodules and breast can-\n",
      "cer, conspicuously absent from these successful applications is the use of cnns\n",
      "for gout diagnosis from mskus images.\n",
      "fig.\n",
      "1. (a) mskus images.\n",
      "in medical image analysis, recent works have attempted to inject the recorded\n",
      "gaze information of clinicians into deep cnn models for helping the models to\n",
      "predict correctly based on lesion area.\n",
      "[9,10] modeled the visual search\n",
      "behavior of radiologists for breast cancer using cnn and injected human visual\n",
      "attention into cnn to detect missing cancer in mammography.\n",
      "[15]\n",
      "demonstrated that the eye movement of radiologists can be a new supervision\n",
      "form to train the cnn model.\n",
      "[11] proposed the use\n",
      "of a teacher-student knowledge transfer framework for us image analysis, which\n",
      "combines doctor’s eye-gaze data with us images as input to a large teacher\n",
      "model, whose outputs and intermediate feature maps are used to condition a\n",
      "student model.\n",
      "although these methods have led to promising results, they can\n",
      "be diﬃcult to implement due to the need to collect doctors’ eye movement data\n",
      "for each image, along with certain restrictions on the network structure.\n",
      "(1)\n",
      "where to adjust: modeling sonographers’ gaze map to emphasize the region that\n",
      "needs adjust; (2) what to adjust: classify the instances to systemically detect\n",
      "predictions made based on unreasonable/biased reasoning and adjust; (3) how\n",
      "to adjust: developing a training mechanism to strike the balance between gout\n",
      "prediction accuracy and attention reasonability.\n",
      "2\n",
      "method\n",
      "fig.\n",
      "the mskus image i0 ∈ rh×w ×3 is ﬁrst input into cnn encoder that con-\n",
      "tains ﬁve convolution blocks.\n",
      "considering that ﬂatten operation leads to losing the\n",
      "spatial information, the absolute position encoding [14] is combined with the ﬂat-\n",
      "ten feature map via element-wise addition to form the input of the transformer\n",
      "layer.\n",
      "in the cnn decoder part, a pure cnn architecture progressively up-samples\n",
      "the feature maps into the original image resolution and implements pixel-wise\n",
      "prediction for modeling sonographers’ gaze map.\n",
      "in addition, the transformer’s out-\n",
      "put is fused with the feature map from the decoding process by an element-wise\n",
      "product operation to further enhance the long-range and multi-scale visual infor-\n",
      "mation.\n",
      "up: unreasonable precise: although the\n",
      "gout diagnosis is precise, amount of attention\n",
      "is given to irrelevant feature of mskus image.\n",
      "our target of adjustment is to reduce\n",
      "imprecise and unreasonable predictions.\n",
      "in\n",
      "this way, cnns not only ﬁnish correct gout\n",
      "diagnosis, but also acquire the attention\n",
      "region that agreements with the sonographers’\n",
      "gaze map.\n",
      "3\n",
      "experiments\n",
      "mskus dataset collection.\n",
      "informed written\n",
      "consent was obtained at the time of recruitment.\n",
      "dataset totally contains 1127\n",
      "us images from diﬀerent patients including 509 gout images and 618 healthy\n",
      "images.\n",
      "the resolution of the mskus images were resized to 224 × 224.\n",
      "during\n",
      "experiments, we randomly divided 10% of the dataset into testing sets, then\n",
      "the remaining data was divided equally into two parts for the diﬀerent phases\n",
      "of the training.\n",
      "we collected the eye movement data with the tobii 4c\n",
      "eye-tracker operating at 90 hz.\n",
      "the mskus images were displayed on a 1920 ×\n",
      "1080 27-inch lcd screen.\n",
      "binary maps\n",
      "of the same size as the corresponding mskus images were generated using the\n",
      "gaze data, with the pixel corresponding to the point of gaze marked with a’1’\n",
      "and the other pixels marked with a’0’.\n",
      "a sonographer gaze map s was generated\n",
      "for each binary map by convolving it with a truncated gaussian kernel g(σx,y),\n",
      "where g has 299 pixels along x dimension, and 119 pixels along y dimension.\n",
      "five metrics were used to evaluate model performance:\n",
      "accuracy (acc), area under curve (auc), correlation coeﬃcient (cc), sim-\n",
      "ilarity (sim) and kullback-leibler divergence (kld)\n",
      "acc and auc were\n",
      "implemented to assess the gout classiﬁcation performance of each model, while\n",
      "cc, sim, and kld were used to evaluate the similarity of the areas that the\n",
      "model and sonographers focus on during diagnoses.\n",
      "the results, shown in table 1,\n",
      "revealed that using our tls mechanism led to a signiﬁcant improvement in all\n",
      "metrics.\n",
      "resnet34 with tls acquired\n",
      "the highest improvement in acc with a 4.41% increase, and resnet18 with\n",
      "tls had a 0.027 boost in auc.\n",
      "our tls mechanism consistently performed\n",
      "well in improving the gout classiﬁcation performance of the cnn models.\n",
      "the performances of models training wi/wo our mechanism in mskus.\n",
      "consequently, it was possible to\n",
      "use predicted gaze maps for both the training and testing phases of the classiﬁca-\n",
      "tion models without any notable performance decrease.\n",
      "this removed the need to\n",
      "collect eye movement maps during the training and testing phases, signiﬁcantly\n",
      "lightening the workload of data collection.\n",
      "therefore, our tls mechanism, which\n",
      "involved predicting the gaze maps, could potentially be used in clinical environ-\n",
      "ments.\n",
      "this would allow us to bypass the need to collect the real gaze maps of\n",
      "the doctors while classifying newly acquired us images, and thus improved the\n",
      "clinical implications of our mechanism, “thinking like sonographers”.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf:\n",
      "dynamic contrast-enhanced ultrasound (ceus) video with\n",
      "microbubble contrast agents reﬂects the microvessel distribution and\n",
      "dynamic microvessel perfusion, and may provide more discriminative\n",
      "information than conventional gray ultrasound (us).\n",
      "in this paper, we propose a novel framework to diagnose\n",
      "thyroid nodules based on dynamic ceus video by considering microves-\n",
      "sel inﬁltration and via segmented conﬁdence mapping assists diagnosis.\n",
      "speciﬁcally, the temporal projection attention (tpa) is proposed to\n",
      "complement and interact with the semantic information of microvessel\n",
      "perfusion from the time dimension of dynamic ceus.\n",
      "the experimental results on clinical\n",
      "ceus video data indicate that our approach can attain an diagnostic\n",
      "accuracy of 88.79% for thyroid nodule and perform better than conven-\n",
      "tional methods.\n",
      "in addition, we also achieve an optimal dice of 85.54%\n",
      "compared to other classical segmentation methods.\n",
      "therefore, consid-\n",
      "eration of dynamic microvessel perfusion and inﬁltrative expansion is\n",
      "helpful for ceus-based diagnosis and segmentation of thyroid nodules.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 17.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "in practice, ceus video allows the\n",
      "dynamic observation of microvascular perfusion through intravenous injection of\n",
      "contrast agents.\n",
      "many microvessels around nodules are constantly inﬁltrating and growing into\n",
      "the surrounding tissue.\n",
      "ˆpn} represents the inﬁltration pro-\n",
      "cess of microvessels from gray us to ceus, and ˆpn is the ﬁnal segmentation result.\n",
      "used the spatial feature enhancement for disease diagnosis based on dynamic\n",
      "ceus.\n",
      "even the sota segmentation methods cannot accurately identify the lesion\n",
      "area for blurred lesion boundaries, thus, the existing automatic diagnosis network\n",
      "using ceus still requires manual labeling of pixel-level labels which will lose key\n",
      "information around the tissues\n",
      "whether the awareness of inﬁltrative area information can\n",
      "be helpful in the improvement of diagnostic accuracy is still unexplored.\n",
      "first, the temporal projection\n",
      "attention (tpa) is proposed to complement and interact with the semantic\n",
      "information of microvessel perfusion from the time dimension.\n",
      "the tasks of\n",
      "lesion area recognition and diﬀerential diagnosis are pixel-level and image-level\n",
      "classiﬁcations, and some low-level features of these two tasks can be shared inter-\n",
      "actively [8].\n",
      "after that,\n",
      "in the temporal-based lesions area recognition (tlar) module, an enhanced\n",
      "v-net with the tpa is implemented to identify the relatively clear lesion area\n",
      "which are visible on both gray us and ceus video.\n",
      "2.1\n",
      "temporal-based lesions area recognition (tlar)\n",
      "the great challenge of automatic recognition of lesion area from ceus video is\n",
      "that the semantic information of the lesion area is diﬀerent in the ceus video of\n",
      "the diﬀerent microvessel perfusion periods.\n",
      "especially in the perfusion period and\n",
      "the regression period, the semantic information of lesions cannot be fully depicted\n",
      "in an isolated ceus frame.\n",
      "thus, the interactive fusion of semantic information\n",
      "of the whole microvessel perfusion period will promote the identiﬁcation of the\n",
      "lesion area, and we design the temporal projection attention (tpa) to realize\n",
      "this idea.\n",
      "given\n",
      "a\n",
      "feature\n",
      "f4th\n",
      "∈\n",
      "rc×t × h\n",
      "16 × w\n",
      "16 after four down-sampling operations in encoder, its original 3d fea-\n",
      "ture map is projected [11] to 2d plane to get keys and queries: k, q ∈ rc× h\n",
      "16 × w\n",
      "16 ,\n",
      "and we use global average pooling (gap) and global maximum pooling (gmp)\n",
      "as temporal projection operations.\n",
      "then, we concatenate l with q to fur-\n",
      "ther obtain the global attention g ∈ rc×1× h\n",
      "16 × w\n",
      "16 by two consecutive 1 × 1 2d\n",
      "convolutions and dimension expend.\n",
      "then, we use\n",
      "parallel average pooling and full connection operation to reweight the channel\n",
      "information of f ′′\n",
      "4th to obtain the reweighted feature f ′\n",
      "4th ∈ rc×t × h\n",
      "16 × w\n",
      "16 .\n",
      "based on the fact that\n",
      "inceptext [15] has experimentally demonstrated that asymmetric convolution\n",
      "can eﬀectively solve the problem of highly variable size and aspect ratio, we use\n",
      "asymmetric convolution in the ipo unit.\n",
      "thus, we\n",
      "design a task focus loss lta to generate conﬁdence maps p, as follows:\n",
      "lmse =\n",
      "n\n",
      "\u0006\n",
      "i=1\n",
      "1\n",
      "ω\n",
      "\u0006\n",
      "p∈ω\n",
      "∥gi(pi), ˆpi(pi)∥2\n",
      "(8)\n",
      "thyroid nodule diagnosis in dynamic ceus via mia\n",
      "175\n",
      "lta =\n",
      "1\n",
      "2σ2\n",
      "n\n",
      "\u0006\n",
      "i=1\n",
      "∥pi − \u0007pi∥2\n",
      "2 + log σ\n",
      "(9)\n",
      "where gi is the label of ˆpi, which is generated by the operation of saf(d(i,j), αi);\n",
      "pi denotes pixel in the image domain ω, σ is a learnable parameter to eliminate\n",
      "the hidden uncertainty information.\n",
      "as\n",
      "the weight parameter, we set λ1, λ2, λ3 are 0.5,0.2,0.3 in the experiments.\n",
      "3\n",
      "experiments\n",
      "dataset.\n",
      "on the one hand, the percutaneous biopsy based pathological\n",
      "examination was implemented to determine the ground-truth of malignant and\n",
      "benign.\n",
      "on the other hand, a sonographer with more than 10 years of experience\n",
      "manually annotated the nodule lesion mask to obtain the pixel-level ground-\n",
      "truth of thyroid nodules segmentation.\n",
      "all data were approved by the institu-\n",
      "tional review board of nanjing drum tower hospital, and all patients signed\n",
      "the informed consent before enrollment into the study.\n",
      "implementation details.\n",
      "our network was implemented using pytorch frame-\n",
      "work with the single 12 gb gpu of nvidia rtx 3060.\n",
      "in addition, we carried out data augmentation,\n",
      "including random rotation and cropping, and we resize the resolution of input\n",
      "table 1.\n",
      "quantitative lesion recognition results are compared with sota methods\n",
      "and ablation experiments.\n",
      "three indexes including dice, recall, and iou, were used to evaluate\n",
      "the lesion recognition task, while ﬁve indexes, namely average accuracy (acc),\n",
      "sensitivity (se), speciﬁcity (sp), f1-score (f1), and auc, were used to evaluate\n",
      "the diagnosis task.\n",
      "experimental results.\n",
      "table 1 revealed that the modules (tpa, saf, and ipo)\n",
      "used in the network greatly improved the segmentation performance compared\n",
      "to baseline, increasing dice and recall scores by 7.60% and 7.23%, respectively.\n",
      "quantitative diagnostic results are compared with sota methods and abla-\n",
      "tion experiments.\n",
      "for fair comparison, all methods used the manually\n",
      "annotated lesion mask to assist the diagnosis.\n",
      "experimental results in table 2\n",
      "revealed that our baseline network could be useful for the diagnosis.\n",
      "figure 3 (c) showed that the diagnosis accuracy\n",
      "increased along with the increment of α and then tended to become stable when\n",
      "α was close to 9.\n",
      "therefore, for balancing the eﬃciency and performance, the\n",
      "number of ipo was set as n = 3 and α was set as α = {1, 5, 9} to generate a\n",
      "group of conﬁdence maps that can simulate the process of microvessel inﬁltration.\n",
      "a4 of the supplementary\n",
      "material.)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf:\n",
      "survival prediction is crucial for cancer patients as it provides early\n",
      "prognostic information for treatment planning.\n",
      "recently, deep survival models\n",
      "based on deep learning and medical images have shown promising performance\n",
      "for survival prediction.\n",
      "however, existing deep survival models are not well devel-\n",
      "oped in utilizing multi-modality images (e.g., pet-ct) and in extracting region-\n",
      "speciﬁc information (e.g., the prognostic information in primary tumor (pt) and\n",
      "metastatic lymph node (mln) regions).\n",
      "in view of this, we propose a merging-\n",
      "diverging learning framework for survival prediction from multi-modality images.\n",
      "our framework\n",
      "is demonstrated on survival prediction from pet-ct images in head and neck\n",
      "(h&n) cancer, by designing an x-shape merging-diverging hybrid transformer\n",
      "network (named xsurv).\n",
      "our xsurv combines the complementary information\n",
      "in pet and ct images and extracts the region-speciﬁc prognostic information\n",
      "in pt and mln regions.\n",
      "extensive experiments on the public dataset of head\n",
      "and neck tumor segmentation and outcome prediction challenge (hecktor\n",
      "2022) demonstrate that our xsurv outperforms state-of-the-art survival prediction\n",
      "methods.\n",
      "keywords: survival prediction · transformer · head and neck cancer\n",
      "supplementary information the online version contains supplementary material available at\n",
      "https://doi.org/10.1007/978-3-031-43987-2_39.\n",
      "https://doi.org/10.1007/978-3-031-43987-2_39\n",
      "merging-diverging hybrid transformer networks\n",
      "401\n",
      "1\n",
      "introduction\n",
      "head and neck (h&n) cancer refers to malignant tumors in h&n regions, which is\n",
      "among the most common cancers worldwide [1]. survival prediction, a regression task\n",
      "that models the survival outcomes of patients, is crucial for h&n cancer patients: it pro-\n",
      "vides early prognostic information to guide treatment planning and potentially improves\n",
      "the overall survival outcomes of patients [2].\n",
      "therefore, survival prediction from pet-ct images in h&n cancer has\n",
      "attracted wide attention and serves as a key research area.\n",
      "for instance, head and neck\n",
      "tumor segmentation and outcome prediction challenges (hecktor) have been held\n",
      "for the last three years to facilitate the development of new algorithms for survival\n",
      "prediction from pet-ct images in h&n cancer\n",
      "[5–7].\n",
      "traditional survival prediction methods are usually based on radiomics [8], where\n",
      "handcrafted radiomics features are extracted from pre-segmented tumor regions and\n",
      "then are modeled by statistical survival models, such as the cox proportional hazard\n",
      "(coxph) model\n",
      "in addition, deep survival models based on deep learning have\n",
      "been proposed to perform end-to-end survival prediction from medical images, where\n",
      "pre-segmented tumor masks are often unrequired [10].\n",
      "deep survival models usually\n",
      "adopt convolutional neural networks (cnns) to extract image features, and recently\n",
      "visual transformers (vit) have been adopted for its capabilities to capture long-range\n",
      "dependency within images [11, 12].\n",
      "for survival prediction in\n",
      "h&n cancer, deep survival models have achieved top performance in the hecktor\n",
      "2021/2022 and are regarded as state-of-the-art\n",
      "firstly, existing deep survival models are underdeveloped in utilizing complemen-\n",
      "tary multi-modality information, such as the metabolic and anatomical information in\n",
      "pet and ct images.\n",
      "[17, 18] or rely on early fusion (i.e., concatenating\n",
      "multi-modality images as multi-channel inputs) to combine multi-modality informa-\n",
      "tion [11, 14–16, 19].\n",
      "however, early fusion has difﬁculties in extracting intra-modality information due to\n",
      "entangled (concatenated) images for feature extraction, while late fusion has difﬁcul-\n",
      "ties in extracting inter-modality information due to fully independent feature extraction.\n",
      "however, the performance of this method heavily relies on using tumor\n",
      "segmentation masks as inputs, which limits its generalizability.\n",
      "secondly, although deep survival models have advantages in performing end-to-end\n",
      "survival prediction without requiring tumor masks, this also incurs difﬁculties in extract-\n",
      "ing region-speciﬁc information, such as the prognostic information in primary tumor\n",
      "402\n",
      "m. meng et al.\n",
      "(pt) and metastatic lymph node (mln) regions.\n",
      "to address this limitation, recent deep\n",
      "survival models adopted multi-task learning for joint tumor segmentation and survival\n",
      "prediction, to implicitly guide the model to extract features related to tumor regions\n",
      "however, most of them only considered pt segmentation and ignored the\n",
      "prognostic information in mln regions [11, 24–26].\n",
      "meng et al.\n",
      "[16] performed survival\n",
      "prediction with joint pt-mln segmentation and achieved one of the top performances\n",
      "in hecktor 2022.\n",
      "our xsurv has a merg-\n",
      "ing encoder to fuse complementary anatomical and metabolic information in pet and\n",
      "ct images and has a diverging decoder to extract region-speciﬁc prognostic informa-\n",
      "tion in pt and mln regions.\n",
      "this\n",
      "framework is specialized in leveraging multi-modality images and extracting region-\n",
      "speciﬁc information, which potentially could be applied to many survival prediction\n",
      "tasks with multi-modality imaging.\n",
      "extensive experiments on the public dataset of hecktor 2022\n",
      "our xsurv performs joint survival prediction and segmentation,\n",
      "where the two decoder branches are trained to perform pt/mln segmentation and\n",
      "provide pt-/mln-related deep features for survival prediction (refer to sect.\n",
      "our\n",
      "xsurv also can be enhanced by leveraging the radiomics features extracted from the\n",
      "xsurv-segmented pt/mln regions (refer to sect.\n",
      "our implementation is provided\n",
      "at https://github.com/mungomeng/survival-xsurv.\n",
      "2.1\n",
      "pet-ct merging encoder\n",
      "assuming nconv, nself , and ncross are three architecture parameters, each encoder branch\n",
      "consists of nconv conv blocks, nself hybrid parallel self-attention (hpsa) blocks, and\n",
      "ncross hpca blocks.\n",
      "in this study, we set nconv,\n",
      "nself , and ncross as 1, 1, and 3, as this setting achieved the best validation results (refer\n",
      "to the supplementary materials).\n",
      "other architecture details are also presented in the\n",
      "supplementary materials.\n",
      "the idea of adopting convolutions and transformers in parallel has been explored\n",
      "for segmentation [28], which suggests that parallelly aggregating global and local infor-\n",
      "mation is beneﬁcial for feature learning.\n",
      "404\n",
      "m. meng et al.\n",
      "fig.\n",
      "different from the vanilla attention gate (ag) block\n",
      "[29], rag blocks leverage the gating signals from two decoder branches and generate\n",
      "mutually exclusive (softmax-activated) attention maps.\n",
      "the output of the last conv block in the pt/mln branch is fed into a segmentation\n",
      "head, which generates pt/mln segmentation masks using a sigmoid-activated 1 × 1\n",
      "× 1 convolutional layer.\n",
      "2.3\n",
      "multi-task learning\n",
      "following existing multi-task deep survival models [11, 16, 24–26], our xsurv is end-\n",
      "to-end trained for survival prediction and pt-mln segmentation using a combined loss:\n",
      "l = lsurv+λ(lpt +lmln),wheretheλisaparametertobalancethesurvivalprediction\n",
      "term lsurv and the pt/mln segmentation terms lpt/mln.\n",
      "the loss functions are detailed in the supplementary\n",
      "materials.\n",
      "the λ is set as 1 in the experiments as default.\n",
      "merging-diverging hybrid transformer networks\n",
      "405\n",
      "2.4\n",
      "radiomics enhancement\n",
      "our xsurv also can be enhanced by leveraging radiomics features (denoted as radio-\n",
      "xsurv).\n",
      "following [16], radiomics features are extracted from the xsurv-segmented\n",
      "pt/mln regions via pyradiomics\n",
      "[33] and selected by least absolute shrinkage and\n",
      "selection operator (lasso) regression.\n",
      "the process of radiomics feature extraction is\n",
      "providedinthesupplementarymaterials.then,acoxphmodel[9]isadoptedtointegrate\n",
      "the selected radiomics features and the xsurv-predicted survival score to make the ﬁnal\n",
      "prediction.\n",
      "in addition, clinical indicators (e.g., age, gender) also can be integrated by\n",
      "the coxph model.\n",
      "3\n",
      "experimental setup\n",
      "3.1\n",
      "dataset and preprocessing\n",
      "we adopted the training dataset of hecktor 2022 (refer to https://hecktor.grand-cha\n",
      "llenge.org/), including 488 h&n cancer patients acquired from seven medical centers\n",
      "each patient underwent pretreatment pet/ct and has clinical indicators.\n",
      "we present\n",
      "the distributions of all clinical indicators in the supplementary materials.\n",
      "recurrence-\n",
      "free survival (rfs), including time-to-event in days and censored-or-not status, was\n",
      "provided as ground truth for survival prediction, while pt and mln annotations were\n",
      "provided for segmentation.\n",
      "we resampled pet-ct images into isotropic voxels where 1 voxel corresponds to\n",
      "1 mm3.\n",
      "each image was cropped to 160 × 160 × 160 voxels with the tumor located\n",
      "in the center.\n",
      "pet images were standardized using z-score normalization, while ct\n",
      "images were clipped to [−1024, 1024] and then mapped to [−1, 1].\n",
      "3.2\n",
      "implementation details\n",
      "we implemented our xsurv using pytorch on a 12 gb geforce gtx titan x gpu.\n",
      "data augmentation was applied in real-time\n",
      "during training to minimize overﬁtting, including random afﬁne transformations and\n",
      "random cropping to 112 × 112 × 112 voxels.\n",
      "in our experiments, one training iteration (including data augmentation) took roughly\n",
      "4.2 s, and one inference iteration took roughly 0.61 s.\n",
      "406\n",
      "m. meng et al.\n",
      "3.3\n",
      "experimental settings\n",
      "we compared our xsurv to six state-of-the-art survival prediction methods, includ-\n",
      "ing two traditional radiomics-based methods and four deep survival models.\n",
      "[14], transformer-based multimodal\n",
      "networks for segmentation and survival prediction (tmss)\n",
      "deepmtlr-coxph, icare, and radio-deepmts achieved top performance in\n",
      "hecktor 2021 and 2022.\n",
      "for a fair comparison, all methods took the same pre-\n",
      "processed images and clinical indicators as inputs.\n",
      "survival prediction and segmenta-\n",
      "tion were evaluated using concordance index (c-index) and dice similarity coefﬁcient\n",
      "(dsc), which are the standard evaluation metrics in the challenges [6, 7, 35].\n",
      "we also performed two ablation studies on the encoder and decoder separately: (i)\n",
      "we replaced hpca/hpsa blocks with conv blocks and compared different strategies to\n",
      "combinepet-ctimages.(ii)weremovedragblocksandcompareddifferentstrategies\n",
      "to extract pt/mln-related information.\n",
      "methods\n",
      "survival\n",
      "prediction\n",
      "(c-index)\n",
      "pt segmentation\n",
      "(dsc)\n",
      "mln\n",
      "segmentation\n",
      "(dsc)\n",
      "coxph\n",
      "our xsurv achieved a higher c-index than all compared methods, which demon-\n",
      "strates that our xsurv has achieved state-of-the-art performance in survival prediction\n",
      "of h&n cancer.\n",
      "when radiomics enhancement was adopted in xsurv and deepmts,\n",
      "our radio-xsurv also outperformed the radio-deepmts and achieved the highest c-\n",
      "index.\n",
      "moreover, the segmentation results of multi-task deep survival models (tmss,\n",
      "deepmts, and xsurv) are also reported in table 1.\n",
      "we attribute\n",
      "these performance improvements to the use of our proposed merging-diverging learning\n",
      "framework, hpca block, and rag block, which can be evidenced by ablation studies.\n",
      "[19]’s study, which suggests that\n",
      "early and late fusion cannot effectively leverage the complementary information in pet-\n",
      "ct images.\n",
      "as we have mentioned, early and late fusion have difﬁculties in extracting\n",
      "intra- and inter-modality information, respectively.\n",
      "our encoder ﬁrst adopts conv/hpsa\n",
      "blocks to extract intra-modality information and then leverages hpca blocks to discover\n",
      "their interactions, which achieved the highest c-index.\n",
      "for pt and mln segmentation,\n",
      "our encoder also achieved the highest dscs, which indicates that our encoder also can\n",
      "improve segmentation.\n",
      "[22] were compared and showed poor\n",
      "performance.\n",
      "this is likely attributed to the fact that leveraging non-local attention at\n",
      "multiple scales has corrupted local spatial information, which degraded the segmentation\n",
      "performanceanddistractedthemodelfromptandmlnregions.torelievethisproblem,\n",
      "in tang et al.’s study\n",
      "[22], tumor segmentation masks were fed into the model as explicit\n",
      "guidance to tumor regions.\n",
      "however, it is intractable to have segmentation masks at the\n",
      "inference stage in clinical practice.\n",
      "we\n",
      "found that, even without adopting ag, using a dual-branch decoder for pt and mln\n",
      "segmentation resulted in a higher c-index than using a single-branch decoder, which\n",
      "demonstratestheeffectivenessofourdivergingdecoderdesign.adoptingvanillaag[29]\n",
      "orraginthedual-branchdecoderfurtherimprovedsurvivalprediction.comparedtothe\n",
      "vanillaag,ourragcontributedtoalargerimprovement,andthisenabledourdecoderto\n",
      "achieve the highest c-index.\n",
      "in the supplementary materials, we visualized the attention\n",
      "maps produced by rag blocks, where the attention maps can precisely locate pt/mln\n",
      "regions and screen out pt-/mln-related features.\n",
      "for pt and mln segmentation, using\n",
      "a single-branch decoder for pt- or mln-only segmentation achieved the highest dscs.\n",
      "this is expected as the model can leverage all its capabilities to segment only one target.\n",
      "nevertheless, our decoder still achieved the second-best dscs in both pt and mln\n",
      "segmentation with a small gap.\n",
      "408\n",
      "m. meng et al.\n",
      "table 2. ablation study on the pet-ct merging encoder.\n",
      "methods\n",
      "survival\n",
      "prediction\n",
      "(c-index)\n",
      "pt segmentation\n",
      "(dsc)\n",
      "mln\n",
      "segmentation\n",
      "(dsc)\n",
      "sbe with ce =\n",
      "ce: the channel numbers or embedding dimensions used in the encoder.\n",
      "methods\n",
      "survival\n",
      "prediction\n",
      "(c-index)\n",
      "pt segmentation\n",
      "(dsc)\n",
      "mln\n",
      "segmentation\n",
      "(dsc)\n",
      "sbd with cd =\n",
      "[256, 128, 64, 32,\n",
      "16]\n",
      "only pt\n",
      "0.751\n",
      "0.803\n",
      "/\n",
      "only mln\n",
      "0.746\n",
      "/\n",
      "0.758\n",
      "pt and mln\n",
      "0.765\n",
      "0.790\n",
      "0.734\n",
      "dbd with cd =\n",
      "[128, 64, 32, 16, 8]\n",
      "no ag\n",
      "0.770\n",
      "0.792\n",
      "0.740\n",
      "vanilla ag\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf:\n",
      "in recent years, computational pathology has seen tremen-\n",
      "dous progress driven by deep learning methods in segmentation and\n",
      "classiﬁcation tasks aiding prognostic and diagnostic settings.\n",
      "nuclei seg-\n",
      "mentation, for instance, is an important task for diagnosing diﬀerent\n",
      "cancers.\n",
      "however, training deep learning models for nuclei segmentation\n",
      "requires large amounts of annotated data, which is expensive to col-\n",
      "lect and label.\n",
      "this necessitates explorations into generative modeling of\n",
      "histopathological images.\n",
      "in this work, we use recent advances in con-\n",
      "ditional diﬀusion modeling to formulate a ﬁrst-of-its-kind nuclei-aware\n",
      "semantic tissue generation framework (nasdm) which can synthesize\n",
      "realistic tissue samples given a semantic instance mask of up to six dif-\n",
      "ferent nuclei types, enabling pixel-perfect nuclei localization in generated\n",
      "samples.\n",
      "these synthetic images are useful in applications in pathology\n",
      "pedagogy, validation of models, and supplementation of existing nuclei\n",
      "segmentation datasets.\n",
      "we demonstrate that nasdm is able to syn-\n",
      "thesize high-quality histopathology images of the colon with superior\n",
      "quality and semantic controllability over existing generative methods.\n",
      "implementation: https://github.com/4m4n5/nasdm.\n",
      "segmentation models ﬁnd\n",
      "applications in spatial identiﬁcation of diﬀerent nuclei types\n",
      "https://doi.org/10.1007/978-3-031-43987-2_76\n",
      "nasdm: nuclei-aware histopathology image generation\n",
      "787\n",
      "be used to generate histopathology images with speciﬁc characteristics, such as\n",
      "visual patterns identifying rare cancer subtypes [4].\n",
      "as such, generative models\n",
      "can be sampled to emphasize each disease subtype equally and generate more\n",
      "balanced datasets, thus preventing dataset biases getting ampliﬁed by the mod-\n",
      "els [7].\n",
      "generative models have the potential to improve the pedagogy, trustwor-\n",
      "thiness, generalization, and coverage of disease diagnosis in the ﬁeld of histology\n",
      "by aiding both deep learning models and human pathologists.\n",
      "addition-\n",
      "ally, conditional generation of annotated data adds even further value to the\n",
      "proposition as labeling medical images involves tremendous time, labor, and\n",
      "training costs.\n",
      "[8]\n",
      "have achieved tremendous success in conditional and unconditional generation\n",
      "of real-world images [3].\n",
      "further, the semantic diﬀusion model (sdm) demon-\n",
      "strated the use of ddpms for generating images given semantic layout [27].\n",
      "in\n",
      "this work, (1) we leverage recently discovered capabilities of ddpms to design a\n",
      "ﬁrst-of-its-kind nuclei-aware semantic diﬀusion model (nasdm) that can gener-\n",
      "ate realistic tissue patches given a semantic mask comprising of multiple nuclei\n",
      "types, (2) we train our framework on the lizard dataset\n",
      "[5] consisting of colon\n",
      "histology images and achieve state-of-the-art generation capabilities, and (3) we\n",
      "perform extensive ablative, qualitative, and quantitative analyses to establish\n",
      "the proﬁciency of our framework on this tissue generation task.\n",
      "2\n",
      "related work\n",
      "deep learning based generative models for histopathology images have seen\n",
      "tremendous progress in recent years due to advances in digital pathology, com-\n",
      "pute power, and neural network architectures.\n",
      "more recently,\n",
      "denoising diﬀusion models have been shown to generate highly compelling images\n",
      "by incrementally adding information to noise [8].\n",
      "success of diﬀusion models in\n",
      "generating realistic images led to various conditional [12,21,22] and uncondi-\n",
      "tional [3,9,19] diﬀusion models that generate realistic samples with high ﬁdelity.\n",
      "semantic image synthesis is a task\n",
      "involving generating diverse realistic images from semantic layouts.\n",
      "gan-based\n",
      "semantic image synthesis works [20,24,25] generally struggled at generating high\n",
      "quality and enforcing semantic correspondence at the same time.\n",
      "to this end,\n",
      "a semantic diﬀusion model has been proposed that uses conditional denoising\n",
      "diﬀusion probabilistic model and achieves both better ﬁdelity and diversity [27].\n",
      "we use this progress in the ﬁeld of conditional diﬀusion models and semantic\n",
      "image synthesis to formulate our nasdm framework.\n",
      "788\n",
      "a. shrivastava and p. t. fletcher\n",
      "3\n",
      "method\n",
      "in this paper, we describe our framework for generating tissue patches con-\n",
      "ditioned on semantic layouts of nuclei.\n",
      "given a nuclei segmentation mask, we\n",
      "intend to generate realistic synthetic patches.\n",
      "in this section, we (1) describe our\n",
      "data preparation, (2) detail our stain-normalization strategy, (3) review condi-\n",
      "tional denoising diﬀusion probabilistic models, (4) outline the network architec-\n",
      "ture used to condition on semantic label map, and (5) highlight the classiﬁer-free\n",
      "guidance mechanism that we employ at sampling time.\n",
      "3.1\n",
      "data processing\n",
      "we use the lizard dataset\n",
      "this dataset con-\n",
      "sists of histology image regions of colon tissue from six diﬀerent data sources\n",
      "at 20× objective magniﬁcation.\n",
      "the images are accompanied by full segmenta-\n",
      "tion annotation for diﬀerent types of nuclei, namely, epithelial cells, connective\n",
      "tissue cells, lymphocytes, plasma cells, neutrophils, and eosinophils.\n",
      "a gener-\n",
      "ative model trained on this dataset can be used to eﬀectively synthesize the\n",
      "colonic tumor micro-environments.\n",
      "the dataset contains 238 image regions, with\n",
      "an average size of 1055 × 934 pixels.\n",
      "as there are substantial visual variations\n",
      "across images, we construct a representative test set by randomly sampling a\n",
      "7.5% area from each image and its corresponding mask to be held-out for test-\n",
      "ing.\n",
      "the test and train image regions are further divided into smaller image\n",
      "patches of 128 × 128 pixels at two diﬀerent objective magniﬁcations: (1) at 20×,\n",
      "the images are directly split into 128 × 128 pixels patches, whereas (2) at 10×,\n",
      "we generate 256 × 256 patches and resize them to 128 × 128 for training.\n",
      "as such, at (1) 20× we extract a total of 54,735 patches for training\n",
      "and 4,991 patches as a held-out set, while at (2) 20× magniﬁcation we generate\n",
      "12,409 training patches and 655 patches are held out.\n",
      "3.2\n",
      "stain normalization\n",
      "a common issue in deep learning with h&e stained histopathology slides is the\n",
      "visual bias introduced by variations in the staining protocol and the raw mate-\n",
      "rials of chemicals leading to diﬀerent colors across slides prepared at diﬀerent\n",
      "labs [1].\n",
      "a conditional\n",
      "nasdm: nuclei-aware histopathology image generation\n",
      "789\n",
      "fig.\n",
      "1. nasdm training framework: given a real image x0 and semantic mask y,\n",
      "we construct the conditioning signal by expanding the mask and adding an instance\n",
      "edge map.\n",
      "the corrupted image xt, timestep t, and semantic condition y are\n",
      "then fed into the denoising model which predicts ˆϵ as the amount of noise added to the\n",
      "model.\n",
      "the denoising neural network can be\n",
      "parameterized in several ways, however, it has been observed that using a noise-\n",
      "prediction based formulation results in the best image quality [8].\n",
      "overall, our\n",
      "nasdm denoising model is trained to predicting the noise added to the input\n",
      "image given the semantic layout y and the timestep t using the loss described\n",
      "as follows:\n",
      "lsimple = et,x,ϵ\n",
      "therefore, following the strategy in improved ddpms [8], we train a network to\n",
      "directly predict an interpolation coeﬃcient v per dimension, which is turned into\n",
      "variances and optimized directly using the kl divergence between the estimated\n",
      "distribution pθ(xt−1 | xt, y) and the diﬀusion posterior q(xt−1 | xt, x0) as lvlb =\n",
      "dkl(pθ(xt−1 | xt, y) ∥ q(xt−1 | xt, x0)).\n",
      "(5)\n",
      "3.4\n",
      "conditioning on semantic mask\n",
      "nasdm requires our neural network noise-predictor ϵθ(xt, y, t) to eﬀectively\n",
      "process the information from the nuclei semantic map.\n",
      "for this purpose, we\n",
      "leverage a modiﬁed u-net architecture described in wang et al.\n",
      "[27], where\n",
      "semantic information is injected into the decoder of the denoising network using\n",
      "multi-layer, spatially-adaptive normalization operators.\n",
      "1, we\n",
      "construct the semantic mask such that each channel of the mask corresponds to\n",
      "a unique nuclei type.\n",
      "3.5\n",
      "classiﬁer-free guidance\n",
      "to improve the sample quality and agreement with the conditioning signal, we\n",
      "employ classiﬁer-free guidance [10], which essentially ampliﬁes the conditional\n",
      "distribution using unconditional outputs while sampling.\n",
      "during training, the\n",
      "conditioning signal, i.e., the semantic label map, is randomly replaced with a\n",
      "null mask for a certain percentage of samples.\n",
      "− ∇xt log p(xt),\n",
      "∝ ∇xt log p(y | xt),\n",
      "(6)\n",
      "nasdm: nuclei-aware histopathology image generation\n",
      "791\n",
      "fig.\n",
      "2. guidance scale ablation: for a given mask, we generate images using diﬀer-\n",
      "ent values of the guidance scale, s. the fid and is metrics are computed by generating\n",
      "images for all masks in the test set at 20× magniﬁcation.\n",
      "where ∅ denotes an empty semantic mask.\n",
      "(7)\n",
      "4\n",
      "experiments\n",
      "in this section, we ﬁrst describe our implementation details and training proce-\n",
      "dure.\n",
      "we then per-\n",
      "form quantitative and qualitative assessments to demonstrate the eﬃcacy of our\n",
      "nuclei-aware semantic histopathology generation model.\n",
      "in all following exper-\n",
      "iments, we synthesize images using the semantic masks of the held-out dataset\n",
      "at the concerned objective magniﬁcation.\n",
      "we then compute fr´echet inception\n",
      "distance (fid) and inception score (is) metrics between the synthetic and real\n",
      "images in the held-out set.\n",
      "4.1\n",
      "implementation details\n",
      "our diﬀusion model is implemented using a semantic unet architecture\n",
      "(sect. 3.4), trained using the objective in (5).\n",
      "additionally, we adopt an exponential moving average (ema)\n",
      "of the denoising network weights with 0.999 decay.\n",
      "the whole framework is implemented using pytorch and\n",
      "trained on 4 nvidia tesla a100 gpus with a batch-size of 40 per gpu. code\n",
      "will be made public on publication or request.\n",
      "quantitative assessment: we report the performance of our method using\n",
      "fr´echet inception distance (fid) and inception score (is) with the metrics reported in\n",
      "existing works.\n",
      "∗note that performance reported for best competing method on the colon data\n",
      "is from our own implementation, performances for both this and our method should\n",
      "improve with better tuning.\n",
      "[18] colon\n",
      "morphology\n",
      "18.8\n",
      "2.2\n",
      "nasdm (ours)\n",
      "colon\n",
      "semantic mask 14.1\n",
      "2.7\n",
      "4.2\n",
      "ablation over guidance scale (s)\n",
      "in this study, we test the eﬀectiveness of the classiﬁer-free guidance strategy.\n",
      "2, increase\n",
      "in guidance scale initially results in better image quality as more detail is added\n",
      "to visual structures of nuclei.\n",
      "however, with further increase, the image quality\n",
      "degrades as the model overemphasizes the nuclei and staining textures.\n",
      "3.1, we generate patches at two dif-\n",
      "ferent objective magniﬁcations of 10× and 20×. in this\n",
      "section, we contrast the generative performance of the\n",
      "models trained on these magniﬁcation levels respectively.\n",
      "from the table on right, we observe that the model trained at 20× objective\n",
      "magniﬁcation produces better generative metrics.\n",
      "4.4\n",
      "quantitative analysis\n",
      "to the best of our knowledge, ours is the only work that is able to synthesize\n",
      "histology images given a semantic mask, making a direct quantitative compari-\n",
      "son tricky.\n",
      "however, the standard generative metric fr´echet inception distance\n",
      "(fid) measures the distance between distributions of generated and real images\n",
      "in the inception-v3\n",
      "[14] latent space, where a lower fid indicates that the model\n",
      "is able to generate images that are very similar to real data.\n",
      "nasdm: nuclei-aware histopathology image generation\n",
      "793\n",
      "fig.\n",
      "3. qualitative results: we generate synthetic images given masks with each\n",
      "type of nuclei in diﬀerent environments to demonstrate the proﬁciency of the model to\n",
      "generate realistic nuclei arrangements.\n",
      "we now qualitatively discuss the proﬁciency\n",
      "of our model in generating realistic visual patterns in synthetic histopathology\n",
      "images (refer fig. 3).\n",
      "in the synthetic images, we can see\n",
      "that the lymphocytes are accurately circular, while neutrophils and eosinophils\n",
      "have a more lobed structure.\n",
      "epithelial cells are most diﬃcult to generate in a convincing\n",
      "manner, however, we can see that model is able to capture the nuances well and\n",
      "generates accurate chromatin distributions.\n",
      "5\n",
      "conclusion and future works\n",
      "in this work, we present nasdm, a nuclei-aware semantic tissue generation\n",
      "framework.\n",
      "additionally, this framework can be extended to also generate semantic\n",
      "masks enabling an end-to-end tissue generation framework that ﬁrst generates a\n",
      "mask and then synthesizes the corresponding patch.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf:\n",
      "despite the fact that the appearance of the tumor-associated stroma\n",
      "contributes to diagnostic impressions, its assessment has not been stan-\n",
      "dardized.\n",
      "given the crucial role of the tumor microenvironment in tumor\n",
      "progression, it is hypothesized that the morphological analysis of stroma\n",
      "could have diagnostic and prognostic value.\n",
      "the model achieved an average testing auroc of 86.53% on a large\n",
      "curated dataset with over 1.1 million stroma patches.\n",
      "our experimental\n",
      "results indicate that stromal alterations are detectable in the presence of\n",
      "prostate cancer and highlight the potential for tumor-associated stroma\n",
      "to serve as a diagnostic biomarker in prostate cancer.\n",
      "understanding ﬁeld eﬀect is essential for cancer research\n",
      "as it provides insights into the mechanisms underlying tumor development and\n",
      "progression.\n",
      "tumor-associated stroma, which consists of various cell types, such\n",
      "as ﬁbroblasts, smooth muscle cells, and nerve cells, is an integral component\n",
      "of the tumor microenvironment that plays a critical role in tumor development\n",
      "and progression.\n",
      "reactive stroma, a distinct phenotype of stromal cells, arises\n",
      "in response to signaling pathways from cancerous cells and is characterized by\n",
      "altered stromal cells and increased extracellular matrix components [7,8].\n",
      "altered stroma can create a\n",
      "pro-tumorigenic environment by producing a multitude of chemokines, growth\n",
      "factors, and releasing reactive oxygen species [9,10], which can lead to tumor\n",
      "development and aggressiveness\n",
      "manual review for tumor-associated stroma is time-consuming and lacks\n",
      "quantitative metrics\n",
      "machine learning algorithms have been used to quantify\n",
      "the percentage of tumor to stroma in bladder cancer patients, but required\n",
      "dichotomizing patients based on a threshold\n",
      "software has been used to seg-\n",
      "ment tumor and stroma tissue in breast cancer patient samples, but the method\n",
      "required constant supervision by a pathologist [15].\n",
      "combining the information from both modalities can provide a more\n",
      "644\n",
      "z. wang et al.\n",
      "accurate understanding of the tumor microenvironment.\n",
      "in this work, we explore\n",
      "the ﬁeld eﬀect in prostate cancer by analyzing tumor-associated stroma in multi-\n",
      "modal histopathological images.\n",
      "our main contributions can be summarized as\n",
      "follows:\n",
      "– to the best of our knowledge, we present the ﬁrst deep-learning approach\n",
      "to characterize prostate tumor-associated stroma by integrating histological\n",
      "image analysis from both whole-mount and biopsy slides.\n",
      "– we proposed a novel approach for stroma classiﬁcation with spatial graphs\n",
      "modeling, which enable more accurate and eﬃcient analysis of tumor microen-\n",
      "vironment in prostate cancer pathology.\n",
      "given the spatial nature of cancer\n",
      "ﬁeld eﬀect and tumor microenvironment, our graph-based method oﬀers valu-\n",
      "able insights into stroma region analysis.\n",
      "– we developed a comprehensive pipeline for constructing tumor-associated\n",
      "stroma datasets across multiple data sources, and employed adversarial train-\n",
      "ing and neighborhood consistency regularization techniques to learn robust\n",
      "multimodal-invariant image representations.\n",
      "2\n",
      "method\n",
      "2.1\n",
      "stroma tissue segmentation\n",
      "accurately analyzing tumor-associated stroma requires a critical pre-processing\n",
      "step of segmenting stromal tissue from the background, including epithelial tis-\n",
      "sue.\n",
      "this segmentation task is challenging due to the complex and heterogeneous\n",
      "appearance of the stroma.\n",
      "to address this, we propose utilizing the pointrend\n",
      "model [17], which can handle complex shapes and appearances and produce\n",
      "smooth and accurate segmentations through iterative object boundary reﬁne-\n",
      "ment.\n",
      "moreover, the model’s eﬃciency and ability to process large images quickly\n",
      "make it suitable for analyzing whole-mount slides.\n",
      "by leveraging the pointrend\n",
      "model, we can generate stromal segmentation masks for more precise down-\n",
      "stream analysis.\n",
      "the spatial\n",
      "relationship can reveal valuable information about the tumor microenvironment,\n",
      "and neighboring stroma cells can undergo similar phenotypic changes in response\n",
      "to cancer.\n",
      "the stroma segmentation module generates a stroma mask to\n",
      "isolate the stromal tissue, which is then used to construct spatial patch graphs for the\n",
      "proposed deep-learning model.\n",
      "fig.\n",
      "the use of neighbor\n",
      "sampling enables eﬃcient processing of large images and allows for stochastic\n",
      "training of the model.\n",
      "to predict tumor-associated binary labels of stroma patches, we employ a\n",
      "message-passing approach that propagates patch features in the spatial graph.\n",
      "the gat uses an attention mechanism\n",
      "on node features to construct a weighting kernel that determines the impor-\n",
      "tance of nodes in the message-passing process.\n",
      "the\n",
      "gat layer is deﬁned as\n",
      "ge (vi) =\n",
      "\u0002\n",
      "vj∈n e\n",
      "vi∪{vi}\n",
      "αvi,vjw⃗hvj\n",
      "(1)\n",
      "where w ∈ rm×n is a learnable matrix transforming n-dimensional features to\n",
      "m-dimensional features.\n",
      "based on this assumption, ncr introduces a neighbor consistency loss\n",
      "deep learning for prostate tumor-associated stroma identiﬁcation\n",
      "647\n",
      "to encourage similar predictions of stroma patches that are similar in feature\n",
      "space.\n",
      "2.4\n",
      "adversarial multi-modal learning\n",
      "biopsy and whole-mount slides provide complementary multi-modal informa-\n",
      "tion on the tumor microenvironment, and combining them can provide a more\n",
      "comprehensive understanding of tumor-associated stroma.\n",
      "however, using data\n",
      "from multiple modalities can introduce systematic shifts, which can impact the\n",
      "performance of a deep learning model.\n",
      "speciﬁcally, whole-mount slides typically\n",
      "contain larger tissue sections and are processed using diﬀerent protocols than\n",
      "biopsy slides, which can result in diﬀerences in image quality, brightness, and\n",
      "contrast.\n",
      "these technical diﬀerences can aﬀect the pixel intensity distributions\n",
      "of the images, leading to systematic shifts in the features that the deep learning\n",
      "model learns to associate with tumor-associated stroma.\n",
      "for instance, a model\n",
      "trained on whole-mount slides only may not generalize well to biopsy slides due\n",
      "to systematic shifts, hindering model performance in the clinical application\n",
      "scenario.\n",
      "to address the above issues, we propose an adversarial multi-modal learning\n",
      "(aml) module to force the feature extractor to produce multimodal-invariant\n",
      "representations on multiple source images.\n",
      "the module takes\n",
      "the stroma embedding as an input and predicts the source of the image (biopsy\n",
      "or whole-mount) using multilayer perceptron (mlp) with cross-entropy loss\n",
      "function laml.\n",
      "all mod-\n",
      "ules were concurrently optimized in an end-to-end manner.\n",
      "3\n",
      "experiment\n",
      "3.1\n",
      "dataset\n",
      "in our study, we utilized three datasets for tumor-associated stroma analysis.\n",
      "(1) dataset a comprises 513 tiles extracted from the whole mount slides of 40\n",
      "patients, sourced from the archives of the pathology department at cedars-\n",
      "sinai medical center (irb# pro00029960).\n",
      "it combines two sets of tiles: 224\n",
      "images from 20 patients featuring stroma, normal glands, low-grade and high-\n",
      "grade cancer\n",
      "[22], along with 289 images from 20 patients with dense high-grade\n",
      "cancer (gleason grades 4 and 5) and cribriform/non-cribriform glands [23].\n",
      "each\n",
      "tile measures 1200×1200 pixels and is extracted from whole slide images cap-\n",
      "tured at 20x magniﬁcation (0.5 microns per pixel).\n",
      "the tiles were annotated\n",
      "at the pixel-level by expert pathologists to generate stroma tissue segmentation\n",
      "masks and were cross-evaluated and normalized to account for stain variabil-\n",
      "ity.\n",
      "(2) dataset b included 97 whole mount slides with an average size of over\n",
      "174,000×142,000 pixels at 40x magniﬁcation.\n",
      "the prostate tissue within these\n",
      "slides had an average tumor area proportion of 9%, with an average tumor area of\n",
      "77 square mm.\n",
      "dataset a was utilized for training the stroma segmentation model.\n",
      "extensive\n",
      "data augmentation techniques, such as image scaling and staining perturbation,\n",
      "were employed during the training process.\n",
      "the model achieved an average test\n",
      "dice score of 95.57 ± 0.29 through 5-fold cross-validation.\n",
      "this model was then\n",
      "applied to generate stroma masks for all slides in datasets b and c. to precisely\n",
      "isolate stroma tissues and avoid data bleeding from epithelial tissues, we only\n",
      "extracted patches where over 99.5% of the regions were identiﬁed as stroma at\n",
      "40x magniﬁcation to construct the stroma classiﬁcation dataset.\n",
      "for positive tumor-associated stroma patches, we sampled patches near\n",
      "tumor glands within annotated tumor region boundaries, as we presumed that\n",
      "tumor regions represent zones in which the greatest amount of damage has pro-\n",
      "gressed.\n",
      "to incorporate multi-modal information, we\n",
      "randomly sampled negative stroma patches from all biopsy slides in dataset c.\n",
      "overall, we selected over 1.1 million stroma patches of size 256×256 pixels at 40x\n",
      "magniﬁcation for experiments.\n",
      "during model training and testing, we performed\n",
      "stain normalization and standard image augmentation methods.\n",
      "3.2\n",
      "model training and evaluation\n",
      "for constructing knn-based patch graphs, we limited the graph size by setting\n",
      "k = 4 and layer number l = 3.\n",
      "all models were\n",
      "implemented using pytorch on a single tesla v100 gpu. to evaluate the model\n",
      "performance, we perform 5-fold cross-validation, where all slides are stratiﬁed by\n",
      "source origin and divided into 5 subsets.\n",
      "we measure the prediction performance using the area under the receiver\n",
      "operating characteristic (auroc), f1 score, precision, and recall.\n",
      "4\n",
      "results and discussions\n",
      "table 1.\n",
      "performance comparison with model variants.\n",
      "results are averaged over 5\n",
      "folds and shown in terms of mean value ± standard deviation.\n",
      "to evaluate the eﬀectiveness of our proposed method, we conducted an abla-\n",
      "tion study by comparing the performance of diﬀerent model variants presented\n",
      "in table 1.\n",
      "we systematically add\n",
      "one or more modules to the base model to evaluate their performance contri-\n",
      "bution.\n",
      "the results show that the full model outperforms the base model by a\n",
      "large margin with 10.04% in auroc and 10.97% in f1 score, and each module\n",
      "contributes to the overall performance.\n",
      "compared to the base model, the addi-\n",
      "tion of the gat module resulted in a signiﬁcant improvement in all metrics,\n",
      "suggesting spatial information captured by the patch graph was valuable for\n",
      "stroma classiﬁcation.\n",
      "the most notable performance improvement was achieved\n",
      "by the aml module, with a 5.72% increase in auroc and 5.55% increase in\n",
      "650\n",
      "z. wang et al.\n",
      "recall.\n",
      "this improvement indicates that aml helps the model better capture the\n",
      "multimodal-invariant features that are associated with tumor-associated stroma\n",
      "while reducing the false negative prediction by eliminating the inﬂuence of sys-\n",
      "tematic shift cross modalities.\n",
      "finally, the addition of the ncr module further\n",
      "increased the average model performance and improved the model robustness\n",
      "across 5 folds.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_63.pdf:\n",
      "automated detection of cervical abnormal cells from thin-\n",
      "prep cytologic test (tct) images is essential for eﬃcient cervical abnor-\n",
      "mal screening by computer-aided diagnosis system.\n",
      "however, the detec-\n",
      "tion performance is inﬂuenced by noise samples in the training dataset,\n",
      "mainly due to the subjective diﬀerences among cytologists in annotating\n",
      "the training samples.\n",
      "in this paper, we propose a cervical\n",
      "abnormal cell detection method optimized by a novel distillation strat-\n",
      "egy based on local-scale consistency reﬁnement.\n",
      "then, a pre-trained patch correction network (pcn)\n",
      "is leveraged to obtain local-scale features and conduct further reﬁne-\n",
      "ment for these suspicious cell patches.\n",
      "our experiments demon-\n",
      "strate that our distillation method can greatly optimize the performance\n",
      "of cervical abnormal cell detection without changing the detector’s net-\n",
      "work structure in the inference.\n",
      "the code is publicly available at https://\n",
      "github.com/feimanman/cervical-abnormal-cell-detection.\n",
      "keywords: cervical abnormal cell detection · consistency learning ·\n",
      "cervical cytologic images\n",
      "1\n",
      "introduction\n",
      "cervical cancer is the second most common cancer among adult women.\n",
      "nevertheless, delayed\n",
      "diagnosis of cervical cancer until an advanced stage will have a negative impact\n",
      "on patient prognosis and consume medical resources.\n",
      "currently, early screening\n",
      "of cervical cancer is recommended worldwide as an eﬀective method to prevent\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "therefore, there is a high demand\n",
      "for automated cervical abnormality screening to facilitate eﬃcient and accurate\n",
      "identiﬁcation of cervical abnormalities.\n",
      "with the development of deep learning\n",
      "[2] developed an attention feature pyramid network\n",
      "(attfpn) for automatic abnormal cervical cell detection in cervical cytopatho-\n",
      "logical images to assist pathologists in making more accurate diagnoses.\n",
      "[11] proposed to explore contextual\n",
      "relationships to boost the performance of cervical abnormal cell detection.\n",
      "it is worth mentioning that all of the aforementioned\n",
      "detection methods inevitably produce false positive results, which should be fur-\n",
      "ther reﬁned by pathologists for manual checking or classiﬁcation models estab-\n",
      "lished for automatic screening.\n",
      "[23] proposed a\n",
      "three-stage method including cell-level detection, image-level classiﬁcation, and\n",
      "case-level diagnosis obtained by an svm classiﬁer.\n",
      "although the above-mentioned attempts can improve the screening perfor-\n",
      "mance signiﬁcantly, there are several issues that need to be addressed: 1) object\n",
      "detection methods often require accurate annotated data to guarantee perfor-\n",
      "mance with robustness and generalization.\n",
      "however, due to legal limitations, the\n",
      "scarcity of positive samples, and especially the subjectivity diﬀerences between\n",
      "cytopathologists for manual annotations [20], it is likely to generate noisy sam-\n",
      "ples that aﬀect the performance of the detection model.\n",
      "to address these issues, we propose a novel method for cervical abnormal\n",
      "cell detection using distillation from local-scale consistency reﬁnement.\n",
      "in\n",
      "addition, we propose an roi-correlation consistency (rcc) loss between roi\n",
      "features and local-scale features from the pcn, which encourages the detector\n",
      "to explore the feature correlations of the suspicious cells.\n",
      "our proposed method\n",
      "achieves improved performance during inference without changing the detector\n",
      "structure.\n",
      "concerning the huge size of the whole slide image\n",
      "(wsi) and the infeasibility to handle a wsi scan for detection, we crop the\n",
      "wsi into images with the size of 1024 × 1024 as input to the detection.\n",
      "we implement the detection to locate\n",
      "the suspicious lesion cervical cells and extract the top k patches from the orig-\n",
      "inal image.\n",
      "our\n",
      "framework leverages a local-scale classiﬁcation reﬁnement mechanism to guide\n",
      "the training of the detection model.\n",
      "the pcn is employed to\n",
      "reﬁne and enhance the retinanet proposal classiﬁer, which is trained from a\n",
      "large number of patches collected in advance with more excellent classiﬁcation\n",
      "performance.\n",
      "more speciﬁcally, the input image is processed by the base detector fd(·)\n",
      "ﬁrstly to obtain the primary proposal information.\n",
      "the proposed pcn fc(·) takes\n",
      "the top-k patches as inputs, which are cropped from original images according\n",
      "to the proposal location, denoted as ip = cr(i, p), where cr(·) denotes the crop\n",
      "function, i and p denote input image and proposal boxes predicted by fd(·),\n",
      "respectively.\n",
      "(1)\n",
      "the key idea is to augment the base detector fd(·) with the pcn fc(·) in parallel\n",
      "to enhance the proposal classiﬁcation capability.\n",
      "speciﬁcally, the ranking loss is given by:\n",
      "lrank(sd, sc) = max {0, sc − sd + margin} ,\n",
      "(2)\n",
      "where sc is the classiﬁcation reﬁnement score and sd is the detection score,\n",
      "which enforces sd > sc + margin in training.\n",
      "based on the consistency strategy [14], which enhances the consistency of\n",
      "the intrinsic relation among diﬀerent models, we propose roi-correlation consis-\n",
      "tency, which regularizes the network to maintain the consistency of the seman-\n",
      "tic relation between patches under roi features and local-scale features, and\n",
      "thereby encourage the detector to explore the feature interaction between cells\n",
      "from the extracted patches to improve the network performance.\n",
      "and each sample undergoes the roi align layer to obtain the top\n",
      "k rois, we denote the activation map of rois as f r ∈ rb×k×h×w ×c, where\n",
      "h and w are the spatial dimension of the feature map, and c is the channel\n",
      "number.\n",
      "we average pooling the\n",
      "feature map f r along the spatial dimension and reshape it into ar ∈ rbk×c,\n",
      "and then the case-wise gram matrix gr ∈ rbk×bk is computed as:\n",
      "gr = ar · (ar)t ,\n",
      "(3)\n",
      "where gij is the inner product between the vectorized activation map ar\n",
      "i and\n",
      "ar\n",
      "j , whose intuitive meaning is the similarity between the activations of ith roi\n",
      "and jth roi within the input mini-batch.\n",
      "we perform average pooling on the feature map f c across\n",
      "the spatial dimension and then reshape it into ac ∈ rbk×hw c, the case-wise\n",
      "robust cervical abnormal cell detection\n",
      "657\n",
      "gram matrix gc ∈ rbk×bk and the ﬁnal relation matrix rc are computed\n",
      "as:\n",
      "gc = ac · (ac)t ,\n",
      "(5)\n",
      "rc =\n",
      "\u0002\n",
      "gc\n",
      "1\n",
      "\u0003\u0003gc\n",
      "1\n",
      "\u0003\u0003\n",
      "2\n",
      ", · · · ,\n",
      "gc\n",
      "bk\n",
      "\u0003\u0003gc\n",
      "bk\n",
      "\u0003\u0003\n",
      "2\n",
      "\u0004t\n",
      ".\n",
      "(6)\n",
      "the rcc requires the correlation matrix to be stable under roi features\n",
      "and local-scale features to preserve the semantic relation between patches.\n",
      "by minimizing\n",
      "lrcc during the training process, the network could be enhanced to capture the\n",
      "intrinsic relation between patches, thus helping to extract additional semantic\n",
      "information from cells.\n",
      "2.4\n",
      "optimization\n",
      "to better optimize the retinanet detector in a reinforced way, we take the fol-\n",
      "lowing training strategy, which consists of three major stages.\n",
      "in the ﬁrst stage,\n",
      "we collect images with doctors’ labels for training and initialized the detection\n",
      "net.\n",
      "in the second stage, we train pcn with cross-entropy loss until convergence.\n",
      "in the last stage, we freeze the pcn and optimize the detector.\n",
      "3\n",
      "experimental results\n",
      "3.1\n",
      "dataset and experimental setup\n",
      "dataset.\n",
      "for cervical cell detection, our dataset includes 3761 images of 1024×\n",
      "1024 pixels cropped from wsis.\n",
      "performance comparison with state-of-the-art methods.\n",
      "[11]\n",
      "44.6\n",
      "77.5\n",
      "47.7\n",
      "60.0\n",
      "retinanet [12]\n",
      "45.7\n",
      "81.3\n",
      "46.2\n",
      "58.8\n",
      "proposed method\n",
      "51.1\n",
      "86.6\n",
      "54.3\n",
      "62.5\n",
      "images, while pathologists b and c had 10 years of experience each.\n",
      "initially, the\n",
      "images were randomly assigned to pathologist b or c for initial labeling.\n",
      "any discrepancies found were checked and re-labeled by pathologist\n",
      "a. these images were divided into the training set and the testing set according\n",
      "to the ratio of 9:1.\n",
      "implementation details.\n",
      "the model is implemented by pytorch on 2 nvidia tesla p100 gpus.\n",
      "we conduct a quantitative evaluation using two metrics: the coco-style [13]\n",
      "average precision (ap) and average recall (ar).\n",
      "we calculate the average ap\n",
      "over multiple iou thresholds from 0.5 to 0.95 with a step size of 0.05, and indi-\n",
      "vidually evaluated ap at the iou thresholds of 0.5 and 0.75 (denoted as ap.5\n",
      "and ap.75), respectively.\n",
      "3.2\n",
      "evaluation of cervical abnormal cell detection\n",
      "comparison with sota methods.\n",
      "we compare the performance of our pro-\n",
      "posed method against known methods for cervical lesion detection as well as rep-\n",
      "resentative methods for object detection.\n",
      "(2) based on retinanet, our\n",
      "method improves the detection performance signiﬁcantly, especially ap.5 shows\n",
      "great performance improvement.\n",
      "(a) shows input images\n",
      "with ground-truth annotations.\n",
      "table 2. performance of ablation study for our local-scale consistency reﬁnement.\n",
      "+ranking loss and rcc loss 51.1 86.6 54.3\n",
      "62.5\n",
      "better performance, especially in ap.75, with an improvement of 2.8.\n",
      "(3) with both ranking\n",
      "loss and rcc loss, our method has the best performance, which surpasses the\n",
      "baseline model by a large margin, validating the eﬀectiveness of our method.\n",
      "1. those feature\n",
      "maps are from the conv3 stages of the class-subnet backbone.\n",
      "speciﬁcally, we\n",
      "sum and average the features in the channel dimension, and upsample them to\n",
      "the original image size.\n",
      ", our method can really learn better\n",
      "feature representations for abnormal cells, with the help of our proposed classi-\n",
      "ﬁcation ranking reﬁnement and roi-correlation consistency learning.\n",
      "our work can achieve better performance without adding new modules\n",
      "during inference.\n",
      "experiments demonstrate the eﬀectiveness and robustness of\n",
      "our method on the task of cervical abnormal cell detection.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_77.pdf:\n",
      "quantitative immunoﬂuorescence (qif) enables identifying\n",
      "immune cell subtypes across histopathology images.\n",
      "the triaangil’s eﬃcacy for\n",
      "microenvironment characterization from qif images is demonstrated in\n",
      "problems of predicting (1) response to immunotherapy (n = 122) and\n",
      "(2) overall survival (n = 135) in patients with lung cancer in comparison\n",
      "with four hand-crafted approaches namely dentil, gg, ccg, spatil,\n",
      "and deep learning with gnn.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 77.\n",
      "https://doi.org/10.1007/978-3-031-43987-2_77\n",
      "798\n",
      "s. arabyarmohammadi et al.\n",
      "1\n",
      "introduction\n",
      "the tumor microenvironment (tme) is comprised of cancer, immune (e.g. b\n",
      "lymphocytes, and t lymphocytes), stromal, and other cells together with non-\n",
      "cellular tissue components\n",
      "it is well acknowledged that tumors\n",
      "evolve in close interaction with their microenvironment.\n",
      "quantitatively charac-\n",
      "terizing tme has the potential to predict tumor aggressiveness and treatment\n",
      "response [3,23,24,30].\n",
      "immunotherapy\n",
      "(io) is the standard treatment for patients with advanced non-small cell lung\n",
      "cancer (nsclc)\n",
      "[19] but only 27–45% of patients respond to this treatment [21].\n",
      "therefore, better algorithms and improved biomarkers are essential for identify-\n",
      "ing which cancer patients are most likely to respond to io in advance of treat-\n",
      "ment.\n",
      "2\n",
      "previous related work and novel contributions\n",
      "many studies have only looked at the density of a single biomarker (e.g. tils),\n",
      "to show that a high density of tils is associated with improved patient sur-\n",
      "vival and treatment response in nsclc\n",
      "[3,24]. other works have attempted to\n",
      "characterize the spatial arrangement of cells in tme using computational graph-\n",
      "based approaches.\n",
      "[9] which attempted to characterize the interplay between\n",
      "immune and cancer cells and has proven to be helpful in predicting the recur-\n",
      "rence in early-stage nsclc.\n",
      "this in turn allows for development of\n",
      "machine classiﬁers to predict outcome and response in lung cancer patients\n",
      "treated with io.\n",
      "(2) triangil includes a set of quantitative metrics that capture the interplay\n",
      "within and between nuclei corresponding to diﬀerent types/families.\n",
      "[2,5,9,14]\n",
      "while triangil measurements are able to consider inter- and intra-family\n",
      "relationships.\n",
      "(3) although deep learning (dl) models (e.g., graph neural networks(gnn))\n",
      "have shown great capabilities in solving complex problems in the biomedical\n",
      "ﬁeld, these tend to be black-box in nature.\n",
      "these complex interactions enhance our understanding\n",
      "of the tme and will help pave the way for new therapeutic strategies that\n",
      "leverage these insights.\n",
      "next, we\n",
      "extract a series of features including clustering coeﬃcient, average degree from\n",
      "the resulting subgraph.\n",
      "in this manner, a total of 126 features (sup-\n",
      "plemental table 1) are extracted (42 features for absence of one family ×3).\n",
      "2) triangulation-based\n",
      "connections:\n",
      "a delaunay triangulation is con-\n",
      "structed by the nodes of α, β, γ (fig.\n",
      "next, a series of features\n",
      "were extracted from the remaining subgraph (e.g. number of edges between the\n",
      "nodes of α and β, β and γ, α and γ; complete list of features in supplemental\n",
      "table 1).\n",
      "next, we call gettrianglefeatures() function to quantify\n",
      "triangular relationships by extracting features from the resulting subgraphs\n",
      "(e.g. perimeter and area of triangles; complete list of features in supplemental\n",
      "table 1).\n",
      "4\n",
      "experimental results and discussion\n",
      "4.1\n",
      "dataset\n",
      "the cohort employed in this study was composed of pre-treatment tumor biopsy\n",
      "specimens from patients with nsclc from ﬁve centers (two centers for training\n",
      "triangular analysis of geographical interplay of lymphocytes (triangil)\n",
      "801\n",
      "algorithm 1: finding triangles\n",
      "input: a jagged array del : delaunay graph with three vertices of every\n",
      "triangle in each row, a hashmap φ : maps nodes to their type\n",
      "output: triangle features trifeatset\n",
      "let triindex ← ∅ be the list for triangle indices\n",
      "for i = 1 to i = length(del) do\n",
      "let marker ← ∅ be a auxiliary list to keep the viewed markers\n",
      "for j = 1 to 3 do\n",
      "if φ(del(i, j))\n",
      "the entire analysis was\n",
      "carried out using 122 patients in experiment 1 (73 in st, and 49 in sv) and\n",
      "135 patients in experiment 2 (81 in st, and 54 in sv).\n",
      "specimens were ana-\n",
      "lyzed with a multiplexed quantitative immunoﬂuorescence (qif) panel using\n",
      "the method described in [22].\n",
      "from each whole slide image, 7 representative\n",
      "tiles were obtained and used to train the software inform to deﬁne background,\n",
      "tumor and stromal compartments.\n",
      "then, individual cells were segmented based\n",
      "on nuclear dapi staining and the segmentation performance was controlled by\n",
      "direct visualization of samples by a trained observer.\n",
      "[3,24]\n",
      "(supplemental table 2).\n",
      "(a) representative qif image.\n",
      "after selecting every two cell types, features are extracted from their\n",
      "convex hulls (e.g. the number of clusters of each cell type, area intersected\n",
      "between clusters [9]; complete list of combinations in supplemental table 3).\n",
      "gnn: a recent study [31] demonstrated that transformer-based [29] gnns are\n",
      "able to learn the arrangement of tiles across pathology images for survival analysis.\n",
      "here, for each tile in the slide, a delaunay graph was constructed regardless of cell\n",
      "subtypes, and tile-level feature representations (e.g.side length minimum, maxi-\n",
      "mum, mean, and standard deviation, triangle area minimum, maximum, mean,\n",
      "and standard deviation) were aggregated by a transformer according to their spa-\n",
      "tial arrangement [31].\n",
      "our approach utilized the weisfeiler-lehman (wl) test\n",
      "well-known approaches, such\n",
      "as graphsage [10], are considered as continuous approximations to the wl test.\n",
      "4.3\n",
      "experiment 1: immunotherapy response prediction in lung\n",
      "cancer\n",
      "design: triangil was also trained to diﬀerentiate between patients who\n",
      "responded to io and those who did not.\n",
      "therefore, triangil approach is not only predic-\n",
      "tive of treatment response but more critically it enables biological interpretations\n",
      "that a dl model might not be able to provide.\n",
      "the leftmost column shows a part of a qif image.\n",
      "(color ﬁgure online)\n",
      "804\n",
      "s. arabyarmohammadi et al.\n",
      "4.4\n",
      "experiment 2: predicting survival in lung cancer patients\n",
      "treated with immunotherapy\n",
      "design: st was used to construct a least absolute shrinkage and selection oper-\n",
      "ator (lasso)\n",
      "lasso features are\n",
      "listed in supplemental table 4.\n",
      "kaplan-meier (km) survival curves [26] were plotted and the model perfor-\n",
      "mance was summarized by hazard ratio (hr), with corresponding (95% conﬁ-\n",
      "dence intervals (ci)) using the log-rank test, and harrell’s concordance index\n",
      "(c-index) on sv.\n",
      "5\n",
      "concluding remarks\n",
      "we presented a new approach, triangular analysis of geographical interplay of\n",
      "lymphocytes (triangil), to quantitatively chartacterize the spatial arrange-\n",
      "ment and relative geographical interplay of multiple cell families across patho-\n",
      "logical images.\n",
      "compared to previous spatial graph-based methods, triangil\n",
      "quantiﬁes the spatial interplay between multiple cell families, providing a more\n",
      "comprehensive portrait of the tumor microenvironment.\n",
      "although ﬁve cell families were studies in this work, triangil\n",
      "is ﬂexible and could include other cell types (e.g., macrophages).\n",
      "future work\n",
      "will entail larger validation studies and also evaluation on other use cases.\n",
      "acknowledgements.\n",
      "research reported in this publication was supported by the\n",
      "national cancer institute under award numbers r01ca268287a1, u01 ca269181,\n",
      "r01 ca26820701a1, r01ca249992- 01a1, r01ca202752- 01a1, r01ca208236- 01a1,\n",
      "r01ca216579- 01a1, r01ca220581-01a1, r01ca257612- 01a1, 1u01ca239055- 01,\n",
      "1u01ca248226- 01, 1u54ca254566- 01, national heart, lung and blood institute\n",
      "1r01hl15127701a1, r01hl15807101a1, national institute of biomedical imaging\n",
      "and bioengineering 1r43eb028736- 01, va merit review award ibx004121a from\n",
      "the united states department of veterans aﬀairs biomedical laboratory research and\n",
      "development service the oﬃce of the assistant secretary of defense for health aﬀairs,\n",
      "through the breast cancer research program (w81xwh- 19- 1-0668), the prostate\n",
      "cancer research program (w81xwh- 20-1- 0851), the lung cancer research program\n",
      "(w81xwh-18-1-0440, w81xwh-20-1-0595), the peer reviewed cancer research pro-\n",
      "gram (w81xwh- 18-1-0404, w81xwh- 21-1-0345, w81xwh- 21-1-0160), the kidney\n",
      "precision medicine project (kpmp) glue grant and sponsored research agreements\n",
      "from bristol myers-squibb, boehringer-ingelheim, eli-lilly and astrazeneca.\n",
      "the con-\n",
      "tent is solely the responsibility of the authors and does not necessarily represent the\n",
      "oﬃcial views of the national institutes of health, the u.s. department of veterans\n",
      "aﬀairs, the department of defense, or the united states government.\n",
      "auﬀarth, b., l´opez, m., cerquides, j.: comparison of redundancy and relevance\n",
      "measures for feature selection in tissue classiﬁcation of ct images.\n",
      "computerized image-based detection and grading of\n",
      "lymphocytic inﬁltration in her2+ breast cancer histopathology.\n",
      "rev. 63(5), 277 (1956)\n",
      "5. corredor, g., et al.: spatial architecture and arrangement of tumor-inﬁltrating\n",
      "lymphocytes for predicting likelihood of recurrence in early-stage non-small cell\n",
      "lung cancer.\n",
      "ding, r., et al.: image analysis reveals molecularly distinct patterns of tils in\n",
      "nsclc associated with treatment outcome.\n",
      "human pathol.\n",
      "leman, a., weisfeiler, b.: a reduction of a graph to a canonical form and an\n",
      "algebra arising during this reduction.\n",
      "20. newman, m.:\n",
      "sato, j., et al.: cd20+ tumor-inﬁltrating immune cells and cd204+ m2\n",
      "macrophages are associated with prognosis in thymic carcinoma.\n",
      "schalper, k.a., et al.: objective measurement and clinical signiﬁcance of tils in\n",
      "non-small cell lung cancer 107(3).\n",
      "simon, r.m., subramanian, j., li, m.c., menezes, s.: using cross-validation to\n",
      "evaluate predictive accuracy of survival risk classiﬁers based on high-dimensional\n",
      "data.\n",
      "https://doi.org/10.48550/\n",
      "arxiv.1706.03762\n",
      "30. whiteside, t.: the tumor microenvironment and its role in promoting tumor\n",
      "growth.\n",
      "transformer as a spatially aware multi-instance learning frame-\n",
      "work to predict the risk of death for early-stage non-small cell lung cancer.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf:\n",
      "however, com-\n",
      "pared to h&e staining, ihc staining can be much more expensive in\n",
      "terms of both labor and the laboratory equipment required.\n",
      "to remedy this situation, we present a new loss\n",
      "function, adaptive supervised patchnce (asp), to directly deal with\n",
      "the input to target inconsistencies in a proposed h&e-to-ihc image-\n",
      "to-image translation framework.\n",
      "the asp loss is built upon a patch-\n",
      "based contrastive learning criterion, named supervised patchnce (sp),\n",
      "and augments it further with weight scheduling to mitigate the negative\n",
      "impact of noisy supervision.\n",
      "in our exper-\n",
      "iment, we demonstrate that our proposed method outperforms existing\n",
      "image-to-image translation methods for stain translation to multiple ihc\n",
      "stains.\n",
      "keywords: generative adversarial network · contrastive learning ·\n",
      "h&e-to-ihc stain translation\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 61.\n",
      "for instance, the her2 (human epidermal growth factor\n",
      "receptor 2) biomarker is associated with aggressive breast tumor development\n",
      "and is essential in forming a precise treatment plan.\n",
      "despite its capability to\n",
      "provide highly valuable diagnostic information, the process of ihc staining is\n",
      "very labor-intensive, time-consuming and requires specialized histotechnologists\n",
      "and laboratory equipments [2].\n",
      "in routine diagnostics, on account of its much lower cost, an\n",
      "h&e-stained slide is prepared by pathologists in order to determine whether or\n",
      "not to also apply the ihc stains for a more precise assessment of the disease.\n",
      "to that end, researchers have recently proposed to use gan-based image-to-\n",
      "image translation (i2it) algorithms for transforming h&e-stained slides into\n",
      "ihc.\n",
      "despite the progress, the outstanding challenge in training such i2it frame-\n",
      "works is the lack of aligned h&e-ihc image pairs, or in other words, the incon-\n",
      "sistencies in the h&e-ihc groundtruth pairs.\n",
      "this inevitably prevents pixel-perfect image correspondences due to\n",
      "the slice-to-slice changes in cell morphology, staining-induced degradation (e.g.\n",
      "tissue-tearing), imaging artifacts that may vary among slices (e.g. camera out-of-\n",
      "focus) and multi-slice registration errors.\n",
      "in the latter, comparing the groundtruth ihc image to the input h&e image,\n",
      "one can clearly see the inconsistencies – nearly the entire left half of the tissue\n",
      "present in the h&e image is missing.\n",
      "moreover, existing approaches have also exploited using expert\n",
      "annotations such as per-cell labels [9], semantic masks\n",
      "however,\n",
      "the robustness of such approaches that punish absolute errors in the generated\n",
      "image to dealing with gt inconsistencies remains unclear.\n",
      "the\n",
      "634\n",
      "f. li et al.\n",
      "work we present in this paper is based on the important realization that even\n",
      "when pairs of consecutive tissue slices do not yield images that are pixel-perfect\n",
      "aligned, it is highly likely that the corresponding patches in the two stains share\n",
      "the same diagnostic label.\n",
      "therefore, we set our goal to\n",
      "meaningfully leverage such correlations to beneﬁt the h&e-to-ihc i2it while\n",
      "being resilient to any inconsistencies.\n",
      "furthermore, based on the observation that\n",
      "any dissimilarity between the patch embeddings at corresponding locations in the\n",
      "generated and groundtruth ihc images is indicative to the level of inconsistency\n",
      "of the gt at that location, we employ an adaptive weighting scheme in asp.\n",
      "we evaluated existing\n",
      "i2it methods and ours for multiple ihc stains and demonstrate the superior\n",
      "performance achieved by our method both qualitatively and quantitatively.\n",
      "fig.\n",
      "the patch embeddings z are extracted by a shared network f.\n",
      "adaptive supervised patchnce loss\n",
      "635\n",
      "2\n",
      "method description\n",
      "2.1\n",
      "the supervised patchnce (sp) loss\n",
      "before getting to our asp loss, we need to ﬁrst introduce the sp loss as a robust\n",
      "means to learning from inconsistent gt image pairs.\n",
      "it takes the\n",
      "same form as the patchnce loss as introduced in [11], except that it is applied\n",
      "on the generated-gt image pair (instead of the input-generated pair).\n",
      "it does so by minimizing a patch-based infonce loss [10],\n",
      "which encourages the network to associate the corresponding patches with each\n",
      "other in the learned embedding space, while disassociating them from the non-\n",
      "corresponding ones.\n",
      "with infonce, the patchnce loss is set up as follows:\n",
      "given the anchor embedding ˆzy of a patch in the output image, the positive zx\n",
      "is the embedding of the corresponding patch from the input image, while the\n",
      "negatives \u0005zx are embeddings of the non-corresponding ones, i.e. lpatchnce =\n",
      "linfonce(ˆzy , zx, \u0005zx).\n",
      "as for the sp loss, given the embedding of an output patch ˆzy as anchor,\n",
      "we now designate the embedding of the corresponding patch in the groundtruth\n",
      "image zy as the positive and the embeddings of the non-corresponding ones\n",
      "\u0005zy as the negatives.\n",
      "it is worth noting that, despite the fact\n",
      "that a similar patchwise constrastive loss was proposed in [1] for supervised i2it,\n",
      "it is one of our contributions in this paper to explicitly exploit the robustness\n",
      "of this contrastive loss in the context of h&e-to-ihc translation where the gt\n",
      "pairs can be highly inconsistent for reasons mentioned previously.\n",
      "2.2\n",
      "the adaptive supervised patchnce (asp) loss\n",
      "to learn selectively from more consistent groundtruth locations, we further pro-\n",
      "pose to augment the supervised patchnce loss in an adaptive manner.\n",
      "2, we show an example pair of generated vs\n",
      "gt ihc images that contain signiﬁcant inconsistencies and their anchor-positive\n",
      "similarity heat map.\n",
      "to that end, we further augment the weight so that it is also a function of the\n",
      "training iterations.\n",
      "such scheduling of the weights is done so that in the beginning\n",
      "of the training, the weights are uniform in order not to wrongly bias the network\n",
      "when the embeddings are still indiscriminative.\n",
      "(2)\n",
      "we refer to the new augmented supervised patchnce loss as the adaptive\n",
      "supervised patchnce (asp) loss, which can be expressed as:\n",
      "lasp(g, h, x, y, t) = e(x,y)∼(x,y )\n",
      "l\n",
      "\b\n",
      "l=1\n",
      "sl\n",
      "\b\n",
      "s=1\n",
      "wt(ˆzl,s\n",
      "y , zl,s\n",
      "y )\n",
      "w l\n",
      "t\n",
      "·linfonce(ˆzl,s\n",
      "y , zl,s\n",
      "y , \u0005zl,s\n",
      "y ),\n",
      "(3)\n",
      "adaptive supervised patchnce loss\n",
      "637\n",
      "fig.\n",
      "2. (a) input h&e image x, (b) generated ihc image ˆy, (c) groundtruth ihc\n",
      "image y, and (d) heat map of the anchor-positive cosine similarities produced by a\n",
      "trained network at corresponding locations:\n",
      "4. left to right: (a) input h&e image; (b) groundtruth ihc image; (c) generated\n",
      "image without lsp; (d) with lsp; (e) with l(lambda,linear)\n",
      "asp\n",
      ".\n",
      "cut is from [11]\n",
      "3\n",
      "experiments\n",
      "datasets.\n",
      "the following datasets are used in our experiments: the breast cancer\n",
      "immunohistochemical (bci) challenge dataset\n",
      "note\n",
      "that we have additionally normalized the brightness levels of all bci images to\n",
      "the same level.\n",
      "due to the page limit, from the mist dataset, here we only\n",
      "present detailed results on her2 and er.\n",
      "additional results on mistki67 and mistpr are provided in\n",
      "the supplementary materials.\n",
      "implementation details.\n",
      "to compare a pair of images, generated and groundtruth, we\n",
      "use the standard ssim (structural similarity index measure) and phv (per-\n",
      "ceptual hash value) as described in [8].\n",
      "4, we compare visually the generated ihc\n",
      "images by our framework.\n",
      "it can be observed that by using either lsp or lasp,\n",
      "the pathological representations in the generated images are signiﬁcantly more\n",
      "accurate.\n",
      "for those methods, fig. 5 visually illustrates the extent of\n",
      "hallucinations which we believe is the reason for their poor quantitative perfor-\n",
      "mance.\n",
      "with lsp already being a\n",
      "strong baseline, using diﬀerent adaptive strategies can provide further gains in\n",
      "performance.\n",
      "false morphological\n",
      "alterations cause the tissue structures in the translated images to no longer match\n",
      "those in the input h&e image, especially the nuclei.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf:\n",
      "weakly supervised classiﬁcation of whole slide images (wsis) in dig-\n",
      "ital pathology typically involves making slide-level predictions by aggregating\n",
      "predictions from embeddings extracted from multiple individual tiles.\n",
      "we\n",
      "validated the method using four hematoxylin and eosin stained wsi classiﬁca-\n",
      "tion tasks: human epidermal growth factor receptor 2 status and estrogen receptor\n",
      "status in primary breast cancer, breast cancer metastasis in lymph node tissue,\n",
      "and cell of origin classiﬁcation in diffuse large b-cell lymphoma.\n",
      "using the publicly available herohe challenge data\n",
      "set, the method achieved a state-of-the-art performance of 90% area under the\n",
      "receiver operating characteristic curve.\n",
      "additionally, we present a novel model\n",
      "explainability method that could identify cells associated with different classiﬁca-\n",
      "tion groups, thus providing supplementary validation of the classiﬁcation model.\n",
      "keywords: deep learning · whole slide images · hematoxylin and eosin\n",
      "1\n",
      "introduction\n",
      "accurate diagnosis plays an important role in achieving the best treatment outcomes\n",
      "for people with cancer [1]. identiﬁcation of cancer biomarkers permits more granular\n",
      "classiﬁcation of tumors, leading to better diagnosis, prognosis, and treatment decisions\n",
      "for many cancers, clinically reliable genomic, molecular, or imaging biomarkers\n",
      "supplementary information the online version contains supplementary material available at\n",
      "https://doi.org/10.1007/978-3-031-43987-2_75.\n",
      "https://doi.org/10.1007/978-3-031-43987-2_75\n",
      "deep cellular embeddings: an explainable plug and play improvement\n",
      "777\n",
      "havenotbeenidentiﬁedandbiomarkeridentiﬁcationtechniques(e.g.,ﬂuorescenceinsitu\n",
      "hybridization) have limitations that can restrict their clinical use.\n",
      "however, visual examination of h&e-stained\n",
      "slides is insufﬁcient for classiﬁcation of some tumors because identifying morphological\n",
      "differencesbetweenmolecularlydeﬁnedsubtypesisbeyondthelimitofhumandetection.\n",
      "the introduction of digital pathology (dp) has enabled application of machine learn-\n",
      "ing approaches to extract otherwise inaccessible diagnostic and prognostic information\n",
      "from h&e-stained whole slide images (wsis)\n",
      "however, many of these models use embeddings\n",
      "derived from tiles extracted using pretrained networks, and these often fail to capture\n",
      "useful information from individual cells.\n",
      "our\n",
      "new method achieved better performance on wsi classiﬁcation tasks and had a greater\n",
      "level of explainability than models that used only tile-level embeddings.\n",
      "2\n",
      "embedding extraction scheme\n",
      "transfer learning using backbones pretrained on natural images is a common method\n",
      "that addresses the challenge of using data sets that largely lack annotation.\n",
      "however,\n",
      "using backbones pretrained on natural images is not optimal for classiﬁcation of clinical\n",
      "images [11].\n",
      "the backbone\n",
      "was trained with the bootstrap your own latent (byol) method [13] using four publicly\n",
      "available data sets from the cancer genome atlas (tcga) and three data sets from\n",
      "private vendors that included healthy and malignant tissue from a range of organs [14].\n",
      "2.1\n",
      "tile-level embeddings\n",
      "following standard practice, we extracted tiles with dimensions of 256 × 256 pixels\n",
      "from wsis (digitized at 40 × magniﬁcation) on a spatial grid without overlap.\n",
      "we extracted deep cell-level embeddings by ﬁrst detecting individual cellular bound-\n",
      "aries using stardist [18] and extracting 32 × 32-pixel image crops centered around each\n",
      "segmented nucleus to create cell-patch images.\n",
      "we then used the pre-trained resnet50\n",
      "model to extract cell-level embeddings in a similar manner to the extraction of the tile-\n",
      "level embeddings.\n",
      "since resnet50 has a spatial reduction factor of 32 in the output of the\n",
      "cnn, the 32 × 32-pixel image had a 1:1 spatial resolution in the output.\n",
      "to ensure the\n",
      "cell-level embeddings contained features relevant to the cells, prior to the mean pooling\n",
      "in resnet50 we increased the spatial image resolution to 16 × 16 pixels in the output\n",
      "from the cnn by enlarging the 32 × 32-pixel cell-patch images to 128 × 128 pixels\n",
      "and skipping the last 4-layers in the network.\n",
      "because of heterogeneity in the size of cells detected, each 32 × 32-pixel cell-\n",
      "patch image contained different proportions of cellular and noncellular features.\n",
      "higher\n",
      "proportions of noncellular features in an image may cause the resultant embeddings to\n",
      "be dominated by noncellular tissue features or other background features.\n",
      "therefore, to\n",
      "limit the information used to create the cell-level embeddings to only cellular features, we\n",
      "removed portions of the cell-patch images that were outside of the segmented nuclei by\n",
      "setting their pixel values to black (rgb 0, 0, 0).\n",
      "finally, to prevent the size of individual\n",
      "nuclei or amount of background in each cell-patch image from dominating over the cell-\n",
      "level features, we modiﬁed the resnet50 global average pooling layer to only average\n",
      "deep cellular embeddings: an explainable plug and play improvement\n",
      "779\n",
      "the features inside the boundary of the segmented nuclei, rather than averaging across\n",
      "the whole output tensor from the cnn.\n",
      "2.3\n",
      "combined embeddings\n",
      "to create a combined representation of the tile-level and cell-level embeddings, we\n",
      "ﬁrst applied a nuclei segmentation network to each tile.\n",
      "in addition to the wsi classiﬁcation results presented in the next sections, we also\n",
      "performed experiments to compare the ability of combined embeddings and tile-level\n",
      "embeddings to predict nuclei-related features that were manually extracted from the\n",
      "images and to identify tiles where nuclei had been ablated.\n",
      "the details and results of these\n",
      "experiments are available in supplementary materials and provide further evidence of\n",
      "the improved ability to capture cell-level information when using combined embeddings\n",
      "compared with tile-level embeddings alone.\n",
      "[19] (the code was\n",
      "adapted from a publicly available implementation [20]).\n",
      "when comparing the combined\n",
      "embedding extraction method with the tile-level only embeddings, parameters were ﬁxed\n",
      "to demonstrate differences in performance without additional parameter tuning.\n",
      "[22] since it consumes less memory\n",
      "(the code was adapted from a publicly available implementation [23]).\n",
      "for breast cancer human epidermal growth factor receptor 2 (her2) prediction,\n",
      "we used data from the herohe challenge data set [26].\n",
      "for\n",
      "prediction of estrogen receptor (er) status, we used images from the tcga-breast\n",
      "invasive carcinoma (tcga-brca) data set [28] for which the er status was known.\n",
      "deep cellular embeddings: an explainable plug and play improvement\n",
      "781\n",
      "for these two tasks we used artifact-free tiles from tumor regions detected with an\n",
      "in-house tumor detection model.\n",
      "for breast cancer metastasis detection in lymph node tissue, we used wsis of h&e-\n",
      "stained healthy lymph node tissue and lymph node tissue with breast cancer metastases\n",
      "from the publicly available camelyon16 challenge data set [16, 29].\n",
      "4\n",
      "model classiﬁcation performance\n",
      "for the her2 prediction, er prediction, and metastasis detection classiﬁcation tasks,\n",
      "combined embeddings outperformed tile-level only embeddings irrespective of the\n",
      "downstream classiﬁer architecture used (fig. 4).\n",
      "fig.\n",
      "4. model performance using the xformer and a-mil architectures for the breast cancer\n",
      "her2 status, breast cancer er status, and breast cancer metastasis detection in lymph node tissue\n",
      "classiﬁcation tasks.\n",
      "error bars represent 95% conﬁdence intervals computed by a 5000-sample\n",
      "bias-corrected and accelerated bootstrap.\n",
      "in fact, for the her2 classiﬁcation task, combined embeddings obtained using the\n",
      "xformer architecture achieved, to our knowledge, the best performance yet reported on\n",
      "the herohe challenge data set (area under the receiver operating characteristic curve\n",
      "[auc], 90%; f1 score, 82%).\n",
      "for coo classiﬁcation in dlbcl, not only did the combined embeddings achieve\n",
      "better performance than the tile-level only embeddings with both the xformer and a-\n",
      "mil architectures (fig. 5) on the ct1 test set and ct2 holdout data set, but they also\n",
      "782\n",
      "j. gildenblat et al.\n",
      "had a signiﬁcant advantage versus tile-only level embeddings in respect of the additional\n",
      "insights they provided through cell-level model explainability (sect. 4.1).\n",
      "fig.\n",
      "5. model performance using the xformer and a-mil architectures for the coo in dlbcl\n",
      "classiﬁcation task.\n",
      "error bars represent 95% conﬁdence intervals computed by a 5000-sample\n",
      "bias-corrected and accelerated bootstrap.\n",
      "4.1\n",
      "model explainability\n",
      "tile-based approaches in dp often use explainability methods such as gradient-weighted\n",
      "classactivationmapping[30]tohighlightpartsoftheimagethatcorrespondwithcertain\n",
      "category outputs.\n",
      "to\n",
      "gain insights into cell-level patterns that were very difﬁcult or impossible to obtain from\n",
      "tile-level only embeddings, we applied an explainability method that assigned attention\n",
      "weights to the cellular average part of the embedding.\n",
      "the cellular average embedding is\n",
      "1\n",
      "n\n",
      "n−1\n",
      "\u0002\n",
      "i=0\n",
      "eij\n",
      "where eij ∈ r256 is the cellular embedding extracted from every detected cell in the tile\n",
      "j\n",
      "\u0003\n",
      "i ∈\n",
      "\u0004\n",
      "1, 2, . .\n",
      ", nj\n",
      "\u0005\u0006\n",
      "where nj is the number of cells in the tile j. this can be rewritten\n",
      "as a weighted average of the cellular embeddings\n",
      "n−1\n",
      "\u0002\n",
      "i=0\n",
      "eijsigmoid(wi)/\n",
      "n−1\n",
      "\u0002\n",
      "i=0\n",
      "sigmoid(wi)\n",
      "where wi ∈ r256 are the per cell attention weights that if initialized to 0 result in the\n",
      "original cellular average embedding.\n",
      "6. cells with positive attention gradients shifted the output towards\n",
      "deep cellular embeddings: an explainable plug and play improvement\n",
      "783\n",
      "a classiﬁcation of tumor and are labeled green.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf:\n",
      "histology analysis of the tumor micro-environment inte-\n",
      "grated with genomic assays is the gold standard for most cancers in\n",
      "modern medicine.\n",
      "this paper proposes a gene-induced multimodal pre-\n",
      "training (gimp) framework, which jointly incorporates genomics and\n",
      "whole slide images (wsis) for classiﬁcation tasks.\n",
      "our work aims at\n",
      "dealing with the main challenges of multi-modality image-omic classi-\n",
      "ﬁcation w.r.t.\n",
      "experimental results\n",
      "on the tcga dataset show the superiority of our network architectures\n",
      "and our pre-training framework, achieving 99.47% in accuracy for image-\n",
      "omic classiﬁcation.\n",
      "keywords: multimodal learning · whole slide image classiﬁcation\n",
      "1\n",
      "introduction\n",
      "pathological image-omic analysis is the cornerstone of modern medicine and\n",
      "demonstrates promise in a variety of diﬀerent tasks such as cancer diagnosis\n",
      "and prognosis [12].\n",
      "with the recent advance of digital pathology and sequencing\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2_49.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43987-2_49\n",
      "gene-induced multimodal pre-training for image-omic classiﬁcation\n",
      "509\n",
      "technologies, modern cancer screening has jointly incorporated genomics and\n",
      "histology analysis of whole slide images (wsis).\n",
      "though deep learning techniques have revolutionized medical imaging, design-\n",
      "ing a task-speciﬁc algorithm for image-omic multi-modality analysis is challeng-\n",
      "ing.\n",
      "(1) the gigapixel wsis, which generally yield 15,000 foreground patches\n",
      "during pre-processing, make attention-based backbones [6] hard to extract pre-\n",
      "cise image (wsi)-level representations.\n",
      "(3) image-omic\n",
      "feature fusion [2,3] may fail to model high-order relevance and the inherent struc-\n",
      "tural characteristics of each modality, making the fusion less eﬀective.\n",
      "[22] via global feature, local feature\n",
      "or multi-granularity alignment.\n",
      "besides, vision-language models in the computer\n",
      "vision community stand out for their remarkable versatility [13,14]. neverthe-\n",
      "less, constrained by computing resources, the most commonly used multimodal\n",
      "representation learning strategy, contrastive learning, which relies on a large\n",
      "number of negative samples to avoid model collapse\n",
      "a big domain gap also hampers their usage in leverag-\n",
      "ing the structural characteristic of tumor micro-environment and genomic assay.\n",
      "recently, the literature corpus has proposed some methods for accomplishing\n",
      "speciﬁc image-omic tasks via kronecker product fusion [2] or co-attention map-\n",
      "ping between wsis and genomics data [3].\n",
      "in this paper, we propose a task-speciﬁc framework dubbed gene-induced\n",
      "multimodal pre-training (gimp) for image-omic classiﬁcation.\n",
      "furthermore, to model the high-order\n",
      "relevance of the two modalities, we combine cls tokens of paired image and\n",
      "genomic data to form uniﬁed representations and propose a triplet learning mod-\n",
      "ule to diﬀerentiate patient-level positive and negative samples in a mini-batch.\n",
      "it is worth mentioning that although our uniﬁed representation fuses features\n",
      "from the whole gene expression cohort and partial wsis in a mini-batch, we\n",
      "510\n",
      "t. jin et al.\n",
      "fig.\n",
      "given a batch of image-omic pairs, we ran-\n",
      "domly select a ﬁxed-length patch cohort and mask parts of the patch embeddings.\n",
      "experimental results demonstrate that our gimp achieves signiﬁ-\n",
      "cant improvement in accuracy than other image-omic competitors, and our mul-\n",
      "timodal framework shows competitive performance even without pre-training.\n",
      "2\n",
      "method\n",
      "given a multimodal dataset d consisting of pairs of wsi pathological images and\n",
      "genomic data (xi, xg), our gimp learns feature representations via accomplish-\n",
      "ing masked patch modeling and triplets learning.\n",
      "-training for image-omic classiﬁcation\n",
      "511\n",
      "2.1\n",
      "group multi-head self attention\n",
      "in this section, we propose group multi-head self attention (groupmsa), a\n",
      "specialized gene encoder to capture structured features in genomic data cohorts.\n",
      "speciﬁcally, inspired by tokenisation techniques in natural language process-\n",
      "ing\n",
      "∈ rnge is partitioned into nf non-\n",
      "overlapping fragments, and we then use a linear projection head to acquire\n",
      "fragment features\n",
      "hf ∈ rnf ×d, where d is the hidden dimension.\n",
      "firstly, the fragment features are divided into groups and there\n",
      "are ngr learnable group tokens linked to each group resulting in (nf/ngr\n",
      "after that, we\n",
      "model cross-group interactions by another msa layer on the global scale with\n",
      "the locally learned group tokens and a ﬁnal classiﬁcation token clsge ∈ rd.\n",
      "finally, groupmsa could learn dense semantics from the genomic data cohort.\n",
      "2.2\n",
      "patch aggregator with eﬃcient attention operation\n",
      "let’s denote the whole slide pathological image with h×w spatial resolution and\n",
      "c channels by xi ∈\n",
      "we follow the preprocessing strategy of clam\n",
      "[11] to acquire patch-level embedding sequence, i.e., each foreground patch with\n",
      "256×256 pixels is fed into an imagenet-pretrained resnet50 and the background\n",
      "region is discarded.\n",
      "∈ r1024\u0003np\n",
      "j=1 denote the sequence of patch\n",
      "embeddings corresponding to wsi xi and note that the total patch number np\n",
      "is image-speciﬁc.\n",
      "[20] to aggre-\n",
      "gate patch embeddings and yield image-level predictions.\n",
      "speciﬁcally, the input\n",
      "sequence hp is ﬁrst embedded into a d-dimensional feature space and combined\n",
      "with a classiﬁcation token clsimg, yielding h0\n",
      "p ∈ r(np+1)×d.\n",
      "in addition, in order to construct the mini-batch, the sub-\n",
      "sequences we intercept in the mpm pre-training phase may not be suﬃciently\n",
      "representative of the image-level characteristics.\n",
      "to overcome these issues, we\n",
      "further propose a gene-induced triplet learning module, which uses pathological\n",
      "images and genomic data as input and extracts high-order and discriminative\n",
      "features via cls tokens.\n",
      "the loss function for optimizing triplet learning is computed\n",
      "by:\n",
      "ltri = max(\n",
      "\n",
      "\n",
      "x − x+\n",
      "\n",
      "2\n",
      "2 + δ −\n",
      "\n",
      "\n",
      "x − x−\n",
      "\n",
      "2\n",
      "2 , 0),\n",
      "(4)\n",
      "gene-induced multimodal pre-training for image-omic classiﬁcation\n",
      "513\n",
      "δ indicates a threshold, e.g., δ = 0.8.\n",
      "applying the pre-trained backbone to image-omic\n",
      "classiﬁcation task is straightforward, since gimp pre-training allows it to learn\n",
      "representative patient-level features.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "experimental setup\n",
      "datasets.\n",
      "among 946 image-omic pairs, 470 of them belong to luad\n",
      "and 476 cases are lusc.\n",
      "implementation details.\n",
      "the pre-training process of all algorithms is con-\n",
      "ducted on the training set, without any extra data augmentation.\n",
      "at last, we measure the performance on the test set.\n",
      "all\n",
      "experiments are conducted on a single nvidia geforce rtx 3090.\n",
      "3.2\n",
      "comparison between gimp and other methods\n",
      "we conduct comparisons between gimp and three competitors under diﬀer-\n",
      "ent settings.\n",
      "(a)\n",
      "image-omic gimp pre-trained, (b) gimp pre-trained without gene inducing, (c) biovil\n",
      "[1] pre-trained, (d) mgca\n",
      "[23],\n",
      "three popular multimodal pre-training algorithms in medical text-image classiﬁ-\n",
      "cation task.\n",
      "even without pre-training stage, gimp\n",
      "shows competitive performance compared to porpoise [4], pathomic fusion\n",
      "[2], and mcat\n",
      "[3], three inﬂuential image-omic classiﬁcation architectures.\n",
      "compari-\n",
      "gene-induced multimodal pre-training for image-omic classiﬁcation\n",
      "515\n",
      "table 2. ablation study on tcga lung cancer dataset.\n",
      "moreover, compared to the mentioned self-supervised methods biovil\n",
      "in the ﬁrst two rows, groupmsa achieves\n",
      "0.53% improvement compared to snn [7], a popular genetic encoders used\n",
      "in porpoise [4] and pathomic fusion [2].\n",
      "“aggregator\n",
      "+ triplet” indicates using unimodal image features to build triplets.\n",
      "we can\n",
      "likewise ﬁnd that the lack of precise global representation leads to worse per-\n",
      "formance.\n",
      "we can observe a\n",
      "performance drop without mpm module, e.g., from 99.47% to 95.26%, which\n",
      "demonstrates that local pathological information is equally critical as high-order\n",
      "relevance.\n",
      "516\n",
      "t. jin et al.\n",
      "4\n",
      "conclusion\n",
      "in this paper, we propose a novel multimodal pre-training method to exploit\n",
      "the complementary relationship of genomic data and pathological images.\n",
      "experimental\n",
      "results demonstrate the superior performance of the proposed gimp compared\n",
      "to other state-of-the-art methods.\n",
      "the contribution of each proposed component\n",
      "of gimp is also demonstrated in the experiments.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf:\n",
      "the utility of machine learning models in histopathology\n",
      "image analysis for disease diagnosis has been extensively studied.\n",
      "while\n",
      "most current techniques utilize small ﬁelds of view (so-called local fea-\n",
      "tures) to link histopathology images to patient outcome, in this work we\n",
      "investigate the combination of global (i.e., contextual) and local features\n",
      "in a graph-based neural network for patient risk stratiﬁcation.\n",
      "we compared the performance of our proposed model against the state-\n",
      "of-the-art (sota) techniques in histopathology risk stratiﬁcation in two\n",
      "cancer datasets.\n",
      "keywords: histopathology · risk assessment · graph processing\n",
      "1\n",
      "introduction\n",
      "the examination of tissue and cells using microscope (referred to as histology)\n",
      "has been a key component of cancer diagnosis and prognostication since more\n",
      "than a hundred years ago.\n",
      "the great rise of deep learning in the past decade and our ability to digitize\n",
      "histopathology slides using high-throughput slide scanners have fueled inter-\n",
      "ests in the applications of deep learning in histopathology image analysis.\n",
      "the\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 74.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "et al.\n",
      "majority of the eﬀorts, so far, focus on the deployment of these models for diag-\n",
      "nosis and classiﬁcation [27].\n",
      "while prognostication and survival analysis oﬀer invaluable insights\n",
      "for patient management, biological studies and drug development eﬀorts, they\n",
      "require careful tracking of patients for a lengthy period of time; rendering this\n",
      "as a task that requires a signiﬁcant amount of eﬀort and funding.\n",
      "in the machine learning domain, patient prognostication can be treated as a\n",
      "weakly supervised problem, which a model would predict the outcome (e.g., time\n",
      "to cancer recurrence) based on the histopathology images.\n",
      "diﬀerent mil variations have shown supe-\n",
      "rior performances in grading or subtype classiﬁcation in comparison to outcome\n",
      "prediction [10].\n",
      "[17] by utilizing mes-\n",
      "sage passing mechanism via edges connecting the nodes (i.e., small patches in our\n",
      "case).\n",
      "while local contexts mainly capture cell-cell\n",
      "interactions, global patterns such as immune cell inﬁltration patterns and tumor\n",
      "invasion in normal tissue structures (e.g., depth of invasion through myometrium\n",
      "in endometrial cancer [1]) could capture critical information about outcome [10].\n",
      "hence, locally focused methods are unable to beneﬁt from the coarse properties\n",
      "of slides due to their high dimensions which may lead to poor performance.\n",
      "the code and graph\n",
      "embeddings are publicly available at https://github.com/pazadimo/all-in\n",
      "2\n",
      "related works\n",
      "2.1\n",
      "weakly supervised learning in histopathology\n",
      "utilizing weakly supervised learning for modeling histopathology problems has\n",
      "been getting popular due to the high resolution of slides and substantial time\n",
      "all-in\n",
      "767\n",
      "and ﬁnancial costs associated with annotating them as well as the development\n",
      "of powerful deep discriminative models in the recent years\n",
      "[24].\n",
      "such models are used to perform nuclei segmentation\n",
      "however, current gnn-based risk\n",
      "assessment variants are only focused on short-range interactions [16,17] or con-\n",
      "sider local contexts\n",
      "we hypothesize that graph-based models’ performance\n",
      "in survival prediction improves by leveraging both ﬁne and coarse properties.\n",
      "below, we have provided\n",
      "details of each module.\n",
      "3.1\n",
      "problem formulation\n",
      "for pn, which is the n-th patient, a set of patches {patchj}m\n",
      "j=1 is extracted\n",
      "from the related whole slide images.\n",
      "it\n",
      "utilizes global and local augmentations of the input patchj and passes them\n",
      "to the student (sθ1,v it ) and teacher (tθ2,v it ) models to ﬁnd their respective\n",
      "768\n",
      "p. azadi et al.\n",
      "fig.\n",
      "c) a graph is constructed and the new local instance-level embeddings\n",
      "are obtained through the message-passing process.\n",
      "by exploiting the message\n",
      "passing mechanism, this module iteratively aggregates features from neighbors\n",
      "of each vertex and generates the new node representations.\n",
      "intuitively, the number of super-nodes\n",
      "k should not be very large or small, as the former encourages them to only\n",
      "represent local clusters and the latter leads to larger clusters and loses subtle\n",
      "all-in\n",
      "769\n",
      "details.\n",
      "also, tr(.) represents the trace\n",
      "of matrix and an,norm is the normalized adjacency matrix.\n",
      "overall, utilizing these two terms encourages the model to extract super-\n",
      "nodes by leaning more towards the strongly associated vertexes and keeping\n",
      "them against weakly connected ones\n",
      "[5], while the main survival loss still controls\n",
      "the global extraction process.\n",
      "3.5\n",
      "fine-coarse distillation\n",
      "we propose our ﬁne-coarse morphological feature distillation module to leverage\n",
      "all-scale interactions in the ﬁnal prediction by ﬁnding a local and a global patient-\n",
      "level representations (ˆhl,n, ˆhg,n).\n",
      "4\n",
      "experiments and results\n",
      "4.1\n",
      "dataset\n",
      "we utilize two prostate cancer (pca) datasets to evaluate the performance of\n",
      "our proposed model.\n",
      "the ﬁrst set (pca-as) includes 179 pca patients who\n",
      "were managed with active surveillance (as).\n",
      "radical therapy is considered\n",
      "overtreatment in these patients, so they are instead monitored with regular\n",
      "serum prostate-speciﬁc antigen (psa) measurements, physical examinations,\n",
      "sequential biopsies, and magnetic resonance imaging [23].\n",
      "this treatment involves placing a\n",
      "radioactive material inside the body to safely deliver larger dose of radiation at\n",
      "all-in\n",
      "771\n",
      "table 1. comparison of our method against baselines and ablation study on policies.\n",
      "we also utilized the prostate cancer grade assessment (panda) challenge\n",
      "dataset [7] that includes more than 10,000 pca needle biopsy slides (no outcome\n",
      "data) as an external dataset for training the encoder of our model.\n",
      "4.2\n",
      "experiments\n",
      "we evaluate the models’ performance in two scenarios utilizing several objective\n",
      "metrics.\n",
      "implementation details are available in supplementary material.\n",
      "patch-gcn [10]) models that were\n",
      "utilized recently for histopathology risk assessment.\n",
      "superior performance of our mca policy implies that balanced exploitation of\n",
      "ﬁne and coarse features with shared weights may provide more robust contex-\n",
      "tual information compared to using mixed guided information or utilizing them\n",
      "independently.\n",
      "we evaluate model performances via kaplan-\n",
      "meier curve [15] (cut-oﬀ set as the ratio of patients with recurrence within 3\n",
      "772\n",
      "p. azadi et al.\n",
      "fig.\n",
      "while\n",
      "none of the baselines are capable of assigning patients into risk groups with\n",
      "statistical signiﬁcance, our distillation policies achieve signiﬁcant separation in\n",
      "both pca-as and pca-bt datasets; suggesting that global histo-morphological\n",
      "properties improve patient stratiﬁcation performance.\n",
      "this group should be managed diﬀerently from\n",
      "the rest of the low-risk prostate cancer patients in the clinic.\n",
      "while a prognostic biomarker provides information\n",
      "about a patient’s outcome (without speciﬁc recommendation on the next course\n",
      "of action), a predictive biomarker gives insights about the eﬀect of a therapeutic\n",
      "intervention and potential actions that can be taken.\n",
      "ablation study.\n",
      "we also assess the impact of our vit on\n",
      "the baselines (full-results in appendix), showing that it can, on average, improve\n",
      "their performance by an increase of ∼ 0.03 in c-index for pca-as.\n",
      "however, the\n",
      "best baseline with vit still has poorer performance compared to our model in\n",
      "both datasets, while the number of parameters (reported for vit embeddings’\n",
      "size in table 1) in our full-model is about half of this baseline.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf:\n",
      "survival outcome assessment is challenging and inherently\n",
      "associated with multiple clinical factors (e.g., imaging and genomics\n",
      "biomarkers) in cancer.\n",
      "we\n",
      "emphasize the unsupervised pretraining to capture the intrinsic interac-\n",
      "tion between tissue microenvironments in gigapixel whole slide images\n",
      "(wsis) and a wide range of genomics data (e.g., mrna-sequence,\n",
      "copy number variant, and methylation).\n",
      "after the multimodal knowl-\n",
      "edge aggregation in pretraining, our task-speciﬁc model ﬁnetuning could\n",
      "expand the scope of data utility applicable to both multi- and single-\n",
      "modal data (e.g., image- or genomics-only).\n",
      "keywords: histopathological image analysis · multimodal learning ·\n",
      "cancer diagnosis · survival prediction\n",
      "1\n",
      "introduction\n",
      "cancers are a group of heterogeneous diseases reﬂecting deep interactions\n",
      "between pathological and genomics variants in tumor tissue environments\n",
      "high-resolution pathological images have\n",
      "proven their unique beneﬁts for improving prognostic biomarkers prediction via\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 60.\n",
      "https://doi.org/10.1007/978-3-031-43987-2_60\n",
      "pathology-and-genomics multimodal transformer for survival prediction\n",
      "623\n",
      "exploring the tissue microenvironmental features\n",
      "despite their impor-\n",
      "tance, seldom eﬀorts jointly exploit the multimodal value between cancer image\n",
      "morphology and molecular biomarkers.\n",
      "the major goal of multimodal data learning is to extract complementary con-\n",
      "textual information across modalities\n",
      "supervised studies [5–7] have allowed\n",
      "multimodal data fusion among image and non-image biomarkers.\n",
      "to alle-\n",
      "viate label requirement, unsupervised learning evaluates the intrinsic similar-\n",
      "ity among multimodal representations for data fusion.\n",
      "for example, integrating\n",
      "image, genomics, and clinical information can be achieved via a predeﬁned unsu-\n",
      "pervised similarity evaluation [4]. to broaden the data utility, the study\n",
      "[28]\n",
      "leverages the pathology and genomic knowledge from the teacher model to guide\n",
      "the pathology-only student model for glioma grading.\n",
      "meanwhile, the size of multimodal medical\n",
      "datasets is not as large as natural vision-language datasets, which necessitates\n",
      "the need for data-eﬃcient analytics to address the training diﬃculty.\n",
      "to tackle above challenges, we propose a pathology-and-genomics multimodal\n",
      "framework (i.e., pathomics) for survival prediction (fig. 1).\n",
      "to overcome the gap of modality\n",
      "heterogeneity between images and genomics, we project the multimodal embed-\n",
      "dings into the same latent space by evaluating the similarity among them.\n",
      "as a result, the task-speciﬁc ﬁnetuning broadens the dataset\n",
      "usage (fig 1b and c), which is not limited by data modality (e.g., both single-\n",
      "and multi-modal data).\n",
      "our\n",
      "approach could achieve comparable performance even with fewer ﬁnetuned data\n",
      "(e.g., only use 50% of the ﬁnetuned data) when compared with using the entire\n",
      "ﬁnetuning dataset.\n",
      "1a, in the pretraining, our unsu-\n",
      "pervised data fusion aims to capture the interaction pattern of image and\n",
      "genomics features.\n",
      "overall, we formulate the objective of multimodal feature\n",
      "learning by converting image patches and tabular genomics data into group-\n",
      "wise embeddings, and then extracting multimodal patient-wise embeddings.\n",
      "more speciﬁcally, we construct group-wise representations for both image and\n",
      "genomics modalities.\n",
      "for image feature representation, we randomly divide image\n",
      "patches into groups; meanwhile, for each type of genomics data, we construct\n",
      "groups of genes depending on their clinical relevance\n",
      "1b\n",
      "and c, our approach enables three types of ﬁnetuning modal modes (i.e., multi-\n",
      "modal, image-only, and genomics-only) towards prognostic prediction, expanding\n",
      "the downstream data utility from the pretrained model.\n",
      "fig.\n",
      "in (a), we show the pipeline of extracting image\n",
      "and genomics feature embedding via an unsupervised pretraining towards multimodal\n",
      "data fusion.\n",
      "group-wise image and genomics embedding.\n",
      "the group-wise genomics representation is\n",
      "deﬁned as gn ∈ r1×dg, where n ∈ n, dg is the attribute dimension in each group\n",
      "which could be various.\n",
      "to better extract high-dimensional group-wise genomics\n",
      "representation, we use a self-normalizing network (snn) together with scaled\n",
      "exponential linear units (selu) and alpha dropout for feature extraction to\n",
      "generate the group-wise embedding gn ∈ r1×256 for each group.\n",
      "for group-wise wsis representation, we ﬁrst cropped all tissue-region image\n",
      "tiles from the entire wsi and extracted cnn-based (e.g., resnet50)\n",
      "di-\n",
      "dimensional features for each image tile k as hk ∈ r1×di, where di = 1, 024,\n",
      "k ∈ k and k is the number of image patches.\n",
      "we construct the group-wise wsis\n",
      "representation by randomly splitting image tile features into n groups (i.e., the\n",
      "same number as genomics categories).\n",
      "therefore, group-wise image representa-\n",
      "tion could be deﬁned as in ∈ rkn×1024, where n ∈ n and kn represents tile\n",
      "k in group\n",
      "[17], which is\n",
      "able to weight the feature embeddings in the group, together with a dimension\n",
      "deduction (e.g., fully-connected layers) to achieve the group-wise embedding.\n",
      "1a, we propose a pathology-and-genomics multimodal model containing\n",
      "two model streams, including a pathological image and a genomics data stream.\n",
      "in the pathological image stream,\n",
      "the patient-wise image representation is aggregated by n group representations\n",
      "as ip ∈ rn×256, where p ∈ p and p is the number of patients.\n",
      "due to the domain\n",
      "gap between image and molecular feature heterogeneity, a proper design of\n",
      "multimodal fusion is crucial to advance integrative analysis.\n",
      "in the pretrain-\n",
      "ing stage, we develop an unsupervised data fusion strategy by decreasing the\n",
      "mean square error (mse) loss to map images and genomics embeddings into\n",
      "the same space.\n",
      "ideally, the image and genomics embeddings belonging to the\n",
      "same patient should have a higher relevance between each other.\n",
      "mse measures\n",
      "the average squared diﬀerence between multimodal embeddings.\n",
      "in this way, the\n",
      "pretrained model is trained to map the paired image and genomics embeddings\n",
      "to be closer in the latent space, leading to strengthen the interaction between\n",
      "diﬀerent modalities.\n",
      "lfusion = argmin 1\n",
      "p\n",
      "p\n",
      "\u0003\n",
      "p=1\n",
      "((ip\n",
      "embedding − gp\n",
      "embedding)2)\n",
      "(4)\n",
      "in the single modality ﬁnetuning, even if we use image-only data, the model is\n",
      "able to produce genomic-related image feature embedding due to the multimodal\n",
      "knowledge aggregation already obtained from the model pretraining.\n",
      "as a result,\n",
      "our cross-modal information aggregation relaxes the modality requirement in the\n",
      "ﬁnetuning stage.\n",
      "1b, for multimodal ﬁnetuning, we deploy a\n",
      "concatenation layer to obtain the fused multimodal feature representation and\n",
      "implement a risk classiﬁer (fc layer) to achieve the ﬁnal survival stratiﬁcation\n",
      "(see appendix 2).\n",
      "3\n",
      "experiments and results\n",
      "datasets.\n",
      "all image and genomics data are publicly available.\n",
      "experimental settings and implementations.\n",
      "we implement two types of\n",
      "settings that involve internal and external datasets for model pretraining and\n",
      "ﬁnetuning.\n",
      "then, we implement four-fold cross-validation on the\n",
      "pathology-and-genomics multimodal transformer for survival prediction\n",
      "627\n",
      "fig.\n",
      "2. dataset usage.\n",
      "for the external setting, we implement pretraining and ﬁnetuning on the\n",
      "diﬀerent datasets, as shown in fig 2b; we use tcga-coad for pretraining;\n",
      "then, we only use tcga-read for ﬁnetuning and ﬁnal evaluation.\n",
      "we imple-\n",
      "ment a ﬁve-fold cross-validation for pretraining, and the best pretrained models\n",
      "are used for ﬁnetuning.\n",
      "for all experiments, we calculate the average\n",
      "performance on the evaluation set across the best models.\n",
      "the concordance index (c-index) is used to measure the survival prediction\n",
      "performance.\n",
      "for each experiment, we reported the average c-index among\n",
      "three-times repeated experiments.\n",
      "in table 1, our approach shows improved survival prediction per-\n",
      "formance on both tcga-coad and tcga-read datasets.\n",
      "we recognize that the combination of image and mrna\n",
      "sequencing data leads to reﬂecting distinguishing survival outcomes.\n",
      "in the meantime,\n",
      "on the tcga-read, our single-modality ﬁnetuned model achieves a better\n",
      "performance than multimodal ﬁnetuned baseline models (e.g., with model pre-\n",
      "training via image and methylation data, we have only used the image data for\n",
      "ﬁnetuning and achieved a c-index of 74.85%, which is about 4% higher than the\n",
      "best baseline models).\n",
      "we show that with a single-modal ﬁnetuning strategy, the\n",
      "model could generate meaningful embedding to combine image- and genomic-\n",
      "related patterns.\n",
      "in table 1, our method could\n",
      "yield better performance compared with baselines on the small dataset across\n",
      "the combination of images and multiple types of genomics data.\n",
      "table 1.\n",
      "the comparison of c-index performance on tcga-coad and tcga-read\n",
      "dataset.\n",
      "[30]\n",
      "image+mrna\n",
      "-\n",
      "58.70 ± 1.10\n",
      "image+mrna\n",
      "70.19 ± 1.45\n",
      "image+cna\n",
      "–\n",
      "51.50 ± 2.60\n",
      "image+cna\n",
      "62.50 ± 2.52\n",
      "image+methy\n",
      "–\n",
      "65.61 ± 1.86\n",
      "image+methy\n",
      "55.78 ± 1.22\n",
      "ab-mil\n",
      "[17]\n",
      "image+mrna\n",
      "–\n",
      "54.12 ± 2.88\n",
      "image+mrna\n",
      "68.79 ± 1.44\n",
      "image+cna\n",
      "–\n",
      "54.68 ± 2.44\n",
      "image+cna\n",
      "66.72 ± 0.81\n",
      "image+methy\n",
      "–\n",
      "49.66 ± 1.58\n",
      "image+methy\n",
      "55.78 ± 1.22\n",
      "transmil\n",
      "[26]\n",
      "image+mrna\n",
      "–\n",
      "54.15 ± 1.02\n",
      "image+mrna\n",
      "67.91 ± 2.35\n",
      "image+cna\n",
      "–\n",
      "59.80 ± 0.98\n",
      "image+cna\n",
      "62.75 ± 1.92\n",
      "image+methy\n",
      "–\n",
      "53.35 ± 1.78\n",
      "image+methy\n",
      "53.09 ± 1.46\n",
      "mcat\n",
      "[6]\n",
      "image+mrna\n",
      "-\n",
      "65.02 ± 3.10\n",
      "image+mrna\n",
      "70.27 ± 2.75\n",
      "image+cna\n",
      "–\n",
      "64.66 ± 2.31\n",
      "image+cna\n",
      "60.50 ± 1.25\n",
      "image+methy\n",
      "–\n",
      "60.98 ± 2.43\n",
      "image+methy\n",
      "59.78 ± 1.20\n",
      "porpoi-se [7] image+mrna\n",
      "–\n",
      "65.31 ± 1.26\n",
      "image+mrna\n",
      "68.18 ± 1.62\n",
      "image+cna\n",
      "–\n",
      "57.32 ± 1.78\n",
      "image+cna\n",
      "60.19 ± 1.48\n",
      "image+methy\n",
      "–\n",
      "61.84 ± 1.10\n",
      "image+methy\n",
      "68.80 ± 0.92\n",
      "ours\n",
      "image+mrna\n",
      "image+mrna\n",
      "67.32 ± 1.69 image+mrna\n",
      "74.35 ± 1.15\n",
      "image\n",
      "63.78 ± 1.22\n",
      "image\n",
      "74.85 ± 0.37\n",
      "mrna\n",
      "60.76 ± 0.88\n",
      "mrna\n",
      "59.61 ± 1.37\n",
      "image+cna\n",
      "image+cna\n",
      "61.19 ± 1.03\n",
      "image+cna\n",
      "73.95 ± 1.05\n",
      "image\n",
      "58.06 ± 1.54\n",
      "image\n",
      "71.18 ± 1.39\n",
      "cna\n",
      "56.43 ± 1.02\n",
      "cna\n",
      "63.95 ± 0.55\n",
      "image+methy\n",
      "image+methy\n",
      "67.22 ± 1.67\n",
      "image+methy\n",
      "71.80 ± 2.03\n",
      "image\n",
      "60.43 ± 0.72\n",
      "image\n",
      "64.42 ± 0.72\n",
      "methy\n",
      "61.06 ± 1.34\n",
      "methy\n",
      "65.42 ± 0.91\n",
      "pathology-and-genomics multimodal transformer for survival prediction\n",
      "629\n",
      "finetuning data ratio\n",
      "our proposed method\n",
      "average of baselines\n",
      "image and mrna data\n",
      "image and cna data\n",
      "image and methylation \n",
      "data\n",
      "line color\n",
      "marker on the line\n",
      "a. data efficiency evaluation on tcga-coad\n",
      "b. data efficiency evaluation on tcga-read\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "finetuning data ratio\n",
      "70\n",
      "65\n",
      "60\n",
      "55\n",
      "50\n",
      "45\n",
      "40\n",
      "c-index\n",
      "75\n",
      "70\n",
      "65\n",
      "60\n",
      "55\n",
      "c-index\n",
      "fig.\n",
      "we show the average\n",
      "c-index of baselines, the detailed results are shown in the appendix 3.2.\n",
      "ablation analysis.\n",
      "3a, by using 50% of tcga-coad ﬁnetuning data, our approach\n",
      "achieves the c-index of 64.80%, which is higher than the average performance\n",
      "of baselines in several modalities.\n",
      "3b, our model retains a good\n",
      "performance by using 50% or 75% of tcga-read ﬁnetuning data compared\n",
      "with the average of c-index across baselines (e.g., 72.32% versus 64.23%).\n",
      "for\n",
      "evaluating the eﬀect of cross-modality information extraction in the pretraining,\n",
      "we kept supervised model training (i.e., the ﬁnetuning stage) while removing\n",
      "the unsupervised pretraining.\n",
      "the performance is lower 2%-10% than ours on\n",
      "multi- and single-modality data.\n",
      "for evaluating the genomics data usage, we\n",
      "designed two settings: (1) combining all types of genomics data and categorizing\n",
      "them by groups; (2) removing category information while keeping using diﬀerent\n",
      "types of genomics data separately.\n",
      "4\n",
      "conclusion\n",
      "developing data-eﬃcient multimodal learning is crucial to advance the survival\n",
      "assessment of cancer patients in a variety of clinical data scenarios.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf:\n",
      "whole slide image (wsi) classiﬁcation is an essential task in com-\n",
      "putational pathology.\n",
      "the experimental results on two public wsi datasets demonstrate\n",
      "that the pro-posed mspt outperforms all the compared algorithms, suggesting its\n",
      "potential applications.\n",
      "keywords: whole slide image · multiple instance learning · multi-scale\n",
      "feature · prototypical transformer\n",
      "1\n",
      "introduction\n",
      "histopathological images are regarded as the ‘gold standard’ in the diagnosis of cancers.\n",
      "with the advent of the whole slide image (wsi) scanner, deep learning has gained its\n",
      "reputation in the ﬁeld of computational pathology [1–3].\n",
      "in this\n",
      "context, a wsi is considered as a bag, and the cropped patches within the slide are the\n",
      "supplementary information the online version contains supplementary material available at\n",
      "https://doi.org/10.1007/978-3-031-43987-2_58.\n",
      "however, since the average bag size of\n",
      "a wsi is more than 8000 at 20 × magniﬁcation, it is computationally infeasible to use\n",
      "the conventional transformer and other stacked self-attention network architectures in\n",
      "mil-related tasks.\n",
      "these clustering-based mil algorithms can signiﬁcantly\n",
      "reduce the redundant instances, and thereby improving the training efﬁciency for wsi\n",
      "classiﬁcation.however,itisdifferentfork-meanstospecifytheclusternumberaswellas\n",
      "the initial cluster centers, and different initial values may lead to different cluster results,\n",
      "thus affecting the performance of mil.\n",
      "therefore, it is necessary to develop a method that can\n",
      "fully exploit the potential complementary information between critical instances and\n",
      "prototypes to improve representation learning of prototypes.\n",
      "on the other hand, when pathologists analysis the wsis, they always observe the\n",
      "tissues at various resolutions [17]. inspired by this diagnostic manner, some works use\n",
      "multi-scaleinformationofwsistoimprovediagnosticaccuracy.forexample,lietal.[9]\n",
      "adopted a pyramidal concatenation mechanism to fuse the multi-scale features of wsis,\n",
      "in which the feature vectors of low-resolution patches are replicated and concatenated\n",
      "with the those of their corresponding high-resolution patches; hou et al.\n",
      "the mlp-mixer adopts two types of mlp layers to\n",
      "allow information communication in different dimensions of data.\n",
      "it can effec-\n",
      "tively capture multi-scale information in wsi to improve the performance of wsi\n",
      "classiﬁcation.\n",
      "it is a time consuming and tedious task for pathologists to annotate\n",
      "the patch-level labels in gigapixel wsis, thus, a common practice is to use a pre-trained\n",
      "encoder network to extract instance-level features, such as an imagenet pre-trained\n",
      "encoder or a self-supervised pre-trained encoder.\n",
      "1, the optimization\n",
      "process can be divided into two steps: 1) the initial cluster prototype bag pbag is obtained\n",
      "in the pre-processing stage by using the k-means clustering on xbag; ; 2) pt uses xbag\n",
      "to optimize pbag via the self-attention mechanism in transformer.\n",
      "these attention scores are\n",
      "then weighted to xbag to update the pk ∈ r1×dk for completing the calibration of the\n",
      "clustering prototypes ˆp ∈ rk×dk.\n",
      "606\n",
      "s. ding et al.\n",
      "as mentioned above, existing clustering-based mil methods use the k-means clus-\n",
      "tering to identify instances prototypes in the bag, where the most important instances\n",
      "that contain the key semantic information may be ignored.\n",
      "speciﬁcally, the procedure of mffm is described as follows:\n",
      "we ﬁrst perform the feature concatenation operation on the multi-scale output\n",
      "clustering prototypes\n",
      "\n",
      "ˆp20×, ˆp10×, ˆp5×\n",
      "\u000b\n",
      "to construct a feature pyramid\n",
      "⌣p:\n",
      "concat\n",
      "\n",
      "ˆp20×, ˆp10×, ˆp5×\n",
      "\u000b\n",
      "→\n",
      "⌣p ∈ rk×3dk\n",
      "(3)\n",
      "where dk is the feature vector dimension of the prototypes.\n",
      "= h t\n",
      "1 + w4σ\n",
      "\u0004\n",
      "w3ln\n",
      "\u0004\n",
      "h t\n",
      "1\n",
      "\u0005\u0005\n",
      "(4)\n",
      "where ln denotes the layer normalization, σ denotes the activation function imple-\n",
      "mented by gelu, w1 ∈ rk×c, w2 ∈ rc×k, w3 ∈ r3dk×dsandw4 ∈ rds×3dk are the\n",
      "weight matrices of mlp layers.c and ds are tunable hidden widths in the token-mixing\n",
      "and channel-mixing mlp, respectively.\n",
      "finally, the h is fed to the gated attention pooling (gap)\n",
      "3\n",
      "experiments and results\n",
      "3.1\n",
      "datasets\n",
      "to evaluate the effectiveness of mspt, we conducted experiments on two public dataset,\n",
      "namely camelyon16 [24] and tcga-nsclc.\n",
      "after pre-processing, a total of 2.4 million patches at\n",
      "×20 magniﬁcation, 0.56 million patches at ×10 magniﬁcation, and 0.16 million patches\n",
      "at ×5 magniﬁcation, with an average of about 5900, 1400, and 400 patches per bag.\n",
      "the dataset yields 4.3 million patches at 20× magniﬁcation, 1.1\n",
      "million patches at 10× magniﬁcation, and 0.30 million patches at 5× magniﬁcation with\n",
      "an average of about 5000, 1200, and 350 patches per bag.\n",
      "3.2\n",
      "experiment setup and evaluation metrics\n",
      "in wsi pre-processing, each slide is cropped into non-overlapping 256 × 256 patches\n",
      "at different magniﬁcations, and a threshold is set to ﬁlter out background ones.\n",
      "after\n",
      "patching, we use a pre-trained resnet18 model to convert each 256 × 256 patch into\n",
      "a 512- dimensional feature vector.\n",
      "608\n",
      "s. ding et al.\n",
      "3.3\n",
      "implementation details\n",
      "for the feature extractor, we employed the simclr encoder trained by lee et al.\n",
      "all models were implemented by python 3.8 with pytorch toolkit 1.11.0\n",
      "on a platform equipped with an nvidia geforce rtx 3090 gpu.\n",
      "3.4\n",
      "comparisons experiment\n",
      "comparison algorithms.\n",
      "[11]\n",
      "0.9225\n",
      "0.9734\n",
      "0.9095 ± 0.014\n",
      "0.9432 ± 0.016\n",
      "remix [16]\n",
      "0.9458\n",
      "0.9740\n",
      "0.9167 ± 0.013\n",
      "0.9509 ± 0.016\n",
      "pt (ours)\n",
      "0.9458\n",
      "0.9809\n",
      "0.9257 ± 0.011\n",
      "0.9567 ± 0.013\n",
      "mspt (ours)\n",
      "0.9536\n",
      "0.9869\n",
      "0.9289 ± 0.011\n",
      "0.9622 ± 0.015\n",
      "experimental results.\n",
      "it achieves the best classiﬁcation performance of 0.9289\n",
      "multi-scale prototypical transformer\n",
      "609\n",
      "± 0.011 and 0.9622 ± 0.015 on the acc and auc.\n",
      "in the camelyon16 dataset, the performance\n",
      "of both pt and prototype-bag increases with the increase of k value, and achieves the\n",
      "best results with k = 16.\n",
      "these experimental results demonstrate that pt can effectively\n",
      "re-calibrate the clustering prototypes to achieve superior results.\n",
      "compared\n",
      "with other multi-scale variants, the proposed mspt improves acc by at least 0.78%\n",
      "and 0.85% on camelyon16 and tcga-nsclc, respectively, which proves that the\n",
      "mlp-mixer in mffm can effectively enhance the information communication among\n",
      "phenotypes and their features, thus improving the performance of feature aggregation.\n",
      "we provide more empirical studies, i.e., the effect of the multi-\n",
      "resolution scheme, the visualization results, and the training budgets, in supplementary\n",
      "materials to better understand mspt.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf:\n",
      "in pathological image analysis, determination of gland mor-\n",
      "phology in histology images of the colon is essential to determine\n",
      "the grade of colon cancer.\n",
      "however, manual segmentation of glands is\n",
      "extremely challenging and there is a need to develop automatic methods\n",
      "for segmenting gland instances.\n",
      "recently, due to the powerful noise-to-\n",
      "image denoising pipeline, the diﬀusion model has become one of the hot\n",
      "spots in computer vision research and has been explored in the ﬁeld of\n",
      "image segmentation.\n",
      "in this paper, we propose an instance segmenta-\n",
      "tion method based on the diﬀusion model that can perform automatic\n",
      "gland instance segmentation.\n",
      "firstly, we model the instance segmenta-\n",
      "tion process for colon histology images as a denoising process based on\n",
      "the diﬀusion model.\n",
      "thirdly, to improve\n",
      "the distinction between the object and the background, we apply con-\n",
      "ditional encoding to enhance the intermediate features with the original\n",
      "image encoding.\n",
      "to objectively validate the proposed method, we com-\n",
      "pared state-of-the-art deep learning model on the 2015 miccai gland\n",
      "segmentation challenge (glas) dataset and the colorectal adenocarci-\n",
      "noma gland (crag) dataset.\n",
      "the experimental results show that our\n",
      "method improves the accuracy of segmentation and proves the eﬃcacy\n",
      "of the method.\n",
      "gland segmentation · diﬀusion model · colon histology\n",
      "images\n",
      "1\n",
      "introduction\n",
      "colorectal cancer is a prevalent form of cancer characterized by colorectal ade-\n",
      "nocarcinoma, which develops in the colon or rectum’s inner lining and exhibits\n",
      "glandular structures\n",
      "https://doi.org/10.1007/978-3-031-43987-2_64\n",
      "instance-aware diﬀusion model for gland segmentation\n",
      "663\n",
      "gland formation is a crucial factor in determining tumor grade and diﬀeren-\n",
      "tiation.\n",
      "accurate segmentation of glandular instances on histological images is\n",
      "essential for evaluating glandular morphology and assessing colorectal adeno-\n",
      "carcinoma malignancy.\n",
      "however, manual annotation of glandular instances is a\n",
      "time-consuming and expertise-demanding process.\n",
      "hence, automated methods\n",
      "for glandular instance segmentation hold signiﬁcant value in clinical practice.\n",
      "fig.\n",
      "1. (a–b) example images from the crag dataset.\n",
      "(c–d) example images from\n",
      "the glas dataset.\n",
      "automated segmentation has been explored using deep learning techniques\n",
      "[21,33], including u-net [17], fcn\n",
      "[10,11] and their vari-\n",
      "ations for semantic segmentation\n",
      "there are also methods that combine\n",
      "information bottleneck for detection and segmentation [23].\n",
      "additionally, two-\n",
      "stage instance segmentation methods like mask r-cnn\n",
      "[3]\n",
      "have been utilized, combining object detection and segmentation sub-networks.\n",
      "limitations arise from\n",
      "image scaling and cropping, leading to information loss or distortion, resulting\n",
      "in ineﬀective boundary recognition and over-/under-segmentation.\n",
      "to overcome\n",
      "these limitations, we aim to perform gland instance segmentation to accurately\n",
      "identify the target location and prevent misclassiﬁcation of background tissue.\n",
      "in the task of image synthesis, diﬀusion model has evolved to achieve\n",
      "state-of-the-art performance in terms of quality and mode coverage compared\n",
      "with gan [32].\n",
      "[4] treats the object detection task as a generative task on\n",
      "the bounding box space in images to handle projection detection.\n",
      "several studies\n",
      "have explored the feasibility of using diﬀusion model in image segmentation [26].\n",
      "these methods generate segmentation maps from noisy images and demonstrate\n",
      "better representation of segmentation details compared to previous deep learning\n",
      "methods.\n",
      "in this paper, we propose a new method for gland instance segmentation\n",
      "based on the diﬀusion model.\n",
      "(1) our method utilizes a diﬀusion model to per-\n",
      "form denoising and tackle the task of gland instance segmentation in histology\n",
      "images.\n",
      "the noise boxes are generated from gaussian noise, and the predicted\n",
      "ground truth (gt) boxes and segmentation masks are performed during the dif-\n",
      "fusion process.\n",
      "(2) to improve segmentation, we use instance-aware techniques\n",
      "664\n",
      "m. sun et al.\n",
      "to recover lost details during denoising.\n",
      "this includes employing a ﬁlter and a\n",
      "multi-scale mask branch to create a global mask and reﬁne ﬁner segmentation\n",
      "details.\n",
      "(3) to enhance object-background diﬀerentiation, we utilize conditional\n",
      "encoding to augment intermediate features with the original image encoding.\n",
      "this method eﬀectively integrates the abundant information from the original\n",
      "image, thereby enhancing the distinction between the objects and the surround-\n",
      "ing background.\n",
      "our proposed method was trained and tested on the 2015 mic-\n",
      "cai gland segmentation (glas) challenge dataset [20] and colorectal adeno-\n",
      "carcinoma gland (crag) dataset\n",
      "[6] (as shown in fig. 1), and the experiment\n",
      "results demonstrate the eﬃcacy of the method.\n",
      "the image encoder consists of\n",
      "a backbone that extracts multi-scale features from the input image.\n",
      "the image decoder\n",
      "based on a diﬀusion model incorporates the original image features as conditions to\n",
      "enhance the intermediate features.\n",
      "2\n",
      "method\n",
      "in this section, we present the architecture of our proposed method, which\n",
      "includes an image encoder, an image decoder, and a mask branch.\n",
      "2.\n",
      "2.1\n",
      "image encoder\n",
      "we propose to perform subsequent operations on the features of the original\n",
      "image, so we use an image encoder for advanced feature extraction.\n",
      "the image\n",
      "encoder takes the original image as input and we use a convolutional neural\n",
      "network such as resnet [8] for feature extraction and a feaure pyramid network\n",
      "instance-aware diﬀusion model for gland segmentation\n",
      "665\n",
      "(fpn)\n",
      "the input image is x and the output is a high-level feature fr.\n",
      "the image encoder operates only once and uses the\n",
      "fr as condition to progressively reﬁne and generate predictions from the noisy\n",
      "boxes.\n",
      "2.2\n",
      "image decoder\n",
      "we designed our model based on the diﬀusion model\n",
      "∈ (0, 1), t ∈ {1, ..., t} determines the amount of noise\n",
      "that is introduced at each stage.\n",
      "our image decoder is based on diﬀusion model, which can be viewed as a\n",
      "noise-to-gt denoising process.\n",
      "the neural network fθ(zt, t) is trained to predict z0 from the zt based on\n",
      "the corresponding image x. in addition, to achieve complementary information\n",
      "by integrating the segmentation information from zt into the original image\n",
      "encoding, we introduce conditional encoding, which uses the encoding features\n",
      "of the current step to enhance its intermediate features.\n",
      "in this stage, we use the mask branch to fuse the diﬀerent scale information of\n",
      "the fpn and output the mask feature fmask.\n",
      "the diﬀusion process decodes roi\n",
      "features into local masks, and multi-scale features can be supplemented with\n",
      "more detailed information for predicting global masks to compensate for the\n",
      "detail lost in the diﬀusion process, and we believe that instance masks require\n",
      "a larger perceptual domain because of the higher demands on instance edges.\n",
      "the optimal value for the parameter γ is usually determined based\n",
      "on achieving the best overall performance on the validation set.\n",
      "3\n",
      "experiments and results\n",
      "we presented the segmentation results of our model compared to the ground\n",
      "truth in fig. 3, and provided both qualitative and quantitative evaluations that\n",
      "validate the eﬀectiveness of our proposed network for gland instance segmenta-\n",
      "tion.\n",
      "data and evaluation metrics: we evaluated the eﬀectiveness of the pro-\n",
      "posed model on two datasets: the glas dataset and the crag dataset.\n",
      "the\n",
      "glas dataset comprises 85 training and 80 testing images, divided into 60 images\n",
      "in test a and 20 images in test b.\n",
      "the crag dataset consists of 173 training\n",
      "and 40 testing images.\n",
      "furthermore, to enhance the training dataset and mitigate the risk\n",
      "of overﬁtting, we employed random combinations of image ﬂipping, translation,\n",
      "gaussian blur, brightness variation, and other augmentation techniques.\n",
      "we assessed the segmentation results using three metrics from the glas chal-\n",
      "lenge: (1) object f1, which measures the accuracy of detecting individual glands,\n",
      "instance-aware diﬀusion model for gland segmentation\n",
      "667\n",
      "fig.\n",
      "the instance segmentation results on the glas dataset and crag dataset.\n",
      "from top to bottom: the original images, the ground truth, and the segmentation\n",
      "results produced by our method.\n",
      "(2) object dice, which evaluates the volume-based accuracy of gland segmen-\n",
      "tation, and (3) object hausdorﬀ, which assesses the shape similarity between\n",
      "the segmentation result and the ground truth.\n",
      "we assigned each method three\n",
      "ranking numbers based on these metrics and computed their sum to determine\n",
      "the ﬁnal ranking for each method’s overall performance.\n",
      "implementation details: in our experiments, we choose the resnet-50 with\n",
      "fpn as the backbone in the proposed method.\n",
      "the backbone is pretrained on\n",
      "imagenet.\n",
      "image decoder, mask branch and mask fcn head are trained end-to-\n",
      "end.\n",
      "we trained on the glas and crag datasets in a python 3.8.3 environment\n",
      "on ubuntu 18.04, using pytorch 1.10 and cuda 11.4.\n",
      "= 0.02. training was performed on a100 gpu with a\n",
      "batch size of 2.\n",
      "results on the glas challenge dataset: we conducted experiments to\n",
      "evaluate the performance of our proposed model by comparing it with the dse\n",
      "model\n",
      "[6], the gcsba-net [25], and the mpcnn [19]. table 1\n",
      "provides an overview of the average performance of these models.\n",
      "our proposed model demonstrated a enhancement in performance, surpass-\n",
      "ing the second-best method on both test a and test b datasets.\n",
      "speciﬁcally,\n",
      "on test a, we observed an improvement of 0.006, 0.01, and 1.793 in object\n",
      "f1, object dice, and object hausdorf.\n",
      "similarly, on test b, resulting in an\n",
      "improvement of 0.022, 0.014 and 3.694 in object f1, object dice, and object\n",
      "hausdorf, respectively.\n",
      "although test b presented a more challenging task due to\n",
      "the presence of complex morphology in the images, our proposed model demon-\n",
      "strated accurate segmentation in all cases.\n",
      "the experimental results highlighted\n",
      "the eﬀectiveness of our approach in improving the accuracy of gland instance\n",
      "segmentation.\n",
      "668\n",
      "m. sun et al.\n",
      "results on the crag dataset: the proposed model was additionally evalu-\n",
      "ated on the crag dataset by comparing it against the gcsba-net, doubleu-\n",
      "net, dse model, mild-net, and dcan.\n",
      "the average performance of these\n",
      "models is shown in table 2.\n",
      "our experimental results demonstrate that our pro-\n",
      "posed method achieves superior performance, with improvements of 0.017, 0.012,\n",
      "and 4.026 for object f1, object dice, and object hausdorﬀ, respectively, com-\n",
      "pared to the second-best method.\n",
      "these results demonstrate the eﬀectiveness of\n",
      "our method in segmenting diﬀerent datasets.\n",
      "ablation studies: our network utilizes the mask branch and conditional\n",
      "encoding to enhance performance and segmentation quality.\n",
      "the mask branch is responsible for multi-scale feature extraction and\n",
      "fusion with the backbone network, as well as reﬁning the image decoder’s out-\n",
      "put.\n",
      "without the mask branch, direct usage of original image features lacks\n",
      "multi-scale information and results in less accurate segmentation.\n",
      "conditional\n",
      "encoding is employed to establish a connection between input image features\n",
      "table 1.\n",
      "the experimental results on glas challenge dataset.\n",
      "the experimental results on the crag dataset.\n",
      "[2]\n",
      "0.736 5\n",
      "0.794 6\n",
      "218.76\n",
      "6\n",
      "17\n",
      "instance-aware diﬀusion model for gland segmentation\n",
      "669\n",
      "and the diﬀusion model.\n",
      "when employing mask branch, our approach resulted in\n",
      "an improvement of 0.082, 0.09, 0.07 in object f1, and 0.07, 0.078, 0.07 in object\n",
      "dice, while object hausdorﬀ decreased by 10.29, 11.11, 24.47 on glas test a,\n",
      "glas test b, and crag, respectively.\n",
      "similarly, by utilizing conditional encod-\n",
      "ing, we observed an improvement of 0.048, 0.034, 0.052 in object f1, and 0.026,\n",
      "0.042, 0.057 in object dice, while object hausdorﬀ decreased by 6.771, 8.115,\n",
      "12.141 on glas test a, glas test b, and crag, respectively.\n",
      "table 3.\n",
      "the ablation study results on the crag and glas datasets demonstrate the\n",
      "impact of diﬀerent modules on performance.\n",
      "the mask branch module contributes to\n",
      "multi-scale feature extraction, while the conditional encoding module establishes the\n",
      "connection between input image features and the diﬀusion model.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf:\n",
      "in computational pathology, nuclei segmentation from his-\n",
      "tology images is a fundamental task.\n",
      "while deep learning based nuclei\n",
      "segmentation methods yield excellent results, they rely on a large amount\n",
      "of annotated images; however, annotating nuclei from histology images\n",
      "is tedious and time-consuming.\n",
      "to get rid of labeling burden completely,\n",
      "we propose a label-free approach for nuclei segmentation, motivated\n",
      "from one pronounced yet omitted property that characterizes histology\n",
      "images and nuclei: intra-image self similarity (iiss), that is, within\n",
      "an image, nuclei are similar in their shapes and appearances.\n",
      "first, we\n",
      "leverage traditional machine learning and image processing techniques\n",
      "to generate a pseudo segmentation map, whose connected components\n",
      "form candidate nuclei, both positive or negative.\n",
      "finally, we apply the learned u-net to produce ﬁnal nuclei segmenta-\n",
      "tion.\n",
      "experimental results demonstrate the eﬀectiveness of our design and,\n",
      "to the best of our knowledge, it achieves the state-of-the-art per-\n",
      "formances of label-free segmentation on the benchmark monuseg\n",
      "dataset with a mean dice score of 79.2%.\n",
      "keywords: label-free · nuclei segmentation · pseudo label\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "(color ﬁgure\n",
      "online)\n",
      "1\n",
      "introduction\n",
      "nuclei segmentation is a fundamental step in histology image analysis.\n",
      "however, accurate pixel-level\n",
      "annotation of nuclei is not always accessible for segmentation labeling is a labor-\n",
      "intensive and time-consuming procedure.\n",
      "unsupervised learning (ul) methods achieved great success in the data\n",
      "dependency problem for nuclei segmentation, which learns from the structural\n",
      "properties in the data without any manual annotations.\n",
      "traditional ul nuclei segmenta-\n",
      "tion methods include watershed [6], contour detection\n",
      "these methods focus on either pixel value or shape informa-\n",
      "tion but fail to take advantage of both of them.\n",
      "therefore, some researchers [11–15] resort to deep ul segmentation models to\n",
      "better utilize both pixel value and shape information and develop a robust app-\n",
      "roach.\n",
      "the common and eﬀective way is to employ image clustering by maximiz-\n",
      "ing mutual information between image and predicted labels to distinguish fore-\n",
      "ground and background regions.\n",
      "many image-clustering-based deep ul meth-\n",
      "ods for natural tasks still achieve strong performances in nuclei segmentation.\n",
      "[11] constrain a convolutional neural network (cnn) with super-\n",
      "pixel level segmentation results.\n",
      "while reasonable results are obtained, these deep clustering-based\n",
      "label-free nuclei segmentation using intra-image self similarity\n",
      "675\n",
      "methods still suﬀer diﬃculties: (i) poor segmentation of the regions between\n",
      "adjacent nuclei.\n",
      "deep clustering models succeed in transferring images to high-\n",
      "dimensional feature space and obtaining image segmentation results by means\n",
      "of clustering pixels’ features.\n",
      "(ii) underutilization of intra-image\n",
      "self similarity (iiss) information.\n",
      "1, in terms of value, shape\n",
      "and texture, nuclei show a similar appearance within the same image but vary\n",
      "greatly among diﬀerent images1.\n",
      "this phenomenon oﬀers valuable information\n",
      "for networks to use but the current clustering models do not take this into\n",
      "account.\n",
      "to address the above issues and motivated by the iiss property, we hereby\n",
      "propose a novel self-similarity-driven segmentation network (ssimnet) for\n",
      "unsupervised nuclei segmentation.\n",
      "2, instead of designing com-\n",
      "plex discriminative network architectures, our framework derives knowledge from\n",
      "the iiss property to aid the segmentation.\n",
      "speciﬁcally, we obtain candidate\n",
      "nuclei with some unsupervised image processing.\n",
      "for the obtained candidates,\n",
      "it is common that adjacent nuclei merged into one candidate due to imperfect\n",
      "staining and low image quality, which violate the iiss property.\n",
      "finally, we apply\n",
      "the learned ssimnet to produce the ﬁnal nuclei segmentation.\n",
      "to validate the eﬀectiveness of our method, we conduct extensive experiments\n",
      "on the monuseg dataset [16,17] based on ten existing unsupervised segmentation\n",
      "methods [9,11–15,18–20].\n",
      "our method outperforms all comparison methods with\n",
      "an average dice score of 0.792 and aggregated jaccard index of 0.498 on the\n",
      "monuseg dataset which is close to the supervised method.\n",
      "2\n",
      "method\n",
      "as shown in fig. 2, our ssimnet aims at unsupervised segmentation of nuclei\n",
      "from histology images.\n",
      "speciﬁcally, by using a matrix factorization on hema-\n",
      "toxylin and eosin (h&e) stained histology images, we get the hematoxylin chan-\n",
      "nel image for clustering, active contour reﬁning and softening to generate the\n",
      "ﬁnal soft candidate label.\n",
      "last, while testing\n",
      "on the test image, to adapt the network to learn nucleus similarity within the\n",
      "same image, we ﬁne tune the network with soft pseudo labels of some patches\n",
      "in current test images.\n",
      "1 note that in our experiments, we use an image of size 10002 or 5002.\n",
      "suppose that we are given a training set is =\n",
      "{is\n",
      "i }n\n",
      "i=1 of histopathology images without any manual annotation.\n",
      "for each\n",
      "image, stained tissue colors are results from light attenuation, which depends\n",
      "on the type and amount of dyes that the tissues have absorbed.\n",
      "this property\n",
      "is prescribed by the beer-lambert law:\n",
      "v = log(i0/i) = wh,\n",
      "(1)\n",
      "where i ∈ r3×n represents the histology image with three color channels and\n",
      "n pixels, i0 is the illuminating light intensity of sample with i0\n",
      "= 255 for 8-bit\n",
      "images in our cases,\n",
      "note that usually histopathology images are\n",
      "stained with h&e and nuclei mainly absorb hematoxylin\n",
      "to reduce the noise in clustering results, we use\n",
      "active contour method as a smoothing operation to get hard candidate labels:\n",
      "p = {pi}n\n",
      "i=1 = {activecontour(fcm(it\n",
      "i ))}n\n",
      "i=1\n",
      "(3)\n",
      "label-free nuclei segmentation using intra-image self similarity\n",
      "677\n",
      "label smoothing.\n",
      "since hard label is overconﬁdent at the border of nuclei,\n",
      "which is detrimental to the training of the network, we soften the hard label one\n",
      "by one for each connected component in pi using the following formulation:\n",
      "(4), we obtain our soft\n",
      "candidate labels ˜p from p.\n",
      "2.2\n",
      "data puriﬁcation and ssimnet learning\n",
      "so far, soft candidate labels ˜pi have been acquired for each image is\n",
      "i .\n",
      "we sample k patches with overlap from original image\n",
      "is\n",
      "i .\n",
      "by denoting our segmentation network\n",
      "as f, our ﬁnal loss function to supervise the network training can be formulated\n",
      "as:\n",
      "loss =\n",
      "\u0002\n",
      "˜x∈ ˜\n",
      "x,˜y∈ ˜y,˜z∈ ˜\n",
      "z\n",
      "λlbce(f(˜x), ˜y) +\n",
      "also, we can obtain tissue patches and corresponding pseudo labels for each\n",
      "image in the test set termed as setk = ( ˜\n",
      "x t\n",
      "k , ˜yt\n",
      "k , ˜zt\n",
      "k ).\n",
      "right: illustration of average con-\n",
      "vex hull area and the average of connected component area based on usmi.\n",
      "(color ﬁgure online)\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "datasets and settings\n",
      "monuseg.\n",
      "multi-organ nuclei segmentation [16,17] (monuseg) is used to\n",
      "evaluated our ssimnet.\n",
      "the monuseg dataset consists of 44 h&e stained\n",
      "histopathology images with 28,846 manually annotated nuclei.\n",
      "with 1000 × 1000\n",
      "pixel resolution, these images were extracted from whole slide images from the\n",
      "the cancer genome atlas (tcga) repository, representing 9 diﬀerent organs\n",
      "from 44 individuals.\n",
      "the\n",
      "training and test set each consisted of 32 images tiles selected and extracted\n",
      "from a set of non-small cell lung cancer (nsclc), head and neck squamous\n",
      "cell carcinoma (hnscc), glioblastoma multiforme (gbm) and lower grade\n",
      "glioma (lgg) tissue images.\n",
      "we compare our ssimnet with several current unsupervised segmen-\n",
      "tation methods.\n",
      "we follow the dcgn [15] to conduct comparison experiments.\n",
      "we crop the image indataset into patches of 256 × 256 pixels for training.\n",
      "all the\n",
      "methods were trained for 150 epochs on monuseg and 200 epochs on cpm17\n",
      "each time and experimented with an initial learning rate of 5e−5 and a decay\n",
      "of 0.98 per epoch.\n",
      "our experiment repeated ten times on monuseg dataset and\n",
      "only once on cpm17 dataset for an augmented convenience.\n",
      "moreover, we ﬁne tune the network with only ﬁve epochs for each\n",
      "image on test set with optimizer parameter saved in checkpoint.\n",
      "label-free nuclei segmentation using intra-image self similarity\n",
      "679\n",
      "table 1.\n",
      "performance of the nuclei segmentation on monuseg dataset.\n",
      "51.0± 0.9(52.4)\n",
      "3.2\n",
      "experimental results\n",
      "to evaluate the eﬀectiveness of ssimnet, we compare it with several deep learn-\n",
      "ing based and conventional unsupervised segmentation methods on the men-\n",
      "tioned datasets, including minibatch k-means (termed as mkmeans), gaus-\n",
      "sian mixture model [9] (termed as gmm), invariant information clustering\n",
      "[12] (termed as iic), double dip\n",
      "[19] (termed as dcagmm), deep image clustering\n",
      "besides, we conduct an additional comparison experiment based on cpm17\n",
      "dataset to demonstrate the generalization of our method.\n",
      "as shown in table 2,\n",
      "our method again achieves the top performances.\n",
      "moreover, as the image size of\n",
      "cpm17 is smaller than that of monuseg, the performance gain is not as big as\n",
      "on the monuseg dataset.\n",
      "4. comparison of unsupervised nuclei segmentation results on monuseg.\n",
      "(color ﬁgure online)\n",
      "table 2. performance of the nuclei segmentation on cpm17 dataset.\n",
      "as shown in table 3, each component in our\n",
      "ssimnet can bring diﬀerent degrees of improvement, which shows that all of the\n",
      "label softening, data puriﬁcation and ﬁnetuning process are signiﬁcant parts of\n",
      "our ssimnet and play an indispensable role in achieving superior performance.\n",
      "labelsoftening\n",
      "datapuriﬁcation\n",
      "finetune\n",
      "precision%↑\n",
      "recall%↑\n",
      "dice%↑\n",
      "aji%↑\n",
      "✓\n",
      "✓\n",
      "78.8\n",
      "78.7\n",
      "78.3\n",
      "45.6\n",
      "✓\n",
      "✓\n",
      "79.6\n",
      "77.3\n",
      "77.9\n",
      "47.6\n",
      "✓\n",
      "✓\n",
      "80.8\n",
      "76.1\n",
      "76.7\n",
      "44.1\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "82.0\n",
      "77.2\n",
      "79.2\n",
      "49.8\n",
      "label-free nuclei segmentation using intra-image self similarity\n",
      "681\n",
      "4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf:\n",
      "whole-slide histopathology image (wsi) is regarded as the\n",
      "gold standard for survival prediction of breast cancer (bc) across dif-\n",
      "ferent subtypes.\n",
      "then,\n",
      "besides aligning the embeddings of diﬀerent types of nodes across the\n",
      "source and target domains, we proposed a novel tumor-tils interaction\n",
      "alignment (ttia) module to ensure that the distribution of interaction\n",
      "weights are similar in both domains.\n",
      "we evaluated the performance of\n",
      "our method on the brca cohort derived from the cancer genome atlas\n",
      "(tcga), and the experimental results indicated that t2uda outper-\n",
      "formed other domain adaption methods for predicting patients’ clinical\n",
      "outcomes.\n",
      "https://doi.org/10.1007/978-3-031-43987-2_59\n",
      "t2uda\n",
      "613\n",
      "keywords: tumor-inﬁltrating lymphocytes · unsupervised domain\n",
      "adaption · prognosis prediction · graph attention network · breast\n",
      "cancer\n",
      "1\n",
      "introduction\n",
      "breast cancer (bc) is the most common cancer diagnosed among females and\n",
      "the second leading cause of cancer death among women after lung cancer [1].\n",
      "thus, eﬀective and accurate prognosis of bc\n",
      "as well as stratifying cancer patients into diﬀerent subgroups for personalized\n",
      "cancer management has attracted more attention than ever before.\n",
      "among diﬀerent types of imaging biomarkers, histopathological images are\n",
      "generally considered the golden standard for bc prognosis since they can confer\n",
      "important cell-level information that can reﬂect the aggressiveness of bc [4].\n",
      "recently, with the availability of digitalized whole-slide pathological images\n",
      "(wsis), many computational models have been employed for the prognosis pre-\n",
      "diction of various subtypes of bc.\n",
      "[5] presented a novel\n",
      "approach for predicting the prognosis of er-positive bc patients by quantifying\n",
      "nuclear shape and orientation from histopathological images.\n",
      "[9] developed graph neural networks for unsupervised domain adaptation\n",
      "in histopathological image analysis, based on a backbone for embedding input\n",
      "images into a feature space, and a graph neural layer for propagating the super-\n",
      "vision signals of images with labels.\n",
      "although much progress has been achieved, most of the existing studies\n",
      "applied the feature alignment strategy to reduce the distribution diﬀerence\n",
      "between source and target domains.\n",
      "it can be expected that better\n",
      "prognosis performance can be achieved if we leveraged the tils-tumor interac-\n",
      "tion information to resolve the survival analysis task on the target domain.\n",
      "in order to pre-\n",
      "serve the node-level and interaction-level similarities across diﬀerent domains,\n",
      "we not only aligned the embedding for diﬀerent types of nodes but also designed\n",
      "a novel tumor-tils interaction alignment (ttia) module to ensure that the\n",
      "distribution of the interaction weights are similar in both domains.\n",
      "we evalu-\n",
      "ated the performance of our method on the breast invasive carcinoma (brca)\n",
      "cohort derived from the cancer genome atlas (tcga), and the experimental\n",
      "results indicated that t2uda outperforms other domain adaption methods for\n",
      "predicting patients’ clinical outcomes.\n",
      "2\n",
      "method\n",
      "we summarized the proposed t2uda network in fig. 1, which consists of three\n",
      "parts, i.e., graph attention network-based framework, feature alignment(fa),\n",
      "and tils-tumor interaction alignment(ttia).\n",
      "source graph \n",
      "construction\n",
      "target graph \n",
      "construction\n",
      "gat\n",
      "sagpool\n",
      "gat\n",
      "sagpool\n",
      "gat\n",
      "sagpool\n",
      "cox\n",
      "gat\n",
      "sagpoo\n",
      "l\n",
      "gat\n",
      "sagpool\n",
      "cox\n",
      " \n",
      " tumor patch\n",
      "tils patch\n",
      " ℒ\n",
      "gat\n",
      "sagpool\n",
      "fa\n",
      "fa\n",
      "target sample\n",
      "source sample\n",
      "ttia\n",
      "fa: feature alignment\n",
      "ttia:  tumor-tils interaction alignment \n",
      "fig.\n",
      "we obtained valid patches of 512 × 512 pixels from\n",
      "pathological images and segment the tils and tumor tissues using a pre-trained\n",
      "u-net++ model.\n",
      "then, the principal\n",
      "component analysis (pca) is implemented to reduce dimensionalities of the node\n",
      "features to 128.\n",
      "calculating\n",
      "tils-tumor\n",
      "interaction\n",
      "via\n",
      "graph\n",
      "attention\n",
      "net-\n",
      "works(gats).\n",
      "feature alignment.\n",
      "in the proposed gat-based transfer learning framework,\n",
      "the feature alignment component was employed on its ﬁrst two layers.\n",
      "[15].\n",
      "here, we adopted mmd for feature alignment due to its ability to measure the\n",
      "distance between two distributions without explicit assumptions on the data\n",
      "distribution, we showed the objective function of mmd in our method as follows:\n",
      "lf a =\n",
      "\u0004\n",
      "r=1,2\n",
      "\u0004\n",
      "k∈l,t\n",
      "\u0006\u0006\u0006\u0006\u0006\n",
      "1\n",
      "n\n",
      "n\n",
      "\u0004\n",
      "i=1\n",
      "(fi,k)r − 1\n",
      "m\n",
      "m\n",
      "\u0004\n",
      "i=1\n",
      "\u0007\n",
      "f ′\n",
      "i,k\n",
      "\br\n",
      "\u0006\u0006\u0006\u0006\u0006\n",
      "2\n",
      "h\n",
      "(4)\n",
      "where h is a hilbert space, f represents the features from the source, f ′ rep-\n",
      "resents the feature from the target, r represents the layer number, k ∈ {l, t}\n",
      "referred to tils or tumor node.\n",
      "the illustration of the proposed interaction weight alignment module.\n",
      "tils-tumor interaction alignment.\n",
      "to achieve domain-adaptive prognosis prediction, the ﬁnal\n",
      "loss function included the cox loss, fa loss, and ttia loss as the following\n",
      "formula:\n",
      "lt = lcox + αlf a + βlt t ia,\n",
      "(7)\n",
      "where α and β represent the weights assigned to the importance of fa component\n",
      "and ttia component respectively.\n",
      "3\n",
      "experiments and results\n",
      "datasets.\n",
      "we conducted our experiments on the breast invasive carcinoma\n",
      "(brca) dataset from the cancer genome atlas (tcga).\n",
      "we hope to investigate if\n",
      "the proposed t2uda could be used to help improve the prognosis performance\n",
      "of (er+) or (er−) with the aid of the survival information on its counterpart.\n",
      "3.1\n",
      "implementation details and evaluation metrics\n",
      "the dimension of intermediate layers in gat was 256.\n",
      "during training,\n",
      "the model was trained for 150 epochs for both the main experiment and all\n",
      "comparative experiments.\n",
      "we evaluated the performance of our model using\n",
      "the concordance index (ci) and area under the curve (auc) as performance\n",
      "metrics.\n",
      "both ci and auc range from 0 to 1, with larger values indicating better\n",
      "prediction performance and\n",
      "quantitative performance comparison between diﬀerent unsupervised\n",
      "domain adaptation methods and our method.\n",
      "3.2\n",
      "result and discussion\n",
      "in this study, we compared the performance of our proposed model with sev-\n",
      "eral existing domain adaptation methods, including 1) ddc\n",
      "the experimental results were\n",
      "presented in table 1.\n",
      "first, our\n",
      "proposed method outperformed feature alignment-based methods such as ddc\n",
      "and deepjdot in terms of both ci and auc values.\n",
      "we also evaluated the contributions of the key components of our framework\n",
      "and found that t2uda performed better than source only and t2uda-v1,\n",
      "which shows the advantage of minimizing diﬀerences in tils-tumor interaction\n",
      "weights.\n",
      "in addition, we also evaluated the patient stratiﬁcation performance of dif-\n",
      "ferent methods.\n",
      "3, our proposed t2uda outperformed feature\n",
      "alignment-based methods (such as ddc and deepjdot), adversarial-based\n",
      "methods (such as dann and mdd), and t2uda-v1 in stratiﬁcation perfor-\n",
      "mance, proving that considering the interaction between tils and tumors as\n",
      "migration knowledge leads to better prognostic results.\n",
      "4\n",
      "conclusion\n",
      "in this paper, we presented an unsupervised domain adaptation algorithm that\n",
      "leverages tils-tumor interactions to predict patients’ survival in a target bc\n",
      "subtype(t2uda).\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_73.pdf:\n",
      "in computation pathology, the pyramid structure of gigapixel\n",
      "whole slide images (wsis) has recently been studied for capturing vari-\n",
      "ous information from individual cell interactions to tissue microenviron-\n",
      "ments.\n",
      "considering that the information from diﬀerent resolutions is\n",
      "complementary and can beneﬁt each other during the learning process,\n",
      "we further design a novel bidirectional interaction block to establish com-\n",
      "munication between diﬀerent levels within the wsi pyramids.\n",
      "we evaluate our methods\n",
      "on two public wsi datasets from tcga projects, i.e., kidney carcinoma\n",
      "(kica) and esophageal carcinoma (esca).\n",
      "experimental results show\n",
      "that our higt outperforms both hierarchical and non-hierarchical state-\n",
      "of-the-art methods on both tumor subtyping and staging tasks.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 73.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43987-2_73\n",
      "756\n",
      "z. guo et al.\n",
      "1\n",
      "introduction\n",
      "histopathology is considered the gold standard for diagnosing and treating many\n",
      "cancers [19].\n",
      "the tissue slices are usually scanned into whole slide images\n",
      "(wsis) and serve as important references for pathologists.\n",
      "unlike natural images,\n",
      "wsis typically contain billions of pixels and also have a pyramid structure, as\n",
      "shown in fig.\n",
      "typically, mil-based wsi\n",
      "analysis methods have three steps: (1) crop the huge wsi into numerous image\n",
      "patches; (2) extract instance features from the cropped patches; and (3) aggre-\n",
      "gate instance features together to obtain slide-level prediction results.\n",
      "many\n",
      "advanced mil models emerged in the past few years.\n",
      "such graph-\n",
      "transformer architecture has also been introduced into wsi analysis [15,20] to\n",
      "mine the thorough global and local correlations between diﬀerent image patches.\n",
      "diﬀerent resolution levels in the wsi pyramids contain diﬀerent and comple-\n",
      "mentary information [3].\n",
      "the images at a high-resolution level contain cellular-\n",
      "level information, such as the nucleus and chromatin morphology features\n",
      "[10].\n",
      "at a low-resolution level, tissue-related information like the extent of tumor-\n",
      "immune localization can be found [1], while the whole wsi describes the entire\n",
      "tissue microenvironment, such as intra-tumoral heterogeneity and tumor inva-\n",
      "sion\n",
      "speciﬁ-\n",
      "cally, we abstract the multi-resolution wsi pyramid as a heterogeneous hierar-\n",
      "chical graph and devise a hierarchical interaction graph-transformer architec-\n",
      "ture to learn both short-range and long-range correlations among diﬀerent image\n",
      "patches within diﬀerent resolutions.\n",
      "considering that the information from dif-\n",
      "ferent resolutions is complementary and can beneﬁt each other, we specially\n",
      "design a bidirectional interaction block in our hierarchical interaction vit mod-\n",
      "hierarchical interaction graph-transformer\n",
      "757\n",
      "fig.\n",
      "to reduce the tremendous computation and memory\n",
      "cost, we further adopt the eﬃcient pooling operation after the hierarchical gnn\n",
      "part to reduce the number of tokens and introduce the separable self-attention\n",
      "mechanism in hierarchical interaction vit modules to reduce the computation\n",
      "burden.\n",
      "the extensive experiments with promising results on two public wsi\n",
      "datasets from tcga projects, i.e., kidney carcinoma (kica) and esophageal\n",
      "carcinoma (esca), validate the eﬀectiveness and eﬃciency of our framework\n",
      "on both tumor subtyping and staging tasks.\n",
      "1, a wsi is cropped into numerous non-overlapping 512 ×\n",
      "512 image patches under diﬀerent magniﬁcations (i.e., ×5, ×10) by using a\n",
      "sliding window strategy, where the otsu algorithm\n",
      "[16] to\n",
      "extract the feature embedding of each image patch.\n",
      "n is the total number of the\n",
      "region nodes and m is the number of patch nodes belonging to a certain region\n",
      "node, and c denotes the dimension of feature embedding (1,024 in our experi-\n",
      "ments).\n",
      "2.2\n",
      "hierarchical graph neural network\n",
      "to learn the short-range relationship among diﬀerent patches within the wsi\n",
      "pyramid, we propose a new hierarchical graph message propagation operation,\n",
      "called raconv+.\n",
      "therefore, the layer-wise\n",
      "graph message propagation can be represented as:\n",
      "h(l+1) = σ\n",
      "\u0005\n",
      "a · h(l) · w (l)\u0006\n",
      ",\n",
      "(3)\n",
      "where a represents the attention score matrix, and the attention score for the\n",
      "j-th row and j′-th column of the matrix is given by eq.\n",
      "note that here we introduced ssa into the pl block to reduce the\n",
      "computation complexity of attention calculation from quadratic to linear while\n",
      "maintaining the performance [13].\n",
      "2.4\n",
      "slide-level prediction\n",
      "in the ﬁnal stage of our framework, we design a fusion block to combine the\n",
      "coarse-grained and ﬁne-grained features learned from the wsi pyramids.\n",
      "specif-\n",
      "ically, we use an element-wise summation operation to fuse the coarse-grained\n",
      "thumbnail feature and patch-level features from the hierarchical interaction\n",
      "gnn part, and then further fuse the ﬁne-grained patch-level features from the\n",
      "hivit part with a concatenation operation.\n",
      "3\n",
      "experiments\n",
      "datasets and evaluation metrics.\n",
      "the kica dataset consists of 371 cases of kidney carcinoma,\n",
      "of which 279 are classiﬁed as early-stage and 92 as late-stage.\n",
      "the esca dataset comprises 161 cases of esophageal car-\n",
      "cinoma, with 96 cases classiﬁed as early-stage and 65 as late-stage.\n",
      "hierarchical interaction graph-transformer\n",
      "761\n",
      "experimental\n",
      "setup.\n",
      "the\n",
      "proposed\n",
      "framework\n",
      "was\n",
      "implemented\n",
      "by\n",
      "pytorch\n",
      "all experiments were conducted on a\n",
      "workstation with eight nvidia geforce rtx 3090 (24 gb) gpus.\n",
      "even for the non-hierarchical graph-transformer baseline\n",
      "la-mil and hierarchical transformer model hipt, our model approaches at\n",
      "least around 3% and 2% improvement on auc and acc in the classiﬁcation of\n",
      "the staging of the kica dataset.\n",
      "we analyze the computation cost during the\n",
      "experiments to compare the eﬃciency between our methods and existing state-of-\n",
      "the-art approaches.\n",
      "besides we visualized the model size (mb) and the training\n",
      "memory allocation of gpu (gb) v.s. performance in kica’s typing and staging\n",
      "task plots in fig.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf:\n",
      "contrastive learning has gained popularity due to its robust-\n",
      "ness with good feature representation performance.\n",
      "however, cosine\n",
      "distance, the commonly used similarity metric in contrastive learning,\n",
      "is not well suited to represent the distance between two data points,\n",
      "especially on a nonlinear feature manifold.\n",
      "inspired by manifold learn-\n",
      "ing, we propose a novel extension of contrastive learning that leverages\n",
      "geodesic distance between features as a similarity metric for histopathol-\n",
      "ogy whole slide image classiﬁcation.\n",
      "to reduce the computational over-\n",
      "head in manifold learning, we propose geodesic-distance-based feature\n",
      "clustering for eﬃcient contrastive loss evaluation using prototypes with-\n",
      "out time-consuming pairwise feature similarity comparison.\n",
      "the eﬃcacy\n",
      "of the proposed method is evaluated on two real-world histopathology\n",
      "image datasets.\n",
      "keywords: contrastive learning · manifold learning · geodesic\n",
      "distance · histopathology image classiﬁcation · multiple instance\n",
      "learning\n",
      "1\n",
      "introduction\n",
      "whole slide image (wsi) classiﬁcation is a crucial process to diagnose diseases\n",
      "in digital pathology.\n",
      "although mil does\n",
      "not require perfect per-patch label assignment, it is important to construct good\n",
      "feature vectors that are easily separated into diﬀerent classes to make the clas-\n",
      "siﬁcation more accurate.\n",
      "1. comparison of geodesic and cosine distance in n-dimensional space.\n",
      "[3] introduced the utilization of\n",
      "data augmentation and a learnable nonlinear transformation between the feature\n",
      "embedding and the contrastive loss to generally improve the quality of feature\n",
      "embedding.\n",
      "[6] employed a dynamic dictionary along with a momentum\n",
      "encoder in the contrastive learning model to serve as an alternative to the super-\n",
      "vised pre-trained imagenet model in various computer vision tasks.\n",
      "[5] integrated the k-means clustering and contrastive learning model by\n",
      "introducing prototypes as latent variables and assigning each sample to multi-\n",
      "ple prototypes to learn the hierarchical semantic structure of the dataset.\n",
      "these\n",
      "prior works used cosine distance as their distance measurement, which computes\n",
      "the angle between two feature vectors as shown in fig.\n",
      "although cosine dis-\n",
      "tance is a commonly used distance metric in contrastive learning, we observed\n",
      "that the cosine distance approximates the diﬀerence between local neighbors and\n",
      "is insuﬃcient to represent the distance between far-away points on a complicated,\n",
      "nonlinear manifold.\n",
      "the main motivation of this work is to extend the current contrastive learn-\n",
      "ing to represent the nonlinear feature manifold inspired by manifold learning.\n",
      "owing to the manifold distribution hypothesis [8], the relative distance between\n",
      "high-dimensional data is preserved on a low-dimensional manifold.\n",
      "isomap [12]\n",
      "is a well-known manifold learning approach that represents the manifold struc-\n",
      "ture by using geodesic distance (i.e., the shortest path length between points on\n",
      "the manifold).\n",
      "there are several previous works that use manifold learning for\n",
      "image classiﬁcation and reconstruction tasks, such as lu et al.\n",
      "however, the use of geodesic distance on the feature manifold for image\n",
      "classiﬁcation is a recent development.\n",
      "[2] applied the random walk\n",
      "algorithm on the nearest neighbor graph to compute the pairwise geodesic dis-\n",
      "tance and proposed the n-pair loss to maximize the similarity between samples\n",
      "from the same class for image retrieval and clustering applications.\n",
      "[4]\n",
      "employed the geodesic distance computed using the dijkstra algorithm on the k-\n",
      "nearest neighbor graph to measure the correlation between the original samples\n",
      "deep manifold contrastive learning\n",
      "685\n",
      "and then further divided each class into sub-classes to deal with the problems of\n",
      "high spectral dimension and channel redundancy in the hyperspectral images.\n",
      "however, this method captured the nonlinear data manifold structure on the\n",
      "original data (not on the feature vectors) only once at the beginning stage,\n",
      "which is not updated in the further training process.\n",
      "in this study, we propose a hybrid method that combines manifold learn-\n",
      "ing and contrastive learning to generate a good feature extractor (encoder) for\n",
      "histopathology image classiﬁcation.\n",
      "our method uses the sub-classes and proto-\n",
      "types as in conventional contrastive learning, but we propose the use of geodesic\n",
      "distance in generating the sub-classes to represent the non-linear feature mani-\n",
      "fold more accurately.\n",
      "by doing this, we achieve better separation between fea-\n",
      "tures with large margins, resulting in improved mil classiﬁcation performance.\n",
      "the main contributions of our work can be summarized as follows:\n",
      "– we introduce a novel integration of manifold geodesic distance in contrastive\n",
      "learning, which results in better feature representation for the non-linear fea-\n",
      "ture manifold.\n",
      "– we propose a geodesic-distance-based feature clustering for eﬃcient con-\n",
      "trastive loss evaluation using prototypes without brute-force pairwise fea-\n",
      "ture similarity comparison while approximating the overall manifold geometry\n",
      "well, which results in reduced computation.\n",
      "– we demonstrate that the proposed method outperforms other state-of-the-\n",
      "art (sota) methods with a much smaller number of sub-classes without\n",
      "complicated prototype assignment (e.g., hierarchical clustering).\n",
      "to the best of our knowledge, this work is the ﬁrst attempt to leverage manifold\n",
      "geodesic distance in contrastive learning for histopathology wsi classiﬁcation.\n",
      "2. it is composed of two\n",
      "stages: (1) train the feature extractor using deep manifold embedding learning\n",
      "and (2) train the wsi classiﬁer using the deep manifold embedding extracted\n",
      "from the ﬁrst stage.\n",
      "the input wsis are pre-processed to extract 256 × 256 × 3\n",
      "dimensional patches from the tumor area at a 10× magniﬁcation level.\n",
      "patches\n",
      "with less than 50% tissue coverage are excluded from the experiment.\n",
      "2.1\n",
      "deep manifold embedding learning\n",
      "as illustrated in fig.\n",
      "the output is then passed through two diﬀerent paths, namely, deep manifold\n",
      "and softmax paths.\n",
      "2. overview of our proposed method, which is composed of two stages: (a) deep\n",
      "manifold embedding learning and (b) mil classiﬁcation.\n",
      "deep manifold.\n",
      "in this stage, the patches from each class are further grouped\n",
      "into sub-classes based on manifold geodesic distance.\n",
      "each node (patch feature) is connecting to its k-nearest\n",
      "neighbors (knn) based on the weighted edges computed with euclidean dis-\n",
      "tance, given that the neighbor samples on the manifold should have a higher\n",
      "potential to be in the same sub-class.\n",
      "the geodesic distance matrix m on the\n",
      "manifold is then computed between each sample pair by using dijkstra’s algo-\n",
      "rithm based on the gc.\n",
      "for the deep manifold training, we adopted two losses: (1)\n",
      "intra-subclass loss lintra and (2) inter-subclass loss linter.\n",
      "lintra is formulated as follows:\n",
      "lintra =\n",
      "1\n",
      "j · i\n",
      "j\n",
      "\u0002\n",
      "j=1\n",
      "i\n",
      "\u0002\n",
      "i=1\n",
      "(f(xi\n",
      "j) − p+)t (f(xi\n",
      "j) − p+)\n",
      "(1)\n",
      "deep manifold contrastive learning\n",
      "687\n",
      "where xi\n",
      "j is the i-th patch in the j-th batch, j represents the total number of\n",
      "batches, i represents the total number of patches per batch, f(·) is the feature\n",
      "extractor, and p+ indicates the positive prototype of the patch (i.e., the proto-\n",
      "type of the subclass containing xi\n",
      "j).\n",
      "linter = 1\n",
      "j\n",
      "j\n",
      "\u0002\n",
      "j=1\n",
      "(△ − d(f(qa\n",
      "j ), p b))\n",
      "(2)\n",
      "d(y, z) = max{sup\n",
      "y∈y\n",
      "d(y, z), sup\n",
      "z∈z\n",
      "d(z, y )}\n",
      "(3)\n",
      "where f(qa\n",
      "j ) is a set of patch features in batch j from class a, p b is a set of\n",
      "prototypes from the sub-classes of class b, and △ is a positive margin between\n",
      "classes on data manifold.\n",
      "then, the manifold loss\n",
      "is formulated as\n",
      "lmanifold = lintra + linter\n",
      "(4)\n",
      "another path via softmax is simply trained on outputs from the feature\n",
      "extractor with the ground truth slide-level labels y by the cross-entropy loss\n",
      "lce, which is deﬁned as follows:\n",
      "finally, the total loss for the ﬁrst stage is deﬁned as follows:\n",
      "ltotal = lmanifold + lce\n",
      "(6)\n",
      "2.2\n",
      "mil classiﬁcation\n",
      "as illustrated in fig.\n",
      "2(b), in the second stage, the pre-trained feature extractor\n",
      "from the previous stage is then deployed to extract features for bag generation.\n",
      "a total of 50 bags are generated for each wsi, in which each bag is composed\n",
      "of the concatenation of the features from 100 patches in 512 dimensions.\n",
      "we collected 121\n",
      "wsis for the training set, and the remaining wsis were used as the testing set.\n",
      "3.2\n",
      "implementation detail\n",
      "we used a pre-trained vgg16 with imagenet as the initial encoder, which was\n",
      "further modiﬁed via deep manifold model training using the proposed manifold\n",
      "and cross-entropy loss functions.\n",
      "in the deep manifold\n",
      "embedding learning model, the learning rates were set to 1e-4 with a decay rate\n",
      "of 1e-6 for the ihccs subtype classiﬁcation and to 1e-5 with a decay rate of\n",
      "1e-8 for the liver cancer type classiﬁcation.\n",
      "we used\n",
      "batch sizes 64 and 4 for training the deep manifold embedding learning model and\n",
      "the mil classiﬁcation model, respectively.\n",
      "the number of epochs for the deep\n",
      "manifold embedding learning model was 50, while 50 and 200 epochs for the\n",
      "ihccs subtype classiﬁcation and liver cancer type classiﬁcation, respectively.\n",
      "as for the optimizer, we used stochastic gradient decay for both stages.\n",
      "the\n",
      "result shown in the tables is the average result from 10 iterations of the mil\n",
      "classiﬁcation model.\n",
      "3.3\n",
      "experimental results\n",
      "the performance of diﬀerent models from two diﬀerent datasets is reported in\n",
      "this section.\n",
      "the mil classiﬁcation result of the ihccs subtype\n",
      "deep manifold contrastive learning\n",
      "689\n",
      "classiﬁcation is shown in table 1.\n",
      "our proposed method outperformed the base-\n",
      "line cnn by about 4% increment in accuracy, precision, recall, and f1 score.\n",
      "table 1. classiﬁcation performance on ihccs subtype and liver cancer type dataset.\n",
      "f1\n",
      "cnn\n",
      "na\n",
      "0.7315\n",
      "0.7372\n",
      "0.7315\n",
      "0.7270\n",
      "0.7710\n",
      "0.7781\n",
      "0.7719\n",
      "0.7657\n",
      "pcl\n",
      "500-800-1000\n",
      "0.7386\n",
      "0.7478\n",
      "0.7394\n",
      "0.7354\n",
      "0.8146\n",
      "0.7898\n",
      "0.8146\n",
      "0.7979\n",
      "hcsc\n",
      "2-10-100\n",
      "0.7230\n",
      "0.7265\n",
      "0.7230\n",
      "0.7231\n",
      "0.7995\n",
      "0.8524 0.7995\n",
      "0.7825\n",
      "ours\n",
      "20\n",
      "0.7703 0.7710 0.7678 0.7668 0.8239 0.8351\n",
      "0.8239 0.8227\n",
      "table 2. ablation study of prototype assignment strategies.\n",
      "our\n",
      "method achieved about 5% improvement in accuracy against the baseline and\n",
      "1% to 2% improvement in accuracy against the sota methods.\n",
      "moreover, it\n",
      "outperformed the sota methods with far fewer prototypes and without com-\n",
      "plicated hierarchical prototype assignments.\n",
      "to further evaluate the eﬀect of\n",
      "prototypes, we conducted an ablation study for diﬀerent prototype assignment\n",
      "strategies as shown in table 2.\n",
      "when both are used together, it implies a hierarchi-\n",
      "cal prototype assignment where local prototypes interact with the corresponding\n",
      "global prototype.\n",
      "meanwhile, the combination of both prototypes achieved a similar performance\n",
      "to that of the model with local prototypes only.\n",
      "since the hierarchical (global +\n",
      "local) assignment did not show a signiﬁcant improvement but instead increased\n",
      "computation, we used only local sub-class prototypes in our ﬁnal experiment\n",
      "setting.\n",
      "jeong\n",
      "table 3. classiﬁcation performance of geodesic distance and cosine distance.\n",
      "since one of our contributions is the use of geodesic distance, we assessed the\n",
      "eﬃcacy of the method by comparing it with the performance using cosine dis-\n",
      "tance, as shown in table 3. to measure the performance of the cosine-distance-\n",
      "based method, we simply replaced our proposed manifold loss with nt-xent\n",
      "loss [3], which uses cosine distance in their feature similarity measurement.\n",
      "two\n",
      "cosine distance experiments were conducted as follows: (1) use only their ground-\n",
      "truth class without further dividing the samples into sub-classes (i.e., global pro-\n",
      "totypes) and (2) divide the samples from each class into 10 sub-classes by using\n",
      "k-means clustering (i.e., local prototypes).\n",
      "as shown in table 3, using multiple\n",
      "local prototypes shows slightly better performance compared to using global pro-\n",
      "totypes.\n",
      "by switching the nt-xent loss with our geodesic-based manifold loss,\n",
      "the overall performance is increased by about 2%.\n",
      "red dots represent sdt samples and\n",
      "blue dots represent ldt samples from the ihccs dataset (corresponding histol-\n",
      "ogy thumbnail images are shown on the right).\n",
      "it is clearly shown\n",
      "that geodesic distance can correctly measure the feature distance (similarity) on\n",
      "the manifold so that sdt and ldt groups are located far away in the t-sne\n",
      "plot, whereas cosine distance failed to separate these groups and they are located\n",
      "nearby in the plot.\n",
      "deep manifold contrastive learning\n",
      "691\n",
      "3.4\n",
      "conclusion and future work\n",
      "in this paper, we proposed a novel geodesic-distance-based contrastive learn-\n",
      "ing for histopathology image classiﬁcation.\n",
      "unlike conventional cosine-distance-\n",
      "based contrastive learning methods, our method can represent nonlinear feature\n",
      "manifold better and generate better discriminative features.\n",
      "in the future, we\n",
      "plan to optimize the algorithm and apply our method to other datasets and\n",
      "tasks, such as multi-class classiﬁcation problems and natural image datasets.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_72.pdf:\n",
      "the major limitation of previous learning frame-\n",
      "works for whole slide image (wsi) based survival prediction is that the\n",
      "contextual interactions of pathological components (e.g., tumor, stroma,\n",
      "lymphocyte, etc.)\n",
      "speciﬁcally, we ﬁrst uti-\n",
      "lize a multi-scope analysis strategy, which leverages an in-slide superpixel\n",
      "and a cross-slide clustering, to mine the spatial and semantic priors of\n",
      "wsis.\n",
      "furthermore, based on the extracted spatial prior, a hierarchical\n",
      "graph convolutional network is proposed to progressively learn the topo-\n",
      "logical features of the variant microenvironments ranging from patch-\n",
      "level to tissue-level.\n",
      "in addition, guided by the identiﬁed semantic prior,\n",
      "tissue-level features are further aggregated to represent the meaningful\n",
      "pathological components, whose contextual interactions are established\n",
      "and quantiﬁed by the designed transformer-based prediction head.\n",
      "experimental results demonstrate that our proposed\n",
      "method yields superior performance and richer interpretability compared\n",
      "to the state-of-the-art approaches.\n",
      "keywords: whole slide image · survival prediction · contextual\n",
      "interaction · graph neural network · transformer\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp.\n",
      "https://doi.org/10.1007/978-3-031-43987-2_72\n",
      "746\n",
      "w. hou et al.\n",
      "1\n",
      "introduction\n",
      "the ability to predict the future risk of patients with cancer can signiﬁcantly\n",
      "assist clinical management decisions, such as treatment and monitoring [21].\n",
      "generally, pathologists need to manually assess the pathological images obtained\n",
      "by whole-slide scanning systems for clinical decision-making, e.g., cancer diagno-\n",
      "sis and prognosis\n",
      "however, due to the complex morphology and structure\n",
      "of human tissues and the continuum of histologic features phenotyped across\n",
      "the diagnostic spectrum, it is a tedious and time-consuming task to manually\n",
      "assess the whole slide image (wsi) [12].\n",
      "therefore, automated wsi analysis method for survival prediction\n",
      "task is highly demanded yet challenging in clinical practice.\n",
      "over the years, deep learning has greatly promoted the development of\n",
      "computational pathology, including wsi analysis [9,17,24].\n",
      "due to the huge\n",
      "size, wsis are generally cropped to numerous patches with a ﬁxed size and\n",
      "encoded to patch features by a cnn encoder (e.g., imagenet pretrained\n",
      "resnet50\n",
      "first, to mine more comprehensive and in-depth\n",
      "attribute priors of wsi, we propose a multi-scope analysis strategy consisting\n",
      "of in-slide superpixels and cross-slide clustering, which can not only extract the\n",
      "spatial prior but also identify the semantic prior of wsis.\n",
      "speciﬁcally, based on the extracted\n",
      "spatial topology, the hierarchical graph convolution layer in hgt progressively\n",
      "aggregate the patch-level features to the tissue-level features, so as to learn\n",
      "the topological features of variant microenvironments ranging from ﬁne-grained\n",
      "(e.g., cell) to coarse-grained (e.g., necrosis, epithelium, etc.).\n",
      "then, under the\n",
      "guidance of the identiﬁed semantic prior, the tissue-level features are further\n",
      "sorted and assigned to form the feature embedding of pathological components.\n",
      "extensive experiments on three cancer\n",
      "cohorts (i.e., crc, tcga-lihc and tcga-kirc) demonstrates the eﬀective-\n",
      "ness and interpretability of our framework.\n",
      "due to the huge size,\n",
      "wsis are generally cropped to numerous patches with a ﬁxed size (i.e., 256×256)\n",
      "and encoded to patch features vpatch ∈ rn×d in the embedding space d by a\n",
      "cnn encoder (i.e., imagenet pretrained resnet50\n",
      "[11]) for further analysis,\n",
      "where n is the number of patches, d = 1024 is the feature dimension.\n",
      "the goal\n",
      "of wsi-based cancer survival prediction is to learn the feature embedding of v\n",
      "in a supervised manner and output the survival risk o ∈ r1.\n",
      "however, conventional patch-level analysis cannot model complex pathologi-\n",
      "cal patterns (e.g., tumor lymphocyte inﬁltration, immune cell composition, etc.),\n",
      "resulting in limited cancer survival prediction performance.\n",
      "to this end, we pro-\n",
      "posed a novel learning network, i.e., hgt, which utilized the spatial and seman-\n",
      "tic priors mined by a multi-scope analysis strategy (i.e., in-slide superpixel and\n",
      "cross-slide clustering) to represent and capture the contextual interaction of\n",
      "pathological components.\n",
      "however, the conventional patch-level anal-\n",
      "ysis is diﬃcult to meet this requirement.\n",
      "intuitively, the\n",
      "cropped patches and segmented tissues in a wsi can be considered as hierar-\n",
      "chical entities ranging from ﬁne-grained level (e.g., cell) to coarse-grained level\n",
      "(e.g., necrosis, epithelium, etc.).\n",
      "then, the patches in each superpixel are further connected in an\n",
      "8-adjacent manner, thus generating patch adjacency matrix epatch ∈ rn×n.\n",
      "the\n",
      "spatial assignment matrix between cropped patches and segmented tissues is\n",
      "denoted as aspa ∈ rn×m.\n",
      "based on the spatial topology extracted\n",
      "by in-slide superpixel, the patch graph convolutional layer (patch gcl) is\n",
      "designed to learn the feature of the ﬁne-grained microenvironment (e.g., cell)\n",
      "through the message passing between adjacent patches, which can be represented\n",
      "as:\n",
      "v\n",
      "′\n",
      "patch = σ(graphconv(vpatch, epatch)),\n",
      "(1)\n",
      "where σ(·) denotes the activation function, such as relu.\n",
      "graphconv denotes\n",
      "the graph convolutional operation, e.g., gcnconv [15], graphsage\n",
      "third, based on the spatial assignment\n",
      "matrix aspa, the learned patch-level features can be aggregated to the tissue-level\n",
      "hierarchical graph transformer\n",
      "749\n",
      "features which contain the information of necrosis, epithelium, etc.\n",
      "the tissue graph convolutional\n",
      "layer (tissue gcl) is further designed to learn the feature of this coarse-grained\n",
      "microenvironment, which can be represented as:\n",
      "v\n",
      "′\n",
      "tissue = σ(graphconv(vtissue, etissue)).\n",
      "however, existing analysis frameworks for\n",
      "wsi often ignore the capture of contextual interactions of pathological compo-\n",
      "nents (e.g., tumor, stroma, lymphocyte, etc.), resulting in limited performance\n",
      "and interpretability.\n",
      "the semantic assignment matrix between segmented tissues\n",
      "and pathological components is denoted as asem ∈ rm×k.\n",
      "under the guidance of the semantic prior iden-\n",
      "tiﬁed by cross-slide clustering, the learned tissue features v\n",
      "′\n",
      "tissue can be further\n",
      "aggregated, forming a series meaningful component embeddings p ′ speciﬁc to\n",
      "the cancer type.\n",
      "⎞\n",
      "⎠ ,\n",
      "(6)\n",
      "where δi denote the censorship of i-th patient, o(i) and o(j) denote the survival\n",
      "output of i-th and j-th patient in a batch, respectively.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "experimental settings\n",
      "dataset.\n",
      "the average patch number of each wsi is\n",
      "18727, 3680, 3742 for crc, tcga-lihc, tcga-kirc, respectively.\n",
      "implementation details.\n",
      "our graph convolutional model is implemented by pytorch geometric [7].\n",
      "the number of transformer heads is 8, and the attention scores of all heads\n",
      "are averaged to produce the heatmap of contextual interactions.\n",
      "ci\n",
      "ranges from 0 to 1, where a larger ci indicates better performance.\n",
      "in this study, we conduct a 5-fold evaluation procedure with 5 runs\n",
      "to evaluate the survival prediction performance for each method.\n",
      "for fair comparison, same cnn\n",
      "extractor (i.e. imagenet pretrained resnet50\n",
      "generally, most mil methods, i.e., deepsets, abmil, dsmil,\n",
      "transmil mainly focus on a few key instances for prediction, but they do\n",
      "not have signiﬁcant advantages in cancer prognosis.\n",
      "deepattnmisl has a certain semantic\n",
      "perception ability for patch, which achieves better performance in lihc cohort.\n",
      "patchgcn is capable to capture the local contextual interactions between patch,\n",
      "which also achieves satisﬁed performance in kirc cohort.\n",
      "experimental results of ci. results not signiﬁcantly worse than the best\n",
      "(p-value > 0.05, two-sample t-test) are shown in bold.\n",
      "the trained classiﬁcation model can be used to determine the biological\n",
      "semantics of the pathological components extracted by our model with a major\n",
      "voting rule.\n",
      "figure 3 shows the original image, spatial topology, proportion and\n",
      "fig.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_57.pdf:\n",
      "nucleus segmentation is usually the ﬁrst step in pathologi-\n",
      "cal image analysis tasks.\n",
      "generalizable nucleus segmentation refers to the\n",
      "problem of training a segmentation model that is robust to domain gaps\n",
      "between the source and target domains.\n",
      "the domain gaps are usually\n",
      "believed to be caused by the varied image acquisition conditions, e.g.,\n",
      "diﬀerent scanners, tissues, or staining protocols.\n",
      "first, we introduce a re-coloring method that relieves dra-\n",
      "matic image color variations between diﬀerent domains.\n",
      "we evaluate the proposed methods on\n",
      "two h&e stained image datasets, named consep and cpm17, and two\n",
      "ihc stained image datasets, called deepliif and bc-deepliif.\n",
      "exten-\n",
      "sive experimental results justify the eﬀectiveness of our proposed darc\n",
      "model.\n",
      "keywords: domain generalization · nucleus segmentation · instance\n",
      "normalization\n",
      "1\n",
      "introduction\n",
      "automatic nucleus segmentation has captured wide research interests in recent\n",
      "years due to its importance in pathological image analysis [1–4].\n",
      "however, as\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 57.\n",
      "1. example image patches from diﬀerent datasets.\n",
      "their appearance diﬀers sig-\n",
      "niﬁcantly from each other due to variations in image modalities, staining protocols,\n",
      "scanner types, and tissues.\n",
      "shown in fig.\n",
      "1, the variations in image modalities, staining protocols, scanner\n",
      "types, and tissues signiﬁcantly aﬀect the appearance of nucleus images, result-\n",
      "ing in notable gap between source and target domains [5–7].\n",
      "therefore, it is highly desirable to train a robust\n",
      "nucleus segmentation model that is generalizable to diﬀerent domains.\n",
      "[12,13] and they can be roughly grouped into data augmentation-, representation\n",
      "learning-, and optimization-based methods.\n",
      "it is a consensus that a generalizable nucleus segmentation model should be\n",
      "robust to image appearance variation caused by the change in staining proto-\n",
      "cols, scanner types, and tissues, as illustrated in fig.\n",
      "it performs both semantic segmentation and contour detection for nucleus\n",
      "darc: distribution-aware re-coloring model\n",
      "593\n",
      "fig.\n",
      "the whole model is trained in an end-to-end\n",
      "manner.\n",
      "darc ﬁrst re-colors each image to relieve the impact caused by diﬀerent\n",
      "image acquisition conditions.\n",
      "the re-colored image is then fed into the u-net encoder\n",
      "and the ratio prediction head.\n",
      "then, the re-colored image is fed into darc again with ρ for ﬁnal prediction.\n",
      "the area of each nucleus instance is obtained via subtraction between\n",
      "the segmentation and contour prediction maps [1].\n",
      "details of the baseline model\n",
      "is provided in the supplementary material.\n",
      "darc ﬁrst\n",
      "re-colors each image to relieve the inﬂuence caused by image acquisition condi-\n",
      "tions.\n",
      "the re-colored image is then fed into the u-net encoder and the ratio pre-\n",
      "diction head.\n",
      "with the predicted ratio, the dain layers can estimate feature\n",
      "statistics more robustly and facilitate more accurate nucleus segmentation.\n",
      "2.2\n",
      "nucleus image re-coloring\n",
      "we propose the re-coloring (rc) method to overcome the color change in dif-\n",
      "ferent domains.\n",
      "speciﬁcally, given a rgb image i, e.g., an h&e or ihc stained\n",
      "image, we ﬁrst obtain its grayscale image ig.\n",
      "in this way, we obtain an initial re-colored\n",
      "image ir.\n",
      "however, de-colorization results in the loss of ﬁne-grained textures and may\n",
      "harm the segmentation accuracy.\n",
      "to handle this problem, we compensate ir\n",
      "with the original semantic information contained in i. recent works\n",
      "[40] show\n",
      "that semantic information can be reﬂected via the order of pixels according\n",
      "to their gray value.\n",
      "[41] to\n",
      "combine the semantic information in i with the color values in ir.\n",
      "594\n",
      "s. chen et al.\n",
      "algorithm 1. re-coloring\n",
      "input:\n",
      "the input rgb image\n",
      "rh×w ×3;\n",
      "the module t whose input and output channel numbers are 1 and 3, respectively;\n",
      "output:\n",
      "the re-colored image io ∈\n",
      "rh×w ×3;\n",
      "1: de-colorizing i to obtain ig;\n",
      "2: ir ← t(ig)\n",
      "3: reshaping i and ir to rhw ×3\n",
      "4: sortindex ← argsort(argsort(i))\n",
      "5: sortv alue ← sort(ir)\n",
      "6: io ← assignv alue(sortindex, sortv alue)\n",
      "7: return io\n",
      "table 1. evaluation on the impact of foreground-background ratio to model perfor-\n",
      "mance.\n",
      "details of the module t are included in the supplementary material.\n",
      "in this way, the re-colored image is advantageous in two aspects.\n",
      "first,\n",
      "the appearance diﬀerence between pathological images caused by the change in\n",
      "scanners and staining protocols is eliminated.\n",
      "second, the re-colored image pre-\n",
      "serves ﬁne-grained structure information, enabling precise instance segmentation\n",
      "to be possible.\n",
      "however, for\n",
      "dense-prediction tasks like semantic segmentation or contour detection, adopt-\n",
      "ing in alone cannot fully address the feature statistics variation problem.\n",
      "speciﬁcally, an image with more nucleus instances\n",
      "produces more responses in feature maps and thus higher feature statistic val-\n",
      "ues, and vice versa.\n",
      "the diﬀerence in this ratio causes interference to nucleus\n",
      "segmentation.\n",
      "the c-dimensional feature vector on its\n",
      "pixel (i, j) is denoted as xij;\n",
      "the modules eμ and eδ that re-estimate feature statistics;\n",
      "δsr a ∈ r1×1×c that is obtained via running mean of δs in the training stage;\n",
      "the momentum factor α used to update δsr a ;\n",
      "(optional) δs = f(ρ);\n",
      "output:\n",
      "normalized feature maps y ∈ rh×w ×c;\n",
      "1: μ ←\n",
      "1\n",
      "hw\n",
      "h\n",
      "\u0002\n",
      "i=1\n",
      "w\n",
      "\u0002\n",
      "j=1\n",
      "xij\n",
      "2: δ2 ←\n",
      "1\n",
      "hw\n",
      "h\n",
      "\u0002\n",
      "i=1\n",
      "w\n",
      "\u0002\n",
      "j=1\n",
      "(xij − μ)2\n",
      "3: if δs is given then\n",
      "4:\n",
      "// using δs to re-estimate feature statistics for ﬁnal segmentation\n",
      "5:\n",
      "μ′, δ′ ← eμ (μ, δ, δs), eδ (μ, δ, δs)\n",
      "6:\n",
      "if training then\n",
      "7:\n",
      "// updating δsr a during training\n",
      "8:\n",
      "δsr a ← (1 − α)δsr a + αδs\n",
      "9:\n",
      "end if\n",
      "10: else\n",
      "11:\n",
      "// using δsr a to re-estimate feature statistics for ratio prediction\n",
      "12:\n",
      "μ′, δ′ ← eμ (μ, δ, δsr a ), eδ (μ, δ, δsr a )\n",
      "13: end if\n",
      "14: y ← (x − μ′)/δ′\n",
      "15: return y\n",
      "to verify the above viewpoint, we evaluate the baseline model under diﬀerent\n",
      "foreground-background ratios.\n",
      "speciﬁcally, we ﬁrst remove the foreground pixels\n",
      "via in-painting [27], and then pad the original testing images with the obtained\n",
      "background patches.\n",
      "we adopt b to denote the ratio between the size of the\n",
      "obtained new image and the original image size.\n",
      "compared with the original\n",
      "images, the new images have the same foreground regions but more background\n",
      "pixels, and thus have diﬀerent foreground-background ratios.\n",
      "finally, we evaluate\n",
      "the performance of the baseline model with diﬀerent b values.\n",
      "experimental\n",
      "results are presented in table 1.\n",
      "it is shown that the value of b aﬀects the model\n",
      "performance signiﬁcantly.\n",
      "the above problem is common in nucleus segmentation because pathologi-\n",
      "cal images from diﬀerent organs or tissues tend to have signiﬁcantly diﬀerent\n",
      "foreground-background ratios.\n",
      "however, this phenomenon is often ignored in\n",
      "existing research.\n",
      "the structures of eμ and eδ are included in the\n",
      "supplemental materials.\n",
      "596\n",
      "s. chen et al.\n",
      "as shown in fig.\n",
      "2, to obtain the foreground-background ratio ρ of one input\n",
      "image, we ﬁrst feed it to the model encoder with δsra as the additional input.\n",
      "δsra acts as pseudo residuals of feature statistics and is obtained in the training\n",
      "stage via averaging δs in a momentum fashion.\n",
      "here, f is a 1 × 1 convolutional layer that transforms ρ\n",
      "to a feature vector whose dimension is the same as the target layer’s channel\n",
      "number.\n",
      "after that, the input image is fed into the model again with δs as\n",
      "additional input and ﬁnally makes more accurate predictions.\n",
      "the training of rph requires an extra loss term lrph, which is formulated\n",
      "as bellow:\n",
      "lrph = lbce(ρ, ρg) + lmse(f(ρ), f(ρg)),\n",
      "(1)\n",
      "where ρg denotes the ground truth foreground-background ratio, and lbce\n",
      "and lmse denote the binary cross entropy loss and the mean squared error,\n",
      "respectively.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "datasets\n",
      "the proposed method is evaluated on four datasets, including two h&e stained\n",
      "image datasets\n",
      "consep [3] contains 28 training and\n",
      "14 validation images, whose sizes are 1000×1000 pixels.\n",
      "the images are extracted\n",
      "from 16 colorectal adenocarcinoma wsis, each of which belongs to an individual\n",
      "patient, and scanned with an omnyx vl120 scanner within the department of\n",
      "pathology at university hospitals coventry and warwickshire, uk. cpm17\n",
      "[28] contains 32 training and 32 validation images, whose sizes are 500 ×\n",
      "the images are selected from a set of glioblastoma multiforme, lower\n",
      "grade glioma, head and neck squamous cell carcinoma, and non-small cell\n",
      "lung cancer whole slide tissue images.\n",
      "[29] contains 575 training and\n",
      "91 validation images, whose sizes are 512 × 512 pixels.\n",
      "the images are extracted\n",
      "from the slides of lung and bladder tissues.\n",
      "[29,32] contains 385\n",
      "training and 66 validation ki67 stained images of breast carcinoma, whose sizes\n",
      "are 512 × 512 pixels.\n",
      "3.2\n",
      "implementation details\n",
      "in the training stage, patches of size 224×224 pixels are randomly cropped from\n",
      "the original samples.\n",
      "we adopt the standard augmentation, like image color jittering and\n",
      "gaussian blurring.\n",
      "in all experiments, the segmentation and contour detection\n",
      "predictions are penalized using the binary cross entropy loss.\n",
      "darc: distribution-aware re-coloring model\n",
      "597\n",
      "table 2. comparisons in generalization performance on nucleus segmentation datasets.\n",
      "results in each column are related to models trained on one domain and evaluated on\n",
      "the other three unseen domains.\n",
      "results are in percentages.\n",
      "methods\n",
      "consep\n",
      "cpm17\n",
      "deepliif\n",
      "bc-deepliif average\n",
      "aji\n",
      "dice\n",
      "aji\n",
      "dice\n",
      "aji\n",
      "dice\n",
      "aji\n",
      "dice\n",
      "aji\n",
      "dice\n",
      "baseline (bn)\n",
      "16.67 24.10 33.30 61.18 08.42 38.17 21.27 39.92\n",
      "19.92\n",
      "40.84\n",
      "baseline (in)\n",
      "32.13 48.67 33.94 65.83 41.48 67.17 21.52 37.49\n",
      "32.27\n",
      "54.79\n",
      "bin\n",
      "models\n",
      "#parameters (m)\n",
      "inference time (s/image)\n",
      "baseline (in)\n",
      "5.03\n",
      "0.0164\n",
      "darcenc\n",
      "5.47\n",
      "0.0253\n",
      "3.3\n",
      "experimental results and analyses\n",
      "in this paper, the models are compared using the aji\n",
      "in\n",
      "the experiments, models trained on one of the datasets will be evaluated on the\n",
      "three unseen ones.\n",
      "to avoid the inﬂuence of the diﬀerent sample numbers of the\n",
      "datasets, we calculate the average scores within each unseen domain respectively\n",
      "and then average them across domains.\n",
      "in this paper, we re-implement some existing popular domain generalization\n",
      "algorithms for comparisons under the same training conditions.\n",
      "speciﬁcally, we\n",
      "re-implement the tent [34], bin [19], dsu [20], frequency amplitude nor-\n",
      "malization (ampnorm)\n",
      "[38] and stain mix-up [39] methods that are popular\n",
      "in pathological image analysis.\n",
      "their performances are presented in table 2.\n",
      "the normalization layers in the encoder with dain and uses bn in its decoder.\n",
      "as shown in table 2, darcenc achieves the best average performance among all\n",
      "methods.\n",
      "speciﬁcally, darcenc improves the baseline model’s average aji and\n",
      "dice scores by 4.81% and 7.04%.\n",
      "compared with the other domain generalization\n",
      "methods, dain, dain w/o ratio, darcall and darcenc achieve impressive\n",
      "performances on bc-deepliif, which justify that re-estimating the instance-\n",
      "wise statistics is important for improving the domain generalization ability of\n",
      "models trained on bc-deepliif.\n",
      "compared\n",
      "with the baseline model, rc improves the average aji and dice scores by 1.41%\n",
      "and 2.59%, and dain improves the average aji and dice scores by 1.13% and\n",
      "4.08%.\n",
      "compared with the variant model without foreground-background ratio\n",
      "prediction, dain improves the average aji and dice scores by 0.90% and 3.74%.\n",
      "finally, the combinations of rc and dain, i.e., darcall and darcenc, achieve\n",
      "the best average scores.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf:\n",
      "however,\n",
      "two major challenges hinder accurate nuclei segmentation from thyroid\n",
      "cytology.\n",
      "firstly, unbalanced distribution of nuclei morphology across\n",
      "diﬀerent tbsrtc categories can lead to a biased model.\n",
      "secondly,\n",
      "the insuﬃciency of densely annotated images results in a less gener-\n",
      "alized model.\n",
      "in contrast, image-wise tbsrtc labels, while contain-\n",
      "ing lightweight information, can be deeply explored for segmentation\n",
      "guidance.\n",
      "to this end, we propose a tbsrtc-category aware nuclei\n",
      "segmentation framework (tcsegnet).\n",
      "to top up the small amount of\n",
      "pixel-wise annotations and eliminate the category preference, a larger\n",
      "amount of image-wise labels are taken in as the complementary supervi-\n",
      "sion signal in tcsegnet.\n",
      "this integration of data can eﬀectively guide\n",
      "the pixel-wise nuclei segmentation task with a latent global context.\n",
      "we also propose a semi-supervised extension of tcsegnet that lever-\n",
      "ages images with only tbsrtc-category labels.\n",
      "our tcsegnet outperforms state-of-the-art segmentation\n",
      "approaches with an improvement of 2.0% dice and 2.7% iou; besides,\n",
      "the semi-supervised extension can further boost this margin.\n",
      "in conclu-\n",
      "sion, our study explores the weak annotations by constructing an image-\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43987-2_56\n",
      "tcseqnet\n",
      "581\n",
      "wise-label-guided nuclei segmentation framework, which has the poten-\n",
      "tial medical importance to assist thyroid abnormality examination.\n",
      "keywords: unbalanced nuclei segmentation · semi-supervised\n",
      "learning · thyroid cytology · tbsrtc diagnostic category\n",
      "1\n",
      "introduction\n",
      "thyroid cancer is the most common cancer of the endocrine system, account-\n",
      "ing for 2.1% of all malignant cancers [1].\n",
      "the emergence of computational pathology allows automatic diagno-\n",
      "sis of thyroid cancer, and nuclei segmentation becomes one of the most critical\n",
      "diagnostic tasks [4,5], as the shapes of nuclei, whether round, oval, or elongated,\n",
      "can provide valuable information for further analysis [6].\n",
      "for example, small and\n",
      "scattered thyroid cells with a light hue and relatively low cell density are usually\n",
      "low-grade and indicative of early-stage cancer; whereas large and dark cells with\n",
      "extreme-dense agglomeration are usually middle- or late-grade [3]. correspond-\n",
      "ingly, accurate location of cell boundaries is essential for both pathologists and\n",
      "computer-aided diagnosis (cad) systems to assist decision [7].\n",
      "however, nuclei segmentation in thyroid cytopathology is still challenged\n",
      "by the varying cellularity of images from diﬀerent tbsrtc categories [3,8].\n",
      "for example, benign cells (i & ii) present high sparsity and are diﬃcult to be\n",
      "distinguished from background tissues, thus may account for a relatively small\n",
      "proportion when equal images are involved in a training set [3].\n",
      "in this way, an unbalanced distribution\n",
      "across diﬀerent categories resulted, correspondingly, the training leads to biased\n",
      "models with lower accuracy\n",
      "such distinct morphological diﬀerences can\n",
      "be characterized by the tbsrtc category, which thus inspires us to utilize\n",
      "the handy image-wise grading labels to guide the nuclei segmentation model\n",
      "learning from unbalanced datasets.\n",
      "moreover, amongst multiple annotation paradigms [12], pixel-\n",
      "level labeling is the most time-consuming and laborious, whereas the image-wise\n",
      "diagnostic labels, i.e. tbsrtc categories, are comparatively simpler.\n",
      "despite\n",
      "the labeling intensity, prevalent nuclei segmentation methods, e.g., cia-net\n",
      "[15], are limited to pixel-wise annotations, where\n",
      "the potential beneﬁts of integrating accessible image-wise labels are unaware.\n",
      "to narrow the gap discussed, we propose a novel tbsrtc-category-aware\n",
      "nuclei segmentation framework.\n",
      "(1) we pro-\n",
      "pose a cytopathology nuclei segmentation network named tcsegnet, to pro-\n",
      "vide supplementary guidance to facilitate the learning of nuclei boundaries.\n",
      "582\n",
      "j. zhu et al.\n",
      "innovatively, our approach can help reduce bias in the learning process of the\n",
      "segmentation model with the routine unbalanced training set.\n",
      "(2) we expand\n",
      "tcsegnet to semi-tcsegnet to leverage image-wise labels in a semi-supervised\n",
      "learning manner, which signiﬁcantly reduces the reliance on annotation-intensive\n",
      "pixel-wise labels.\n",
      "additionally, an hsv-intensity noise is designed speciﬁcally\n",
      "for cytopathology images to boost the generalization ability.\n",
      "(3) we establish a\n",
      "dataset of thyroid cytopathology image patches of 224 × 224, where 4,965 image\n",
      "labels are provided following tbsrtc, and 1,473 of them are densely anno-\n",
      "tated [3] (to be on github upon acceptance).\n",
      "to the best of our knowledge,\n",
      "it is the ﬁrst publicized thyroid cytopathology dataset of both image-wise and\n",
      "pixel-wise labels.\n",
      "tcsegnet uti-\n",
      "lizes annotation-lightweight image-wise tbsrtc-category labels to aid in the learning\n",
      "of unbalanced nuclei morphology in segmentation.\n",
      "we propose a novel tbsrtc-category aware segmentation net-\n",
      "work (tcsegnet) to segment nuclei boundaries in cytopathology images, which\n",
      "tcseqnet\n",
      "583\n",
      "is guided by tbsrtc-category label to learn from unbalanced data.\n",
      "considering\n",
      "the spatial distributions of thyroid cells in cytopathology images, our design\n",
      "provides extended global information for more accurate segmentation.\n",
      "formally, the overall segmentation loss lseg to train\n",
      "our model is a combination of the binary cross-entropy loss (bce), i.e.\n",
      "lseg = γni · bce(ˆycnn\n",
      "ni , yni)\n",
      "this block consists of two learnable fully connected\n",
      "layers that process the feature extracted by the cnn and transformer branches\n",
      "separately, which obtains image-wise tbsrtc-category prediction denoted as\n",
      "ˆycnn\n",
      "cls and ˆytrans\n",
      "cls\n",
      ".\n",
      "= ce(ˆycnn\n",
      "cls , ycls) + γcls · ce(ˆytrans\n",
      "cls\n",
      ", ycls),\n",
      "(2)\n",
      "where ycls is the image-wise tbsrtc-category label, and the balancing coeﬃ-\n",
      "cient γcls is set to 3, as the global feature captured by the transformer branch\n",
      "is tightly correlated with the image-level classiﬁcation tag.\n",
      "to leverage images that only have\n",
      "image-wise labels, we extend to a semi-supervised mean teacher [18] framework\n",
      "called semi-tcsegnet.\n",
      "in this framework, both the student and teacher share the\n",
      "same full-supervised nuclei segmentation architecture of tcsegnet.\n",
      "the weights\n",
      "of the teacher θt are updated with the exponential moving average (ema) of the\n",
      "weights of student θs, and smoothing coeﬃcient α = 0.99, following the previous\n",
      "work [19].\n",
      "formally, the weights of the teacher at e-th epoch are updated by\n",
      "θe\n",
      "t = αθe−1\n",
      "t\n",
      "+ (1 − α)θe\n",
      "s.\n",
      "(4)\n",
      "584\n",
      "j. zhu et al.\n",
      "during the training stage, the teacher model assigns pixel-wise soft labels to\n",
      "the images with exclusive image-wise labels, thus expanding the scale of labeled\n",
      "data to the student model.\n",
      "(3) computed on fully-annotated\n",
      "data, and the semi-supervised loss lss computed on data with only image-wise\n",
      "labels.\n",
      "it follows that the overall training objective in semi-tcsegnet is to min-\n",
      "imize the loss function l = ls + λss · lss, where lss measures the segmentation\n",
      "consistency between the teacher and student models via l2-norm.\n",
      "the traditional method of integrating gaussian noise\n",
      "in the mean teacher [18] may be problematic when working with cytopathology\n",
      "images that have an imbalanced color distribution.\n",
      "speciﬁcally, xv is the pixel value\n",
      "of the image’s v channel in hsv space, and hyper-parameter λv is set as 0.5\n",
      "to control the amplitude of the intensity-based noise.\n",
      "finally, the value of the\n",
      "obtained noise is clamped to [−0.2, 0.2] before being added to the images.\n",
      "3\n",
      "experiments\n",
      "image dataset.\n",
      "we construct a clinical thyroid cytopathology dataset with\n",
      "images of both image-wise and pixel-wise labels as a benchmark (appear in\n",
      "github upon acceptance)\n",
      "some representative images are presented in fig.\n",
      "the dataset comprises 4,965 h&e stained\n",
      "image patches and labels of tbsrtc, where a subset of 1,473 images was\n",
      "densely annotated for nuclei boundaries by three experienced cytopathologists\n",
      "and reached a total number of 31,064 elaborately annotated nuclei.\n",
      "patient-level\n",
      "images were partitioned ﬁrst for training and test images, and patch-level cura-\n",
      "tion was performed.\n",
      "we divided the dataset with image-wise labels into 80%\n",
      "training samples and the remaining 20% testing samples.\n",
      "our collection of thy-\n",
      "roid cytopathology images was granted with an ethics approval document.\n",
      "quantitative comparisons in both fully-supervised and semi-supervised man-\n",
      "ners.\n",
      "the best performance is highlighted in bold, where we can observe that both\n",
      "tcsegnet and its semi-supervised extension outperform state-of-the-art.\n",
      "[15]\n",
      "0.866\n",
      "0.775\n",
      "mtmt-net [27]\n",
      "0.878\n",
      "0.789\n",
      "semi-tcsegnet (ours) 0.889 0.805\n",
      "implementations.\n",
      "the proposed method and compared methods are imple-\n",
      "mented on a single nvidia geforce rtx 3090 gpu card.\n",
      "both tcsegnet and semi-tcsegnet use sgd optimizer with a\n",
      "momentum of 0.9 and a weight decay of 10−4.\n",
      "we set the batch size for tcsegnet to 8, and for semi-tcsegnet\n",
      "to 10, i.e. 8 fully-annotated images and 2 partially-annotated images per batch.\n",
      "we compared tcsegnet\n",
      "with the fully-supervised counterparts, including method speciﬁc for segmenta-\n",
      "tion in general image [20,22], medical image\n",
      "intersection over union (iou) and dice score were applied as\n",
      "the evaluation metrics, where a higher value indicated a better semantic seg-\n",
      "mentation performance.\n",
      "3. examples of segmented nuclei in thyroid cytopathology images by tcsegnet\n",
      "and prevent fully-supervised models methods.\n",
      "experimental results.\n",
      "the results in table 1 indicated that tcsegnet can\n",
      "achieve the highest performance by a dice score of 87.7% and an iou of 78.8%.\n",
      "the performance values in the challenging regions are highlighted with red boxes\n",
      "in fig.\n",
      "our approach is capable\n",
      "to address the current issue in the recognition and segmentation of small iso-\n",
      "lated cells graded in the i category, which is always ignored by the unbalanced\n",
      "pixel-wise cell morphology with other approaches.\n",
      "also, it yields that the incor-\n",
      "poration of tbsrtc-category can contribute to a partial alleviation of a biased\n",
      "model, resulting in more satisfying segmentation performance experimentally.\n",
      "furthermore, the fact that the tbsrtc-category label is easy to obtain endows\n",
      "tcseqnet\n",
      "587\n",
      "the applicability of our model to various circumstances that nuclei in various\n",
      "sizes, shapes, and dyeing styles can be accurately recognized and segmented.\n",
      "moreover, with the semi-supervised\n",
      "learning, semi-tcsegnet can further boost the performance to an 88.9% dice\n",
      "score, and 80.5% iou, by leveraging additional data with image-wise tbsrtc-\n",
      "category labels solely.\n",
      "the performance improvement of 1.2% dice, 1.7% iou,\n",
      "together with the general improvement is shown in the boxplot in fig.\n",
      "4 (c, d), as\n",
      "a demonstration of the advantage using full data resources with semi-tcsegnet.\n",
      "fig.\n",
      "c and d are the distribution of the dice score and iou of all mentioned models\n",
      "respectively.\n",
      "our models presented a general improvement across the metrics.\n",
      "ablation study.\n",
      "the results indicate that performance improvement is\n",
      "accumulated with increasing data size.\n",
      "besides, training with a classiﬁcation-\n",
      "learning block alone can increase the nuclei segmentation performance by 1.7%\n",
      "and 2.6% in the dice score and iou, respectively.\n",
      "meanwhile, trained with\n",
      "specially designed hsv-intensity noise can also increase the performance by\n",
      "0.9% dice and 1.4% iou, showing its potential for generation ability improve-\n",
      "ment.\n",
      "importantly, the beneﬁts from the two blocks are orthonormal, where\n",
      "semi-tcsegnet achieves the optimal performance with the utilization of both.\n",
      "588\n",
      "j. zhu et al.\n",
      "table 2. ablation study for our semi-tcsegnet and functional blocks.\n",
      "classiﬁcation\n",
      "learning\n",
      "hsv-intensity\n",
      "noise\n",
      "dice\n",
      "iou\n",
      "0.867\n",
      "0.771\n",
      "\u0002\n",
      "0.876\n",
      "0.785\n",
      "\u0002\n",
      "0.884\n",
      "0.797\n",
      "\u0002\n",
      "\u0002\n",
      "0.889 0.805\n",
      "w. image-wise data\n",
      "+1k data\n",
      "0.879\n",
      "0.790\n",
      "+2k data\n",
      "0.882\n",
      "0.795\n",
      "4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf:\n",
      "due to the huge gigapixel-level size and diverse nature\n",
      "of whole-slide images (wsis), analyzing them through multiple instance\n",
      "learning (mil) has become a widely-used scheme, which, however, faces\n",
      "the challenges that come with the weakly supervised nature of mil. con-\n",
      "ventional mil methods mostly either utilized instance-level or bag-level\n",
      "supervision to learn informative representations from wsis for down-\n",
      "stream tasks.\n",
      "in this work, we propose a novel mil method for patholog-\n",
      "ical image analysis with integrated instance-level and bag-level supervi-\n",
      "sion (termed iib-mil).\n",
      "extensive experiments demonstrate\n",
      "that iib-mil outperforms state-of-the-art approaches in both bench-\n",
      "marking datasets and addressing the challenging practical clinical task.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 54.\n",
      "as the demand for intelligently pathological image analysis continues to grow,\n",
      "an increasing number of researchers have paid attention to this ﬁeld [12,14,\n",
      "25].\n",
      "however, pathological image analysis remains a challenging task due to the\n",
      "complex and heterogeneous nature\n",
      "[19] of obtained whole slide images (wsis),\n",
      "as well as their huge gigapixel-level size [20]. to address this issue, multiple\n",
      "instance learning (mil)\n",
      "[1] is usually applied to formulate pathological image\n",
      "analysis tasks into weakly supervised learning problems.\n",
      "although mil-based methods have shown impressive\n",
      "potential in solving a wide range of pathological image analysis tasks including\n",
      "cancer grading and subtype diagnosis [23], prognosis prediction [18], genotype-\n",
      "related tasks such as gene mutation prediction [4], etc., it is still an open question\n",
      "regarding learning an informative and eﬀective representation of the entire wsi\n",
      "for down-streaming task based on mil architecture.\n",
      "[9,17], also known as embedding-\n",
      "based mil, involves converting patches (instances) into low-dimensional embed-\n",
      "dings, which are then aggregated into wsi (bag)-level representations to conduct\n",
      "the analysis tasks [22].\n",
      "the instance-level mil, however, faces the problem of noisy labels,\n",
      "which is caused by the common strategy of assigning the wsi labels to patches\n",
      "and the fact that there are lots of patches irrelevant to the wsi labels [3,6].\n",
      "considering these conventional mil methods usually utilize either bag-level\n",
      "or instance-level supervision, leading to suboptimal performance.\n",
      "then we propose to combine bag-level and instance-level supervision\n",
      "to improve the performance of mil.\n",
      "the detailed contributions can\n",
      "be summarized as follows:\n",
      "1) we propose a novel mil method for pathological image analysis that leverages\n",
      "a specially-designed residual transformer backbone and organically integrates\n",
      "both transformer-based bag-level and label-disambiguation-based instance-\n",
      "level supervision for performance enhancement.\n",
      "2) we develop a label-disambiguation module that leverages prototypes and con-\n",
      "ﬁdence bank to tackle the weakly supervised nature of instance-level super-\n",
      "vision and reduce the impact of assigned noisy labels.\n",
      "3) the proposed framework outperforms state-of-the-art (sota) methods on\n",
      "public datasets and in a practical clinical task, demonstrating its superiors\n",
      "in wsi analysis.\n",
      "since\n",
      "bag-level supervision channel is trained to globally summarise information of all\n",
      "patches for prediction, the bag-level outputs are used as the ﬁnal predictions\n",
      "during the test stage.\n",
      "2.2\n",
      "problem formulation\n",
      "assume there is a set of n wsis denoted by s = {s1, s2, ..., sn}.\n",
      "to reduce the\n",
      "computational cost, we used a frozen pre-trained encoder to transform patches\n",
      "into k dimensional embeddings {ei,j|ei,j ∈ rk, i ∈\n",
      "t(·) maps\n",
      "patch embeddings {ei,j, ...} to a lower-dimensional feature space, denoted as\n",
      "{xi,j, ...}, where xi,j = t(ei,j), xi,j ∈ rd is the calibrated embedding, t(·) is\n",
      "composed of transformer layers and skip connections (details are given in the\n",
      "supplementary.).\n",
      "the prototypes, denoted as p ∈ rc×d, are initialized with all-zero vectors\n",
      "and employ momentum-based updates using selected instance features x with\n",
      "564\n",
      "q. ren et al.\n",
      "the highest probability probinst of belonging to their corresponding categories.\n",
      "conﬁdence b ∈ rn×m×c is initialized with all wsi labels and\n",
      "uses momentum-based updates with z. detailed steps are summarized as follows:\n",
      "step 1: obtain the instance classiﬁer output.\n",
      "then, we use a\n",
      "momentum-based update rule to obtain pc,t+1:\n",
      "pc,t+1 = α · pc,t\n",
      "xi,j, if xi,j ∈ setc,t,\n",
      "(3)\n",
      "where α is the momentum coeﬃcient that automatically decreases from α = 0.95\n",
      "to α = 0.8 across epochs.\n",
      "zi,j,\n",
      "(5)\n",
      "where β is the momentum update parameter with a default value of β = 0.99.\n",
      "rm×d → rd and a wsi classiﬁer fbag(·) :\n",
      "rd → rc in turn (architecture details are given in the supplementary.).\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "dataset\n",
      "we evaluate our model with three datasets.\n",
      "(1) luad-gm dataset: the objec-\n",
      "tive is to predict the epidermal growth factor receptor (egfr) gene mutations\n",
      "in patients with lung adenocarcinoma (luad) using 723 whole slide image\n",
      "(wsi) slices, where 47% of cases have egfr mutations.\n",
      "3.2\n",
      "experiment settings\n",
      "the dataset was randomly split into three parts: training, validation, and testing,\n",
      "with 60%, 20%, and 20% of the samples, respectively.\n",
      "the proposed\n",
      "model was implemented in pytorch, trained on a 32gb tesla v100 gpu,\n",
      "using adamw\n",
      "the performance of iib-mil compared with other sota methods.\n",
      "= 1\n",
      "83.19\n",
      "71.23\n",
      "98.05\n",
      "91.39\n",
      "99.05\n",
      "93.12\n",
      "λ = 5\n",
      "85.62\n",
      "78.77\n",
      "98.11\n",
      "90.91\n",
      "99.57\n",
      "95.24\n",
      "λ = 10\n",
      "85.60\n",
      "76.03\n",
      "96.51\n",
      "89.47\n",
      "99.23\n",
      "89.95\n",
      "4\n",
      "results and discussion\n",
      "4.1\n",
      "comparison with state-of-the art methods\n",
      "table 1 presents a performance comparative analysis of iib-mil in relation to\n",
      "other sota methods, including abmil\n",
      "we can also ﬁnd iib-mil outperformed other sota methods, in the\n",
      "three tasks with at least 1.78%, 0.74%, and 0.56% performance enhancement\n",
      "(auc), respectively.\n",
      "4.2\n",
      "ablation studies\n",
      "we conducted ablation studies to assess the eﬃcacy of each component in iib-\n",
      "mil.\n",
      "we also investigated the\n",
      "impact of the warm-up epoch number and found that selecting an appropriate\n",
      "value, such as warmup = 10, can lead to better model performance.\n",
      "further-\n",
      "more, we examined the impact of the weighting factor λ, and the outcomes indi-\n",
      "cated that assigning greater importance to instance-level supervision (λ = 5)\n",
      "helps iib-mil enhance its performance, thus demonstrating the eﬀectiveness of\n",
      "the designed label-disambiguation-based instance-level supervision.\n",
      "4.3\n",
      "model interpretation\n",
      "figure 2(a) shows the t-sne plot of the obtained patch features from the back-\n",
      "bone of the iib-mil.\n",
      "the numbers displayed within\n",
      "each group represent the average likelihood of the egfr mutation predicted\n",
      "by the patches.\n",
      "the\n",
      "resulting heatmap shows the decision mechanism of iib-mil in the accurate\n",
      "distinguishment between egfr mutation-positive and negative samples.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf:\n",
      "we introduce a new ai-ready computational pathology\n",
      "dataset containing restained and co-registered digitized images from\n",
      "eight head-and-neck squamous cell carcinoma patients.\n",
      "as opposed to subjec-\n",
      "tive and error-prone immune cell annotations from individual pathol-\n",
      "ogists (disagreement > 50%) to drive sota deep learning approaches,\n",
      "this dataset provides objective immune and tumor cell annotations via\n",
      "mif/mihc restaining for more reproducible and accurate characteri-\n",
      "zation of tumor immune microenvironment (e.g. for immunotherapy).\n",
      "we demonstrate the eﬀectiveness of this dataset in three use cases: (1)\n",
      "ihc quantiﬁcation of cd3/cd8 tumor-inﬁltrating lymphocytes via style\n",
      "transfer, (2) virtual translation of cheap mihc stains to more expensive\n",
      "mif stains, and (3) virtual tumor/immune cellular phenotyping on stan-\n",
      "dard hematoxylin images.\n",
      "keywords: multiplex immuoﬂuorescence · multiplex\n",
      "immunohistochemistry · tumor microenvironment · virtual\n",
      "stain-to-stain translation\n",
      "p. ghahremani, j. marino, c. h. chung, and s. nadeem—equal contribution.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 68.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43987-2_68\n",
      "an ai-ready multiplex staining dataset\n",
      "705\n",
      "1\n",
      "introduction\n",
      "accurate spatial characterization of tumor immune microenvironment is critical\n",
      "for precise therapeutic stratiﬁcation of cancer patients (e.g. via immunother-\n",
      "apy).\n",
      "currently, this characterization is done manually by individual pathologists\n",
      "on standard hematoxylin-and-eosin (h&e) or singleplex immunohistochemistry\n",
      "(ihc) stained images.\n",
      "however, this results in high interobserver variability\n",
      "among pathologists, primarily due to the large (> 50%) disagreement among\n",
      "pathologists for immune cell phenotyping [10].\n",
      "this is also a big cause of con-\n",
      "cern for publicly available h&e/ihc cell segmentation datasets with immune\n",
      "cell annotations from single pathologists.\n",
      "in contrast, current brightﬁeld mihc staining\n",
      "protocols relying on dab (3,3’-diaminobenzidine) alcohol-insoluble chromogen,\n",
      "even though easily implementable with current clinical staining protocols, suf-\n",
      "fer from occlusion of signal from sequential staining of additional markers.\n",
      "this\n",
      "706\n",
      "p. ghahremani et al.\n",
      "requires only aﬃne registration to align the digitized restained images to obtain\n",
      "non-occluded signal intensity proﬁles for all the markers, similar to mif stain-\n",
      "ing/scanning.\n",
      "in this paper, we introduce a new dataset that can be readily used out-of-\n",
      "the-box with any artiﬁcial intelligence (ai)/deep learning algorithms for spatial\n",
      "characterization of tumor immune microenvironment and several other use cases.\n",
      "to date, only two denovo stained datasets have been released publicly: bci h&e\n",
      "and singleplex ihc her2 dataset [7] and deepliif singleplex ihc ki67 and\n",
      "mif dataset\n",
      "in contrast, we\n",
      "release the ﬁrst denovo mif/mihc stained dataset with tumor and immune\n",
      "markers for more accurate characterization of tumor immune microenvironment.\n",
      "we also demonstrate several interesting use cases: (1) ihc quantiﬁcation of\n",
      "cd3/cd8 tumor-inﬁltrating lymphocytes (tils) via style transfer, (2) virtual\n",
      "translation of cheap mihc stains to more expensive mif stains, and (3) virtual\n",
      "tumor/immune cellular phenotyping on standard hematoxylin images.\n",
      "demographics and other relevant details of the eight anonymized head-and-\n",
      "neck squamous cell carcinoma patients, including ecog performance score, pack-year,\n",
      "and surgical pathology stage (ajcc8).\n",
      "id\n",
      "age gender race\n",
      "ecog smoking py\n",
      "pstage cancer site\n",
      "cancer subsite\n",
      "case1 49\n",
      "male\n",
      "white 3\n",
      "current\n",
      "21\n",
      "1\n",
      "oral cavity ventral tongue\n",
      "case2 64\n",
      "male\n",
      "white 3\n",
      "former\n",
      "20\n",
      "4\n",
      "larynx\n",
      "vocal cord\n",
      "case3 60\n",
      "male\n",
      "black\n",
      "2\n",
      "current\n",
      "45\n",
      "4\n",
      "larynx\n",
      "false vocal cord\n",
      "case4\n",
      "53\n",
      "male\n",
      "white 1\n",
      "current\n",
      "68\n",
      "4\n",
      "larynx\n",
      "supraglottic\n",
      "case5 38\n",
      "male\n",
      "white 0\n",
      "never\n",
      "0\n",
      "4\n",
      "oral cavity lateral tongue\n",
      "case6 76\n",
      "female\n",
      "white 1\n",
      "former\n",
      "30\n",
      "2\n",
      "oral cavity lateral tongue\n",
      "case7 73\n",
      "male\n",
      "white 1\n",
      "former\n",
      "100\n",
      "3\n",
      "larynx\n",
      "glottis\n",
      "case8 56\n",
      "male\n",
      "white 0\n",
      "never\n",
      "0\n",
      "2\n",
      "oral cavity tongue\n",
      "2\n",
      "dataset\n",
      "the complete staining protocols for this dataset are given in the accompany-\n",
      "ing supplementary material.\n",
      "images were acquired at 20× magniﬁcation at\n",
      "moﬃtt cancer center.\n",
      "the demographics and other relevant information for all\n",
      "eight head-and-neck squamous cell carcinoma patients is given in table 1.\n",
      "2.1\n",
      "region-of-interest selection and image registration\n",
      "after scanning the full images at low resolution, nine regions of interest (rois)\n",
      "from each slide were chosen by an experienced pathologist on both mif and\n",
      "mihc images: three in the tumor core (tc), three at the tumor margin (tm),\n",
      "and three outside in the adjacent stroma (s) area.\n",
      "hematoxylin-stained rois were ﬁrst used to align all\n",
      "an ai-ready multiplex staining dataset\n",
      "707\n",
      "the mihc marker images in the open source fiji software using aﬃne registra-\n",
      "tion.\n",
      "after that, hematoxylin- and dapi-stained rois were used as references\n",
      "to align mihc and mif rois again using fiji and subdivided into 512×512\n",
      "patches, resulting in total of 268 co-registered mihc and mif patches (∼33\n",
      "co-registered mif/mihc images per patient).\n",
      "second column shows stains extracted from ﬁrst column\n",
      "mihc-aec images using otsu thresholding.\n",
      "third column shows the corresponding\n",
      "perfectly co-registered original mif images.\n",
      "using the mif image, we separated fore-\n",
      "ground of the mihc-aec image from its background and calculated the mean value\n",
      "of the foreground pixels as well as the background pixels.\n",
      "each square represents an image in the dataset and\n",
      "the top half of each square shows the mean color value of the positive cells, extracted\n",
      "from mihc-aec using its corresponding mif image and the bottom half of it shows\n",
      "the mean color value of its background.\n",
      "the last column shows the rmse and\n",
      "ssim diagrams of all four stains calculated using the extracted stain from ihc-aec\n",
      "images (second column) and the mif images (third column).\n",
      "the low error rate of\n",
      "rmse and high structural similarity seen in these diagrams show high concordance\n",
      "among mihc-aec and mif images.\n",
      "708\n",
      "p. ghahremani\n",
      "3. examples of synthesized ihc images and corresponding input images.\n",
      "style ihc\n",
      "images were taken from the public lyon19 challenge dataset [14].\n",
      "we used grayscale\n",
      "hematoxylin images because they performed better with style transfer.\n",
      "fig.\n",
      "4. examples of hematoxylin, mif dapi, mif cd3 and classiﬁed segmentation\n",
      "mask for this marker.\n",
      "the dapi images were segmented using cellpose [13] and man-\n",
      "ually corrected by a trained technician and approved by a pathologist.\n",
      "the segmented\n",
      "masks were classiﬁed using the cd3 channel intensities.\n",
      "[15] nuclick\n",
      "0.49 ± 0.30 0.37 ± 0.25 0.63 ± 0.37 2.75 ± 5.29\n",
      "our dataset 0.53 ± 0.30 0.41 ± 0.26 0.70 ± 0.36 2.19 ± 2.89\n",
      "3.1\n",
      "ihc cd3/cd8 scoring using mif style transfer\n",
      "we generate a stylized ihc image (fig. 3) using three input images: (1) hema-\n",
      "toxylin image (used for generating the underlying structure of cells in the stylized\n",
      "image), (2) its corresponding mif cd3/cd8 marker image (used for staining\n",
      "positive cells as brown), and (3) sample ihc style image (used for transferring\n",
      "its style to the ﬁnal image).\n",
      "the complete architecture diagram is given in the\n",
      "supplementary material.\n",
      "speciﬁcally, the model consists of two sub-networks:\n",
      "(a) marker generation: this sub-network is used for generating mif marker\n",
      "data from the generated stylized image.\n",
      "[4] for generating the marker images.\n",
      "the cgan network\n",
      "consists of a generator, responsible for generating mif marker images given an\n",
      "ihc image, and a discriminator, responsible for distinguishing the output of the\n",
      "generator from ground truth data.\n",
      "we ﬁrst extract the brown (dab channel)\n",
      "from the given style ihc image, using stain deconvolution.\n",
      "then, we use pairs\n",
      "of the style images and their extracted brown dab marker images to train this\n",
      "sub-network.\n",
      "this sub-network improves staining of the positive cells in the ﬁnal\n",
      "stylized image by comparing the extracted dab marker image from the stylized\n",
      "image and the input mif marker image at each iteration.\n",
      "(b) style transfer: this sub-network creates the stylized ihc image using an\n",
      "attention module, given (1) the input hematoxylin and the mif marker images\n",
      "and (2) the style and its corresponding marker images.\n",
      "for synthetically gen-\n",
      "erating stylized ihc images, we follow the approach outlined in adaattn [8].\n",
      "this sub-network is used to create a stylized image using\n",
      "the structure of the given hematoxylin image while transferring the overall color\n",
      "distribution of the style image to the ﬁnal stylized image.\n",
      "the generated marker\n",
      "image from the ﬁrst sub-network is used for a more accurate colorization of the\n",
      "710\n",
      "p. ghahremani et al.\n",
      "positive cells against the blue hematoxylin counterstain/background; not deﬁn-\n",
      "ing loss functions based on the markers generated by the ﬁrst sub-network leads\n",
      "to discrepancy in the ﬁnal brown dab channel synthesis.\n",
      "for the stylized ihc images with ground truth cd3/cd8 marker images, we\n",
      "also segmented corresponding dapi images using our interactive deep learning\n",
      "impartial\n",
      "[9] tool https://github.com/nadeemlab/impartial and then classiﬁed\n",
      "the segmented masks using the corresponding cd3/cd8 channel intensities, as\n",
      "shown in fig.\n",
      "4. we extracted 268 tiles of size 512×512 from this ﬁnal segmented\n",
      "and co-registered dataset.\n",
      "for the purpose of training and testing all the models,\n",
      "we extract four images of size 256 × 256 from each tile due to the size of the\n",
      "external ihc images, resulting in a total of 1072 images.\n",
      "[14] to use as style ihc images.\n",
      "using\n",
      "these images, we created a dataset of synthetically generated ihc images from\n",
      "the hematoxylin and its marker image as shown in fig.\n",
      "3.\n",
      "we evaluated the eﬀectiveness of our synthetically generated dataset (styl-\n",
      "ized ihc images and corresponding segmented/classiﬁed masks) using our gener-\n",
      "ated dataset with the nuclick training dataset (containing manually segmented\n",
      "cd3/cd8 cells)\n",
      "lyon19 ihc cd3/cd8 images\n",
      "are taken from breast, colon, and prostate cancer patients.\n",
      "we split their training\n",
      "set into training and validation sets, containing 553 and 118 images, respectively,\n",
      "and use their validation set for testing our trained models.\n",
      "5. examples of ground-truth and generated mif data from mihc-aec images.\n",
      "we also tested the trained models on 1,500 randomly selected images from\n",
      "the training set of the lymphocyte assessment hackathon (lysto)\n",
      "[1], con-\n",
      "taining image patches of size 299 × 299 obtained at a magniﬁcation of 40× from\n",
      "breast, prostate, and colon cancer whole slide images stained with cd3 and cd8\n",
      "markers.\n",
      "only the total number of lymphocytes in each image patch are reported\n",
      "in this dataset.\n",
      "to evaluate the performance of trained models on this dataset,\n",
      "we counted the total number of marked lymphocytes in a predicted mask and\n",
      "calculated the diﬀerence between the reported number of lymphocytes in each\n",
      "image with the total number of lymphocytes in the predicted mask by the model.\n",
      "in table 2, the average diﬀerence value (diﬀcount) of lymphocyte number for\n",
      "the whole dataset is reported for each model.\n",
      "3.2\n",
      "virtual translation of cheap mihc to expensive mif stains\n",
      "unlike clinical dab staining, as shown in style ihc images in fig.\n",
      "3, where\n",
      "brown marker channel has a blue hematoxylin nuclear counterstain to stain\n",
      "for all the cells, our mihc aec-stained marker images (fig. 5) do not stain\n",
      "for all the cells including nuclei.\n",
      "in this use case, we show that mihc marker\n",
      "images can be translated to higher quality mif dapi and marker images which\n",
      "fig.\n",
      "6. examples of ground-truth and generated mif immune (cd3) and tumor\n",
      "(panck) markers from standard hematoxylin images.\n",
      "712\n",
      "p. ghahremani et al.\n",
      "stain eﬀectively for all the cells.\n",
      "we trained deepliif on mihc cd3 aec-\n",
      "stained images to infer mif dapi and cd3 marker.\n",
      "some examples of testing\n",
      "the trained model on cd3 images are shown in fig.\n",
      "3.3\n",
      "virtual cellular phenotyping on standard hematoxylin images\n",
      "there are several public h&e/ihc cell segmentation datasets with manual\n",
      "immune cell annotations from single pathologists.\n",
      "these are highly problem-\n",
      "atic given the large (> 50%) disagreement among pathologists on immune cell\n",
      "phenotyping [10].\n",
      "in this last use case, we infer immune and tumor markers\n",
      "from the standard hematoxylin images using again the public deepliif vir-\n",
      "tual translation module [2,3].\n",
      "sample\n",
      "images/results taken from the testing dataset are shown in fig.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf:\n",
      "transformer-based multiple instance learning (mil) frame-\n",
      "work has been proven advanced for whole slide image (wsi) analysis.\n",
      "moreover, the current mil\n",
      "cannot take advantage of a large number of unlabeled wsis for training.\n",
      "in this paper, we propose a novel self-supervised whole slide image rep-\n",
      "resentation learning framework named position-aware masked autoen-\n",
      "coder (pama), which can make full use of abundant unlabeled wsis\n",
      "to improve the discrimination of slide features.\n",
      "moreover, we propose\n",
      "a position-aware cross-attention (paca) module with a kernel reorien-\n",
      "tation (kro) strategy, which makes pama able to maintain spatial\n",
      "integrity and semantic enrichment during the training.\n",
      "the results of experiments show our pama\n",
      "is superior to sota mil methods and ssl methods.\n",
      "keywords: wsi representation learning · self-supervised learning\n",
      "1\n",
      "introduction\n",
      "in the past few years, the development of histopathological whole slide image\n",
      "(wsi) analysis methods has dramatically contributed to the intelligent cancer\n",
      "diagnosis [4,10,15].\n",
      "however, due to the limitation of hardware resources, it is\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 69.\n",
      "recent\n",
      "studies usually divide the wsi analysis into multiple stages.\n",
      "these transformer-based models achieved state-of-the-art performance in\n",
      "sub-type classiﬁcation, survival prediction, gene mutant prediction, etc.\n",
      "[5] explored and posed a new challenge\n",
      "referred to as slide-level self-learning and proposed hipt, which leveraged the\n",
      "hierarchical structure inherent in wsis and constructed multiple levels of the\n",
      "self-supervised learning framework to learn high-resolution image representa-\n",
      "tions.\n",
      "this approach enables mil-based frameworks to take advantage of abun-\n",
      "dant unlabeled wsis, further improving the accuracy and robustness of tumor\n",
      "recognition.\n",
      "the bias and error generated in each level of the representation\n",
      "model will accumulate in the ﬁnal decision model.\n",
      "moreover, the vit [6] back-\n",
      "bone used in hipt is originally designed for nature sense images in ﬁxed sizes\n",
      "whose positional information is consistent.\n",
      "but these masks are manually deﬁned which is\n",
      "not trainable and lacked orientation information.\n",
      "in this paper, we propose a novel whole slide image representation learning\n",
      "framework named position-aware masked autoencoder (pama), which achieves\n",
      "slide-level representation learning by reconstructing the local representations of\n",
      "the wsi in the patch feature space.\n",
      "(1)\n",
      "we propose a novel whole slide image representation learning framework named\n",
      "position-aware masked autoencoder (pama).\n",
      "(2) we pro-\n",
      "pose a position-aware cross-attention (paca) module with a kernel reorienta-\n",
      "tion (kro) strategy, which makes the framework able to maintain the spa-\n",
      "tial integrity and semantic enrichment of slide representation during the self-\n",
      "supervised training.\n",
      "(3) the experiments on two datasets show our pama can\n",
      "achieve competitive performance compared with sota mil methods and ssl\n",
      "methods.\n",
      "fig.\n",
      "1. the overview of the proposed whole slide image representation with position-\n",
      "aware masked autoencoder (pama), where (i) shows the data preprocessing including\n",
      "the patch embedding and anchors clustering, (ii) describes the workﬂow of wsi rep-\n",
      "resentation self-supervised learning with pama, (iii) is the structure of the position-\n",
      "aware cross-attention (paca) module which is the core of the encoder and decoder,\n",
      "and (iv) shows the kernel reorientation (kro) strategy and the detailed process is\n",
      "described in algorithm 1.\n",
      "position-aware masked autoencoder\n",
      "717\n",
      "2\n",
      "methods\n",
      "2.1\n",
      "problem formulation and data preparation\n",
      "mae\n",
      "[7] is a successful ssl framework that learns image presentations by recon-\n",
      "structing the masked image in the original pixel space.\n",
      "first, we divided wsis into non-overlapping image\n",
      "patches and meanwhile removed the background without tissue regions based on\n",
      "a threshold (as shown in fig. 1(i)).\n",
      "afterward, the\n",
      "features for a wsi are represented as x ∈ rnp×df , where df is the dimension of\n",
      "the feature and np is the number of patches in the wsi. inspired by kat\n",
      "[7], we random mask patch tokens with a high masking ratio (i.e.\n",
      "75% in our experiments).\n",
      "718\n",
      "k. wu et al.\n",
      "the message passing between the anchors and patches is achieved by a bi-\n",
      "directional cross-attention between the patches and anchors.\n",
      "= σ(k(n)w(n)\n",
      "q\n",
      "· (x(n)w(n)\n",
      "k )\n",
      "t\n",
      "√de\n",
      "+ϕd(d(n))+ϕp(p(n)))·(x(n)w(n)\n",
      "v ), (1)\n",
      "where wl ∈ rdf ×de, l = q, k, v are learnable parameters with de denoting the\n",
      "dimension of the head output, σ represents the softmax function, and ϕd and ϕp\n",
      "are the embedding functions that respectively take the distance and polar angle\n",
      "as input and output the corresponding trainable embedding values.\n",
      "the embedding of relative distance and polar angle information helps the model\n",
      "maintain the semantic and structural integrity of the wsi and meanwhile pre-\n",
      "vents the wsi representation from collapsing to the local area throughout the\n",
      "training process.\n",
      "in natural scene images, there is\n",
      "natural directional conspicuousness of semantics.\n",
      "but histopathology images have no absolute deﬁnition of\n",
      "direction.\n",
      "the semantics of wsi will not change with rotation and ﬂip.\n",
      "based on the updated polar\n",
      "axis, we can then amend p(n) to p(n+1).\n",
      "the detailed algorithm is described in\n",
      "algorithm 1.\n",
      "position-aware masked autoencoder\n",
      "719\n",
      "3\n",
      "experiments and results\n",
      "3.1\n",
      "datasets\n",
      "we evaluated the proposed method on two datasets, the public tcga-lung and\n",
      "the in-house endometrial dataset, which are introduced as follows.\n",
      "we con-\n",
      "ducted wsi multi-type classiﬁcation experiments on the two datasets.\n",
      "the results of the test set were\n",
      "reported for comparison.\n",
      "3.2\n",
      "implementation details\n",
      "the wsi representation pre-training stage uses all training data and does not\n",
      "involve any supervised information.\n",
      "2. semi-supervised experiments with 10%, 35%, 60% and 85% of labelled data\n",
      "on the endometrial dataset.\n",
      "the usage of [cls] token refers to the mae\n",
      "we imple-\n",
      "mented all the models in python 3.8 with pytorch 1.7 and cuda 10.2 and run\n",
      "the experiments on a computer with 4 gpus of nvidia geforce 2080ti.\n",
      "position-aware masked autoencoder\n",
      "721\n",
      "3.3\n",
      "eﬀectiveness of the wsi representation learning\n",
      "we ﬁrst conducted experiments on the endometrial dataset to verify the eﬀec-\n",
      "tiveness of self-supervised learning for wsi analysis under label-limited condi-\n",
      "tions.\n",
      "2, where the performance obtained with dif-\n",
      "ferent ratios of labeled training wsis are compared.\n",
      "[7] based on the patch\n",
      "features is implemented as the baseline.\n",
      "[7]\n",
      "0.965\n",
      "83.90\n",
      "0.970\n",
      "87.50\n",
      "0.801\n",
      "38.87\n",
      "0.832\n",
      "41.95\n",
      "mae+\n",
      "0.969\n",
      "85.07\n",
      "0.981\n",
      "88.25\n",
      "0.811\n",
      "37.91\n",
      "0.845\n",
      "42.85\n",
      "pama\n",
      "0.982 90.84 0.988 92.48 0.829 43.38 0.851 43.64\n",
      "overall, pama consistently achieves signiﬁcantly better performance across\n",
      "all the label ratios than mae\n",
      "[5] is a two-stage self-learning frame-\n",
      "work, which ﬁrst leverages dino\n",
      "the multi-stage framework accumulated the training bias and noise,\n",
      "which caused an auc gap of hipt\n",
      "we also observed a signiﬁcant improvement when\n",
      "comparing mae+ with mae\n",
      "please refer to the\n",
      "supplementary materials for more detailed results.\n",
      "722\n",
      "k. wu et al.\n",
      "3.4\n",
      "ablation study\n",
      "then, we conducted ablation experiments to verify the necessity of the proposed\n",
      "structural embedding strategy.\n",
      "the results are shown in table 2. overall, pama consistently achieves the best\n",
      "performance.\n",
      "[19] are state-of-\n",
      "the-art methods for histopathological image classiﬁcation.\n",
      "the experiments on two large-scale datasets\n",
      "have demonstrated the eﬀectiveness of pama in the condition of limited-label.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf:\n",
      "segmentation of pathological images is a crucial step for\n",
      "accurate cancer diagnosis.\n",
      "however, acquiring dense annotations of such\n",
      "images for training is labor-intensive and time-consuming.\n",
      "to address\n",
      "this issue, semi-supervised learning (ssl) has the potential for reducing\n",
      "the annotation cost, but it is challenged by a large number of unlabeled\n",
      "training images.\n",
      "in this paper, we propose a novel ssl method based on\n",
      "cross distillation of multiple attentions (cdma) to eﬀectively leverage\n",
      "unlabeled images.\n",
      "additionally, uncertainty minimization is applied to the average\n",
      "prediction of the three branches, which further regularizes predictions\n",
      "on unlabeled images and encourages inter-branch consistency.\n",
      "our pro-\n",
      "posed cdma was compared with eight state-of-the-art ssl methods on\n",
      "the public digestpath dataset, and the experimental results showed that\n",
      "our method outperforms the other approaches under diﬀerent annotation\n",
      "ratios.\n",
      "keywords: semi-supervised learning · knowledge distillation ·\n",
      "attention · uncertainty\n",
      "1\n",
      "introduction\n",
      "automatic segmentation of tumor lesions from pathological images plays an\n",
      "important role in accurate diagnosis and quantitative evaluation of cancers.\n",
      "recently, deep learning has achieved remarkable performance in pathological\n",
      "image segmentation when trained with a large and well-annotated dataset [6,13,\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp.\n",
      "https://doi.org/10.1007/978-3-031-43987-2_55\n",
      "semi-supervised segmentation via cross distillation of multiple attentions\n",
      "571\n",
      "20].\n",
      "however, obtaining dense annotations for pathological images is challenging\n",
      "and time-consuming, due to the extremely large image size (e.g., 10000 × 10000\n",
      "pixels), scattered spatial distribution, and complex shape of lesions.\n",
      "the\n",
      "consistency-based methods impose consistency constraints on the predictions\n",
      "of an unlabeled image under some perturbations.\n",
      "for example, mean teacher\n",
      "(mt)-based methods [14,23] encourage consistent predictions between a teacher\n",
      "and a student model with noises added to the input.\n",
      "[21] introduced\n",
      "a pairwise relation network to exploit semantic consistency between each pair\n",
      "of images in the feature space.\n",
      "[7] proposed\n",
      "to encourage the predictions of auxiliary decoders and a main decoder to be\n",
      "consistent under perturbed hierarchical features.\n",
      "pseudo label-based methods\n",
      "typically generate pseudo labels for labeled images to supervise the network [4].\n",
      "since using a model’s prediction to supervise itself may over-ﬁt its bias, chen\n",
      "et al.\n",
      "however, the pseudo\n",
      "labels are not accurate and contain a lot of noise, using argmax or sharpening\n",
      "operation will lead to over-conﬁdence of potentially wrong predictions, which lim-\n",
      "its the performance of the models.\n",
      "[17] directly\n",
      "applied entropy minimization to the segmentation results.\n",
      "in this work, we propose a novel and eﬃcient method based on cross distilla-\n",
      "tion with multiple attentions (cdma) for semi-supervised pathological image\n",
      "segmentation.\n",
      "unlike mc-net+ [19]\n",
      "that is based on diﬀerent upsampling strategies, our mtnet uses diﬀerent atten-\n",
      "tion mechanisms in three decoder branches that calibrate features in diﬀerent\n",
      "aspects to obtain diverse and complementary outputs.\n",
      "secondly, inspired by the\n",
      "observation that smoothed labels are more eﬀective for noise-robust learning\n",
      "found in recent studies [10,22], we propose a cross decoder knowledge distil-\n",
      "lation (cdkd) strategy to better leverage the diverse predictions of unlabeled\n",
      "images.\n",
      "in addition, we apply an\n",
      "uncertainty minimization-based regularization to the average probability pre-\n",
      "diction across the decoders, which not only increases the network’s conﬁdence,\n",
      "but also improves the inter-decoder consistency for leveraging labeled images.\n",
      "the contribution of this work is three-fold: 1) a novel framework named\n",
      "cdma based on mtnet is introduced for semi-supervised pathological image\n",
      "segmentation, which leverages diﬀerent attention mechanisms for generating\n",
      "diverse and complementary predictions for unlabeled images; 2) a cross decoder\n",
      "knowledge distillation method is proposed for robust and eﬃcient learning from\n",
      "noisy pseudo labels, which is combined with an average prediction-based uncer-\n",
      "tainty minimization to improve the model’s performance; 3) experimental results\n",
      "show that the proposed cdma outperforms eight state-of-the-art ssl methods\n",
      "on the public digestpath dataset [3].\n",
      "fig.\n",
      "1. our cdma for semi-supervised segmentation.\n",
      "cross decoder knowledge distillation\n",
      "(cdkd) is proposed to better deal with noisy pseudo labels, and an uncertainty min-\n",
      "imization is applied to the average probability prediction of the three branches.\n",
      "lsup\n",
      "is only for labeled images.\n",
      "1, the proposed cross distillation of multiple attentions\n",
      "(cdma) framework for semi-supervised pathological image segmentation con-\n",
      "sists of three core modules: 1) a tri-branch network mtnet that uses three\n",
      "diﬀerent attention mechanisms to obtain diverse outputs, 2) a cross decoder\n",
      "knowledge distillation (cdkd) module to reduce the eﬀect of noisy pseudo\n",
      "labels based on soft supervision, and 3) an average prediction-based uncertainty\n",
      "minimization loss to further regularize the predictions on unlabeled images.\n",
      "semi-supervised segmentation via cross distillation of multiple attentions\n",
      "573\n",
      "2.1\n",
      "multi-attention tri-branch network (mtnet)\n",
      "attention is an eﬀective network structure design in fully supervised image seg-\n",
      "mentation\n",
      "it can calibrate the feature maps for better performance by\n",
      "paying more attention to the important spatial positions or channels with only a\n",
      "few extra parameters.\n",
      "however, it has been rarely investigated in semi-supervised\n",
      "segmentation tasks.\n",
      "to more eﬀectively exploit attention mechanisms for semi-\n",
      "supervised pathological image segmentation, our proposed mtnet consists of a\n",
      "shared encoder and three decoder branches that are based on channel attention\n",
      "(ca), spatial attention (sa) and simultaneous channel and spatial attention\n",
      "(csa), respectively.\n",
      "pools\n",
      "avg and pools\n",
      "max represent average\n",
      "pooling and max-pooling across the spatial dimension, respectively.\n",
      "sa branch leverages spatial attention to highlight the most relevant spatial\n",
      "positions and suppress the irrelevant regions in a feature map.\n",
      "poolc\n",
      "avg and poolc\n",
      "max are average and\n",
      "max-pooling across the channel dimension, respectively.\n",
      "a csa block consists of a ca block followed by an sa block,\n",
      "taking advantage of channel and spatial attention simultaneously.\n",
      "for an input image, the logit predictions obtained by the three branches are\n",
      "denoted as zca, zsa and zcsa, respectively.\n",
      "after using a standard softmax\n",
      "operation, their corresponding probability prediction maps are denoted as pca,\n",
      "psa and pcsa, respectively.\n",
      "574\n",
      "l. zhong et al.\n",
      "2.2\n",
      "cross decoder knowledge distillation (cdkd)\n",
      "since the three branches have diﬀerent decision boundaries, using the predictions\n",
      "from one branch as pseudo labels to supervise the others would avoid each branch\n",
      "over-ﬁtting its bias.\n",
      "however, as the predictions for unlabeled training images\n",
      "are noisy and inaccurate, using hard or sharpened pseudo labels [2,19] would\n",
      "strengthen the conﬁdence on incorrect predictions, leading the model to overﬁt\n",
      "the noise [10,22].\n",
      "to address this problem, we introduce cdkd to enhance the\n",
      "ability of our mtnet to leverage unlabeled images and eliminate the negative\n",
      "impact of noisy pseudo labels.\n",
      "(5)\n",
      "2.3\n",
      "average prediction-based uncertainty minimization\n",
      "minimizing the uncertainty (e.g., entropy)\n",
      "[15] has been shown to be an eﬀective\n",
      "regularization for predictions on unlabeled images, which increases the model’s\n",
      "conﬁdence on its predictions.\n",
      "to avoid this problem\n",
      "and further encourage inter-decoder consistency for regularization, we propose\n",
      "an average prediction-based uncertainty minimization:\n",
      "(6)\n",
      "semi-supervised segmentation via cross distillation of multiple attentions\n",
      "575\n",
      "where ¯p = (pcsa + pca + psa)/3 is the average probability map.\n",
      "i is the average probability\n",
      "for class c at pixel i. note that when lum for a pixel is close to zero, the average\n",
      "probability for class c of that pixel is close to 0.0 (1.0), which drives all the\n",
      "decoders to predict it as 0.0 (1.0) and encourages inter-decoder consistency.\n",
      "finally, the overall loss function for our cdma is:\n",
      "l = lsup + λ1lcdkd + λ2lum\n",
      "(7)\n",
      "where lsup = (lcsa\n",
      "sup\n",
      "+ lca\n",
      "sup + lsa\n",
      "sup)/3 is the average supervised learning loss\n",
      "for the three branches on the labeled training images, and the supervised loss\n",
      "for each branch calculates the dice loss and cross entropy loss between the\n",
      "probability prediction (pcsa, pca and psa) and the ground truth label.\n",
      "note that lcdkd and lum are\n",
      "applied on both labeled and unlabeled training images.\n",
      "fig.\n",
      "2. visual comparison between our proposed cdma with state-of-the-art methods\n",
      "for semi-supervised semantic segmentation of wsis.\n",
      "3\n",
      "experiments and results\n",
      "dataset and implementation details.\n",
      "we used the public digestpath data-\n",
      "set [3] for binary segmentation of colonoscopy tumor lesions from whole slide\n",
      "images (wsi) in the experiment.\n",
      "the wsis were collected from four medi-\n",
      "cal institutions of ×20 magniﬁcation (0.475 μm/pixel) with an average size of\n",
      "5000 × 5000.\n",
      "for com-\n",
      "putational feasibility, we cropped the wsis into patches with a size of 256 × 256.\n",
      "576\n",
      "l. zhong et al.\n",
      "at inference time for segmenting a wsi, we used a sliding window of size 256×256\n",
      "with a stride of 192 × 192.\n",
      "the cdma framework was implemented in pytorch, and all experiments\n",
      "were performed on one nvidia 2080ti gpu. mtnet was implemented by\n",
      "extending deeplabv3+\n",
      "the encoder used\n",
      "a backbone of resnet50 pre-trained on imagenet.\n",
      "the kernel size of conv in\n",
      "the sa block is 7 × 7. sgd optimizer was used for training, with weight decay\n",
      "5 × 10−4, momentum 0.9 and epoch number 150.\n",
      "for data augmentation,\n",
      "we adopted random ﬂipping, random rotation, and random gaussian noise.\n",
      "for\n",
      "inference, only the csa branch was used due to the similar performance of the\n",
      "three branches after converge and the increased inference time of their ensemble,\n",
      "and no post-processing was used.\n",
      "[2]; 6) hierarchical consistency enforcement (hce)\n",
      "they were also compared with the lower bound\n",
      "of supervised learning (sl) that only learns from the labeled images.\n",
      "semi-supervised segmentation via cross distillation of multiple attentions\n",
      "577\n",
      "quantitative evaluation of these methods is shown in table 1.\n",
      "[2] showed the best performance for both of\n",
      "the two annotation ratios.\n",
      "our proposed cdma achieved a better performance\n",
      "than all the existing methods, with a dsc score of 69.72% and 72.24% when\n",
      "the annotation ratio was 5% and 10%, respectively.\n",
      "it can be observed that our cdma yields\n",
      "less mis-segmentation compared with cps\n",
      "for ablation study, we set the baseline as using the proposed\n",
      "mtnet with three diﬀerent decoders for supervised learning from labeled images\n",
      "only.\n",
      "it obtained an average dsc of 65.02% and 68.61% under the two annotation\n",
      "ratios respectively.\n",
      "table 2 shows\n",
      "that our lcdkd obtained an average dsc of 68.84% and 71.49% under the two\n",
      "annotation ratios respectively, and it outperformed lcdkd (argmax) and lcdkd\n",
      "(t=1), demonstrating that our cdkd based on softened probability prediction\n",
      "is more eﬀective in dealing with noisy pseudo labels.\n",
      "by introducing our average\n",
      "prediction-based uncertainty minimization lum, the dsc was further improved\n",
      "to 69.72% and 72.24% under the two annotation ratios respectively.\n",
      "the results in the second section of table 2 show that using the\n",
      "same structures for diﬀerent branches, i.e., mtnet(-atten) and mtnet(csa×3),\n",
      "had a lower performance than using diﬀerent attention blocks, and using three\n",
      "578\n",
      "l. zhong et al.\n",
      "attention branches outperformed just using two attention branches.\n",
      "it can also\n",
      "be found that using csa branch for inference had a very close performance to\n",
      "mtnet(ensb), and it is more eﬃcient than the later.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf:\n",
      "whole slide image (wsi) classiﬁcation remains a challenge\n",
      "due to their extremely high resolution and the absence of ﬁne-grained\n",
      "labels.\n",
      "mil methods involve a patch embedding module and a bag-level\n",
      "classiﬁcation module, but they are prohibitively expensive to be trained\n",
      "in an end-to-end manner.\n",
      "such schemes\n",
      "hinder the patch embedder’s access to slide-level semantic labels, result-\n",
      "ing in inconsistency within the entire mil pipeline.\n",
      "we tested our framework on two datasets\n",
      "using three diﬀerent backbones, and our experimental results demonstrate\n",
      "consistent performance improvements over state-of-the-art mil methods.\n",
      "keywords: multiple instance learning · whole slide image ·\n",
      "deep\n",
      "learning\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 45.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "compared to traditional microscope-\n",
      "based observation, whole slide scanning converts glass slides into gigapixel digital\n",
      "images that can be conveniently stored and analyzed.\n",
      "this approach allows for the direct application of existing image classiﬁcation\n",
      "models, but requires additional patch-level labeling.\n",
      "therefore,\n",
      "many weakly-supervised [8,24] and semi-supervised [3,5] methods have been pro-\n",
      "posed to generate patch-level pseudo labels at a lower cost.\n",
      "however, the lack\n",
      "of reliable supervision directly hinders the performance of these methods, and\n",
      "serious class-imbalance problems could arise, as tumor patches may only account\n",
      "for a small portion of the entire wsi [12].\n",
      "in contrast, mil-based methods have become increasingly preferred due to\n",
      "their only demand for slide-level labels [18].\n",
      "the aim is to predict whether there are positive instances,\n",
      "such as tumor patches, in a bag, and if so, the bag is considered positive as well.\n",
      "in practice, a ﬁxed imagenet pre-trained feature extractor g(·) is usually used to\n",
      "convert the tiled patches in a wsi into feature maps due to limited gpu mem-\n",
      "ory.\n",
      "as a result, many\n",
      "methods focus solely on improving a(·) or f(·), leaving g(·) untrained on the wsi\n",
      "dataset (as shown in fig.\n",
      "however, the domain shift between wsi and nat-\n",
      "ural images may lead to sub-optimal representations, so recently there have been\n",
      "methods proposed to ﬁne-tune g(·) using self-supervised techniques [4,12,21] or\n",
      "weakly-supervised techniques [10,13,23] (as shown in fig. 2(c)).\n",
      "(d) our pro-\n",
      "posed icmil which can bridge the loss back-propagation process from f(·) to g(·) by\n",
      "iteratively coupling them during training.\n",
      "to address the challenges mentioned above, we propose a novel mil frame-\n",
      "work called icmil, which can iteratively couple the patch feature embedding pro-\n",
      "cess with the bag-level classiﬁcation process to enhance the eﬀectiveness of mil\n",
      "training (as illustrated in fig. 2(d)).\n",
      "(3) we conduct extensive\n",
      "experiments on two datasets using three diﬀerent backbones and demonstrate\n",
      "the eﬀectiveness of our proposed framework.\n",
      "[6] pre-trained on imagenet [19] (step 1⃝ in fig. 3).\n",
      "3), of which the detailed implementation is pre-\n",
      "sented in sect.\n",
      "after this, g(·) is ﬁne-tuned for the speciﬁc wsi dataset,\n",
      "which allows it to generate improved representations for each instance, thereby\n",
      "enhancing the performance of f(·).\n",
      "moreover, with a better f(·), we can use the\n",
      "iterative coupling technique again, resulting in further performance gains and\n",
      "mitigation to the distribution inconsistencies between instance- and bag-level\n",
      "embeddings.\n",
      "otherwise, a(·) may\n",
      "lead to larger diﬀerence between the decision boundaries of bag-level classifer f(·)\n",
      "and instance-level classiﬁer f ′(·), which may cause icmil taking more time to\n",
      "converge.\n",
      "therefore, in our experiments, we choose to use the attention-based instance\n",
      "aggregation method [9] which has been widely used in many of the existing\n",
      "iteratively coupled multiple instance learning\n",
      "471\n",
      "fig.\n",
      "moreover, incorpo-\n",
      "rating augmented inputs in the training process allows for the better utilization\n",
      "of supervision signals, resulting in a more robust g(·).\n",
      "for a given patch input x, the teacher generates the\n",
      "corresponding pseudo label, while the student receives an augmented image x′\n",
      "and attempts to generate a similar prediction to that of the teacher through a\n",
      "consistency loss lc.\n",
      "the overall\n",
      "loss function for this step is lc + αlw, with α set to 0.5 in our experiments.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "datasets\n",
      "our experiments utilized two datasets, with the ﬁrst being the publicly available\n",
      "breast cancer dataset, camelyon16\n",
      "although patch-level labels are oﬃcially provided in\n",
      "camelyon16, they were not used in our experiments.\n",
      "the ground truth labels are binary classes of low risk and high\n",
      "risk, which were provided by experienced pathologists.\n",
      "3.2\n",
      "implementation details\n",
      "for camelyon16, we tiled the wsis into 256×256 patches on 20× magniﬁcation\n",
      "using the oﬃcial code of [25], while for the hcc dataset the patches are 384×384\n",
      "on 40× magniﬁcation following the pathologists’ advice.\n",
      "for both datasets, we\n",
      "used an imagenet pre-trained resnet50 to initialize g(·).\n",
      "[25]\n",
      "✓\n",
      "93.2\n",
      "84.9\n",
      "89.0\n",
      "83.0\n",
      "85.5\n",
      "78.1\n",
      "ours\n",
      "(w/ max pooling)\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "85.2\n",
      "(+5.7)\n",
      "74.7\n",
      "(+4.1)\n",
      "81.9\n",
      "(+1.6)\n",
      "86.6\n",
      "(+6.5)\n",
      "87.3\n",
      "(+3.0)\n",
      "82.0\n",
      "(+5.2)\n",
      "ours\n",
      "(w/ ab−mil)\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "90.0\n",
      "(+4.6)\n",
      "80.5\n",
      "(+2.5)\n",
      "86.6\n",
      "(+2.1)\n",
      "87.1\n",
      "(+5.9)\n",
      "88.3\n",
      "(+2.3)\n",
      "83.3\n",
      "(+5.2)\n",
      "ours\n",
      "(w/ dtfd−mil)\n",
      "✓\n",
      "✓\n",
      "✓\n",
      "93.7\n",
      "(+0. 5)\n",
      "87.0\n",
      "(+2. 1)\n",
      "90.6\n",
      "(+1. 6)\n",
      "87.7\n",
      "(+4. 7)\n",
      "89.1\n",
      "(+3. 6)\n",
      "83.5\n",
      "(+5. 4)\n",
      "ﬁrstly embedded into a 1024-dimension vector, and then be projected to a 512-\n",
      "dimension hidden space for further bag-level training.\n",
      "experiments were all conducted on a nvidia tesla\n",
      "m40 (12gb).\n",
      "3.3\n",
      "experimental results\n",
      "ablation study.\n",
      "from\n",
      "table 1(a), we can learn that as the number of icmil iteration increases, the\n",
      "performance will also go up until reaching a stable point.\n",
      "since the number of\n",
      "instances is very large in wsi datasets, we empirically recommend to choose\n",
      "to run icmil one iteration for ﬁne-tuning g(·) to achieve the balance between\n",
      "performance gain and time consumption.\n",
      "experimental results are presented in\n",
      "table 2.\n",
      "as shown, our icmil framework consistently improves the performance\n",
      "of three diﬀerent mil baselines (i.e., max pool, ab-mil, and dtfd-mil),\n",
      "demonstrating the eﬀectiveness of bridging the loss back-propagation from bag\n",
      "calssiﬁer to embedder.\n",
      "when used with the\n",
      "state-of-the-art mil method dtfd-mil, icmil further increases its perfor-\n",
      "mance on camelyon16 by 0.5% auc, 2.1% f1, and 1.6% acc.\n",
      "results on the hcc dataset also proves the eﬀectiveness of icmil, despite\n",
      "the minor diﬀerence on the relative performance of baseline methods.\n",
      "mean pool-\n",
      "ing performs better on this dataset due to the large area of tumor in the wsis\n",
      "(about 60% patches are tumor patches), which mitigates the impact of average\n",
      "pooling on instances.\n",
      "also, the performance diﬀerences among diﬀerent vanilla\n",
      "mil methods tends to be smaller on this dataset since risk grading is a harder\n",
      "task than camelyon16.\n",
      "as a\n",
      "result, after applying icmil on the mil baselines, these methods all gain great\n",
      "performance boost on the hcc dataset.\n",
      "4\n",
      "conclusion\n",
      "in this work, we propose icmil, a novel framework that iteratively couples\n",
      "the feature extraction and bag classiﬁcation stages to improve the accuracy of\n",
      "mil models.\n",
      "icmil leverages the category knowledge in the bag classiﬁer as\n",
      "pseudo supervision for embedder ﬁne-tuning, bridging the loss propagation from\n",
      "classiﬁer to embedder.\n",
      "the\n",
      "experimental results show that our method brings consistent improvement to\n",
      "existing mil backbones.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_1.pdf:\n",
      "the proposed ultradet demonstrates signiﬁcant improvement\n",
      "over previous state-of-the-arts and achieves real-time inference speed.\n",
      "while previous works focused on lesion detection in still images [25]\n",
      "and oﬄine videos [9,11,22], this paper explores real-time ultrasound video lesion\n",
      "detection.\n",
      "real-time lesion prompts can assist radiologists during scanning, thus\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2_1.\n",
      "(b) the ntca\n",
      "module leverages temporal contexts to suppress the fp.\n",
      "4.4, the inability to utilize ntc is\n",
      "a key issue leading to the fps reported by general-purpose detectors.\n",
      "to address this issue, we propose a novel ultradet model to leverage ntc.\n",
      "the ntca module\n",
      "leverages roi-level ntc which are crucial for radiologists but ignored in previous\n",
      "works, thereby eﬀectively improving the detection performance in a reliable and\n",
      "interpretable way.\n",
      "experiments on cva-bus dataset [9] demonstrate that ultra-\n",
      "det, with real-time inference speed, signiﬁcantly outperforms previous works,\n",
      "reducing about 50% fps at a recall rate of 0.90.\n",
      "(2) we propose a novel ultradet model, incorpo-\n",
      "mining negative temporal contexts to suppress fps\n",
      "5\n",
      "rating an ntca module that eﬀectively leverages ntc for fp suppression.\n",
      "(3)\n",
      "we conduct extensive experiments to demonstrate the proposed ultradet sig-\n",
      "niﬁcantly outperforms the previous state-of-the-arts.\n",
      "one-stage\n",
      "detectors\n",
      "previous works have explored lesion detection in still images [25] and oﬄine\n",
      "videos\n",
      "thus\n",
      "their performances are far from satisfactory.\n",
      "optical flow [3] is used to guide ultrasound segmentation [12], motion estima-\n",
      "tion\n",
      "the rpn generates proposals consisting of boxes bτ and\n",
      "proposal features qτ using roi align and average pooling:\n",
      "qτ = avgpool (roialign(fτ, bτ))\n",
      "we conduct iof align and average pooling to extract ct,τ:\n",
      "ct,τ = avgpool (iofalign(fτ, bt, ot→τ))\n",
      "4\n",
      "experiments\n",
      "4.1\n",
      "dateset\n",
      "cva-bus dateset.\n",
      "visual comparisons\n",
      "of two versions of labels are available in supplementary materials.\n",
      "quantitative results of real-time lesion detection on cva-bus [9].\n",
      "model\n",
      "type\n",
      "pr80\n",
      "pr90\n",
      "fp80\n",
      "fp90\n",
      "ap50\n",
      "r@16 fps\n",
      "one-stage detectors\n",
      "yolox\n",
      "[5]\n",
      "image 69.73.7 43.47.7 23.84.8 87.624.5 80.41.6 97.50.5\n",
      "[8]\n",
      "image 75.72.5\n",
      "[16]\n",
      "image 87.22.2 72.25.1 11.02.4 23.03.7 89.51.4 98.80.3 56.1\n",
      "defcn\n",
      "[21]\n",
      "image 81.51.8 67.52.3 21.13.2 33.44.3 86.41.3 99.30.3 51.2\n",
      "track-yolo\n",
      "[27]\n",
      "image 90.13.2 72.710.6 5.62.2 37.820.9\n",
      "[14]\n",
      "image 91.30.9\n",
      "[7]\n",
      "image 91.41.3 79.22.9 6.22.0 24.45.6 87.61.7 92.40.9 42.7\n",
      "fgfa\n",
      "to evaluate the highest achievable sensitivity, we report the frame-level\n",
      "average recall rates of top-16 proposals, denoted as r@16.\n",
      "mining negative temporal contexts to suppress fps\n",
      "9\n",
      "4.3\n",
      "implementation details\n",
      "ultradet settings.\n",
      "other hyper-parameters are listed in supplementary materials.\n",
      "we set the feature dimensions of\n",
      "detection heads to 256 and baselines are re-implemented to utilize only previous\n",
      "frames.\n",
      "we compare performances of real-time detectors with\n",
      "the ultradet in table 1.\n",
      "especially, the pr90 of ultradet achieves 90.8%, representing a 5.4% absolute\n",
      "improvement over the best competitor, ptseformer [20].\n",
      "the determination of whether fps can be inhibited\n",
      "10\n",
      "h. yu et al.\n",
      "by ntc is based on manual judgments of experienced radiologists.\n",
      "the ultradet achieves an infer-\n",
      "ence speed of 30.4 fps and already meets the 30 fps requirement.\n",
      "the basicdet reports fps at t = 30 and 40 as it fails to leverage ntc\n",
      "speciﬁcally, we replace the iof align\n",
      "with an roi align and the temporal aggregation with a simple average pool-\n",
      "ing in the temporal dimension.\n",
      "the results demonstrate that both iof align\n",
      "and temporal aggregation are crucial, as removing either of them leads to a\n",
      "noticeable drop in performance.\n",
      "level aggregation provides no performance gains.\n",
      "the ntca\n",
      "module leverages negative temporal contexts that are essential for fp suppres-\n",
      "sion but ignored in previous works, thereby being more eﬀective in suppressing\n",
      "fps.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_50.pdf:\n",
      "histological whole slide images (wsis) can be usually com-\n",
      "promised by artifacts, such as tissue folding and bubbles, which will\n",
      "increase the examination diﬃculty for both pathologists and computer-\n",
      "aided diagnosis (cad) systems.\n",
      "existing approaches to restoring arti-\n",
      "fact images are conﬁned to generative adversarial networks (gans),\n",
      "where the restoration process is formulated as an image-to-image trans-\n",
      "fer.\n",
      "those methods are prone to suﬀer from mode collapse and unex-\n",
      "pected mistransfer in the stain style, leading to unsatisﬁed and unre-\n",
      "alistic restored images.\n",
      "speciﬁcally, artifusion formulates the arti-\n",
      "fact region restoration as a gradual denoising process, and its training\n",
      "relies solely on artifact-free images to simplify the training complexity.\n",
      "however, the complex scanning procedure for histological whole-\n",
      "slide images (wsis) digitization may result in the alteration of tissue structures,\n",
      "due to improper removal, ﬁxation, tissue processing, embedding, and storage [11].\n",
      "https://doi.org/10.1007/978-3-031-43987-2_50\n",
      "artifact restoration in histology images with diﬀusion probabilistic models\n",
      "519\n",
      "for pathologists but also increases the risk of misdiagnosis for computer-aided\n",
      "diagnosis (cad) systems\n",
      "(a) cyclegan [19] formulates\n",
      "the artifact restoration as an image-to-image transfer problem.\n",
      "it leverages two pairs of\n",
      "the generator and discriminator to learn the transfer between the artifact and artifact-\n",
      "free image domains.\n",
      "for example, cyclegan [19] formulates the\n",
      "artifact restoration as an image-to-image transfer problem by learning the trans-\n",
      "fer between the artifact and artifact-free image domains from unpaired images,\n",
      "as depicted in fig.\n",
      "further-\n",
      "more, our approach is trained solely with artifact-free images, which reduces the\n",
      "diﬃculty in data collection.\n",
      "this approach diﬀers from gan-based methods that require either paired or\n",
      "unpaired artifacts and artifact-free images, as our artifusion relies solely on\n",
      "artifact-free images, resulting in a simpliﬁed training process.\n",
      "extensive evaluations on real-world histology datasets and down-\n",
      "stream tasks demonstrate the superiority of our framework in artifact removal\n",
      "performance, which can generate reliable restored images while preserving the\n",
      "stain style.\n",
      "fig.\n",
      "the semantic illustration of inference stage in artifusion for local regional\n",
      "artifact restoration.\n",
      "the proposed histology artifact restoration diﬀusion model\n",
      "artifusion, comprises two stages, namely the training, and inference.\n",
      "during\n",
      "the training stage, artifusion learns to generate regional histology tissue struc-\n",
      "tures based on the contextual information from artifact-free images.\n",
      "in the infer-\n",
      "ence stage, artifusion formulates the artifact restoration as a gradual denoising\n",
      "process.\n",
      "speciﬁcally, it ﬁrst replaces the artifact regions with gaussian noise, and\n",
      "then gradually restores them to artifact-free images using the contextual infor-\n",
      "mation from nearby regions.\n",
      "diﬀusion training stage.\n",
      "the proposed artifusion learns the capability\n",
      "of generating local tissue representation from contextual information during\n",
      "the training stage.\n",
      "[5],\n",
      "which involve a forward process that gradually injects gaussian noise into an\n",
      "artifact restoration in histology images with diﬀusion probabilistic models\n",
      "521\n",
      "artifact-free image and a reverse process that aims to reconstruct images from\n",
      "noise.\n",
      "522\n",
      "z. he et al.\n",
      "artifact restoration in inference stage.\n",
      "during the inference stage, we\n",
      "ﬁrst use a threshold method to detect the artifact region in the input image\n",
      "x0.\n",
      "then, unlike the conventional diﬀusion models [5] that aim to generate the\n",
      "entire image, artifusion selectively performs denoising resampling only in the\n",
      "artifact region to maximally preserve the original morphology and stain style in\n",
      "the artifact-free region, as shown in fig.\n",
      "speciﬁcally, we represent the artifact-\n",
      "free region and the artifact region in the input image as x0⊙(1−m) and x0⊙m,\n",
      "respectively [10], where m is a boolean mask indicating the artifact region and ⊙\n",
      "is the pixel-wise multiplication operator.\n",
      "to perform the denoising resampling,\n",
      "we write the input image xin\n",
      "t\n",
      "at each reverse step from t to t − 1 as the sum of\n",
      "the diﬀused artifact-free region and the denoised artifact region, i.e.,\n",
      "xin\n",
      "t\n",
      "= xsample\n",
      "t\n",
      "⊙ (1 − m)\n",
      "consequently, the ﬁnal restored image is obtained as\n",
      "x0 ⊙ (1 − m)\n",
      "the resulting tokens are then processed by the attention layers,\n",
      "and the auxiliary time token is discarded to retain the original feature dimension\n",
      "to ﬁt the swin-transformer block design after the attention layers.\n",
      "3\n",
      "experiments\n",
      "dataset.\n",
      "to evaluate the performance of artifact restoration, a training set is\n",
      "curated from a subset of camelyon17 [8]1.\n",
      "it comprises a total number of 2445\n",
      "artifact-free images and another 2547 images with artifacts, where all histological\n",
      "images are scaled to the resolution of 256 × 256 pixels at the magnitude of 20×.\n",
      "the test set uses another public histology image dataset [6] with 462 artifact-free\n",
      "1 available at https://camelyon17.grand-challenge.org.\n",
      "artifact restoration in histology images with diﬀusion probabilistic models\n",
      "523\n",
      "images2, where we obtain the paired artifact images by the manually-synthesized\n",
      "artifacts [18].\n",
      "fig.\n",
      "4. artifact restoration on ﬁve real-world artifact images.\n",
      "it highlights the ability of artifusion to\n",
      "progressively remove artifacts from the histology image, resulting in a ﬁnal restored\n",
      "image that is both visually pleasing and scientiﬁcally accurate.\n",
      "implementations.\n",
      "we implement the proposed artifusion and its counter-\n",
      "part in python 3.8.10 and pytorch 1.10.0.\n",
      "all experiments are carried out in\n",
      "parallel on two nvidia rtx a4000 gpu cards with 16 gib memory.\n",
      "consequently, we leverage the prevalent cycle-\n",
      "gan [19] as the baseline for comparison, because of its excellent performance\n",
      "2 available at https://github.com/lu-yizhou/clusterseg.\n",
      "524\n",
      "z. he et al.\n",
      "in the image transfer, and also its nature that requires no paired data can ﬁt\n",
      "our circumstance.\n",
      "unlike cyclegan which requires both artifact-free images\n",
      "and artifact images, artifusion only relies on artifact-free images, leading to\n",
      "a size of the training set that is half that of cyclegan.\n",
      "we use the following metrics: l2\n",
      "distance (l2) with respect to the artifact region, the mean-squared error (mse)\n",
      "over the whole image, structural similarity index (ssim)\n",
      "quantitative comparison of artifusion with cyclegan on artifact restora-\n",
      "tion performance.\n",
      "the ↓ indicates the smaller value, the better performance; and vice\n",
      "versa.\n",
      "methods\n",
      "l2 (×104) ↓\n",
      "mse ↓\n",
      "ssim ↑\n",
      "psnr ↑\n",
      "fsim ↑\n",
      "sre ↑\n",
      "cyclegan (#1)\n",
      "[19]\n",
      "1.119\n",
      "0.5583\n",
      "0.9656\n",
      "42.37\n",
      "0.7188\n",
      "51.42\n",
      "cyclegan (#2) [19]\n",
      "1.893\n",
      "0.5936\n",
      "0.9622\n",
      "42.12\n",
      "0.7162\n",
      "50.21\n",
      "artifusion (u-net)\n",
      "0.5027\n",
      "0.2508\n",
      "0.9850\n",
      "47.61\n",
      "0.8173\n",
      "54.59\n",
      "artifusion (add)\n",
      "0.5007\n",
      "0.2499\n",
      "0.9850\n",
      "47.79\n",
      "0.8184\n",
      "54.76\n",
      "artifusion (full settings)\n",
      "0.4940\n",
      "0.2465\n",
      "0.9860\n",
      "48.08\n",
      "0.8216\n",
      "55.43\n",
      "table 2. comparison of the model complexity and eﬃciency in terms of the number\n",
      "of parameters, flops, and averaged inference time.\n",
      "the quantitative comparison with\n",
      "cyclegan and artifusion are shown in table 1, where some exemplary images\n",
      "are illustrated in fig.\n",
      "for instance, artifusion can reduce the l2 and mse by\n",
      "artifact restoration in histology images with diﬀusion probabilistic models\n",
      "525\n",
      "more than 50%, namely from 1 × 104 to 0.5 × 104 and from 0.55 to 0.25 respec-\n",
      "tively.\n",
      "finally, the concate-\n",
      "nating time token with feature tokens can bring an improvement in terms of all\n",
      "evaluation matrices, making it a better ﬁt for the transformer architecture than\n",
      "the direct summation scheme in u-net [5].\n",
      "in table 2, we compare the model com-\n",
      "plexity in terms of the number of parameters, floating point operations per sec-\n",
      "ond (flops), and averaged inference time on one image.\n",
      "however, a smaller model\n",
      "size can facilitate easier deployment in real clinical practice.\n",
      "evaluations by downstream classiﬁcation task.\n",
      "we consider the performance on the original\n",
      "unprocessed data, denoted as ‘clean’, as the upper bound.\n",
      "then, we manually\n",
      "synthesize the artifact (denoted as ‘artifact’) and evaluate the classiﬁcation per-\n",
      "formance with restoration approaches cyclegan and artifusion.\n",
      "in table 3,\n",
      "comparisons show that the presence of artifacts can result in a signiﬁcant per-\n",
      "formance decline of over 5% across all ﬁve network architectures.\n",
      "importantly,\n",
      "the classiﬁcation accuracy on images restored with artifusion is consistently\n",
      "526\n",
      "z.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf:\n",
      "breast cancer (bc) is one of the most common cancers iden-\n",
      "tiﬁed globally among women, which has become the leading cause of\n",
      "death.\n",
      "multi-modal pathological images contain diﬀerent information for\n",
      "bc diagnosis.\n",
      "hematoxylin and eosin (h&e) staining images could reveal\n",
      "a considerable amount of microscopic anatomy.\n",
      "immunohistochemical\n",
      "(ihc) staining images provide the evaluation of the expression of various\n",
      "biomarkers, such as the human epidermal growth factor receptor (her2)\n",
      "hybridization.\n",
      "in this paper, we propose a multi-modal pre-training model\n",
      "via pathological images for bc diagnosis.\n",
      "the experiments on two datasets (herohe chal-\n",
      "lenge and bci challenge) show state-of-the-art results.\n",
      "keywords: breast cancer · hematoxylin and eosin staining ·\n",
      "immunohistochemical staining · multi-modal pre-training\n",
      "1\n",
      "introduction\n",
      "breast cancer (bc) is one of the most common malignant tumors in women\n",
      "worldwide and it causes nearly 0.7 million deaths in 2020\n",
      "the patholog-\n",
      "ical process is usually the golden standard approach for bc diagnosis, which\n",
      "relies on leveraging diverse complementary information from multi-modal data.\n",
      "in addition to obtaining the histological characteristics of tumors from hema-\n",
      "toxylin and eosin (h&e) staining images, immunohistochemical (ihc) staining\n",
      "images are also widely used for pathological diagnoses, such as the human epi-\n",
      "dermal growth factor receptor 2 (her2), the estrogen receptor (er), and the\n",
      "progesterone receptor (pr) [22].\n",
      "with the development of deep learning, there\n",
      "are a lot of multi-modal fusion methods for cancer diagnosis [6,7,20,21].\n",
      "recently, with the development of transformer, multi-modal pre-training has\n",
      "achieved great success in the ﬁelds of computer vision (cv) and natural language\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "1. one is based on isomorphic data,\n",
      "such as vision-language pre-training [5] and vision-speech-text pre-training [3].\n",
      "bachmann et al.\n",
      "[2] proposed multi-\n",
      "mae to pre-train models with intensity images, depth images, and segmentation\n",
      "maps.\n",
      "in the ﬁeld of medical image analysis, it is widely recognized that using\n",
      "multi-modal data can produce more accurate diagnoses than using single-modal\n",
      "data.\n",
      "however, the development of multi-modal pre-training methods has been\n",
      "limited due to the scarcity of paired multi-modal data.\n",
      "most methods focus on\n",
      "chest x-ray vision-language pre-training [8,11].\n",
      "we choose paired h&e and ihc (only her2) staining images, which\n",
      "are cropped into non-overlapped patches as the input of our model.\n",
      "finally,\n",
      "we use modal-speciﬁc decoders to reconstruct the original h&e and ihc staining\n",
      "images respectively.\n",
      "\u0002 we evaluate the proposed method on two public datasets as herohe chal-\n",
      "lenge and bci challenge, which shows that our method achieves state-of-the-\n",
      "art performance.\n",
      "a pair of images x and y (h&e and\n",
      "ihc) are cropped into n non-overlapped patches, which are randomly masked by ratio\n",
      "λ1 and λ2.\n",
      "2.\n",
      "a pair of h&e and her2 images are cropped into regular non-overlapping\n",
      "patches.\n",
      "then we use the mixed attention module to extract intra-modal and inter-modal\n",
      "complementary information.\n",
      "finally, the modal-speciﬁc tokens are fed into the\n",
      "modal-speciﬁc decoders to reconstruct the original h&e and her2 images.\n",
      "the\n",
      "pre-trained modal-fusion encoder could be used for downstream tasks (e.g., her2\n",
      "status prediction and her2 image generation based on h&e images).\n",
      "an image is cropped into several non-\n",
      "overlapping patches, and these patches are mapped to d dimension tokens with\n",
      "the linear projection and added position embeddings to retain positional informa-\n",
      "tion.\n",
      "we\n",
      "replace the mhsa with mhca in the inter-modal attention to learn complementary\n",
      "information.\n",
      "transformer processing ﬂow.\n",
      "input: a set of patches from one image x = {xi}n\n",
      "i=1, x ∈ rn×(r×r×c)\n",
      "1: transfer patches into linear embeddings\n",
      "2: for i = 1 to n do\n",
      "3:\n",
      "fi ← lp(xi), where f = {fi}n\n",
      "i=1, f ∈ rn×d\n",
      "4: end for\n",
      "5: position encoding concatenation\n",
      "6: f0 ← concat (fp, f), where fp ∈ r1×d\n",
      "7: for l = 1 to l do\n",
      "8:\n",
      "f\n",
      "′\n",
      "l ← mhsa(ln(fl−1))\n",
      "in the her2 staining image generation\n",
      "task, we remain the structure of gan and replace the generator with our pre-trained\n",
      "model.\n",
      "in the her2 status prediction task, we replace the feature extractor with our\n",
      "pre-trained model to obtain representations with her2 semantics.\n",
      "we use mhca to leverage diverse complementary information\n",
      "between two modalities.\n",
      "ax(f)\n",
      "diﬀerent from the transformer encoder, the target of the\n",
      "transformer decoder is used to reconstruct the original image.\n",
      "given a pair of h&e image x and her2 image y ,\n",
      "which is cut into 16 × 16 non-overlapping patches {xi}n\n",
      "i=1 and {yi}n\n",
      "i=1.\n",
      "after the process\n",
      "of the mixed attention module, h&e and her2 patch tokens are fed into the\n",
      "modal-speciﬁc decoders respectively to reconstruct the original h&e image x′\n",
      "and her2 image y ′. the reconstruction loss is computed by the mean squared\n",
      "error between the original images x, y and the generative images x′, y ′, which\n",
      "is computed as\n",
      "lh&e = 1\n",
      "t1\n",
      "t1\n",
      "\u0002\n",
      "i=1\n",
      "| pi − p′\n",
      "i |2, lher2 = 1\n",
      "t2\n",
      "t2\n",
      "\u0002\n",
      "i=1\n",
      "| qi − q′\n",
      "i |2 .\n",
      "4.\n",
      "we choose two relevant tasks: her2 image generation based on h&e images and\n",
      "her2 status prediction.\n",
      "we use pairs of h&e and ihc images for gan training.\n",
      "3\n",
      "experimental results\n",
      "3.1\n",
      "datasets\n",
      "acrobat challenge.\n",
      "in the ﬁnal phase of nonrigid\n",
      "registration, we use the optimized transformation to get the initial displacement\n",
      "ﬁeld, which is optimized across diﬀerent levels of wsis by gradient update.\n",
      "we resize the displacement ﬁeld and apply it to the original moving wsi.\n",
      "after\n",
      "all the wsi pairs are well registered, we convert the padded h&e image to\n",
      "grayscale and apply median blur to it.\n",
      "next, the otsu threshold is applied to\n",
      "extract the foreground area, which is cropped into non-overlapping 256 × 256\n",
      "images.\n",
      "finally, all the chosen images (around 0.35 million) from wsi in the\n",
      "same pair are saved for mmp-mae pre-training.\n",
      "breast cancer immunohistochemical image generation chal-\n",
      "lenge [16] consists of 3896 pairs of images for training and 977 pairs for testing,\n",
      "which are used to generate her2 images based on h&e images.\n",
      "performance comparison on bci challenge.\n",
      "3.2\n",
      "experimental setup\n",
      "experiments are implemented in pytorch\n",
      "in the her2 staining image generation task, we use 2 gpus with a batch size\n",
      "of 4.\n",
      "peak signal to noise ratio (psnr) and\n",
      "structural similarity (ssim) are used as the evaluation indicators for the quality\n",
      "of the her2 generated images.\n",
      "3.3\n",
      "method comparison\n",
      "her2 staining image generation.\n",
      "three methods on bci datasets are\n",
      "compared in our experiments, as shown in table 1.\n",
      "cyclegan is a representa-\n",
      "tive unsupervised method, which doesn’t need paired images for training.\n",
      "so\n",
      "table 2. performance comparison on herohe challenge.\n",
      "method/team auc precision recall f1-score\n",
      "macaroon\n",
      "0.71\n",
      "0.57\n",
      "0.83\n",
      "0.68\n",
      "mitel\n",
      "0.74\n",
      "0.58\n",
      "0.78\n",
      "0.67\n",
      "piaz\n",
      "0.84\n",
      "0.77\n",
      "0.55\n",
      "0.64\n",
      "dratur\n",
      "0.75\n",
      "0.57\n",
      "0.70\n",
      "0.63\n",
      "irisai\n",
      "0.67\n",
      "0.58\n",
      "0.67\n",
      "0.62\n",
      "proposed\n",
      "0.84\n",
      "0.72\n",
      "0.82\n",
      "0.74\n",
      "464\n",
      "m. lu et al.\n",
      "fig.\n",
      "the\n",
      "region in the red box shows our mmp-mae could learn the semantic information from\n",
      "the adjacent area.\n",
      "our mmp-mae further improves\n",
      "the performance, which achieves higher psnr by 1.60, and ssim by 0.007.\n",
      "team piaz and dratur both use a multi-network ensem-\n",
      "ble strategy to improve their performances.\n",
      "team irisai ﬁrst segment the tumor\n",
      "area and then predict the her2 status.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf:\n",
      "accurate segmentation and analysis of membranes from immuno-\n",
      "histochemical (ihc) images are crucial for cancer diagnosis and prognosis.\n",
      "although several fully-supervised deep learning methods for membrane seg-\n",
      "mentation from ihc images have been proposed recently, the high demand for\n",
      "pixel-level annotations makes this process time-consuming and labor-intensive.\n",
      "to overcome this issue, we propose a novel deep framework for membrane seg-\n",
      "mentation that utilizes nuclei point-level supervision.\n",
      "our framework consists of\n",
      "two networks: a seg-net that generates segmentation results for membranes and\n",
      "nuclei, and a tran-net that transforms the segmentation into semantic points.\n",
      "in\n",
      "this way, the accuracy of the semantic points is closely related to the segmenta-\n",
      "tion quality.\n",
      "thus, the inconsistency between the semantic points and the point\n",
      "annotations can be used as effective supervision for cell segmentation.\n",
      "keywords: membrane segmentation · point-based supervision ·\n",
      "immunohistochemical image\n",
      "1\n",
      "introduction\n",
      "accurate quantiﬁcation of immunohistochemistry (ihc) membrane staining images is\n",
      "a crucial aspect of disease assessment [14,15].\n",
      "in clinical diagnosis, pathologists typ-\n",
      "ically grade diseases by manually estimating the proportion of stained membrane area\n",
      "however, this manual\n",
      "l. cui, j. feng, w. yang and l. yang–equally contribution.\n",
      "supplementary information the online version contains supplementary material available at\n",
      "https://doi.org/10.1007/978-3-031-43987-2_52.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "1. illustration of the full supervision (blue line) and point supervision (ours, red line) for\n",
      "membranes and nuclei segmentation.\n",
      "(color ﬁgure online)\n",
      "despite numerous deep learning methods have been proposed for detecting cell\n",
      "nuclei [11,20] from hematoxylin-eosin (h&e) staining images, little attention has\n",
      "focused on analyzing cell membranes from ihc images.\n",
      "currently, only a few fully\n",
      "supervised ihc membrane segmentation methods have been proposed [9,19], demon-\n",
      "strating the superiority of deep learning-based membrane segmentation.\n",
      "in contrast, as annotating the centers of nuclei requires much fewer\n",
      "efforts, weakly supervised learning has been studied for nuclei segmentation\n",
      "nevertheless, how to utilize point annotations to supervise cell membrane segmentation\n",
      "is still under investigation.\n",
      "this study proposes a novel point-based cell membrane segmentation method,\n",
      "which can signiﬁcantly reduce the cost of pixel-level annotation required in conven-\n",
      "tional methods, as shown in fig.\n",
      "we employ a network named seg-net to segment\n",
      "the nuclei and membranes separately, followed by a trans-net to convert the segmen-\n",
      "tation results into semantic points.\n",
      "since the accuracy of semantic points is directly\n",
      "related to the segmentation results, the segmentation quality can be implicitly super-\n",
      "vised by the loss between the semantic points and the point annotations, as shown in\n",
      "fig.\n",
      "2\n",
      "to the best of our knowledge, this is the ﬁrst study that using point-level supervision\n",
      "for membrane segmentation from ihc images, which could signiﬁcantly advance future\n",
      "segment membranes and nuclei via point-level supervision\n",
      "541\n",
      "fig.\n",
      "the illustration of how point annotations are used to supervise the cell segmentation.\n",
      "additionally, our method is the ﬁrst to employ point annotations\n",
      "to simultaneously supervise the segmentation of two objects.\n",
      "extensive experiments\n",
      "conﬁrm the efﬁcacy of the proposed method, attaining performance that is comparable\n",
      "to models trained with fully supervised data.\n",
      "2\n",
      "related works\n",
      "deep learning-based segmentation methods have been widely developed for cell nuclei\n",
      "segmentation from h&e images in recent years, ranging from convolutional neural\n",
      "networks [5,12,24] to transformer-based architectures [8], resulting in continuously\n",
      "improved accuracy in nuclei segmentation.\n",
      "for the task of analyzing ihc membrane-stained images, due to the challenge of\n",
      "pixel-level annotation, existing methods mostly adopt traditional unsupervised algo-\n",
      "rithms, such as watershed [17,26], active contour [3,25], and color deconvolution\n",
      "in recent years, a few\n",
      "fully supervised cell membrane segmentation methods also have emerged [9,19], but\n",
      "the high cost of data annotation limits their applicability to various membrane staining\n",
      "image analysis tasks.\n",
      "to reduce the annotation cost of nuclei segmentation in histopathological images,\n",
      "weakly supervised segmentation training methods have received attention, including:\n",
      "1) unsupervised cell nuclei segmentation methods represented by adversarial-based\n",
      "methods [6,7].\n",
      "however, unsupervised methods are challenged by the difﬁculty of\n",
      "constraining the search space of model parameters, making it hard for the model to\n",
      "handle visually complex pathology images, such as h&e or ihc; 2) weakly super-\n",
      "vised cell nucleus segmentation algorithms with point annotation [16,21].\n",
      "because the\n",
      "cell nucleus shape in h&e images is almost elliptical, point annotation combined with\n",
      "542\n",
      "h. li et al.\n",
      "voronoi diagram [1] were used to generate pseudo-annotations for iterative model train-\n",
      "ing and reﬁnement.\n",
      "although these methods can perform weakly supervised segmenta-\n",
      "tion of cell nuclei from ihc membrane-stained images, they are usually ineffective in\n",
      "segmenting messy cell membranes.\n",
      "therefore, this paper proposes a novel point-supervised cell membrane segmenta-\n",
      "tion method, addressing a major challenge in the ﬁeld.\n",
      "the paper also explores the\n",
      "feasibility of point supervision for the segmentation of two types of objects (cell nuclei\n",
      "and cell membranes) for the ﬁrst time.\n",
      "3\n",
      "method\n",
      "this study aims to explore how to perform membrane and nucleus segmentation in ihc\n",
      "membrane-stained images using only point-level supervision.\n",
      "nuclei segmentation is\n",
      "performed for cell localization and counting, while membrane quantiﬁcation provides\n",
      "clinical evidence for diagnosis.\n",
      "fig.\n",
      "the training stage employs two networks,\n",
      "namely a segmentation network (seg-net) and a transition network (tran-net).\n",
      "during infer-\n",
      "ence, we only adopt the seg-net for segmentation.\n",
      "3.1\n",
      "formulation of the point-level supervised segmentation problem\n",
      "given an input cell image set {ii}n\n",
      "i=1, where n is the number of images in this\n",
      "set, ii ∈ rh×w ×3 with h, w representating the height and width of the image,\n",
      "respectively, and 3 being the number of channels of the image.\n",
      "our goal is to obtain\n",
      "the mask of membranes (\u0002\n",
      "mi ∈ rh×w ×1) and nuclei (\u0003si ∈ rh×w ×1), that is\n",
      "\u0004\n",
      "\u0003mi, \u0003si\n",
      "\u0005\n",
      "= σ(fθ (ii)), where fθ is a segmentation network (seg-net) and with train-\n",
      "able parameters θ, and σ is the sigmoid activation function.\n",
      "we have point annotations\n",
      "pi ∈ rh×w ×(c+1) for image ii, in which c is the number of semantic categories used\n",
      "to describe the states of membrane staining.\n",
      "in order to train fθ to segment membranes \u0002\n",
      "mi and nuclei \u0003si using point annotations\n",
      "pi in image ii, we need to establish the relationship from input to segmentation, and\n",
      "segment membranes and nuclei via point-level supervision\n",
      "543\n",
      "then to point annotation, as shown in eq.\n",
      "gω transforms\n",
      "\u0004\n",
      "\u0002\n",
      "mi, \u0003si\n",
      "\u0005\n",
      "to semantic points \u0003pi ∈ rh×w ×(c+1), it should be noted that \u0002\n",
      "mi and \u0003si\n",
      "respectively provide the semantic and spatial information to gω for semantic points\n",
      "prediction, so that the segmentation performance is crucial for gω.\n",
      "so that, by ﬁtting \u0003pi\n",
      "to pi ( \u0003pi ∼= pi), the segmentation can be supervised.\n",
      "this is because\n",
      "gω is utilized to predict the category of semantic points, which are the center points of\n",
      "cells and related to the membrane.\n",
      "3.3\n",
      "decouple the membranes and nuclei segmentation\n",
      "our goal is to use seg-net to generate masks for both membranes (\u0002\n",
      "mi ∈ rh×w ×1)\n",
      "and nuclei (\u0003si\n",
      "however, the two seg-net\n",
      "channels are interdependent, which can result in nuclei and membranes being insepa-\n",
      "rably segmented.\n",
      "to overcome this issue, we enforce one channel to output the nuclei\n",
      "segmentation using a supervised mask si ∈ rh×w ×1.\n",
      "thus, to provide seman-\n",
      "tic information to tran-net for predicting semantic points, the other channel must con-\n",
      "tain information describing the staining status of the membrane, which in turn decouples\n",
      "membrane segmentation.\n",
      "because both \u0003si and si are single-channel, we employ the naive l1 loss to supervise\n",
      "the segmentation of the nuclei, as shown in eq.\n",
      "(2)\n",
      "3.4\n",
      "constraints for membranes segmentation\n",
      "as there are no annotations available for pixel-level membrane segmentation, the\n",
      "network could result in unwanted over-segmentation of membranes.\n",
      "this over-\n",
      "segmentation can take two forms: (1) segmentation of stained impurities, which can\n",
      "restrict the network’s generalization performance by learning simple color features, and\n",
      "(2) segmentation of nuclei.\n",
      "the purpose of this\n",
      "544\n",
      "h. li et al.\n",
      "loss term is to encourage the network to learn a smoother membrane segmentation that\n",
      "does not capture small stained regions or nuclei.\n",
      "(3)\n",
      "however, relying solely on the ℓnorm\n",
      "i\n",
      "normalization term might lead to a trivial solu-\n",
      "tion, as it only minimizes the average value of the prediction result, potentially resulting\n",
      "in a minimal maximum conﬁdence for the cell membrane segmentation (e.g., 0.03).\n",
      "(4), to\n",
      "constrain the distribution of membrane segmentation results.\n",
      "(4)\n",
      "3.5\n",
      "point-level supervision\n",
      "using gω to detect the central point of the cells is a typical semantic points detection\n",
      "task.\n",
      "the difference is that the input of gω is the output of fθ rather than the image.\n",
      "3.6\n",
      "total loss\n",
      "by leveraging the advantages of the above items, we can obtain the total loss as follows:\n",
      "li = ℓnuclei\n",
      "i\n",
      "+ ℓnorm\n",
      "i\n",
      "+ ℓhinge\n",
      "i\n",
      "+ ℓpoints\n",
      "i\n",
      ",\n",
      "(6)\n",
      "where ℓnorm\n",
      "i\n",
      "and ℓhinge\n",
      "i\n",
      "are antagonistic, and their values are close to 0.5 in the ideal\n",
      "optimal state, which can be achieved at τ\n",
      "4\n",
      "experiments\n",
      "in order to comprehensively verify the proposed method, we conduct extensive exper-\n",
      "iments on two ihc membrane-stained data sets, namely the pdl1 (programmed cell\n",
      "depth 1 light 1) and her2 (human epidermal growth factor receiver 2) datasets.\n",
      "the\n",
      "her2 experiment is dedicated to validate segmentation performance, while the pdl1\n",
      "experiment is utilized to verify the effectiveness of converting segmentation results into\n",
      "clinically relevant indicators.\n",
      "segment membranes and nuclei via point-level supervision\n",
      "545\n",
      "4.1\n",
      "dataset\n",
      "we collected 648 her2 and 1076 pdl1 images at 40x magniﬁcation with a resolution\n",
      "of 1024 × 1024 from wsis.\n",
      "pixel-level annotations are used for\n",
      "testing and fully supervised experiments only.\n",
      "4.2\n",
      "implementation details and evaluation metric\n",
      "we totally train the networks 50 epochs and employ adam optimizer\n",
      "[10] with the\n",
      "initial learning rate of 5×10−4 and the momentum of 0.9.\n",
      "images are randomly cropped\n",
      "to 512 × 512, and data augmentations such as random rotation and ﬂip were employed\n",
      "during model training.\n",
      "we employ the intersection over union (iou) segmentation metric and pixel-level\n",
      "f1 score to validate the performance of the proposed method.\n",
      "however, only point-level\n",
      "annotations are equipped for the pdl1 dataset, we evaluate the segmentation perfor-\n",
      "mance at the point-level by converting the segmentation into key point predictions, and\n",
      "the conversion process details are available in the supplementary materials.\n",
      "table 1 shows the cell segmentation results of the proposed method and\n",
      "six comparison methods on the dataset her2.\n",
      "our proposed method can segment both\n",
      "cell nuclei and membranes simultaneously, outperforming both unsupervised methods\n",
      "and other point-level methods, with an iou score of 0.5774 and an f1 score of 0.6899\n",
      "for membranes, and an iou score of 0.5242 and an f1 score of 0.6795 for nuclei.\n",
      "furthermore, our ablation study shows that the hinge loss and normalization loss play\n",
      "important roles in improving the segmentation performance.\n",
      "notably, other point-level\n",
      "methods not only fail to segment the cell membranes but also have limited performance\n",
      "in segmenting cell nuclei due to over-segmentation and under-segmentation errors, as\n",
      "shown in fig.\n",
      "we chose to compare our method with unsupervised segmentation\n",
      "methods because existing point-supervised segmentation methods are unable to seg-\n",
      "ment cell membranes.\n",
      "among the unsupervised meth-\n",
      "ods, color deconvolution [4] shows the best performance with f1 scores of 0.5984 and\n",
      "0.6136 for negative and positive cells, respectively.\n",
      "besides, qualitative experimental results can be found in the\n",
      "supplementary materials.\n",
      "546\n",
      "h. li et al.\n",
      "table 1. comparison of the cell segmentation results on her2 test data.\n",
      "w/o: without.\n",
      "supervised settings\n",
      "methods\n",
      "membranes\n",
      "nuclei\n",
      "iou\n",
      "f1 score\n",
      "iou\n",
      "f1 score\n",
      "unspervised\n",
      "usar (our implementation)\n",
      ", we present a novel method for precise segmentation of cell membranes and\n",
      "nuclei in immunohistochemical (ihc) membrane staining images using only point-level\n",
      "segment membranes and nuclei via point-level supervision\n",
      "547\n",
      "supervision.\n",
      "our method achieves comparable performance to fully supervised pixel-\n",
      "level annotation methods while signiﬁcantly reducing annotation costs, only requir-\n",
      "ing one-tenth of the cost of pixel-level annotation.\n",
      "this approach effectively reduces\n",
      "the expenses involved in developing, deploying, and adapting ihc membrane-stained\n",
      "image analysis algorithms.\n",
      "in the future, we plan to further optimize the segmentation\n",
      "results of cell nuclei to further boost the performance of the proposed method, and\n",
      "extend it to the whole slide images (wsis).\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf:\n",
      "multiple instance learning is a powerful approach for whole slide\n",
      "image-based diagnosis in the absence of pixel- or patch-level annotations.\n",
      "in spite\n",
      "of the huge size of whole slide images, the number of individual slides is often\n",
      "rather small, leading to a small number of labeled samples.\n",
      "to improve train-\n",
      "ing, we propose and investigate novel data augmentation strategies for multiple\n",
      "instance learning based on the idea of linear and multilinear interpolation of fea-\n",
      "ture vectors within and between individual whole slide images.\n",
      "based on state-\n",
      "of-the-art multiple instance learning architectures and two thyroid cancer data\n",
      "sets, an exhaustive study was conducted considering a range of common data\n",
      "augmentation strategies.\n",
      "keywords: histopathology · data augmentation · mixup · multiple instance\n",
      "learning\n",
      "1\n",
      "motivation\n",
      "whole slide imaging is capable of effectively digitizing specimen slides, showing both\n",
      "the microscopic detail and the larger context, without any signiﬁcant manual effort.\n",
      "due\n",
      "to the enormous resolution of the whole slide images (wsis), a classiﬁcation based\n",
      "on straight-forward convolutional neural network architectures is not feasible.\n",
      "multi-\n",
      "ple instance learning [8,10,13,18,20] (mil) represents a methodology (with a high\n",
      "momentum indicated by a large number of recent publications) to deal with these huge\n",
      "images corresponding to single (global) labels.\n",
      "mil\n",
      "approaches typically consist of a feature extraction stage, a mil pooling stage and a\n",
      "following downstream classiﬁcation.\n",
      "for training the feature extraction stage, clas-\n",
      "sical supervised and self-supervised learning is employed\n",
      "while the majority\n",
      "of methods rely on separate learning stages, also end-to-end approaches have been pro-\n",
      "posed [3,14].\n",
      "general data augmentation strategies, such as rotations, ﬂipping, stain\n",
      "augmentation and normalization and afﬁne transformations, are applicable to increase\n",
      "the amount of data [15].\n",
      "all of these methods are performed in the image domain.\n",
      "here, we consider feature-level data augmentation directly applied to the representation\n",
      "extracted using a convolutional neural network.\n",
      "these methods can be easily combined\n",
      "with image-based augmentation and show the advantage of a high computational efﬁ-\n",
      "ciency (since operations are efﬁcient and pre-computed features can be used)\n",
      "[11] proposed an augmentation strategy based on sampling the\n",
      "patch-descriptors to generate several bags for an individual wsi.\n",
      "this method was originally proposed as data agnostic approach\n",
      "which also shows good results if applied to image data [2,4,16].\n",
      "due to the structure of mil training data, we identiﬁed several options to perform\n",
      "interpolation-based data augmentation.\n",
      "the main contribution of this work is a set of novel data augmentation strategies for\n",
      "mil, based on the interpolation of patch descriptors.\n",
      "for evaluation, a large experimental study was conducted, including 2 histolog-\n",
      "ical data sets, 5 deep learning conﬁgurations for mil, 3 common data augmentation\n",
      "strategies and 4 mixup settings.\n",
      "to obtain an improved understanding of reasons behind the\n",
      "experimental results, we also investigate the feature distributions.\n",
      "2\n",
      "methods\n",
      "in this paper, we consider mil approaches relying on separately trained feature\n",
      "extraction and classiﬁcation stages [9,10,12].\n",
      "the proposed augmentation methods are\n",
      "applied to the patch descriptors obtained after the feature extraction stage.\n",
      "this strategy\n",
      "is highly efﬁcient during training since the features are only computed once (per patch)\n",
      "and for augmentation only simple arithmetic operations are applied to the (smaller)\n",
      "feature vectors.\n",
      "image-based data augmentation strategies (such as stain-augmentation,\n",
      "rotations or deformations) can be combined easily with the feature-based approaches\n",
      "but require individual feature extraction during training.\n",
      "however, to avoid the curse of\n",
      "meta-parameters and thereby experiments these methods are not considered here.\n",
      "mixup-mil: novel data augmentation for multiple instance learning\n",
      "479\n",
      "in the original mixup formulation of zhang et al.\n",
      "the weight α is drawn from a uniform distribution between 0 and 1.\n",
      "a single input (corresponding to a wsi) of a mil approach with a separate feature\n",
      "extraction stage [10] can be expressed as a p-tupel x = (x1, ..., xp ) with xi being the\n",
      "feature vector of an individual patch and p being the number of patches per wsi.\n",
      "1. overview of the proposed feature-based data augmentation approaches.\n",
      "before applying the mixup operation, the vector\n",
      "tupel is randomly shufﬂed (as performed in all experiments).\n",
      "this number was kept stable (1024) during all experiments.\n",
      "the thereby\n",
      "obtained vector tupel (x1′, ..., xp ′) ﬁnally represents the synthetic wsi-based image\n",
      "descriptor.\n",
      "while the intra-mixup method described before represents a linear interpola-\n",
      "tion method, we also investigated a multilinear approach by computing xk ′ such that\n",
      "xk ′ = α◦xi +(1−α)◦xj with α being a random vector and ◦ being the element-wise\n",
      "product.\n",
      "this element-wise linear (multilinear) approach enables even higher variability\n",
      "in the generated samples.\n",
      "2.2\n",
      "experimental setting\n",
      "as experimental architecture, use the dual-stream mil approach proposed by li et\n",
      "al\n",
      "the model makes use of an individual feature extraction stage.\n",
      "due to the limited num-\n",
      "ber of wsis, we did not train the feature extraction stage\n",
      "speciﬁcally, we applied a resnet18 pre-trained on the image-net chal-\n",
      "lenge data, due to the high performance in previous work on similar data [5]. resnet18\n",
      "was assessed as particularly appropriate due to the rather low dimensional output (512\n",
      "mixup-mil: novel data augmentation for multiple instance learning\n",
      "481\n",
      "dimensions).\n",
      "we actively decided not to use a self-supervised contrastive learning app-\n",
      "roach [10] as feature extraction stage since invariant features could interfere with the\n",
      "effect of data augmentation.\n",
      "as comparison, several other augmentation methods on feature level are investi-\n",
      "gated including random sampling, selective random sampling and random noise.\n",
      "in the experiments, we adjust the sample ratio\n",
      "q between the patch-based features for training and testing.\n",
      "the elements of r\n",
      "are randomly sampled (individually for each xi) from a normal distribution n(0, σ′).\n",
      "to incorporate for the fact that the feature dimensions show different magnitudes, σ′\n",
      "is computed as the product of the meta parameter σ and the standard deviation of the\n",
      "respective feature dimension.\n",
      "this differentiation is crucial, due to the different treatment options, in particular with\n",
      "respect to the extent of surgical resection of the thyroid gland [19].\n",
      "the data set utilized\n",
      "in the experiments consists of 80 wsis overall.\n",
      "all images were acquired during clinical routine at the kardinal schwarzenberg\n",
      "hospital.\n",
      "procedures were approved by the ethics committee of the county of salzburg\n",
      "(no. 1088/2021).\n",
      "the mean and median age of patients at the date of dissection was\n",
      "47 and 50 years, respectively.\n",
      "the data set comprised 13 male and 27 female patients,\n",
      "corresponding to a slight gender imbalance.\n",
      "the images were digitized with an olympus vs120-ld100\n",
      "slide loader system.\n",
      "overviews at a 2x magniﬁcation were generated to manually deﬁne\n",
      "scan areas, focus points were automatically deﬁned and adapted if needed.\n",
      "the\n",
      "image ﬁles were stored in the oympus vsi format based on lossless compression.\n",
      "482\n",
      "m. gadermayr et al.\n",
      "inst\n",
      "3/1\n",
      "2/2\n",
      "1/3\n",
      "emb\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.49 0.72\n",
      "0.7\n",
      "0.68 0.69\n",
      "inst\n",
      "3/1\n",
      "2/2\n",
      "1/3\n",
      "emb\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.41 0.79 0.81 0.79 0.78\n",
      "100 %\n",
      "75 %\n",
      "50 %\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.7\n",
      "0.71\n",
      "0.69\n",
      "100 %\n",
      "75 %\n",
      "50 %\n",
      "0.69\n",
      "0.7\n",
      "0.7\n",
      "100 %\n",
      "75 %\n",
      "50 %\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.81\n",
      "0.82\n",
      "0.8\n",
      "100 %\n",
      "75 %\n",
      "50 %\n",
      "0.78\n",
      "0.82\n",
      "0.8\n",
      "100 %\n",
      "75 %\n",
      "50 %\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.7\n",
      "0.72\n",
      "0.73\n",
      "100 %\n",
      "75 %\n",
      "50 %\n",
      "0.69\n",
      "0.71\n",
      "0.71\n",
      "100 %\n",
      "75 %\n",
      "50 %\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.81\n",
      "0.81\n",
      "0.82\n",
      "100 %\n",
      "75 %\n",
      "50 %\n",
      "0.78\n",
      "0.82\n",
      "0.83\n",
      "0\n",
      "25 % 50 % 75 % 100 %\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.7\n",
      "0.7\n",
      "0.72 0.75 0.77\n",
      "0\n",
      "25 % 50 % 75 % 100 %\n",
      "0.69\n",
      "0.7\n",
      "0.71 0.74 0.78\n",
      "0\n",
      "25 % 50 % 75 % 100 %\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.81 0.77\n",
      "0.8\n",
      "0.79 0.79\n",
      "0\n",
      "25 % 50 % 75 % 100 %\n",
      "0.78 0.81 0.82 0.82 0.84\n",
      "intra-mixup linear\n",
      "inter-mixup\n",
      "selective sampling\n",
      "(c) mixup feature interpolation\n",
      "(b) baseline data augmentation\n",
      "(a) no augmentation\n",
      "random sampling\n",
      "instance & embedding-\n",
      "based mil\n",
      "random noise\n",
      "0%\n",
      "50%\n",
      "100%\n",
      "0.69\n",
      "0.7\n",
      "0.71 0.68 0.67\n",
      "v1\n",
      "v2\n",
      "v1\n",
      "v2\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.81 0.76 0.77 0.76 0.75\n",
      "0%\n",
      "50%\n",
      "100%\n",
      "v1\n",
      "v2\n",
      "v1\n",
      "v2\n",
      "0.78 0.76 0.79 0.77 0.77\n",
      "0%\n",
      "50%\n",
      "100%\n",
      "v1\n",
      "v2\n",
      "v1\n",
      "v2\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.7\n",
      "0.7\n",
      "0.68 0.68 0.68\n",
      "0%\n",
      "50%\n",
      "100%\n",
      "v1\n",
      "v2\n",
      "v1\n",
      "v2\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "q\n",
      "0\n",
      "25 % 50 % 75 % 100 %\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0\n",
      "25 % 50 % 75 % 100 %\n",
      "0\n",
      "25 % 50 % 75 % 100 %\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0\n",
      "25 % 50 % 75 % 100 %\n",
      "intra-mixup multilinear\n",
      "dual stream (2/2)\n",
      "embedding-based\n",
      "dual stream (2/2)\n",
      "embedding-based\n",
      "0.7\n",
      "0.7\n",
      "0.72 0.71 0.73\n",
      "0.69\n",
      "0.7\n",
      "0.68 0.72 0.73\n",
      "0.81 0.78 0.79 0.77 0.79\n",
      "0.78\n",
      "0.8\n",
      "0.8\n",
      "0.82 0.82\n",
      "frozen\n",
      "section\n",
      "dataset\n",
      "paraﬃn\n",
      "section\n",
      "dataset\n",
      "0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "0.78\n",
      "0.79\n",
      "0.8\n",
      "0.77\n",
      "0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "0.69\n",
      "0.69\n",
      "0.7\n",
      "0.71\n",
      "0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.7\n",
      "0.71\n",
      "0.7\n",
      "0.71\n",
      "0\n",
      "0.001\n",
      "0.01\n",
      "0.1\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.81\n",
      "0.8\n",
      "0.8\n",
      "0.8\n",
      "fig.\n",
      "subﬁgure\n",
      "(b) shows the scores obtained with baseline data augmentation for embedding-based and dual-\n",
      "stream mil. subﬁgure (c) shows the scores obtained with interpolation between (inter-mixup)\n",
      "mixup-mil: novel data augmentation for multiple instance learning\n",
      "483\n",
      "the data set was randomly separated into training (80 %) and test data (20 %).\n",
      "random shufﬂing\n",
      "of the vector tupels (shufﬂing within the wsis) was applied for all experiments.\n",
      "for feature extraction, a resnet18 net-\n",
      "work, pretrained on the image-net challenge was deployed\n",
      "we use the refer-\n",
      "ence implementation of the dual-stream mil approach [10]. to obtain further insight\n",
      "into the feature distribution, we randomly selected patch descriptor pairs and computed\n",
      "the euclidean distances.\n",
      "subﬁgure (b) show the scores obtained with\n",
      "baseline data augmentation for embedding-based and dual-stream mil. subﬁgure (c)\n",
      "shows the scores obtained with interpolation between patches between (inter-mixup)\n",
      "and within wsis (intra-mixup).\n",
      "without data augmentation, scores between 0.49 and\n",
      "0.72 were obtained for frozen and scores between 0.41 and 0.81 for the parafﬁn data\n",
      "set.\n",
      "with baseline data augmentation, scores between 0.69 and 0.73 were\n",
      "achieved for the frozen and between 0.78 and 0.83 for the parafﬁn data set.\n",
      "intra-\n",
      "mixup showed average accuracy up to 0.78 for the frozen and up to 0.84 for the parafﬁn\n",
      "data set.\n",
      "4\n",
      "discussion\n",
      "in this work, we proposed and examined novel data augmentation strategies based on\n",
      "the idea of interpolations of feature vectors in the mil setting.\n",
      "the considered dual-stream approach, including an embedding\n",
      "and instance-based stream, exhibited slightly improved average scores, compared to\n",
      "embedding-based mil only.\n",
      "with the baseline\n",
      "data augmentation approaches, the maximum improvements were 0.03, and 0.02 for the\n",
      "frozen, and 0.01, and 0.05 for the parafﬁn data set.\n",
      "the inter-mixup approach did not\n",
      "show any systematic improvements.\n",
      "also a clear trend with increasing scores in the case of an increasing\n",
      "ratio of augmented data (β) is visible.\n",
      "in addition, the results showed that the variability between\n",
      "mixup-mil: novel data augmentation for multiple instance learning\n",
      "485\n",
      "classes is, on patch-level, not clearly larger than the variability within a class.\n",
      "we expect that stain nor-\n",
      "malization methods (but not stain augmentation) could be utilized to align the different\n",
      "wsis to provide a more appropriate basis for inter-wsi interpolation.\n",
      "with the proposed intra-mixup augmentation strategy, this effect\n",
      "diminishes, since the amount and quality of training data is increased.\n",
      "to conclude, we proposed novel data augmentation strategies based on the idea\n",
      "of interpolations of image descriptors in the mil setting.\n",
      "based on the experimental\n",
      "results, the multilinear intra-mixup setting proved to be highly effective, while the\n",
      "inter-mixup method showed inferior scores compared to a state-of-the-art baseline.\n",
      "in the future, additional experiments will be conducted including stain normalization\n",
      "methods and larger benchmark data sets to provide further insights.\n",
      "acknowledgement.\n",
      "this work was partially funded by the county of salzburg (no. fhs2019-\n",
      "10-kiamed)\n",
      "references\n",
      "1. buddhavarapu, v.g., jothi, a.a.: an experimental study on classiﬁcation of thyroid\n",
      "histopathology images using transfer learning.\n",
      "in: proceedings of the international con-\n",
      "ference on medical image computing and computer assisted intervention (miccai), pp.\n",
      "519–528 (2020)\n",
      "4. dabouei, a., soleymani, s., taherkhani, f., nasrabadi, n.m.: supermix: supervising the\n",
      "mixing data augmentation.\n",
      "6. galdran, a., carneiro, g., ballester, m.a.g.: balanced-mixup for highly imbalanced med-\n",
      "ical image classiﬁcation.\n",
      "in: proceedings of the conference on medical image computing\n",
      "and computer assisted intervention (miccai), pp. 323–333 (2021)\n",
      "486\n",
      "m. gadermayr et al.\n",
      "7.\n",
      "hou, l., samaras, d., kurc, t.m., gao, y., davis, j.e., saltz, j.h.: patch-based convolutional\n",
      "neural network for whole slide tissue image classiﬁcation.\n",
      "lerousseau, m., et al.: weakly supervised multiple instance learning histopathological tumor\n",
      "segmentation.\n",
      "li, b., li, y., eliceiri, k.w.: dual-stream multiple instance learning network for whole slide\n",
      "image classiﬁcation with self-supervised contrastive learning.\n",
      "li, z., et al.: a novel multiple instance learning framework for covid-19 severity assessment\n",
      "via data augmentation and self-supervised learning.\n",
      "image anal.\n",
      "rymarczyk, d., borowa, a., tabor, j., zielinski, b.: kernel self-attention for weakly-\n",
      "supervised image classiﬁcation using deep multiple instance learning.\n",
      "shao, z., et al.: transmil: transformer based correlated multiple instance learning for\n",
      "whole slide image classiﬁcation.\n",
      "sharma, y., shrivastava, a., ehsan, l., moskaluk, c.a., syed, s., brown, d.: cluster-to-\n",
      "conquer: a framework for end-to-end multi-instance learning for whole slide image classi-\n",
      "ﬁcation.\n",
      "tellez, d., et al.: quantifying the effects of data augmentation and stain color normalization\n",
      "in convolutional neural networks for computational pathology.\n",
      "image anal.\n",
      "verma, v., et al.: manifold mixup: better representations by interpolating hidden states.\n",
      "transpath: transformer-based self-supervised learning for histopathological\n",
      "image classiﬁcation.\n",
      "in: proceedings of the conference on medical image computing and\n",
      "computer assisted intervention (miccai), pp.\n",
      "zhang, h., et al.: dtfd-mil: double-tier feature distillation multiple instance learning for\n",
      "histopathology whole slide image classiﬁcation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf:\n",
      "existing deep learning models have achieved promising per-\n",
      "formance in recognizing skin diseases from dermoscopic images.\n",
      "therefore, it is crucial to automatically discover and\n",
      "identify new semantic categories from new data.\n",
      "in this paper, we propose\n",
      "a new novel class discovery framework for automatically discovering new\n",
      "semantic classes from dermoscopy image datasets based on the knowledge\n",
      "of known classes.\n",
      "speciﬁcally, we ﬁrst use contrastive learning to learn a\n",
      "robust and unbiased feature representation based on all data from known\n",
      "and unknown categories.\n",
      "finally, we further reﬁne the pseudo label by aggregating neighborhood\n",
      "information through local sample similarity to improve the clustering per-\n",
      "formance of the model for unknown categories.\n",
      "we conducted extensive\n",
      "experiments on the dermatology dataset isic 2019, and the experimen-\n",
      "tal results show that our approach can eﬀectively leverage knowledge from\n",
      "known categories to discover new semantic categories.\n",
      "we also further vali-\n",
      "dated the eﬀectiveness of the diﬀerent modules through extensive ablation\n",
      "experiments.\n",
      "keywords: novel class discovery · skin lesion recognition · deep\n",
      "learning\n",
      "1\n",
      "introduction\n",
      "automatic identiﬁcation of lesions from dermoscopic images is of great impor-\n",
      "tance for the diagnosis of skin cancer [16,22].\n",
      "currently, deep learning mod-\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 3.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "to allevi-\n",
      "ate the labeling burden, semi-supervised learning has been proposed to exploit\n",
      "a large amount of unlabeled data to improve performance in the case of limited\n",
      "labeled data [10,15,19].\n",
      "[7,9,24], which aims to transfer knowledge from known classes to discover new\n",
      "semantic classes.\n",
      "most ncd methods follow a two-stage scheme: 1) a stage of\n",
      "fully supervised training on known category data and 2) a stage of clustering\n",
      "on unknown categories [7,9,24].\n",
      "[9] further introduced\n",
      "self-supervised learning in the ﬁrst stage to learn general feature representations.\n",
      "[21] to further\n",
      "exploit the information from known classes to improve the performance of unsu-\n",
      "pervised clustering.\n",
      "in\n",
      "addition, they only consider the global alignment of samples to the category\n",
      "center, ignoring the local inter-sample alignment thus leading to poor clustering\n",
      "performance.\n",
      "speciﬁcally, we ﬁrst use contrastive\n",
      "learning to pretrain the model based on all data from known and unknown cat-\n",
      "egories to learn a robust and general semantic feature representation.\n",
      "the cross-pseudo-supervision strategy is then used to force the model to\n",
      "maintain consistent prediction outputs for diﬀerent views of unlabeled images.\n",
      "finally, to encourage local neighborhood alignment and further reﬁne the pseudo\n",
      "26\n",
      "w. feng et al.\n",
      "fig.\n",
      "labels, we propose a local information aggregation module to aggregate the infor-\n",
      "mation of the neighborhood samples to boost the clustering performance.\n",
      "we\n",
      "conducted extensive experiments on the dermoscopy dataset isic 2019, and the\n",
      "experimental results show that our method outperforms other state-of-the-art\n",
      "comparison algorithms by a large margin.\n",
      "in addition, we also validated the\n",
      "eﬀectiveness of diﬀerent components through extensive ablation experiments.\n",
      "2\n",
      "methodology\n",
      "given an unlabeled dataset {xu\n",
      "i }n u\n",
      "i=1 with n u images, where xu\n",
      "i is the ith unla-\n",
      "beled image.\n",
      "in addition, we also have access to a labeled dataset {xl\n",
      "i, yl\n",
      "i}n l\n",
      "i=1 with\n",
      "n l images, where xl\n",
      "i is the ith labeled image and yl\n",
      "specif-\n",
      "ically, we use xi and x′\n",
      "i to represent diﬀerent augmented versions of the same\n",
      "image in a mini-batch.\n",
      "the unsupervised contrastive loss can be formulated as:\n",
      "lucl\n",
      "i\n",
      "= − log\n",
      "exp (zi · z′\n",
      "i/τ)\n",
      "\u0004\n",
      "n 1[n̸=i] exp (zi · zn/τ)\n",
      "(1)\n",
      "where zi = e(xi) is the deep feature representation of the image xi, e is the\n",
      "feature extractor network, and τ is the temperature value.\n",
      "in addition, to help the feature extractor learn semantically meaningful fea-\n",
      "ture representations, we introduce supervised contrastive learning\n",
      "for an original image xi, we generate two augmented\n",
      "versions of xi, xv1\n",
      "i\n",
      "and xv2\n",
      "i .\n",
      "we then feed these two augmented images into m1\n",
      "and m2 to obtain the predictions for xv1\n",
      "i\n",
      "and xv2\n",
      "i :\n",
      "pv1\n",
      "i,1\n",
      "for an input image\n",
      "xi, if xi is from the known category, we construct the training target as one\n",
      "hot vector, where the ﬁrst cl elements are ground truth labels and the last cu\n",
      "elements are 0.\n",
      "if xi is from the unknown category, we set the ﬁrst cl elements\n",
      "to 0 and use pseudo labels for the remaining cu elements.\n",
      "; yu\n",
      "bu\n",
      "\t\n",
      "∈ rbu×cu will assign bu unknown category samples to\n",
      "cu category prototypes uniformly, i.e., each category prototype will be selected\n",
      "bu/cu times on average.\n",
      "speciﬁcally,\n",
      "we ﬁrst compute the variance of the predicted outputs of the models for the\n",
      "diﬀerent augmented images via kl-divergence:\n",
      "if the variance of the model’s predictions\n",
      "for diﬀerent augmented images is large, the pseudo label may be of low quality,\n",
      "and vice versa.\n",
      "however, it ignores the alignment between local neighborhood\n",
      "samples, i.e., the samples are susceptible to interference from some irrelevant\n",
      "semantic factors such as background and color.\n",
      "here, we propose a local infor-\n",
      "mation aggregation to enhance the alignment of local samples.\n",
      "by aggregating\n",
      "the information of the neighborhood samples, we are able to ensure consistency\n",
      "between local samples, which further improves the clustering performance.\n",
      "3\n",
      "experiments\n",
      "dataset.\n",
      "to validate the eﬀectiveness of the proposed algorithm, we con-\n",
      "duct experiments on the widely used public dermoscopy challenge dataset isic\n",
      "2019\n",
      "the dataset contains a total of 25,331 dermoscopic images from\n",
      "eight categories: melanoma (mel), melanocytic nevus (nv), basal cell carci-\n",
      "noma (bcc), actinic keratosis (ak), benign keratosis (bkl), dermatoﬁbroma\n",
      "(df), vascular lesion (vasc), and squamous cell carcinoma (scc).\n",
      "for task 1 and task 2, we\n",
      "report the average performance of 5 runs.\n",
      "implementation details.\n",
      "for data augmentation, we use ran-\n",
      "dom horizontal/vertical ﬂipping, color jitter, and gaussian blurring following\n",
      "[7].\n",
      "the batch\n",
      "size in all experiments is 512.\n",
      "following [9,23,24], we\n",
      "report the clustering performance on the unlabeled unknown category dataset.\n",
      "clustering performance of diﬀerent comparison algorithms on diﬀerent tasks.\n",
      "following [2,9], we use the average clustering accuracy (acc), normalized\n",
      "mutual information (nmi) and adjusted rand index (ari) to evaluate the clus-\n",
      "tering performance of diﬀerent algorithms.\n",
      "speciﬁcally, we ﬁrst match the clus-\n",
      "tering assignment and ground truth labels by the hungarian algorithm\n",
      "after\n",
      "the optimal assignment is determined, we then compute each metric.\n",
      "we imple-\n",
      "ment all algorithms based on the pytorch framework and conduct experiments\n",
      "on 8 rtx 3090 gpus.\n",
      "comparison with state-of-the-art methods.\n",
      "we compare our algorithms\n",
      "with some state-of-the-art ncd methods, including rankstats [9], rankstats+\n",
      "(rankstats with incremental learning)\n",
      "table 1 shows the clustering performance of each comparison algorithm on\n",
      "diﬀerent ncd tasks.\n",
      "it can be seen that the clustering performance of the bench-\n",
      "mark method is poor, which indicates that the model pre-trained using only the\n",
      "known category data does not provide a good clustering of the unknown category.\n",
      "moreover, the state-of-the-art ncd methods can improve the clustering perfor-\n",
      "mance, which demonstrates the eﬀectiveness of the currently popular two-stage\n",
      "solution.\n",
      "compared with the best comparison algorithm uno, our method yields 5.23%\n",
      "acc improvement, 3.56% nmi improvement, and 2.55% ari improvement on\n",
      "task1, and 3.24% acc improvement, 1.34% nmi improvement, and 2.37% ari\n",
      "improvement on task2, which shows that our method is able to provide more\n",
      "reliable pseudo labels for ncd.\n",
      "we performed ablation exper-\n",
      "iments to verify the eﬀectiveness of each component.\n",
      "it can be\n",
      "observed that cl brings a signiﬁcant performance gain, which indicates that\n",
      "towards novel class discovery: a study in novel skin lesions clustering\n",
      "31\n",
      "table 2. ablation study of each key component.\n",
      "in addition, umcps also improves the clustering performance of the\n",
      "model, which indicates that uniﬁed training helps to the category information\n",
      "interaction.\n",
      "lia further improves the clustering performance, which indicates\n",
      "that local information aggregation helps to provide better pseudo labels.\n",
      "finally,\n",
      "our algorithm incorporates each component to achieve the best performance.\n",
      "as shown in\n",
      "table 3, it can be observed that both components improve the clustering perfor-\n",
      "mance of the model, which indicates that scl helps the model to learn seman-\n",
      "tically meaningful feature representations, while ucl makes the model learn\n",
      "robust unbiased feature representations and avoid its overﬁtting to known cate-\n",
      "gories.\n",
      "as shown in table 3, it can\n",
      "be seen that cps outperforms w/o cps, which indicates that cps encourages\n",
      "the model to maintain consistent predictions for diﬀerent augmented versions\n",
      "32\n",
      "w. feng et al.\n",
      "of the input images, and enhances the generalization performance of the model.\n",
      "umcps achieves the best clustering performance, which shows its ability to use\n",
      "uncertainty to alleviate the eﬀect of noisy pseudo labels and avoid causing error\n",
      "accumulation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf:\n",
      "current solutions mostly need to localize\n",
      "suspicious cells and classify abnormality based on local patches, con-\n",
      "cerning the fact that whole slide images of tct are extremely large.\n",
      "it\n",
      "thus requires many annotations of normal and abnormal cervical cells, to\n",
      "supervise the training of the patch-level classiﬁer for promising perfor-\n",
      "mance.\n",
      "in this paper, we propose cellgan to synthesize cytopatholog-\n",
      "ical images of various cervical cell types for augmenting patch-level cell\n",
      "classiﬁcation.\n",
      "built upon a lightweight backbone, cellgan is equipped\n",
      "with a non-linear class mapping network to eﬀectively incorporate cell\n",
      "type information into image generation.\n",
      "we also propose the skip-layer\n",
      "global context module to model the complex spatial relationship of the\n",
      "cells, and attain high ﬁdelity of the synthesized images through adver-\n",
      "sarial learning.\n",
      "our experiments demonstrate that cellgan can produce\n",
      "visually plausible tct cytopathological images for diﬀerent cell types.\n",
      "we also validate the eﬀectiveness of using cellgan to greatly augment\n",
      "patch-level cell classiﬁcation performance.\n",
      "keywords: conditional image synthesis · generative adversarial\n",
      "network · cytopathological image classiﬁcation · data augmentation\n",
      "1\n",
      "introduction\n",
      "cervical cancer accounts for 6.6% of the total cancer deaths in females world-\n",
      "wide, making it a global threat to healthcare\n",
      "early cytology screening is\n",
      "highly eﬀective for the prevention and timely treatment of cervical cancer\n",
      "[23].\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2_47.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "the nilm cells have no cyto-\n",
      "logical abnormalities while the others are manifestations of cervical abnormal-\n",
      "ity to a diﬀerent extent.\n",
      "by observing cellular features (e.g., nucleus-cytoplasm\n",
      "ratio) and judging cell types, pathologists can provide a diagnosis that is critical\n",
      "to the clinical management of cervical abnormality.\n",
      "after scanning whole-slide images (wsis) from tct samples, automatic\n",
      "tct screening is highly desired due to the large population versus the lim-\n",
      "ited number of pathologists.\n",
      "as the wsi data per sample has a huge size, the\n",
      "idea of identifying abnormal cells in a hierarchical manner has been proposed\n",
      "and investigated by several studies using deep learning\n",
      "the promising performance of cell classiﬁca-\n",
      "tion at the patch level is critical, which contributes to sample-level diagnosis\n",
      "after integrating outcomes from many patches in a wsi.\n",
      "and\n",
      "the eﬀorts in collecting reliably annotated data can hardly be negligible, which\n",
      "requires high expertise due to the intrinsic diﬃculty of visually reading wsis.\n",
      "to alleviate the shortage of suﬃcient data to supervise classiﬁcation, one\n",
      "may adopt traditional data augmentation techniques, which yet may bring little\n",
      "improvement due to scarcely expanded data diversity [26].\n",
      "thus, synthesizing\n",
      "cytopathological images for cervical cells is highly desired to eﬀectively augment\n",
      "training data.\n",
      "existing literature on pathological image synthesis has explored\n",
      "the generation of histopathological images [10,28].\n",
      "in cytopathological images,\n",
      "on the contrary, cervical cells can be spatially isolated from each other, or are\n",
      "highly squeezed and even overlapped.\n",
      "the spatial relationship of individual cells\n",
      "is complex, adding diversity to the image appearance of color, morphology, tex-\n",
      "ture, etc. in addition, the diﬀerences between cell types are mainly related to\n",
      "nuanced cellular attributes, thus requiring ﬁne granularity in modulating syn-\n",
      "thesized images toward the expected cell types.\n",
      "therefore, the task to synthesize\n",
      "realistic cytopathological images becomes very challenging.\n",
      "aiming at augmenting the performance of cervical abnormality screening, we\n",
      "develop a novel conditional generative adversarial network in this paper, namely\n",
      "cellgan, to synthesize cytopathological images for various cell types.\n",
      "we lever-\n",
      "age fastgan [16] as the backbone for the sake of training stability and compu-\n",
      "tational eﬃciency.\n",
      "to the best of our knowledge, our proposed cellgan is\n",
      "the ﬁrst generative model with the capability to synthesize realistic cytopatho-\n",
      "logical images for various cervical cell types.\n",
      "the experimental results validate\n",
      "the visual plausibility of cellgan synthesized images, as well as demonstrate\n",
      "their data augmentation eﬀectiveness on patch-level cell classiﬁcation.\n",
      "2\n",
      "method\n",
      "the dilemma of medical image synthesis lies in the conﬂict between the lim-\n",
      "ited availability of medical image data and the high demand for data amount\n",
      "to train reliable generative models.\n",
      "to ensure the synthesized image quality\n",
      "given relatively limited training samples, the proposed cellgan is built upon\n",
      "fastgan [16] towards stabilized and fast training for few-shot image synthesis.\n",
      "by working in a class-conditional manner, cellgan can explicitly control the\n",
      "cervical squamous cell types in the synthesized cytopathological images, which\n",
      "is critical to augment the downstream classiﬁcation task.\n",
      "1, and more detailed structures of the key\n",
      "components are displayed in supplementary materials.\n",
      "the ﬁrst input of the class\n",
      "label y, which adopts one-hot encoding, provides class-conditional information to\n",
      "indicate the expected cervical cell type in the synthesized image isyn.\n",
      "the second\n",
      "input of the 128-dimensional latent vector z represents the remaining image\n",
      "information, from which isyn is gradually expanded.\n",
      "speciﬁcally, the class label y is ﬁrst projected to a class embed-\n",
      "ding c via a non-linear mapping network, which is implemented using four groups\n",
      "of fully connected layers and leakyrelu activations.\n",
      "we set the dimensions of\n",
      "class embedding c to the same as the latent vector z.\n",
      "then, we pass c through\n",
      "learnable aﬃne transformations, such that the class embedding is specialized\n",
      "to the scaling and bias parameters controlling adaptive instance normalization\n",
      "(adain)\n",
      "the motivation for the design above comes from\n",
      "our hypothesis that the class-conditional information mainly encodes cellular\n",
      "attributes related to cell types, rather than common image appearance.\n",
      "2 in supplementary materials), to better handle the diver-\n",
      "sity of the spatial relationship of the cells.\n",
      "in this way, the proposed\n",
      "sgc module learns a global understanding of the cell-to-cell spatial relationship\n",
      "and injects it into image generation via computationally eﬃcient modeling of\n",
      "long-range dependency.\n",
      "2.2\n",
      "discriminator and adversarial training\n",
      "in an adversarial training setting, the discriminator forces the generator to\n",
      "faithfully match the conditional data distribution of real cervical cytopatho-\n",
      "logical images, thus prompting the generator to produce visually and seman-\n",
      "tically realistic images.\n",
      "in particular, ﬁve resnet-like [7] down-\n",
      "blocks are employed to convert the input image into an 8×8×512 feature map.\n",
      "two simple decoders reconstruct downscaled and randomly cropped versions of\n",
      "input images i′\n",
      "crop and i′\n",
      "resize from 82 and 162 feature maps, respectively.\n",
      "− t (x)∥ℓ1\n",
      "\u0003\n",
      ",\n",
      "(1)\n",
      "where t denotes the image processing (i.e., 1\n",
      "2 downsampling and 1\n",
      "4 random crop-\n",
      "ping) on real image ireal, f is the processed intermediate feature map from the\n",
      "cellgan: conditional cervical cell synthesis\n",
      "491\n",
      "discriminator dis, and dec stands for the reconstruction decoder.\n",
      "this simple\n",
      "self-supervised technique provides a strong regularization in forcing the discrim-\n",
      "inator to extract a good image representation.\n",
      "to provide more detailed feedback from the discriminator, patchgan [12]\n",
      "architecture is adopted to output an 8×8 logit map by using a 1×1 convolution\n",
      "on the last feature map.\n",
      "by penalizing image content at the scale of patches, the\n",
      "color ﬁdelity of synthesized images is guaranteed as illustrated in our ablation\n",
      "study (see fig. 3).\n",
      "to align the class-conditional fake and real data distributions\n",
      "in the adversarial setting, the discriminator directly incorporates class labels\n",
      "as additional inputs in the manner of projection discriminator [20].\n",
      "the class\n",
      "label is projected to a learned 512-dimensional class embedding and takes inner-\n",
      "product at every spatial position of the 8 × 8 × 512 feature map.\n",
      "the resulting\n",
      "8×8 feature map is then added to the aforementioned 8×8 logit map, composing\n",
      "the ﬁnal output of the discriminator.\n",
      "combining all the loss functions above,\n",
      "the total objective ltotal to train the proposed cellgan in an adversarial manner\n",
      "can be expressed as:\n",
      "ltotal = ladv + lrecon + λreglreg,\n",
      "(2)\n",
      "where λreg is empirically set to 0.01 in our experiments.\n",
      "3\n",
      "experimental results\n",
      "3.1\n",
      "dataset and experimental setup\n",
      "dataset.\n",
      "in this study, we collect 14,477 images with 256 × 256 pixels from\n",
      "three collaborative clinical centers.\n",
      "all the images are manually inspected to\n",
      "contain diﬀerent cervical squamous cell types.\n",
      "in total, there are 7,662 nilm,\n",
      "2,275 asc-us, 2,480 lsil, 1,638 asc-h, and 422 hsil images.\n",
      "all the 256×256\n",
      "images with their class labels are selected as the training data.\n",
      "implementation details.\n",
      "[19], diﬀerentiable augmentation\n",
      "[30]\n",
      "and exponential-moving-average optimization\n",
      "[8] is used to measure the overall\n",
      "semantic realism of the synthesized images.\n",
      "all the experiments are conducted\n",
      "using an nvidia geforce rtx 3090 gpu with pytorch\n",
      "[22].\n",
      "3.2\n",
      "evaluation of image synthesis quality\n",
      "we compare cellgan with the state-of-the-art generative models for class-\n",
      "conditional image synthesis, i.e., biggan\n",
      "the quantitative comparison by fid in table 1 also\n",
      "demonstrates the superiority of cellgan in synthesized image quality.\n",
      "to verify the eﬀects of key components in the proposed cellgan, we conduct\n",
      "an ablation study on four model settings in table 2 and fig.\n",
      "the visual results of model i suﬀer from severe color distortions while the other\n",
      "models do not, indicating that the patchgan-based discriminator can guarantee\n",
      "color ﬁdelity by patch-level image content penalty.\n",
      "3. generated images from ablation study of the following key components: (a)\n",
      "patchgan architecture, (b) class mapping network, (c) sgc module.\n",
      "this phenomenon\n",
      "suggests that the implementation of the class mapping network facilitates more\n",
      "distinguishable feature representations for diﬀerent cell types.\n",
      "by comparing the\n",
      "synthesized images from model iii with cellgan, it is observed that adopting\n",
      "sgc modules can yield more clear cell boundaries, which demonstrates the capa-\n",
      "bility of sgc module in modeling complicated cell-to-cell relationships in image\n",
      "space.\n",
      "3.3\n",
      "evaluation of augmentation eﬀectiveness\n",
      "to validate the data augmentation capacity of the proposed cellgan, we con-\n",
      "duct 5-fold cross-validations on the cell classiﬁcation performances of two classi-\n",
      "494\n",
      "z. shen et al.\n",
      "table 3.\n",
      "data augmentation comparison between the proposed cellgan and other\n",
      "synthesis-based methods (↑: higher is better).\n",
      "[11]) using four training data settings for\n",
      "comparison: (1) real data only (the baseline); (2) baseline + biggan synthe-\n",
      "sized images; (3) baseline + ldm synthesized images; (4) baseline + cellgan\n",
      "synthesized images.\n",
      "for each cell type, we randomly select 400 real images and\n",
      "divide them into 5 groups.\n",
      "for diﬀerent data settings, we synthe-\n",
      "size 2,000 images for each cell type using the corresponding generative method,\n",
      "and add them to the training data of each fold.\n",
      "random ﬂip is applied to all data settings since it is reasonable\n",
      "to use traditional data augmentation techniques simultaneously in practice.\n",
      "the experimental accuracy, precision, recall, and f1 score are listed in\n",
      "table 3.\n",
      "meanwhile, the scores of other metrics are all\n",
      "improved by more than 4%, indicating that our synthesized data can signif-\n",
      "icantly enhance the overall classiﬁcation performance.\n",
      "thanks to the visually\n",
      "plausible and semantically realistic synthesized data, cellgan is conducive to\n",
      "the improvement of cell classiﬁcation, thus serving as an eﬃcient tool for aug-\n",
      "menting automatic abnormal cervical cell screening.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_53.pdf:\n",
      "the commonly presented histology stain variation may mod-\n",
      "erately obstruct the diagnosis of human experts, but can considerably\n",
      "downgrade the reliability of deep learning models in various diagnostic\n",
      "tasks.\n",
      "many stain style transfer methods have been proposed to elimi-\n",
      "nate the variance of stain styles across diﬀerent medical institutions or\n",
      "even diﬀerent batches.\n",
      "in this paper, we make the ﬁrst attempt at a dif-\n",
      "fusion probabilistic model to cope with the indispensable stain style\n",
      "transfer in histology image context, called staindiff.\n",
      "speciﬁcally, our\n",
      "diﬀusion framework enables learning from unpaired images by proposing\n",
      "a novel cycle-consistent constraint, whereas existing diﬀusion models are\n",
      "restricted to image generation or fully supervised pixel-to-pixel transla-\n",
      "tion.\n",
      "moreover, given the stochastic nature of staindiff that multiple\n",
      "transferred results can be generated from one input histology image,\n",
      "we further boost and stabilize the performance by the proposal of a\n",
      "novel self-ensemble scheme.\n",
      "our model can avoid the challenging issues\n",
      "in mainstream networks, such as the mode collapses in gans or align-\n",
      "ment between posterior distributions in aes.\n",
      "in conclusion, staindiff\n",
      "suﬃces to increase the stain style transfer quality, where the training is\n",
      "straightforward and the model is simpliﬁed for real-world clinical deploy-\n",
      "ment.\n",
      "speciﬁcally, with dyes such as hematoxylin and eosin, transparent tissue ele-\n",
      "ments can be transformed into distinguishable features [1].\n",
      "yet, the stain variations\n",
      "can cause inconsistencies between human domain experts [11]; and also hinder\n",
      "the performance of computer-aided diagnostic (cad) systems\n",
      "moreover,\n",
      "experiments have shown that stain variations can lead to a signiﬁcant decrease in\n",
      "the accuracy and reproducibility of deep learning algorithms in histology analy-\n",
      "sis.\n",
      "while the conventional color matching [22] and stain\n",
      "separation methods [19] used to be popular; learning-based approaches have\n",
      "become increasingly dominant, because they eliminate the need for challenging\n",
      "manual selection of the template images.\n",
      "another approach, called staingan [26], improves on stst by\n",
      "tailoring a cyclegan [34] to get rid of the dependence on learning from paired\n",
      "histology images and enable an unsupervised learning manner.\n",
      "however, gan approaches and ae\n",
      "suﬀer from the training of extra discriminators and challenging alignment of the\n",
      "posterior distributions, respectively [27].\n",
      "[9], have emerged\n",
      "as an alternative approach that can achieve competitive performance in various\n",
      "staindiﬀ: stain style transfer with diﬀusion\n",
      "551\n",
      "image-related tasks, such as image generation, inpainting, super-resolution, and\n",
      "etc\n",
      "importantly, diﬀusion models oﬀer several advantages over gans and\n",
      "aes, including tractable probabilistic parameterization, stable training proce-\n",
      "dures, and theoretical guarantees [3].\n",
      "additionally, they can avoid some of the\n",
      "challenges encountered by gans and aes, such as the alignment of posterior\n",
      "distributions or training extra discriminators, leading to a simpler model and\n",
      "training process.\n",
      "while the current diﬀusion models focus on\n",
      "image synthesis [9] or supervised image-to-image transaction\n",
      "therefore, we design an\n",
      "innovative cycle-consistent diﬀusion model that allows the transfer of representa-\n",
      "tions between latent spaces at diﬀerent time steps with the same morphological\n",
      "structure preserved in an unsupervised manner, as shown in fig. 1(c).\n",
      "more innovatively, unlike existing diﬀusion\n",
      "models, staindiff is capable of learning from unpaired histology images, mak-\n",
      "ing it a more ﬂexible and practical solution.\n",
      "the model is superior to gan-based\n",
      "methods as the training of additional discriminators is free, and also spares for\n",
      "the diﬃculty in the alignment of posterior probabilities in ae-based approaches.\n",
      "(2) we also propose a self-ensemble scheme to further improve and stabilize the\n",
      "style transfer performance in staindiff.\n",
      "this scheme utilizes the stochastic\n",
      "property of the diﬀusion model to generate multiple slightly diﬀerent outputs\n",
      "from one input at the inference stage.\n",
      "(3) a broad range of histology tasks, such\n",
      "as stain normalization between multiple clients, can be conveniently achieved\n",
      "with minor adjustment to the loss in staindiff.\n",
      "[9] to transfer the\n",
      "stain style between two domains, i.e., x a, x b. however, the traditional training\n",
      "paradigm of conditional ddpms with paired images (xa\n",
      "0 , xb\n",
      "0 )\n",
      "to overcome this\n",
      "limitation, we design an innovative diﬀusion framework for stain style transfer,\n",
      "named staindiff, which leverages the success of cyclegan [34] and style-\n",
      "gan [26] and thus can be trained in an unsupervised manner with a novel\n",
      "cycle-consistency constraint.\n",
      "speciﬁcally, staindiff comprises two forward pro-\n",
      "cesses that perturb the histology image of two stain style domains to noise respec-\n",
      "tively, and two corresponding reverse processes that attempt to reconstruct noise\n",
      "back to original images from the perturbed ones.\n",
      "it comprises two diﬀusion paths, that each learns the histology image gen-\n",
      "eration with respect to one stain style domain.\n",
      "parameterized by the markov chain, the forward process\n",
      "in staindiff follows the vanilla ddpm by perturbing the histology images\n",
      "gradually with gaussian noise, until all structures and morphological context\n",
      "information are lost.\n",
      "formally, given a histology image xa\n",
      "0 with respect to the\n",
      "stain style domain a, a transition kernel q progressively generates a sequence of\n",
      "t latent variables xa\n",
      "1 , xa\n",
      "2 , · · · , xa\n",
      "t thorough the following equation:\n",
      "q(xa\n",
      "t |xa\n",
      "t−1) = n(xa\n",
      "t ;\n",
      "\u0002\n",
      "1 − βtxa\n",
      "t−1, βti),\n",
      "(1)\n",
      "where n(·) denotes the gaussian distribution, i is the identity matrix.\n",
      "identically, we can progress the\n",
      "latent variables xb\n",
      "1 , xb\n",
      "2 , · · · , xb\n",
      "t for the histology image xb\n",
      "0 from the stain style\n",
      "domain b in the same fashion as eq.\n",
      "(1) and generate images characterized by stain style a and b respec-\n",
      "tively, by gradually removing the noise initialized from gaussian prior.\n",
      "[4] is leveraged to train\n",
      "the denoising networks in the transition kernels.\n",
      "due to the absence of pixel-to-\n",
      "pixel paired histology of both stain styles, it is infeasible to learn the interplay\n",
      "between them in a supervised manner as in most previous works.\n",
      "the directed graphical model for the inference stage with self-ensemble scheme\n",
      "of the staindiff.\n",
      "we describe the inference stage of\n",
      "staindiff by transferring the histology images from stain style a to b; while\n",
      "the inverse, namely from b to a, is similar.\n",
      "given a histology image input xa\n",
      "0\n",
      "characterized by stain style a, we begin by perturbing it s steps with eq.\n",
      "next, we use the p-sample [9] iteratively to denoise\n",
      "the ˜xb\n",
      "s and obtain the transferred image ˜xb\n",
      "0 .\n",
      "meanwhile, in some clinical settings, multiple institu-\n",
      "tions or hospitals are involved, where stain normalization is usually employed for\n",
      "multiple stain styles to one style alignment.\n",
      "this modiﬁcation allows us\n",
      "to use staindiff for stain normalization without any other adjustments to the\n",
      "inference process.\n",
      "3\n",
      "experiments\n",
      "datasets.\n",
      "this dataset aims to measure the\n",
      "style transfer performance on 284 histology frames.\n",
      "meanwhile, 500 paired patches are generated from the remaining 100 slides as the\n",
      "test set, where we use pearson correlation coeﬃcient (pc), structural similarity\n",
      "index (ssim) [31] and feature similarity index for image quality assessment\n",
      "(fsim)\n",
      "this dataset evaluates the performance of stain normalization quanti-\n",
      "ﬁed by the downstream nine-category tissue structure classiﬁcation accuracy\n",
      "[14].\n",
      "implementations.\n",
      "all experiments are implemented in python 3.8.13 with\n",
      "pytorch 1.12.1 on two nvidia geforce rtx 3090 gpu cards with 24gib of\n",
      "1 https://mitos-atypia-14.grand-challenge.org.\n",
      "staindiﬀ: stain style transfer with diﬀusion\n",
      "555\n",
      "table 1. comparison of stain style transfer performance on dataset-a. to show the\n",
      "statistical signiﬁcance, the p-values in terms of ssim and fsim are computed with\n",
      "respect to staindiff (full setting).\n",
      "the ‘w/o se’ denotes the exclusion of the self-\n",
      "ensemble scheme from the inference stage.\n",
      "we leverage the adam optimizer with a learning rate of\n",
      "2 × 10−4, and a batch size of 4.\n",
      "the learning scheme follows previous work [18],\n",
      "where the training process continues for 100 epochs if the overall loss did not\n",
      "decrease to the average loss of the previous 20 epochs.\n",
      "all experiments are repeated for 7 runs with diﬀerent ﬁxed random seeds\n",
      "i.e., {0, 1, 2, 3, 4, 5, 6}; and metrics are reported in the form of mean±standard\n",
      "deviation.\n",
      "‘w/o\n",
      "se’ denotes excluding the self-ensemble scheme from the inference stage.\n",
      "moreover, the statistical signiﬁcance of our\n",
      "performance boost is validated by the p-values that are consistently smaller than\n",
      "0.005, as computed from the wilcoxon signed-rank test.\n",
      "table 2 presents the comparison\n",
      "results of the downstream classiﬁcation task, where the histology images in\n",
      "dataset-b are normalized using diﬀerent methods.\n",
      "table 1 and 2 show that incorporating a self-ensemble scheme\n",
      "can both boost the performance of staindiff, and bring down the variations,\n",
      "demonstrating its eﬀectiveness in stabilizing the stain transfer and normaliza-\n",
      "tion.\n",
      "to further investigate the eﬀect of ensemble number m, we conduct ablation\n",
      "on dataset-a. experimentally, the fsim when m = 1, 5, 10, 15, 20, 50 are 0.742,\n",
      "0.749, 0.753, 0.756, 0.759, 0.759 respectively.\n",
      "while a slight performance gain\n",
      "can be achieved with higher m values than 10, the ensemble becomes more time-\n",
      "consuming, as the cost time is linear to m. it implies an optimal m should be\n",
      "selected as a trade-oﬀ between the performance and computational time, such\n",
      "as 10 in this work.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf:\n",
      "vulvovaginal candidiasis (vvc) is the most prevalent human\n",
      "candidal infection, estimated to aﬄict approximately 75% of all women\n",
      "at least once in their lifetime.\n",
      "automatic whole slide image\n",
      "(wsi) classiﬁcation is highly demanded, for the huge burden of dis-\n",
      "ease control and prevention.\n",
      "our experimental results\n",
      "demonstrate that our framework achieves state-of-the-art performance.\n",
      "keywords: whole slide image · vulvovaginal candidiasis ·\n",
      "attention-guided\n",
      "1\n",
      "introduction\n",
      "vulvovaginal candidiasis (vvc) is a type of fungal infection caused by candida,\n",
      "which results in discomforting symptoms, including itching and burning in the\n",
      "genital area [4,18].\n",
      "it is the most prevalent human candidal infection, estimated\n",
      "to aﬄict approximately 75% of all women at least once in their lifetime [1,20],\n",
      "resulting in huge consumption of medical resources.\n",
      "manual reading upon whole slide image (wsi) of tct is time-consuming and\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "1. examples of wsis (usually about 20000 × 20000 pixels), a cropped image of\n",
      "1024 × 1024 pixels from the wsi, and zoom-in views of candida and its position (indi-\n",
      "cated by the red arrows and annotation).\n",
      "for example, momenzadeh et al.\n",
      "[11] implemented\n",
      "automatic diagnosis based on machine learning.\n",
      "many systems of automatic computer-aided wsi screening have been designed\n",
      "for cytopathology [22,23], and histopathology\n",
      "(2) in addition to occupying only\n",
      "a small image space for each candida, the overall candida quantity in wsis is\n",
      "also low compared to the number of other cells.\n",
      "all\n",
      "of the above issues make it diﬃcult for diagnostic models to focus on candida,\n",
      "thus resulting in poor classiﬁcation performance and generalization capability.\n",
      "in this paper, we ﬁnd that the attention for a deep network to focus on can-\n",
      "dida is the key to the high performance of the screening task.\n",
      "our\n",
      "contributions are summarized into three parts: (1) we use a detection task to\n",
      "pre-train the encoder of the classiﬁcation model, moving the network’s attention\n",
      "away from individual cells and onto candida-like objects; (2) we propose skip\n",
      "self-attention (ssa) to take into account multi-scale semantic and texture fea-\n",
      "progressive attention guidance\n",
      "235\n",
      "fig.\n",
      "2. given a wsi, we\n",
      "ﬁrst crop it into multiple images, each of which is sized 1024×1024.\n",
      "for each\n",
      "cropped image, we conduct image-level classiﬁcation to ﬁnd out whether it suﬀers\n",
      "from suspicious candida infection.\n",
      "the image-level classiﬁer produces a score and\n",
      "feature representation of the image under consideration.\n",
      "then scores and features\n",
      "from all cropped images are reorganized and aggregated by a transformer for ﬁnal\n",
      "classiﬁcation by a fully connected (fc) layer.\n",
      "2.1\n",
      "detection task for pre-training\n",
      "we use a pre-trained detection model as prior to initialize the classiﬁcation\n",
      "model.\n",
      "in experimental exploration, we ﬁnd that, if we train the detection net-\n",
      "work directly, the bounding-box annotation indicates the location of candida\n",
      "and can rapidly establish a rough understanding of the morphology of candida.\n",
      "meanwhile, directly training a classiﬁcation model is usually easier to converge.\n",
      "however, in such a task, as candida occupies only a few pixels in an image, it is\n",
      "diﬃcult for the classiﬁer to focus on the target.\n",
      "that, the attention of the clas-\n",
      "siﬁer may spread across the entire image, leading to overﬁtted training quickly.\n",
      "therefore, we argue that the detection and classiﬁcation tasks are comple-\n",
      "mentary to solve our problem.\n",
      "particularly, we pre-train a detector and inherit\n",
      "its advantages in the classiﬁer.\n",
      "3. attention guided image-level classiﬁcation (corresponding to the classiﬁcation\n",
      "model in fig. 2).\n",
      "note that pre-training not only discards the complex posi-\n",
      "tioning task but also makes it easier for the classiﬁcation network to converge\n",
      "especially in the early stage of training.\n",
      "2.2\n",
      "transformer with skip self-attention (ssa)\n",
      "we design a novel skip self-attention (ssa) module to fuse discriminative fea-\n",
      "tures of candida from diﬀerent scales.\n",
      "at a coarse-grained level, there is the phenomenon that a candida\n",
      "usually links multiple host cells and yields a string of them.\n",
      "cnn-based methods have achieved excellent performance in computer-aided\n",
      "diagnosis including cervical cancer\n",
      "in recent years, vision transformer (vit)\n",
      "has been widely used in visual tasks for its global attention mechanism [14],\n",
      "sensitivity to shape information in images [19], and robustness to occlusion [12].\n",
      "nevertheless, such a transformer can be hard to train for our task, due to the\n",
      "large image size, huge network parameters, and huge demand for training data.\n",
      "progressive attention guidance\n",
      "237\n",
      "speciﬁcally, we use the pre-trained cnn-based encoder to extract features\n",
      "for each cropped image.\n",
      "on the contrary,\n",
      "the feature maps extracted from the last layer are high-level, which represents\n",
      "semantics regarding candida.\n",
      "2.3\n",
      "contrastive learning\n",
      "as mentioned in sect.\n",
      "our approach has two key goals: (1) to ensure that the features from the origi-\n",
      "nal image remain consistent after undergoing various image augmentations, and\n",
      "(2) to construct an image without the region of candida, resulting in highly\n",
      "dissimilar features compared to the original.\n",
      "inspired by a weakly supervised learning segmentation method\n",
      "3.\n",
      "to achieve this, we use augmentation and the attention map generated during\n",
      "the training process to construct three types of images and apply contrastive\n",
      "learning to the features extracted from them.\n",
      "for a given image i, we use image\n",
      "augmentation to generate iaug and use the encoder attached with ssa to extract\n",
      "feature, f c\n",
      "aug.\n",
      "we get the masked image imasked by subtracting m from i.\n",
      "m\n",
      "3, the features f, faug and fmasked from the three types\n",
      "of images i, iaug and imasked by the shared classiﬁer.\n",
      "in our task, we hope\n",
      "that the style gap does not aﬀect the feature extraction of the image, so the\n",
      "distance between faug and f should be attracted.\n",
      "if our network has eﬀective attention, the masked image should not contain\n",
      "any candida, so the score of the candida category after the mask s(imasked)\n",
      "should be minimized.\n",
      "otherwise, attention\n",
      "maps that cover the whole image can also result in low ltri.\n",
      "we take the average\n",
      "grayscale of attention map ¯\n",
      "m as a restriction, as shown in the last part of eq. 2.\n",
      "ltri,lam, and lfocus are combined as lcl to constrain each other and take full\n",
      "advantage of contrastive learning, as shown in eq.\n",
      "(3)\n",
      "2.4\n",
      "aggregated classiﬁcation for wsi\n",
      "with the strategies above, we have built the classiﬁer for all cropped images in a\n",
      "wsi.\n",
      "speciﬁcally, for each cropped image, we conduct image-level\n",
      "classiﬁcation to ﬁnd out whether it suﬀers from suspicious candida infection.\n",
      "the image-level classiﬁcation also produces a score, as well as the feature repre-\n",
      "sentation of the image under consideration.\n",
      "then, we reorganize features from\n",
      "all images by ranking their scores and preserving that with top-k scores.\n",
      "3\n",
      "experimental results\n",
      "datasets and experimental setup.\n",
      "each sample is scanned into a wsi following\n",
      "standard cytology protocol, which can be further cropped to 500 images sized\n",
      "1024×1024.\n",
      "for pre-training the detector, we prepare 1467 images with the size of\n",
      "1024×1024 pixels, all of which have bounding-box annotations.\n",
      "the ratio of\n",
      "training and validation is 4:1.\n",
      "for training of the image-level classiﬁcation model, we use 1940 positive\n",
      "images (1467 of which are used in detector pre-training) and 2093 negative\n",
      "images.\n",
      "all images used to pre-train the detector are categorized as training\n",
      "data here.\n",
      "the rest 473 images are split in 5-fold cross-validation, from which\n",
      "we collect experimental results and report later.\n",
      "progressive attention guidance\n",
      "239\n",
      "table 1. image-level classiﬁcation results and ablation study on the three contributions\n",
      "of our method.\n",
      "for implementation details, the models are implemented by pytorch and\n",
      "trained on 4 nvidia tesla v100s gpus.\n",
      "the batch sizes of the\n",
      "detection task, image-level classiﬁcation, and wsi-level classiﬁcation are 8, 8, 16,\n",
      "respectively.\n",
      "to aggregate wsi classiﬁcation, we use top-10 cropped images and\n",
      "their features.\n",
      "we report the performance using ﬁve common metrics: area under\n",
      "the receiver operating characteristic curve (auc), accuracy (acc), sensitivity\n",
      "(sen), speciﬁcity (spe), and f1-score.\n",
      "comparisons for image-level classiﬁcation.\n",
      "we conduct an ablation\n",
      "study to evaluate the contribution of pre-training (pt), skip self-attention (ssa),\n",
      "and contrastive learning (cl) for the image-level classiﬁcation, as shown in\n",
      "table 1.\n",
      "pt\n",
      "shows improvement in all situations, as a reasonable initial focus provides a solid\n",
      "foundation.\n",
      "ssa and cl can bring 2.89% and 6.38% improvement respectively\n",
      "compared to the method without each of them.\n",
      "it shows that ssa and cl can\n",
      "perform better when the model already has the basic ability to localize candida,\n",
      "i.e., after pt.\n",
      "to verify whether our model focuses on important regions of the input image\n",
      "for accurate classiﬁcation, we visualize the model’s attention using grad-cam\n",
      "[16].\n",
      "4. (a) the original image, where the green box indicates candida (enlarged in\n",
      "the left) and the red box shows the prediction of the detection model.\n",
      "to save\n",
      "computation, we did not verify the performance of the methods that performed\n",
      "too poorly on dataset-small.\n",
      "the detection-based method [23] uses a detection\n",
      "network to get suspicious candida and classify wsis with average conﬁdence.\n",
      "resnet trained without our method is the same as the baseline in table 1.\n",
      "we both considered the original trans-\n",
      "mil with pre-trained resnet-50 and the modiﬁed version with our image-level\n",
      "encoder.\n",
      "method\n",
      "dataset-small\n",
      "dataset-large\n",
      "image-level wsi-level\n",
      "auc\n",
      "acc\n",
      "sen\n",
      "auc\n",
      "acc\n",
      "sen\n",
      "detection\n",
      "threshold\n",
      "88.57 ± 9.56\n",
      "80.00 ± 10.0\n",
      "79.03 ± 14.4\n",
      "\\\n",
      "\\\n",
      "\\\n",
      "resnet\n",
      "threshold\n",
      "88.75 ± 6.58\n",
      "77.50 ± 9.35\n",
      "82.17 ± 13.0\n",
      "\\\n",
      "\\\n",
      "\\\n",
      "resnet\n",
      "transmil\n",
      "93.85 ± 3.71\n",
      "89.50 ± 3.67\n",
      "87.99 ± 7.55 80.46\n",
      "86.33\n",
      "67.85\n",
      "ours\n",
      "threshold\n",
      "92.50 ± 5.36\n",
      "86.00 ± 6.44\n",
      "83.04 ± 8.71\n",
      "82.59\n",
      "88.14 64.29\n",
      "ours\n",
      "mlp\n",
      "94.36 ± 3.50\n",
      "91.00 ± 6.44\n",
      "85.11 ± 13.16\n",
      "83.40\n",
      "87.32\n",
      "67.86\n",
      "ours\n",
      "transmil\n",
      "95.35 ± 1.48\n",
      "91.00 ± 3.21\n",
      "85.74 ± 4.25\n",
      "81.19\n",
      "86.78\n",
      "62.86\n",
      "ours\n",
      "transformer 95.78 ± 2.25 91.64 ± 3.17 85.55 ± 5.91\n",
      "84.18 87.67\n",
      "68.57\n",
      "progressive attention guidance\n",
      "241\n",
      "and is the most stable.\n",
      "our attention-based method brings 6% improvement of\n",
      "accuracy on data-small compared to other methods with the same wsi-level\n",
      "method ’threshold’.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf:\n",
      "colorectal cancer is a prevalent form of cancer, and many\n",
      "patients develop colorectal cancer liver metastasis (crlm) as a result.\n",
      "these scans form unique ﬁve-\n",
      "dimensional data (time, phase, and axial, sagittal, and coronal planes\n",
      "in 3d ct).\n",
      "most of the existing deep learning models can readily han-\n",
      "dle four-dimensional data (e.g., time-series 3d ct images) and it is not\n",
      "clear how well they can be extended to handle the additional dimension\n",
      "of phase.\n",
      "our\n",
      "experimental results show that a multi-plane architecture based on 3d\n",
      "bi-directional lstm, which we call mpbd-lstm, works best, achieving\n",
      "an area under curve (auc) of 0.79.\n",
      "on the other hand, analysis of the\n",
      "results shows that there is still great room for further improvement.\n",
      "keywords: colorectal cancer liver metastasis · liver cancer\n",
      "prediction · contrast-enhanced ct scan · bi-directional lstm\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 37.\n",
      "in addition to the axial, sagit-\n",
      "tal, and coronal planes in 3d ct scans, the data comprises contrast-enhanced\n",
      "multiple phases as its 4th dimension, along with diﬀerent timestamps as its 5th\n",
      "dimension.\n",
      "radiologists heavily rely on this data to detect the crlm in the\n",
      "very early stage [15].\n",
      "extensive existing works have demonstrated the power of deep learning on\n",
      "various spatial-temporal data, and can potentially be applied towards the prob-\n",
      "lem of crlm.\n",
      "however, all these methods have only demonstrated their eﬀectiveness\n",
      "towards 3d/4d data (i.e., time-series 2d/3d images), and it is not clear how\n",
      "to best extend them to work with the 5d cect data.\n",
      "[12]\n",
      "shows uni-directional lstm works well on natural videos while several other\n",
      "works show bi-directional lstm is needed in certain medical image segmenta-\n",
      "tion tasks [2,7].\n",
      "1. representative slices from 3d ct images of diﬀerent patients in our dataset,\n",
      "at a/v phases and timestamps t0, t1, t2 (cropped to 256 × 256 for better view).\n",
      "382\n",
      "x. li et al.\n",
      "– patients were previously diagnosed with colorectal cancer tnm stage i to\n",
      "stage iii, and recovered from colorectal radical surgery.\n",
      "– we already determined whether or not the patients had liver metastases\n",
      "within 2 years after the surgery, and manually labeled the dataset based\n",
      "on this.\n",
      "– no potential focal infection in the liver before the colorectal radical surgery.\n",
      "– no metastases in other organs before the liver metastases.\n",
      "– no other malignant tumors.\n",
      "ct images are collected with the following\n",
      "acquisition parameters: window width 150, window level 50, radiation dose 120\n",
      "kv, slice thickness 1 mm, and slice gap 0.8 mm.\n",
      "all images underwent manual\n",
      "quality control to exclude any scans with noticeable artifacts or blurriness and\n",
      "to verify the completeness of all slices.\n",
      "additional statistics on our dataset are\n",
      "presented in table 1 and examples of representative images are shown in fig.\n",
      "μ is the average function.\n",
      "[13,14], uses spatiotemporal lstm\n",
      "(st-lstm) by stacking multiple convlstm units and connecting them in\n",
      "a zigzag pattern to handle spatiotemporal data of 4 dimensions.\n",
      "4) simvp\n",
      "additionally, inspired by the bi-directional lstm used in medical\n",
      "image segmentation task\n",
      "after the computation of the 3d-lstm\n",
      "modules in each plane, we use an average function to combine the output hidden\n",
      "states from both planes.\n",
      "an alternative approach is to additionally connect two planes by combining\n",
      "the hidden states of 3d-lstm modules and taking their average if a module\n",
      "receives two inputs.\n",
      "however, we found that such design actually resulted in a\n",
      "worse performance.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "data augmentation and selection\n",
      "we selected 170 patients who underwent three or more cect scans from our\n",
      "original dataset, and cropped the images to only include the liver area, as shown\n",
      "in fig.\n",
      "for data augmentation, we randomly rotated the images\n",
      "from −30◦ to 30◦ and employed mixup [17].\n",
      "we applied the same augmentation\n",
      "technique consistently to all phases and timestamps of each patient’s data.\n",
      "for\n",
      "each slice, the dimension was 256 × 256 after cropping.\n",
      "the dimension of our ﬁnal input is (3 × 2 × 64 × 64 × 64), representing\n",
      "mpbd-lstm\n",
      "385\n",
      "(t × p × d\n",
      "h × w), where t is the number of timestamps, p is the number\n",
      "of diﬀerent phases, d is the slice depth, h is the height, and w is the width.\n",
      "3.2\n",
      "experiment setup\n",
      "as the data size is limited, 10-fold cross-validation is adopted, and the ratio of\n",
      "training and testing dataset is 0.9 and 0.1, respectively.\n",
      "as this is a classiﬁcation\n",
      "task, we evaluate all models’ performance by their auc scores.\n",
      "additional\n",
      "data on accuracy, sensitivity speciﬁcity, etc. can be found in the supplementary\n",
      "material.\n",
      "furthermore, predrnn-v2 [14], which passes memory ﬂow\n",
      "in a zigzag manner of bi-directional hierarchies, outperforms the uni-directional\n",
      "lstm-based saconvlstm\n",
      "this produced a one-dimensional\n",
      "bi-directional lstm (fig. 2(a), without the gray plane) with an input dimension\n",
      "of 3×128×64×64, which is the same as we used on other models.\n",
      "by replacing the bi-directional connection with\n",
      "a uni-directional connection, the mpbd-lstm model’s performance decreased\n",
      "to 0.768 on the original dataset.\n",
      "this result indicates that the bi-directional\n",
      "connection is crucial for computing temporal information eﬀectively, and its\n",
      "inclusion is essential for achieving high performance in mpbd-lstm.\n",
      "also, as mentioned previously, we initially connected the 3d-lstm mod-\n",
      "ules in two planes with their hidden states.\n",
      "therefore, we removed the inter-plane connections in the early stage, since their\n",
      "hidden states are still added together and averaged after they are processed by\n",
      "the lstm layers.\n",
      "we conducted ablation stud-\n",
      "ies using ct images from diﬀerent timestamps and phases to evaluate the\n",
      "eﬀectiveness of time-series data and multi-phase data.\n",
      "the results, as shown\n",
      "in table 4, indicate that mpbd-lstm achieves auc scores of 0.660, 0.676, and\n",
      "0.709 if only images from timestamps t0, t1, and t2 are used, respectively.\n",
      "these scores suggest that predicting crlm at earlier stages is more challenging\n",
      "since the features about potential metastases in ct images get more signiﬁcant\n",
      "over time.\n",
      "however, all of these scores are signiﬁcantly lower than the result\n",
      "using ct images from all timestamps.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_26.pdf:\n",
      "basal cell carcinoma (bcc) is a prevalent and increasingly\n",
      "diagnosed form of skin cancer that can beneﬁt from automated whole\n",
      "slide image (wsi) analysis.\n",
      "however, traditional methods that utilize\n",
      "popular network structures designed for natural images, such as the\n",
      "imagenet dataset, may result in reduced accuracy due to the signiﬁ-\n",
      "cant diﬀerences between natural and pathology images.\n",
      "in this paper,\n",
      "we analyze skin cancer images using the optimal network obtained by\n",
      "neural architecture search (nas) on the skin cancer dataset.\n",
      "furthermore, unlike traditional unilaterally aug-\n",
      "mented (ua) methods, the proposed supernet skin-cancer net (sc-net)\n",
      "considers the fairness of training and alleviates the eﬀects of evalua-\n",
      "tion bias.\n",
      "we use the sc-net to fairly treat all the architectures in the\n",
      "search space and leveraged evolutionary search to obtain the optimal\n",
      "architecture for a skin cancer dataset.\n",
      "our experiments involve 277,000\n",
      "patches split from 194 slides.\n",
      "keywords: basal cell carcinoma (bcc) · whole-slide pathological\n",
      "images · deep learning · neural architecture search (nas)\n",
      "1\n",
      "introduction\n",
      "skin cancer, the most prevalent cancer globally, has seen increasing incidences\n",
      "over recent decades\n",
      "timely bcc diagnosis is crucial to avoid complex treatments.\n",
      "although his-\n",
      "tological evaluation remains the gold standard for detection [3], deep learning\n",
      "and computer vision advancements can streamline this process.\n",
      "scanned tradi-\n",
      "tional histology slides result in whole slide images (wsis) that can be analyzed\n",
      "by deep learning models, signiﬁcantly easing the histological evaluation burden.\n",
      "recent advancements underscore the promise of this approach [4–6].\n",
      "fig.\n",
      "[7–9] typically employs models like\n",
      "inception net and resnet, designed for natural images like those in the imagenet\n",
      "dataset.\n",
      "the signiﬁcant variance in pathology and natural images can compro-\n",
      "mise these models’ accuracy.\n",
      "however, current nas methods often overlook fairness in architecture\n",
      "ranking, impeding the discovery of top-performing models.\n",
      "we observed that conventional nas methods\n",
      "often overlook fairness ranking during the search, hindering the search for opti-\n",
      "mal solutions.\n",
      "the eﬃcacy of sc-net was conﬁrmed by our experimental\n",
      "results, with our resnet50 achieving 96.2% top-1 accuracy and 96.5% auc,\n",
      "outperforming baseline methods by 4.8% and 4.7% respectively.\n",
      "mod-\n",
      "ule (a) extracts the region of interest (roi) from wsi and generates patches,\n",
      "detection of basal cell carcinoma in whole slide images\n",
      "265\n",
      "while module (b) uses optimal model architecture from nas to analyze features\n",
      "from patches and generate classiﬁcations.\n",
      "fig.\n",
      "a balanced evolutionary algorithm\n",
      "is then used to select the optimal structure from the search space, with the\n",
      "candidate structures’ performance evaluated using mini-batch patch data.\n",
      "the search\n",
      "leverages a supernet s with weights w, with each path γ inheriting weights from\n",
      "w. this makes one-shot nas a two-step optimization process: supernet training\n",
      "and architecture search.\n",
      "et al.\n",
      "network width d∗ corresponds to the network width with the best performance\n",
      "(e.g. classiﬁcation accuracy) on the validation dataset, i.e.,\n",
      "d∗ = arg max\n",
      "d∈d\n",
      "acc(d, w∗\n",
      "d; w∗, s, dv), s.t. flops(d) ≤ fp,\n",
      "(2)\n",
      "where fp is the resource budget of flops.\n",
      "the search for eq. 2 can be eﬃciently\n",
      "performed by various algorithms, such as random or evolutionary search [18].\n",
      "afterward, the performance of the searched optimal width d∗ is analyzed by\n",
      "training from scratch.\n",
      "fig.\n",
      "in the\n",
      "ua principle, some channels are trained twice while others are trained only once or\n",
      "not at all, leading to channel training unfairness and evaluation bias.\n",
      "in contrast, our\n",
      "proposed method ensures that all channels are trained evenly (twice) by training both\n",
      "the width d and its complementary width.\n",
      "[19–21] often employ a uni-\n",
      "laterally augmented (ua) principle to evaluate each width, resulting in unfair\n",
      "training of channels in the supernet.\n",
      "3(a), to search for a\n",
      "dimension d at a layer with a maximum of n channels, the ua principle assigns\n",
      "the left d channels in the supernet to indicate the corresponding architecture as\n",
      "γa (d) =\n",
      "the unfairness can be quantiﬁed\n",
      "detection of basal cell carcinoma in whole slide images\n",
      "267\n",
      "fig.\n",
      "this introduces evaluation bias and leads to sub-optimal\n",
      "results.\n",
      "to mitigate evaluation bias on width, we propose a new sc-net that pro-\n",
      "motes the fairness of channels during training.\n",
      "+ 1) : (n − d)],\n",
      "(6)\n",
      "where ⊎ represents the union of two lists with repeatable elements.\n",
      "+ 1\n",
      "(8)\n",
      "therefore, the training degree t for each channel will always be equal to the\n",
      "same constant value of the width, independent of the channel index, ensuring\n",
      "fairness in terms of channel (ﬁlter) levels.\n",
      "however, the search\n",
      "space involved in nas is large, with more than 1020 possible architectures,\n",
      "requiring an evolutionary search using the multi-objective nsga-ii algorithm\n",
      "to improve the search performance.\n",
      "during the evolutionary search, the width d\n",
      "of each network is represented by the average precision of its corresponding left\n",
      "and right paths in the supernet s, as shown in eq.\n",
      "the optimal width (not\n",
      "subnetwork) is determined as the one that achieves the best performance when\n",
      "trained from scratch.\n",
      "acc(w, d, dv) = 1\n",
      "2 (acc(sl, d; dv) + acc(sr, d; dv))\n",
      "(9)\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "experiment settings\n",
      "table 1.\n",
      "generated dataset split\n",
      "bcc-positive bcc-negative\n",
      "training wsi patch\n",
      "wsi patch\n",
      "118\n",
      "132,981 37\n",
      "90,291\n",
      "testing\n",
      "wsi patch\n",
      "wsi patch\n",
      "30\n",
      "31,651\n",
      "9\n",
      "22,838\n",
      "the dataset, comprised of 194 skin slides acquired from the southern sun pathol-\n",
      "ogy laboratory, includes 148 bcc cases and 46 other types (common nevus,\n",
      "detection of basal cell carcinoma in whole slide images\n",
      "269\n",
      "scc), all manually annotated by a dermatopathologist.\n",
      "the experimental setup involved training models\n",
      "on two nvidia rtx a6000 gpus using pytorch.\n",
      "training used the adam optimizer with a\n",
      "dynamic learning rate reduction strategy, starting with a learning rate of 5e-5\n",
      "following a cosine schedule.\n",
      "table 2. performance comparison on skin cancer dataset\n",
      "type\n",
      "model\n",
      "flops\n",
      "parameters\n",
      "acc\n",
      "se\n",
      "sp\n",
      "f1\n",
      "auc\n",
      "wsi analysis-related\n",
      "tian et al.\n",
      "[24]\n",
      "4.1g\n",
      "25.5m\n",
      "93.8\n",
      "92.9\n",
      "92.2\n",
      "93.0\n",
      "93.0\n",
      "ours\n",
      "ori resnet50\n",
      "4.1g\n",
      "25.5m\n",
      "91.4\n",
      "90.2\n",
      "90.4\n",
      "90.5\n",
      "91.8\n",
      "s resnet50\n",
      "4.1g\n",
      "27.2m\n",
      "96.2\n",
      "94.7\n",
      "95.8\n",
      "95.2\n",
      "96.5\n",
      "ori mobilenetv2\n",
      "300m\n",
      "3.5m\n",
      "86.7\n",
      "86.2\n",
      "85.8\n",
      "86.2\n",
      "87.3\n",
      "s mobilenetv2\n",
      "300m\n",
      "4.0m\n",
      "91.9\n",
      "90.4\n",
      "91.7\n",
      "90.7\n",
      "92.4\n",
      "3.2\n",
      "performance evaluation\n",
      "we validated our algorithm using the curated skin cancer dataset and sc-net\n",
      "as a supernet, testing both heavy and light models.\n",
      "to ensure a fair comparison on our\n",
      "dataset, we selected several papers in the ﬁeld of pathological image analysis,\n",
      "such as [9,22,23], as well as others using the ua principle, such as [18,24].\n",
      "evaluation metrics.\n",
      "our model was evaluated on: (1) accuracy (acc): per-\n",
      "centage of correct classiﬁcations.\n",
      "(4)\n",
      "f1 score (f1): precision and recall’s harmonic mean, indicating label alignment.\n",
      "(5) auc: roc curve area, reﬂecting the false/true positive rate trade-oﬀ.\n",
      "as shown in table 2, the s resnet50 model outperformed in all metrics,\n",
      "showing 4.8%, 4.5%, 5.4%, 4.7% and 4.7% improvements in accuracy, sensitivity,\n",
      "speciﬁcity, f1 score, and auc, respectively, over ori resnet50, and surpassing\n",
      "270\n",
      "h. xu et al.\n",
      "table 3. performance of searched models with diﬀerent searching methods.\n",
      "table 3 presents our experiments testing the\n",
      "sc-net on resnet50 and mobilenetv2 using various supernets and search meth-\n",
      "ods.\n",
      "these results high-\n",
      "light sc-net’s eﬀectiveness as a supernet in bolstering evaluation and search\n",
      "performance.\n",
      "as shown in table 4, our s resnet50 surpassed the original\n",
      "resnet50 on all datasets, gaining 2.3% and 1.8% more auc on chestmnist\n",
      "and dermamnist respectively, proving the model’s robust generalization.\n",
      "detection of basal cell carcinoma in whole slide images\n",
      "271\n",
      "table 4. performance comparison on chestmnist and dermamnist datasets\n",
      "model\n",
      "chestmnist dermamnist\n",
      "auc acc\n",
      "auc acc\n",
      "resnet18\n",
      "76.8\n",
      "94.7\n",
      "91.7\n",
      "73.5\n",
      "resnet50\n",
      "76.9\n",
      "94.7\n",
      "91.3\n",
      "73.5\n",
      "auto-sklearn\n",
      "64.9\n",
      "77.9\n",
      "90.2\n",
      "71.9\n",
      "autokeras\n",
      "74.2\n",
      "93.7\n",
      "91.5\n",
      "74.9\n",
      "google automl 77.8\n",
      "94.8\n",
      "91.4\n",
      "76.8\n",
      "s resnet50\n",
      "79.2\n",
      "95.5\n",
      "93.1\n",
      "77.8\n",
      "4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf:\n",
      "however, existing methods suffer from the over-smoothing problem for their\n",
      "commonly used l1 or l2 loss with posterior average calculations.\n",
      "to ensure the accuracy of the\n",
      "prediction, we further design a structure encoder to extract anatomical information\n",
      "from patient anatomy images and enable the noise predictor to be aware of the\n",
      "dose constraints within several essential organs, i.e., the planning target volume\n",
      "and organs at risk.\n",
      "extensive experiments on an in-house dataset with 130 rectum\n",
      "cancer patients demonstrate the superiority of our method.\n",
      "keywords: radiotherapy treatment · dose prediction · diffusion model ·\n",
      "deep\n",
      "learning\n",
      "1\n",
      "introduction\n",
      "radiotherapy, one of the mainstream treatments for cancer patients, has gained notable\n",
      "advancements in past decades.\n",
      "for promising curative effect, a high-quality radiotherapy\n",
      "plan is demanded to distribute sufﬁcient dose of radiation to the planning target volume\n",
      "(ptv) while minimizing the radiation hazard to organs at risk (oars).\n",
      "to achieve this,\n",
      "radiotherapy plans need to be manually adjusted by the dosimetrists in a trial-and-error\n",
      "manner, which is extremely labor-intensive and time-consuming [1, 2].\n",
      "additionally, the\n",
      "quality of treatment plans might be variable among radiologists due to their different\n",
      "expertise and experience [3].\n",
      "https://doi.org/10.1007/978-3-031-43987-2_19\n",
      "192\n",
      "z. feng et al.\n",
      "recently, the blossom of deep learning (dl) has promoted the automatic medi-\n",
      "cal image processing tasks [4–6], especially for dose prediction [7–14].\n",
      "[10] utilized a progressive reﬁnement unet (prunet) to\n",
      "reﬁne the predictions from low resolution to high resolution.\n",
      "[16] to excavate con-\n",
      "textual information from different scales, thus obtaining accuracy improvements in the\n",
      "dose prediction of rectum cancer.\n",
      "[13] designed a multi-organ constraint loss to enforce the deep\n",
      "model to better consider the dose requirements of different organs.\n",
      "[8] utilized isodose line and gradient information to\n",
      "promote the performance of dose prediction of rectum cancer.\n",
      "[17] constructed an additional segmentation task\n",
      "to provide the dose prediction task with essential anatomical knowledge.\n",
      "although the above methods have achieved good performance in predicting dose\n",
      "distribution, they suffer from the over-smoothing problem.\n",
      "[17, 18], leading to the over-smoothed predicted images without important\n",
      "high-frequency details [19].\n",
      "consequently, exploring an automatic\n",
      "method to generate high-quality predictions with rich high-frequency information is\n",
      "important to improve the performance of dose prediction.\n",
      "fig.\n",
      "[20] has veriﬁed its remarkable potential in modeling\n",
      "complex image distributions in some vision tasks [21–23].\n",
      "unlike other dl models, the\n",
      "diffusion model is trained without any extra assumption about target data distribution,\n",
      "thus evading the average effect and alleviating the over-smoothing problem [24]. figure 1\n",
      "(4) provides an example in which the diffusion-based model predicts a dose map with\n",
      "shaper and clearer boundaries of ray-penetrated areas.\n",
      "to further ensure the accuracy of the predicted dose distribution\n",
      "for both the ptv and oars, we design a dl-based structure encoder to extract the\n",
      "anatomical information from the ct image and the segmentation masks of the ptv and\n",
      "oars.\n",
      "(2) we introduce a structure encoder to extract the anatomical information\n",
      "available in the ct images and organ segmentation masks, and exploit the anatomical\n",
      "information to guide the noise predictor in the diffusion model towards generating more\n",
      "precisepredictions.(3)theproposeddiffdpisextensivelyevaluatedonaclinicaldataset\n",
      "consisting of 130 rectum cancer patients, and the results demonstrate that our approach\n",
      "outperforms other state-of-the-art methods.\n",
      "an image set of cancer patient\n",
      "is deﬁned as {x, y}, where x ∈ rh×w×(2+o) represents the structure images, “2” signiﬁes\n",
      "the ct image and the segmentation mask of the ptv, and o denotes the total number\n",
      "of segmentation mask of oars.\n",
      "meanwhile, y ∈ rh×w×1 is the corresponding dose\n",
      "distribution map for x. concretely, the forward process produces a sequence of noisy\n",
      "images {y0, y1, . .\n",
      "to obtain the anatomic information, a structure encoder\n",
      "g is designed to extract the crucial feature representations from the structure images.\n",
      "in the forward process, the diffdp model employs the markov chain\n",
      "to progressively add noise to the initial dose distribution map y0 ∼ q(y0) until the\n",
      "ﬁnal disturbed image yt becomes completely gaussian noise which is represented as\n",
      "yt ∼ n(yt | 0, i).\n",
      "based on this, we can directly obtain\n",
      "the distribution of yt at any step t from y0 through the following formula:\n",
      "q(yt | y0) = n\n",
      "\u0002\n",
      "yt; √γty0, (1 − γt)i\n",
      "\u0003\n",
      ",\n",
      "(3)\n",
      "where the disturbed image yt is sampled using:\n",
      "the reverse process also harnesses the markov chain to progressively\n",
      "convert the latent variable distribution pθ(yt) into distribution pθ(y0) parameterized by\n",
      "θ. corresponding to the forward process, the reverse one is a denoising transformation\n",
      "under the guidance of structure images x that begins with a standard gaussian distribution\n",
      "yt ∼ n(yt | 0, i).\n",
      "to address this, we design a structure encoder g\n",
      "that effectively extracts the anatomical information from the structure images guiding\n",
      "the noise predictor to generate more accurate dose maps by incorporating extracted\n",
      "structural knowledge.\n",
      "the residual connections are reserved for preventing gradient vanishment in the training.\n",
      "it takes structure image x\n",
      "as input, which includes the ct image and segmentation masks of ptv and oars, and\n",
      "evacuates the compact feature representation in different levels to improve the accuracy\n",
      "of dose prediction.\n",
      "196\n",
      "z. feng et al.\n",
      "2.3\n",
      "noise predictor\n",
      "the purpose of the noise predictor f (xe, yt, γt) is to predict the noise added on the\n",
      "distribution map yt with the guidance of the feature representation xe extracted from the\n",
      "structure images x and current noise intensity γt in each step t. inspired by the great\n",
      "achievements of unet\n",
      "in the encoding procedure, to guide the noise predictor with essential anatomical\n",
      "structure, the feature representations respectively extracted from the structure images x\n",
      "and noisy image yt are simultaneously fed into the noise predictor.\n",
      "then, these two feature maps are fused\n",
      "by element-wise addition, allowing the structure information in x to be transferred to the\n",
      "noise predictor.\n",
      "algorithm 1: training procedure\n",
      "1: input: input image pairs\n",
      "where \n",
      "is the structure image and \n",
      "is the cor-\n",
      "responding dose distribution map, the total number of diffusion steps \n",
      "2: initialize: randomly initialize the noise predictor \n",
      "and pre-trained structure encoder\n",
      "3: repeat\n",
      "4:      sample \n",
      "5:      sample \n",
      "6:      perform the gradient step on equation (9)\n",
      "7: until converged\n",
      "diffdp: radiotherapy dose prediction via a diffusion model\n",
      "197\n",
      "2.5\n",
      "training details\n",
      "we accomplish the proposed network in the pytorch framework.\n",
      "all of our experiments\n",
      "are conducted through one nvidia rtx 3090 gpu with 24 gb memory and a batch\n",
      "size of 16 with an adaptive moment estimation (adam) optimizer.\n",
      "additionally,\n",
      "the noise intensity is initialized to 1e−2 and decayed to 1e−4 linearly along with the\n",
      "increase of steps.\n",
      "3\n",
      "experiments and results\n",
      "dataset and evaluations.\n",
      "we measure the performance of our model on an in-house\n",
      "rectum cancer dataset which contains 130 patients who underwent volumetric modulated\n",
      "arc therapy (vmat) treatment at west china hospital.\n",
      "concretely, for every patient, the\n",
      "ct images, ptv segmentation, oars segmentations, and the clinically planned dose\n",
      "distribution are included.\n",
      "the thickness of the cts is 3 mm and all the images are resized to the resolution\n",
      "of 256 × 256 before the training procedure.\n",
      "we measure the performance of our proposed model with multiple metrics.\n",
      "con-\n",
      "sidering dm represents the minimal absorbed dose covering m% percentage volume of\n",
      "ptv, we involve d98, d2, maximum dose (dmax), and mean dose (dmean) as metrics.\n",
      "to\n",
      "quantify performance more directly, we calculate the difference ) of these metrics\n",
      "between the ground truth and the predicted results.\n",
      "[27] as another essential metric of dose prediction perfor-\n",
      "mance.\n",
      "as ford2 anddmax, our method gains overwhelming performance with\n",
      "0.0008 and 0.0005, respectively.\n",
      "the p-values between the proposed and other sotas are\n",
      "almost all less than 0.05, indicating that the enhancement of performance is statistically\n",
      "meaningful.\n",
      "diffdp: radiotherapy dose prediction via a diffusion model\n",
      "199\n",
      "our method and the ground truth is the smallest, demonstrating the superior performance\n",
      "of the proposed.\n",
      "to study the contributions of key components of the proposed method,\n",
      "we conduct the ablation experiments by 1) removing the structure encoder from the\n",
      "proposed method and concatenating the anatomical images x and noisy image yt together\n",
      "as the original input for diffusion model (denoted as baseline); 2) the proposed diffdp\n",
      "model.\n",
      "we can clearly see the performance\n",
      "for all metrics is enhanced with the structure encoder, demonstrating its effectiveness in\n",
      "the proposed model.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf:\n",
      "cervical cancer is a signiﬁcant health burden worldwide, and\n",
      "computer-aided diagnosis (cad) pipelines have the potential to improve\n",
      "diagnosis eﬃciency and treatment outcomes.\n",
      "however, traditional cad\n",
      "pipelines have limitations due to the requirement of a detection model\n",
      "trained on a large annotated dataset, which can be expensive and time-\n",
      "consuming.\n",
      "they also have a clear performance limit and low data uti-\n",
      "lization eﬃciency.\n",
      "to address these issues, we introduce a two-stage\n",
      "detection-free pipeline, incorporating pooling transformer and moco pre-\n",
      "training strategies, that optimizes data utilization for whole slide images\n",
      "(wsis) while relying solely on sample-level diagnosis labels for training.\n",
      "the experimental results demonstrate the eﬀectiveness of our approach,\n",
      "with performance scaling up as the amount of data increases.\n",
      "overall, our\n",
      "novel pipeline has the potential to fully utilize massive data in wsi classi-\n",
      "ﬁcation and can signiﬁcantly improve cancer diagnosis and treatment.\n",
      "by\n",
      "reducing the reliance on expensive data labeling and detection models,\n",
      "our approach could enable more widespread and cost-eﬀective implemen-\n",
      "tation of cad pipelines in clinical settings.\n",
      "keywords: detection-free · contrastive learning · pathology image\n",
      "classiﬁcation · cervical cancer\n",
      "1\n",
      "introduction\n",
      "cervical cancer is a common and severe disease that aﬀects millions of women\n",
      "globally, particularly in developing countries [9].\n",
      "early diagnosis is vital for suc-\n",
      "cessful treatment, which can signiﬁcantly increase the cure rate [17].\n",
      "several computer-aided cervical cancer screening methods have been pro-\n",
      "posed for whole slide images (wsis) in the literature.\n",
      "in the second step, the patches\n",
      "centered on these detected cells are processed through a classiﬁcation model, to\n",
      "reﬁne the judgment of whether they are positive or negative.\n",
      "some methods improve the ﬁnal classiﬁcation performance by improving the\n",
      "detection model to identify positive cells more reliably.\n",
      "[1] improved\n",
      "the detection performance by incorporating clinical knowledge and attention\n",
      "mechanism into their cell detection model of attfpn.\n",
      "other methods improve the classiﬁcation per-\n",
      "formance by changing the post-processing modules behind the detection model.\n",
      "[5] proposed a progressive identiﬁcation method that leveraged\n",
      "multi-scale visual cues to identify abnormal cells and then an rnn [27] for\n",
      "sample-level classiﬁcation.\n",
      "these methods have achieved good results through continuous improvement\n",
      "on the detection-based pipeline, but there are some common drawbacks.\n",
      "a lot of data would be wasted if only a small part\n",
      "of annotated images (e.g., corresponding to positive cells and bounding boxes)\n",
      "was used as training data.\n",
      "finally, many existing methods focus on detecting\n",
      "and classifying individual cells.\n",
      "the tendency to neglect eﬀective integration of\n",
      "the overall information across the entire wsi results in poor performance in\n",
      "sample-level classiﬁcation.\n",
      "to address the aforementioned issues, we propose a detection-free pipeline\n",
      "in this paper, which does not rely on any detection model.\n",
      "instead, our pipeline\n",
      "requires only sample-level diagnosis labels, which are naturally available in clin-\n",
      "ical scenarios and thus get rid of additional image labeling.\n",
      "to attain this goal,\n",
      "we have designed a two-stage pipeline as in fig.\n",
      "1. in the coarse-grained stage,\n",
      "we crop and downsample a wsi into multiple images, and conduct sample-level\n",
      "classiﬁcation roughly based on all resized images.\n",
      "then, in the ﬁne-grained\n",
      "stage, we use these key patches for ﬁne prediction of the sample.\n",
      "the two stages\n",
      "in our pipeline adopt the same network design (i.e., encoder + pooling trans-\n",
      "detection-free pipeline\n",
      "245\n",
      "former), which makes our solution friendly to develop and to use.\n",
      "as a summary, our pipeline surpasses pre-\n",
      "vious detection-based methods and achieves state-of-the-art performance with\n",
      "large-scale training.\n",
      "our experiments show that our method becomes more eﬀec-\n",
      "tive when increasing the data size for training.\n",
      "moreover, while many patholog-\n",
      "ical images are also based on wsis, our pipeline has a high potential to extend\n",
      "to other pathological tasks.\n",
      "for feasibility of computation, we crop\n",
      "a wsi into mutiple images.\n",
      "the cropped images are passed through the coarse-grained\n",
      "and ﬁne-grained stages, where only sample-level diagnosis labels of wsis, instead of\n",
      "any additional manual labeling, are required for training.\n",
      "two-stage pipeline with attention guided selection.\n",
      "the overview of\n",
      "our two-stage pipeline is shown in fig.\n",
      "[18], so we crop each\n",
      "wsi into local images sized 1024 × 1024 pixels.\n",
      "the images are then processed\n",
      "through the coarse-grained and ﬁne-grained stages in order to obtain the wsi-\n",
      "level classiﬁcation results, respectively.\n",
      "in general, the purpose of the coarse-\n",
      "grained stage is to replace the detection model and identify local images that\n",
      "may contain abnormal positive cells.\n",
      "the ﬁne-grained stage then integrates these\n",
      "key regions, producing reﬁned classiﬁcation for the sample.\n",
      "to complete sample-level classiﬁcation, both stages share basically the same\n",
      "network architecture.\n",
      "the input images are ﬁrst processed by a cnn encoder to\n",
      "246\n",
      "m. cao et al.\n",
      "extract features.\n",
      "additionally, the input images for both stages are 256 × 256.\n",
      "in the coarse-grained stage, in order to allow the model to examine as many\n",
      "local images as possible, we resize the cropped local images from 1024 × 1024 to\n",
      "256 × 256.\n",
      "in the ﬁne-grained stage, we enlarge suspicious local abnormality and\n",
      "thus crop input images to 256 × 256 from 1024 × 1024.\n",
      "for the coarse-grained stage, after passing the resized local images through\n",
      "encoder and pooling transformer, we obtain a rough prediction result at the\n",
      "sample level.\n",
      "in addition, we calculate\n",
      "the attention score to identify the local image inputs that are most likely to yield\n",
      "positive reading.\n",
      "f t\n",
      "\u0002\n",
      "dx0\n",
      ")f,\n",
      "(1)\n",
      "where x0 represents classiﬁcation token (which is a commonly used setting in\n",
      "transformer [7,22]), and dx0 is 512 in our implementation\n",
      ", f represents the fea-\n",
      "ture vector of a certain input local image.\n",
      "after calculating attention scores, we\n",
      "preserve top-8 (resized) local images with the highest scores from the entire wsi\n",
      "for subsequent ﬁne-grained classiﬁcation.\n",
      "next, in the ﬁne-grained stage, each local image that has passed attention\n",
      "guided selection is cropped into 16 patches of the size 256 × 256.\n",
      "the network of the ﬁne-grained stage is the same as that of\n",
      "the coarse-grained stage, but the weights of the encoder is pre-trained in an\n",
      "unsupervised manner (sect. 2).\n",
      "the same ce loss supervised by sample-level\n",
      "ground truth is used for the ﬁne-grained stage here.\n",
      "for inference, the output of\n",
      "the ﬁne-grained stage will be treated as the ﬁnal result of the test wsi.\n",
      "fig.\n",
      "we use a transformer network to aggregate features of\n",
      "multiple inputs and to derive the sample-level outcome in both coarse-grained\n",
      "and ﬁne-grained stages.\n",
      "we have observed that diﬀerent local images of the same\n",
      "sample often have patterns of grouped similarity (such as the ﬁrst two images\n",
      "in the upper-right of fig.\n",
      "for negative samples, most of the local images are\n",
      "similar with each other.\n",
      "for positive samples, the images of abnormal cells are\n",
      "inclined to be grouped into several clusters.\n",
      "therefore, inspired by [2,14], we propose pooling transformer that is eﬀec-\n",
      "tive to reduce the redundancy and distortion from the input images.\n",
      "within each clustered class, we average the features and aggregate a sin-\n",
      "gle token.\n",
      "[25,26] pre-training on imagenet\n",
      "[6], we also perform pre-\n",
      "training for ﬁne-grained encoder on a large scale of pathology images.\n",
      "for data, wsi naturally has the advantage of having a large amount\n",
      "of training data.\n",
      "therefore, we only need 2,000–3,000 wsi samples to obtain\n",
      "a dataset that can even be compared to imagenet in quantity.\n",
      "in our task, since the structural features of cells are relatively\n",
      "weak compared to natural images, it is not suitable to model the loss function\n",
      "using masks.\n",
      "speciﬁcally, in the same training batch, a patch (256 × 256, the same to the\n",
      "input size of the ﬁne-grained stage) and its augmented patch are treated as a\n",
      "positive pair (note that here “positive/negative” is deﬁned in the context of\n",
      "contrastive learning), and their features are required to be as similar as possible.\n",
      "using this method, we can pre-train a feature encoder in an\n",
      "unsupervised manner and initialize it into our encoder for the ﬁne-grained stage.\n",
      "248\n",
      "m. cao et al.\n",
      "3\n",
      "experiment and results\n",
      "dataset and experimental setup.\n",
      "we conduct the\n",
      "experiment with initial learning rate of 1.0 × 10−4, batch size of 4, and sgd\n",
      "optimizer [21] for 30 epochs each stage.\n",
      "in this section, we experiment to compare\n",
      "our method with popular state-of-the-art (sota) methods, which are all fully\n",
      "supervised and detection-based.\n",
      "the detection dataset has 3761 images and 7623 cell-level annotations.\n",
      "method\n",
      "accuracy↑\n",
      "precision↑\n",
      "recall↑\n",
      "f1-score↑\n",
      "attfpn+average [1]\n",
      "78.33 ± 1.51\n",
      "71.23 ± 2.10\n",
      "79.39 ± 1.56\n",
      "74.10 ± 2.33\n",
      "while our method has a large margin\n",
      "with most methods in the table, the improvement against [28] (top-ranked in\n",
      "current detection-based methods) is relatively limited.\n",
      "in this section, we experiment to demonstrate the eﬀective-\n",
      "ness of all the proposed parts in our pipeline.\n",
      "here, cg means the classiﬁcation passes only the coarse-grained stage.\n",
      "as can be seen, its performance is low, in that the resized images sacriﬁces the\n",
      "resolution and thus perform poorly for image-based classiﬁcation.\n",
      "fg refers to\n",
      "classifying in the ﬁne-grained stage.\n",
      "it is worth noting that without the atten-\n",
      "tion scores provided by the coarse-grained stage, we have no way of knowing\n",
      "which local images might contain suspicious positive cells.\n",
      "thus, we use random\n",
      "selection to experiment for fg only, as exhaustively checking all local images is\n",
      "computationally forbidden.\n",
      "as can be seen, the classiﬁcation result is the lowest\n",
      "because it lacks enough access to the key image content in wsis.\n",
      "conﬁguration\n",
      "metric (%)\n",
      "cg fg pt cl acc\n",
      "precision\n",
      "recall\n",
      "f1 score\n",
      "✔\n",
      "–\n",
      "–\n",
      "–\n",
      "74.96 ± 1.23 71.39 ± 1.21 82.49 ± 1.39 75.34 ± 1.66\n",
      "–\n",
      "✔\n",
      "–\n",
      "–\n",
      "73.11 ± 2.10 70.48 ± 1.45 81.59 ± 1.45 74.09 ± 1.49\n",
      "✔\n",
      "✔\n",
      "–\n",
      "–\n",
      "79.72 ± 1.30 74.64 ± 1.34 84.45 ± 1.95 78.48 ± 1.23\n",
      "✔\n",
      "✔\n",
      "✔\n",
      "–\n",
      "81.34 ± 1.56 77.36 ± 1.23 84.79 ± 0.98 80.72 ± 0.93\n",
      "✔\n",
      "✔\n",
      "✔\n",
      "✔\n",
      "83.84 ± 1.56 78.36 ± 1.23 85.22 ± 0.98 82.12 ± 0.93\n",
      "by combining the two stages for attention guided selection, it is eﬀective\n",
      "to improve the classiﬁcation performance compared to the two previous exper-\n",
      "iments.\n",
      "ultimately, we combine them together\n",
      "to achieve the best performance.\n",
      "for the experiment of sample numbers, we compare the best\n",
      "fully supervised detection-based method (retinanet+gat [28]) with ours under\n",
      "the sample numbers of 500, 1000, 2000, and 5384.\n",
      "although our method\n",
      "initially has poorer performance, it has shown an impressive growth trend.\n",
      "and\n",
      "at our current maximum data number (5384), the proposed pipeline has already\n",
      "exceeded the performance of the detection-based method.\n",
      "the above results also\n",
      "demonstrate that our new pipeline method has greater potential, even though it\n",
      "requires no cell-level image annotation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf:\n",
      "in medical image analysis, imbalanced noisy dataset classiﬁ-\n",
      "cation poses a long-standing and critical problem since clinical large-\n",
      "scale datasets often attain noisy labels and imbalanced distributions\n",
      "through annotation and collection.\n",
      "additionally, the factor of class hardness hinder-\n",
      "ing label noise removal remains undiscovered, causing a critical neces-\n",
      "sity for an approach to enhance the classiﬁcation performance of noisy\n",
      "imbalanced medical datasets with various class hardness.\n",
      "to address this\n",
      "paradox, we propose a robust classiﬁer that trains on a multi-stage noise\n",
      "removal framework, which jointly rectiﬁes the adverse eﬀects of label\n",
      "noise, imbalanced distribution, and class hardness.\n",
      "multi-environment risk\n",
      "minimization (mer) strategy captures data-to-label causal features for\n",
      "noise identiﬁcation, and the rescaling class-aware gaussian mixture\n",
      "modeling (rcgm) learns class-invariant detection mappings for noise\n",
      "removal.\n",
      "extensive experiments on two imbalanced noisy clinical datasets\n",
      "demonstrate the capability and potential of our method for boosting the\n",
      "performance of medical image classiﬁcation.\n",
      "keywords: imbalanced data · noisy labels · medical image analysis\n",
      "1\n",
      "introduction\n",
      "image classiﬁcation is a signiﬁcant challenge in medical image analysis.\n",
      "although\n",
      "some classiﬁcation methods achieve promising performance on balanced and\n",
      "clean medical datasets, balanced datasets with high-accuracy annotations are\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43987-2 30.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43987-2_30\n",
      "learning robust medical image classiﬁer by minimizing invariant risk\n",
      "307\n",
      "fig.\n",
      "existing approaches for non-ideal medical image classiﬁcation can be sum-\n",
      "marized into noisy classiﬁcation, imbalanced recognition, and noisy imbalanced\n",
      "identiﬁcation.\n",
      "imbalanced recognition approaches [9,15,21] utilize\n",
      "augmented embeddings and imbalance-invariant training loss to re-balance the\n",
      "long-tailed medical data artiﬁcially, but the disturbance from noisy labels leads\n",
      "to uncasual feature learning, impeding the recognition of tail classes.\n",
      "in this work, we propose a multi-stage noise removal framework to address\n",
      "these concerns jointly.\n",
      "the main contributions of our work include: 1) we decom-\n",
      "pose the negative eﬀects in practical medical image classiﬁcation, 2) we minimize\n",
      "308\n",
      "j. li et al.\n",
      "the invariant risk to tackle noise identiﬁcation inﬂuenced by multiple factors,\n",
      "enabling the classiﬁer to learn causal features and be distribution-invariant, 3)\n",
      "a re-scaling class-aware gaussian mixture modeling (cgmm) approach is pro-\n",
      "posed to distinguish noise labels under various class hardness, 4) we evaluate our\n",
      "method on two medical image datasets, and conduct thorough ablation studies\n",
      "to demonstrate our approach’s eﬀectiveness.\n",
      "further, we denote the backbone\n",
      "as h(·; θ), x → z mapping data manifold to the latent manifold, the classiﬁer\n",
      "head as g(·; γ), z → c linking latent space to the category logit space, and the\n",
      "identiﬁer as f(·; φ), z → c. we aim to train a robust medical image classiﬁca-\n",
      "tion model composed of a representation backbone and a classiﬁer head on label\n",
      "noise and imbalance distribution, resulting in a minimized loss on the testing\n",
      "dataset:\n",
      "given that backbone mapping is independent\n",
      "of noisy imbalanced eﬀects, we conduct further disentanglement by deﬁning e as\n",
      "the negative eﬀects and p as constant for ﬁxed probability mappings:\n",
      "p(y = c|x, e) = ph(z|x) · pg(y = c|z, e)\n",
      "= ph(z|x) · {pg(y = c|z) ·\n",
      "by bayes theorem, we decompose the eﬀect into imbalance,\n",
      "learning robust medical image classiﬁer by minimizing invariant risk\n",
      "309\n",
      "fig.\n",
      "noise identiﬁer f is optimized across three constructed environments with h ﬁxed\n",
      "during mer.\n",
      "furthermore, the\n",
      "impact of hardness eﬀects has not been considered in previous studies, which\n",
      "adds an extra dimension to noise removal.\n",
      "in essence, the fundamental idea\n",
      "of noisy classiﬁcation involves utilizing clean data for classiﬁer training, which\n",
      "determines the importance of noise identiﬁcation and removal.\n",
      "however, they fail to consider the inﬂuence of imbalanced distri-\n",
      "butions, which might cause a biased gradient direction on the optimization sub-\n",
      "space.\n",
      "following [25], we minimize the invariant risk [2] across multi-environment\n",
      "for independent detector learning.\n",
      "by assuming that the robust classiﬁer per-\n",
      "forms well on every data distribution, we solve the optimizing object by ﬁnding\n",
      "the optima to reduce the averaged distance for gradient shift:\n",
      "310\n",
      "j. li et al.\n",
      "min\n",
      "hθ:x→z\n",
      "fφ:z→y\n",
      "\u0002\n",
      "ε∈etr\n",
      "l(fφ ◦ hθ)\n",
      "s.t.\n",
      "∈ etr,\n",
      "(3)\n",
      "where ε represents an environment (distribution) for classiﬁer fφ and backbone\n",
      "hθ; and l denotes the empirical loss for classiﬁcation.\n",
      "in practice, all environments\n",
      "are established from the training set with the same class categories.\n",
      "from the perspective of clustering, deﬁnite and immense\n",
      "gaps between two congregate groups contribute to more accurate decisions.\n",
      "how-\n",
      "ever, in medical image analysis, an overlooked mismatch exists between class\n",
      "hardness and diﬃculty in noise identiﬁcation.\n",
      "=\n",
      "\u0002\n",
      "k∈{c,n}\n",
      "αikpk\n",
      "i\n",
      "\n",
      "qij | μk\n",
      "i , σk\n",
      "i\n",
      "\u000b\n",
      ",\n",
      "(5)\n",
      "which produces more accurate and independent measurements of label quality.\n",
      "learning robust medical image classiﬁer by minimizing invariant risk\n",
      "311\n",
      "instead of assigning a hard label to the potential noisy data as [8] which also\n",
      "employs a class-speciﬁc gmm to cluster the uncertainty, we further re-scale the\n",
      "conﬁdence score of class-wise noisy data.\n",
      "τ\n",
      "(7)\n",
      "2.5\n",
      "overall learning framework for imbalanced and noisy data\n",
      "in contrast to two-stage noise removal and imbalance classiﬁcation techniques,\n",
      "our approach applies a multi-stage protocol: warm-up phases, noise removal\n",
      "phases, and ﬁne-tuning phases as shown in fig.\n",
      "2. in the warm-up stage, we train\n",
      "backbone h and classiﬁer g\n",
      "a few epochs by assuming that g only remembers\n",
      "clean images with less empirical loss.\n",
      "sqrt sampler is applied to re-balance the\n",
      "data, and cross-stage kl\n",
      "3\n",
      "experiment\n",
      "3.1\n",
      "dataset and evaluation metric\n",
      "we evaluated our approach on two medical image datasets with imbalanced class\n",
      "distributions and noisy labels.\n",
      "[22], is a dermato-\n",
      "scopic image dataset for skin-lesion classiﬁcation with 10,015 images divided into\n",
      "seven categories.\n",
      "it contains a training set with 7,007 images, a validation set\n",
      "with 1,003 images, and a testing set with 2,005 images.\n",
      "[29], is a histopathology image\n",
      "dataset manually annotated into four cancer categories by three pathological\n",
      "312\n",
      "j. li et al.\n",
      "table 1.\n",
      "the second-best performances are underlined.\n",
      "consequently, chaoyang dataset\n",
      "consists of a training set with 2,181 images, a validation set with 713 images,\n",
      "and a testing set with 1,426 images, where the validation and testing sets have\n",
      "clean labels.\n",
      "the evaluation metrics are macro-f1, b-acc, and mcc.\n",
      "3.2\n",
      "implementation details\n",
      "we mainly follow the training settings of fcd\n",
      "resnet-18 pretrained on the\n",
      "imagenet is the backbone.\n",
      "learning rates are 0.06, 0.001,\n",
      "0.06 and 0.006 with the cosine schedule for four stages, respectively.\n",
      "the size of input image is 224 × 224.\n",
      "the scale and threshold in rcgm\n",
      "are 0.6 and 0.1, respectively.\n",
      "learning robust medical image classiﬁer by minimizing invariant risk\n",
      "313\n",
      "3.3\n",
      "comparison with state-of-the-art methods\n",
      "we compare our model with state-of-the-art methods which contain noisy meth-\n",
      "ods (including dividemix [13], nl\n",
      "we train all approaches under the same data augmenta-\n",
      "tions and network architecture.\n",
      "in noisy methods, nl and gce also suﬀer great\n",
      "performance declines.\n",
      "we mix these weakly-performed approaches with meth-\n",
      "ods from the other category, observing the accuracy improvement.\n",
      "our framework achieves improvements in all metrics on both datasets,\n",
      "demonstrating the rationality of the assumption and the eﬀectiveness of our\n",
      "framework.\n",
      "fig.\n",
      "(a) and (b) quantitative performance comparison of diﬀerent\n",
      "components of our method on ham10000 and chaoyang datasets, respectively.\n",
      "3, we evaluate the eﬀectiveness of the components in our\n",
      "method by decomposing them on extensive experiments.\n",
      "we choose the ﬁrst\n",
      "stage of fcd\n",
      "figure 3a and 3b show that only using mer\n",
      "or rcgm achieves better performance than our strong baseline on both datasets.\n",
      "for example, mer achieves 5.37% and 1.15% improvements on ham10000 and\n",
      "chaoyang, respectively, demonstrating the eﬀectiveness of our noise removal\n",
      "314\n",
      "j. li et al.\n",
      "techniques.\n",
      "further, our multi-stage noise removal technique outperforms single\n",
      "mer and rcgm, revealing that the decomposition for noise eﬀect and hard-\n",
      "ness eﬀect works on noisy imbalanced datasets.\n",
      "furthermore, similar performance trends reveal the robustness of scale s.\n",
      "4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_15.pdf:\n",
      "however, existing classiﬁcation approaches rely on\n",
      "generic radiomic features that may not be optimal for the task, whilst\n",
      "deep learning methods tend to over-ﬁt to the high-dimensional vol-\n",
      "ume inputs.\n",
      "in this work, we propose a novel radiomics-informed deep-\n",
      "learning method, ridl, that combines the advantages of deep learning\n",
      "and radiomic approaches to improve af sub-type classiﬁcation.\n",
      "unlike\n",
      "existing hybrid techniques that mostly rely on na¨ıve feature concatena-\n",
      "tion, we observe that radiomic feature selection methods can serve as\n",
      "an information prior, and propose supplementing low-level deep neural\n",
      "network (dnn) features with locally computed radiomic features.\n",
      "furthermore, we ensure complementary\n",
      "information is learned by deep and radiomic features by designing a\n",
      "novel feature de-correlation loss.\n",
      "the disease can lead to stroke and heart failure, and has a mortal-\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43990-2 15.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "[8]. accurate knowledge of the disease type is there-\n",
      "fore highly valuable for treatment planning and has high prognostic value [22].\n",
      "[7] showed that eat volume,\n",
      "approximated from left-atrium ct images, can be used as a predictor for af\n",
      "recurrence.\n",
      "[22] trained a random forest model to classify af sub-\n",
      "type based on radiomic features and volume measurements, achieving 85.3%\n",
      "auc.\n",
      "although these methods demonstrate the usefulness of radiomic features\n",
      "for af sub-type classiﬁcation, such features are generic and not speciﬁc to the\n",
      "task, which can limit model performance [12].\n",
      "na¨ıvely using deep neural networks (dnns) to predict\n",
      "af sub-types from ct volumes yields poor results however due to over-ﬁtting\n",
      "on high-dimensional volume inputs (see results for dnn in table 1).\n",
      "although these methods propose diﬀerent ways\n",
      "of using both approaches, they do not explicitly address the limitations of either\n",
      "approach or explore ways to combine their complementary advantages.\n",
      "we note that textural radiomic features identiﬁed by feature selection methods\n",
      "can serve as an information prior to supplement low-level features from dnns,\n",
      "since they are designed to capture low-level context and have predictive power\n",
      "[23].\n",
      "furthermore, we encourage the dnn to learn features\n",
      "complementary to radiomic features to obtain more comprehensive signals and\n",
      "design a novel feature de-correlation loss.\n",
      "1. unlike exist-\n",
      "ing works, our method is designed to directly addresses the limitations of both\n",
      "deep learning and radiomic approaches and achieves state-of-the-art performance\n",
      "on af sub-type classiﬁcation.\n",
      "our method also\n",
      "fuses locally computed radiomic features with low-level dnn features and encourages\n",
      "complementary deep and radiomic features to be learned.\n",
      "– furthermore, we enforce feature de-correlation using a novel feature-bank\n",
      "design to ensure complementary deep and radiomic features are extracted.\n",
      "2\n",
      "methodology\n",
      "we combine radiomic and deep learning approaches using two novel components:\n",
      "1) feature fusion of local radiomic features and low-level dnn features to improve\n",
      "local context, 2) encouraging complementary deep and radiomic features through\n",
      "feature de-correlation.\n",
      "(b) feature bank implementation for feature de-\n",
      "correlation.\n",
      "local features can be calculated for multiple texture features and patch\n",
      "size p, which are then concatenated to obtain rl\n",
      "i ∈ rl×h×w ×d, where l is the\n",
      "total number of features used and h, w, and d are original input dimensions.\n",
      "we note that only texture radiomic features are used for local calculation since\n",
      "they are speciﬁcally intended to capture local context.\n",
      "radiomics-informed deep learning (ridl)\n",
      "157\n",
      "rl\n",
      "i is then concatenated with low-level dnn features, zi ∈ rc×h×w ×d,\n",
      "to supplement the dnn with local radiomic features.\n",
      "z′\n",
      "i = a(rl\n",
      "i ⊕ zi) ⊗ (rl\n",
      "i ⊕ zi)\n",
      "(3)\n",
      "where z′\n",
      "i is the fused feature, ⊕ is channel concatenation, and ⊗ is element-wise\n",
      "multiplication.\n",
      "the learned attention tensor a(rl\n",
      "i ⊕zi) has dimensions (c +l)×\n",
      "1 × 1 × 1 and is broadcasted along the volume dimension, such that attention is\n",
      "applied channel-wise and spatial feature distributions are preserved.\n",
      "2.2\n",
      "encouraging complementary deep and radiomic features\n",
      "through feature de-correlation\n",
      "global radiomic features are also included in our model by concatenation\n",
      "with high-level dnn features before the classiﬁcation layer.\n",
      "unlike existing\n",
      "approaches however, we encourage our dnn to learn features complementary\n",
      "to radiomic features by enforcing de-correlation between the two.\n",
      "we\n",
      "instead propose a novel feature-bank implementation with exponential weighting\n",
      "to estimate sample statistics.\n",
      "(4)\n",
      "the ﬁrst b samples, where b is the batch size, belong to the training sample\n",
      "of the current iteration, and their losses are back-propagated to encourage deep\n",
      "features to have zero correlation with radiomic features.\n",
      "to provide\n",
      "further regularization and prevent over-ﬁtting, we perform an additional self-\n",
      "reconstruction task, using loss lrec, which we describe in more detail in the\n",
      "supplementary materials.\n",
      "(6)\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "implementation details\n",
      "dataset.\n",
      "volumes are resized to the same aspect ratio to ensure consistent dimensions\n",
      "across samples.\n",
      "we use ﬁve-fold cross-validation and report average\n",
      "test performance across folds.\n",
      "cross-validation is implemented by splitting the\n",
      "dataset into ﬁve equal subsets and using three subsets for training, one subset\n",
      "for validation, and one subset for testing.\n",
      "data\n",
      "acquisition procedures and statistics are given in the supplementary materials.\n",
      "setup.\n",
      "we use the pyradiomic package\n",
      "bottle-neck features are averaged\n",
      "across spatial dimensions for classiﬁcation, whilst decoder outputs are used for\n",
      "self-reconstruction regularization.\n",
      "additional experiments and details are included in the supplementary materials.\n",
      "radiomics-informed deep learning (ridl)\n",
      "159\n",
      "3.2\n",
      "comparison with state-of-the-art methodologies\n",
      "we compare our method with alternative state-of-the-art approaches based on\n",
      "radiomics, deep learning, and hybrid techniques.\n",
      "dnn⋆ is a na¨ıve implementation using the m3dunet model.\n",
      "baseline† is a na¨ıve hybrid\n",
      "implementation using simple feature concatenation.\n",
      "we perform ablation experiments to demonstrate\n",
      "improvements from using local radiomic features, global radiomic features, and\n",
      "feature de-correlation loss.\n",
      "using feature\n",
      "de-correlation further boosts performance and leads to the best overall results.\n",
      "dnn⋆ is a na¨ıve implementation using the m3dunet model.\n",
      "baseline† is a na¨ıve hybrid\n",
      "implementation using simple feature concatenation.\n",
      "feature used for local calculation\n",
      "selected\n",
      "auc (%)\n",
      "map (%)\n",
      "gldm dependencenonuniformitynormalized\n",
      "✗\n",
      "86.1 ± 0.8\n",
      "85.5 ± 0.9\n",
      "glrlm longrunemphasis\n",
      "✗\n",
      "86.4 ± 0.8\n",
      "85.4 ± 0.8\n",
      "gldm largedependenceemphasis\n",
      "✗\n",
      "85.6 ± 0.7\n",
      "84.6 ± 0.9\n",
      "glcm idn (ours)\n",
      "✓\n",
      "86.9 ± 0.6\n",
      "86.3 ± 0.6\n",
      "we can see that using discarded features leads to worse performance in gen-\n",
      "eral.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf:\n",
      "drawing on the capabil-\n",
      "ity of the contrastive language-image pre-training (clip) model to learn\n",
      "generalized visual representations from text annotations, in this paper,\n",
      "we propose clip-lung, a textual knowledge-guided framework for lung\n",
      "nodule malignancy prediction.\n",
      "third, we align image features with both class and attribute\n",
      "features via contrastive learning, rectifying false positives and false nega-\n",
      "tives in latent space.\n",
      "experimental results on the benchmark lidc-idri\n",
      "dataset demonstrate the superiority of clip-lung, in both classiﬁcation\n",
      "performance and interpretability of attention maps.\n",
      "keywords: lung nodule classiﬁcation · vision-language model ·\n",
      "usually, the malignancy predic-\n",
      "tion is often formulated as benign-malignant binary classiﬁcation [9,10,19], and\n",
      "the higher classiﬁcation performance and explainable attention maps are impres-\n",
      "sive.\n",
      "indeed, the ordinal regression-\n",
      "based methods are able to learn ordered manifolds and to further enhance the\n",
      "prediction accuracy.\n",
      "however, the aforementioned methods still face challenges in distinguishing\n",
      "visually similar samples with adjacent rank labels.\n",
      "1,\n",
      "this text information is beneﬁcial for distinguishing visually similar pairs, while we\n",
      "conduct this behavior by applying contrastive learning that pulls semantic-closer\n",
      "samples and pushes away semantic-farther ones.\n",
      "to integrate text annotations into the image-domain learning process, an\n",
      "eﬀective text encoder providing accurate textual features is required.\n",
      "fortu-\n",
      "nately, recent advances in vision-language models, such as contrastive language-\n",
      "image pre-training (clip)\n",
      "2. illustration of the proposed clip-lung.\n",
      "pose clip-lung, a framework to classify lung nodules using image-text pairs.\n",
      "then,\n",
      "we design a textual knowledge-guided contrastive learning based on obtained\n",
      "image features and textual features involving classes and attributes.\n",
      "experimen-\n",
      "1) we propose clip-lung for lung nodule malignancy prediction, which lever-\n",
      "ages clinical textual knowledge to enhance the image encoder and classiﬁer.\n",
      "2) we design a channel-wise conditional prompt module to establish consistent\n",
      "relationships among the correlated text tokens and feature maps.\n",
      "3) we simultaneously align the image features with class and attribute fea-\n",
      "tures through contrastive learning while generating more explainable atten-\n",
      "tion maps.\n",
      "in this paper, we arrange the lung nodule classiﬁcation\n",
      "dataset as {i, y, c, a}, where i = {ii}n\n",
      "i=1 is an image set containing n lung nod-\n",
      "ule images.\n",
      "finally, a = {am}m\n",
      "m=1 is the set of attribute embeddings, where each element\n",
      "am ∈ rd×1 is a vector representing the embedding of an attribute word such as\n",
      "“spiculation”.\n",
      ", the training framework contains an image encoder fθ\n",
      "and a text encoder gφ.\n",
      "first, the input image ii is fed into fθ and then gener-\n",
      "ates the feature maps.\n",
      "note that\n",
      "the vectors f t,:, lt,:, l′\n",
      "t,:, and ck,: are with the same dimension\n",
      "consequently, we have image feature f ∈ rt ×d, class feature c\n",
      "hence, the element-\n",
      "wise multiplication wi · ai is unique to ii.\n",
      "2.3\n",
      "channel-wise conditional prompt\n",
      "cocoop [20] ﬁrstly proposed to learn language contexts for vision-language mod-\n",
      "els conditioned on visual features.\n",
      "hence,\n",
      "the conditional prompt for the t-th token is lt+l′\n",
      "t. in addition, ccp also outputs\n",
      "the f t,: for image-class and image-attribute contrastive learning.\n",
      "in this paper, we conduct such image-text contrastive learning\n",
      "by utilizing pre-trained clip text encoder\n",
      "image-class alignment.\n",
      "first, the same to clip, we align the image and class\n",
      "information by minimizing the cross-entropy (ce) loss for the sample {ii, yi}:\n",
      "lic = −\n",
      "t\n",
      "\u0003\n",
      "t=1\n",
      "k\n",
      "\u0003\n",
      "k=1\n",
      "yilog\n",
      "exp(σ(f t,:, ck,:)/τ)\n",
      "\u0002k\n",
      "k′=1 exp(σ(f t,:, ck′,:)/τ)\n",
      ",\n",
      "(2)\n",
      "where ck,: = gφ(ck\n",
      "\u0004(l1+l′\n",
      "1, l2+l′\n",
      "2, . .\n",
      "therefore, lic implements\n",
      "the contrastive learning between channel-wise features and corresponding class\n",
      "features, i.e., the ensemble of grouped image-class alignment results.\n",
      "image-attribute alignment.\n",
      "in addition to image-class alignment, we further\n",
      "expect the image features to correlate with speciﬁc attributes.\n",
      "so we conduct\n",
      "image-attribute alignment by minimizing the infonce loss [5,16]:\n",
      "lia = −\n",
      "t\n",
      "\u0003\n",
      "t=1\n",
      "m\n",
      "\u0003\n",
      "m=1\n",
      "log\n",
      "exp(σ(f t,:, wm,: · am,:)/τ)\n",
      "\u0002m\n",
      "m′=1 exp(σ(f t,:, wm′,: · am′,:)/τ)\n",
      ".\n",
      "class-attribute alignment.\n",
      "although the image features have been aligned\n",
      "with classes and attributes, the class embeddings obtained by the pre-trained\n",
      "clip encoder may shift in the latent space, which may result in inconsistent\n",
      "class space and attribute space, i.e., annotated attributes do not match the\n",
      "corresponding classes, which is contradictory to the actual clinical diagnosis.\n",
      "to\n",
      "avoid this weakness, we further align the class and attribute features:\n",
      "lca = −\n",
      "k\n",
      "\u0003\n",
      "k=1\n",
      "m\n",
      "\u0003\n",
      "m=1\n",
      "log\n",
      "exp(σ(ck,:, wm,: · am,:)/τ)\n",
      "\u0002m\n",
      "m′=1 exp(σ(ck,:, wm′,: · am′,:)/τ)\n",
      ",\n",
      "(4)\n",
      "and this loss implies semantic consistency between classes and attributes.\n",
      "408\n",
      "y. lei et al.\n",
      "finally, the total loss function is deﬁned as follows:\n",
      "l = ei i∈i\n",
      "\u0005\n",
      "lce +\n",
      "note that during the\n",
      "inference phase, test images are only fed into the trained image encoder and clas-\n",
      "siﬁer.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "dataset and implementation details\n",
      "dataset.\n",
      "following [9,11], we modiﬁed the ﬁrst layer of\n",
      "the image encoder to be with 32 channels.\n",
      "according to existing works [11,18], we\n",
      "regard a nodule with an average score between 2.5 and 3.5 as unsure nodules, benign\n",
      "and malignant categories are those with scores lower than 2.5 and larger than 3.5,\n",
      "respectively.\n",
      "experimental settings.\n",
      "in this paper, we apply the clip pre-trained vit-\n",
      "b/16 as the text encoder for clip-lung, and the image encoder we used is\n",
      "resnet-18\n",
      "the image\n",
      "encoder is initialized randomly.\n",
      "the learning rate is 0.001 following the cosine decay, while the optimizer\n",
      "is stochastic gradient descent with momentum 0.9 and weight decay 0.00005.\n",
      "all of\n",
      "our experiments are implemented with pytorch [15] and trained with nvidia\n",
      "a100 gpus.\n",
      "the experimental results are reported with average values through\n",
      "ﬁve-fold cross-validation.\n",
      "3.2\n",
      "experimental results and analysis\n",
      "performance comparisons.\n",
      "in table 1, we compare the classiﬁcation perfor-\n",
      "mances on the lidc-a dataset, where we regard the benign-unsure-malignant\n",
      "clip-lung for lung nodule malignancy prediction\n",
      "409\n",
      "table 1.\n",
      "table 2 presents a performance comparison of clip-lung on the lidc-b and\n",
      "lidc-c datasets.\n",
      "consequently, aligning these distinct feature types becomes challenging, resulting\n",
      "in a bias towards the text features associated with malignant nodules.\n",
      "410\n",
      "y. lei et al.\n",
      "fig.\n",
      "3, we\n",
      "can see that clip yields a non-compact latent space for two kinds of nodules.\n",
      "cocoop and clip-lung alleviate this phenomenon, which demonstrates that\n",
      "the learnable prompts guided by nodule classes are more eﬀective than ﬁxed\n",
      "prompt engineering.\n",
      "based on lic, lia and lca improve the\n",
      "performances on lidc-a, indicating the eﬀectiveness of capturing ﬁne-grained\n",
      "features of ordinal ranks using class and attribute texts.\n",
      "that is to\n",
      "say, lia is more important in latent space rectiﬁcation, i.e., image-attribute con-\n",
      "sistency.\n",
      "in addition, we observe that lic+lia performs better than lia+lca,\n",
      "which is attributed to that lca regularizes the image features indirectly.\n",
      "clip-lung for lung nodule malignancy prediction\n",
      "411\n",
      "4\n",
      "conclusion\n",
      "in this paper, we proposed a textual knowledge-guided framework for pulmonary\n",
      "nodule classiﬁcation, named clip-lung.\n",
      "clip-\n",
      "lung aligned the diﬀerent modalities of features generated from nodule classes,\n",
      "attributes, and images through contrastive learning.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf:\n",
      "esophageal cancer is a signiﬁcant global health concern, and\n",
      "radiotherapy (rt) is a common treatment option.\n",
      "accurate delineation\n",
      "of the gross tumor volume (gtv) is essential for optimal treatment out-\n",
      "comes.\n",
      "in clinical practice, patients may undergo a second round of rt to\n",
      "achieve complete tumor control when the ﬁrst course of treatment fails\n",
      "to eradicate cancer completely.\n",
      "however, manual delineation is labor-\n",
      "intensive, and automatic segmentation of esophageal gtv is diﬃcult due\n",
      "to the ambiguous boundary of the tumor.\n",
      "detailed tumor information\n",
      "naturally exists in the previous stage, however the correlation between\n",
      "the ﬁrst and second course rt is rarely explored.\n",
      "we propose a novel\n",
      "prior anatomy and rt information enhanced second-course esophageal\n",
      "gtv segmentation network (artseg).\n",
      "a region-preserving attention\n",
      "module (ram) is designed to understand the long-range prior knowl-\n",
      "edge of the esophageal structure, while preserving the regional patterns.\n",
      "sparsely labeled medical images for various isolated tasks necessitate\n",
      "eﬃcient utilization of knowledge from relevant datasets and tasks.\n",
      "to\n",
      "achieve this, we train our network in an information-querying manner.\n",
      "artseg incorporates various prior knowledge, including: 1) tumor vol-\n",
      "ume variation between ﬁrst and second rt courses, 2) cancer cell pro-\n",
      "liferation, and 3) reliance of gtv on esophageal anatomy.\n",
      "extensive\n",
      "quantitative and qualitative experiments validate our designs.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43990-2_48.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43990-2_48\n",
      "512\n",
      "y. sun et al.\n",
      "keywords: second course radiotherapy · esophageal gross tumor\n",
      "volume · data eﬃcient learning · prior anatomical information ·\n",
      "attention\n",
      "1\n",
      "introduction\n",
      "esophageal cancer is a signiﬁcant contributor to cancer-related deaths glob-\n",
      "ally [3,15].\n",
      "one eﬀective treatment option is radiotherapy (rt), which utilizes\n",
      "high-energy radiation to target cancerous cells [4].\n",
      "to ensure optimal treatment\n",
      "outcomes, both the cancerous region and the adjacent organ-at-risk (oar) must\n",
      "be accurately delineated, to focus the high-energy radiation solely on the cancer-\n",
      "ous area while protecting the oars from any harm.\n",
      "in the clinical setting, patients may undergo a second round of rt treatment\n",
      "to achieve complete tumor control when initial treatment fails to completely\n",
      "eradicate cancer\n",
      "moreover, the automatic delineation of the gtv in the esophagus\n",
      "poses a signiﬁcant diﬃculty, primarily attributable to the low contrast between\n",
      "the esophageal gtv and the neighboring tissue, as well as the limited datasets.\n",
      "recently, advances in deep learning [21] have promoted research in auto-\n",
      "matic esophageal gtv segmentation from computed tomography (ct)\n",
      "[9,10] improve the segmentation accuracy\n",
      "by incorporating additional information from paired positron emission tomog-\n",
      "raphy (pet).\n",
      "nevertheless, such approaches require several imaging modalities,\n",
      "which can be both costly and time-consuming, while disregarding any knowledge\n",
      "from previous treatment or anatomical understanding.\n",
      "we proposed a novel prior anatomy and rt\n",
      "information enhanced second-course esophageal gtv segmentation network\n",
      "(artseg).\n",
      "a region-preserving attention module (ram) is designed to eﬀec-\n",
      "tively capture the long-range prior knowledge in the esophageal structure, while\n",
      "preserving regional tumor patterns.\n",
      "to the best of our knowledge, we are the\n",
      "ﬁrst to reveal the domain gap between the ﬁrst and second courses for gtv\n",
      "segmentation, and explicitly leverage prior information from the ﬁrst course to\n",
      "improve gtv segmentation performance in the second course.\n",
      "the medical images are labeled sparsely, which are isolated by diﬀerent\n",
      "tasks [20].\n",
      "meanwhile, an ideal method for automatic esophageal gtv segmenta-\n",
      "tion in the second course of rt should consider three key aspects: 1) changes in\n",
      "tumor volume after the ﬁrst course of rt, 2) the proliferation of cancerous cells\n",
      "from a tumor to neighboring healthy cells, and 3) the anatomical-dependent\n",
      "second-course esophageal gtv segmentation\n",
      "513\n",
      "fig.\n",
      "our training approach leverages multi-center datasets containing relevant anno-\n",
      "tations, that challenges the network to retrieve information from e1 using the features\n",
      "from e2.\n",
      "our training strategy leverages three datasets that introduce\n",
      "prior knowledge to the network of the following three key aspects: 1) tumor volume\n",
      "variation, 2) cancer cell proliferation, and 3) reliance of gtv on esophageal anatomy.\n",
      "nature of gtv on esophageal locations.\n",
      "to achieve this, we eﬃciently exploit\n",
      "knowledge from multi-center datasets that are not tailored for second-course\n",
      "gtv segmentation.\n",
      "our training strategy does not speciﬁc to any tasks but\n",
      "challenges the network to retrieve information from another encoder with aug-\n",
      "mented inputs, which enables the network to learn from the above three aspects.\n",
      "extensive quantitative and qualitative experiments validate our designs.\n",
      "2\n",
      "network architecture\n",
      "in the ﬁrst course of rt, a ct image denoted as i1 is utilized to manually\n",
      "delineate the esophageal gtv, g1.\n",
      "during the second course of rt, a ct image\n",
      "i2 of the same patient is acquired.\n",
      "however, i2 is not aligned with i1 due to soft\n",
      "tissue movement and changes in tumor volume that occurred during the ﬁrst\n",
      "course of treatment.\n",
      "both images i1/2 have the spatial shape of h × w ×\n",
      "d.\n",
      "our objective is to predict the esophageal gtv g2 of the second course.\n",
      "it would be advantageous to leverage insights from the ﬁrst course, as it com-\n",
      "prises comprehensive information pertaining to the tumor in its preceding phase.\n",
      "the features f d\n",
      "1/2 are reshaped to hw d\n",
      "23d\n",
      "× c before passed to the mha, where\n",
      "c is the channel dimension.\n",
      "the attentive features f d\n",
      "a can be formulated as:\n",
      "f d\n",
      "a = mha(q, k, v ) = mha(f d\n",
      "2 , f d\n",
      "1 , f d\n",
      "1 ), d = 3, 4.\n",
      "(2)\n",
      "since mha perturbs the positional information, we preserve the tumor local\n",
      "patterns by concatenating original features to the attentive features at the chan-\n",
      "nel dimension, followed by a 1 × 1 × 1 bottleneck convolution ξ1×1×1 to squeeze\n",
      "the channel features (named as ram), as shown in the following equations,\n",
      "f d =\n",
      "\u0002concat(f d\n",
      "1 , f d\n",
      "2 ),\n",
      "d\n",
      "3\n",
      "training strategy\n",
      "the network should learn from three aspects: 1) tumor volume variation: the\n",
      "structural changes of the tumor from the ﬁrst to the second course; 2) cancer\n",
      "cell proliferation: the tumor in esophageal cancer tends to inﬁltrate into the\n",
      "adjacent tissue; 3) reliance of gtv on esophageal anatomy: the anatom-\n",
      "ical dependency between esophageal gtv and the position of the esophagus.\n",
      "medical images are sparsely labeled which are isolated by diﬀerent tasks [20],\n",
      "and are often inadequate.\n",
      "in order to fully leverage both public and private datasets, the training objec-\n",
      "tive should not be speciﬁc to any tasks.\n",
      "1, our strategy is to challenge the network to retrieve information from\n",
      "augmented inputs in e1 using the features from e2, which can incorporate a wide\n",
      "range of datasets that are not tailored for second-course gtv segmentation.\n",
      "3.1\n",
      "tumor volume variation\n",
      "the diﬀerences in tumor volume between the ﬁrst and second courses following\n",
      "an rt treatment can have a negative impact on the state-of-the-art (sota)\n",
      "learning-based techniques, which will be discussed in sect.\n",
      "in sp, i1\n",
      "p and i2\n",
      "p are the ﬁrst and second\n",
      "course ct images, while g1\n",
      "p and g2\n",
      "p are the corresponding gtv annotations.\n",
      "second-course esophageal gtv segmentation\n",
      "515\n",
      "3.2\n",
      "cancer cell proliferation\n",
      "the paired dataset sp for the ﬁrst and second courses is limited, whereas an\n",
      "unpaired gtv dataset sv = {iv; gv} can be easily obtained in a standard clini-\n",
      "cal workﬂow with a substantial amount.\n",
      "sv lacks its counterpart for the second\n",
      "course, in which iv/gv are the ct image and the corresponding annotation for\n",
      "gtv. to address this, we apply two distinct randomized augmentations, p1, p2,\n",
      "to mimic the unregistered issue of the ﬁrst and second course ct.\n",
      "(4)\n",
      "the esophageal tumor can proliferate with varying morphologies into the sur-\n",
      "rounding tissues.\n",
      "3.3\n",
      "reliance of gtv on esophageal anatomy\n",
      "to make full use of the datasets of relevant tasks, we incorporate a public esoph-\n",
      "agus segmentation dataset, denoted as se = {ie; ge}, where ie/ge represent the\n",
      "ct images and corresponding annotations of the esophagus structure.\n",
      "by aug-\n",
      "menting the data as described in eq.\n",
      "similarly, data from\n",
      "the paired sp is also augmented by p1/2 to increase the network’s robustness.\n",
      "in summary, our training strategy is not dataset-speciﬁc or target-speciﬁc,\n",
      "thus allowing the integration of prior knowledge from multi-center esophageal\n",
      "gtv-related datasets, which eﬀectively improves the network’s ability to retrieve\n",
      "information for the second course from the three key aspects stated in sect.\n",
      "3.\n",
      "4\n",
      "experiments\n",
      "4.1\n",
      "experimental setup\n",
      "datasets.\n",
      "the paired ﬁrst-second course dataset, sp, is collected from sun yat-\n",
      "sen university cancer center (ethics approval number: b2023-107-01), com-\n",
      "prising paired ct scans of 69 distinct patients from south china.\n",
      "for both sp and sv, physicians annotated the esophageal\n",
      "cancer gtv in each ct.\n",
      "the results suggest a domain gap between the ﬁrst and second courses, which\n",
      "indicates increased diﬃculty in gtv segmentation for the second course.\n",
      "asterisks\n",
      "indicate p-value < 0.05 for the performance gap between the ﬁrst and second course.\n",
      "(color ﬁgure online)\n",
      "have esophageal cancer.\n",
      "implementation details.\n",
      "the augmentations p1/2 involve a combination of ran-\n",
      "dom 3d resized cropping, ﬂipping, rotation in the transverse plane, and gaussian\n",
      "noise.\n",
      "the network is implemented using pytorch\n",
      "[1], and detailed conﬁgurations are in the supplementary material.\n",
      "experiments are performed on an nvidia rtx 3090 gpu with 24gb memory.\n",
      "performance metrics.\n",
      "dice score (dsc), averaged surface distance (asd)\n",
      "and hausdorﬀ distance (hsd) are used as metrics for evaluation.\n",
      "the wilcoxon\n",
      "signed-rank test is used to compare the performance of diﬀerent methods.\n",
      "second-course esophageal gtv segmentation\n",
      "517\n",
      "4.2\n",
      "domain gap between the first and second course\n",
      "as previously mentioned, the volume of the tumors changes after the ﬁrst course\n",
      "of rt.\n",
      "the\n",
      "results presented in table 1 indicate a performance gap between gtv segmen-\n",
      "tation in the ﬁrst and second courses, with the latter being more challenging.\n",
      "notably, the paired ﬁrst-second course dataset stest\n",
      "p\n",
      "pertains to the same group\n",
      "of patients, thereby ensuring that any performance drop can be attributed solely\n",
      "to diﬀerences in courses of rt, rather than variations across diﬀerent patients.\n",
      "therefore, for accurate second-course gtv segmentation,\n",
      "we need to explicitly propagate prior information from the ﬁrst course using dual\n",
      "encoders in artseg, and incorporate learning about tumor changes.\n",
      "4.3\n",
      "evaluations of second-course gtv segmentation performance\n",
      "combination of various datasets.\n",
      "table 2 presents the information gain\n",
      "derived from multi-center datasets using quantiﬁed metrics for segmentation\n",
      "performance.\n",
      "when prior information from the ﬁrst course is explicitly introduced\n",
      "using sp, artseg outperforms other baselines for gtv segmentation in the\n",
      "second course, which reaches a dsc of 66.73%.\n",
      "however, in fig. 3, it can be\n",
      "observed that the model failed to accurately track the gtv area along the\n",
      "esophagus (orange arrows) due to the soft and elongated nature of the esophageal\n",
      "tissue, which deforms easily during ct scans performed at diﬀerent times.\n",
      "meanwhile, the esophageal tumor comprises two\n",
      "primary regions, the original part located in the esophagus and the extended\n",
      "part that has invaded the surrounding tissue.\n",
      "quantitative comparison of gtv segmentation performance in the second\n",
      "course.\n",
      "our proposed artseg+ram achieved better overall performance, where\n",
      "asterisks indicate artseg+ram outperforms other methods with p-value < 0.05.\n",
      "methods\n",
      "strain\n",
      "p\n",
      "sv se dsc (%) ↑\n",
      "asd(mm) ↓\n",
      "hsd(mm) ↓\n",
      "inference\n",
      "mean ± std.\n",
      "med.\n",
      "mean ± std.\n",
      "med.\n",
      "the impact of diﬀerent prior knowledge on esophageal tumor detection.\n",
      "our proposed approach, encompassing comprehensive prior knowledge, shows\n",
      "superior performance.\n",
      "(color\n",
      "ﬁgure online)\n",
      "further improve the dsc to 74.54% by utilizing comprehensive knowledge of\n",
      "both the tumor morphology and esophageal structures.\n",
      "although\n",
      "introducing\n",
      "the\n",
      "esophageal structural prior knowledge using se can improve the performance\n",
      "in dsc and asd (table 2), the increase in hsd (38.22 to 47.89 mm; 21.71 to\n",
      "27.00 mm) indicates that there are outliers far from the ground truth bound-\n",
      "aries.\n",
      "second-course esophageal gtv segmentation\n",
      "519\n",
      "however, there is no performance gain with mha as shown in table 2, and the\n",
      "hsd further increased to 27.33 mm.\n",
      "to tackle the aforementioned problem, we propose ram which involves the\n",
      "concatenation of the original features with attention outputs, allowing for the\n",
      "preservation of convolution-generated regional tumor patterns while eﬀectively\n",
      "comprehending long-range prior knowledge speciﬁc to the esophagus.\n",
      "for the method’s generalizability, analysis of diverse imaging pro-\n",
      "tocols and segmentation backbones are inadequate.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_8.pdf:\n",
      "how-\n",
      "ever, the use of gadolinium-based contrast agents (gbca) to obtain\n",
      "ce-mri may be associated with nephrogenic systemic ﬁbrosis and may\n",
      "lead to bioaccumulation in the brain, posing a potential risk to human\n",
      "health.\n",
      "moreover, and likely more important, the use of gadolinium-based\n",
      "contrast agents requires the cannulation of a vein, and the injection of\n",
      "the contrast media which is cumbersome and places a burden on the\n",
      "patient.\n",
      "to reduce the use of contrast agents, diﬀusion-weighted imaging\n",
      "(dwi) is emerging as a key imaging technique, although currently usu-\n",
      "ally complementing breast ce-mri.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43990-2 8.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43990-2_8\n",
      "80\n",
      "t. zhang et al.\n",
      "reﬁned feature maps, and leverage hierarchical representation informa-\n",
      "tion fused at diﬀerent scales while utilizing the contributions from diﬀer-\n",
      "ent sequences from a model-driven approach by introducing the weighted\n",
      "diﬀerence module.\n",
      "keywords: contrast-enhanced mri · diﬀusion-weighted imaging ·\n",
      "deep learning · multi-sequence fusion · breast cancer\n",
      "1\n",
      "introduction\n",
      "breast cancer is the most common cancer and the leading cause of cancer death\n",
      "in women\n",
      "early detection of breast cancer allows patients to receive timely\n",
      "treatment, which may have less burden and a higher probability of survival [6].\n",
      "among current clinical imaging modalities, magnetic resonance imaging (mri)\n",
      "has the highest sensitivity for breast cancer detection\n",
      "however, the use of\n",
      "gadolinium-based contrast agents (gbca) requires iv-cannulation, which is a\n",
      "burden to patients, time consuming and cumbersome in a screening situation.\n",
      "moreover, contrast administration can lead to allergic reactions and ﬁnaly ce-\n",
      "mri may be associated with nephrogenic systemic ﬁbrosis and lead to bioaccu-\n",
      "mulation in the brain, posing a potential risk to human health\n",
      "in\n",
      "2017, the european medicines agency concluded its review of gbca, conﬁrm-\n",
      "ing recommendations to restrict the use of certain linear gbca used in mri\n",
      "body scans and to suspend the authorization of other contrast agents, albeit\n",
      "macrocyclic agents can still be freely used [10].\n",
      "with the development of computer technology, artiﬁcial intelligence-based\n",
      "methods have shown potential in image generation and have received extensive\n",
      "attention.\n",
      "among them, synthe-\n",
      "sis of ce-mri is very important as mentioned above, but few studies have been\n",
      "done by researchers in this area due to its challenging nature.\n",
      "therefore, it is necessary to\n",
      "focus on the most promising sequences to synthesize ce-mri.\n",
      "diﬀusion-weighted imaging (dwi) is emerging as a key imaging technique\n",
      "to complement breast ce-mri [3]\n",
      "ii we invented hierarchical fusion module, weighted diﬀerence module and\n",
      "multi-sequence attention module to enhance the fusion at diﬀerent scale, to\n",
      "control the contribution of diﬀerent sequence and maximising the usage of\n",
      "the information within and across sequences.\n",
      "all mris were resampled to 1 mm isotropic voxels and uniformly sized, resulting\n",
      "in volumes of 352 × 352 pixel images with 176 slices per mri, and subsequent\n",
      "registration was performed based on advanced normalization tools (ants)\n",
      "the l1-norm is used as a reconstruction loss to measure\n",
      "the diﬀerence between the reconstructed image and the ground truth.\n",
      "inspired by adc, a weighted diﬀerence module is\n",
      "designed, in which the neural network is used to simulate the dynamic analysis\n",
      "of the ln function, and the element-wise subtraction algorithm is used to extract\n",
      "the diﬀerentiation features between dwis with diﬀerent b-values, and ﬁnally the\n",
      "features are weighted to obtain weighted feature maps (fdwi, eq. 2).\n",
      "[fθl(sl) − fθh(sh)]/(bh − bl)\n",
      "(2)\n",
      "where sl and sh represent the image signals obtained from lower b value bl and\n",
      "higher bh, fθl and fθh represent the corresponding neural networks for dwi with\n",
      "a lower and higher b value.\n",
      "the input feature maps (fconcat) go through the maximum pooling layer\n",
      "and the average pooling layer respectively, and then are added element-wise\n",
      "after passing through the shared fully connected neural network, and ﬁnally the\n",
      "weight map as is generated after passing through the activation function, as\n",
      "shown in eq.\n",
      "(4)\n",
      "where ⊗ represents element-wise multiplication, ⊕ represents element-wise\n",
      "summation, σ represents the sigmoid function, θfc represents the corresponding\n",
      "network parameters of the shared fully-connected neural network, and avgpool\n",
      "and maxpool represent average pooling and maximum pooling operations,\n",
      "respectively.\n",
      "in the synthesis process, the generator g tries to generate an image according\n",
      "to the input multi-sequence mri (d1, d2, d3, d4, t1), and the discriminator d\n",
      "tries to distinguish the generated image g(d1, d2, d3, d4, t1) from the real image\n",
      "y, and at the same time, the generator tries to generate a realistic image to\n",
      "mislead the discriminator.\n",
      "[log (1 − d(g(d1, d2, d3, d4, t1)))]\n",
      "(6)\n",
      "where prodata(d1, d2, d3, d4, t1) represents the empirical joint distribution of\n",
      "inputs d1 (dwib0), d2 (dwib150), d3 (dwib800), d4 (dwib1500) and t1 (t1-\n",
      "weighted mri), λ1 is a non-negative trade-oﬀ parameter, and l1-norm is used\n",
      "to measure the diﬀerence between the generated image and the corresponding\n",
      "ground truth.\n",
      "the numbers of ﬁlters\n",
      "are 32, 64, 128, 256 and 512, respectively.\n",
      "2.3\n",
      "visualization\n",
      "the t1-weighted images and the contrast-enhanced images were subtracted to\n",
      "obtain a diﬀerence mri to clearly reveal the enhanced regions in the ce-mri.\n",
      "2. detailed structure of the hierarchical fusion module.\n",
      "2.4\n",
      "experiment settings\n",
      "based on the ratio of 8:2, the training set and independent test set of the in-house\n",
      "dataset have 612 and 153 cases, respectively.\n",
      "structural similarity index mea-\n",
      "surement (ssim), peak signal-to-noise ratio (psnr) and normalized mean\n",
      "synthesis of contrast-enhanced breast mri\n",
      "85\n",
      "squared error (nmse) were used as metrics, all formulas as follows:\n",
      "ssim =\n",
      "(2μy(x)μg(x) + c1)(2σy(x)g(x) + c2)\n",
      "(μ2\n",
      "y(x) + μ2\n",
      "g(x) +\n",
      "[5]\n",
      "t1+dwis\n",
      "87.58 ± 2.68\n",
      "27.80 ± 1.56\n",
      "0.0692 ± 0.035\n",
      "proposed\n",
      "t1+dwis\n",
      "89.93 ± 2.91\n",
      "28.92 ± 1.63\n",
      "0.0585 ± 0.026\n",
      "where g(x) represents a generated image, y(x) represents a ground-truth\n",
      "image, μy(x) and μg(x) represent the mean of y(x) and g(x), respectively, σy(x)\n",
      "and σg(x) represent the variance of y(x) and g(x), respectively, σy(x)g(x) repre-\n",
      "sents the covariance of y(x) and g(x), and c1 and c2 represent positive constants\n",
      "used to avoid null denominators.\n",
      "3\n",
      "results\n",
      "first, we compare the performance of diﬀerent existing methods on synthetic\n",
      "ce-mri using our source data, the quantitative indicators used include psnr,\n",
      "ssim and nmse.\n",
      "it may\n",
      "be because their model can only combine bi-modality and cannot integrate the\n",
      "features of all sequences, so it cannot mine the diﬀerence features between mul-\n",
      "tiple b-values, which limits the performance of the model.\n",
      "[5] used full-sequence mri to synthesize ce-mri, it\n",
      "would be advantageous to obtain synthetic ce-mri images using as little data\n",
      "as possible, taking advantage of the most contributing sequences.\n",
      "they did not\n",
      "take advantage of multi-b-value dwi, nor did they use the hierarchical fusion\n",
      "module to fully fuse the hierarchical features of multi-sequence mri.\n",
      "86\n",
      "t. zhang et al.\n",
      "fig.\n",
      "finally, the addition of the multi-sequence attention mod-\n",
      "ule further improved the performance of the model, with ssim of 89.93 ± 2.91,\n",
      "psnr of 28.92 ± 1.63, and nmse of 0.0585 ± 0.026.\n",
      "3. it can be\n",
      "seen from the visualization results that after the diﬀerence between the generated\n",
      "ce-mri and the original t1-weighted mri, the lesion position of the breast is\n",
      "highlighted, the red circle represents the highlighted area, which proves that our\n",
      "method can eﬀectively synthesize contrast-enhanced images, highlighting the\n",
      "same parts as the real enhanced position.\n",
      "see supplementary material for more\n",
      "visualization results, including visualizations of breast ce-mri synthesized in\n",
      "axial, coronal, and sagittal planes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf:\n",
      "early detection and diagnosis of breast cancer using ultrasound images\n",
      "are crucial for timely diagnostic decision and treatment in clinical application.\n",
      "however, the similarity between tumors and background and also severe shadow\n",
      "noises in ultrasound images make accurate segmentation of breast tumor chal-\n",
      "lenging.\n",
      "in this paper, we propose a large pre-trained model for breast tumor seg-\n",
      "mentation, with robust performance when applied to new datasets.\n",
      "speciﬁcally,\n",
      "our model is built upon unet backbone with deep supervision for each stage of\n",
      "the decoder.\n",
      "besides using dice score, we also design discriminator-based loss\n",
      "on each stage of the decoder to penalize the distribution dissimilarity from multi-\n",
      "scales.\n",
      "our proposed model is validated on a large clinical dataset with more than\n",
      "10000 cases, and shows signiﬁcant improvement than other representative models.\n",
      "keywords: large-scale clinical dataset · deep-supervision · multi-scale\n",
      "segmentation · breast ultrasound images\n",
      "registration number: 4319\n",
      "1\n",
      "introduction\n",
      "breast cancer is a serious health problem with high incidence and wide prevalence for\n",
      "women throughout the world\n",
      "regular screening and early detection are crucial for\n",
      "effective diagnosis and treatment, and hence for improved prognosis and survival rate [3,\n",
      "4].\n",
      "manual delineation always depends on the\n",
      "experience of radiologists, which tends to be subjective and time-consuming [9, 10].\n",
      "89–96, 2023.\n",
      "https://doi.org/10.1007/978-3-031-43990-2_9\n",
      "90\n",
      "m. li et al.\n",
      "therefore, there is a high demand for automatic and robust methods to achieve accurate\n",
      "breast tumor segmentation.\n",
      "however, due to speckle noise and shadows in ultrasound\n",
      "images, breast tumor boundaries tend to be blurry and are difﬁcult to be distinguished\n",
      "from background.\n",
      "these issues pose challenges and difﬁculties for accurate\n",
      "breast tumor segmentation in ultrasound images.\n",
      "various approaches based on deep learning have been developed for tumor seg-\n",
      "mentation with promising results [13–19].\n",
      "[13] designed a multi-scale u-net\n",
      "to extract more semantic and diverse features for medical image segmentation, using\n",
      "multiple convolution sequences and convolution kernels with different receptive ﬁelds.\n",
      "[14] raised a deeply-supervised encoder-decoder network, which is connected\n",
      "through a series of nested and dense skip pathways to reduce semantic gap between fea-\n",
      "ture maps.\n",
      "in [15], a multi-scale selection and multi-channel fusion segmentation model\n",
      "was built, which gathers global information from multiple receptive ﬁelds and integrates\n",
      "multi-level features from different network positions for accurate pancreas segmenta-\n",
      "tion.\n",
      "[17]\n",
      "introduced a unet 3+ for medical image segmentation, which incorporates low-level\n",
      "and high-level feature maps in different scales and learns full-scale aggregated feature\n",
      "representations.\n",
      "[18] established a convolution neural network optimized by\n",
      "super-pixel and support vector machine, segmenting multiple organs from ct scans\n",
      "to assist physicians diagnosis.\n",
      "[19] introduced channel and position attention\n",
      "module into deep learning neural network to obtain contextual information for colorec-\n",
      "tal tumors segmentation in ct scans.\n",
      "however, although these proposed models have\n",
      "achieved satisfactory results in different medical segmentation tasks, their performances\n",
      "are limited for breast tumor segmentation in ultrasound images due to the low image\n",
      "contrast and blurry tissue boundary.\n",
      "to address these challenges, we present, to the best of our knowledge, the ﬁrst\n",
      "work to adopt multi-scale features collected from large set of clinical ultrasound images\n",
      "for breast tumor segmentation.\n",
      "the main contributions of our work are as follows: (1)\n",
      "we propose a well-pruned simple but effective network for breast tumor segmentation,\n",
      "which shows remarkable and solid performance on large clinical dataset; (2) our large\n",
      "pretrained model is evaluated on two additional public datasets without ﬁne-tuning and\n",
      "shows extremely stabilized improvement, indicating that our model has outstanding\n",
      "generalizability and good robustness against multi-site data data.\n",
      "in particular, we apply\n",
      "similarity constraint for each stage of the unet decoder to obtain consistent and sta-\n",
      "ble segmentation maps.\n",
      "instead of using dice score in the ﬁnal layer of unet, we also\n",
      "use dice loss on each of the decoder stages.\n",
      "besides, we integrate an adversarial loss\n",
      "as additional constraint to penalize the distribution dissimilarity between the predicted\n",
      "developing large pre-trained model for breast tumor segmentation\n",
      "91\n",
      "segmentation map and the ground truth.\n",
      "in the framework of gan, we take our segmen-\n",
      "tation network as the generator and a convolutional neural network as the discriminator.\n",
      "therefore, we formulate the overall loss for the generator,\n",
      "namely the segmentation network, as\n",
      "loverall =\n",
      "\u00024\n",
      "n=1 αn · (1 − 2\n",
      "\u0003\u0003\u0003p(n) ∩ g\n",
      "\u0003\u0003\u0003 ·\n",
      "\u0004\u0003\u0003\u0003p(n)\u0003\u0003\u0003\n",
      "(1)\n",
      "where sn represents the segmentation network and cn represents the involved convo-\n",
      "lutional network.\n",
      "θsn and θcn refer to the parameters in the segmentation and convo-\n",
      "lutional network, respectively.\n",
      "p(n) represents the segmentation maps obtained from the\n",
      "n-th stage in the segmentation network, and g refers to the corresponding ground truth.\n",
      "psn(proba) denotes the distribution of probability maps.\n",
      "it should be noted that, in unet, there are 4 stages and hence\n",
      "we employ 4 cnns for each of them without sharing their weights.\n",
      "the ﬁrst constraint is used to enhance prediction\n",
      "similarity to the standard ones, for preliminary segmentation.\n",
      "the second constraint is used to\n",
      "capture data distribution to maintain consistency in high-dimensional space for map reﬁnement.\n",
      "cnθcn(g) represents the\n",
      "probability for the input of cn coming from the original dataset.\n",
      "intheimplementation,weupdatethesegmentationnetworkandallthediscriminators\n",
      "alternatingly in each iteration until both the generator and discriminators are converged.\n",
      "92\n",
      "m. li et al.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "dataset and implementation details\n",
      "we collected 10927 cases for this research from yunnan cancer hospital.\n",
      "five-fold cross validation is performed on\n",
      "the dataset in all experiments to verify our proposed network.\n",
      "in order to com-\n",
      "prehensively evaluate segmentation efﬁciency of our model, dice similarity coefﬁcient\n",
      "(dsc), precision, recall, jaccard, and root mean squared error (rmse) are used as\n",
      "evaluation metrics in this work.\n",
      "quantitative comparison with state-of-the-art segmentation methods on large-scale\n",
      "clinical breast ultrasound dataset.\n",
      "our proposed algorithm is conducted on pytorch, and all experiments are performed\n",
      "on nvidia tesla a100 gpu.\n",
      "the epochs to train all models are 100 and the batch size\n",
      "in training process is set as 4.\n",
      "3.2\n",
      "comparison with state-of-the-art methods\n",
      "to verify the advantages of our proposed model for breast tumor segmentation in ultra-\n",
      "sound images, we compare our deep-supervised convolutional network with the state-of-\n",
      "the-art tumor segmentation methods, including deepres\n",
      "[13], unet++\n",
      "developing large pre-trained model for breast tumor segmentation\n",
      "93\n",
      "[14], segnet [25], attu-net [16], u2-net\n",
      "the comparison exper-\n",
      "iments are carried on a large-scale clinical breast ultrasound dataset, and the quantitative\n",
      "results are reported in table 1.\n",
      "it is obvious that our proposed model achieves the opti-\n",
      "mal performance compared with other segmentation models.\n",
      "these results indicate the effectiveness of\n",
      "the proposed model in delineating breast tumors in ultrasound images.\n",
      "representative segmentation results using different methods are provided in fig.\n",
      "2. segmentation results of ﬁve subjects obtained from different models.\n",
      "red boxes are used\n",
      "to highlight the boundaries which are difﬁcult to segment.\n",
      "fig.\n",
      "3. five evaluation criteria for total cases from different frameworks: stage i, stage ii, stage\n",
      "iii, and stage iv (from left to right in each index).\n",
      "four\n",
      "groups of frameworks (stage i, stage ii, stage iii and stage iv) are designed, with the\n",
      "94\n",
      "m. li et al.\n",
      "numerals denoting the level of deep supervision counting from the last deconvolutional\n",
      "layer.\n",
      "we test these four frameworks on the in-house breast ultrasound dataset, and verify\n",
      "their segmentation performance using the same ﬁve evaluation criteria.\n",
      "the obtained quan-\n",
      "titative results are shown in table 2, where stage iv model achieves the optimal dsc,\n",
      "precision, recall, and jaccard.\n",
      "that is, the segmentation ability of the\n",
      "proposed stage iv is ameliorated from every possible perspective.\n",
      "moreover, stage\n",
      "iv obtains minimal rmse compared with other three models (0.68 mm vs 0.84 mm,\n",
      "0.82 mm, 0.75 mm), which means better matching of the predicted maps from stage iv\n",
      "with the corresponding ground truth.\n",
      "all these comparison results verify the superiority\n",
      "of deep supervision for breast tumor segmentation in ultrasound images.\n",
      "quantitative results among four groups of segmentation frameworks stage i, stage ii,\n",
      "stage iii, and stage iv.\n",
      "method\n",
      "dsc (%)↑\n",
      "precision (%)↑\n",
      "recall (%)↑\n",
      "jaccard (%)↑\n",
      "rmse (mm)↓\n",
      "stage i\n",
      "70.52 ± 1.25\n",
      "70.01 ± 1.77\n",
      "76.16 ± 2.04\n",
      "55.72 ± 1.53\n",
      "0.84 ± 0.09\n",
      "stage ii\n",
      "73.43 ± 1.07\n",
      "76.10 ± 1.30\n",
      "74.19 ± 2.30\n",
      "58.99 ± 1.37\n",
      "0.82 ± 0.08\n",
      "stage iii\n",
      "77.67 ± 1.06\n",
      "79.63 ± 1.43\n",
      "78.17 ± 1.98\n",
      "64.42 ± 1.47\n",
      "0.75 ± 0.07\n",
      "stage iv\n",
      "80.97 ± 0.88\n",
      "81.64 ± 1.30\n",
      "82.19 ± 1.61\n",
      "68.83 ± 1.38\n",
      "0.68 ± 0.06\n",
      "3.4\n",
      "performance on two external public datasets\n",
      "in order to evaluate the generalizability of the proposed model, we introduce external\n",
      "dataset 1 and dataset 2 for external validation experiments.\n",
      "speciﬁcally, dataset 1 and\n",
      "dataset 2 are used as testing data to evaluate the generalization performance of the\n",
      "models trained on our own dataset without ﬁne tuning, and the corresponding results\n",
      "are shown in table 3.\n",
      "promising performance demonstrates outstanding generalization\n",
      "ability of our large pre-trained model, with a dsc score of 81.35% and a recall of\n",
      "80.96% on dataset 1, and a dsc score of 77.16% and a recall of 93.22% on dataset 2.\n",
      "table 3.\n",
      "external validation performance on two independent publicly-available dataset 1 and\n",
      "dataset 2.\n",
      "dataset\n",
      "dsc (%)\n",
      "precision (%)\n",
      "recall (%)\n",
      "jaccard (%)\n",
      "rmse (mm)\n",
      "dataset 1\n",
      "81.35 ± 2.04\n",
      "85.75 ± 2.97\n",
      "80.96 ± 2.09\n",
      "70.65 ± 3.07\n",
      "0.19 ± 0.01\n",
      "dataset 2\n",
      "77.16 ± 2.51\n",
      "68.34 ± 4.03\n",
      "93.22 ± 0.65\n",
      "65.26 ± 3.67\n",
      "0.42 ± 0.01\n",
      "developing large pre-trained model for breast tumor segmentation\n",
      "95\n",
      "4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf:\n",
      "however, the eﬀectiveness of deep neural net-\n",
      "works is often limited by the lack of interpretability and the need for\n",
      "signiﬁcant amount of manual annotations.\n",
      "moreover, considering many stud-\n",
      "ies overlooking interactive information relevant to diagnosis, we accord-\n",
      "ingly utilize transformer-based attention in our network to mutualize\n",
      "multi-view pathological information, and further employ a bidirectional\n",
      "fusion learning (bfl) to more eﬀectively fuse multi-view information.\n",
      "experimental results demonstrate that our proposed model signiﬁcantly\n",
      "improves both mammogram classiﬁcation performance and interpretabil-\n",
      "ity through incorporation of gaze data and cross-view interactive infor-\n",
      "mation.\n",
      "keywords: mammogram classiﬁcation · gaze · multi-view\n",
      "interaction · bidirectional fusion learning\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43990-2 7.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43990-2_7\n",
      "mammo-net for multi-view mammogram classiﬁcation\n",
      "69\n",
      "1\n",
      "introduction\n",
      "breast cancer is the most prevalent form of cancer among women and can have\n",
      "serious physical and mental health consequences if left unchecked [5].\n",
      "early detec-\n",
      "tion through mammography is critical for early treatment and prevention\n",
      "[19].\n",
      "mammograms provide images of breast tissue, which are taken from two views:\n",
      "the cranio-caudal (cc) view, and the medio-lateral oblique (mlo) view [4].\n",
      "by\n",
      "identifying breast cancer early, patients can receive targeted treatment before\n",
      "the disease progresses.\n",
      "however, these models often require a\n",
      "large number of manual annotations and lack interpretability, which can prevent\n",
      "their broader applications in breast cancer diagnosis.\n",
      "by using gaze data to guide model training,\n",
      "we can improve model interpretability and performance [24].\n",
      "radiologists’ eye movements can be automatically and unobtrusively\n",
      "recorded during the process of reading mammograms, providing a valuable source\n",
      "of data without the need for manual labeling.\n",
      "leveraging gaze from radiol-\n",
      "ogists to aid in model training not only increases eﬃciency and minimizes the risk\n",
      "of errors linked to manual annotation, but also can be seamlessly implemented\n",
      "without aﬀecting radiologists’ normal clinical interpretation of mammograms.\n",
      "mammography primarily detects two types of breast lesions: masses and\n",
      "microcalciﬁcations [16].\n",
      "in this work, we propose a novel diagnostic model, namely mammo-net,\n",
      "which integrates radiologists’ gaze data and interactive information between\n",
      "cc-view and mlo-view to enhance diagnostic performance.\n",
      "• we demonstrate the eﬀectiveness of our approach through experiments using\n",
      "mammography datasets, which show the superiority of mammo-net.\n",
      "we use several resnet blocks pre-trained on imagenet\n",
      "then, we use global average pooling (gap) and fully connected\n",
      "layers to compute the feature vectors produced by the model.\n",
      "this allows the model to\n",
      "mimic the attention of radiologists and enhance diagnostic performance.\n",
      "+ μlbf l.\n",
      "(8)\n",
      "3\n",
      "experiments and results\n",
      "3.1\n",
      "datasets\n",
      "mammogram dataset.\n",
      "our experiments were conducted on cbis-ddsm\n",
      "[12] and inbreast [16].\n",
      "it is worth noting that the oﬃcial inbreast dataset does not provide\n",
      "image-level labels, so we obtained these labels following shen et al.\n",
      "eye movement data was collected by reviewing all cases in\n",
      "inbreast using a tobii pro nano eye tracker.\n",
      "the scenario is shown in appendix\n",
      "and can be accessed at https://github.com/jamesqfreeman/miceye.\n",
      "74\n",
      "c. ji et al.\n",
      "3.2\n",
      "implementation details\n",
      "we trained our model using the adam optimizer [10] with a learning rate of\n",
      "10−4 (partly implemented by mindspore).\n",
      "to overcome the problem of limited\n",
      "data, we employed various data augmentation techniques, including translation,\n",
      "rotation, and ﬂipping.\n",
      "[26]\n",
      "0.859\n",
      "0.791\n",
      "mlo-view\n",
      "0.663\n",
      "0.716\n",
      "cc-view\n",
      "0.650\n",
      "0.704\n",
      "cross-view\n",
      "0.762\n",
      "0.755\n",
      "cross-view+bfl\n",
      "0.786\n",
      "0.812\n",
      "cross-view+ra\n",
      "0.864\n",
      "0.830\n",
      "cross-view+bfl+ra (mammo-net) 0.889 0.849\n",
      "mammo-net for multi-view mammogram classiﬁcation\n",
      "75\n",
      "performance comparison.\n",
      "we also compare our model with other methods that use eye movement\n",
      "supervision as shown in table 1.\n",
      "we believe that one possible reason for the inferior\n",
      "performance of ga-net compared to mammo-net might be the use of a simple\n",
      "mse loss by ga-net, which neglects the coarse nature of the gaze data.\n",
      "[8] proposed a double-model that fuses gaze maps with original images\n",
      "before training.\n",
      "this model requires gaze input during both the training\n",
      "and inference stages, which limits its practical use in hospitals without eye-\n",
      "trackers.\n",
      "in contrast, our method does not rely on gaze input during inference\n",
      "stage.\n",
      "for each exam, we present gaze heat\n",
      "maps generated from eye movement data.\n",
      "the results of the visualization demonstrate that the model’s capability in\n",
      "localizing lesions becomes more precise when radiologist attention is incorpo-\n",
      "rated in the training stage.\n",
      "table 1\n",
      "suggests that each part of the proposed framework contributes to the increased\n",
      "performance.\n",
      "our\n",
      "experimental results on mammography datasets demonstrate the superiority of\n",
      "our proposed model.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf:\n",
      "the question of “what the symmet-\n",
      "rical bi-mg would look like when the asymmetrical abnormalities have\n",
      "been removed ?” has not yet received strong attention in the develop-\n",
      "ment of algorithms on mammograms.\n",
      "we conduct experiments on three pub-\n",
      "lic and one in-house dataset, and demonstrate that our method outper-\n",
      "forms existing methods in abnormality classiﬁcation, segmentation, and\n",
      "localization tasks.\n",
      "keywords: bilateral mammogram · asymmetric transformer ·\n",
      "disentanglement · self-adversarial learning · synthesis\n",
      "1\n",
      "introduction\n",
      "breast cancer (bc) is the most common cancer in women and incidence is\n",
      "increasing [14].\n",
      "besides using the data-driven manner, to\n",
      "achieve accurate diagnosis and interpretation of the ai-assisted system output,\n",
      "it is essential to consider mammogram domain knowledge in a model-driven\n",
      "fashion.\n",
      "it can\n",
      "provide valuable diagnostic information and guide the model in learning the\n",
      "diagnostic process akin to that of a human radiologist.\n",
      "recently, two studies\n",
      "explored generating healthy latent features of target mammograms by referenc-\n",
      "ing contralateral mammograms, achieving state-of-the-art (sota) classiﬁcation\n",
      "performance\n",
      "image generation techniques for gen-\n",
      "erating symmetric bi-mg have not yet been investigated.\n",
      "a more interpretable and pristine strategy is disentangle-\n",
      "ment learning [9,17] which utilizes synthetic images to supervise the model in\n",
      "separating asymmetric anomalies from normal regions at the image level.\n",
      "in this work, we present a novel end-to-end framework, disasymnet, which\n",
      "consists of an asymmetric transformer-based classiﬁcation (asyc) module and an\n",
      "asymmetric abnormality disentanglement (asyd) module.\n",
      "additionally, we leverage a\n",
      "self-adversarial learning scheme to reinforce two modules’ capacity, where the\n",
      "feedback from the asyc is used to guide the asyd’s disentangling, and the\n",
      "asyd’s output is used to reﬁne the asyc in detecting subtle abnormalities.\n",
      "to\n",
      "disentanglement of asymmetrical abnormality on bilateral mammograms\n",
      "59\n",
      "fig.\n",
      "the schematic overview of the proposed disasymnet.\n",
      "facilitate the learning of semantic symmetry, we also introduce synthesis, com-\n",
      "bining randomly created synthetic asymmetrical bi-mg with real mammograms\n",
      "to supervise the learning process.\n",
      "(3) we\n",
      "demonstrate the robustness of our approach on four mammogram datasets for\n",
      "classiﬁcation, segmentation, and localization tasks.\n",
      "we employ an online class activation mapping (cam) module\n",
      "[10,11] to generate heatmaps for segmentation and localization.\n",
      "the sa and ca modules use multi-head attention (mha),\n",
      "ψh=8\n",
      "mha(fq, fk, fv ) with the number of heads h = 8, which is a standard compo-\n",
      "nent in transformers and has already gained popularity in medical image ﬁelds\n",
      "[1,16,26].\n",
      "then, the starting feature f, and the attention fea-\n",
      "tures fsa and fca are concatenated in the channel dimension and fed into the\n",
      "ffn layers to fuse the information and maintain the same size as f. the trans-\n",
      "former block is repeated n = 12 times to iteratively integrate information from\n",
      "bi-mg, resulting in the output feature f r\n",
      "out, f l\n",
      "out = ψn=12\n",
      "asyt (f r, f l).\n",
      "unlike previous studies [18,19], which only generated normal features\n",
      "in the latent space, our asyd module use weights shared u-net-like decoders ψg,\n",
      "to generate both abnormal (xab) and normal (xn) images for each side through\n",
      "a two-channel separation, as xn, xab = ψg(fout).\n",
      "we constrain the model to\n",
      "reconstruct images realistically using l1 loss (ll1) with the guidance of cams\n",
      "(m), as follows, lrec = ll1((1−m)x, (1−m)xn)+ll1(mx, xab).\n",
      "however, it is\n",
      "disentanglement of asymmetrical abnormality on bilateral mammograms\n",
      "61\n",
      "diﬃcult to train the generator in a supervised manner due to the lack of annota-\n",
      "tions of the location for asymmetrical pairs.\n",
      "2.3\n",
      "asymmetric synthesis for supervised reconstruction\n",
      "to alleviate the lack of annotation pixel-wise asymmetry annotations, in this\n",
      "study, we propose a random synthesis method to supervise disentanglement.\n",
      "training with synthetic artifacts is a low-cost but eﬃcient way to supervise the\n",
      "model to better reconstruct images [15,17].\n",
      "the alpha weights αk is a 2d gaussian distribution map, in which the co-variance\n",
      "is determined by the size of k-th tumor t, representing the transparency of the\n",
      "pixels of the tumor.\n",
      "speciﬁcally, to maintain the rule of weakly-\n",
      "supervised learning of segmentation and localization tasks, we collect the tumors\n",
      "from the ddsm dataset as t and train the model on the inbreast dataset.\n",
      "thus, the supervised reconstruction loss is lsyn =\n",
      "ll1(x|real, xn|fake), where x|real is the real image before synthesis and xn|fake\n",
      "is the disentangled normal image from the synthesised image x|fake.\n",
      "2.4\n",
      "loss function\n",
      "for each training step, there are two objectives, training asyc and asyd mod-\n",
      "ule, and then is the reﬁnement of asyc.\n",
      "the values of weight terms\n",
      "λ1, λ2, λ3, and λ4 are experimentally set to be 1, 0.1, 1, and 0.5, respectively.\n",
      "the loss of the second objective is lrefine as aforementioned.\n",
      "3\n",
      "experimental\n",
      "3.1\n",
      "datasets\n",
      "this study reports experiments on four mammography datasets.\n",
      "the inbreast\n",
      "dataset [7] consists of 115 exams with bi-rads labels and pixel-wise anno-\n",
      "62\n",
      "x. wang et al.\n",
      "tations, comprising a total of 87 normal (bi-rads = 1) and 342 abnormal\n",
      "(bi-rads ̸= 1) images.\n",
      "the ddsm dataset [3] consists of 2,620 cases, encom-\n",
      "passing 6,406 normal and 4,042 (benign and malignant) images with outlines\n",
      "generated by an experienced mammographer.\n",
      "the vindr-mammo dataset [8]\n",
      "includes 5,000 cases with bi-rads assessments and bounding box annotations,\n",
      "consisting of 13,404 normal (bi-rads = 1) and 6,580 abnormal (bi-rads\n",
      "̸= 1) images.\n",
      "the in-house dataset comprises 43,258 mammography exams from\n",
      "10,670 women between 2004–2020, collected from a hospital with irb approvals.\n",
      "in this study, we randomly select 20% women of the full dataset, comprising 6,000\n",
      "normal (bi-rads = 1) and 28,732 abnormal (bi-rads ̸= 1) images.\n",
      "2. abnormality classiﬁcation performance of disasymnet in terms of auc trained\n",
      "on diﬀerent sizes of training sets.\n",
      "disentanglement of asymmetrical abnormality on bilateral mammograms\n",
      "63\n",
      "3.2\n",
      "experimental settings\n",
      "the mammogram pre-processing is conducted following the pipeline proposed by\n",
      "[5].\n",
      "then we standardize the image size to 1024 × 512 pixels.\n",
      "for training models,\n",
      "we employ random zooming and random cropping for data augmentation.\n",
      "we\n",
      "employ the resnet-18 [2] with on imagenet pre-trained weights as the common\n",
      "backbone for all methods.\n",
      "all experiments are implemented in\n",
      "the pytorch framework and an nvidia rtx a6000 gpu (48 gb).\n",
      "the training\n",
      "takes 3–24 h (related to the size of the dataset) on each dataset.\n",
      "to assess the performance of diﬀerent models in classiﬁcation tasks, we\n",
      "calculate the area under the receiver operating characteristic curve (auc) met-\n",
      "ric.\n",
      "for the segmentation task, we utilize intersec-\n",
      "tion over union (iou), intersection over reference (ior), and dice coeﬃcients.\n",
      "3.3\n",
      "experimental results\n",
      "we compare our proposed disasymnet with single view-based baseline\n",
      "resnet18,\n",
      "[16], and\n",
      "attention-based mv methods proposed by wang et al., [20] on classiﬁcation, seg-\n",
      "mentation, and localization tasks.\n",
      "the features from the bi-mg are simply concatenated and passed to\n",
      "the classiﬁer.\n",
      "comparison of performance in diﬀerent tasks: for the classiﬁcation\n",
      "task, the auc results of abnormal classiﬁcation are shown in table 1.\n",
      "additionally,\n",
      "our “asyd” only method improves the performance compared to the late-fusion\n",
      "method, demonstrating that our disentanglement-based self-adversarial learn-\n",
      "ing strategy can reﬁne classiﬁers and enhance the model’s ability to classify\n",
      "anomalies and asymmetries.\n",
      "the proposed “synthesis” method further enhances\n",
      "64\n",
      "x. wang et al.\n",
      "table 2. comparison of weakly supervised abnormalities segmentation and localization\n",
      "tasks on public datasets.\n",
      "segmentation task\n",
      "localization task\n",
      "inbreast\n",
      "inbreast\n",
      "ddsm\n",
      "vindr-mammo\n",
      "methods\n",
      "iou\n",
      "ior\n",
      "dice\n",
      "mean tiou mean tior mean tiou mean tior mean tiou mean tior\n",
      "resnet18\n",
      "(color ﬁgure online)\n",
      "the performance of our proposed method.\n",
      "moreover, we investigate the abil-\n",
      "ity of diﬀerent methods to classify abnormalities under various percentages\n",
      "of ddsm, vindr, and in-house datasets.\n",
      "the inbreast dataset was excluded\n",
      "from this experiment due to its small size.\n",
      "figure 2 illustrates the robustness of\n",
      "our method’s advantage and our approach consistently outperformed the other\n",
      "methods, regardless of the size of the training data used and data sources.\n",
      "for\n",
      "the weakly supervised segmentation and localization tasks, results are shown in\n",
      "table 2.\n",
      "the results demonstrate that our proposed framework achieves superior\n",
      "segmentation and localization performance compared to other existing methods\n",
      "across all evaluation metrics.\n",
      "the results of the ablation experiment also reveal\n",
      "that all modules incorporated in our framework oﬀer improvements for the tasks.\n",
      "without using pixel-level asymmetry ground\n",
      "disentanglement of asymmetrical abnormality on bilateral mammograms\n",
      "65\n",
      "truth from the “synthesis” method, our generator tends to excessively remove\n",
      "asymmetric abnormalities at the cost of leading to the formation of black holes or\n",
      "areas that are visibly darker than the surrounding tissue because of the limitation\n",
      "of our discriminator and lack of pixel-level supervision.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf:\n",
      "despite recent developments in ct planning that enabled\n",
      "automation in patient positioning, time-consuming scout scans are still\n",
      "needed to compute dose proﬁle and ensure the patient is properly posi-\n",
      "tioned.\n",
      "in this paper, we present a novel method which eliminates the\n",
      "need for scout scans in ct lung cancer screening by estimating patient\n",
      "scan range, isocenter, and water equivalent diameter (wed) from 3d\n",
      "camera images.\n",
      "we demonstrate the eﬀec-\n",
      "tiveness of our method on a testing set of 110 pairs of depth data and ct\n",
      "scan, resulting in an average error of 5 mm in estimating the isocenter,\n",
      "13 mm in determining the scan range, 10 mm and 16 mm in estimating\n",
      "the ap and lateral wed respectively.\n",
      "ct lung cancer screening is a low-\n",
      "dose ct (ldct) scan of the chest that can detect lung cancer at an early stage,\n",
      "when it is most treatable.\n",
      "however, the current workﬂow for performing ct lung\n",
      "scans still requires an experienced technician to manually perform pre-scanning\n",
      "steps, which greatly decreases the throughput of this high volume procedure.\n",
      "while recent advances in human body modeling [4,5,12,13,15] have allowed for\n",
      "automation of patient positioning, scout scans are still required as they are used\n",
      "by automatic exposure control system in the ct scanners to compute the dose\n",
      "to be delivered in order to maintain constant image quality [3].\n",
      "any patient movement during the time between the two scans may cause mis-\n",
      "alignment and incorrect dose proﬁle, which could ultimately result in a repeat\n",
      "of the entire process.\n",
      "we introduce a novel method for estimating patient scanning parameters\n",
      "from non-ionizing 3d camera images to eliminate the need for scout scans dur-\n",
      "ing pre-scanning.\n",
      "addi-\n",
      "tionally, we introduce a novel approach for updating the estimated wed in\n",
      "real-time, which allows for reﬁnement of the scan parameters during acquisition,\n",
      "thus increasing accuracy.\n",
      "the contributions of this work can be summarized as follows:\n",
      "– a novel workﬂow for automated ct lung cancer screening without the need\n",
      "for scout scan\n",
      "– a clinically relevant method meeting iec 62985:2019 requirements on wed\n",
      "estimation.\n",
      "– a generative model of patient wed trained on over 60, 000 patients.\n",
      "– a novel method for real-time reﬁnement of wed, which can be used for dose\n",
      "modulation\n",
      "2\n",
      "method\n",
      "water equivalent diameter (wed) is a robust patient-size descriptor [17] used\n",
      "for ct dose planning.\n",
      "it represents the diameter of a cylinder of water having\n",
      "the same averaged absorbed dose as the material contained in an axial plane at a\n",
      "given craniocaudal position z\n",
      "while wed can be derived from ct images, paired ct scans and camera\n",
      "images are rarely available, making direct regression through supervised learning\n",
      "challenging.\n",
      "we propose a semi-supervised approach to estimate wed from\n",
      "depth images.\n",
      "we then train an encoder network to map the patient depth image to\n",
      "the wed manifold.\n",
      "426\n",
      "b. teixeira et al.\n",
      "2.2\n",
      "depth encoder training\n",
      "after training our generative model on a large collection of unpaired ct scans,\n",
      "we train our encoder network on a smaller collection of paired depth images\n",
      "and ct scans.\n",
      "[1] taking as input\n",
      "the depth image and outputting a latent vector in the previously learned latent\n",
      "space.\n",
      "2.3\n",
      "real-time wed reﬁnement\n",
      "while the depth image provides critical information on the patient anatomy,\n",
      "it may not always be suﬃcient to accurately predict the wed proﬁles.\n",
      "for\n",
      "example, some patients may have implants or other medical devices that cannot\n",
      "be guessed solely from the depth image.\n",
      "additionally, since the encoder is trained\n",
      "on a smaller data collection, it may not be able to perfectly project the depth\n",
      "image to the wed manifold.\n",
      "first, we use our encoder network to initialize\n",
      "the latent vector to a point in the manifold that is close to the current patient.\n",
      "after the latent vector\n",
      "has been optimized to ﬁt the previously scanned data, a large deviation between\n",
      "the optimized prediction and the ground truth proﬁles may indicate that our\n",
      "approach is not able to ﬁnd a point in the manifold that is close to the data.\n",
      "(color ﬁgure online)\n",
      "of depth image and ct scan from 2, 742 patients from 6 diﬀerent sites across\n",
      "north america and europe acquired using a ceiling-mounted kinect 2 camera.\n",
      "our evaluation set consists of 110 pairs of depth image and ct scan from 110\n",
      "patients from a separate site in europe.\n",
      "3.2\n",
      "patient preparation\n",
      "patient positioning is the ﬁrst step in lung cancer screening workﬂow.\n",
      "[7] taking the camera depth\n",
      "image as input and outputting a gaussian heatmap centered at the patient’s lung\n",
      "top location.\n",
      "to ensure the lung is fully visible in the ct image, we\n",
      "added a 2 cm oﬀset on our prediction towards the outside of the lung.\n",
      "we then\n",
      "deﬁned the accuracy as whether the lung is fully visible in the ct image when\n",
      "using the oﬀset prediction.\n",
      "wed proﬁle regression with and without real-\n",
      "time reﬁnement.\n",
      "second column shows the performance of our model without real-time reﬁnement.\n",
      "third and fourth columns show the performance of our model with real-time reﬁnement\n",
      "every 5 cm and 2 cm respectively.\n",
      "while the original prediction was oﬀ towards the center of the lung,\n",
      "the real-time reﬁnement was able to correct the error.\n",
      "isocenter.\n",
      "[1] taking the camera depth image as input and\n",
      "outputting the patient isocenter.\n",
      "the encoder was trained on our paired\n",
      "ct scan and depth image dataset of 2, 742 patients.\n",
      "[7]\n",
      "taking the camera depth image as input and outputting the water equivalent\n",
      "diameter proﬁle.\n",
      "method (lateral)\n",
      "mean error 90th perc error max error\n",
      "direct regression\n",
      "45.07\n",
      "76.70\n",
      "101.50\n",
      "proposed (initial)\n",
      "27.06\n",
      "52.88\n",
      "79.27\n",
      "proposed (reﬁned, w = 5) 19.18\n",
      "42.44\n",
      "73.69\n",
      "proposed (reﬁned, w = 2) 15.93\n",
      "35.93\n",
      "61.68\n",
      "method (ap)\n",
      "direct regression\n",
      "45.71\n",
      "71.85\n",
      "82.84\n",
      "proposed (initial)\n",
      "16.52\n",
      "31.00\n",
      "40.89\n",
      "proposed (reﬁned, w = 5) 12.19\n",
      "25.73\n",
      "37.36\n",
      "proposed (reﬁned, w = 2) 10.40\n",
      "22.44\n",
      "33.85\n",
      "then measured the performance of our model before and after diﬀerent degrees\n",
      "of real-time reﬁnement, using the same optimizer and learning rate.\n",
      "bringing in real-time reﬁnement greatly improves the results with a\n",
      "mean lateral error over 40% and a 90th percentile lateral error over 20% lower\n",
      "than before reﬁnement.\n",
      "ap proﬁles show similar results with a mean ap error\n",
      "improvement of nearly 40% and a 90th percentile ap error improvement close\n",
      "to 30%.\n",
      "when using our proposed method with a 20 mm window reﬁnement,\n",
      "our proposed approach outperforms the direct regression baseline by over 60%\n",
      "for lateral proﬁle and nearly 80% for ap.\n",
      "figures 3 highlights the beneﬁts of using real-time reﬁnement.\n",
      "4. qualitative analysis of the proposed method with 2 cm reﬁnement on patient\n",
      "with diﬀerent morphology.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_54.pdf:\n",
      "colorectal polyps detected during colonoscopy are strongly\n",
      "associated with colorectal cancer, making polyp segmentation a critical\n",
      "clinical decision-making tool for diagnosis and treatment planning.\n",
      "how-\n",
      "ever, accurate polyp segmentation remains a challenging task, particu-\n",
      "larly in cases involving diminutive polyps and other intestinal substances\n",
      "that produce a high false-positive rate.\n",
      "previous polyp segmentation net-\n",
      "works based on supervised binary masks may have lacked global seman-\n",
      "tic perception of polyps, resulting in a loss of capture and discrimination\n",
      "capability for polyps in complex scenarios.\n",
      "to address this issue, we\n",
      "propose a novel gaussian-probabilistic guided semantic fusion method\n",
      "that progressively fuses the probability information of polyp positions\n",
      "with the decoder supervised by binary masks.\n",
      "extensive experiments on ﬁve widely adopted datasets\n",
      "show that petnet outperforms existing methods in identifying polyp\n",
      "t. ling and c. wu—equal contributions.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43990-2_54.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43990-2_54\n",
      "petnet improves complex polyp segmentation\n",
      "573\n",
      "camouﬂage, appearance changes, and small polyp scenes, and achieves a\n",
      "speed about 27fps in edge computing devices.\n",
      "keywords: colonoscopy · polyp segmentation · vision transformer\n",
      "1\n",
      "introduction\n",
      "colorectal cancer (crc) remains a major health burden with elevated mortal-\n",
      "ity worldwide [1].\n",
      "polyp segmentation is a fundamental task\n",
      "in the computer-aided detection (cade) of polyps during colonoscopy, which is\n",
      "of great signiﬁcance in the clinical prevention of crc.\n",
      "traditional machine learning approaches in polyp segmentation primarily\n",
      "focus on learning low-level features, such as texture, shape, or color distribu-\n",
      "tion\n",
      "furthermore, transformer [3,17,19,20] models have also been proposed for\n",
      "polyp segmentation, and achieve the state-of-the-art(sota) performance.\n",
      "however, supervised segmentation learning solely based on binary\n",
      "masks may not be eﬀective in discriminating polyps in complex clinical scenarios.\n",
      "endoscopic images often contain pseudo-polyp objects with strong boundaries,\n",
      "such as colon folds, blood vessels, and air bubbles, which can result in false\n",
      "positives.\n",
      "however, this method has limitations in accurately segmenting\n",
      "polyp boundaries, which are crucial for clinical decision-making.\n",
      "therefore, the primary challenge lies in enhancing polyp segmentation per-\n",
      "formance in complex scenarios by precisely preserving the polyp segmentation\n",
      "boundaries, while simultaneously maximizing the decoder’s attention on the\n",
      "overall pattern of the polyps.\n",
      "in this paper, we propose a novel transformer-based polyp segmentation\n",
      "framework, petnet, which addresses the aforementioned challenges and achieves\n",
      "sota performance in locating polyps with high precision.\n",
      "our contributions are\n",
      "threefold:\n",
      "• we propose a novel gaussian-probabilistic guided semantic fusion method for\n",
      "polyp segmentation, which improves the decoder’s global perception of polyp\n",
      "locations and discrimination capability for polyps in complex scenarios.\n",
      "574\n",
      "t. ling et al.\n",
      "• we evaluate the performance of petnet on ﬁve widely adopted datasets,\n",
      "demonstrating its superior ability to identify polyp camouﬂage and small\n",
      "polyp scenes, achieving state-of-the-art performance in locating polyps with\n",
      "high precision.\n",
      "• we design several polyp instance-level evaluation metrics, considering that\n",
      "conventional pixel-level calculation methods cannot explicitly and compre-\n",
      "hensively evaluate the overall performance of polyp segmentation algorithms.\n",
      "1, petnet is an end-to-end polyp segmentation framework con-\n",
      "sists of three core module groups.\n",
      "(b) depicts the stage encoder.\n",
      "petnet improves complex polyp segmentation\n",
      "575\n",
      "fig.\n",
      "we add a\n",
      "mta layer to encode the last level features, enhancing the model’s semantic\n",
      "representation and accelerating the training process [16].\n",
      "moreover, the encoder\n",
      "output features are presented as {xe\n",
      "i }4\n",
      "i=1 with channels of [2c, 4c, 8c, 16c].\n",
      "2.3\n",
      "gaussian-probabilistic modeling group\n",
      "to incorporate both polyp location probability and surface pattern information\n",
      "in a progressive manner, we propose the gaussian probabilistic-induced tran-\n",
      "sition (git) method.\n",
      "+ (y − yo)2\u0003\u0003\n",
      "(1)\n",
      "where (xo, yo) is the mass of each polyp in the binary image f(x, y).\n",
      "finally, we determine the ﬁnal gaussian\n",
      "probabilistic mask pg for all polyps within an image mask by computing the\n",
      "element-wise maximum.\n",
      "∈ r(ci+cg)×hi×wi in an multi-layer sandwiches manner.\n",
      "we further introduce residual learn-\n",
      "ing in a parallel manner at diﬀerent group-aware scales.\n",
      "building on this manner, we pro-\n",
      "pose the ensemble method that integrates multiple simple decoders to enhance\n",
      "petnet improves complex polyp segmentation\n",
      "577\n",
      "the detection and discrimination of diﬃcult polyp samples.\n",
      "the output mask p is obtained by element-wise\n",
      "summation of pi, where i represents the binary decoder index.\n",
      "in our evaluation, we also examine the decoder cfm utilized in [3],\n",
      "which shares the same input features (excluding the ﬁrst level) as the fus.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "datasets settings\n",
      "to evaluate models fairly, we completely follow pranet\n",
      "[4] and use ﬁve public\n",
      "datasets, including 548 and 900 images from clinicdb\n",
      "[5]\n",
      "as training sets, and the remaining images as validation sets.\n",
      "we also test the\n",
      "generalization capability of all models on three unseen datasets (etis [13] with\n",
      "196 images, cvc-colondb\n",
      "[8] with 380 images, and endoscene\n",
      "[15] with 60\n",
      "images).\n",
      "further-\n",
      "more, we employ intermediate decoder outputs to calculate auxiliary losses for\n",
      "convergence acceleration.\n",
      "578\n",
      "t. ling et al.\n",
      "3.3\n",
      "evaluation metrics\n",
      "conventional evaluation metrics for polyp segmentation are typically limited to\n",
      "pixel-level calculations.\n",
      "false positives (fps) occur when a wrong detection\n",
      "output is provided for a negative region, and false negatives (fns) occur\n",
      "when a polyp is missed in a positive image.\n",
      "table s1 displays the results of our model’s\n",
      "training and learning performance.\n",
      "our model achieves comparable performance\n",
      "to the sota model on the kvasir-seg and clinicdb datasets.\n",
      "results show that\n",
      "petnet achieves excellent generalization performance compared with previous\n",
      "models.\n",
      "we also observe a\n",
      "performance mismatch phenomenon in pixel-level evaluation and instance-level\n",
      "evaluation.\n",
      "we selected images from two unseen datasets with 0∼2% polyp labeled area\n",
      "to perform the test.\n",
      "as shown, petnet demonstrates great strength in both\n",
      "datasets, which indicates that one of the major advantages of our model lies in\n",
      "detecting small polyps with lower false-positive rates.\n",
      "table 3 presents the results of our ablation study, where we\n",
      "investigate the contribution of the two key components of our model, namely the\n",
      "gaussian-probabilistic guided semantic fusion method and ensemble decoders.\n",
      "we observe that while the impact of each binary decoder varies, all sub binary\n",
      "decoders contribute to the overall performance.\n",
      "furthermore, the git method\n",
      "signiﬁcantly enhances instance-level evaluation without incurring performance\n",
      "penalty in pixel-level evaluation, especially in unseen datasets.\n",
      "petnet improves complex polyp segmentation\n",
      "579\n",
      "table 1.\n",
      "small polyps are deﬁned as the polyp area accounts for 0∼2% of the\n",
      "entire image.\n",
      ", we deployed petnet on the edge computing device nvidia jetson\n",
      "orin and optimized its performance using tensorrt.\n",
      "our results demonstrate\n",
      "that petnet achieves real-time denoising and segmentation of polyps with high\n",
      "accuracy, achieving a speed of 27 frames per second on the device(video s1).\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf:\n",
      "age-related macular degeneration (amd) is the leading\n",
      "cause of blindness in the elderly.\n",
      "current grading systems based on imag-\n",
      "ing biomarkers only coarsely group disease stages into broad categories\n",
      "that lack prognostic value for future disease progression.\n",
      "in quantitative experi-\n",
      "ments we found our method yields temporal biomarkers that are predic-\n",
      "tive of conversion to late amd.\n",
      "furthermore, these clusters were highly\n",
      "interpretable to ophthalmologists who conﬁrmed that many of the clus-\n",
      "ters represent dynamics that have previously been linked to the progres-\n",
      "sion of amd, even though they are currently not included in any clinical\n",
      "grading system.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43990-2_68.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43990-2_68\n",
      "clustering disease trajectories for temporal biomarker proposal in amd\n",
      "725\n",
      "keywords: contrastive learning · biomarker discovery · clustering ·\n",
      "disease trajectories · age-related macular degeneration\n",
      "1\n",
      "introduction\n",
      "age-related macular degeneration (amd) is the leading cause of blindness in the\n",
      "elderly, aﬀecting nearly 200 million people worldwide [24].\n",
      "patients with early\n",
      "stages of the disease exhibit few symptoms until suddenly converting to the late\n",
      "stage, at which point their central vision rapidly deteriorates\n",
      "[12]. clinicians\n",
      "currently diagnose amd, and stratify patients, using biomarkers derived from\n",
      "optical coherence tomography (oct), which provides high-resolution images of\n",
      "fig.\n",
      "1. our method ﬁnds common patterns of disease progression in datasets of lon-\n",
      "gitudinal images.\n",
      "others have proposed deep-learning-\n",
      "based methods to discover new biomarkers at scale by clustering oct images\n",
      "or detecting anomalous features [17,18,23].\n",
      "however, these approaches neglect\n",
      "temporal relationships between images and the obtained biomarkers are by def-\n",
      "inition static and cannot capture the dynamic nature of the disease.\n",
      "in experiments\n",
      "involving 160,558 retinal scans, four ophthalmologists veriﬁed that our method\n",
      "identiﬁed several candidates for temporal biomarkers of amd.\n",
      "moreover, our\n",
      "clusters demonstrated greater prognostic value for late-stage amd when com-\n",
      "pared to the widely adopted amd grading system.\n",
      "the established\n",
      "amd grading system stratiﬁes early and intermediate stages solely by the size\n",
      "of drusen in a single oct image [1,6,7,10].\n",
      "the degree of atrophy can be staged using crora (complete retinal pigment\n",
      "epithelium and outer retinal atrophy), which measures the width in μm of focal\n",
      "atrophy in oct\n",
      "however, this only works if biomarkers are known\n",
      "a priori and requires manual annotation of entire time series.\n",
      "automated discovery of unknown biomarkers: prior work for automated\n",
      "biomarker discovery in amd explores the latent feature space of encoders trained\n",
      "for image reconstruction [18,23], segmentation\n",
      "however, these neural networks are prone to overﬁt to their speciﬁc\n",
      "task and lose semantic information regarding the disease.\n",
      "contrastive methods\n",
      "[3,8,26] encode invariance to a set of image transformations, which are uncorre-\n",
      "lated with disease features, resulting in a more expressive feature space.\n",
      "however, all aforementioned methods group single images acquired at one point\n",
      "in time, and in doing so neglect temporal dynamics.\n",
      "[5].\n",
      "3\n",
      "materials and methods\n",
      "3.1\n",
      "oct image datasets\n",
      "we use two retinal oct datasets curated in the scope of the pinnacle study\n",
      "[20].\n",
      "we ﬁrst design and test our method on a development dataset, which was\n",
      "collected from the southampton eye unit.\n",
      "all images were acquired using topcon 3d oct devices (topcon cor-\n",
      "poration, tokyo, japan).\n",
      "after strict quality control, the development dataset\n",
      "consists of 46,496 scans of 6,236 eyes from 3,456 patients.\n",
      "eyes were scanned 7.7\n",
      "times over 1.9 years on average at irregular time intervals.\n",
      "eyes were\n",
      "scanned 16.6 times over 3.5 years on average.\n",
      "[8] in eq. 1 to train a resnet50 (4x) model f\n",
      "over each batch of twice transformed images x\n",
      "l(x)\n",
      "= 2 − 2\n",
      "⟨f(x), f ′(x)⟩\n",
      "||f(x)||2 · ||f ′(x)||2\n",
      "(1)\n",
      "where the output of the momentum updated ‘teacher’ network f ′ is passed\n",
      "through a stop-gradient, so that only the student network f is updated.\n",
      "as\n",
      "several of the contrastive transformations designed for natural images are inap-\n",
      "plicable to medical images, such as solarisation, colour shift and greyscale, we\n",
      "use the set tailored for retinal oct images by holland et al.\n",
      "[9]. models were\n",
      "trained on the entire dataset for 120,000 steps using the adam optimiser with a\n",
      "momentum of 0.9, weight decay of 1.5·10−6 and a learning rate of 5·10−4.\n",
      "after\n",
      "training f, we ﬁrst remove the ﬁnal linear layer before projecting all labelled\n",
      "images to the feature space of 2048 dimensions.\n",
      "fig.\n",
      "we illustrate clusters assignments, denoted by colour,\n",
      "resulting from three combinations of φ and λ.\n",
      "3.3\n",
      "extracting sub-trajectories via partitioning\n",
      "naively clustering whole time series of patients ignores two characteristics of\n",
      "longitudinal data.\n",
      "firstly, individual time series are not directly comparable as\n",
      "patients enter and leave the study at diﬀerent stages of their overall progression.\n",
      "secondly, longer time series can record multiple successive transitions in disease\n",
      "stage.\n",
      "for each eye, we ﬁrst form piecewise-linear trajectories by linking points in fea-\n",
      "ture space that were derived from consecutively acquired oct images.\n",
      "we then\n",
      "extract sub-trajectories by ﬁnding all sequences of images spanning 1.0 ± 0.5\n",
      "years of elapsed time within each trajectory.\n",
      "next, to avoid oversampling trajec-\n",
      "tories with a shorter time interval between images, we randomly sample at most\n",
      "one sub-trajectory in every 0.5-year time interval.\n",
      "however, by ignoring intermediary images, this metric does not respect the\n",
      "disease pathway along which patients progress.\n",
      "dtw ﬁnds the optimal temporal alignment between\n",
      "two time series before computing their distance.\n",
      "this re-alignment allows us to\n",
      "match sub-trajectories that traverse the same disease states in the same order,\n",
      "irrespective of the rate of change between states.\n",
      "3. we show four clusters from the development dataset (left half) and the equiv-\n",
      "alent clusters in the unseen dataset (right half).\n",
      "clusters show two representative sub-trajectories originating from\n",
      "diﬀerent patients, each containing ﬁve longitudinal images with the time and location\n",
      "of greatest progression marked by arrows.\n",
      "3.5\n",
      "qualitative and quantitative evaluation of clusters\n",
      "initially, we tune the hyperparameters, λ, φ and k, on the development dataset\n",
      "by heuristically selecting values that result in higher uniformity between sub-\n",
      "trajectories within each cluster.\n",
      "the oph-\n",
      "thalmologists then review these clusters and conﬁrm whether they capture the\n",
      "clustering disease trajectories for temporal biomarker proposal in amd\n",
      "731\n",
      "same temporal biomarkers observed in the development dataset.\n",
      "we also\n",
      "include a demographic baseline using age and sex.\n",
      "finally, to demonstrate\n",
      "the performance gap between our interpretable approach and black-box super-\n",
      "vised learning algorithms, we include a fully supervised deep learning baseline\n",
      "by ﬁtting an svr directly to the feature space.\n",
      "each experiment uses 10-fold\n",
      "cross validation on random 80/20 partitions, while ensuring a patient-wise split.\n",
      "finally, we repeat the entire method, starting from sub-trajectory extraction,\n",
      "followed by clustering and then regression experiments, using 7 random seeds\n",
      "and report the means and standard deviations.\n",
      "development dataset\n",
      "time to late amd ↓ time to cnv ↓ time to crora ↓ current visual acuity ↓\n",
      "demographic\n",
      "0.756±0.01\n",
      "0.822±0.012\n",
      "0.703±0.028\n",
      "0.381±0.007\n",
      "current grading system\n",
      "0.757±0.01\n",
      "0.819±0.012\n",
      "0.685±0.035\n",
      "0.367±0.008\n",
      "single timepoint clusters\n",
      "0.747±0.013\n",
      "0.776±0.015\n",
      "0.630±0.05\n",
      "0.230±0.005\n",
      "sub-trajectory clusters\n",
      "0.739±0.01\n",
      "0.748±0.011\n",
      "0.636±0.031\n",
      "0.375±0.007\n",
      "fully supervised\n",
      "0.709±0.015\n",
      "0.726±0.012\n",
      "0.609±0.033\n",
      "0.199±0.004\n",
      "unseen dataset\n",
      "demographic\n",
      "1.343±0.027\n",
      "1.241±0.017\n",
      "1.216±0.062\n",
      "0.188±0.007\n",
      "current grading system\n",
      "1.308±0.018\n",
      "1.244±0.022\n",
      "1.286±0.053\n",
      "0.177±0.008\n",
      "single timepoint clusters\n",
      "1.325±0.049\n",
      "1.341±0.080\n",
      "1.297±0.096\n",
      "0.136±0.005\n",
      "sub-trajectory clusters\n",
      "1.322±0.029\n",
      "1.235±0.027\n",
      "1.257±0.056\n",
      "0.188±0.006\n",
      "fully supervised\n",
      "1.301±0.044\n",
      "1.298±0.08\n",
      "1.255±0.097\n",
      "0.135±0.006\n",
      "4\n",
      "experiments and results\n",
      "sub-trajectory clusters are candidate temporal biomarkers: by ﬁrst\n",
      "applying our method to the development dataset we found that using λ = 0.75,\n",
      "φ = 0.75 and k = 30 resulted in the most uniform and homogeneous clusters\n",
      "while still limiting the total number of clusters to a reasonable amount.\n",
      "achiev-\n",
      "ing the same cluster quality with smaller values of φ required many more clusters\n",
      "in order to encode all combinations of possible start and end disease states.\n",
      "the\n",
      "expert ophthalmologists remarked that many of the identiﬁed clusters capture\n",
      "732\n",
      "r. holland et al.\n",
      "dynamics that have already been linked to the progression of amd, even though\n",
      "they are not currently included in any clinical grading system.\n",
      "they named these as ‘rapid growth of drusen pig-\n",
      "ment epithelial detachments (ped)’, ‘regression of drusen ped’, ‘development\n",
      "of subretinal ﬂuid’, ‘development of intraretinal ﬂuid’, ‘development of hyper-\n",
      "transmission’ and ‘stable state’ (no signs of progression at each disease state).\n",
      "in all tasks the standard biomarkers are only marginally more indicative of risk\n",
      "than the patient’s age and sex.\n",
      "this experiment conﬁrms that our clusters are\n",
      "related to disease progression.\n",
      "5\n",
      "discussion and conclusion\n",
      "motivated to improve inadequate grading systems for amd that do not incor-\n",
      "porate temporal dynamics we developed a method to automatically propose\n",
      "biomarkers that are time-dependent, interpretable, and predictive of conversion\n",
      "to late-stage amd.\n",
      "furthermore, we experimentally demonstrated that the found clusters predict\n",
      "conversion to late-stage amd on par with the established grading system.\n",
      "in the future, biomarkers identiﬁed by our method can be further reﬁned\n",
      "by clinicians.\n",
      "we will also use the full volumetric image to model progression\n",
      "dynamics outside the macular.\n",
      "as late stage patients were overrepresented in our\n",
      "datasets, we also intend to apply our method to datasets with greater numbers\n",
      "of patients progressing from earlier disease stages.\n",
      "ultimately, we envision that\n",
      "proposals from our method may inform the next generation of grading systems\n",
      "for amd that incorporate the temporal dimension intrinsic to this dynamic dis-\n",
      "ease.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf:\n",
      "multi-organ segmentation in abdominal computed tomog-\n",
      "raphy (ct) images is of great importance for diagnosis of abdominal\n",
      "lesions and subsequent treatment planning.\n",
      "though deep learning based\n",
      "methods have attained high performance, they rely heavily on large-\n",
      "scale pixel-level annotations that are time-consuming and labor-intensive\n",
      "to obtain.\n",
      "due to its low dependency on annotation, weakly supervised\n",
      "segmentation has attracted great attention.\n",
      "however, there is still a large\n",
      "performance gap between current weakly-supervised methods and fully\n",
      "supervised learning, leaving room for exploration.\n",
      "in this work, we pro-\n",
      "pose a novel 3d framework with two consistency constraints for scribble-\n",
      "supervised multiple abdominal organ segmentation from ct. speciﬁ-\n",
      "cally, we employ a triple-branch multi-dilated network (tdnet) with\n",
      "one encoder and three decoders using diﬀerent dilation rates to cap-\n",
      "ture features from diﬀerent receptive ﬁelds that are complementary to\n",
      "each other to generate high-quality soft pseudo labels.\n",
      "experiments on\n",
      "the public word dataset show that our method outperforms ﬁve exist-\n",
      "ing scribble-supervised methods.\n",
      "keywords: weakly-supervised learning · scribble annotation ·\n",
      "uncertainty · consistency\n",
      "1\n",
      "introduction\n",
      "abdominal organ segmentation from medical images is an essential work in clin-\n",
      "ical diagnosis and treatment planning of abdominal lesions [17].\n",
      "https://doi.org/10.1007/978-3-031-43990-2_4\n",
      "34\n",
      "m. han et al.\n",
      "learning methods based on convolution neural network (cnn) have achieved\n",
      "impressive performance in medical image segmentation tasks [2,24].\n",
      "[5], and image-\n",
      "level tags\n",
      "compared with the other weak annotations, scribbles can\n",
      "provide more location information about the segmentation targets, especially\n",
      "for objects with irregular shapes [1].\n",
      "therefore, this work focuses on exploring\n",
      "high-performance models for multiple abdominal organ segmentation based on\n",
      "scribble annotations.\n",
      "training cnns for segmentation with scribble annotations has been increas-\n",
      "ingly studied recently.\n",
      "pseudo label learning methods deal with unannotated pixels by generating fake\n",
      "semantic labels for learning.\n",
      "[11] proposed to leverage minimum span-\n",
      "ning trees to generate low-level and high-level aﬃnity matrices based on color\n",
      "information and semantic features to reﬁne the pseudo labels.\n",
      "[22] introduced the condi-\n",
      "tional random field (crf) regularization loss for image segmentation directly.\n",
      "recently,\n",
      "consistency strategies that encourage consistent outputs of the network for the\n",
      "same input under diﬀerent perturbations have achieved increasing attentions.\n",
      "[26] proposed a framework composed\n",
      "of mix augmentation and cycle consistency.\n",
      "although these scribble-supervised\n",
      "methods have achieved promising results, their performance is still much lower\n",
      "than that of fully-supervised training, leaving room for improvement.\n",
      "diﬀerently from most existing weakly supervised methods that are designed\n",
      "for 2d slice segmentation with a single or few organs, we propose a highly opti-\n",
      "mized 3d triple-branch network with one encoder and three diﬀerent decoders,\n",
      "named tdnet, to learn from scribble annotations for segmentation of multiple\n",
      "abdominal organs.\n",
      "particularly, the decoders are assigned with diﬀerent dilation\n",
      "rates [25] to learn features from diﬀerent receptive ﬁelds that are complementary\n",
      "to each other for segmentation, which also improves the robustness of dealing\n",
      "with organs at diﬀerent scales as well as the feature learning ability of the shared\n",
      "encoder.\n",
      "in addition, we extend the consistency to the class-related information\n",
      "scribble-based 3d multiple abdominal organ segmentation\n",
      "35\n",
      "ℒ\n",
      "ℒ\n",
      "uncertainty \n",
      "rectified\n",
      "ℒ\n",
      "ℒ\n",
      "image\n",
      "scribble\n",
      "1\n",
      "2\n",
      "encoder\n",
      "decoder \n",
      "1, \n",
      "= 1\n",
      "decoder \n",
      ", \n",
      "= 6\n",
      "decoder \n",
      "2, \n",
      "= 3\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "ℒ\n",
      "ℒ\n",
      "ℒ\n",
      "−\n",
      "−\n",
      "×\n",
      "∗ ×\n",
      "×\n",
      "× (∗)\n",
      "project in sagittal view\n",
      "reshape matrix\n",
      "×\n",
      "multiply operation\n",
      "transpose matrix \n",
      "(b) class affinity calculation\n",
      "(a) the proposed tdnet\n",
      "project in coronal view\n",
      "project in axial view\n",
      "class affinity calculation\n",
      "fig.\n",
      "speciﬁ-\n",
      "cally, we generate the class aﬃnity matrices in diﬀerent decoders and encourage\n",
      "them to be consistent after projection in diﬀerent views.\n",
      "the contributions of this paper are summarized as follows: 1) we propose\n",
      "a novel 3d triple-branch multi-dilated network called tdnet for scribble-\n",
      "supervised segmentation.\n",
      "by equipping with varying dilation rates, the network\n",
      "can better leverage multi-scale context for dealing with organs at diﬀerent scales.\n",
      "2) we propose two novel consistency loss functions, i.e., uncertainty-weighted\n",
      "soft pseudo label consistency (uspc) loss and multi-view projection-based\n",
      "class-similarity consistency (mpcc) loss, to regularize the prediction from the\n",
      "pixel-wise and class-wise perspectives respectively, which helps the segmenta-\n",
      "tion network obtain reliable predictions on unannotated pixels.\n",
      "3) experiments\n",
      "results show our proposed method outperforms ﬁve existing scribble-supervised\n",
      "methods on the public dataset word [17] for multiple abdominal organ seg-\n",
      "mentation.\n",
      "2\n",
      "method\n",
      "figure 1 shows the proposed framework for scribble-supervised medical image\n",
      "segmentation.\n",
      "the decoders’ outputs are\n",
      "averaged to generate a soft pseudo label that is rectiﬁed by uncertainty and then\n",
      "used to supervise each branch.\n",
      "to better deal with multi-class segmentation, a\n",
      "class similarity consistency loss is also used for regularization.\n",
      "let x, s be a training image and the corresponding scribble\n",
      "annotation, respectively.\n",
      "let c denote the number of classes for segmentation,\n",
      "and ω = ωs ∪ ωu denote the whole set of voxels in x, where ωs is the set of\n",
      "labeled pixels annotated in s, and ωu is the unlabeled pixel set.\n",
      "decoders using con-\n",
      "volution with large dilation rates can better leverage the global information but\n",
      "may lose some details for accurate segmentation.\n",
      "in this work, our tdnet is\n",
      "implemented by introducing two auxiliary decoders into a 3d unet\n",
      "as the\n",
      "three decoders capture features at diﬀerent scales that are complementary to\n",
      "each other, an ensemble of them would be more robust than a single branch.\n",
      "therefore, we take an average of p1, p2, p3 to get a better soft pseudo label\n",
      "¯p = (p1 + p2 + p3)/3 that is used to supervise each branch during training.\n",
      "kl() is the kullback-\n",
      "scribble-based 3d multiple abdominal organ segmentation\n",
      "37\n",
      "leibler divergence.\n",
      "multi-view projection-based class-similarity consistency (mpcc).\n",
      "for multi-class segmentation tasks, it is important to learn inter-class relation-\n",
      "ship for better distinguishing them.\n",
      "1. in order to save computing\n",
      "resources, we project the soft pseudo labels along each dimension and then cal-\n",
      "culate the aﬃnity matrices, which also strengthens the class relationship infor-\n",
      "mation learning.\n",
      "here, the aﬃnity matrices\n",
      "represents the relationship between any pair of classes along the dimensions.\n",
      "then we constraint the consistency among the corresponding aﬃnity matrices\n",
      "by multi-view projection-based class-similarity consistency (mpcc) loss:\n",
      "lmp cc =\n",
      "1\n",
      "3 × 3\n",
      "\u0002\n",
      "v\n",
      "\u0002\n",
      "n=1,2,3\n",
      "kl(qv\n",
      "n∥ ¯qv)\n",
      "(3)\n",
      "where v ∈ {axial, sagittal, coronal} is the view index, and ¯qv is the average class\n",
      "aﬃnity matrix in a certain view obtained by the three decoders.\n",
      "in this way, the model can learn\n",
      "accurate information from scribble annotations, which also avoids getting stuck\n",
      "in a degenerate solution due to low-quality pseudo labels at an early stage.\n",
      "3\n",
      "experiments and results\n",
      "3.1\n",
      "dataset and implementation details\n",
      "we used the publicly available abdomen ct dataset word [17] for experiments,\n",
      "which consists of 150 abdominal ct volumes from patients with rectal cancer,\n",
      "prostate cancer or cervical cancer before radiotherapy.\n",
      "we aimed to segment seven organs: the liver,\n",
      "spleen, left kidney, right kidney, stomach, gallbladder and pancreas.\n",
      "we used the commonly-\n",
      "adopted dice similarity coeﬃcient (dsc), 95% hausdorﬀ distance (hd95) and\n",
      "the average surface distance (asd) for quantitative evaluation.\n",
      "our framework was implemented in pytorch\n",
      "[3] as the backbone network for\n",
      "all experiments, and extended it with three decoders by embedding two auxil-\n",
      "iary decoders with diﬀerent dilation rates, as detailed in sect.\n",
      "the stochastic gradient descent (sgd) optimizer with\n",
      "momentum of 0.9 and weight decay of 10−4 was used to minimize the overall\n",
      "loss function formulated in eq. 5, where α=10.0 and β=1.0 based on the best\n",
      "performance on the validation set.\n",
      "the ﬁnal segmentation\n",
      "results were obtained by using a sliding window strategy.\n",
      "for a fair comparison,\n",
      "we used the primary decoder’s outputs as the ﬁnal results during the inference\n",
      "stage and did not use any post-processing methods.\n",
      "note that all experiments\n",
      "were conducted in the same experimental setting.\n",
      "the existing methods are\n",
      "implemented with the help of open source codebase from [14].\n",
      "scribble-based 3d multiple abdominal organ segmentation\n",
      "39\n",
      "table 1.\n",
      "best view in color.\n",
      "3.2\n",
      "comparison with other methods\n",
      "we compared our method with ﬁve weakly supervised segmentation meth-\n",
      "ods with the same set of scribbles, including pce only [12], total variation\n",
      "loss (tv)\n",
      "[15], the average dsc was\n",
      "increased by 2.67 percent points, and the average asd and hd95 were decreased\n",
      "by 5.44 mm and 16.16 mm, respectively.\n",
      "[9] obtained a\n",
      "worse performance than pce, which is mainly because that method classiﬁes pix-\n",
      "els by minimizing the intra-class intensity variance, making it diﬃcult to achieve\n",
      "good segmentation due to the low contrast.\n",
      "3.\n",
      "visualization\n",
      "of\n",
      "the\n",
      "improvement obtained by using\n",
      "diﬀerent dilation rates and uncer-\n",
      "tainty rectifying.\n",
      "it can be obviously seen that the results obtained by\n",
      "our method are closer to the ground truth, with less mis-segmentation in both\n",
      "slice level and volume level.\n",
      "3.3\n",
      "ablation experiment\n",
      "we then performed ablation experiments to investigate the contribution of\n",
      "each part of our method, and the quantitative results on the validation set\n",
      "are shown in table 2, where lusp c(−ω) means using lusp c without pixel-\n",
      "wise uncertainty rectifying.\n",
      "it can be observed that by using lusp c(−ω) with muti-\n",
      "ple decoders, the model segmentation performance is greatly enhanced with\n",
      "average dsc increasing by 7.70%, asd and hd95 decreasing by 16.11 mm and\n",
      "48.87 mm, respectively.\n",
      "by equipping each decoders with diﬀerent dilation rates,\n",
      "the model’s performance is further improved, especially in terms of asd and\n",
      "hd95, which proves our hypothesis that learning features from diﬀerent scales\n",
      "can improve the segmentation accuracy.\n",
      "3 demonstrates that over-segmentation can be mitigated by using diﬀerent\n",
      "dilation rates in the three decoders, and using the uncertainty-weighted pseudo\n",
      "labels can further improve the segmentation accuracy with small false positive\n",
      "regions removing.\n",
      "additionally, table 2 shows that combining lusp c and lmp cc obtained the\n",
      "best performance, where the average dsc, asd and hd95 were 84.75%, 2.64 mm\n",
      "and 7.91 mm, respectively, which demonstrates the eﬀectiveness of the proposed\n",
      "class similarity consistency.\n",
      "scribble-based 3d multiple abdominal organ segmentation\n",
      "41\n",
      "4\n",
      "conclusion\n",
      "in this paper, we proposed a scribble-supervised multiple abdominal organ seg-\n",
      "mentation method consisting of a 3d triple-branch multi-dilated network with\n",
      "two-level consistency constraints.\n",
      "by equipping each decoder with diﬀerent dila-\n",
      "tion rates, the model leverages features at diﬀerent scales to obtain high-quality\n",
      "soft pseudo labels.\n",
      "experiments on a public abdominal ct dataset word\n",
      "demonstrated the eﬀectiveness of the proposed method, which outperforms\n",
      "ﬁve existing scribble-based methods and narrows the performance gap between\n",
      "weakly-supervised and fully-supervised segmentation methods.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf:\n",
      "two board-certiﬁed oncologists were invited\n",
      "for evaluating the vce-mri in two aspects: image quality and effectiveness in\n",
      "primary tumor delineation.\n",
      "image quality of vce-mri evaluation includes dis-\n",
      "tinguishability between real contrast-enhanced mri (ce-mri) and vce-mri,\n",
      "clarity of tumor-to-normal tissue interface, veracity of contrast enhancement in\n",
      "tumorinvasionriskareas,andefﬁcacyinprimarytumorstaging.forprimarytumor\n",
      "delineation, the gtv was manually delineated by oncologists.\n",
      "results showed the\n",
      "mean accuracy to distinguish vce-mri from ce-mri was 53.33%; no signiﬁ-\n",
      "cant difference was observed in clarity of tumor-to-normal tissue interface between\n",
      "vce-mri and ce-mri; for the veracity of contrast enhancement in tumor inva-\n",
      "sion risk areas and efﬁcacy in primary tumor staging, a jaccard index of 76.04%\n",
      "and accuracy of 86.67% were obtained, respectively.\n",
      "the image quality evalu-\n",
      "ation suggests that the quality of vce-mri is approximated to real ce-mri.\n",
      "in tumor delineation evaluation, the dice similarity coefﬁcient and hausdorff\n",
      "distance of the gtvs that delineated from vce-mri and ce-mri were 0.762\n",
      "(0.673–0.859) and 1.932 mm (0.763 mm–2.974 mm) respectively, which were\n",
      "clinically acceptable according to the experience of the radiation oncologists.\n",
      "in china,\n",
      "npc accounts for up to 50% of all head and neck cancers, while in southeast asia,\n",
      "npc accounts for more than 70% of all head and neck cancers [3]. radiotherapy (rt) is\n",
      "currently the main treatment remedy, which needs precise tumor delineation to ensure a\n",
      "satisfactory rt outcome.\n",
      "however, accurately delineating the npc tumor is challenging\n",
      "due to the highly inﬁltrative nature of npc and its complex location, which is surrounded\n",
      "by critical organs such as brainstem, spinal cord, temporal lobes, etc. to improve the\n",
      "visibility of npc tumor for precise gross-tumor-volume (gtv) delineation, contrast-\n",
      "enhanced mri (ce-mri) is administrated through injection of gadolinium-based con-\n",
      "trast agents (gbcas) during mri scanning.\n",
      "nsf can cause severe physical impairment, such as joint contractures\n",
      "of ﬁngers, elbows, and knees, and can progress to involve critical organs such as the\n",
      "heart, diaphragm, pleura, pericardium, kidney, liver, and lung\n",
      "currently, there\n",
      "is no effective treatment for nsf, making it crucial to ﬁnd a ce-mri alternative for\n",
      "patients at risk of nsf.\n",
      "in recent years, artiﬁcial intelligence (ai), especially deep learning, plays a game-\n",
      "changingroleinmedicalimaging[7,8],whichshowedgreatpotentialtoeliminatetheuse\n",
      "of the toxic gbcas through synthesizing virtual contrast-enhanced mri (vce-mri)\n",
      "from gadolinium-free sequences, such as t1-weighted (t1w) and t2-weighted (t2w)\n",
      "mri\n",
      "in addition to the advantage\n",
      "of eliminating the use of gbca, vce-mri synthesis can also speed up the clinical\n",
      "workﬂow by eliminating the need for acquiring ce-mri scan, which saves time for\n",
      "both clinical staff and patients.\n",
      "however, current studies mostly focus on algorithms\n",
      "development while lack comprehensive clinical evaluations to demonstrate the efﬁcacy\n",
      "of the synthetic vce-mri in clinical settings.\n",
      "rigorous clinical evaluations can establish the safety and efﬁcacy of ai-based tech-\n",
      "niques, identify potential biases and limitations, and facilitate the integration of clinical\n",
      "expertise to ensure accurate and meaningful results [13].\n",
      "furthermore, the clinical evalu-\n",
      "ation of ai-based techniques can help identify areas for improvement and optimization,\n",
      "leading to development of more effective algorithms.\n",
      "clinical evaluation of ai-assisted virtual contrast enhanced mri\n",
      "543\n",
      "to bridge this bench-to-bedside research gap, in this study, we conducted a series of\n",
      "clinicalevaluationstoassesstheeffectivenessofsyntheticvce-mriinnpcdelineation,\n",
      "with a particular focus on assessment in vce-mri image quality and primary gtv\n",
      "delineation.\n",
      "this dataset included 303 biopsy-proven (stage i-ivb) npc patients who received radi-\n",
      "ation treatment during 2012–2016.\n",
      "mri images were automatically registered as mri images for each\n",
      "patient were scanned in the same position.\n",
      "the use of this dataset was approved by the\n",
      "institutional review board of the university of hong kong/hospital authority hong\n",
      "kong west cluster (hku/ha hkw irb) with reference number uw21-412, and the\n",
      "research ethics committee (kowloon central/kowloon east) with reference number\n",
      "kc/ke-18-0085/er-1.\n",
      "for model development, 288 patients were used for model development and\n",
      "15 patients were used to synthesize vce-mri for clinical evaluation.\n",
      "prior to model training, mri images were resampled to 256*224\n",
      "by bilinear interpolation [14] due to the inconsistent matrix sizes of the three datasets.\n",
      "in this work, we obtained\n",
      "12806 image pairs for model training.\n",
      "different from the original study, which used\n",
      "single institutional data for model development and utilized min-max value of the whole\n",
      "dataset for data normalization, in this work, we used mean and standard deviation of\n",
      "each individual patient to normalize mri intensities due to the heterogeneity of the mri\n",
      "intensities across institutions [15].\n",
      "544\n",
      "w. li et al.\n",
      "table 1. details of the multi-institutional patient characteristics.\n",
      "fs: ﬁeld strength; tr: repetition\n",
      "time; te: echo time; no.: number; avg: average.\n",
      "(train/test)\n",
      "avg. age\n",
      "modality\n",
      "tr (ms)\n",
      "te (ms)\n",
      "institution-1\n",
      "(siemens-1.5t)\n",
      "110 (105/5)\n",
      "56 ± 11\n",
      "t1w\n",
      "562–739\n",
      "13–17\n",
      "t2w\n",
      "7640\n",
      "97\n",
      "ce-mri\n",
      "562–739\n",
      "13–17\n",
      "institution-2\n",
      "(philips-3t)\n",
      "58 (53/5)\n",
      "49 ± 15\n",
      "t1w\n",
      "4.8–9.4\n",
      "2.4–8.0\n",
      "t2w\n",
      "3500–4900\n",
      "50–80\n",
      "ce-mri\n",
      "4.8–9.4\n",
      "2.4–8.0\n",
      "institution-3\n",
      "(siemens-3t)\n",
      "135 (130/5)\n",
      "57 ± 12\n",
      "t1w\n",
      "620\n",
      "9.8\n",
      "t2w\n",
      "2500\n",
      "74\n",
      "ce-mri\n",
      "3.42\n",
      "1.11\n",
      "2.3\n",
      "clinical evaluations\n",
      "the evaluation methods used in this study included image quality assessment of vce-\n",
      "mri and primary gtv delineation.\n",
      "two board-certiﬁed radiation oncologists (with\n",
      "8 years’ and 6 years’ clinical experience, respectively) were invited to perform the\n",
      "vce-mri quality assessment and gtv delineation according to their clinical expe-\n",
      "rience.\n",
      "image quality assessment of vce-mri.\n",
      "to evaluate the image quality of synthetic\n",
      "vce-mri against the real ce-mri, we conducted four rt-related evaluations: (i) dis-\n",
      "tinguishability between ce-mri and vce-mri; (ii) clarity of tumor-to-normal tissue\n",
      "interface; (iii) veracity of contrast enhancement in tumor invasion risk areas; and (iv)\n",
      "efﬁcacy in primary tumor staging.\n",
      "the mri volumes were shown in axial view, sagittal view and coronal\n",
      "view, and the oncologists can scroll through the slices to view adjacent image slices.\n",
      "(i) distinguishability between ce-mri and vce-mri. to evaluate the reality of vce-\n",
      "mri, oncologists were invited to differentiate the synthetic patients (i.e., image\n",
      "volumes that generated from synthetic vce-mri) from real patients (i.e., image\n",
      "volumes that generated from real ce-mri).\n",
      "different from the previous studies\n",
      "that utilized limited number (20-50 slices, axial view) of 2d image slices for reality\n",
      "evaluation [9, 10], we used 3d volumes in this study to help oncologists visualize\n",
      "the inter-slice adjacent information.\n",
      "the judgement results were recorded, and the\n",
      "accuracy of each institution and the overall accuracy were calculated.\n",
      "(iii) veracity of contrast enhancement in tumor invasion risk areas.\n",
      "to better evaluate the veracity of contrast\n",
      "enhancement in vce-mri, we selected 25 tumor invasion risk areas according to\n",
      "[16], including 13 high-risk areas and 12 medium-risk areas, and asked oncologists\n",
      "to determine whether these areas were at risk of being invaded according to the\n",
      "contrast-enhanced tumor regions.\n",
      "the 13 high-risk areas include: retropharyngeal\n",
      "space, parapharyngeal space, levator veli palatine muscle, prestyloid compartment,\n",
      "tensor veli palatine muscle, poststyloid compartment, nasal cavity, pterygoid pro-\n",
      "cess, basis of sphenoid bone, petrous apex, prevertebral muscle, clivus, and foramen\n",
      "lacerum.\n",
      "the 12 medium-risk areas include foramen ovale, great wing of sphenoid\n",
      "bone, medial pterygoid muscle, oropharynx, cavernous sinus, sphenoidal sinus,\n",
      "pterygopalatine fossa, lateral pterygoid muscle, hypoglossal canal, foramen rotun-\n",
      "dum, ethmoid sinus, and jugular foramen.\n",
      "a critical rt-related application of ce-mri is\n",
      "tumor staging, which plays a critical role in treatment planning and prognosis pre-\n",
      "diction\n",
      "to assess the efﬁcacy of vce-mri in npc tumor staging, oncologists\n",
      "were asked to determine the stage of the primary tumor shown in ce-mri and\n",
      "vce-mri.\n",
      "gtv delineation is the foremost prerequisite for a success-\n",
      "ful rt treatment of npc tumor, which demands excellent precision [19].\n",
      "for\n",
      "comparison, ce-mri was also imported to eclipse for tumor delineation but assigned\n",
      "as a different patient, which were shown to oncologists in a random and blind manner.\n",
      "dsc is a broadly used metric to compare the agree-\n",
      "ment between two segmentations [23].\n",
      "it measures the spatial overlap between two\n",
      "segmentations, which ranges from 0 (no spatial overlap) to 1 (complete overlap).\n",
      "even though dsc is a well-accepted segmentation compari-\n",
      "son metric, it is easily inﬂuenced by the size of contours.\n",
      "small contours typically receive\n",
      "lower dsc than larger contours [24].therefore, hd was applied as a supplementary to\n",
      "make a more thorough comparison.\n",
      "(3)\n",
      "where d(x, cvce) and d(y, cce) represent the distance from point x in contour cce to\n",
      "contour cvce and the distance from point y in contour cvce to contour cce.\n",
      "3\n",
      "results and discussion\n",
      "3.1\n",
      "image quality of vce-mri\n",
      "table 2 summarizes the results of the four vce-mri quality evaluation metrics, includ-\n",
      "ing: (i) distinguishability between ce-mri and vce-mri; (ii) clarity of tumor-to-\n",
      "normal tissue interface; (iii) veracity of contrast enhancement in tumor invasion risk\n",
      "areas; and (iv) efﬁcacy in primary tumor staging.\n",
      "the overall judgement\n",
      "accuracy for the mri volumes was 53.33%, which is close to a random guess\n",
      "accuracy (i.e., 50%).\n",
      "the average scores for real and synthetic\n",
      "patients were 3.6 and 3, 3.6 and 3.8, 3.8 and 3.6 for institution-1, institution-2, and\n",
      "institution-3, respectively.\n",
      "(iii) veracity of contrast enhancement in tumor invasion risk areas.\n",
      "the average ji obtained from institution-1, institution-2, and\n",
      "institution-3 dataset were similar with a result of 71.54%, 74.78% and 75.85%,\n",
      "respectively.\n",
      "for the\n",
      "institution-2 data, all synthetic patients observed the same stages as real patients.\n",
      "for the two t-stage disagreement patients, one synthetic patient was staged as phase\n",
      "iv while the corresponding real patient was staged as phase iii, the other synthetic\n",
      "patient was staged as i while corresponding real patient was staged as phase iii.\n",
      "table 2. image quality evaluation results of vce-mri: (a) distinguishability between ce-mri\n",
      "and vce-mri; (b) clarity of tumor-to-normal tissue interface; (c) veracity of contrast enhance-\n",
      "ment in risk areas; and (d) t-staging.\n",
      "abbreviations: inst: institution; c.a.: center-based average;\n",
      "o.a.: overall average; syn: synthetic.\n",
      "(a)\n",
      "(b)\n",
      "inst-1\n",
      "inst-2\n",
      "inst-3\n",
      "inst-1\n",
      "inst-2\n",
      "inst-3\n",
      "/\n",
      "/\n",
      "/\n",
      "real\n",
      "syn\n",
      "real\n",
      "syn\n",
      "real\n",
      "syn\n",
      "c.a.\n",
      "70%\n",
      "40%\n",
      "50%\n",
      "3.6\n",
      "3\n",
      "3.6\n",
      "3.8\n",
      "3.8\n",
      "3.6\n",
      "o.a.\n",
      "53.33%\n",
      "real: 3.67\n",
      "syn: 3.47\n",
      "(c)\n",
      "(d)\n",
      "inst-1\n",
      "inst-2\n",
      "inst-3\n",
      "inst-1\n",
      "inst-2\n",
      "inst-3\n",
      "c.a.\n",
      "71.54%\n",
      "74.78%\n",
      "75.85%\n",
      "80%\n",
      "100%\n",
      "80%\n",
      "o.a.\n",
      "74.06%\n",
      "86.67%\n",
      "548\n",
      "w. li et al.\n",
      "figure 1 illustrates an example of the synthetic vce-mri.\n",
      "the deep learning model integrated the complementary information of\n",
      "t1w mri and t2w mri, and successfully synthesized vce-mri with similar contrast\n",
      "and tumor volume as ce-mri, with no obvious contrast differences in tumor regions,\n",
      "as shown in difference map between ce-mri and vce-mri.\n",
      "fig.\n",
      "1. illustration of the synthetic vce-mri.\n",
      "3.2\n",
      "primary gtv delineation\n",
      "the average dsc and hd between the cce and cvce was 0.762 (0.673–0.859) with a\n",
      "median of 0.774, and 1.932 mm (0.763 mm–2.974 mm) with a median of 1.913 mm,\n",
      "respectively.\n",
      "for institution-1, institution-2, and institution-3, the average dsc were\n",
      "0.741, 0.794 and 0.751 respectively, while the average hd were 2.303 mm, 1.456 mm,\n",
      "and 2.037 mm respectively.\n",
      "figure 2 illustrated the delineated primary gtv contours\n",
      "from an average patient with the dsc of 0.765 and hd of 1.938 mm.\n",
      "2. illustration of the primary gtvs from a typical patient with an average dsc and hd.\n",
      "clinical evaluation of ai-assisted virtual contrast enhanced mri\n",
      "549\n",
      "4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf:\n",
      "data augmentation (da) is a key factor in medical image\n",
      "analysis, such as in prostate cancer (pca) detection on magnetic reso-\n",
      "nance images.\n",
      "however, such augmentations do not substantially\n",
      "increase the organ as well as tumor shape variability in the training set,\n",
      "limiting the model’s ability to generalize to unseen cases with more diverse\n",
      "localized soft-tissue deformations.\n",
      "we propose a new anatomy-informed\n",
      "transformation that leverages information from adjacent organs to sim-\n",
      "ulate typical physiological deformations of the prostate and generates\n",
      "unique lesion shapes without altering their label.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43990-2_50.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43990-2_50\n",
      "532\n",
      "b. kovacs et al.\n",
      "computational requirements, it can be easily integrated into common da\n",
      "frameworks.\n",
      "we demonstrate the eﬀectiveness of our augmentation on a\n",
      "dataset of 774 biopsy-conﬁrmed examinations, by evaluating a state-of-\n",
      "the-art method for pca detection with diﬀerent augmentation settings.\n",
      "keywords: data augmentation · soft-tissue deformation · prostate\n",
      "cancer detection\n",
      "1\n",
      "introduction\n",
      "data augmentation (da) is a key factor in the success of deep neural networks\n",
      "(dnn) as it artiﬁcially enlarges the training set to increase their generaliza-\n",
      "tion ability as well as robustness [22].\n",
      "it plays a crucial role in medical image\n",
      "analysis [8] where annotated datasets are only available with limited size.\n",
      "dnns\n",
      "have already successfully supported radiologists in the interpretation of mag-\n",
      "netic resonance images (mri) for prostate cancer (pca) diagnosis [3].\n",
      "however,\n",
      "the da scheme received less attention, despite its potential to leverage the data\n",
      "characteristic and address overﬁtting as the root of generalization problems.\n",
      "state-of-the-art approaches still rely on simplistic spatial transformations,\n",
      "like translation, rotation, cropping, and scaling by globally augmenting the mri\n",
      "sequences [12,20].\n",
      "however, soft tissue deformations, which\n",
      "are currently missing from the da schemes, are known to signiﬁcantly aﬀect the\n",
      "image morphology and therefore play a critical role in accurate diagnosis\n",
      "both lesion and prostate shape geometrical appearance inﬂuence the clinical\n",
      "assessment of prostate imaging-reporting and data system (pi-rads)\n",
      "ignoring\n",
      "these deformations in the da scheme can potentially limit model performance.\n",
      "model-driven transformations attempting to simulate organ functions - like\n",
      "respiration, urinary excretion, cardiovascular- and digestion mechanics - oﬀer a\n",
      "high degree of diversity while also providing realistic transformations.\n",
      "currently,\n",
      "the ﬁnite element method (fem) is the standard for modeling biomechanics [13].\n",
      "however, their computation is overly complex\n",
      "motion mod-\n",
      "els have not been integrated into any deep learning framework as an online data\n",
      "augmentation yet, thereby leaving the high potential of inducing application-\n",
      "speciﬁc knowledge into the training procedure unexploited.\n",
      "anatomy-informed data augmentation\n",
      "533\n",
      "in this work we propose an anatomy-informed spatial augmentation, which\n",
      "leverages information from adjacent organs to mimic typical deformations of the\n",
      "prostate.\n",
      "due to its lightweight computational requirements, it can be easily\n",
      "integrated into common da frameworks.\n",
      "inducing this kind of soft tissue deforma-\n",
      "tion ultimately led to improved model performance in patient- and lesion-level\n",
      "pca detection on an independent test set.\n",
      "fig.\n",
      "the proposed anatomy-informed prostate augmentation.\n",
      "due to its lightweight computational requirements, it can\n",
      "be easily integrated into online network training.\n",
      "we\n",
      "make it publicly available in batchgenerators [9] and integrate it into a nnu-net\n",
      "trainer https://github.com/mic-dkfz/anatomy_informed_da.\n",
      "2.2\n",
      "experimental setting\n",
      "we evaluate our anatomy-informed da qualitatively as well as quantitatively.\n",
      "we derive\n",
      "the diagnosis through semantic segmentation of the malignant lesions following\n",
      "previous studies [5,11,12,20,21].\n",
      "semantic segmentation provides interpretable\n",
      "predictions that are sensitive to spatial transformations, making it appropriate\n",
      "for testing spatial das.\n",
      "to compare the performance of the trained models to\n",
      "radiologists, we calculate their performance using the clinical pi-rads scores\n",
      "and histopathological ground truths.\n",
      "afterward, we evaluate\n",
      "model performances on object-level using the free-response receiver operating\n",
      "characteristic (froc) and the number of detections at the radiologists’ lesion\n",
      "level performance for pi-rads ≥ 4, at 0.32 average number of false positives\n",
      "per scan.\n",
      "objects were derived by applying a threshold of 0.5 to the softmax\n",
      "outputs followed by connected component analysis to identify connected regions\n",
      "in the segmentation maps.\n",
      "[8], which is an extensive augmentation pipeline\n",
      "containing simple spatial transformations, namely translation, rotation and\n",
      "scaling.\n",
      "2. random deformable transformations as implemented in the nnu-net [8]\n",
      "da pipeline extending the basic da scheme (1) to test its presence in the\n",
      "medical domain.\n",
      "our hypothesis is that it will produce counterproductive\n",
      "examples, resulting in inferior performance compared to our proposed da.\n",
      "anatomy-informed data augmentation\n",
      "535\n",
      "3.\n",
      "the ethics\n",
      "committee of the medical faculty heidelberg approved the study (s-164/2019)\n",
      "and waived informed consent to enable analysis of a consecutive cohort.\n",
      "all\n",
      "experiments were performed in accordance with the declaration of helsinki\n",
      "[2] and relevant data privacy regulations.\n",
      "malignancy of the segmented lesions was determined from\n",
      "a systematic-enhanced lesion ground-truth histopathological assessment, which\n",
      "has demonstrated reliable ground-truth assessment with sensitivity comparable\n",
      "to radical prostatectomy [17].\n",
      "based on the biopsy results, every cspca\n",
      "lesion was segmented on the t2-weighted sequences retrospectively by multiple\n",
      "in-house investigators under the supervision of a board-certiﬁed radiologist.\n",
      "in\n",
      "addition to the lesions, the rectum and the bladder segmentations were auto-\n",
      "matically predicted by a model built upon nnu-net [8] trained iteratively on\n",
      "an in-house cohort initially containing a small portion of our cohort.\n",
      "multiple\n",
      "radiologists conﬁrmed the quality of the predicted segmentations.\n",
      "the mri\n",
      "sequences were registered using b-spline transformation based on mutual infor-\n",
      "mation to match the ground-truth segmentations across all modalities [12,14].\n",
      "as the limited number of exams with cspca and the small lesion size com-\n",
      "pared to the whole image can cause instability during training, we adapted\n",
      "the cropping strategy from [21] by keeping the organ segmentations to use the\n",
      "anatomy-informed da (oﬀsets of ±9 mm axial to the prostate and ±11.25 mm\n",
      "in the axial plane to the rectum and the bladder).\n",
      "the images are preprocessed\n",
      "by the automated algorithm of nnu-net [8].\n",
      "compared\n",
      "to the standard nnu-net settings, we implemented balanced sampling regard-\n",
      "ing the prevalence of cspca and reduced the number of epochs to 350 to avoid\n",
      "overﬁtting.\n",
      "the middle images show the original\n",
      "mri sequence, the left images simulate rectal space evacuation, while the right images\n",
      "rectal distension.\n",
      "anatomy-informed data augmentation\n",
      "537\n",
      "in table 1\n",
      "we summarize the patient-level pauroc and f1-scores; and\n",
      "lesion-level froc results on the independent test set showing the advantage\n",
      "of using anatomy-informed da.\n",
      "to further highlight the practical advantage of\n",
      "the proposed augmentation, we compare the performance of the trained models\n",
      "to the radiologists’ diagnostic performance for pi-rads ≥ 4, which locate the\n",
      "most informative performance point clinically on the roc diagram, see fig.\n",
      "the radiologists’ perfor-\n",
      "mance with pi-rads ≥ 4 is marked to locate the most informative performance point\n",
      "clinically.\n",
      "both variants of the proposed anatomy-informed da (3.a and 3.b) increased\n",
      "the sensitivity value around the clinical pi-rads ≥ 4 performance point compared to\n",
      "the simple (1) and random elastic (3) da schemes, approaching it closely.\n",
      "extending the basic da scheme with the proposed anatomy-informed defor-\n",
      "mation not only increased the sensitivity closely matching the radiologists’\n",
      "patient-level diagnostic performance but also improved the detection of pca\n",
      "538\n",
      "b. kovacs et al.\n",
      "on a lesion level.\n",
      "interestingly, while the use of random deformable transforma-\n",
      "tion also improved lesion-level performance, it did not approach the diagnostic\n",
      "performance of the radiologists, unlike the anatomy-informed da.\n",
      "at the selected patient- and object-level working points, the model with the\n",
      "proposed rectum- and bladder-informed da scheme reached the best results with\n",
      "signiﬁcant improvements (p < 0.05) compared to the model with the basic da\n",
      "setting by increasing the f1-score with 5.11% and identifying 4 more lesions\n",
      "(5.3%) from the 76 lesions in our test set.\n",
      "the time overhead introduced by anatomy-informed augmentation caused no\n",
      "increase in the training time, the gpu remained the main bottleneck.\n",
      "towards radiologists’ performance.\n",
      "inducing lesion shape variability via\n",
      "anatomy-informed augmentation to the training process improved the lesion\n",
      "detection performance and increased the sensitivity value towards radiologist-\n",
      "level performance in pca diagnosis in contrast to the training with the basic\n",
      "da setting.\n",
      "these soft tissue deformations are part of physiology, but only one\n",
      "snapshot is captured from the many possible functional states within each indi-\n",
      "vidual mr examination.\n",
      "we got additional, but slight improvements\n",
      "by extending the da scheme with bladder distensions.\n",
      "a possible explanation for\n",
      "this result is that less than 30% of the lesions are located close to the bladder, and\n",
      "our dataset did not contain enough training examples for more improvements.\n",
      "our proposed anatomy-\n",
      "informed transformation was designed to mimic real-world deformations in order\n",
      "to preserve essential image features.\n",
      "to support the importance of realism in da quantitatively, we\n",
      "compared the performance of the basic and our anatomy-informed da scheme\n",
      "with that of the random deformable transformation.\n",
      "the random deformable da\n",
      "scheme generated high lesion shape variability, but it resulted in lower perfor-\n",
      "mance values.\n",
      "this could be due to the fact that it can also cause implausible or\n",
      "anatomy-informed data augmentation\n",
      "539\n",
      "even harmful image warping, distorting important features, and producing coun-\n",
      "terproductive training examples.\n",
      "in comparison, our proposed anatomy-informed\n",
      "da outperformed the basic and random deformable da, demonstrating the sig-\n",
      "niﬁcance of realistic transformations for achieving superior model performance.\n",
      "its limitation is the need for additional organ segmenta-\n",
      "tions, which requires additional eﬀort from the annotator.\n",
      "however, pre-trained\n",
      "networks for segmenting anatomical structures like nnu-net\n",
      "additionally, our\n",
      "transformation computation allows certain errors in the organ segmentations\n",
      "compared to applications where fully accurate segmentations are needed.\n",
      "the\n",
      "success of anatomy-informed da opens the research question of whether it\n",
      "enhances performance across diverse datasets and model backbones.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf:\n",
      "this paper presents a ﬁrst attempt to jointly predict\n",
      "molecular markers and histology features and model their interactions\n",
      "for classifying diﬀuse glioma bases on whole slide images.\n",
      "our\n",
      "experiments show that our method outperforms other state-of-the-art\n",
      "methods in classifying diﬀuse glioma, as well as related histology and\n",
      "molecular markers on a multi-institutional dataset.\n",
      "hence, automatic algorithms\n",
      "based on histology whole slide images (wsis)\n",
      "[15], namely digital pathology,\n",
      "promise to oﬀer rapid diagnosis and aid precise treatment.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43990-2_52.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "the role of key molecular\n",
      "markers, i.e., isocitrate dehydrogenas (idh) mutations, co-deletion of chromo-\n",
      "some 1p/19q and homozygous deletion (homdel) of cyclin-dependent kinase\n",
      "inhibitor 2a/b (cdkn), have been highlighted as major diagnostic markers\n",
      "for glioma, while histology features that are traditionally emphasized are now\n",
      "considered as reference, although still relevant in many cases.\n",
      "for instance, in\n",
      "the new pathology scheme, glioblastoma is increasingly diagnosed according to\n",
      "idh mutations, while previously its diagnosis mostly relies on histology features,\n",
      "including necrosis and microvascular proliferation (nmp).1\n",
      "however, the primary approaches to assess molecular markers include gene\n",
      "sequencing and immuno-staining, which are time-consuming and expensive than\n",
      "histology assessment.\n",
      "[25] devised a dual-pool contrastive\n",
      "learning for classifying fundus and x-ray images.\n",
      "note for wsis with patch number< n,\n",
      "we adopt a biological repeat strategy for dimension alignment.\n",
      "4\n",
      "experiments and results\n",
      "4.1\n",
      "implementation details\n",
      "the proposed deepmo-glioma is trained on the training set for 70 epochs,\n",
      "with batch size of 8 and initial learning rate of 0.003 with adam optimizer\n",
      "[11] together with weight decay.\n",
      "key hyper-parameters are listed in table 1 of\n",
      "supplementary material.\n",
      "all hyper-parameters are tuned to achieve the best per-\n",
      "formance over the validation set.\n",
      "all experiments are conducted on a computer\n",
      "with an intel(r) xeon(r) e5-2698 cpu @2.20 ghz, 256 gb ram and 4 nvidia\n",
      "tesla v100 gpus.\n",
      "additionally, our method is implemented on pytorch with\n",
      "python environment.\n",
      "performance of classifying glioma based on who 2021 criteria\n",
      "refer to supplementary fig.\n",
      "4.2\n",
      "performance evaluation\n",
      "1) glioma classiﬁcation.\n",
      "[16] are mil framework, while\n",
      "others are commonly-used image classiﬁcation methods, set as our baselines.\n",
      "the left panel of table 1 shows that deepmo-glioma performs the best, achiev-\n",
      "ing at least 6.1%, 13.1%,\n",
      "3.1% and 11.0% improvement over other models in\n",
      "accuracy, sensitivity, speciﬁcity and auc, respectively, indicating that our model\n",
      "558\n",
      "x. wang et al.\n",
      "table 2. performance in predicting genomic markers, histology and ablation studies.\n",
      "figure 3 plots the rocs of all models, demonstrating the\n",
      "superior performance of our model over other comparison models.\n",
      "3) network interpretability.\n",
      "an additional visualization experiment is con-\n",
      "ducted based on patch decision scores to test the interpretability of our method.\n",
      "due to the page limit, the results are presented in supplementary fig.\n",
      "1.\n",
      "4.3\n",
      "results of ablation experiments\n",
      "1) cplc-graph network.\n",
      "the right panels of table 1 shows that the performance after remov-\n",
      "ing lc loss decreases in all metrics, causing a reduction of 6.1%, 15.0%, 4.3%\n",
      "and 9.8%, in accuracy, sensitivity, speciﬁcity and f1-score, respectively.\n",
      "from table 1, we observe that the proposed dcc loss improves\n",
      "the performance in terms of accuracy by 9.1%.\n",
      "such performance is also\n",
      "found in comparing the rocs in fig.\n",
      "our experiments demonstrate that our model has achieved superior\n",
      "performance over other state-of-the-art methods, serving as a potentially useful\n",
      "tool for digital pathology based on wsis in the era of molecular pathology.\n",
      "acknowledgments.\n",
      "not necessarily those of the nhs, the nihr or the department of health and social\n",
      "care.\n",
      "32(4), e13060 (2022)\n",
      "2. campanella, g., et al.: clinical-grade computational pathology using weakly super-\n",
      "vised deep learning on whole slide images.\n",
      "imaging 41(4), 757–770 (2020)\n",
      "4. dosovitskiy, a., et al.: an image is worth 16x16 words: transformers for image\n",
      "recognition at scale.\n",
      "he, k., zhang, x., ren, s., sun, j.: deep residual learning for image recognition.\n",
      "imboden, s., et al.: implementation of the 2021 molecular esgo/estro/esp\n",
      "risk groups in endometrial cancer.\n",
      "jiang, s., zanazzi, g.j., hassanpour, s.: predicting prognosis and idh mutation\n",
      "status for patients with lower-grade gliomas using whole slide images.\n",
      "liang, s., et al.: clinical practice guidelines for the diagnosis and treatment of\n",
      "adult diﬀuse glioma-related epilepsy.\n",
      "lu, m.y., williamson, d.f., chen, t.y., chen, r.j., barbieri, m., mahmood,\n",
      "f.: data-eﬃcient and weakly supervised computational pathology on whole-slide\n",
      "images.\n",
      "shao, z., bian, h., chen, y., wang, y., zhang, j., ji, x., et al.: transmil: trans-\n",
      "former based correlated multiple instance learning for whole slide image classiﬁca-\n",
      "tion.\n",
      "syst. 34, 2136–2147 (2021)\n",
      "17. simonyan, k., zisserman, a.: very deep convolutional networks for large-scale\n",
      "image recognition.\n",
      "trpkov, k., et al.: new developments in existing who entities and evolving molecu-\n",
      "lar concepts: the genitourinary pathology society (gups) update on renal neoplasia.\n",
      "deep learning-based six-type classiﬁer for lung cancer and mimics\n",
      "from histopathological whole slide images: a retrospective study.\n",
      "zhang, l., wei, y., fu, y., price, s., schönlieb, c.b., li, c.: mutual contrastive\n",
      "low-rank learning to disentangle whole slide image representations for glioma grad-\n",
      "ing.\n",
      "zhang, y., luo, l., dou, q., heng, p.a.: triplet attention and dual-pool contrastive\n",
      "learning for clinic-driven multi-label medical image classiﬁcation.\n",
      "image anal.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_2.pdf:\n",
      "many renal cancers are incidentally found on non-contrast\n",
      "ct (ncct) images.\n",
      "on contrast-enhanced ct (cect) images, most\n",
      "kidney tumors, especially renal cancers, have diﬀerent intensity values\n",
      "compared to normal tissues.\n",
      "however, on ncct images, some tumors\n",
      "called isodensity tumors, have similar intensity values to the surround-\n",
      "ing normal tissues, and can only be detected through a change in organ\n",
      "shape.\n",
      "several deep learning methods which segment kidney tumors from\n",
      "cect images have been proposed and showed promising results.\n",
      "how-\n",
      "ever, these methods fail to capture such changes in organ shape on ncct\n",
      "images.\n",
      "in this paper, we present a novel framework, which can explicitly\n",
      "capture protruded regions in kidneys to enable a better segmentation of\n",
      "kidney tumors.\n",
      "we created a synthetic mask dataset that simulates a\n",
      "protuberance, and trained a segmentation network to separate the pro-\n",
      "truded regions from the normal kidney regions.\n",
      "to achieve the segmen-\n",
      "tation of whole tumors, our framework consists of three networks.\n",
      "the\n",
      "ﬁrst network is a conventional semantic segmentation network which\n",
      "extracts a kidney region mask and an initial tumor region mask.\n",
      "the proposed\n",
      "method was evaluated on a publicly available kits19 dataset, which con-\n",
      "tains 108 ncct images, and showed that our method achieved a higher\n",
      "dice score of 0.615 (+0.097) and sensitivity of 0.721 (+0.103) compared\n",
      "to 3d-unet.\n",
      "to the best of our knowledge, this is the ﬁrst deep learning\n",
      "method that is speciﬁcally designed for kidney tumor segmentation on\n",
      "ncct images.\n",
      "keywords: renal cancer · tumor segmentation · non-contrast ct\n",
      "1\n",
      "introduction\n",
      "over 430,000 new cases of renal cancer were reported in 2020 in the world\n",
      "https://doi.org/10.1007/978-3-031-43990-2_2\n",
      "14\n",
      "t. hatsutani et al.\n",
      "than 7 cm) often the whole kidney is removed, however, when the tumor size\n",
      "is small (less than 4 cm), partial nephrectomy is the preferred treatment\n",
      "however, early-stage renal cancers are\n",
      "usually asymptomatic, therefore they are often incidentally found during other\n",
      "examinations [19], which includes non-contrast ct (ncct) scans.\n",
      "segmentation of kidney tumors on ncct images adds challenges compared\n",
      "to contrast-enhanced ct (cect) images, due to low contrast and lack of multi-\n",
      "phase images.\n",
      "on cect images, the kidney tumors have diﬀerent intensity val-\n",
      "ues compared to the normal tissues.\n",
      "there are several works that demonstrated\n",
      "successful segmentation of kidney tumors with high precision [13,21].\n",
      "however,\n",
      "on ncct images, as shown in fig.\n",
      "[3] is the go-to network for segmenting kidney tumors on cect\n",
      "images.\n",
      "however, convolutional neural networks (cnns) are biased towards tex-\n",
      "ture features [5].\n",
      "therefore, without any intervention, they may fail to capture\n",
      "the protuberance caused by isodensity tumors on ncct images.\n",
      "our goal is to segment kidney tumors includ-\n",
      "ing isodensity types on ncct images.\n",
      "to achieve this goal, we create a syn-\n",
      "thetic dataset, which has separate annotations for normal kidneys and protruded\n",
      "regions, and train a segmentation network to separate the protruded regions from\n",
      "the normal kidney regions.\n",
      "in order to segment whole tumors, our framework\n",
      "consists of three networks.\n",
      "this proposed framework\n",
      "enables a better segmentation of isodensity tumors and boosts the performance\n",
      "of segmentation of kidney tumors on ncct images.\n",
      "present a pioneering work for segmentation of kidney tumors on ncct\n",
      "images.\n",
      "2. propose a novel framework that explicitly captures protuberances in a kid-\n",
      "ney to enable a better segmentation of tumors including isodensity types on\n",
      "ncct images.\n",
      "segmentation of kidney tumors on ncct images\n",
      "15\n",
      "fig.\n",
      "1. example cect and ncct images.\n",
      "a) cect image.\n",
      "b) ncct image.\n",
      "2\n",
      "related work\n",
      "the release of two public ct image datasets with kidney and tumor masks from\n",
      "the 2019/2021 kidney and kidney tumor segmentation challenge [8] (kits19,\n",
      "kits21) attracted researchers to develop various methods for segmentation.\n",
      "looking at the top 3 teams from each challenge\n",
      "however, the paper notes that modifying\n",
      "the architecture resulted in only slight improvement.\n",
      "the other 5 teams took\n",
      "a similar approach to nnu-net’s coarse-to-ﬁne cascaded network [12], where it\n",
      "predicts from a low-resolution image in the ﬁrst stage and then predicts kidneys\n",
      "and tumors from a high-resolution image in the second stage.\n",
      "to overcome this issue, we\n",
      "developed a framework that speciﬁcally incorporates protuberances in kidneys,\n",
      "allowing for an eﬀective segmentation of tumors on ncct images.\n",
      "[14] developed a computer-aided diagnosis system to detect exophytic\n",
      "kidney tumors on ncct images using belief propagation and manifold diﬀusion\n",
      "to search for protuberances.\n",
      "in our work, we will not only segment protruded tumors but also\n",
      "other tumors as well.\n",
      "this enables us to extract a part of tumors that forms\n",
      "protuberance, but our goal is segmenting all visible kidney tumors on ncct\n",
      "images.\n",
      "in detail,\n",
      "we perform a summation of the initial tumor mask and the protruded region\n",
      "mask, and then concatenate the result with the input image.\n",
      "3.1\n",
      "step1: training base network\n",
      "in the ﬁrst step, we train the base network, which is a standard segmentation\n",
      "network, to extract kidney and tumor masks from the images.\n",
      "and as a loss function, we use the dice loss [16] and\n",
      "the cross-entropy loss equally.\n",
      "segmentation of kidney tumors on ncct images\n",
      "17\n",
      "3.2\n",
      "step2: training protuberance detection network\n",
      "in the second step, we train the protuberance detection network alone to separate\n",
      "protruded regions from the normal kidney masks.\n",
      "to enable a segmentation of protruded regions only, a sep-\n",
      "arate annotation of each region is usually required.\n",
      "the output of the protuberance detection network will likely have more false\n",
      "positives than the base network since it has no access to the input image.\n",
      "the ﬁrst channel is the input image, and the second\n",
      "channel is the result of summation of the initial tumor mask and the protruded\n",
      "region mask.\n",
      "we concatenate the input image so that the last network can remove\n",
      "false positives from the predicted masks as well as predicting the missing tumor\n",
      "regions from the protuberance detection network.\n",
      "by having multiple modules in this manner, the network can ﬁx the\n",
      "initial mistakes in early modules and corrects in later modules.\n",
      "4\n",
      "experiments\n",
      "no prior work exists that uses ncct images from kits19\n",
      "thus, we ﬁrst\n",
      "created our baseline model and compared the performance with existing methods\n",
      "on cect images.\n",
      "we then trained the model using ncct images and\n",
      "compared with our proposed method.\n",
      "4.1\n",
      "datasets and preprocessing\n",
      "we used a dataset from kits19\n",
      "[8] which contains both cect and ncct\n",
      "images.\n",
      "for cect images, there are 210 images for training and validation\n",
      "and, 90 images for testing.\n",
      "for ncct images, there are 108 images, which are\n",
      "diﬀerent series of the 210 images.\n",
      "the ground truth masks are only available for\n",
      "the 210 cect images.\n",
      "thus, we transfer the masks to ncct images.\n",
      "the images were ﬁrst clipped to the intensity value range of [−90, 210] and\n",
      "normalized from −1 to 1.\n",
      "during\n",
      "the training, the images were randomly cropped to a patch size of 128×128×128\n",
      "voxels.\n",
      "we applied random rotation, random scaling and random noise addition\n",
      "as data augmentation.\n",
      "we applied some\n",
      "augmentations during training to input masks to simulate the incoming inputs\n",
      "from the base network.\n",
      "therefore, we applied gaussian blurring, gaussian\n",
      "noise addition and intensity value shifting.\n",
      "segmentation of kidney tumors on ncct images\n",
      "19\n",
      "table 1.\n",
      "dice performance of existing method and our baseline model.\n",
      "evaluated using\n",
      "cect images from kits19.\n",
      "composite dice is an average dice between kidney and\n",
      "tumor dice.\n",
      "the results were obtained by submitting our predicted masks to the grand\n",
      "challenge page.\n",
      "[13] 0.9123\n",
      "0.9737\n",
      "0.8509\n",
      "our baseline model\n",
      "0.8832\n",
      "0.9728\n",
      "0.7935\n",
      "4.2\n",
      "training details and evaluation metrics\n",
      "our model was trained using sgd with a 0.9 momentum and a weight decay of\n",
      "1e−7.\n",
      "we conducted our experiments using\n",
      "jax (v.0.4.1)\n",
      "we trained the model using a single\n",
      "nvidia rtx a5000 gpu.\n",
      "for the experiment on cect images, we used the dice score as our evalu-\n",
      "ation metrics following the same formula from kits19.\n",
      "for the experiment on\n",
      "ncct images, we also evaluated the sensitivity and false positives per image\n",
      "(fps/image).\n",
      "5\n",
      "results\n",
      "5.1\n",
      "performance on cect images\n",
      "to show that our model is properly tuned, we compare our baseline model with\n",
      "an existing method using cect images.\n",
      "we used this\n",
      "baseline model as our base network for the experiments on ncct images.\n",
      "5.2\n",
      "performance on ncct images\n",
      "table 2 shows our experimental results and ablation studies on ncct images.\n",
      "the ablation studies show that adding each component (cect\n",
      "images and the protuberance detection network) resulted in an increase in the\n",
      "performance.\n",
      "while adding cect images contributed the most for the increase\n",
      "in tumor dice and sensitivity, adding the protuberance detection network further\n",
      "pushed the performance.\n",
      "however, the false positives per image (fps/image)\n",
      "20\n",
      "t. hatsutani et al.\n",
      "table 2.\n",
      "result of our proposed method on ncct images from kits19.\n",
      "the values\n",
      "are average values of a ﬁve-fold cross-validation.\n",
      "protuberance detection network with cect images tumor dice sensitivity fps/image\n",
      "✗\n",
      "✗\n",
      "0.518\n",
      "0.618\n",
      "0.283\n",
      "✗\n",
      "✓\n",
      "0.585\n",
      "0.686\n",
      "0.340\n",
      "✓\n",
      "✓\n",
      "0.615\n",
      "0.721\n",
      "0.421\n",
      "fig.\n",
      "the protuberance detection network cannot dis-\n",
      "tinguish the protrusions that were caused by tumors or cysts, so the output\n",
      "from this network has many fps at this stage.\n",
      "thus, the fusion network has to\n",
      "eliminate cysts by looking again the input image, however, it may have failed to\n",
      "eliminate some cysts (fig.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf:\n",
      "we propose a method (named mskdex) to estimate\n",
      "ﬁne-grained muscle properties from a plain x-ray image, a low-cost, low-\n",
      "radiation, and highly accessible imaging modality, through musculoskele-\n",
      "tal decomposition leveraging ﬁne-grained segmentation in ct.\n",
      "we train\n",
      "a multi-channel quantitative image translation model to decompose an\n",
      "x-ray image into projections of ct of individual muscles to infer the lean\n",
      "muscle mass and muscle volume.\n",
      "we propose the object-wise intensity-\n",
      "sum loss, a simple yet surprisingly eﬀective metric invariant to muscle\n",
      "deformation and projection direction, utilizing information in ct and\n",
      "x-ray images collected from the same patient.\n",
      "while our method is basi-\n",
      "cally an unpaired image-to-image translation, we also exploit the nature\n",
      "of the bone’s rigidity, which provides the paired data through 2d-3d\n",
      "rigid registration, adding strong pixel-wise supervision in unpaired train-\n",
      "ing.\n",
      "the average pearson correlation coeﬃcient between the predicted and\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43990-2_47.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "we\n",
      "believe our method opened up a new musculoskeletal diagnosis method\n",
      "and has the potential to be extended to broader applications in multi-\n",
      "channel quantitative image translation tasks.\n",
      "keywords: muscles · radiography · generative adversarial networks\n",
      "(gan) · sarcopenia · image-to-image translation\n",
      "1\n",
      "introduction\n",
      "sarcopenia is a prevalent musculoskeletal disease characterized by the inevitable\n",
      "loss of skeletal muscle, causing increased risks of all-cause mortality and dis-\n",
      "ability that result in heavy healthcare costs [1–6].\n",
      "however, dxa and ct\n",
      "require special equipment that is much less accessible in a small clinic.\n",
      "further-\n",
      "more, ct requires high radiation exposure, and dxa allows the measurement\n",
      "of only overall body composition, which lacks details in individual muscles such\n",
      "as the iliacus muscle, which overlays with the gluteus maximus muscle in dxa\n",
      "images.\n",
      "although several recent works used x-ray images for bone mineral den-\n",
      "sity (bmd) estimation and osteoporosis diagnosis [12–15], only a few works esti-\n",
      "mated muscle metrics and sarcopenia diagnosis [16,17], and the deep learning\n",
      "technology used is old.\n",
      "recently, bmd-gan [15] was proposed for estimating\n",
      "bmd through x-ray image decomposition using x-ray and ct images aligned by\n",
      "2d-3d registration.\n",
      "[17]\n",
      "proposed an x-ray image decomposition for individual muscles.\n",
      "in this study, we propose mskdex: musculoskeletal (msk) decomposition\n",
      "from a plain x-ray image for the ﬁne-grained estimation of lean muscle mass\n",
      "and volume of each individual muscle, which are useful metrics for evaluating\n",
      "muscle diseases including sarcopenia.\n",
      "the contribution of this paper\n",
      "is three-fold: 1) proposal of the object-wise intensity-sum (owis) loss, a simple\n",
      "yet eﬀective metric invariant to muscle deformation and projection direction, for\n",
      "quantitative learning of the absolute volume and lean mass of the muscles, 2)\n",
      "proposal of partially aligned training utilizing the aligned (paired) dataset for\n",
      "the rigid object for the pixel-wise supervision in an unpaired image translation\n",
      "task, 3) extensive evaluation of the performance using a 539-patient dataset.\n",
      "patient #1 (young, male) and patient #2 (old, female) had similar\n",
      "bmi and almost the same gluteus maximus volume, while the lean muscle mass was\n",
      "signiﬁcantly diﬀerent, likely due to the fatty degeneration in patient #2, which was\n",
      "clearly observable in the projections of the lean muscle mass volume.\n",
      "fig.\n",
      "three types of object-wise drrs (of seg-\n",
      "mented individual muscle/bone regions) were obtained from ct through segmentation\n",
      "[18], intensity conversion [19,20], 2d-3d registration for bones\n",
      "a decomposition model was trained using\n",
      "gan loss and proposed gc loss chain, owis loss, and bone loss to decompose an\n",
      "x-ray image into drrs whose intensity sum derives the metric of volume and mass.\n",
      "2\n",
      "method\n",
      "2.1\n",
      "dataset preparation\n",
      "figure 2 illustrates the overview of the proposed mskdex.\n",
      "we collected a dataset\n",
      "of 552 patients subject to the total hip arthroplasty surgery (455 females and\n",
      "500\n",
      "y. gu et al.\n",
      "97 males, height 156.9 ± 8.3 cm, weight 57.5 ± 11.9 kg, bmi 23.294 ± 3.951\n",
      "we acquired a pair of pre-operative x-ray and ct images from\n",
      "each patient, assuming consistency in bone shape, lean muscle mass, and muscle\n",
      "volume.\n",
      "automated segmentation of individual bones and muscles was obtained\n",
      "from ct\n",
      "three diﬀerent intensity conversions were applied to the segmented\n",
      "ct; 1) the original intensity, 2) intensity of 1.0 for voxels inside the structure\n",
      "and 0.0 for voxels outside to estimate muscle volume, 3) intensity correspond-\n",
      "ing to the lean muscle mass density based on a conversion function from the\n",
      "hounsﬁeld unit (hu) to the mass density [19,20] to estimate lean muscle mass.\n",
      "then, object-wise drrs\n",
      "for the three conversions were generated for each segmented individual object\n",
      "(bone/muscle) region.\n",
      "the summation of all the objects of wvdrrs becomes an image\n",
      "with a contrast similar to the real x-ray image used to calculate the reconstruc-\n",
      "tion gradient correlation (gc) loss [17,22].\n",
      "a 2d-3d registration [21] of each\n",
      "bone between ct and x-ray image of the same patient was performed to obtain\n",
      "its drr aligned with the x-ray image, which is used in the proposed partially\n",
      "aligned training.\n",
      "instead, we exploited the invariant property of muscles using the newly proposed\n",
      "intensity-sum loss.\n",
      "2.2\n",
      "model training\n",
      "we train a decomposition model g to decompose an x-ray image into the\n",
      "drrs = {v drr, mdrr, wv drr} to infer the lean muscle mass and muscle\n",
      "volume, adopting cyclegan\n",
      "the gan loss lup\n",
      "gan we use is formulated in supplemental materials.\n",
      "we call the summation of a drr over all the chan-\n",
      "nels (objects) the virtual x-ray image deﬁned as iv x = v (idrr) = \u0002\n",
      "i idrr\n",
      "i\n",
      ",\n",
      "where idrr\n",
      "i\n",
      "is the i-th object image of a drr.\n",
      "= eix − gc(ix, v (g(ix)w v drr))\n",
      "(1)\n",
      "mskdex\n",
      "501\n",
      "to maintain the structure consistency between an x-ray image and decomposed\n",
      "drr, where g(ix)w v drr is the decomposed wvdrr.\n",
      "+gc(st(g(ix)w v drr\n",
      "i\n",
      "), g(ix)mdrr\n",
      "i\n",
      ")\n",
      "\u0005\n",
      "(2)\n",
      "to chain the structural constraints from wvdrr to vdrr and mdrr, where\n",
      "the g(ix)w v drr\n",
      "i\n",
      ", g(ix)v drr\n",
      "i\n",
      ", and g(ix)mdrr\n",
      "i\n",
      "are i-th object image of the\n",
      "decomposed wvdrr, vdrr, and mdrr, respectively.\n",
      "unlike general images, our drrs embedded\n",
      "speciﬁc information so that the intensity sum represents physical metrics (mass\n",
      "and volume).\n",
      "furthermore, the conventional method did not utilize the paired\n",
      "information of an x-ray image and drr (obtained from the same patient).\n",
      "we\n",
      "took advantage of the paired information, proposing the object-wise intensity-\n",
      "sum loss, a simple yet eﬀective metric invariant to patient pose and projection\n",
      "direction, for quantitative learning.\n",
      "− s(idrr\n",
      "i\n",
      ")\n",
      "\u0006\u0006\u0006,\n",
      "(3)\n",
      "where idrr\n",
      "i\n",
      "and s(·) are the i-th object image of drr and the intensity sum-\n",
      "mation operator (sum over the intensity of an image), respectively.\n",
      "the h and\n",
      "w are the image height and weight, respectively, served as temperatures for\n",
      "numeric stabilizability.\n",
      "[21] to align the pelvis and femur drrs\n",
      "with the paired x-ray images for partially aligned training to improve overall\n",
      "performance, including muscle metrics estimation.\n",
      "3\n",
      "experiments and results\n",
      "the automatic segmentation results of 552 cts were visually veriﬁed, and 13\n",
      "cases with severe segmentation failures were omitted from our analysis, resulting\n",
      "in 539 cts.\n",
      "the baseline of our experiment was the\n",
      "vanilla cyclegan with the reconstruction gc loss proposed in [17].\n",
      "from 3d ct images with three metrics, pearson correlation coeﬃcient (pcc),\n",
      "intra-class correlation coeﬃcient (icc), and mean absolute error (mae).\n",
      "addi-\n",
      "tionally, we evaluated the image quality of predicted drrs of the bones by com-\n",
      "paring them with the aligned drrs using peak-signal-noise-ratio (psnr) and\n",
      "structural similarity index measure (ssim).\n",
      "implementation details are described\n",
      "in supplemental materials.\n",
      "signiﬁcant improvements by the proposed method were\n",
      "observed, achieving high pccs of 0.877 and 0.901 of the lean muscle mass and\n",
      "muscle volume estimations, respectively, for the gluteus medius, and 0.865 and\n",
      "0.873, respectively, for the iliacus.\n",
      "the overall intensity of the conventional method was clearly diﬀerent\n",
      "from the reference, while the proposed method decomposed the x-ray image\n",
      "considering the structural faithfulness and quantitative accuracy, outperform-\n",
      "ing the conventional method signiﬁcantly.\n",
      "more detailed results and a visualiza-\n",
      "tion video can be found in supplemental materials.\n",
      "we observed signiﬁcant improvements from the baseline by both\n",
      "proposed features, owis loss and partially aligned training lb.\n",
      "the average pcc for the muscles was improved from 0.457 to 0.826 by adding\n",
      "the owis loss (λis = 100) and to 0.796 by adding the bone loss, while their\n",
      "combination achieved the best average pcc of 0.855, demonstrating the supe-\n",
      "rior ability of quantitative learning of the proposed mskdex.\n",
      "the results also\n",
      "suggested that the weight balance for loss terms needs to be made to achieve the\n",
      "best performance.\n",
      "more detailed results are shown in supplemental materials.\n",
      "performance comparison between the conventional and proposed methods in\n",
      "a cross-validation study using 539 data.\n",
      "sac.\n",
      "0\n",
      "(false)\n",
      ".415\n",
      ".419\n",
      ".469\n",
      ".368\n",
      ".473\n",
      ".600\n",
      ".542\n",
      ".265\n",
      "0\n",
      "(true)\n",
      ".734\n",
      ".799\n",
      ".788\n",
      ".855\n",
      ".784\n",
      ".813\n",
      ".954 .798\n",
      "100\n",
      "(false)\n",
      ".799\n",
      ".815\n",
      ".829\n",
      ".854\n",
      ".815\n",
      ".842\n",
      ".925\n",
      ".774\n",
      "100\n",
      "(true)\n",
      ".854\n",
      ".857\n",
      ".837\n",
      ".883 .854\n",
      ".846 .947\n",
      ".898\n",
      "1000 (false)\n",
      ".798\n",
      ".765\n",
      ".770\n",
      ".839\n",
      ".704\n",
      ".840\n",
      ".870\n",
      ".767\n",
      "1000 (true)\n",
      ".795\n",
      ".776\n",
      ".767\n",
      ".828\n",
      ".748\n",
      ".820\n",
      ".929\n",
      ".745\n",
      "mskdex\n",
      "505\n",
      "4\n",
      "summary\n",
      "we proposed mskdex, a method for ﬁne-grained estimation of the lean muscle\n",
      "mass and volume from a plain x-ray image (2d) through the musculoskele-\n",
      "tal decomposition, which, in fact, recovers ct (3d) information.\n",
      "our method\n",
      "decomposes an x-ray image into drrs of objects to infer the lean muscle mass\n",
      "and volume considering the structural faithfulness (by the gradient correlation\n",
      "loss chain) and quantitative accuracy (by the object-wise intensity-sum loss and\n",
      "aligned bones training), outperforming the conventional method by a large mar-\n",
      "gin as shown in sect.\n",
      "the prediction of muscles overlapped\n",
      "with the pelvis in the x-ray image can leverage the strong pixel-wise supervision\n",
      "by the aligned pelvis’s drr, which can be considered as a type of calibration.\n",
      "our future works are the validation with a large-scale dataset and extension to\n",
      "the decomposition into a larger number of objects.\n",
      "acknowledgement.\n",
      "https://doi.org/10.1007/s40520-016-0704-\n",
      "5\n",
      "4. petermann-rocha, f., balntze, v., gray, s.r., et al.: global prevalence of sar-\n",
      "copenia and severe sarcopenia: a systematic review and meta-analysis.\n",
      "https://doi.org/10.1002/jcsm.12890\n",
      "6. edwards, m.h., dennision, e.m., sayer, a.a., et al.: osteoporosis and sarcopenia\n",
      "in older age.\n",
      "nana, a., slater, g.j., stewart, a.d., burke, l.m.: methodology review: using\n",
      "dual-energy x-ray absorptiometry (dxa) for the assessment of body composition\n",
      "in athletes and active people.\n",
      "https://doi.org/10.1123/ijsnem.2013-0228\n",
      "9. feliciano, e.m.c., et al.: evaluation of automated computed tomography segmen-\n",
      "tation to assess body composition and mortality associations in cancer patients.\n",
      "hsieh, c.-i., zheng, k., lin, c., mei, l., et al.: automated bone mineral density\n",
      "prediction and fracture risk assessment using plain radiographs via deep learning.\n",
      "wang, f., zheng, k., lu, le, et al.: lumbar bone mineral density estimation from\n",
      "chest x-ray images: anatomy-aware attentive multi-roi modeling.\n",
      "gu, y., et al.: bmd-gan: bone mineral density estimation using x-ray image\n",
      "decomposition into projections of bone-segmented quantitative computed tomog-\n",
      "raphy using hierarchical learning.\n",
      "medical image computing and computer assisted intervention\n",
      "– miccai 2022.\n",
      "https://doi.org/10.1038/s41598-023-35075-x\n",
      "18. hiasa, y., otake, y., et al.: automated muscle segmentation from clinical ct using\n",
      "bayesian u-net for personalized musculoskeletal modeling.\n",
      "aubrey, j., esfandiari, n., baracos, v.e., buteau, f.a., et al.: measurement of\n",
      "skeletal muscle radiation attenuation and basis of its biological variation.\n",
      "otake, y., et al.: intraoperative image-based multiview 2d/3d registration for\n",
      "image-guided orthopaedic surgery: incorporation of ﬁducial-based c-arm tracking\n",
      "and gpu-acceleration.\n",
      "hiasa, y., et al.: cross-modality image synthesis from unpaired data using cycle-\n",
      "gan.\n",
      "zhu, j.-y., park, t., isola, p., efros, a.a.: unpaired image-to-image translation\n",
      "using cycle-consistent adversarial networks.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf:\n",
      "in this paper for the ﬁrst time, we propose\n",
      "evidential graph transformer network, a combination of attention map-\n",
      "ping and uncertainty estimation to increase the performance and inter-\n",
      "pretability of surgical margin assessment.\n",
      "the performance of the model was\n",
      "compared with diﬀerent baselines in an ex-vivo cross-validation scheme,\n",
      "with extensive ablation study.\n",
      "results: the purposed model outperformed all baselines, statistically\n",
      "signiﬁcantly, with average balanced accuracy of 91.6%.\n",
      "conclusion: deployment of ex-vivo models is challenging\n",
      "due to the tissue heterogeneity of intra-operative data.\n",
      "the proposed\n",
      "evidential graph transformer is a powerful tool that while providing\n",
      "the attention distribution of biochemical subbands, improve the surgical\n",
      "deployment power by providing decision conﬁdence.\n",
      "keywords: intra-operative deployment · uncertainty estimation ·\n",
      "interpretation · graph transformer network · breast cancer margin\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14226, pp.\n",
      "in bcs the surgeon removes breast cancer while attempting to\n",
      "preserve as much healthy tissue as possible to prevent permanent deformation\n",
      "and to enhance cosmesis.\n",
      "the success of clinical deployment of learning models heavily relies on\n",
      "approaches that are not only accurate but also interpretable.\n",
      "particularly, graph transformer networks (gtn) has have shown\n",
      "to further enhance the transparency of underlying relation between the graph\n",
      "nodes and decision making via attention mechanism [11].\n",
      "biological data, specially those acquired intra-opertively, are heterogeneous\n",
      "by nature.\n",
      "while the use of ex-vivo data collected under speciﬁc protocols are\n",
      "beneﬁcial to develop baseline models, intra-operative deployment of these models\n",
      "is challenging.\n",
      "for iknife, the ex-vivo data is usually collected from homogeneous\n",
      "regions of resected specimens under the guidance of a trained pathologist, ver-\n",
      "uncertainty-aware models in computer-assisted interventions can provide\n",
      "clinicians with feedback on prediction conﬁdence to increase their reliability dur-\n",
      "ing deployment.\n",
      "since the evidential approach jointly gen-\n",
      "erates the network prediction and uncertainty estimation, it seems more suitable\n",
      "for computationally eﬃcient intra-operative deployment.\n",
      "in this paper, we propose evidential graph transformer (egt), a combina-\n",
      "tion of graph-based feature-level attention mechanism with sample-level uncer-\n",
      "tainty estimation, to increase the performance and interpretability of surgical\n",
      "margin assessment.\n",
      "this is done by implementing the evidential loss and pre-\n",
      "diction functions within a graph transformer model to output the uncertainty,\n",
      "intermediate attention, and model prediction.\n",
      "to demonstrate the state-of-the-\n",
      "art performance of the proposed approach on mass spectrometry data, the model\n",
      "is compared with diﬀerent baselines in both cross-validation and prospective\n",
      "schemes on ex-vivo data.\n",
      "furthermore, the performance of model is also inves-\n",
      "tigated intraoperatively.\n",
      "the study\n",
      "is approved by the institutional research ethics board and patients consent to be\n",
      "included.\n",
      "each spectrum is then labeled based both on\n",
      "surgeons comments during the operation and post-operative pathology report.\n",
      "in the context of surgical margin assessment, the attentions reveal the rele-\n",
      "vant metabolic ranges to cancerous tissue, while uncertainty helps identify and\n",
      "ﬁlter data with unseen pathology.\n",
      "to\n",
      "ﬁt the dirichlet distribution to the output layer of our network, we use a loss\n",
      "function consisting of the prediction error lp\n",
      "i and the evidence adjustment le\n",
      "i\n",
      "li(θ) = lp\n",
      "i (θ) + λle\n",
      "i(θ)\n",
      "(2)\n",
      "where λ is the annealing coeﬃcient to balance the two terms.\n",
      "is kl diver-\n",
      "gence to the uniform dirichlet distribution [18].\n",
      "2.3\n",
      "experiments\n",
      "network/graph ablation: we explore the hyper-parameters of the proposed\n",
      "model in an extensive ablation study.\n",
      "for the evidential loss, we evaluate the choice of loss function\n",
      "(the 3 previously mentioned), and the annealing coeﬃcient (5–50 with step size\n",
      "evidential graph transformer\n",
      "567\n",
      "of 5).\n",
      "ex-vivo evaluation: the performance of the proposed network is compared\n",
      "with 3 baseline models including gtn, graph convolution network\n",
      "four-fold cross validation is used for compar-\n",
      "ison of the diﬀerent approaches, to increase the generalizability (3 folds for\n",
      "train/validation, test on remaining unseen fold, report average test performance).\n",
      "all experiments are implemented using pytorch\n",
      "with adam optimizer, learning rate of 10−4, batch size of 32, and early stopping\n",
      "based on validation loss.\n",
      "to demonstrate the robustness of the model and ensure\n",
      "it is not overﬁtting, we also report the performance of the ensemble model from\n",
      "the 4-fold cross validation study on the 5th unseen prospective test fold.\n",
      "clinical relevance: hormone receptor status plays an important role in deter-\n",
      "mining breast cancer prognosis and tailoring treatment plans for patients [6].\n",
      "these\n",
      "hormones are involved in diﬀerent types of signaling that the cell depends on [5].\n",
      "intra-operative deployment:\n",
      "to explore the intra-operative capability of\n",
      "the models, we deploy the ensemble models of the proposed method as well as\n",
      "the baselines from the cross-validation study to the bcs iknife stream.\n",
      "3\n",
      "results and discussion\n",
      "ablation study and ex-vivo evaluation: according to our ablation study,\n",
      "hyper parameters of 11 attention heads, 11 hidden features per attention head,\n",
      "the cross entropy loss function, and annealing coeﬃcient of 30, result in higher\n",
      "performances when compared to other conﬁgurations (370k learnable parame-\n",
      "ters).\n",
      "the performance of egt in comparison with the mentioned baselines are\n",
      "summarized in table 1.\n",
      "as can be seen, the proposed egt model with aver-\n",
      "age accuracy of 94.1% outperformed all the baselines statistically signiﬁcantly\n",
      "(maximum p-values of 0.02 in one-tail paired wilcoxon signed-rank test).\n",
      "lastly, when compared to other state-of-\n",
      "the-art baselines with uncertainty estimation mechanisms, the proposed evi-\n",
      "dential graph transformer network (average balanced accuracy of 91.6 ± 4.3%\n",
      "in table 1) outperforms mc dropout\n",
      "average(standard deviation) of accuracy (acc), balanced accuracy (bac)\n",
      "sensitivity (sen), speciﬁcity (spc), and the area under the curve (auc) for the\n",
      "proposed evidential graph transformer in comparison with graph transformer (gtn),\n",
      "graph convolution (gcn), and non-graph convolution (cnn) baselines.\n",
      "right eﬀect of uncertain data exclusion on accuracy and auc during model deploy-\n",
      "ment.\n",
      "this information can be provided during deployment to further augment\n",
      "surgical decision making for uncertain data instances.\n",
      "the result of our graph structure ablation shows the drop of average acc\n",
      "to 85.6% by randomizing the edges in the graph (p-value 0.004).\n",
      "although\n",
      "the model still trained due to node aggregation, random graph structure acts\n",
      "as noise and aﬀects the performance.\n",
      "we have also found that there’s\n",
      "more attention in this range for pr negative breast cancer in comparison pr\n",
      "positive, which is in concordance with previous literature demonstrating that\n",
      "these subtypes have higher glutamine metabolic activity [4,5].\n",
      "intra-operative deployment:\n",
      "the raw intra-operative iknife data (y-axis\n",
      "is m/z spectral range and x-axis is the surgery timeline) along with the tem-\n",
      "poral reference labels extracted from surgeon’s call-outs and pathology report\n",
      "are shown in fig.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf:\n",
      "however, recent research has revealed that deep\n",
      "neural networks for skin lesion recognition may overly depend on disease-\n",
      "irrelevant image artifacts (i.e. dark corners, dense hairs), leading to poor\n",
      "generalization in unseen environments.\n",
      "concretely, epvt leverages a set of\n",
      "domain prompts, each of which plays as a domain expert, to capture\n",
      "domain-speciﬁc knowledge; and a shared prompt for general knowledge\n",
      "over the entire dataset.\n",
      "experiments on four out-of-distribution datasets and six\n",
      "diﬀerent biased isic datasets demonstrate the superior generalization\n",
      "ability of epvt in skin lesion recognition across various environments.\n",
      "keywords: skin lesions · prompt · domain generalization · debiasing\n",
      "1\n",
      "introduction\n",
      "skin cancer is a serious and widespread form of cancer that requires early detec-\n",
      "tion for successful treatment.\n",
      "computer-aided diagnosis systems (cad) using\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43990-2_24.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "however, recent research has revealed that the success of these models\n",
      "may be a result of overly relying on “spurious cues” in dermoscopic images, such\n",
      "as rulers, gel bubbles, dark corners, and hairs\n",
      "when a deep learning model overﬁts speciﬁc artifacts instead of\n",
      "learning the correct dermoscopic patterns, it may fail to identify skin lesions in\n",
      "real-world environments where the artifacts are absent or inconsistent.\n",
      "to alleviate the artifact bias and enhance the model’s generalization ability,\n",
      "we rethink the problem from the domain generalization (dg) perspective, where\n",
      "a model trained within multiple diﬀerent but related domains are expected to\n",
      "perform well in unseen test domains.\n",
      "1, we deﬁne the domain\n",
      "labels based on the types of artifacts present in the training images, which can\n",
      "provide environment-aware prior knowledge reﬂecting a range of noisy contexts.\n",
      "previous dg algorithms learning domain-invariant features from source\n",
      "domains have succeeded in natural image tasks [2,17,19], but cannot directly\n",
      "apply to medical images, in particular skin images, due to the vast cross-domain\n",
      "diversity of skin lesions in terms of shapes, colors, textures, etc.\n",
      "to overcome the above problems, we propose an environment-aware prompt\n",
      "vision transformer (epvt) for domain generalization of skin lesion recognition.\n",
      "epvt: environment-aware prompt vision transformer\n",
      "251\n",
      "fig.\n",
      "2. the overview of our environment-aware prompt vision transformer (epvt).\n",
      "[8] is adopted to fully model the\n",
      "relationship between image tokens and prompt vectors.\n",
      "on the other hand, to\n",
      "encourage cross-domain information sharing while preserving the domain-speciﬁc\n",
      "knowledge of each domain prompt, we propose a domain prompt generator based\n",
      "on low-rank weights updating.\n",
      "additionally, we devise a domain mixup strategy to resolve\n",
      "the problem of co-occurring artifacts in dermoscopic images and mitigate the\n",
      "resulting noisy domain label assignments.\n",
      "our contributions can be summarized as: (1) we resolve an artifacts-derived\n",
      "biasing problem in skin cancer diagnosis using a novel environment-aware prompt\n",
      "learning-based dg algorithm, epvt; (2) epvt takes advantage of a vit-\n",
      "based domain-aware prompt learning and a novel domain prompt generator to\n",
      "improve domain-speciﬁc and cross-domain knowledge learning simultaneously;\n",
      "(3) a domain mixup strategy is devised to reduce the co-artifacts speciﬁc to\n",
      "dermoscopic images; (4) extensive experiments on four out-of-distribution skin\n",
      "datasets and six biased isic datasets demonstrate the outperforming general-\n",
      "ization ability and robustness of epvt under heterogeneous distribution shifts.\n",
      "(1)\n",
      "where f is the feature encoder of the vit, x0 denotes the class token, e0 is\n",
      "the image patch embedding, fm is the feature extracted by vit with the m-th\n",
      "prompt, and 0 is the index of the ﬁrst layer.\n",
      "domain prompts pd are a set of\n",
      "learnable tokens, with each prompt p m being fed into the vision transformer\n",
      "along with the image and corresponding class tokens from a speciﬁc domain.\n",
      "through optimizing, each prompt becomes a domain expert only responsible\n",
      "for the images from its own domain.\n",
      "our approach is inspired by model adaptation\n",
      "and multi-task learning techniques used in natural language processing [13,26].\n",
      "aghajanyan et al.\n",
      "to this end, we decompose each p m into a hadamard\n",
      "product between a randomly initialized shared prompt p ∗ and a rank-one matrix\n",
      "pk obtained from two randomly initialized learnable vectors uk and vk, which\n",
      "is:\n",
      "p m = p ∗ ⊙ pk\n",
      "where\n",
      "pk = uk · vt\n",
      "k\n",
      "(2)\n",
      "epvt: environment-aware prompt vision transformer\n",
      "253\n",
      "where p m represents the domain-speciﬁc prompt, computed by hadamard prod-\n",
      "uct of p ∗ and pk.\n",
      "here, p ∗ ∈ rs×d is utilized to learn general knowledge, with s\n",
      "and d representing the dimensions of the prompt vector and feature embedding\n",
      "respectively.\n",
      "by using the hadamard product, the model can eﬃciently leverage cross-domain\n",
      "knowledge for target domain prediction.\n",
      "2.3\n",
      "mitigating the co-artifacts issue\n",
      "the artifacts-based domain labels can provide domain information for der-\n",
      "moscopic images.\n",
      "instead of assign-\n",
      "ing a hard prediction label (“0” or “1”) to each image, in each batch, we mix\n",
      "every image using two randomly selected images from two diﬀerent domains.\n",
      "we then\n",
      "apply the cross-entropy loss to the corresponding labels of bot images, as shown\n",
      "in fig.\n",
      "this strategy can overcome\n",
      "the challenge of ambiguous domain labels in dermoscopic images and improve\n",
      "the performance of our model.\n",
      "2.4\n",
      "optimization\n",
      "so far, we have introduced lmixup in eq. 3 for optimizing our model.\n",
      "however,\n",
      "since our goal is to generalize the model to unseen environments, we also need\n",
      "to take advantage of each domain prompt.\n",
      "[30] that learns the linear correlation\n",
      "between the domain prompts and the target image prediction.\n",
      "=\n",
      "m\n",
      "\u0002\n",
      "m=1\n",
      "wm · p m,\n",
      "s.t.\n",
      "m\n",
      "\u0002\n",
      "m=1\n",
      "wm = 1\n",
      "(4)\n",
      "where a represents an adapter containing a two-layer mlp with a softmax layer,\n",
      "and wm denotes the learned weights.\n",
      "to train the adapter a, we simulate the inference process for each image in\n",
      "the source domain by treating it as an image from the pseudo-target domain.\n",
      "then\n",
      "we calculated the adapted prompt padapted for the pseudo-target environment\n",
      "image x using the adapter a: padapted = a( ˆ\n",
      "fm(x)).\n",
      "to ensure that the adapter learns the correct linear correlation between the\n",
      "domain prompts and the target image, we use the domain label from source\n",
      "domains to directly supervise the weights wm.\n",
      "we also use the cross-entropy loss\n",
      "to maintain the model performance with the adapted prompt:\n",
      "ladapted = lce(h( ˆfm(x)), y)\n",
      "3\n",
      "experiments\n",
      "experimental setup: we consider two challenging melanoma-benign classi-\n",
      "ﬁcation settings that can eﬀectively evaluate the generalization ability of our\n",
      "model in diﬀerent environments and closely mimic real-world scenarios.\n",
      "we use\n",
      "the artifacts annotations from [3] and divide the training set of isic2019 into ﬁve\n",
      "groups: dark corner, hair, gel bubble, ruler, and clean, with 2351, 4884, 1640, 672,\n",
      "and 2796 images, respectively.\n",
      "it’s worth noting that isic2019, derm7pt-\n",
      "dermoscopic, and ph2 are dermoscopic images, while derm7pt-clinical and\n",
      "pad are clinical images.\n",
      "(2) trap set debiasing: we train and test our epvt\n",
      "with its baseline on six trap sets [3] with increasing bias levels, ranging from 0\n",
      "(randomly split training and testing sets from the isic2019 dataset) to 1 (the\n",
      "highest bias level where the correlation between artifacts and class label is in\n",
      "the opposite direction in the dataset splits).\n",
      "more details about these datasets\n",
      "and splits are provided in the complementary material.\n",
      "implementation details: for a fair comparison, we train all models using vit-\n",
      "base/16\n",
      "[8] backbone pre-trained on imagenet and report the roc-auc with\n",
      "ﬁve random seeds.\n",
      "we conduct a grid search over learning rate\n",
      "(from 3e−4 to 5e−6), weight decay (from 1e−2 to 1e−5), and the length of the\n",
      "epvt: environment-aware prompt vision transformer\n",
      "255\n",
      "table 1.\n",
      "the comparison on out-of-distribution datasets\n",
      "method\n",
      "derm7pt_d\n",
      "derm7pt_c\n",
      "pad\n",
      "ph2\n",
      "average\n",
      "erm\n",
      "81.24 ± 1.6\n",
      "71.61 ± 1.9\n",
      "82.62 ± 1.6\n",
      "83.06 ± 1.9\n",
      "79.63 ± 1.5\n",
      "dro\n",
      "[30] 82.38 ± 1.0\n",
      "71.61 ± 1.7\n",
      "83.81 ± 1.4\n",
      "91.33 ± 1.8\n",
      "82.06 ± 1.6\n",
      "selfreg [15]\n",
      "81.83 ± 1.9\n",
      "73.29 ± 1.4\n",
      "85.27 ± 1.3\n",
      "85.16 ± 3.3\n",
      "81.12 ± 1.0\n",
      "epvt (ours)\n",
      "83.69 ± 1.4 73.96 ± 1.6 86.67 ± 1.5\n",
      "91.91 ± 1.5 84.11 ± 1.4\n",
      "prompt (from 4 to 16, when available) and report the best performance of all\n",
      "models.\n",
      "we resize the input image to a size of 224 × 224 and adopt\n",
      "the standard data augmentation like random ﬂip, crop, rotation, and color jitter.\n",
      "an early stopping with the patience of 22 is set and with a total of 60 epochs\n",
      "for ood evaluation and 100 epochs for trap set debiasing.\n",
      "all experiments are\n",
      "conducted on a single nvidia rtx 3090 gpu.\n",
      "out-of-distribution evaluation: table 1 presents a comprehensive compar-\n",
      "ison of our epvt algorithm with existing domain generalization methods.\n",
      "the\n",
      "results clearly demonstrate the superiority of our approach, with the best perfor-\n",
      "mance on three out of four ood datasets and remarkable improvements over the\n",
      "erm algorithm, especially achieving 4.1% and 8.9% improvement on the pad\n",
      "and ph2 datasets, respectively.\n",
      "although some algorithms may perform simi-\n",
      "larly to our model on one of the four datasets, none can consistently match the\n",
      "performance of our method across all four datasets.\n",
      "particularly, our approach\n",
      "showcases the highest average performance, with a 2.05% improvement over the\n",
      "second-best algorithm across all four datasets.\n",
      "these ﬁndings highlight the eﬀec-\n",
      "tiveness of our algorithm in learning robust features and its strong generalization\n",
      "abilities across diverse environments.\n",
      "firstly, we observe\n",
      "that the baseline model with prompt only improves the average performance by\n",
      "256\n",
      "s. yan et al.\n",
      "table 2. ablation study on out-of-distribution datasets\n",
      "method\n",
      "derm7pt_d\n",
      "derm7pt_c\n",
      "pad\n",
      "ph2\n",
      "average\n",
      "baseline\n",
      "81.24 ± 1.6\n",
      "71.61 ± 1.9\n",
      "82.62 ± 1.6\n",
      "83.06 ± 1.9\n",
      "79.63 ± 1.5\n",
      "+p\n",
      "82.13 ± 1.1\n",
      "71.41 ± 1.3\n",
      "82.15 ± 1.6\n",
      "84.21 ± 1.4\n",
      "79.73 ± 1.3\n",
      "+p+a\n",
      "82.55 ± 1.6\n",
      "72.86 ± 1.1\n",
      "81.02 ± 1.5\n",
      "84.97 ± 1.8\n",
      "81.10 ± 1.6\n",
      "+p+a+m\n",
      "81.43 ± 1.4\n",
      "73.18 ± 1.5\n",
      "85.78 ± 1.9\n",
      "89.28 ± 1.3\n",
      "82.42 ± 1.7\n",
      "3. (a) deibiasing evaluation (b) domain distance (c) domain weights\n",
      "0.1%, showing that simply combining prompt does not very helpful for domain\n",
      "generalization.\n",
      "when we combine the adapter, the model’s average performance\n",
      "improves by 1.37%, but it performs worse than erm on pad dataset.\n",
      "subse-\n",
      "quently, we added domain mixup and domain prompt generator to the model,\n",
      "resulting in signiﬁcant further improvements in the model’s average performance\n",
      "by 1.32% and 1.69%, respectively.\n",
      "the consistently better performance than the\n",
      "baseline on all four datasets also highlights the importance of addressing co-\n",
      "artifacts and cross-domain learning for dg in skin lesion recognition.\n",
      "trap set debiasing: in fig.\n",
      "3a, we present the performance of the erm base-\n",
      "line and our epvt on six biased isic2019 datasets.\n",
      "each point on the graph\n",
      "represents an algorithm that is trained and tested on a speciﬁc bias degree split.\n",
      "the graph shows that the erm baseline performs better than our epvt when\n",
      "the bias is low (0 and 0.3).\n",
      "as the bias degree increases, the correlation between artifacts and\n",
      "class labels decreases, and overﬁtting the train set causes the performance of\n",
      "erm to drop dramatically on the test set with a signiﬁcant distribution diﬀer-\n",
      "ence.\n",
      "in contrast, our epvt exhibits greater robustness to diﬀerent bias levels.\n",
      "notably, our epvt outperforms the erm baseline by 9.4% on the bias 1 dataset.\n",
      "[9] between each domain and the target\n",
      "dataset using the extracted feature, representing the domain distance between\n",
      "epvt: environment-aware prompt vision transformer\n",
      "257\n",
      "them.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_48.pdf:\n",
      "both cnn-based and transformer-based object detection\n",
      "with bounding box representation have been extensively studied in com-\n",
      "puter vision and medical image analysis, but circular object detection in\n",
      "medical images is still underexplored.\n",
      "speciﬁcally, queries with circle representation\n",
      "in transformer decoder iteratively reﬁne the circular object detection\n",
      "results, and a circle cross attention module is introduced to compute the\n",
      "similarity between circular queries and image features.\n",
      "moreover, our approach is easy to generalize to\n",
      "the segmentation task by adding a simple segmentation branch to cir-\n",
      "cleformer.\n",
      "we evaluate our method in circular nuclei detection and seg-\n",
      "mentation on the public monuseg dataset, and the experimental results\n",
      "show that our method achieves promising performance compared with\n",
      "the state-of-the-art approaches.\n",
      "our code is released at: https://\n",
      "github.com/zhanghx-iim-ahu/circleformer.\n",
      "keywords: circular object analysis · circular queries · transformer\n",
      "1\n",
      "introduction\n",
      "nuclei detection is a highly challenging task and plays an important role in many\n",
      "biological applications such as cancer diagnosis and drug discovery.\n",
      "1.\n",
      "transformer-based\n",
      "circle\n",
      "detection and segmentation.\n",
      "however,\n",
      "recent studies on transformer-based detection methods are designed for rectan-\n",
      "gle object detection in computer vision, which are not speciﬁcally designed for\n",
      "circular objects in medical images.\n",
      "circleformer: circular nuclei detection\n",
      "495\n",
      "in this paper, we introduce circleformer, a transformer-based circular object\n",
      "detection for medical image analysis.\n",
      "we propose a novel circle cross\n",
      "attention module which enables us to apply circle center (x, y) to extract image\n",
      "features around a circle and make use of circle radius to modulate the cross\n",
      "attention map.\n",
      "we evaluate our circleformer on\n",
      "the public monuseg dataset for nuclei detection in whole slide images.\n",
      "experi-\n",
      "mental results show that our method outperforms both cnn-based methods for\n",
      "box detection and circular object detection.\n",
      "to further study the generalization ability of our approach, we add a\n",
      "simple segmentation branch to circleformer following the recent query based\n",
      "instance segmentation models [2,17] and verify its performance on monuseg as\n",
      "well.\n",
      "kx,y = concat(fx,y, pe(x, y)), vx,y = fx.y,\n",
      "(3)\n",
      "496\n",
      "h. zhang et al.\n",
      "where fx,y ∈ rd denote the image feature at position (x, y) and an mlp(csq) :\n",
      "rd → rd is used to obtain a scaled vector conditioned on content information\n",
      "for a query.\n",
      "by representing a circle query as (x, y, r), we can reﬁne the circle query\n",
      "layer-by-layer in the transformer decoder.\n",
      "in this way, the circle query\n",
      "representation is suitable for circular object detection and is able to accelerate\n",
      "the learning convergence via layer-by-layer reﬁnement scheme.\n",
      "2.3\n",
      "circle cross attention\n",
      "we propose circle-modulated attention and deformable circle cross attention to\n",
      "consider size information of circular object detection in cross attention module.\n",
      "the circle radius modulated positional atten-\n",
      "tion map provides beneﬁts to extract image features of objects with diﬀerent\n",
      "scales.\n",
      "ma((x, y), (xref, yref))\n",
      "given an input feature map f ∈ rc×h×w , let i index a query element\n",
      "with content feature zi and a reference point pi, the deformable circle cross\n",
      "attention feature is calculated by:\n",
      "cda(zi, pi, f) =\n",
      "m\n",
      "\u0002\n",
      "m=1\n",
      "wm\n",
      "k\n",
      "\u0002\n",
      "k=1\n",
      "attnmik ˙w ′\n",
      "mf ((pix + δrmik\n",
      "×ri,ref ×\n",
      "experiments show that cda-c initialization of reference\n",
      "points outperforms others.\n",
      "2.5\n",
      "circle instance segmentation\n",
      "a mask is predicted from a decoder embedding by ˆmi = ffn(ffn(fi)\n",
      "we\n",
      "use dice and bce as the segmentation loss: lseg = λdiceldice(mi, ˆmi) +\n",
      "λbcelbce(mi, ˆmi) between prediction ˆmi and the groundtruth mi.\n",
      "2.6\n",
      "generalized circle iou\n",
      "circlenet extends intersection over union (iou) of bounding boxes to circle iou\n",
      "(ciou) and shows that the ciou is a valid overlap metric for detection of circular\n",
      "objects in medical images.\n",
      "we show that gciou can bring consistent improvement\n",
      "on circular object detection.\n",
      "figure 4 shows the diﬀerent measurements between\n",
      "two rectangles and circles.\n",
      "following detr, i-th each element of the groundtruth\n",
      "set is yi = (li, ci), where li is the target class label (which may be ∅) and\n",
      "ci = (x, y, r).\n",
      "+ i{li̸=∅}lcircle(ci, ˆcσ(i)),\n",
      "(6)\n",
      "where σ ∈ sn is a permutation of all prediction elements, ˆyσ(i) = (ˆlσ(i), ˆcσ(i)) is\n",
      "the prediction, λfocal ∈ r are hyperparameters, and lfocal is focal loss [8].\n",
      "mi is the ground truth obtained by roi align [5]\n",
      "corresponding to ˆmi.\n",
      "498\n",
      "h. zhang et al.\n",
      "3\n",
      "experiment\n",
      "3.1\n",
      "dataset and evaluation\n",
      "monuseg dataset.\n",
      "monuseg dataset is a public dataset from the 2018 multi-\n",
      "organ nuclei segmentation challenge [6].\n",
      "it contains 30 training/validataion\n",
      "tissue images sampled from a separate whole slide image of h&e stained tissue\n",
      "and 14 testing images of lung and brain tissue images.\n",
      "following [16], we ran-\n",
      "domly sample 10 patches with size 512 ×512 from each image and create 200\n",
      "training images, 100 validation images and 140 testing images.\n",
      "[16], and ap m for the instance segmentation evaluation metrics.\n",
      "s\n",
      "and m are used to measure the performance of small scale with area less than\n",
      "322 and median scale with area between 322 and 962.\n",
      "3.2\n",
      "implementation details\n",
      "two variants of our proposed method for nuclei detection, circleformer and\n",
      "circleformer-d are built with a circle cross attention module and a deformable\n",
      "circle cross attention module, respectively.\n",
      "circleformer-d-joint (ours) extends\n",
      "circleformer-d to include instance segmentation as additional output.\n",
      "since the maximum number of objects per image in the dataset is\n",
      "close to 1000, we set the number of queries to 1000.\n",
      "we use λiou = 2.0, λc = 5.0,\n",
      "λdice = 8.0 and λbce = 2.0 in the experiments.\n",
      "in summary, our circleformer designed for circular object detection\n",
      "achieves superior performance compared to both cnn-based box detection and\n",
      "cnn-based circle detection approaches.\n",
      "our circleformer with detection head also yields better performance than\n",
      "transformer-based methods.\n",
      "[10]\n",
      "box\n",
      "✓\n",
      "resnet50\n",
      "49.6\n",
      "89.5\n",
      "51.5\n",
      "50.1\n",
      "31.9\n",
      "circleformer\n",
      "circle\n",
      "resnet50\n",
      "49.7\n",
      "88.8\n",
      "50.9\n",
      "51.1\n",
      "35.4\n",
      "circleformer-d\n",
      "circle\n",
      "✓\n",
      "resnet50\n",
      "52.9\n",
      "89.6\n",
      "58.7\n",
      "54.1\n",
      "31.7\n",
      "circleformer-d-joint\n",
      "circle\n",
      "✓\n",
      "resnet50\n",
      "53.0\n",
      "90.0\n",
      "59.0\n",
      "53.9\n",
      "32.8\n",
      "table 2. results of nuclei joint detection and segmentation on monuseg dataset.\n",
      "detection\n",
      "segmentation\n",
      "methods\n",
      "ap\n",
      "ap(50)\n",
      "ap(75)\n",
      "[17]\n",
      "44.8\n",
      "82.6\n",
      "45.2\n",
      "45.5\n",
      "27.5\n",
      "41.3\n",
      "80.6\n",
      "38.8\n",
      "41.3\n",
      "40.7\n",
      "deformable-detr-joint\n",
      "45.7\n",
      "86.7\n",
      "43.6\n",
      "46.2\n",
      "28.2\n",
      "43.5\n",
      "84.8\n",
      "40.5\n",
      "43.5\n",
      "42.3\n",
      "circleformer-d-joint\n",
      "53.0\n",
      "90.0\n",
      "59.0\n",
      "53.8\n",
      "32.8\n",
      "44.4\n",
      "84.5\n",
      "43.5\n",
      "44.4\n",
      "45.3\n",
      "jointly outputs detection and segmentation results additionally boosts the detec-\n",
      "tion results of circleformer-d.\n",
      "experiments of joint nuclei detection and segmentation are listed in table 2.\n",
      "our method outperforms queryinst\n",
      "[2], a cnn-based instance segmentation\n",
      "method and soit\n",
      "[17], an transformer-based instance segmentation approach.\n",
      "we extend transformer-based box detection method to provide additional seg-\n",
      "mentation output inside the detection region, denoted as deformable-detr-\n",
      "joint.\n",
      "our method with circular query representation largely improves both\n",
      "detection and segmentation results.\n",
      "to summarize, our method with only detection head outperforms both cnn-\n",
      "based methods and transformer based approaches in most evaluation metrics\n",
      "for circular nuclei detection task.\n",
      "our circleformer-d-joint provides superior\n",
      "results compared to cnn-based and transformer-based instance segmentation\n",
      "methods.\n",
      "also, our method with joint detection and segmentation outputs also\n",
      "improves the detection-only setting.\n",
      "in circleformer, the proposed circle-modulated attention (c-ma) improves\n",
      "the performance of box ap from 45.7% to 48.6% box ap (row 1 and row 2\n",
      "in p1).\n",
      "we replaced circle iou (ciou) loss with generalized circle iou (gciou)\n",
      "loss, the performance is further boosted by 2.2% (row 2 and row 3 in p1).\n",
      "we obtain similar observations of circleformer-d. when using standard\n",
      "deformable attention (sda), learning ciou loss gives a 1.2% improvement on\n",
      "box ap compared to using box iou (row 1 and row 2 in p2).\n",
      "replacing ciou\n",
      "with gciou, the performances of sda (row 2 and row 5 in p2), cda-r (row\n",
      "3 and row 6 in p2) and cda-c (row 4 and row 7 in p2) are boostd by 0.3%\n",
      "box ap, 0.7% box ap and 1.8% box ap, respectively.\n",
      "we\n",
      "vary the number of heads for multi-head attention and the performance of the\n",
      "model is shown in the table 4.\n",
      "we ﬁnd that the performance increases gradually\n",
      "as the number of heads increases up to 8.\n",
      "however, the performance drops when\n",
      "the number of head is 16.\n",
      "we assume increasing the number of heads brings too\n",
      "many parameters and makes the model diﬃcult to converge.\n",
      "we\n",
      "ﬁnd that 4 reference points give the best performance.\n",
      "therefore, we choose to\n",
      "use 8 attention heads of decoder and use 4 reference points in the cross attention\n",
      "module through all the experiments.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf:\n",
      "whole slide image (wsi) classiﬁcation is a critical task\n",
      "in computational pathology, requiring the processing of gigapixel-sized\n",
      "images, which is challenging for current deep-learning methods.\n",
      "due to the lack of task-speciﬁc annotated data, these features\n",
      "are either obtained from well-established backbones on natural images,\n",
      "or, more recently from self-supervised models pretrained on histopathol-\n",
      "ogy.\n",
      "however, both approaches yield task-agnostic features, resulting in\n",
      "performance loss compared to the appropriate task-related supervision, if\n",
      "available.\n",
      "extensive experiments on three wsi\n",
      "datasets, tcga-brca, tcga-crc, and bright, demonstrate the\n",
      "superiority of prompt-mil over conventional mil methods, achieving a\n",
      "relative improvement of 1.49%–4.03% in accuracy and 0.25%–8.97% in\n",
      "auroc while using fewer than 0.3% additional parameters.\n",
      "compared\n",
      "to conventional full ﬁne-tuning approaches, we ﬁne-tune less than 1.3%\n",
      "of the parameters, yet achieve a relative improvement of 1.29%–13.61%\n",
      "in accuracy and 3.22%–27.18% in auroc and reduce gpu memory\n",
      "consumption by 38%–45% while training 21%–27% faster.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43993-3_60.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43993-3_60\n",
      "prompt-mil: boosting mil schemes via prompt tuning\n",
      "625\n",
      "keywords: whole slide image classiﬁcation · multiple instance\n",
      "learning · prompt tuning\n",
      "1\n",
      "introduction\n",
      "whole slide image (wsi) classiﬁcation is a critical task in computational pathol-\n",
      "ogy enabling disease diagnosis and subtyping using automatic tools.\n",
      "imagenet pretrained networks have been widely used as mil feature extractors.\n",
      "more recently, self-supervised learning (ssl), using a large amount of unlabeled\n",
      "histopathology data, has become quite popular for wsi classiﬁcation [5,13] as\n",
      "it outperforms imagenet feature encoders.\n",
      "most existing mil methods do not ﬁne-tune their feature extractor together\n",
      "with their classiﬁcation task; this stems from the requirement for far larger\n",
      "gpu memory than is available currently due to the gigapixel nature of wsis,\n",
      "e.g. training a wsi at 10x magniﬁcation may require more than 300 gb of\n",
      "gpu memory.\n",
      "these methods show better performance compared to\n",
      "conventional mil; they suﬀer, however, from two limitations.\n",
      "first, they are\n",
      "imagenet-pretrained and do not leverage the powerful learning capabilities of\n",
      "histology-trained ssl models.\n",
      "however, we ﬁnd that conven-\n",
      "tional ﬁne-tuning approaches, where the entire network is ﬁne-tuned, achieve low\n",
      "performance.\n",
      "the poor performance\n",
      "is probably caused by the large network over-ﬁtted to the limited downstream\n",
      "training data, leading to suboptimal feature representation.\n",
      "indeed, especially\n",
      "for weakly supervised wsi classiﬁcation, where annotated data for downstream\n",
      "tasks is signiﬁcantly less compared to natural image datasets, conventional ﬁne-\n",
      "tuning schemes can prove to be quite challenging.\n",
      "to address the subpar performance of ssl-pretrained vision transformers,\n",
      "we utilize the prompt tuning techniques.\n",
      "initially proposed in natural language\n",
      "processing, a prompt is a trainable or a pre-deﬁned natural language statement\n",
      "that is provided as additional input to a transformer to guide the neural net-\n",
      "work towards learning a speciﬁc task or objective [3,12].\n",
      "recently, prompts have also been adopted in computer vision and\n",
      "demonstrated superior performance compared to conventional ﬁne-tuning meth-\n",
      "ods [10].\n",
      "our con-\n",
      "tributions are:\n",
      "– fine-tuning: unlike existing works in histopathology image analysis,\n",
      "prompt-mil is ﬁne-tuned using prompts rather than conventional full ﬁne-\n",
      "tuning methods.\n",
      "extensive experiments on three public wsi datasets, tcga-brca, tcga-\n",
      "crc, and bright demonstrate the superiority of prompt-mil over conven-\n",
      "tional mil methods, achieving a relative improvement of 1.49%–4.03% in accu-\n",
      "racy and 0.25%–8.97% in auroc by using only less than 0.3% additional\n",
      "parameters.\n",
      "compared to the conventional full ﬁne-tuning approach, we ﬁne-\n",
      "tune less than 1.3% of the parameters, yet achieve a relative improvement of\n",
      "1.29%–13.61% in accuracy and 3.22%–27.18% in auroc.\n",
      "given a wsi and its label y, the image is tiled into n tissue\n",
      "prompt-mil: boosting mil schemes via prompt tuning\n",
      "627\n",
      "patches/instances {x1, x2, . . .\n",
      "the vit ﬁrst divides an input image xi into w smaller patches [z1, z2, . . .\n",
      "an input image xi is cropped into w small\n",
      "patches z1, . .\n",
      ", f(xn, p)]), y),\n",
      "(7)\n",
      "where only the parameters of the g(·) and the prompt p are optimized, while\n",
      "the feature extractor model f(·) is frozen.\n",
      "training the entire pipeline in an end-to-end fashion on gigapixel images is\n",
      "infeasible using the current hardware.\n",
      "in this step, we just conduct a forward pass like the inference stage, with-\n",
      "out storing the memory-intensive computational graph for back-propagation.\n",
      "3\n",
      "experiments and discussion\n",
      "3.1\n",
      "datasets\n",
      "we assessed prompt-mil using three histopathological wsi datasets: tcga-\n",
      "brca\n",
      "reported metrics\n",
      "(in %age) are the average across 3 runs.\n",
      "“num. of parameters” represents the number\n",
      "of optimized parameters\n",
      "dataset\n",
      "tcga-brca\n",
      "tcga-crc\n",
      "bright\n",
      "num. of\n",
      "metric\n",
      "accuracy auroc accuracy auroc accuracy auroc parameters\n",
      "conventional mil\n",
      "92.10\n",
      "96.65\n",
      "73.02\n",
      "69.24\n",
      "62.08\n",
      "80.96\n",
      "70k\n",
      "full ﬁne-tuning\n",
      "88.14\n",
      "93.78\n",
      "74.53\n",
      "56.63\n",
      "56.13\n",
      "75.87\n",
      "5.6m\n",
      "prompt-mil (ours) 93.47\n",
      "96.89\n",
      "75.47\n",
      "75.45\n",
      "64.58\n",
      "81.31\n",
      "70k + 192\n",
      "630\n",
      "j. zhang et al.\n",
      "3.2\n",
      "implementation details\n",
      "we cropped non-overlapping 224 × 224 sized patches in all our experiments\n",
      "and used vit-tiny (vit-t/16)\n",
      "for ssl pretraining,\n",
      "we leveraged the dino framework [4] with the default hyperparameters, but\n",
      "adjusted the batch size to 256 and employed the global average pooling for token\n",
      "aggregation.\n",
      "we\n",
      "applied a cosine annealing learning rate decay policy in all our experiments.\n",
      "for all full\n",
      "ﬁne-tuning experiments, we used the learning rate in the corresponding prompt\n",
      "experiment as the base learning rate.\n",
      "all model implementations were in pytorch\n",
      "evaluation of prompt tuning performance: we compared the proposed\n",
      "prompt-mil with two baselines: 1) a conventional mil model with a frozen\n",
      "feature extractor [13], 2) ﬁne-tuning all parameters in the feature model (full\n",
      "ﬁne-tuning).\n",
      "compared to the conventional mil method, prompt-mil added negli-\n",
      "gible parameters (192, less than 0.3% of the total parameters), achieving a\n",
      "relative improvement of 1.49% in accuracy and 0.25% in auroc on tcga-\n",
      "brca, 3.36% in accuracy and 8.97% in auroc on tcga-crc, and 4.03% in\n",
      "accuracy and 0.43% in auroc on bright.\n",
      "the observed improvement can\n",
      "be attributed to a more optimal alignment between the feature representation\n",
      "learned during the ssl pretraining and the downstream task, i.e., the prompt\n",
      "explicitly calibrated the features toward the downstream task.\n",
      "compared to the full ﬁne-tuning method, our\n",
      "method achieved a relative improvement of 1.29% to 13.61% in accuracy and\n",
      "3.22% to 27.18% in auroc on the three datasets.\n",
      "wsi size\n",
      "44k × 21k 26k × 21k 22k × 17k 11k × 16k\n",
      "#tissue patches\n",
      "9212\n",
      "4765\n",
      "2307\n",
      "1108\n",
      "gpu mem.\n",
      "full ﬁne-tuning\n",
      "21.81g\n",
      "18.22g\n",
      "16.37g\n",
      "12.71g\n",
      "prompt (ours)\n",
      "12.04g\n",
      "10.66g\n",
      "10.00g\n",
      "7.90g\n",
      "reduction percentage 44.79%\n",
      "41.50%\n",
      "38.92%\n",
      "37.84%\n",
      "time per slide full ﬁne-tuning\n",
      "17.73 s\n",
      "8.92 s\n",
      "4.37 s\n",
      "2.15 s\n",
      "prompt (ours)\n",
      "13.92 s\n",
      "7.09 s\n",
      "3.35s\n",
      "1.56 s\n",
      "reduction percentage 21.49%\n",
      "20.51%\n",
      "23.32%\n",
      "27.27%\n",
      "table 3. comparison of accuracy and auroc on three datasets for a pathological\n",
      "foundation model.\n",
      "evaluation on the pathological foundation models: we demonstrated\n",
      "our prompt-mil also had a better performance when used with the pathologi-\n",
      "cal foundation model.\n",
      "in table 3, we showed\n",
      "that our method robustly boosted the performance on both tcga (the same\n",
      "632\n",
      "j. zhang et al.\n",
      "domain as the foundation model trained on) and bright (a diﬀerent domain).\n",
      "the improvement is more prominent in bright, which further conﬁrmed that\n",
      "prompt-mil aligns the feature extractor to be more task-speciﬁc.\n",
      "table 4. performance with a diﬀerent number of prompt tokens.\n",
      "for two diﬀerent wsi\n",
      "classiﬁcation tasks, one token was enough to boost the performance of the conventional\n",
      "mil schemes.\n",
      "on the tcga-\n",
      "brca dataset, our prompt-mil model with 1 to 3 prompt tokens reported\n",
      "similar performance.\n",
      "on the bright dataset, the performance of our model\n",
      "dropped with the increased number of prompt tokens.\n",
      "empirically, this ablation\n",
      "study shows that for classiﬁcation tasks, one prompt token is suﬃcient to boost\n",
      "the performance of conventional mil methods.\n",
      "4\n",
      "conclusion\n",
      "in this work, we introduced a new framework, prompt-mil, which combines the\n",
      "use of multiple instance learning (mil) with prompts to improve the perfor-\n",
      "mance of wsi classiﬁcation.\n",
      "in such a scheme, only a small fraction of parameters calibrates the pretrained\n",
      "representations to encode task-speciﬁc information, so the entire training can be\n",
      "performed in an end-to-end manner.\n",
      "extensive experiments demonstrated the superiority\n",
      "of prompt-mil over the conventional mil as well as the conventional fully ﬁne-\n",
      "tuning methods.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf:\n",
      "consideration of subgroups or domains within medical image\n",
      "datasets is crucial for the development and evaluation of robust and gen-\n",
      "eralizable machine learning systems.\n",
      "the variational deep embed-\n",
      "ding (vade) model is trained to learn lower-dimensional representations\n",
      "of images based on a mixture-of-gaussians latent space prior distribu-\n",
      "tion while optimizing cluster assignments.\n",
      "our experimental results demonstrate that\n",
      "the considered models are capable of separating digital pathology images\n",
      "into meaningful subgroups.\n",
      "we provide a general-purpose implementa-\n",
      "tion of all considered deep clustering methods as part of the open source\n",
      "python package domid (https://github.com/didsr/domid).\n",
      "keywords: domain identiﬁcation · deep clustering · subgroup\n",
      "identiﬁcation · variational autoencoder · generative model\n",
      "1\n",
      "introduction\n",
      "machine learning (ml), speciﬁcally deep learning (dl), algorithms have shown\n",
      "exceptional performance on numerous medical image analysis tasks [2].\n",
      "never-\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43993-3 64.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "for a generalizabil-\n",
      "ity assessment, reporting only aggregate performance measures is not suﬃcient.\n",
      "due to model complexity and limited training data, ml performance often varies\n",
      "across data subgroups or domains, such as diﬀerent patient subpopulations or\n",
      "varied data acquisition scenarios.\n",
      "aggregate performance measures (e.g., sensi-\n",
      "tivity, speciﬁcity, roc auc) can be dominated by the larger subgroups, masking\n",
      "the poor ml model performance on smaller but clinically important subgroups\n",
      "[11].\n",
      "thus, achieving (through training) and demonstrating (as part of testing)\n",
      "satisfactory ml model performance across relevant subgroups is crucial before\n",
      "the real-world clinical deployment of a medical ml system [13].\n",
      "however, a challenging situation arises when relevant subgroups are unrec-\n",
      "ognized.\n",
      "however, due to the complexity and high dimensionality of the medical\n",
      "imaging data and the resulting diﬃculty in establishing a concrete notion of\n",
      "similarity, extracting low-dimensional characteristics becomes the key to estab-\n",
      "lishing the best criteria for grouping.\n",
      "unsupervised generative clustering aims\n",
      "to simultaneously address both domain identiﬁcation and dimensionality reduc-\n",
      "tion.\n",
      "deep unsupervised clustering algorithms could map the medical imaging\n",
      "data back to their causal factors or underlying domains, such as image acqui-\n",
      "sition equipment, patient subpopulations, or other meaningful data subgroups.\n",
      "the resulting algorithmic cluster assignments could then be\n",
      "used to improve ml algorithm training, or for generalizability and robustness\n",
      "evaluation.\n",
      "2\n",
      "methods\n",
      "we provide a pytorch-based implementation of all deep clustering algorithms\n",
      "described below (vade, cdvade, and dec) in the open source python package\n",
      "domid that is publicly available under https://github.com/didsr/domid.\n",
      "2.1\n",
      "variational deep embedding (vade)\n",
      "variational deep embedding (vade)\n",
      "the encoder learns to\n",
      "668\n",
      "m. sidulova et al.\n",
      "compress the high-dimensional input images x into lower-dimensional latent rep-\n",
      "resentations z. using a mixture-of-gaussians (mog) prior distribution for the\n",
      "latent representations z, we examine subgroups or domains within the dataset,\n",
      "revealed by the individual gaussians within the learned latent space, and how z\n",
      "aﬀects the generation of x. the model can be used to perform inference, where\n",
      "observed images x are mapped to corresponding latent variables z and their\n",
      "cluster/domain assignments c. we denote the latent space dimensionality by d\n",
      "(i.e., z ∈ rd), and the number of clusters by d (i.e., c ∈ {1, 2, . . .\n",
      "the\n",
      "trained decoder cnn can also be used to generate synthetic images from the\n",
      "algorithmically identiﬁed subgroups.\n",
      "finally, the cluster assignments can be determined via\n",
      "q(c|x)\n",
      "we\n",
      "refer to [6] for details.\n",
      "in all our experiments, we apply vade with cnn architectures for the\n",
      "encoder and decoder.\n",
      "speciﬁcally, the generative process of cdvade takes the form\n",
      "p(c) = cat(π)\n",
      "(4)\n",
      "p(z|c) = n\n",
      "\u0004\n",
      "z; μc, diag\n",
      "\u0004\n",
      "σ2\n",
      "c\n",
      "\u0005\u0005\n",
      ",\n",
      "(5)\n",
      "\u0004\n",
      "μxy, log σ2\n",
      "xy\n",
      "\u0005\n",
      "= f(z, y; φ),\n",
      "(6)\n",
      "p(x|z, y) = n\n",
      "\u0004\n",
      "x; μxy, diag\n",
      "\u0004\n",
      "σ2\n",
      "xy\n",
      "\u0005\u0005\n",
      "(7)\n",
      "since our goal is to ﬁnd clusters c that are unassociated with the available\n",
      "variables y of choice and to learn latent representations z that do not contain\n",
      "information about y, the generative process of cdvade assumes that z, c are\n",
      "jointly independent of y.\n",
      "the changes compared to the generative process of vade can also be\n",
      "regarded as imposing a structure on the model, where the encoder learns hidden\n",
      "representations of the image x conditioned to the additional variables y (i.e.,\n",
      "q(z|x, y)), but acts as an identity function with respect to y (i.e., y can be\n",
      "regarded as being simply concatenated to the latent space representations z).\n",
      "in all our experiments, we use the same cnn\n",
      "architectures for the encoder and decoder as in vade (see sect. 2.1).\n",
      "in our dec experiments,\n",
      "we use the same autoencoder architecture and the same initialization as for the\n",
      "vade.\n",
      "typically, clustering is performed on top of\n",
      "features extracted with the use of an encoder neural network, and the cluster\n",
      "assignments are determined by using conventional clustering algorithms, such\n",
      "as k-means, on top of the learned latent representations [1,5,7,12].\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "colored mnist\n",
      "the colored mnist is an extension to the classic mnist dataset [3], which\n",
      "contains binary images of handwritten digits.\n",
      "the colored mnist includes col-\n",
      "ored images of the same digits, where each number and background have a color\n",
      "assignment.\n",
      "we present results of the experiments with ﬁve distinct colors and\n",
      "ﬁve digits of mnist (0–4).\n",
      "to enhance computational eﬃciency and expedite\n",
      "experiments, we utilized only 1% of the mnist images, which were sampled\n",
      "at random.\n",
      "this simple dataset can be used to investigate whether a given\n",
      "clustering algorithm will categorize the images by color or by the digit label\n",
      "conditionally decoded variational deep embedding (cdvade)\n",
      "we use latent space\n",
      "dimensionality d = 20 for all models.\n",
      "fig.\n",
      "2 a summary of the results for the experiments on the colored mnist\n",
      "dataset is presented.\n",
      "notably,\n",
      "both vade and dec end up clustering the data by color, as it is the most\n",
      "striking distinguishing characteristic of these images.\n",
      "on the other hand, the\n",
      "predicted domains of cdvade have no association with color, and the data are\n",
      "separated by the shapes in the images, distinguishing some of the digit labels\n",
      "(albeit imperfectly).\n",
      "human epidermal growth factor receptor 2 (her2 or\n",
      "her2/neu) is a protein involved in normal cell growth, which plays an impor-\n",
      "tant role in the diagnosis and treatment of breast cancer\n",
      "we use a subset of this dataset consisting of 672 images (the remainder is\n",
      "held out for future research).\n",
      "the dimensions of the\n",
      "images vary from 600 to 826 pixels, and we scale all data to a uniform size of\n",
      "128 × 128 pixels before further processing.\n",
      "672\n",
      "m. sidulova et al.\n",
      "this retrospective human subject dataset has been made available to us by\n",
      "the authors of the prior studies [4,8], who are not associated with this paper.\n",
      "we evaluate\n",
      "the performance and behavior of the dec, vade, and cdvade models on\n",
      "the her2 dataset.\n",
      "the dimensionality of the latent\n",
      "embedding space was set to d = 500 for all three models.\n",
      "3. results summary for vade, cdvade, and dec, all with d = 3. example\n",
      "images from the identiﬁed clusters are visualized for each method.\n",
      "figure 3 demonstrates that even without scrutinizing, one can observe a\n",
      "strong visual separation between the algorithmically identiﬁed image domains\n",
      "for both vade and dec experiments.\n",
      "3 images tend to have slightly visible boundaries but a com-\n",
      "paratively uniform light appearance overall.\n",
      "in the second predicted domain,\n",
      "images have less visible boundaries and more pail staining.\n",
      "in the third pre-\n",
      "dicted domain, images have more visible staining and sharper edges compared\n",
      "to the other domains.\n",
      "the pearson’s correlation coeﬃcient between the\n",
      "clustering assignments c of vade and the her2/neu scores is 0.46.\n",
      "4. boxplots of her2/neu scores per predicted domain for all experiments.\n",
      "the correlation coeﬃcient between the cdvade cluster assignments and the\n",
      "her2/neu scores is 0.39.\n",
      "moreover, the clusters identiﬁed by\n",
      "cdvade are distinctly diﬀerent from those of vade, with a 0.43 proportion of\n",
      "agreement between the two algorithms (after matching the two sets of cluster\n",
      "assignments using the hungarian algorithm).\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf:\n",
      "accurate 3d mitochondria instance segmentation in electron\n",
      "microscopy (em) is a challenging problem and serves as a prerequisite\n",
      "to empirically analyze their distributions and morphology.\n",
      "further,\n",
      "we introduce a semantic foreground-background adversarial loss during\n",
      "training that aids in delineating the region of mitochondria instances\n",
      "from the background clutter.\n",
      "our extensive experiments on three bench-\n",
      "marks, lucchi, mitoem-r and mitoem-h, reveal the beneﬁts of the\n",
      "proposed contributions achieving state-of-the-art results on all three\n",
      "datasets.\n",
      "keywords: electron microscopy · mitochondria instance\n",
      "segmentation · spatio-temporal transformer · hybrid\n",
      "cnn-transformers\n",
      "1\n",
      "introduction\n",
      "mitochondria are membrane-bound organelles that generate the primary energy\n",
      "required to power the cell activities, thereby crucial for metabolism.\n",
      "mitochon-\n",
      "drial dysfunction, which occurs when mitochondria are not functioning properly\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43993-3_59.\n",
      "[23,25]. elec-\n",
      "tron microscopy (em) images are typically utilized to reveal the corresponding\n",
      "3d geometry and size of mitochondria at a nanometer scale, thereby facilitating\n",
      "basic biological research at ﬁner scales.\n",
      "therefore, automatic instance segmenta-\n",
      "tion of mitochondria is desired, since manually segmenting from a large amount\n",
      "of data is particularly laborious and demanding.\n",
      "however, automatic 3d mito-\n",
      "chondria instance segmentation is a challenging task, since complete shape of\n",
      "mitochondria can be sophisticated and multiple instances can also experience\n",
      "entanglement with each other resulting in unclear boundaries.\n",
      "here, we look\n",
      "into the problem of accurate 3d mitochondria instance segmentation.\n",
      "earlier works on mitochondria segmentation employ standard image process-\n",
      "ing and machine learning methods\n",
      "[20,21,33].\n",
      "in\n",
      "case of bottom-up mitochondria instance segmentation approaches, a binary\n",
      "segmentation mask, an aﬃnity map or a binary mask with boundary instances\n",
      "is computed typically using a 3d u-net\n",
      "on the other hand, top-down methods typ-\n",
      "ically rely on techniques such as mask r-cnn [7] for segmentation.\n",
      "when designing a attention-based framework for 3d mitochondria instance\n",
      "segmentation, a straightforward way is to compute joint spatio-temporal self-\n",
      "attention where all pairwise interactions are modelled between all spatio-\n",
      "temporal tokens.\n",
      "the focus of our design is the introduction of a split\n",
      "spatio-temporal attention (sst) module that captures long-range dependen-\n",
      "cies within the cubic volume of human and rat mitochondria samples.\n",
      "– to accurately delineate the region of mitochondria instances from the clut-\n",
      "tered background, we further introduce a semantic foreground-background\n",
      "(fg-bg) adversarial loss during the training that aids in learning improved\n",
      "instance-level features.\n",
      "– we conduct experiments on three commonly used benchmarks: lucchi\n",
      "our stt-unet achieves state-of-the-art\n",
      "3d mitochondria instance segmentation with spatio-temporal transformers\n",
      "615\n",
      "fig.\n",
      "1. qualitative 3d instance segmentation comparison between the recent res-\n",
      "unet [16] and our proposed stt-unet approach on the example input regions\n",
      "from mitoem-h and mitoem-r validation sets.\n",
      "here, we present the corresponding\n",
      "segmentation predictions of the baseline and our approach along with the ground truth.\n",
      "our stt-unet approach achieves superior segmentation performance by accurately\n",
      "segmenting 16% more cell instances in these examples, compared to res-unet-r.\n",
      "segmentation performance on all three datasets.\n",
      "2\n",
      "related work\n",
      "most recent approaches for 3d mitochondria instance segmentation utilize con-\n",
      "volution based designs within the “u-shaped” 3d encoder-decoder architecture.\n",
      "in such an architecture, the encoder aims to generate a low-dimensional rep-\n",
      "resentation of the 3d data by gradually performing the downsampling of the\n",
      "extracted features.\n",
      "on the other hand, the decoder performs upsampling of\n",
      "these extracted feature representations to the input resolution for segmenta-\n",
      "tion prediction.\n",
      "although such a cnn-based designs [11,16,34] has achieved\n",
      "promising segmentation results compared to traditional methods, they struggle\n",
      "to eﬀectively capture long-range dependencies due to their limited local recep-\n",
      "tive ﬁeld.\n",
      "inspired from success in natural language processing [32], recently\n",
      "vision transformers (vits)\n",
      "[6,13,19,30,31] have been successfully utilized in dif-\n",
      "ferent computer vision problems due to their capabilities at modelling long-range\n",
      "dependencies and enabling the model to attend to all the elements in the input\n",
      "616\n",
      "o. thawakar et al.\n",
      "sequence.\n",
      "the core component in vits is the self-attention mechanism that that\n",
      "learns the relationships between sequence elements by performing relevance esti-\n",
      "mation of one item to other items.\n",
      "the other attention such as [1,8,10,29,35]\n",
      "have demonstrated remarkable eﬃcacy in eﬀectively managing volumetric data.\n",
      "inspired by vits\n",
      "[10,19] and based on the observation that attention-based vision\n",
      "transformers architectures are an intuitive design choice for modelling long-range\n",
      "global contextual relationships in volume data, we investigate designing a cnn-\n",
      "transformers based framework for the task of 3d mitochondria instance segmen-\n",
      "tation.\n",
      "the input volume is denoised\n",
      "using an interpolation network adapted for medical images [9].\n",
      "the decoder outputs semantic mask and instance\n",
      "boundary, which are then post-processed using connected component labelling\n",
      "to generate ﬁnal instance masks.\n",
      "while\n",
      "self-attention has been shown to be beneﬁcial when combined with convolutional\n",
      "layers for diﬀerent medical imaging tasks, to the best of our knowledge, no pre-\n",
      "vious attempt to design spatio-temporal self-attention as an exclusive building\n",
      "block for the problem of 3d mitochondria instance segmentation exists in liter-\n",
      "ature.\n",
      "next, we present our approach that eﬀectively utilizes an eﬃcient spatio-\n",
      "temporal attention mechanism for 3d mitochondria instance segmentation.\n",
      "3.2\n",
      "spatio-temporal transformer res-unet (stt-unet)\n",
      "figure 2(a) presents the overall architecture of the proposed hybrid transformers-\n",
      "cnn based 3d mitochondria instance segmentation approach, named stt-\n",
      "unet.\n",
      "it comprises a denoising module, transformer based encoder-decoder\n",
      "3d mitochondria instance segmentation with spatio-temporal transformers\n",
      "617\n",
      "fig.\n",
      "2. (a) overall architecture of our stt-unet framework for 3d mitochondria\n",
      "instance segmentation.\n",
      "the resulting reconstructed volume is then fed to our split\n",
      "spatio-temporal attention based encoder-decoder to generate the semantic-level mito-\n",
      "chondria segmentation masks.\n",
      "consequently, the semantic masks from the decoder\n",
      "are then input to the instance segmentation module to generate the ﬁnal instance\n",
      "masks.\n",
      "the entire framework is trained using the standard bce loss (lbce) and our\n",
      "semantic foreground-background (fg-bg) adversarial loss (lfg−bg).\n",
      "with split spatio-temporal attention and an instance segmentation block.\n",
      "the\n",
      "denoising module alleviates the segmentation faults caused by anomalies in the\n",
      "em images, as in the baseline.\n",
      "the resulting denoised\n",
      "output is then processed by our transformer based encoder-decoder with split\n",
      "spatio-temporal attention to generate the semantic masks.\n",
      "consequently, these\n",
      "semantic masks are post-processed by an instance segmentation module using\n",
      "a connected component labelling scheme, thereby generating the ﬁnal instance-\n",
      "level segmentation output prediction.\n",
      "to further enhance the semantic segmen-\n",
      "tation quality with cluttered background we introduced semantic adversarial loss\n",
      "which leads to improved semantic segmentation in noisy background.\n",
      "2(b), that strives to capture long-range dependencies within the cubic\n",
      "volume of human and rat samples.\n",
      "the spatial attention reﬁnes the instance\n",
      "level features from input features along the spatial dimensions, whereas the tem-\n",
      "poral attention eﬀectively learns the inter-dependencies between the input vol-\n",
      "ume.\n",
      "as shown in fig 2(b), the normalized 3d input volume of denoised features x\n",
      "of size (t ×h ×w ×c) where t is volume size, (h ×w) is spatial dimension of\n",
      "volume and c is number of channels.\n",
      "the spatial and temporal attention is deﬁned as,\n",
      "xs = softmax(qskt\n",
      "s\n",
      "√dk\n",
      ")vs\n",
      "(1)\n",
      "xt = softmax(qtpkt\n",
      "tp\n",
      "√dk\n",
      ")vtp\n",
      "(2)\n",
      "where, xs is spatial attention map, xt is temporal attention map and dk is\n",
      "dimension of qs and ks.\n",
      "we empirically\n",
      "observe that fusing spatial and temporal features through a deformable convolu-\n",
      "tion, instead of concatenation through a conv. layer or addition, leads to better\n",
      "performance.\n",
      "the resulting spatio-temporal features of decoder are then input\n",
      "to instance segmentation block to generate ﬁnal instance masks, as in baseline.\n",
      "semantic fg-bg adversarial loss: as discussed earlier, a common chal-\n",
      "lenge in mitochondria instance segmentation is to accurately delineate the region\n",
      "of mitochondria instances from the cluttered background.\n",
      "to address this, we\n",
      "introduce a semantic foreground-background (fg-bg) adversarial loss during\n",
      "the training to enhance the fg-bg separability.\n",
      "the discriminator takes the input volume i\n",
      "3d mitochondria instance segmentation with spatio-temporal transformers\n",
      "619\n",
      "along with the corresponding mask as an input.\n",
      "while the discriminator d attempts to\n",
      "distinguish between ground truth and predicted masks (mgt and mpred, respec-\n",
      "tively), the model ψ learns to output semantic mask such that the predicted\n",
      "masks mpred are close to ground truth mgt.\n",
      "+ λ1ψ[d(fgt) − d(fpr)]\n",
      "(4)\n",
      "consequently, the overall loss for training is: l = lbce + λ · lfg−bg, where,\n",
      "lbce is bce loss, λ = 0.5 and lfg−bg is semantic adversarial loss.\n",
      "table 1.\n",
      "[16]\n",
      "0.895\n",
      "0.945\n",
      "res-unet-r + mrda\n",
      "[4]\n",
      "0.897\n",
      "0.946\n",
      "stt-unet (ours)\n",
      "0.913\n",
      "0.962\n",
      "4\n",
      "experiments\n",
      "dataset: we evaluate our approach on three datasets: mitoem-r [36], mitoem-\n",
      "h\n",
      "[36] is a dense mitochondria instance seg-\n",
      "mentation dataset from isbi 2021 challenge.\n",
      "the dataset consists of 2 em image\n",
      "volumes (30 μm3) of resolution of 8 × 8 × 30 nm, from rat tissues (mitoem-r)\n",
      "and human tissue (mitoem-h) samples, respectively.\n",
      "each volume has 1000\n",
      "grayscale images of resolution (4096 × 4096) of mitochondria, out of which train\n",
      "set has 400, validation set contains 100 and test set has 500 images.\n",
      "[22]\n",
      "is a sparse mitochondria semantic segmentation dataset with training and test\n",
      "volume size of 165 × 1024 × 768.\n",
      "implementation details: we implement our approach using pytorch1.9\n",
      "during training\n",
      "of mitoem, for the fair comparison, we adopt same data augmentation technique\n",
      "from [36].\n",
      "[16], we do not follow multi-scale training and\n",
      "620\n",
      "o. thawakar et al.\n",
      "perform single stage training for 200k iterations.\n",
      "for lucchi, we follow training\n",
      "details of [16,36] for semantic segmentation.\n",
      "our stt-unet achieves state-of-the-art per-\n",
      "formance on both sets.\n",
      "note that [16] employs two decoders for mitoem-h. in contrast, we\n",
      "utilize only a single decoder for both mitoem-h and mitoem-r sets, while still\n",
      "achieving improved segmentation performance.\n",
      "fig 3 presents the segmentation\n",
      "predictions of our approach on example input regions from the validation set.\n",
      "our approach achieves promising segmentation results despite the noise in the\n",
      "input samples.\n",
      "3. qualitative 3d instance segmentation results of our stt-unet on the exam-\n",
      "ple input regions from mitoem-h and mitoem-r val sets.\n",
      "baseline per-\n",
      "formance comparison.\n",
      "methods mitoem-r mitoem-h\n",
      "baseline\n",
      "0.921\n",
      "0.823\n",
      "deisgn choice\n",
      "mitoem-r mitoem-h\n",
      "spatial\n",
      "0.914\n",
      "0.812\n",
      "spatial-temporal\n",
      "0.922\n",
      "0.817\n",
      "temporal-spatial\n",
      "0.937\n",
      "0.832\n",
      "spatial|temporal\n",
      "0.958\n",
      "0.849\n",
      "3d mitochondria instance segmentation with spatio-temporal transformers\n",
      "621\n",
      "ablation study: table 3 shows a baseline comparison when progressively inte-\n",
      "grating our contributions: sst module and semantic foreground-background\n",
      "adversarial loss.\n",
      "the introduction of sst module improves performance from\n",
      "0.921 to 0.941 with a gain of 2.7%.\n",
      "the performance is further improved by\n",
      "1%, when introducing our semantic foreground-background adversarial loss.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf:\n",
      "nuclei segmentation is a fundamental but challenging task\n",
      "in the quantitative analysis of histopathology images.\n",
      "although fully-\n",
      "supervised deep learning-based methods have made signiﬁcant progress,\n",
      "a large number of labeled images are required to achieve great segmenta-\n",
      "tion performance.\n",
      "considering that manually labeling all nuclei instances\n",
      "for a dataset is ineﬃcient, obtaining a large-scale human-annotated\n",
      "dataset is time-consuming and labor-intensive.\n",
      "therefore, augmenting a\n",
      "dataset with only a few labeled images to improve the segmentation per-\n",
      "formance is of signiﬁcant research and application value.\n",
      "in this paper,\n",
      "we introduce the ﬁrst diﬀusion-based augmentation method for nuclei\n",
      "segmentation.\n",
      "the idea is to synthesize a large number of labeled images\n",
      "to facilitate training the segmentation model.\n",
      "in the ﬁrst step, we train an unconditional diﬀusion\n",
      "model to synthesize the nuclei structure that is deﬁned as the repre-\n",
      "sentation of pixel-level semantic and distance transform.\n",
      "each synthetic\n",
      "nuclei structure will serve as a constraint on histopathology image syn-\n",
      "thesis and is further post-processed to be an instance map.\n",
      "in the second\n",
      "step, we train a conditioned diﬀusion model to synthesize histopathology\n",
      "images based on nuclei structures.\n",
      "the synthetic histopathology images\n",
      "paired with synthetic instance maps will be added to the real dataset for\n",
      "this work is supported by chinese key-area research and development program of\n",
      "guangdong province (2020b0101350001), and the guangdong basic and applied basic\n",
      "research foundation (2023a1515011464, 2020b1515020048), and the national natural\n",
      "science foundation of china (no. 62102267, no. 61976250), and the shenzhen science\n",
      "and technology program (jcyj20220818103001002, jcyj20220530141211024), and\n",
      "the guangdong provincial key laboratory of big data computing, the chinese uni-\n",
      "versity of hong kong, shenzhen.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43993-3 57.\n",
      "https://doi.org/10.1007/978-3-031-43993-3_57\n",
      "diﬀusion-based data augmentation for nuclei image segmentation\n",
      "593\n",
      "training the segmentation model.\n",
      "the experimental results show that by\n",
      "augmenting 10% labeled real dataset with synthetic samples, one can\n",
      "achieve comparable segmentation results with the fully-supervised base-\n",
      "line.\n",
      "keywords: data augmentation · nuclei segmentation · diﬀusion\n",
      "models\n",
      "fig.\n",
      "1\n",
      "introduction\n",
      "nuclei segmentation is a fundamental step in medical image analysis.\n",
      "accu-\n",
      "rately segmenting nuclei helps analyze histopathology images to facilitate clin-\n",
      "ical diagnosis and prognosis.\n",
      "in recent years, many deep learning based nuclei\n",
      "segmentation methods have been proposed [5,18,19,23].\n",
      "most of these methods\n",
      "are fully-supervised so the great segmentation performance usually relies on a\n",
      "large number of labeled images.\n",
      "however, manually labeling the pixels belonging\n",
      "to all nucleus boundaries in an image is time-consuming and requires domain\n",
      "knowledge.\n",
      "in practice, it is hard to obtain an amount of histopathology images\n",
      "with dense pixel-wise annotations but feasible to collect a few labeled images.\n",
      "a question is raised naturally: can we expand the training dataset with a small\n",
      "proportion of images labeled to reach or even exceed the segmentation perfor-\n",
      "mance of the fully-supervised baseline?\n",
      "intuitively, since the labeled images are\n",
      "samples from the population of histopathology images, if the underlying distri-\n",
      "bution of histopathology images is learned, one can generate inﬁnite images and\n",
      "their pixel-level labels to augment the original dataset.\n",
      "therefore, it is demanded\n",
      "to develop a tool that is capable of learning distributions and generating new\n",
      "paired samples for segmentation.\n",
      "[2,4,12,16,20] have been widely used\n",
      "in data augmentation [11,22,27,31].\n",
      "specially, a newly proposed gan-based\n",
      "method can synthesize labeled histopathology image for nuclei segmentation\n",
      "[21].\n",
      "while gans are able to generate high quality images, they are known\n",
      "for unstable training and lack of diversity in generation due to the adversarial\n",
      "training strategy.\n",
      "due to the theo-\n",
      "retical basis and impressive performance of diﬀusion models, they were soon\n",
      "applied to a variety of vision tasks, such as inpainting, superresolution [30],\n",
      "text-to-image translation, anomaly detection and segmentation\n",
      "as\n",
      "likelihood-based models, diﬀusion models do not require adversarial training and\n",
      "outperform gans on the diversity of generated images [3], which are naturally\n",
      "more suitable for data augmentation.\n",
      "in this paper, we propose a novel diﬀusion-\n",
      "based augmentation framework for nuclei segmentation.\n",
      "the proposed method\n",
      "consists of two steps: unconditional nuclei structure synthesis and conditional\n",
      "histopathology image synthesis.\n",
      "on the training stage, we train the unconditional diﬀusion\n",
      "model using nuclei structures calculated from instance maps and the conditional\n",
      "diﬀusion model using paired images and nuclei structures.\n",
      "on the testing stage,\n",
      "the nuclei structures and the corresponding images are generated successively\n",
      "by the two models.\n",
      "as far as our knowledge, we are the ﬁrst to apply diﬀusion\n",
      "models on histopathology image augmentation for nuclei segmentation.\n",
      "our contributions are: (1) a diﬀusion-based data augmentation framework\n",
      "that can generate histopathology images and their segmentation labels from\n",
      "scratch; (2) an unconditional nuclei structure synthesis model and a condi-\n",
      "tional histopathology image synthesis model; (3) experiments show that with\n",
      "our method, by augmenting only 10% labeled training data, one can obtain\n",
      "segmentation results comparable to the fully-supervised baseline.\n",
      "2\n",
      "method\n",
      "our goal is to augment a dataset containing a limited number of labeled images\n",
      "with more samples to improve the segmentation performance.\n",
      "to increase the\n",
      "diversity of labeled images, it is preferred to synthesize both images and their\n",
      "corresponding instance maps.\n",
      "we propose a two-step strategy for generating new\n",
      "labeled images.\n",
      "since it is not viable\n",
      "to directly generate an instance map, we instead choose to generate its surrogate\n",
      "– nuclei structure, which is deﬁned as the concatenation of pixel-level semantic\n",
      "and distance transform.\n",
      "pixel-level semantic is a binary map where 1 or 0 indi-\n",
      "cates whether a pixel belongs to a nucleus or not.\n",
      "clearly, the nuclei struc-\n",
      "ture is a 3-channel map with the same size as the image.\n",
      "as nuclei instances\n",
      "diﬀusion-based data augmentation for nuclei image segmentation\n",
      "595\n",
      "can be identiﬁed from the nuclei structure, we can easily construct the corre-\n",
      "sponding instance map by performance marker-controlled watershed algorithm\n",
      "on the nuclei structure\n",
      "2.2\n",
      "conditional histopathology image synthesis\n",
      "in the second step, we synthesize histopathology images conditioned on nuclei\n",
      "structures.\n",
      "there are usually two ways to synthesize images constrained\n",
      "596\n",
      "x. yu et al.\n",
      "fig.\n",
      "2. the proposed diﬀusion-based data augmentation framework.\n",
      "we ﬁrst generate\n",
      "a nuclei structure with an unconditional diﬀusion model, and then generate images\n",
      "conditioned on the nuclei structure.\n",
      "the instance map from the nuclei structure is\n",
      "paired with the synthetic image to forms a new sample.\n",
      "by certain conditions: classiﬁer-guided diﬀusion\n",
      "unlike the network of unconditional nuclei structure synthesis which inputs\n",
      "the noisy nuclei structure yt and outputs the prediction of ϵt(yt, t), the network\n",
      "of conditional nuclei image synthesis takes the noisy nuclei image xt and the\n",
      "corresponding nuclei structure y as inputs and the prediction of ϵt(xt, t, y) as\n",
      "output.\n",
      "therefore, the conditional network should be equipped with the ability\n",
      "to well align the paired histopathology image and nuclei structure.\n",
      "since nuclei\n",
      "structures and histopathology images have diﬀerent feature spaces, simply con-\n",
      "catenating or passing them through a cross-attention module [7,15,17] before\n",
      "entering the u-net will degrade image ﬁdelity and yield unclear correspondence\n",
      "between synthetic nuclei image and its nuclei structure.\n",
      "inspired by [28], we\n",
      "embed information of the nuclei structure into feature maps of nuclei image\n",
      "by the spatially-adaptive normalization (spade) module [25].\n",
      "in other words,\n",
      "the spatial and morphological information of nuclei modulates the normalized\n",
      "diﬀusion-based data augmentation for nuclei image segmentation\n",
      "597\n",
      "feature maps such that the nuclei are generated in the right places while the\n",
      "background is left to be created freely.\n",
      "the network of conditional nuclei image synthesis also applies a u-net archi-\n",
      "tecture.\n",
      "3\n",
      "experiments and results\n",
      "3.1\n",
      "implementation details\n",
      "datasets.\n",
      "we conduct experiments on two datasets: monuseg\n",
      "the monuseg dataset has 44 labeled images of size 1000 × 1000, 30 for\n",
      "training and 14 for testing.\n",
      "the kumar dataset consists of 30 1000×1000 labeled\n",
      "images from seven organs of the cancer genome atlas (tcga) database.\n",
      "the\n",
      "dataset is splited into 16 training images and 14 testing images.\n",
      "to validate the eﬀectiveness of the proposed aug-\n",
      "mentation method, we create 4 subsets of each training dataset with 10%, 20%,\n",
      "50% and 100% nuclei instance labels.\n",
      "precisely, we ﬁrst crop all images of each\n",
      "dataset into 256 × 256 patches with stride 128, then obtain the features of all\n",
      "patches with pretrained resnet50\n",
      "for the conditional histopathology image synthesis network,\n",
      "each layer of the encoder and the decoder has 2 resblocks and 2 condresblocks\n",
      "respectively, and last 3 layers contain attnblocks.\n",
      "we use adamw optimizer with learning rates of 10−4\n",
      "and 2 × 10−5 for the two training stages, respectively.\n",
      "the syn-\n",
      "thetic nuclei structures are generate by the nuclei structure synthesis network\n",
      "and the corresponding images are generated by the histopathology image synthe-\n",
      "sis network with the classiﬁer-free guidance scale w = 2.\n",
      "we then obtain the augmented subsets\n",
      "by adding the synthetic paired images to the corresponding labeled subsets.\n",
      "598\n",
      "x. yu et al.\n",
      "nuclei segmentation.\n",
      "the eﬀectiveness of the proposed augmentation method\n",
      "can be evaluated by comparing the segmentation performance of using the four\n",
      "labeled subsets and using the corresponding augmented subsets to train a seg-\n",
      "mentation model.\n",
      "we choose to train two nuclei segmentation models – hover-\n",
      "net [5] and pff-net [18].\n",
      "to quantify the segmentation performance, we use\n",
      "two metrics: dice coeﬃcient and aggregated jaccard index (aji)\n",
      "the third and fourth row show selected synthetic images and corresponding\n",
      "nuclei with similar style as the real one in the same column.\n",
      "3.2\n",
      "eﬀectiveness of the proposed data augmentation method\n",
      "fig.\n",
      "3 shows the synthetic samples from the models trained on the subset with\n",
      "10% labeled images.\n",
      "first, the synthetic\n",
      "samples look realistic: the patterns of synthetic nuclei structures and textures\n",
      "of synthetic images are close to the real samples.\n",
      "second, due to the conditional\n",
      "mechanism of the image synthesis network and the classiﬁer-guidance sampling,\n",
      "the synthetic images are well aligned with the corresponding nuclei structures,\n",
      "which is the prerequisite to be additional segmentation training samples.\n",
      "third,\n",
      "the synthetic nuclei structures and images show great diversity: the synthetic\n",
      "samples resemble diﬀerent styles of the real ones but with apparent diﬀerences.\n",
      "we then train segmentation models on the four labeled subsets of monuseg\n",
      "and kumar dataset and corresponding augmented subsets with both real and\n",
      "synthetic labeled images.\n",
      "with a speciﬁc labeling proportion, say 10%, we name\n",
      "diﬀusion-based data augmentation for nuclei image segmentation\n",
      "599\n",
      "the original subset as 10% labeled subset and the augmented on as 10% aug-\n",
      "mented subset.\n",
      "table 1 show the segmentation performances with hover-net.\n",
      "for monuseg\n",
      "dataset, it is clear that the segmentation metrics drop with fewer labeled images.\n",
      "for example, with only 10% labeled images, dice and aji reduce by 2.4% and\n",
      "3.1%, respectively.\n",
      "however, by augmenting the 10% labeled subset, dice and\n",
      "aji exceed the fully-supervised baseline by 0.9% and 1.3%.\n",
      "for the 20% and\n",
      "50% case, the two metrics obtained by augmented subset are of the same level\n",
      "as using all labeled images.\n",
      "note that the metrics of 10% augmented subset\n",
      "are higher than those of 20% augmented subset, which might be attributed to\n",
      "the indetermination of the diﬀusion model training and sampling.\n",
      "interestingly,\n",
      "augmenting the full dataset also helps: dice increases by 1.3% and aji increases\n",
      "by 1.6% compared with the original full dataset.\n",
      "therefore, the proposed aug-\n",
      "mentation method consistently improves segmentation performance of diﬀerent\n",
      "labeling proportion.\n",
      "for kumar dataset, by augmenting 10% labeled subset,\n",
      "aji increases to a level comparable with that using 100% labeled images; by\n",
      "augmenting 20% and 50% labeled subset, ajis exceed the fully-supervised base-\n",
      "line.\n",
      "these results demonstrate the eﬀectiveness of the proposed augmentation\n",
      "method that we can achieve the same or higher level segmentation performance\n",
      "of the fully-supervised baseline by augmenting a dataset with a small amount of\n",
      "labeled images.\n",
      "generalization of the proposed data augmentation.\n",
      "moreover, we have\n",
      "similar observations when using pff-net as the segmentation model.\n",
      "table 2\n",
      "shows the segmentation results with pff-net.\n",
      "this indicates the generalization of our proposed augmentation method.\n",
      "eﬀectiveness of the proposed data augmentation method with hover-net.\n",
      "training data\n",
      "monuseg\n",
      "kumar\n",
      "dice\n",
      "aji\n",
      "dice\n",
      "aji\n",
      "10% labeled\n",
      "0.7969 0.6344 0.8040 0.5939\n",
      "10% augmented\n",
      "0.8291 0.6785 0.8049 0.6161\n",
      "20% labeled\n",
      "0.8118 0.6501 0.8078 0.6098\n",
      "20% augmented\n",
      "0.8219 0.6657 0.8192 0.6255\n",
      "50% labeled\n",
      "0.8182 0.6603 0.8175 0.6201\n",
      "50% augmented\n",
      "0.8291 0.6764 0.8158 0.6307\n",
      "100% labeled\n",
      "0.8206 0.6652 0.8150 0.6183\n",
      "100% augmented 0.8336 0.6810 0.8210 0.6301\n",
      "600\n",
      "x. yu et al.\n",
      "table 2. generalization of the proposed data augmentation method with pff-net.\n",
      "training data\n",
      "monuseg\n",
      "kumar\n",
      "dice\n",
      "aji\n",
      "dice\n",
      "aji\n",
      "10% labeled\n",
      "0.7489 0.5290 0.7685 0.5965\n",
      "10% augmented\n",
      "0.7764 0.5618 0.8051 0.6458\n",
      "20% labeled\n",
      "0.7691 0.5629 0.7786 0.6087\n",
      "20% augmented\n",
      "0.7891 0.5927 0.8019 0.6400\n",
      "50% labeled\n",
      "0.7663 0.5661 0.7797 0.6175\n",
      "50% augmented\n",
      "0.7902 0.5998 0.8104 0.6524\n",
      "100% labeled\n",
      "0.7809 0.5708 0.8032 0.6461\n",
      "100% augmented 0.7872 0.5860 0.8125 0.6550\n",
      "4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf:\n",
      "most existing methods classify\n",
      "nuclei independently or do not make full use of the semantic similarity\n",
      "betweennucleiandtheirgroupingfeatures.inthispaper,weproposeanovel\n",
      "end-to-end nuclei detection and classiﬁcation framework based on a group-\n",
      "ingtransformer-basedclassiﬁer.thenucleiclassiﬁerlearnsandupdatesthe\n",
      "representations of nuclei groups and categories via hierarchically grouping\n",
      "the nucleus embeddings.\n",
      "experimental results show that the proposed method signiﬁ-\n",
      "cantly outperforms the existing models on three datasets.\n",
      "keywords: nuclei classiﬁcation · prompt tuning · clustering ·\n",
      "transformer\n",
      "1\n",
      "introduction\n",
      "nucleus classiﬁcation is to identify the cell types from digital pathology image,\n",
      "assisting pathologists in cancer diagnosis and prognosis\n",
      "for example, the\n",
      "this work was supported in part by the chinese key-area research and development\n",
      "program of guangdong province (2020b0101350001), in part by the national natural\n",
      "science foundation of china (no. 62102267, no. 61976250), in part by the guangdong\n",
      "basic and applied basic research foundation (2023a1515011464, 2020b1515020048),\n",
      "in part by the shenzhen science and technology program (jcyj20220818103001002,\n",
      "jcyj20220530141211024), and the guangdong provincial key laboratory of big data\n",
      "computing, the chinese university of hong kong, shenzhen.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43993-3_55.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43993-3_55\n",
      "570\n",
      "j. huang et al.\n",
      "involvement of tumor-inﬁltrating lymphocytes (tils) is a critical prognostic\n",
      "variable for the evaluation of breast/lung cancer\n",
      "thus, we aim to automatically classify cell nuclei in pathological images.\n",
      "a number of methods [7,10,14,23–25,33,34] have been proposed for auto-\n",
      "matic nuclei segmentation and classiﬁcation.\n",
      "diﬀerently, in\n",
      "pathological images, experts often identify nuclear communities via their rela-\n",
      "tionships and spatial distribution.\n",
      "however, the semantics similarity and dissimilar-\n",
      "ity between nucleus instances as well as the category representations have not\n",
      "been fully exploited.\n",
      "based on these observations, we develop a learnable grouping transformer\n",
      "based classiﬁer (gtc) that leverages the similarity between nuclei and their clus-\n",
      "ter representations to infer their types.\n",
      "on the other hand, there exist domain gaps in the patho-\n",
      "logical images of diﬀerent organs, staining, and institutions, which makes it nec-\n",
      "essary to ﬁne-tune models to new applications.\n",
      "inspired by the prompt tuning methods [13,16,20] which train continuous\n",
      "prompts with frozen pretrained models for natural language processing tasks,\n",
      "we propose a grouping prompt based learning strategy for eﬃcient tuning.\n",
      "our contributions are: (1) a prompt-based grouping transformer framework for\n",
      "end-to-end detection and classiﬁcation of nuclei; (2) a novel grouping prompt\n",
      "learning mechanism that exploits nucleus clusters to guide feature learning with\n",
      "low tuning costs; (3) experimental results show that our method achieves the\n",
      "state-of-the-art on three public benchmarks.\n",
      "1, we propose a novel framework, prompt-based grouping\n",
      "transformer (pgt), which directly outputs the coordinates of nuclei centroids\n",
      "and leverages grouping prompts for cell-type prediction.\n",
      "the pixel-level feature maps output from stage 2 to stage 4 of the\n",
      "backbone are extracted.\n",
      "then the stage-4 feature map is downsampled with a\n",
      "3 × 3 convolution of stride 2 to yield another lower-resolution feature map.\n",
      "then we utilize the hard assignment strategy\n",
      "for a typical swin-transformer backbone, an input pathological image i ∈\n",
      "rh×w ×3 is divided into hw\n",
      "e2 image patches of size e × e. we ﬁrst embed each\n",
      "574\n",
      "j. huang et al.\n",
      "image patch into a d-dimensional latent space via a linear projection.\n",
      "in the pre-tuning phase, we adopt the swin-b backbone\n",
      "pre-trained on imagenet, replace the gtc head in our model (fig. 1) with 2\n",
      "fc layers, and train the overall framework without prompts and gtc.\n",
      "3\n",
      "experiments and results\n",
      "3.1\n",
      "datasets and implementation details\n",
      "consep1\n",
      "[10] is a colorectal nuclear dataset with three types, consisting\n",
      "of 41 h&e stained image tiles from 16 colorectal adenocarcinoma whole-slide\n",
      "images (wsis).\n",
      "[1] is a breast cancer dataset with three types and consists of\n",
      "120 image tiles from 113 patients.\n",
      "[9] has 291 histology images of colon tissue from six datasets, containing\n",
      "nearly half a million labeled nuclei in h&e stained colon tissue.\n",
      "the wsis are\n",
      "at 20× magniﬁcation with an average size of 1,016 × 917 pixels.\n",
      "our implementation and the setting of hyper-parameters are based on\n",
      "mmdetection\n",
      "random crop,\n",
      "ﬂipping, and scaling are used for data augmentation.\n",
      "more details are listed in the supplementary material.\n",
      "3.2\n",
      "comparison with the state-of-the-art\n",
      "the proposed method is compared with the state-of-the-art models: the exist-\n",
      "ing methods for detecting and classifying cells in pathological images, i.e., hover-\n",
      "net\n",
      "[7], and the sate-of-the-art methods for object\n",
      "detection in natural images, i.e., ddod\n",
      "the details are listed in the supplementary material.\n",
      "with a frozen backbone, the performances of ‘w/o pt’ and\n",
      "‘w/o gtc’ are both dropping, which veriﬁes the strength of the prompt tuning\n",
      "and the gtc module, respectively.\n",
      "table 3 shows the eﬀect of diﬀerent numbers of grouping prompts on\n",
      "consep dataset.\n",
      "fd denotes the mean of detection f-scores of all testing images.\n",
      "as shown in table 4, we calculate fd of each testing\n",
      "image as sample data and conduct t-test to obtain p-values on the consep\n",
      "dataset.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_50.pdf:\n",
      "however, attention\n",
      "maps of vits are often fragmented, leading to unsatisfactory explana-\n",
      "tions.\n",
      "it replaces\n",
      "all linear transformations with the b-cos transform to promote weight-\n",
      "input alignment.\n",
      "they are often still\n",
      "considered black boxes, limiting their application in safety-critical domains such\n",
      "e. klaiman—equal contribution.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43993-3_50.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "however, with the\n",
      "rise of transformers [31] in computational pathology, and their increasing appli-\n",
      "cation to cancer classiﬁcation, segmentation, survival prediction, and mutation\n",
      "detection tasks [26,32,33], the old tools need to be reconsidered.\n",
      "but these often lead to fragmented and unsat-\n",
      "isfactory explanations [10].\n",
      "recent research on understanding vision models has mostly focused on attri-\n",
      "bution methods [13,20], which aim to identify important parts of an image and\n",
      "highlight them in a saliency map.\n",
      "perturbation-based techniques, such\n",
      "as shap [22], are another way to extract salient features from images.\n",
      "the image shows an eosinophil,\n",
      "which is characterized by its split, but connected nucleus, large speciﬁc granules (pink\n",
      "structures in the cytoplasm), and dense chromatin (dark spots inside the nuclei)\n",
      "[11] combines multi-scale images and dino [8]\n",
      "pre-training to learn hierarchical visual concepts in a self-supervised fashion.\n",
      "6, we show that the b-cos transformer produces superior fea-\n",
      "ture maps over various post-hoc approaches – suggesting that our architecture\n",
      "does indeed learn human-plausible features that are independent of the speciﬁc\n",
      "visualization technique used.\n",
      "3\n",
      "methods\n",
      "we focus on the original vision transformer [12]: the input image is divided\n",
      "into non-overlapping patches, ﬂattened, and projected into a latent space of\n",
      "dimension d. class tokens\n",
      "this ensures that only weight vectors with higher cosine similarity to the inputs\n",
      "are selected, which further increases the alignment pressure during optimization.\n",
      "4. we compute the central kernel alignment (cka), which measures the repre-\n",
      "sentation similarity between each hidden layer.\n",
      "when trained with the binary cross-entropy loss\n",
      "(bce) instead of the categorical cross-entropy loss (cce), the alignment is higher.\n",
      "query, key, and value thus capture more patterns in an image – which the\n",
      "attention mechanism can then attend to.\n",
      "this can be shown visually by plotting\n",
      "the centered kernel alignment (cka).\n",
      "4\n",
      "implementation and evaluation details\n",
      "task-based evaluation: cancer classiﬁcation and segmentation is an\n",
      "important ﬁrst step for many downstream tasks such as grading or staging.\n",
      "we classify image patches from\n",
      "the public colorectal cancer dataset nct-crc-he-100k\n",
      "we then performed the conover post-hoc test\n",
      "after friedman with adjusted p-values according to the two-stage benjamini-hochberg\n",
      "procedure.\n",
      "520\n",
      "m. tran et al.\n",
      "table 2. results of swin and bwin (ours) experiments on the test set of nct-crc-he-\n",
      "100k and munich-aml-morphology.\n",
      "to assess this, we propose a blinded study with four steps: (i)\n",
      "randomly selecting images from the test set of tcga-coad-20x (32 samples)\n",
      "and munich-aml-morphology (56 samples), (ii) plotting the last-layer attention\n",
      "and transformer attributions for each image, (iii) anonymizing and randomly\n",
      "shuﬄing the outputs, (iv) submitting them to two domain experts in histology\n",
      "and cytology for evaluation.\n",
      "most importantly, we show them all the available\n",
      "saliency maps without pre-selecting them to get their unbiased opinion.\n",
      "implementation details: in our experiments, we compare diﬀerent variants of\n",
      "the b-cos vision transformer and the vision transformer.\n",
      "speciﬁcally, we imple-\n",
      "ment two versions of vit: vit-t/8 and vit-s/8.\n",
      "we explore\n",
      "whether this is also true for transformers in our experiments.\n",
      "we believe\n",
      "this is due to the simultaneous optimization of two objectives: classiﬁcation loss\n",
      "and weight-input alignment.\n",
      "in many visualization techniques, we see that bvt,\n",
      "unlike vit, focuses exclusively on these structures (fig. 3).\n",
      "a third expert points out that vit might overﬁt certain patterns in\n",
      "this dataset, which could aid the model in improving its performance.\n",
      "[21] is a\n",
      "popular alternative to vit (e.g., it is currently the sota feature extractor for\n",
      "histopathological images [33]).\n",
      "in our experiments (table 2), we observe that bwin outperforms swin by\n",
      "up to 2.7% and 4.8% in f1-score on nct-crc-he-100k and munich-aml-\n",
      "morphology, respectively.\n",
      "5: when bvt is trained from scratch, the model faces a trade-oﬀ between\n",
      "522\n",
      "m. tran et al.\n",
      "learning the weight and input alignment and ﬁnding the appropriate inductive\n",
      "bias to solve the classiﬁcation task.\n",
      "by reintroducing many of the inductive biases\n",
      "of cnns through the window attention in the case of swin or transfer learning\n",
      "in the case of bvt, the model likely overcomes this initial problem.\n",
      "moreover, we would like to emphasize that the modiﬁed models have no\n",
      "negative impact on the model’s performance.\n",
      "we have also shown that bvt is competitive with vit in terms\n",
      "of quantitative performance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf:\n",
      "state-of-the-art object detection and segmentation meth-\n",
      "ods for microscopy images rely on supervised machine learning, which\n",
      "requires laborious manual annotation of training data.\n",
      "here we present\n",
      "a self-supervised method based on time arrow prediction pre-training\n",
      "that learns dense image representations from raw, unlabeled live-cell\n",
      "microscopy videos.\n",
      "our method builds upon the task of predicting the\n",
      "correct order of time-ﬂipped image regions via a single-image feature\n",
      "extractor followed by a time arrow prediction head that operates on the\n",
      "fused features.\n",
      "we furthermore demonstrate the utility of these represen-\n",
      "tations on several live-cell microscopy datasets for detection and segmen-\n",
      "tation of dividing cells, as well as for cell state classiﬁcation.\n",
      "keywords: self-supervised learning · live-cell microscopy\n",
      "1\n",
      "introduction\n",
      "live-cell microscopy is a fundamental tool to study the spatio-temporal dynam-\n",
      "ics of biological systems\n",
      "the resulting datasets can consist of terabytes\n",
      "of raw videos that require automatic methods for downstream tasks such as clas-\n",
      "siﬁcation, segmentation, and tracking of objects (e.g. cells or nuclei).\n",
      "the\n",
      "manual creation of these annotations, however, is laborious and often constitutes\n",
      "a practical bottleneck in the analysis of microscopy experiments [6].\n",
      "in ssl one ﬁrst deﬁnes a pretext task\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43993-3_52.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "d) the learned tap\n",
      "representations z are used as input to a downstream model d.\n",
      "which can be formulated solely based on unlabeled images (e.g. inpainting [8], or\n",
      "rotation prediction [5]) and tasks a neural network to solve it, with the aim of\n",
      "generating latent representations that capture high-level image semantics.\n",
      "in a\n",
      "second step, these representations can then be either ﬁnetuned or used directly\n",
      "(e.g. via linear probing) for a downstream task (e.g. image classiﬁcation) with\n",
      "available ground truth [7,10,18].\n",
      "in this paper we investigate whether time arrow prediction, i.e. the prediction\n",
      "of the correct order of temporally shuﬄed image frames extracted from live-cell\n",
      "microscopy videos, can serve as a suitable pretext task to generate meaningful\n",
      "representations of microscopy images.\n",
      "we are motivated by the observation that\n",
      "for most biological systems the temporal dynamics of local image features are\n",
      "closely related to their semantic content: whereas static background regions are\n",
      "time-symmetric, processes such as cell divisions or cell death are inherently time-\n",
      "asymmetric (cf. fig.\n",
      "importantly, we are interested in dense representations\n",
      "of individual images as they are useful for both image-level (e.g. classiﬁcation)\n",
      "or pixel-level (e.g. segmentation) downstream tasks.\n",
      "to that end, we propose\n",
      "a time arrow prediction pre-training scheme, which we call tap, that uses a\n",
      "feature extractor operating on single images followed by a time arrow predic-\n",
      "tion head operating on the fused representations of consecutive time points.\n",
      "[19] and has since then seen numerous\n",
      "self-supervised dense representation learning\n",
      "539\n",
      "applications for image-level tasks, such as action recognition, video retrieval,\n",
      "and motion classiﬁcation\n",
      "concretely our contributions are: i) we introduce the time\n",
      "arrow prediction pretext task to the domain of live-cell microscopy and propose\n",
      "the tap pre-training scheme, which learns dense representations (in contrast to\n",
      "only image-level representations) from raw, unlabeled live-cell microscopy videos,\n",
      "ii) we propose a custom (permutation-equivariant) time arrow prediction head\n",
      "that enables robust training, iii) we show via attribution maps that the repre-\n",
      "sentations learned by tap capture biologically relevant processes such as cell\n",
      "divisions, and ﬁnally iv) we demonstrate that tap representations are beneﬁcial\n",
      "for common image-level and pixel-level downstream tasks in live-cell microscopy,\n",
      "especially in the low training data regime.\n",
      "2\n",
      "method\n",
      "our proposed tap pre-training takes as input a set {i} of live-cell microscopy\n",
      "image sequences\n",
      "×h×w with the goal to produce a feature extractor f\n",
      "that generates c-dimensional dense representations\n",
      "z = f(x) ∈ rc×h×w from\n",
      "single images x ∈ rh×w (cf. fig.\n",
      "=\n",
      "e˜zt\n",
      "i ·˜zj/τ\n",
      "\u0003c\n",
      "j=1 e˜zt\n",
      "i ·˜zj/τ\n",
      "(2)\n",
      "here ˜z ∈ rc×2hw denotes the stacked features z ﬂattened across the non-channel\n",
      "dimensions, and τ is a temperature parameter.\n",
      "throughout the experiments\n",
      "we use λ = 0.01 and τ = 0.2.\n",
      "note that instead of creating image pairs from\n",
      "consecutive video frames we can as well choose a custom time step δt ∈ n and\n",
      "sample x1 ⊂\n",
      "in contrast to common models (e.g.\n",
      "resnet [9]) that lack this symmetry, we here directly incorporate this induc-\n",
      "tive bias via a permutation-equivariant head h that is a generalization of the set\n",
      "permutation-equivariant layer proposed in [32] to dense inputs.\n",
      "the last layer hl includes an additional global average pooling\n",
      "along the spatial dimensions to yield the ﬁnal logits ˆy ∈ r2.\n",
      "augmentations: to avoid overﬁtting on artiﬁcial image cues that could be\n",
      "discriminative of the temporal order (such as a globally consistent cell drift,\n",
      "or decay of image intensity due to photo-bleaching) we apply the following aug-\n",
      "mentations (with probability 0.5) to each image patch pair x1, x2: ﬂips, arbitrary\n",
      "rotations and elastic transformations (jointly for x1 and x2), translations for x1\n",
      "and x2 (independently), spatial scaling, additive gaussian noise, and intensity\n",
      "shifting and scaling (jointly+independently).\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "datasets\n",
      "to demonstrate the utility of tap for a diverse set of specimen and microscopy\n",
      "modalities we use the following four diﬀerent datasets:\n",
      "hela.\n",
      "human cervical cancer cells expressing histone 2b-gfp imaged by ﬂuo-\n",
      "rescence microscopy every 30 min\n",
      "3b), imaged by ﬂuorescence microscopy every 4 min [27,28].\n",
      "3a), imaged by spinning disk confocal microscopy every 5 min [4,20].\n",
      "3c) imaged by phase-contrast microscopy\n",
      "every 3 min\n",
      "2. a) tap validation accuracy for diﬀerent image augmentations on crops of back-\n",
      "ground, interphase (non-dividing), and mitotic (dividing) cells (from hela dataset).\n",
      "we\n",
      "show results of three runs per model.\n",
      "3.2\n",
      "implementation details:\n",
      "for the feature extractor f we use a 2d u-net [21] with depth 3 and c = 32\n",
      "output features, batch normalization and leaky relu activation (approx.\n",
      "the time arrow prediction head h consists of two permutation-\n",
      "equivariant layers with batch normalization and leaky relu activation, followed\n",
      "by global average pooling and a ﬁnal permutation-equivariant layer (approx.\n",
      "total training time for a single tap model is roughly 8h\n",
      "on a single gpu. tap is implemented in pytorch.\n",
      "3.3\n",
      "time arrow prediction pretraining\n",
      "we ﬁrst study how well the time arrow prediction pretext task can be solved\n",
      "depending on diﬀerent image structures and used data augmentations.\n",
      "to\n",
      "that end, we train tap networks with an increasing number of augmentations\n",
      "on hela and compute the tap classiﬁcation accuracy for consecutive image\n",
      "patches x1, x2 that contain either background, interphase (non-dividing) cells,\n",
      "or mitotic (dividing) cells.\n",
      "50% irrespective of the used augmentations, suggesting the\n",
      "absence of predictive cues in the background for this dataset.\n",
      "in contrast, on\n",
      "regions with cell divisions the accuracy reaches almost 100%, conﬁrming that\n",
      "542\n",
      "b. gallusser et al.\n",
      "tap is able to pick up on strong time-asymmetric image features.\n",
      "when\n",
      "using more data augmentations the accuracy decreases by roughly 12% points,\n",
      "suggesting that data augmentation is key to avoid overﬁtting on confounding\n",
      "cues.\n",
      "strikingly, the attribution maps highlight only a few distributed, yet\n",
      "highly localized image regions.\n",
      "when inspecting the top six most discriminative\n",
      "regions and their temporal context for a single image frame, we ﬁnd that virtually\n",
      "all of them contain cell divisions (cf. fig.\n",
      "moreover, when examining the\n",
      "attribution maps for full videos, we ﬁnd that indeed most highlighted regions\n",
      "correspond to mitotic cells, underlining the strong potential of tap to reveal\n",
      "time-asymmetric biological phenomena from raw microscopy videos alone (cf.\n",
      "supplementary video 1).\n",
      "3. a single image frame overlayed with tap attribution maps (computed with\n",
      "grad-cam [23]) for a) flywing, b) mdck, and c) yeast.\n",
      "first we test the learned representations on two\n",
      "image-level classiﬁcation tasks, and later on two dense segmentation tasks.\n",
      "fig.\n",
      "in fig. 4a we show average precision (ap)\n",
      "on a held-out test set while varying the amount of available training data.\n",
      "as\n",
      "expected, the performance of the supervised baseline drops substantially for low\n",
      "amounts of training data and surprisingly is already outperformed by a linear\n",
      "classiﬁer (100 params) on top of tap representations (e.g. 0.90 vs. 0.77 for 76\n",
      "labeled crops).\n",
      "notably, already at\n",
      "30% training data it reaches the same performance (0.97) as the baseline model\n",
      "trained on the full training set.\n",
      "fig.\n",
      "5. a) mitosis segmentation in flywing for two consecutive timepoints with\n",
      "ﬁxed/ﬁnetuned tap representations vs. a supervised u-net baseline (green).\n",
      "mitosis segmentation on flywing: we now apply tap on a pixel-level down-\n",
      "stream task to fully exploit that the learned tap representations are dense.\n",
      "to evaluate performance, we\n",
      "match a predicted/ground truth object if their intersection over union (iou) is\n",
      "greater than 0.5, and report the f1 score after matching.\n",
      "training a u-net on ﬁxed tap representa-\n",
      "tions always outperforms the baseline, and when only using 3% of the train-\n",
      "ing data it reaches similar performance as the baseline trained on all available\n",
      "labels (0.67 vs. 0.68, fig. 5a).\n",
      "emerging bud detection on yeast: finally, we test tap on the challenging\n",
      "task of segmenting emerging buds in phase contrast images of yeast colonies.\n",
      "we train tap networks on yeast and generate a dataset of 1205 crops of size\n",
      "5 × 192 × 192 where we densely label yeast buds in the central frame (deﬁned\n",
      "self-supervised dense representation learning\n",
      "545\n",
      "as buds that appeared less than 13 frames ago) based on available segmenta-\n",
      "tion data [17].\n",
      "we evaluate all methods on held out test videos by interpreting\n",
      "the resulting 2d+time segmentations as 3d objects and computing the f1 score\n",
      "using an iou threshold of 0.25.\n",
      "we show\n",
      "that tap uncovers sparse time-asymmetric biological processes and events in raw\n",
      "unlabeled recordings without any human supervision.\n",
      "although in this\n",
      "work we focus on 2d+t image sequences, the principle of tap should generalize\n",
      "to 3d+t datasets, for which dense ground truth creation is often prohibitively\n",
      "expensive and therefore the beneﬁts of modern deep learning are not fully tapped\n",
      "into.\n",
      "we leave this to future work, together with the application of tap to cell\n",
      "tracking algorithms, in which accurate mitosis detection is a crucial component.\n",
      "acknowledgements.\n",
      "we thank albert dominguez (epfl) and uwe schmidt for\n",
      "helpful comments, natalie dye (pol dresden) and franz gruber for providing the\n",
      "flywing dataset, benedikt mairhörmann and kurt schmoller (helmholtz munich)\n",
      "for providing additional yeast training data, and alan lowe (ucl) for providing the\n",
      "mdck dataset.\n",
      "elife 4, e07090 (2015)\n",
      "5. gidaris, s., singh, p., komodakis, n.: unsupervised representation learning by\n",
      "predicting image rotations.\n",
      "in: iclr, openreview.net (2018)\n",
      "546\n",
      "b. gallusser et al.\n",
      "6. greenwald, n.f., miller, g., moen, e., kong, a., kagel, a., et al.: whole-cell\n",
      "segmentation of tissue images with human-level performance using large-scale data\n",
      "annotation and deep learning.\n",
      "he, k., zhang, x., ren, s., sun, j.: deep residual learning for image recognition.\n",
      "hsu, j., gu, j., wu, g., chiu, w., yeung, s.: capturing implicit hierarchical\n",
      "structure in 3d biomedical images with self-supervised hyperbolic representations.\n",
      "padovani, f., mairhörmann, b., falter-braun, p., lengefeld, j., schmoller, k.m.:\n",
      "segmentation, tracking and cell cycle analysis of live-cell imaging data with cell-\n",
      "acdc.\n",
      "padovani, f., mairhörmann, b., lengefeld, j., falter-braun, p., schmoller, k.:\n",
      "cell-acdc: segmentation, tracking, annotation and quantiﬁcation of microscopy\n",
      "imaging data (dataset).\n",
      "ronneberger, o., fischer, p., brox, t.: u-net: convolutional networks for biomed-\n",
      "ical image segmentation.\n",
      "25. stringer, c., wang, t., michaelos, m., pachitariu, m.: cellpose: a generalist algo-\n",
      "rithm for cellular segmentation.\n",
      "tomer, r., khairy, k., keller, p.j.: shedding light on the system: studying embry-\n",
      "onic development with light sheet microscopy.\n",
      "ulicna, k., vallardi, g., charras, g., lowe, a.r.: automated deep lineage tree\n",
      "analysis using a bayesian single cell tracking approach.\n",
      "ulman, v., maška, m., magnusson, k.e.g., ronneberger, o., haubold, c., et al.:\n",
      "an objective comparison of cell-tracking algorithms.\n",
      "wei, d., lim, j., zisserman, a., freeman, w.t.: learning and using the arrow of\n",
      "time.\n",
      "weigert, m., schmidt, u., haase, r., sugawara, k., myers, g.: star-convex poly-\n",
      "hedra for 3d object detection and segmentation in microscopy.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf:\n",
      "brachial plexopathy is a form of peripheral neuropathy, which occurs\n",
      "when there is damage to the brachial plexus (bp).\n",
      "this paper\n",
      "proposes a texture pattern based convolutional neural network, called tppnet,\n",
      "to carry out abnormal prediction of bp from multiple routine magnetic reso-\n",
      "nance image (mri) pulse sequences, i.e. t2, t1, and t1 post-gadolinium contrast\n",
      "administration.\n",
      "different from classic cnns, the input of the proposed tppnet is\n",
      "multiple texture patterns instead of images.\n",
      "it has several special characteristics including 1) avoidance of\n",
      "image augmentation, 2) huge number of channels, 3) simple end-to-end architec-\n",
      "ture, 4) free from the interference of multi-texture-pattern arrangements.\n",
      "ablation\n",
      "study and comparisons demonstrate that the proposed tppnet yields outstand-\n",
      "ing performances with the accuracies of 96.1%, 93.5% and 93.6% over t2, t1\n",
      "and post-gadolinium sequences which exceed at least 1.3%, 5.3% and 3.4% over\n",
      "state-of-the-art methods for classiﬁcation of normal vs. abnormal brachial plexus.\n",
      "it occurs when there is\n",
      "damage to the brachial plexus (bp) which is a complex nerve network under the skin\n",
      "of the shoulder.\n",
      "supplementary information the online version contains supplementary material available at\n",
      "https://doi.org/10.1007/978-3-031-43993-3_46.\n",
      "it is due to compression or direct invasion\n",
      "of the nerves by tumor which will bring many serious symptoms [3].\n",
      "[4]. automatic identiﬁ-\n",
      "cation of the bp in mri and ultrasound images has become a hot topic.\n",
      "many radiomics studies have experimentally demonstrated that image texture has\n",
      "great potential for differentiation of different tissue types and pathologies\n",
      "in the\n",
      "past several decades, many state-of-the-art methods have been proposed to extract texture\n",
      "patterns\n",
      "however, how\n",
      "to arrange these glcms to form the 3d volume to optimize the performance is a major\n",
      "challenge.\n",
      "finally, we analyze the model’s performance in the experimental section.\n",
      "the major\n",
      "contributions of this study include 1) directed triangle construction idea for tpp, 2) huge\n",
      "number of tpp matrices as the heterogeneity representations of bp, 3) tppnet with 15\n",
      "layers and huge number of channels, 4) the bp dataset containing mr images and their\n",
      "corresponding roi masks.\n",
      "the range of the age are varying from 15 to 85 years old.\n",
      "the female patient\n",
      "number and male patient number are almost even.\n",
      "there-\n",
      "fore, each case underwent several essential image adjustments such as multi-series\n",
      "splitting,two-seriesmerging,sliceswapping,artifactcheckingandboundarycorrections.\n",
      "table 1.\n",
      "image \n",
      "total\n",
      "normal\n",
      "abnormal\n",
      "t2\n",
      "t1\n",
      "pg\n",
      "t2\n",
      "t1\n",
      "pg\n",
      "mri\n",
      "462\n",
      "123\n",
      "123\n",
      "123\n",
      "31\n",
      "31\n",
      "31\n",
      "mask\n",
      "462\n",
      "123\n",
      "123\n",
      "123\n",
      "31\n",
      "31\n",
      "31\n",
      "to yield the roi, ﬁrstly, we randomly sampled −40% of the sequences including\n",
      "both normal and abnormal ones that were manually segmented with itk- snap by two\n",
      "skilled trainees [14, 15].\n",
      "then, the manual segmentations were utilized to train a 3d\n",
      "nnunet model which was utilized to train the model which was used to predict rois\n",
      "for the rest series [16].\n",
      "the predicted segmentations were manually divided into three\n",
      "groups, i.e. good, fair and poor.\n",
      "this process\n",
      "was repeated until no improvements in the predictions for the remaining sequences was\n",
      "seen.\n",
      "only patients that had all three sequences segmented (t2, t1\n",
      "and post-gadolinium) were included in the dataset.\n",
      "in general, image textures extracted by these methods contain both\n",
      "local texture properties and global texture information.\n",
      "in summary, as the requirement of the image texture and deep learning, an excellent\n",
      "image texture pattern should have some essential features including 1) local proper-\n",
      "ties to characterize the micro-unit of the image texture, 2) global properties to represent\n",
      "a texture neural network to predict the abnormal brachial plexus\n",
      "473\n",
      "the macro-structure of the image texture, 3) uniform shapes under nonuniform-shape\n",
      "images, 4) invariant or robustness under some common geometric transforms such as\n",
      "rotation, scaling and so on.\n",
      "according above requirements, we developed a method to produce a serial of novel\n",
      "texture patterns by introducing a directed triangle idea with an adjacent triple pixel as a\n",
      "ternary group, called triple point pattern (tpp), to extract the local texture information.\n",
      "then, a statistical method like histogram is employed to count the number of the same\n",
      "type of pixel-triplets within the roi or throughout the whole image.\n",
      "finally, a three-\n",
      "dimensional (3d) tpp matrix is formed to characterize the image texture globally as the\n",
      "following:\n",
      "two-dimensional image:\n",
      "tpp(pi,pc,pj)(x, y, z) =\n",
      "m −1\n",
      "\u0002\n",
      "m=0\n",
      "n−1\n",
      "\u0002\n",
      "j=0\n",
      "⎧\n",
      "⎪⎪⎨\n",
      "⎪⎪⎩\n",
      "1\n",
      "i((m, n) + pi) = x&\n",
      "i((m, n) + pc) = y&\n",
      "i\n",
      "\u0007\n",
      "(m, n) + pj\n",
      "\b\n",
      "=\n",
      "z\n",
      "0\n",
      "others\n",
      "(1)\n",
      "where i is a mxn image, x, y, and z is the pixel triplet, x,y,\n",
      "1. directed triangle idea for tpp construction in 2d images where p0 is the concerned pixel,\n",
      "p1 …p8 are its adjacent pixels.\n",
      "three-dimensional image:\n",
      "tpp(pi,pc,pj)(x, y, z) =\n",
      "m −1\n",
      "\u0002\n",
      "m=0\n",
      "n−1\n",
      "\u0002\n",
      "n=0\n",
      "k−1\n",
      "\u0002\n",
      "k=0\n",
      "⎧\n",
      "⎪⎪⎨\n",
      "⎪⎪⎩\n",
      "1\n",
      "i((m, n, k) + pi)\n",
      "= x&\n",
      "i((m, n, k) + pc) = y&\n",
      "i\n",
      "\u0007\n",
      "(m, n, k) + pj\n",
      "\b\n",
      "= z\n",
      "0\n",
      "others\n",
      "(2)\n",
      "where i is a three dimensional image with the shape of mxnxk, pc = (0,0,0), other\n",
      "parameters are similar to the two-dimensional image.\n",
      "1, the tpp is formed by the concerned pixel and its two adjacent pixels in\n",
      "two-dimensional(2d) images.\n",
      "similarly, the tpp in 3d images is constructed by one\n",
      "474\n",
      "w. cao et al.\n",
      "concerned voxel and its two neighboring voxels.\n",
      "more details could be found in the\n",
      "supplementary material.\n",
      "as the construction idea of tpp, there are four independent\n",
      "modes categorized by the concerned angle, i.e. 45°, 90°, 135° and 180° in 2d images,\n",
      "which produce 8 tpps, 8 tpps, 8 tpps and 8 tpps respectively.\n",
      "analogously, the 3d\n",
      "image has twelve independent angle modes, i.e. 35.26°, 45°, 54.74°, 60°, 70.53°, 90°,\n",
      "109.47°, 120°, 125.26°, 135°, 144.74°, and 180°.\n",
      "totally there are 32 tpps in 2d images and\n",
      "325 tpps in 3d images and every tpp could produce one corresponding tpp matrix.\n",
      "2. the pipeline of the proposed tppnet over 3d images where nml denote normal, abn\n",
      "denotes abnormal, i in (2) is the block id, r is the adaptive argument to control the ﬁlter number.\n",
      "in our study, these isomorphic tpp matrices are not dropped from the tpp matrix\n",
      "set because they are equivalent to image rotations and re-scaling.\n",
      "image scaling can\n",
      "result in the image pixels increasing.\n",
      "therefore, data augmentation could be\n",
      "omitted when we combine tpp with deep learning for this study.\n",
      "as its deﬁnition, the\n",
      "tpp matrix should be a cubic array with the shape of lxlxl where l is the gray level\n",
      "of the image.\n",
      "based\n",
      "on the construction idea of tpp, the size of the tpp matrix depends on the gray level of\n",
      "the image.\n",
      "for the same image or roi, the larger the gray level, the sparser the matrix\n",
      "will be.\n",
      "therefore,\n",
      "the image requires a re-scaling step to lower its gray level to avoid the sparsity of\n",
      "the tpp matrix.\n",
      "it has four particular features as follows:\n",
      "1) avoidance of image augmentation.\n",
      "due to the stability of tpp matrix under rota-\n",
      "tion, scale and afﬁne transformations, image augmentation could be omitted in the\n",
      "preprocessing step which can lead to image deformation.\n",
      "for\n",
      "2d images, there are at least 32 channels if more displacements of tpp is considered.\n",
      "similarly, we could generate no less than 325 tpps in 3d images.\n",
      "3) simple end-to-end architecture.\n",
      "4) free from the interference of multi-texture-pattern arrangements.\n",
      "since each channel\n",
      "is corresponding with one tpp, it can solve the pattern arrangement issue occurred\n",
      "in glcm-cnn.\n",
      "table 2. test performances for different gray levels over t2, t1 and post-gadolinium where acc\n",
      "denotes accuracy and pg denotes post-gadolinium.\n",
      "gray \n",
      "level \n",
      "t2\n",
      "t1\n",
      "pg \n",
      "acc \n",
      "loss\n",
      "acc \n",
      "loss\n",
      "acc \n",
      "loss\n",
      "8 \n",
      "0.942±0.032\n",
      "0.319±0.028\n",
      "0.902±0.022\n",
      "0.395±0.207\n",
      "0.929±0.023\n",
      "0.292±0.087\n",
      "12\n",
      "0.961±0.025\n",
      "0.277±0.107\n",
      "0.935±0.021\n",
      "0.307±0.164\n",
      "0.936±0.027\n",
      "0.279±0.046\n",
      "16\n",
      "0.948±0.034\n",
      "0.350±0.333\n",
      "0.922±0.028\n",
      "0.306±0.107\n",
      "0.921±0.041\n",
      "0.313±0.053\n",
      "20\n",
      "0.948±0.026\n",
      "0.342±0.126\n",
      "0.922±0.035\n",
      "0.363±0.098\n",
      "0.942±0.034\n",
      "0.302±0.124\n",
      "24\n",
      "0.948±0.034\n",
      "0.348±0.162\n",
      "0.929±0.023\n",
      "0.309±0.067\n",
      "0.922±0.046\n",
      "0.322±0.176\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "preparations\n",
      "some important speciﬁcities of our computing platform contain: one amd epyc 7352\n",
      "24-core processor, 1 tb memory and four nivida a100-sxm gpus with 320 gb\n",
      "gpu memory.\n",
      "3.2\n",
      "ablation studies\n",
      "since all images in our dataset are 3d images, therefore, the initial channel is set 325\n",
      "which is equal to the tpp number.\n",
      "the loss functions in the following experiments\n",
      "shared categorical_crossentropy.\n",
      "all performances listed in this section\n",
      "are the average of performances with 5-fold cross-validation.\n",
      "3.2.1\n",
      "impact of gray level\n",
      "the image gray level determines the shape of the tpp matrix.\n",
      "while\n",
      "rescaling the image intensity, an arc tangent approach is utilized to yield the new image.\n",
      "their performances evaluated by\n",
      "accuracies are listed in table2 which tells us that t2 sequence yields the highest accuracy\n",
      "of 96.1% when the gray level is 12.\n",
      "other performances could be read\n",
      "in supplementary materials.\n",
      "table 3. test performances for three intensity rescaling approaches over t2, t1 and post-\n",
      "gadolinium where the gray level is set 12, acc denotes accuracy, pg denotes post -gadolinium,\n",
      "and artan is arc tangent function.\n",
      "0.936±0.027 0.279±0.046\n",
      "rescaling\n",
      "approach\n",
      "t2\n",
      "t1\n",
      "pg\n",
      "table 4. test performances comparison between multi-channel and solo-channel over t2, t1 and\n",
      "post-gadolinium where the intensity rescaling function is arc tangent, the gray level is set 20, acc\n",
      "denotes accuracy and pg denotes post -gadolinium.\n",
      "acc\n",
      "loss\n",
      "acc\n",
      "loss\n",
      "acc\n",
      "loss\n",
      "325\n",
      "0.948±0.026 0.342±0.126 0.922±0.035 0.363±0.098 0.934±0.034 0.302±0.124\n",
      "1\n",
      "0.863±0.027 0.274±0.091 0.810±0.145 0.458±0.105 0.811±0.047 0.590±0.107\n",
      "input\n",
      "channel\n",
      "t2\n",
      "t1\n",
      "pg\n",
      "a texture neural network to predict the abnormal brachial plexus\n",
      "477\n",
      "3.2.2\n",
      "impact of intensity rescaling approaches\n",
      "rescaling approaches of image intensity could also bring impacts on the bp’s differ-\n",
      "entiation while producing the tpp matrix.\n",
      "to test the performances fairly, we test above rescaling methods at the same gray\n",
      "level 12.\n",
      "the yielded performances evaluated by accuracies are shown in table 3 where\n",
      "arc tangent method achieves the highest accuracy of 96.1% over the t2 sequence.\n",
      "other\n",
      "performances are shown in supplementary materials.\n",
      "3.2.3\n",
      "multi-channels vs solo-channel\n",
      "we carry out experiments to train the tppnet model and make tests with arc tangent\n",
      "rescaling approach under gray level 16.\n",
      "performances with\n",
      "accuracies and loss are listed in table 4.\n",
      "other performances are listed in supplementary\n",
      "materials.\n",
      "all approaches shared the same image shape of 128 × 128 × 64 with 1\n",
      "channel.\n",
      "their performances are\n",
      "478\n",
      "w. cao et al.\n",
      "table 5. performance comparison between tppnet t1 sequence where acc is accuracy, auc\n",
      "denotes spe denotes speciﬁcity.\n",
      "3. t2 and post-\n",
      "gadolinium’s performances could be found in supplementary materials.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf:\n",
      "this paper addresses the need for improved ct-guidance\n",
      "during needle-based liver procedures (i.e., tumor ablation), while reduces\n",
      "the need for contrast agent injection during such interventions.\n",
      "to\n",
      "achieve this objective, we augment the intraoperative ct with the pre-\n",
      "operative vascular network deformed to match the current acquisition.\n",
      "first, a neural network learns local image features in a non-contrasted\n",
      "ct image by leveraging the known preoperative vessel tree geometry\n",
      "and topology extracted from a matching contrasted ct image.\n",
      "then,\n",
      "the augmented ct is generated by fusing the labeled vascular tree and\n",
      "the non-contrasted intraoperative ct.\n",
      "our method is trained and val-\n",
      "idated on porcine data, achieving an average dice score of 0.81 on the\n",
      "predicted vessel tree instead of 0.51 when a medical expert segments the\n",
      "non-contrasted ct.\n",
      "source code of this work is publicly avail-\n",
      "able at https://github.com/sidaty1/intraoperative ct augmentation.\n",
      "keywords: liver tumor ablation · needle-based procedures ·\n",
      "patient-speciﬁc interventions · ct-guidance · medical image\n",
      "augmentation\n",
      "1\n",
      "introduction\n",
      "needle-based liver tumor ablation techniques (e.g., radiofrequency, microwave,\n",
      "laser, cryoablation) have a great potential for local curative tumor control\n",
      "[1],\n",
      "with comparable results to surgery in the early stages for both primary and\n",
      "secondary cancers.\n",
      "ct-guidance is a widely used imaging modality for placing\n",
      "the needles, monitoring the treatment, and following up patients.\n",
      "https://doi.org/10.1007/978-3-031-43996-4_28\n",
      "292\n",
      "s. el hadramy et al.\n",
      "injection of contrast agents to visualize the intrahepatic vessels and the target\n",
      "tumor(s).\n",
      "in standard clinical settings, the insertion of each needle requires multiple\n",
      "check points during its progression, ﬁne-tune maneuvers, and eventual reposi-\n",
      "tioning.\n",
      "however, intrahepatic vessels (and some tumors) are only visible after\n",
      "contrast-enhancement, which has a short lifespan and dose-related deleterious\n",
      "kidney eﬀects.\n",
      "a workaround to shortcut these limitations is to\n",
      "perform an image fusion between previous contrasted and intraoperative non-\n",
      "contrasted images.\n",
      "in this work, we propose a method for visualizing intrahepatic structures\n",
      "after organ motion and needle-induced deformations, in non-injected images, by\n",
      "exploiting image features that are generally not perceivable by the human eye\n",
      "in common clinical workﬂows.\n",
      "to address this challenge, two main strategies could be considered: image\n",
      "fusion and image processing techniques.\n",
      "image fusion typically relies on the\n",
      "estimation of rigid or non-rigid transformations between 2 images, to bring into\n",
      "the intraoperative image structures of interest only visible in the preoperative\n",
      "data.\n",
      "recent deep learning approaches\n",
      "[11,12,14] have proved to be a successful alternative to solve image fusion prob-\n",
      "lems, even when a large non-linear mapping is required.\n",
      "when ground-truth\n",
      "displacement ﬁelds are not known, state-of-the-art methods use unsupervised\n",
      "techniques, usually an encoder-decoder architecture [7,13], to learn the unknown\n",
      "displacement ﬁeld between the 2 images.\n",
      "however, such unsupervised methods\n",
      "fail at solving our problem due to lack of similar image features between the\n",
      "contrasted (cct) and non-contrasted (ncct) image in the vascular tree region\n",
      "(see sect. 3.3).\n",
      "on the other hand, deep learning techniques have proven to be very eﬃcient\n",
      "at solving image processing challenges [15].\n",
      "for instance, image segmentation\n",
      "[16], image style transfer\n",
      "[17], or contrast-enhancement to cite a few.\n",
      "yet, seg-\n",
      "menting vessels from non-contrasted images remains a challenge for the medical\n",
      "imaging community\n",
      "[16]. style transfer aims to transfer the style of one image\n",
      "to another while preserving its content\n",
      "contrast-enhancement methods could be an\n",
      "alternative.\n",
      "[20], a deep neural network\n",
      "synthesizes contrast-enhanced ct from non contrast-enhanced ct. neverthe-\n",
      "less, results obtained by this method are not suﬃciently robust and accurate to\n",
      "provide an augmented intraoperative ct on which needle-based procedures can\n",
      "be guided.\n",
      "intraoperative ct augmentation for needle-based liver interventions\n",
      "293\n",
      "in this paper we propose an alternative approach, where a neural network\n",
      "learns local image features in a ncct image by leveraging the known preopera-\n",
      "tive vessel tree geometry and topology extracted from a matching (undeformed)\n",
      "cct.\n",
      "then, the augmented ct is generated by fusing the deformed vascular\n",
      "tree with the non-contrasted intraoperative ct. section 2 presents the method\n",
      "and its integration in the medical workﬂow.\n",
      "a few days or a week before the intervention, a preoperative\n",
      "diagnostic multiphase contrast-enhanced image (mpcect) is acquired (fig. 1,\n",
      "yellow box).\n",
      "the day of the intervention, a second mpcect image is acquired\n",
      "before starting the needle insertion, followed by a series of standard, non-injected\n",
      "acquisitions to guide the needle insertion (fig. 1, blue box).\n",
      "using such a non-\n",
      "contrasted intraoperative image as input, our method performs a combined\n",
      "non-rigid registration and augmentation of the intraoperative ct by\n",
      "adding anatomical features (mainly intrahepatic vessels and tumors) from the\n",
      "preoperative image to the current image.\n",
      "to achieve this result, our method only\n",
      "requires to process and train on the baseline mpcect image (fig. 1, red box).\n",
      "the neural network trained\n",
      "on preoperative mpcect avoids contrast agent injections during the intervention.\n",
      "fig.\n",
      "finally, the augmented ct is created by fusing the\n",
      "segmented image and labels with the intraoperative ncct.\n",
      "since vascular structures are not visible in non-contrasted images,\n",
      "the extraction of this map is done by segmenting the cct and then using this\n",
      "segmentation as a mask in the ncct.\n",
      "mathematical morphology operators, in\n",
      "particular a dilation operation [23], are performed on the segmented region of\n",
      "interest to slightly increase its dimensions.\n",
      "this is needed to compensate for\n",
      "segmentation errors and the slight anatomical motion that may exist between\n",
      "the contrasted and non-contrasted image acquisitions.\n",
      "in practice, the acquisition\n",
      "protocols limit the shift between the ncct and cct acquisitions, and only a\n",
      "few sequential dilation operations are needed to ensure we capture the true vessel\n",
      "ﬁngerprint in the ncct image.\n",
      "note that the resulting vessel map is not a binary\n",
      "mask, but a subset of the image limited to the volume covered by the vessels.\n",
      "2.2\n",
      "data augmentation\n",
      "the preoperative mpcect provides a couple of registered ncct and cct\n",
      "images.\n",
      "therefore, we augment the data set by applying multiple random deformations\n",
      "to the original images.\n",
      "random deformations are created by considering a pre-\n",
      "deﬁned set of control points for which we deﬁne a displacement ﬁeld with a\n",
      "random normal distribution.\n",
      "the displacement ﬁeld of the full volume is then\n",
      "obtained by linearly interpolating the control points’ displacement ﬁeld to the\n",
      "rest of the volume.\n",
      "our network learns to ﬁnd the image features (or vessel ﬁngerprint)\n",
      "present in the vessel map, in a given ncct assuming the knowledge of its geom-\n",
      "etry, topology, and the distribution of contrast from the preoperative mpcect.\n",
      "3. it consists of a four lay-\n",
      "ers analysis (left side) and synthesis (right side) paths that provide a non-linear\n",
      "mapping between low resolution input and output images.\n",
      "in the last layer, a 1 × 1 × 1 convolution reduces the\n",
      "number of output channels to one, yielding the vessel map in the intraoperative\n",
      "intraoperative ct augmentation for needle-based liver interventions\n",
      "295\n",
      "image.\n",
      "3. our neural network uses a four-path encoder-decoder architecture and takes\n",
      "as input a two-channel image corresponding to the intraoperative ncct image con-\n",
      "catenated with the preoperative vessel map.\n",
      "the output is the intraoperative vessel\n",
      "map.\n",
      "2.4\n",
      "augmented ct\n",
      "once the network has been trained on the patient-speciﬁc preoperative data, the\n",
      "next step is to augment and visualize the intraoperative ncct.\n",
      "2.1 are not reversible (i.e. the\n",
      "segmented vessel tree cannot be recovered from the vm by applying the same\n",
      "number of erosion operations).\n",
      "also, neighboring branches in the vessel tree\n",
      "could end up being fused, thus changing the topology of the vessel map.\n",
      "therefore, to retrieve the correct segmented (yet deformed) vascular tree, we\n",
      "compute a displacement ﬁeld between the pre- and intraoperative vms.\n",
      "the resulting displacement ﬁeld is\n",
      "applied on the preoperative segmentation to retrieve the intraoperative vessel\n",
      "tree segmentation.\n",
      "4.\n",
      "– the augmented image is obtained by fusing the predicted intraoperative seg-\n",
      "mentation with the intraoperative ncct image.\n",
      "the augmented vessels are\n",
      "displayed in green to ensure the clinician is aware this is not a true cct\n",
      "image (see fig. 5).\n",
      "– it is also possible to add anatomical labels to the intraoperative augmented\n",
      "ct to further assist the clinician.\n",
      "to achieve this objective, we compute a\n",
      "graph data structure from the preoperative segmentation.\n",
      "we use the graph structure to associate each anatomical label\n",
      "(manually deﬁned) with a strahler [6] graph ordering.\n",
      "the same process is\n",
      "applied to the predicted intraoperative segmentation.\n",
      "this makes it possible\n",
      "to correctly map the preoperative anatomical labels (e.g. vessel name) and\n",
      "display them on the augmented image.\n",
      "fig.\n",
      "4. this ﬁgure illustrates the diﬀerent stages of the pipeline adopted to generate the\n",
      "vm and show how the vessel tree topology is retrieved from the predicted intraoperative\n",
      "vm by computing a displacement ﬁeld between the preoperative vm and the predicted\n",
      "vm.\n",
      "this ﬁeld is applied to the preoperative segmentation to get the intraoperative\n",
      "one.\n",
      "3\n",
      "results and discussion\n",
      "3.1\n",
      "dataset and implementation details\n",
      "to validate our approach, 4 couples of mpcect abdominal porcine images\n",
      "were acquired from 4 diﬀerent subjects.\n",
      "we recall that an\n",
      "mpcect contains a set of registered ncct and cct images.\n",
      "these images are\n",
      "then cropped and down-sampled to 256 × 256 × 256, and the voxels intensities\n",
      "are scaled between 0 and 255.\n",
      "finally, we extract the vm from each mpcect\n",
      "sample and apply 3 dilation operations, which demonstrated the best perfor-\n",
      "mance in terms of prediction accuracy and robustness on our data.\n",
      "[25] and others do not ﬁt\n",
      "our problem since they do not include the ncct images.\n",
      "for a given subject, we\n",
      "generate 100 displacement ﬁelds using the data augmentation strategy explained\n",
      "above with 50 voxels for the control points spacing in the three spatial directions\n",
      "intraoperative ct augmentation for needle-based liver interventions\n",
      "297\n",
      "and a standard deviation of 5 voxels for the normal distributions.\n",
      "our\n",
      "method is implemented in tensorﬂow 2.4, on a geforce rtx 3090.\n",
      "the\n",
      "training process converges in about 1,000 epochs with a batch size of 1 and 200\n",
      "steps per epoch.\n",
      "3.2\n",
      "results\n",
      "to assess our method, we use a dice score to measure the overlap between our\n",
      "predicted segmentation and the ground truth.\n",
      "being a commonly used metric\n",
      "for segmentation problems, dice aligns the nature of our problem as well as the\n",
      "clinical impact of our solution.\n",
      "an example of a subject intraoperative augmented ct is illustrated in\n",
      "fig.\n",
      "5, where the three images correspond respectively to the initial non injected\n",
      "ct, the augmented ct without and with labels.\n",
      "the green vessels correspond to the ground truth\n",
      "intraoperative segmentation, the orange ones to the predicted intraoperative\n",
      "segmentation and ﬁnally the gray vessel tree corresponds to the preoperative\n",
      "cct vessel tree.\n",
      "the middle image shows\n",
      "the augmented ct with the predicted vessel tree (in green).\n",
      "the rightmost image shows\n",
      "the augmented image with anatomical labels transferred from the preoperative image\n",
      "segmentation and labelling.\n",
      "6. assessment of our method for subject 1.\n",
      "(color ﬁgure online)\n",
      "qualitative assessment: to further demonstrate the value of our method,\n",
      "we have asked two clinicians to manually segment the ncct images in the\n",
      "intraoperative mpcect data.\n",
      "our method outperforms the results of both clinicians, with\n",
      "an average dice score of 0.81 against 0.51 as a mean for the clinical experts.\n",
      "using the data of the subject 1, a u-net was trained\n",
      "to segment the vessel tree of the intraoperative ncct image.\n",
      "the network only\n",
      "managed to segment a small portion of the main portal vein branch.\n",
      "we also studied the inﬂuence of the diﬀusion kernel applied to\n",
      "the initial segmentation.\n",
      "we have seen, on our experimental data, that 3 dilation\n",
      "operations were suﬃcient to compensate for the possible motion between ncct\n",
      "and cct acquisitions.\n",
      "comparison with voxelmorph: the problem that we address can be seen\n",
      "from diﬀerent angles.\n",
      "in particular, we could attempt to solve it by register-\n",
      "ing the preoperative ncct to the intraoperative one and then applying the\n",
      "intraoperative ct augmentation for needle-based liver interventions\n",
      "299\n",
      "resulting displacement ﬁeld to the known preoperative segmentation.\n",
      "however,\n",
      "state-of-the-art registration methods such as voxelmorph [7] and others do not\n",
      "necessarily guarantee a diﬀeomorphic [8] displacement ﬁeld that ensures the\n",
      "continuity of the displacement ﬁeld inside the parenchyma where the intensity\n",
      "is quite homogeneous on the ncct.\n",
      "while the voxelmorph\n",
      "network accurately registers the liver shape, the displacement ﬁeld is almost null\n",
      "in the region of vessels inside the parenchyma.\n",
      "therefore, the preoperative vessel\n",
      "segmentation is not correctly transferred into the intraoperative image.\n",
      "fig.\n",
      "7. illustration of voxelmorph registration between ncct preoperative and intra-\n",
      "operative images.\n",
      "df stands for\n",
      "the displacement ﬁelds on x and y predicted by voxelmorph method.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf:\n",
      "first, we build a video\n",
      "transformer, which captures both local and global long-range dependen-\n",
      "cies across spatial and temporal dimensions.\n",
      "second, we pre-train our\n",
      "transformer model using global and local views via a self-supervised\n",
      "manner, aiming to make it robust to spatial-temporal variations and\n",
      "discriminative across diﬀerent scenes.\n",
      "with experiments on 3 diﬀerent types of down-\n",
      "stream tasks, including classiﬁcation, segmentation, and detection, our\n",
      "endo-fm surpasses the current state-of-the-art (sota) self-supervised\n",
      "pre-training and adapter-based transfer learning methods by a signiﬁcant\n",
      "margin, such as vcl (3.1% f1, 4.8% dice, and 5.5% f1 for classiﬁcation,\n",
      "segmentation, and detection) and st-adapter (5.9% f1, 9.6% dice, and\n",
      "9.9% f1 for classiﬁcation, segmentation, and detection).\n",
      "keywords: foundation model · endoscopy video · pre-train\n",
      "1\n",
      "introduction\n",
      "foundation models pre-trained on large-scale data have recently showed suc-\n",
      "cess in various downstream tasks on medical images including classiﬁcation [9],\n",
      "z. wang and c. liu—equal contributions.\n",
      "[33], and segmentation\n",
      "it is arguable that a\n",
      "speciﬁc foundation model trained on some certain type of data is useful at the\n",
      "moment.\n",
      "[20,21], involves pre-training on large-scale\n",
      "image-text pairs and relies on large language models to learn cross-modality\n",
      "features.\n",
      "however, since clinical routines for endoscopy videos typically do not\n",
      "involve text data, a pure image-based foundation model is currently more fea-\n",
      "sible.\n",
      "this would indicate that our video transformer\n",
      "could have suﬃcient capacity to model the rich spatial-temporal information of\n",
      "endoscopy videos.\n",
      "to learn rich spatial-temporal information from endoscopy video data [12],\n",
      "our endo-fm is pre-trained via a self-supervised manner by narrowing the gap\n",
      "between feature representations from diﬀerent spatial-temporal views of the same\n",
      "video.\n",
      "experimental results on 3 diﬀerent types of\n",
      "downstream tasks demonstrate the eﬀectiveness of endo-fm, surpassing the\n",
      "current state-of-the-art self-supervised pre-training and adapter-based transfer\n",
      "learning methods by a signiﬁcant margin, such as vcl (3.1% f1, 4.8% dice, and\n",
      "5.5% f1 for classiﬁcation, segmentation, and detection) and st-adapter (5.9%\n",
      "f1, 9.6% dice, and 9.9% f1 for classiﬁcation, segmentation, and detection).\n",
      "the spatial and\n",
      "temporal attention mechanisms in our model capture long-range dependencies\n",
      "across both spatial and temporal dimensions, with a larger receptive ﬁeld than\n",
      "conventional convolutional kernels\n",
      "our model also includes a learnable class\n",
      "token, representing the global features learned by the model along the spatial\n",
      "and temporal dimensions.\n",
      "speciﬁ-\n",
      "cally, we ﬁx the spatial and temporal positional encoding vectors to the highest\n",
      "resolution of the input view for each dimension, making it easy to interpolate\n",
      "for views with smaller spatial size or lower temporal frame rate.\n",
      "diﬀerent from image-based pre-training [33], our\n",
      "video-oriented pre-training is designed to capture the relationships between dif-\n",
      "ferent spatial-temporal variations.\n",
      "moreover, by predicting the nuanced diﬀerences of tissue and lesions in\n",
      "a view with a high frame rate from another with a low frame rate, the model is\n",
      "encouraged to learn more comprehensive motion-related contextual information.\n",
      "to prevent\n",
      "the problem of the teacher and student models constantly outputting the same\n",
      "value during pre-training, we update the student model θ through backpropaga-\n",
      "tion, while the teacher model φ is updated through exponential moving average\n",
      "(ema) using the student’s weights.\n",
      "this is achieved by updating the teacher’s\n",
      "weights as φt ← αφt−1 + (1 − α)θt at each training iteration t. here, α is a\n",
      "momentum hyper-parameter that determines the updating rate.\n",
      "these videos are captured using diﬀerent sur-\n",
      "gical systems and in a wide range of environmental conditions [10].\n",
      "to address\n",
      "this variability, we apply temporally consistent spatial augmentations\n",
      "our augmentation approach includes random hori-\n",
      "zontal ﬂips, color jitter, gaussian blur, solarization, and so on, which enhances\n",
      "the robustness and generalizability of endo-fm.\n",
      "for endo-fm, we set the patch size p as 16 and embedding dimension d as\n",
      "768.\n",
      "the mlp head projects the dimension\n",
      "of class token to 65536.\n",
      "the ema update momentum α is 0.996.\n",
      "the\n",
      "pre-training is ﬁnished with 30 epochs with a cosine schedule [16].\n",
      "3\n",
      "experiment\n",
      "3.1\n",
      "datasets and downstream setup\n",
      "we collect all possible public endoscope video datasets and a new one from\n",
      "baoshan branch of renji hospital for pre-training.\n",
      "method\n",
      "venue\n",
      "pre-training\n",
      "polypdiag\n",
      "cvc-12k\n",
      "kumc\n",
      "time (h)\n",
      "(classiﬁcation) (segmentation) (detection)\n",
      "scratch (rand. init.)\n",
      "n/a\n",
      "83.5±1.3\n",
      "53.2±3.2\n",
      "73.5±4.3\n",
      "timesformer\n",
      "[24]\n",
      "neurips’22\n",
      "8.1\n",
      "84.8±0.7\n",
      "64.3±1.9\n",
      "74.9±2.9\n",
      "endo-fm (ours)\n",
      "20.4\n",
      "90.7±0.4\n",
      "73.9±1.2\n",
      "84.1±1.3\n",
      "of 5 s on average.\n",
      "we evaluate our pre-trained endo-fm on three downstream\n",
      "tasks: disease diagnosis (polypdiag [32]), polyp segmentation (cvc-12k [2]), and\n",
      "detection (kumc [15]).\n",
      "2) cvc-12k: a transunet equipped with endo-fm as the\n",
      "backbone is implemented.\n",
      "3) kumc: we implement a stft\n",
      "the same\n",
      "experimental setup is applied to all the experiments for fair comparisons.\n",
      "we can observe that\n",
      "the scratch model shows low performance on all 3 downstream tasks, especially\n",
      "for segmentation.\n",
      "compared with training from scratch, our endo-fm achieves\n",
      "+7.2% f1, +20.7% dice, and +10.6% f1 improvements for classiﬁcation, seg-\n",
      "108\n",
      "z. wang et al.\n",
      "mentation, and detection tasks, respectively, indicating the high eﬀectiveness of\n",
      "our proposed pre-training approach.\n",
      "such signiﬁcant improvements are\n",
      "beneﬁted from our speciﬁc spatial-temporal pre-training designed for endoscopy\n",
      "videos to tackle the complex context information and dynamic scenes.\n",
      "we can\n",
      "learn that both spatial and temporal sampling for local views can help improve\n",
      "the performance and their combination produces a plus, yielding +4.3% f1\n",
      "improvement.\n",
      "furthermore, our proposed dynamic matching scheme boosts the\n",
      "performance to 89.7%, demonstrating the importance of capturing the motion\n",
      "related context information from dynamic scenes.\n",
      "additionally, the performance\n",
      "is further improved with video augmentations from 89.7% to 90.7%.\n",
      "it indicates that joint prediction scenarios, where we\n",
      "predict vg from both vl (cross-view matching) and vg (dynamic motion match-\n",
      "ing), result in optimal performance.\n",
      "we ﬁnd that incorporating more views\n",
      "and increasing the length variations of local views yields better performance.\n",
      "these improvements stem from the spatial-temporal change invariant and\n",
      "cross-video discriminative features learned from the diverse endoscopy videos.\n",
      "exten-\n",
      "sive experimental results on 3 downstream tasks demonstrate the eﬀectiveness\n",
      "of endo-fm, signiﬁcantly outperforming other state-of-the-art video-based pre-\n",
      "training methods, and showcasing its potential for clinical application.\n",
      "[14] model, which is developed for segmentation\n",
      "task, we try to apply sam for our downstream task cvc-12k with the same\n",
      "ﬁne-tuning scheme as endo-fm.\n",
      "the experimental results show that sam can\n",
      "achieve comparable performance with our endo-fm for the downstream seg-\n",
      "mentation task.\n",
      "moreover, besides\n",
      "segmentation, endo-fm can also be easily applied to other types of tasks includ-\n",
      "ing classiﬁcation and detection.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_58.pdf:\n",
      "we propose a bevel tip oce needle probe for percutaneous inser-\n",
      "tions, where biomechanical characterization of deep tissue could enable\n",
      "precise needle placement, e.g., in prostate biopsy.\n",
      "using a novel setup, we simulate deep tissue indentations where\n",
      "frictional forces and bulk sample displacement can aﬀect biomechanical\n",
      "characterization.\n",
      "performing surface and deep tissue indentation experi-\n",
      "ments, we compare our approach with external force and needle position\n",
      "measurements at the needle shaft.\n",
      "compared to surface\n",
      "indentations, external force-position measurements are strongly aﬀected\n",
      "by frictional forces and bulk displacement and show a relative error of\n",
      "49.2% and 42.4% for soft and stiﬀ phantoms, respectively.\n",
      "in contrast,\n",
      "quantitative oce measurements show a reduced relative error of 26.4%\n",
      "and 4.9% for deep indentations of soft and stiﬀ phantoms, respectively.\n",
      "finally, we demonstrate that the oce measurements can be used to\n",
      "eﬀectively discriminate the tissue mimicking phantoms.\n",
      "[4]. diﬀerent imaging modalities\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43996-4 58.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "handheld oce systems for intraoperative assessment [2,23] have\n",
      "also been proposed.\n",
      "taking prostate cancer as an example, biomechanical characterization could\n",
      "guide needle placement for improved cancer detection rates while reducing com-\n",
      "plications associated with increased core counts, e.g. pain and erectile dysfunc-\n",
      "tion [14,18].\n",
      "however, the measurement of both the applied load and the local\n",
      "sample compression is challenging.\n",
      "furthermore, the prostate\n",
      "is known to display large bulk displacement caused by patient movement and\n",
      "needle insertions\n",
      "tip force sensing for estimating elastic properties has been proposed [5] but bulk\n",
      "tissue displacement of deep tissue was not considered.\n",
      "we design an experimental setup\n",
      "that can simulate friction forces and bulk displacement occurring during needle\n",
      "biopsy (fig. 1).\n",
      "we consider tissue-mimicking phantoms for surface and deep tis-\n",
      "sue indentation experiments and compare our results with force-position curves\n",
      "externally measured at the needle shaft.\n",
      "we then present an experimental setup for simulating\n",
      "friction and bulk displacement and describe the conducted surface and deep tis-\n",
      "sue indentation experiments.\n",
      "the forward viewing ﬁber (fiber 1) images sample\n",
      "compression while the load sensing ﬁber (fiber 2) visualizes the displacement of\n",
      "a reference epoxy layer that is deformed under load.\n",
      "friction\n",
      "forces (red) and tip forces (grey) are superimposed and the forward motion of the needle\n",
      "(black) only partially results in sample compression (green) due to bulk displacement\n",
      "(blue).\n",
      "middle: experimental setup used for indentation experiments, with a linear\n",
      "actuator (a), an axial force sensor (b), the oce needle probe (c) and the sample\n",
      "layers (d).\n",
      "friction can be added by puncturing multiple layers and bulk displacement is simulated\n",
      "by placing the sample on springs.\n",
      "(color ﬁgure online)\n",
      "610\n",
      "r. mieling et al.\n",
      "2.2\n",
      "oce measurement\n",
      "in unconﬁned compression, the elasticity of the sample can be determined by\n",
      "the relation between stress σ and bulk strain ϵ denoted by the young’s modulus\n",
      "oct\n",
      "mux\n",
      "a-scan fiber 2\n",
      "a-scan fiber 1\n",
      "epoxy\n",
      "2mm\n",
      "trigger\n",
      "fig.\n",
      "we use optical ﬁber 1 to measure sample compression and ﬁber 2 for\n",
      "the displacement of a reference epoxy layer (green) that is deformed under tip forces.\n",
      "to obtain a single parameter for comparing\n",
      "two measurements, we assume a linear relation\n",
      "eoce(ft , ϵl)\n",
      "the phase shift between two a-scans is proportional to the depth\n",
      "dependent displacement δui(z, t)\n",
      "δφi(z, t) = 4 π n δui(z, t)\n",
      "λ0\n",
      ",\n",
      "(3)\n",
      "assuming a refractive index n of 1.45 and 1.5 for tissue (fiber 1) and epoxy\n",
      "(fiber 2), respectively.\n",
      "for ﬁber 1, we employ a\n",
      "moving average with a window size of 0.1 mm.\n",
      "we estimate local strain based on\n",
      "the ﬁnite diﬀerence along the spatial dimension over an axial depth δz of 1 mm.\n",
      "ϵl(t)\n",
      "(5)\n",
      "2.3\n",
      "experimental setup\n",
      "we build an experimental setup for surface and deep tissue indentations with\n",
      "simulated force and bulk displacement (fig. 1).\n",
      "for deep tissue indentations, dif-\n",
      "ferent tissue phantoms are stacked on a sample holder with springs in between.\n",
      "for surface measurements, we position the tissue phantoms separately without\n",
      "additional springs or tissue around the needle shaft.\n",
      "we use a motorized linear\n",
      "stage (zfs25b, thorlabs gmbh, ger) to drive the needle while simultaneously\n",
      "logging motor positions.\n",
      "reference elasticity is determined by unconﬁned compression exper-\n",
      "iments of three cylindrical samples for each material according to eq. 1, using\n",
      "force and position sensor data (see supplementary material).\n",
      "the young’s modu-\n",
      "lus is obtained by linear regression for the combined measurements of each mate-\n",
      "rial.\n",
      "acquisition window during pre-deformation phase (a) considered\n",
      "for oce measurements is indicated by dashed white line.\n",
      "local strain is calculated\n",
      "based on the tracked deformation from the oct phase diﬀerence as visualized in red.\n",
      "612\n",
      "r. mieling et al.\n",
      "then determine a linear ﬁt according to eq. 5 and obtain af = 174.4 mn mm−1\n",
      "from external force sensor and motor position measurements (see supplementary\n",
      "material).\n",
      "2.4\n",
      "indentation experiments\n",
      "in total, we conduct ten oce indentation measurements for each material.\n",
      "three\n",
      "surface measurements with ﬁxed samples and seven deep tissue indentations with\n",
      "simulated friction and bulk displacement.\n",
      "as the beginning of the needle movement\n",
      "might not directly correspond to the beginning of sample indentation, we eval-\n",
      "uate oce measurements only if the estimated tip force is larger than 50 mn.\n",
      "to further ensure that measurements occur within the pre-rupture deformation\n",
      "phase [6,15], only samples below 20 % local strain are considered.\n",
      "3. we evaluate external\n",
      "needle shaft measurements of relative axial force and relative motor position\n",
      "with the same endpoint obtained from local strain estimates.\n",
      "as we can con-\n",
      "sider surface measurements as equivalents to the known elasticity, we regard\n",
      "the relative error (re) of the mean value obtained for deep indentations, with\n",
      "respect to the average estimate during surface indentations.\n",
      "4. (a) oce needle measurements for surface and deep tissue indentations based on\n",
      "the estimated tip force (ﬁber 2) and the detected local strain (ﬁber 1).\n",
      "friction increases the measured axial force,\n",
      "while bulk displacement decreases the observed slope.\n",
      "bulk displacement can occur\n",
      "suddenly due to stick-slip, as seen in two cases of material b. (b) resulting elasticity\n",
      "estimates show overlap between the two materials, hampering quantitative biomechan-\n",
      "ical characterization.\n",
      "for both oce and external measurements and material a and b, respectively.\n",
      "3\n",
      "results\n",
      "the oce measurements for surface and deep tissue indentations are displayed\n",
      "in fig.\n",
      "it can be seen that oce measurements result in separation of both\n",
      "materials while an overlap is visible for external sensors.\n",
      "biomechanical characteriza-\n",
      "tion based on the oce estimates allows complete separation between materials,\n",
      "with auroc and auprc scores of 1.00 (see supplementary material).\n",
      "exter-\n",
      "nal measurements do not enable robust discrimination of materials and yielded\n",
      "auroc and auprc scores of only 0.85 and 0.861, respectively.\n",
      "b surf\n",
      "14.92 ± 3.28 12.10 18.53 361.02 ± 6.94\n",
      "353.37 366.92 3\n",
      "deep 15.39 ± 4.48 10.02 21.75 201.31 ± 89.04 136.56 354.60 7\n",
      "re\n",
      "4.9 %\n",
      "42.4 %\n",
      "ducted indentation experiments demonstrate the feasibility of oce elasticity\n",
      "estimates for deep tissue needle insertions.\n",
      "oce estimates show better agree-\n",
      "ment between surface and deep tissue indentations compared to external mea-\n",
      "surements, as displayed by reduced relative errors of 26.4% and 4.9% for both\n",
      "phantoms, respectively.\n",
      "bulk displacement causes considerate underestimation\n",
      "of elasticity estimates when only needle position and axial forces are considered,\n",
      "shown by relative errors of 49.2% and 42.4% for material a and b, respec-\n",
      "tively.\n",
      "4b and the auroc and auprc\n",
      "scores of 1. note that the high errors for external measurements at the needle\n",
      "shaft are systematic, as friction and bulk displacement are unknown.\n",
      "moreover, considering the\n",
      "standard deviation for oce estimates, improved calibration of our dual-ﬁber\n",
      "needle probe is expected to further improve performance.\n",
      "weighted strain estimation\n",
      "based on oct signal intensity [26] could address the underestimation of local\n",
      "strain during segments of low signal-to-noise-ratio (see supplementary material).\n",
      "compression oce theoretically\n",
      "enables the analysis of non-linear elastic behavior [26] and future experiments\n",
      "will consider non-linear models and unloading cycles better beﬁtting needle-\n",
      "tissue-interaction\n",
      "while the cylindrical tip is advantageous for calculating the\n",
      "optical coherence elastography needle for biomechanical characterization\n",
      "615\n",
      "young’s modulus, it has been shown that the calculation of an equivalent young’s\n",
      "modulus is rarely comparable across diﬀerent techniques and samples\n",
      "further experiments need to include biological soft tissue to validate\n",
      "the approach for clinical application, as our evaluation is currently limited to\n",
      "homogeneous gelatin.\n",
      "this needle probe could also be very useful when con-\n",
      "sidering robotic needle insertions, e.g., to implement feedback control based on\n",
      "elasticity estimates.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf:\n",
      "homologous anatomical landmarks between medical scans\n",
      "are instrumental in quantitative assessment of image registration qual-\n",
      "ity in various clinical applications, such as mri-ultrasound registra-\n",
      "tion for tissue shift correction in ultrasound-guided brain tumor resec-\n",
      "tion.\n",
      "while manually identiﬁed landmark pairs between mri and ultra-\n",
      "sound (us) have greatly facilitated the validation of diﬀerent registra-\n",
      "tion algorithms for the task, the procedure requires signiﬁcant expertise,\n",
      "labor, and time, and can be prone to inter- and intra-rater inconsis-\n",
      "tency.\n",
      "so far, many traditional and machine learning approaches have\n",
      "been presented for anatomical landmark detection, but they primarily\n",
      "focus on mono-modal applications.\n",
      "speciﬁcally, two convolutional neural networks\n",
      "were trained jointly to encode image features in mri and us scans to\n",
      "help match the us image patch that contain the corresponding land-\n",
      "marks in the mri.\n",
      "early surgical treat-\n",
      "ment to remove the maximum amount of cancerous tissues while preserving the\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "the phenomenon is\n",
      "referred to as brain shift, and often invalidates the pre-surgical plan by displacing\n",
      "surgical targets and other vital anatomies.\n",
      "as the true underlying deformation from brain shift is impossible to obtain\n",
      "and the diﬀerences of image features between mri and us are large, quantitative\n",
      "validation of automatic mri-us registration algorithms often rely on homolo-\n",
      "gous anatomical landmarks that are manually labeled between corresponding\n",
      "mri and intra-operative us scans\n",
      "however, manual landmark identiﬁcation\n",
      "requires strong expertise in anatomy and is costly in labor and time.\n",
      "these factors make quality assess-\n",
      "ment of brain shift correction for us-guided brain tumor resection challenging.\n",
      "previously, many groups have proposed algorithms to label landmarks in\n",
      "anatomical scans [4–9].\n",
      "in addition, unlike\n",
      "other applications, where the full anatomy is visible in the scan and all land-\n",
      "marks have consistent spatial arrangements across subjects, intra-operative us\n",
      "of brain tumor resection only contains local regions of the pathology with non-\n",
      "canonical orientations.\n",
      "speciﬁcally, the technique\n",
      "leverages two convolutional neural networks (cnns) to learn features between\n",
      "mri and us that distinguish the inter-modal image patches which are cen-\n",
      "tered at the matching landmarks from those that are not.\n",
      "670\n",
      "s. salari et al.\n",
      "2\n",
      "related work\n",
      "contrastive learning has recently shown great results in a wide range of medical\n",
      "image analysis tasks [12–18].\n",
      "this\n",
      "self-supervised learning set-up allows robust feature learning and embedding\n",
      "without explicit guidance from ﬁne-grained image annotations, and the encoded\n",
      "features can be adopted in various downstream tasks, such as segmentation.\n",
      "a\n",
      "few recent works [19–21] explored the potential of cl in anatomical landmark\n",
      "annotation in head x-ray images for 2d skull landmarks.\n",
      "[19,20]\n",
      "attempted to leverage cl for more eﬃcient and robust learning.\n",
      "[21]\n",
      "used multiscale pixel-wise contrastive proxy tasks for skull landmark detection\n",
      "in x-ray images.\n",
      "these prior works with cl focus on single-modal 2d landmark\n",
      "identiﬁcation with systematic landmark localization protocols and sharp image\n",
      "contrast (i.e., skull in x-ray).\n",
      "in cl,\n",
      "many works have employed the infonce loss function [22,23] in attaining good\n",
      "outcomes.\n",
      "[10] (https://archive.sigma2.no/pages/\n",
      "public/dataset detail.jsf?id=10.11582/2020.00025) to train and evaluate our\n",
      "proposed method.\n",
      "all images were resampled to a uniﬁed dimension of 256 × 256 ×\n",
      "288 voxels, with an isotropic resolution of ∼0.5mm. between mri and the cor-\n",
      "responding us images, matching anatomical landmarks were manually labeled\n",
      "by experts and 15∼16 landmarks were available per case.\n",
      "3.2\n",
      "contrastive learning framework\n",
      "we used two cnns with identical architectures in parallel to extract robust\n",
      "image features from mri and us scans.\n",
      "3.3\n",
      "landmark matching with a 2.5d approach\n",
      "working with 3d images is computationally expensive and can make the model\n",
      "training unstable and prone to overﬁtting, especially when the size of the\n",
      "database is limited.\n",
      "therefore, instead of a full 3d processing, we decided to\n",
      "implement a 2.5d approach [25] to leverage the eﬃciency of 2d cnn in the\n",
      "672\n",
      "s. salari et al.\n",
      "cl framework for the task.\n",
      "in this case, we extracted a series of three adjacent\n",
      "2d image patches in one canonical direction (x-, y-, or z-direction), with the\n",
      "middle slice centred at the true or candidate landmarks in a 3d scan to provide\n",
      "slight spatial context for the middle slice of interest.\n",
      "to construct the full 2.5d\n",
      "formulation, we performed the same image patch series extraction in all x-, y-,\n",
      "and z-directions for a landmark, and this 2.5d patch forms the basis to compute\n",
      "the similarity between the queried us and reference mri patches.\n",
      "note that during network\n",
      "training, instead of 2.5d patches, we compared the 2d image patch series in\n",
      "one canonical direction between mri and us, and 2d patch series in all three\n",
      "directions were used.\n",
      "during the inference stage, the similarity between mri\n",
      "and us 2.5d patches was obtained by summing the similarities of correspond-\n",
      "ing 2d image patch series in each direction, and a match was determined with\n",
      "the highest similarity from all queried us patches.\n",
      "the general overview of the utilized framework for 2d image patch\n",
      "extraction is shown in fig.\n",
      "[11] is a well-known tool for keypoint detection and image\n",
      "registration.\n",
      "it has been widely used in multi-modal medical registration, such\n",
      "as landmark matching for brain shift correction in image-guided neurosurgery\n",
      "[8,26].\n",
      "towards multi-modal anatomical landmark detection\n",
      "673\n",
      "4\n",
      "experimental setup\n",
      "4.1\n",
      "data preprocessing\n",
      "for cl training, both positive and negative sample pairs need to be created.\n",
      "these sample pairs were used to train two cnns to extract relevant\n",
      "image features across mri and us leveraging the infonce loss.\n",
      "the loss function has been\n",
      "widely used and demonstrated great performance in many vision tasks.\n",
      "i are the cropped image patches around the corresponding landmarks in\n",
      "mr and us scans, respectively, and xn\n",
      "i\n",
      "is a mismatched patch in the us image\n",
      "to that cropped around the mri reference landmark.\n",
      "4.3\n",
      "implementation details and evaluation\n",
      "to train our dl model, we made subject-wise division of the entire dataset into\n",
      "70%:15%:15% as the training, validation, and testing sets, respectively.\n",
      "also,\n",
      "to improve the robustness of the network, we used data augmentation for the\n",
      "training data by random rotation, random horizontal ﬂip, and random vertical\n",
      "ﬂip.\n",
      "in order to evaluate the performance of our technique, we used the provided\n",
      "ground truth landmarks from the database and calculated the euclidean distance\n",
      "between the ground truths and predictions.\n",
      "3: an overview of the framework for image feature learning.\n",
      "when inspecting landmark identiﬁcation errors across all subjects between the\n",
      "cl and sift techniques, we also noticed that our cl framework has signiﬁ-\n",
      "cantly lower standard deviations (p <1e-4), implying that our technique has a\n",
      "better performance consistency.\n",
      "6\n",
      "discussion\n",
      "inter-modal anatomical landmark localization is still a diﬃcult task, espe-\n",
      "cially for the described application, where landmarks have no consistent spa-\n",
      "tial arrangement across diﬀerent cases and image features in us are rough.\n",
      "first, while the 2.5d approach is memory eﬃcient and quick,\n",
      "3d approaches may better capture the full corresponding image features.\n",
      "finally, we only\n",
      "employed us scans before resection since tissue removal can further complicate\n",
      "feature matching between mri and us, and requires more elaborate strategies,\n",
      "such as those involving segmentation of resected regions [27].\n",
      "as a baseline comparison, we employed the\n",
      "sift algorithm, which has demonstrated excellent performance in a large variety\n",
      "of computer vision problems for keypoint matching.\n",
      "this could be due to the coarse\n",
      "image features and textures of intra-operative us and the diﬀerences in the\n",
      "physical resolution between mri and us.\n",
      "besides better landmark identiﬁcation accuracy, the tighter standard deviations\n",
      "also imply that our dl approach serves a better role in grasping the local image\n",
      "features within the image patches.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf:\n",
      "in brain tumor resection, accurate removal of cancerous tis-\n",
      "sues while preserving eloquent regions is crucial to the safety and out-\n",
      "comes of the treatment.\n",
      "intra-operative ultrasound (ius) has been adopted\n",
      "to provide real-time images to track brain shift, and inter-modal (i.e.,\n",
      "mri-ius) registration is often required to update the pre-surgical plan.\n",
      "quality control for the registration results during surgery is important\n",
      "to avoid adverse outcomes, but manual veriﬁcation faces great challenges\n",
      "due to diﬃcult 3d visualization and the low contrast of ius.\n",
      "keywords: registration · inter-modal · error estimation · deep\n",
      "learning\n",
      "1\n",
      "introduction\n",
      "resection of early-stage brain tumors can greatly reduce the mortality rate of\n",
      "patients.\n",
      "however, as the true underlying tissue deformation is\n",
      "unknown due to the 3d nature of the surgical data and the time constraint,\n",
      "real-time manual inspection of mri-ius registration results is challenging and\n",
      "error-prone, especially for precision-sensitive neurosurgery.\n",
      "therefore, algorithms\n",
      "that can detect and quantify unreliable inter-modal medical image registration\n",
      "results are highly beneﬁcial.\n",
      "recently, automatic quality assessment for medical image registration has\n",
      "attracted increasing attention [4] from the domains of big medical data analy-\n",
      "sis and surgical interventions.\n",
      "with high eﬃciency, machine, and deep learning\n",
      "techniques have been proposed to allow automatic grading and dense estima-\n",
      "tion of medical image registration errors.\n",
      "more recently, deep learning (dl) techniques that learn task-speciﬁc\n",
      "features have also been adopted in automatic evaluation of medical image reg-\n",
      "istration, with a primary focus on intra-contrast/modal applications, including\n",
      "ct\n",
      "unfortunately, so far, error grading and estimation in\n",
      "inter-contrast/modal registration have rarely been explored, despite the partic-\n",
      "ular demand in surgical applications.\n",
      "although their algorithm performed well in simulated\n",
      "cases, the results on real clinical scans still required improvements.\n",
      "[13] from 2d\n",
      "to 3d and employed the technique in registration error assessment for the ﬁrst\n",
      "time.\n",
      "2\n",
      "methods and materials\n",
      "2.1\n",
      "dataset and preprocessing\n",
      "for methodological development and assessment, we used the resect (retro-\n",
      "spective evaluation of cerebral tumors) dataset\n",
      "[16], which has pre-operative\n",
      "focalerrornet for inter-modal registration error estimation\n",
      "691\n",
      "mri, and ius scans at diﬀerent surgical stages from 23 subjects who underwent\n",
      "low-grade glioma resection surgeries.\n",
      "1. we hypothesized that directly leveraging\n",
      "clinical ius could help learn more realistic image features with potentially better\n",
      "outcomes in clinical applications than with simulated contrasts [9,12].\n",
      "however,\n",
      "since the true brain shift model is impossible to obtain, we followed the strategy\n",
      "of creating silver ground truths for image alignment [9,12], upon which simulated\n",
      "misalignment is augmented in the ius to build and test our dl model.\n",
      "1. left to right: demonstration of sample pre-operative mri, perfectly registered,\n",
      "and deformed ius with a mean registration error of 1.4 mm.\n",
      "to perform spatial misalignment augmentation, we continued to leverage 3d\n",
      "b-spline transformation, similar to earlier reports on the same topic [10,12,17].\n",
      "in short, b-spline transformation can be modeled by a grid of regularly spaced\n",
      "control points and the associated parameters to allow various levels of nonlin-\n",
      "ear deformation.\n",
      "while the spacing of the control points determines the lev-\n",
      "els of details in local deformation ﬁelds, the displacement parameters control\n",
      "the magnitude of the deformation.\n",
      "to ensure that simulated registration errors\n",
      "are of diﬀerent varieties and sizes, we randomly selected the number of control\n",
      "points and the associated displacements (in each 3d axis) with a maximum of\n",
      "20 points and 30 mm, respectively.\n",
      "after misalignment augmentation\n",
      "on the previously co-registered ius, matching pairs of 3d image patches of size\n",
      "33 × 33 × 33 voxels were taken from both the ius volume and the correspond-\n",
      "ing mri.\n",
      "since\n",
      "b-spline transformation oﬀers a displacement vector at each voxel of the ius\n",
      "volume, we directly considered the norm of the vector as the simulated registra-\n",
      "tion error at the associated voxel.\n",
      "in our design, we determined the registration\n",
      "error of the image patch pair as the mean of all voxel-wise errors within the\n",
      "ius patch.\n",
      "finally, the image patch pairs, along with corresponding registration\n",
      "errors were then fed to the proposed dl algorithm for training and validation.\n",
      "with a\n",
      "similar goal as the vision transformer (vit), the focal modulation network was\n",
      "designed to model contextual information in images.\n",
      "it incorporates three main\n",
      "elements to achieve the goal: 1) focal contextualization that comprises a stack of\n",
      "depth-wise convolutional layers to account for long- to short-range dependencies,\n",
      "2) gated aggregation to collect contexts into a modulator for individual query\n",
      "tokens, and 3) element-wise aﬃne transformation to inject the modulator into\n",
      "the query.\n",
      "we designed the focalerrornet as\n",
      "a resnet-like variant of the focal modulation network to better encode rele-\n",
      "vant features across the input image and ensure a better gradient ﬂow.\n",
      "2.3\n",
      "uncertainty quantiﬁcation\n",
      "for registration error regression in surgical applications, knowledge regarding\n",
      "the reliability of the automated results is instrumental for the safety and well-\n",
      "being of the patients.\n",
      "although the concept has\n",
      "been widely applied in image segmentation and classiﬁcation, it has not been\n",
      "employed for registration error estimation, especially in the case of multi-modal\n",
      "situations, such as mri-ius alignment.\n",
      "[9,12]\n",
      "1.69 ± 1.37\n",
      "0.61\n",
      "focalerrornet 0.59 ± 0.57\n",
      "0.82\n",
      "2.4\n",
      "experimental setup and implementation\n",
      "from the transformation augmentation, we acquired 3380 samples of mri-ius\n",
      "pairs.\n",
      "for our experiments, we arbitrarily split the subjects into training, valida-\n",
      "tion, and test sets with the proportion of 60%, 20%, and 20%, respectively.\n",
      "to\n",
      "prevent information leakage, we ensured that each patient was included in only\n",
      "one of the split sets.\n",
      "furthermore, in addition to\n",
      "the transformation augmentation, we also included additional data augmenta-\n",
      "tion, including random noise addition and random image ﬂipping on training sets\n",
      "694\n",
      "s. salari et al.\n",
      "to mitigate overﬁtting and increase the model’s generalizability.\n",
      "3)\n",
      "that was employed for medical image registration error regression.\n",
      "finally, to test the robustness of the focalerrornet, we acquired additional\n",
      "mri-ius patch pairs from the test subjects, by introducing random linear shifts\n",
      "(the max displacement from landmark locations is 10 voxels) from the selected\n",
      "locations in the original set, and evaluated the dl model performance.\n",
      "fig.\n",
      "in addition, the correlations between the\n",
      "predicted and ground truths errors are 0.82 (p < 1e-4) and 0.61 (p < 1e-3) for\n",
      "focalerrornet and 3d cnn, respectively, further conﬁrming the advantage of\n",
      "the proposed technique.\n",
      "these metrics proved the validity of our\n",
      "uncertainty measure and further conﬁrmed the performance of focalerrornet.\n",
      "3.3\n",
      "robustness of the proposed model\n",
      "to examine the performance of our proposed method for image regions that\n",
      "contain fewer potent anatomical features, we acquired additional image pairs\n",
      "from test subjects, according to sect.\n",
      "in this test, patches can contain large areas of zeros (image con-\n",
      "tent out of the scanning fov of the ius).\n",
      "the main reason for the observed\n",
      "performance decline is due to the reduction in suﬃcient image features in ius.\n",
      "however, despite these challenges, we saw an acceptable outcome from focaler-\n",
      "rornet (absolute error = 1.28 mm or ∼1 voxel in clinical mris).\n",
      "696\n",
      "s. salari et al.\n",
      "4\n",
      "discussion\n",
      "in image-guided interventions, there is an urgent need for automatic assessment\n",
      "of image registration quality.\n",
      "first, dissimilar contrasts between\n",
      "images require more elaborate strategies to derive relevant features for error\n",
      "assessment.\n",
      "second, unlike segmentation or classiﬁcation, the ground truths of\n",
      "registration errors are diﬃcult to obtain.\n",
      "to\n",
      "tackle these challenges, we employed 3d focal modulation with depth-wise con-\n",
      "volution to encode contextual information for the image pair.\n",
      "although we admit that residual errors still\n",
      "remain after landmark-based b-spline nonlinear alignment, this approach has\n",
      "been adopted in diﬀerent prior studies, considering the residual landmark reg-\n",
      "istration error is fairly low (mtre of 0.0008 ± 0.0010mm).\n",
      "although simulated\n",
      "ultrasound has been used to provide a perfect alignment with mris, the ﬁdelity\n",
      "of the simulated results is still suboptimal, and this may explain the under-\n",
      "performance of the previous technique in real clinical data [12].\n",
      "to ensure the\n",
      "performance of our focalerrornet, we opted to regress the mean registration\n",
      "error of image patches than simplistic error grades or voxel-wise error maps.\n",
      "we believe that this design choice oﬀers a more stable performance, which is\n",
      "supported by our validation.\n",
      "we adopted uncertainty estimation in inter-modal\n",
      "registration error assessment for the ﬁrst time.\n",
      "furthermore, the use of standard deviation as an uncertainty measurement\n",
      "maintains the same unit as the regressed errors, thus making the interpretation\n",
      "more intuitive.\n",
      "from quantitative and qualitative evaluations using correlation\n",
      "coeﬃcients and scatter plots to assess the association of uncertainty measures\n",
      "with the prediction errors and image entropy, we conﬁrmed the validity of the\n",
      "proposed uncertainty estimation approach.\n",
      "for our focalerrornet, we achieved\n",
      "a prediction error of 0.59 ± 0.57 mm, which is on par with the image resolution\n",
      "(0.5 mm).\n",
      "these signify a robust performance of the focalerrornet.\n",
      "therefore, we created random deformations for\n",
      "patch-wise error estimation, and will further explore data-eﬃcient approaches\n",
      "for registration error assessment.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf:\n",
      "mesoscopic ﬂuorescence lifetime imaging (flim) of tissue\n",
      "ﬂuorophores (i.e., collagen and metabolic co-factors nadh and fad) emission\n",
      "has demonstrated the potential to demarcate the extent of head and neck cancer\n",
      "in patients undergoing surgical procedures of the oral cavity and the orophar-\n",
      "ynx.\n",
      "keywords: tors · positive surgical margin · flim · head and neck oncology\n",
      "supplementary information the online version contains supplementary material available at\n",
      "https://doi.org/10.1007/978-3-031-43996-4_56.\n",
      "achieving clear margins can be challenging in some\n",
      "cases, particularly in tumors with involved deep margins [3, 4].\n",
      "during transoral robotic surgery (tors), surgeons may assess the surgical margin\n",
      "via visual inspection, palpation of the excised specimen and intraoperative frozen sec-\n",
      "tions analysis (ifsa)\n",
      "in the surgical cavity, surgeons visually inspect for residual\n",
      "tumors and use specimen driven or defect-driven frozen section analysis to identify any\n",
      "residual tumor [6, 7].\n",
      "[9] to inspect psms in the excised specimen.\n",
      "while promising, each modality presents certain limitations (e.g., time-consuming anal-\n",
      "ysis, administration of a contrast agent, controlled lighting environment), which has\n",
      "limited their clinical adoption\n",
      "[10, 11].\n",
      "label-free mesoscopic ﬂuorescence lifetime imaging (flim) has been demonstrated\n",
      "as an intraoperative imaging guidance technique with high classiﬁcation performance\n",
      "(auc = 0.94) in identifying in vivo tumor margins at the epithelial surface prior to tumor\n",
      "excision [12].\n",
      "flim can generate optical contrast using autoﬂuorescence derived from\n",
      "tissue ﬂuorophores such as collagen, nadh, and fad.\n",
      "due to the sensitivity of these\n",
      "ﬂuorophores to their microenvironment, the presence of tumor changes their emission\n",
      "properties (i.e., intensity and lifetime characteristics) relative to healthy tissue, thereby\n",
      "enabling the optical detection of cancer\n",
      "each annotated h&e section was registered with the ex vivo and in vivo\n",
      "flim scan images.\n",
      "we implemented\n",
      "the image guidance by augmenting the classiﬁcation map to the surgical view using the\n",
      "predictor output and point locations of the scan.\n",
      "2.1\n",
      "flim hardware and data acquisition\n",
      "this study used a multispectral ﬂuorescence lifetime imaging (flim) device to acquire\n",
      "data [14].\n",
      "390/40 nm attributed to collagen autoﬂuorescence, (2) 470/28 nm to nadh, and (3)\n",
      "542/50 nm to fad.\n",
      "the resulting autoﬂuorescence waveform measurements for each\n",
      "channel are averaged four times, thus with a 480 hz excitation rate, resulting in 120\n",
      "averaged measurements per second\n",
      "the flim device includes a 440 nm continuous wave laser that serves as an aiming\n",
      "beam; this aiming beam enables real-time visualization of the locations where ﬂuores-\n",
      "cence (point measurements) is collected by generating visible blue illumination at the\n",
      "location where data is acquired.\n",
      "segmentation of the ‘aiming beam’ allows for flim\n",
      "data points to be localized as pixel coordinates within a surgical white light image (see\n",
      "fig.\n",
      "an ex vivo flim scan was then performed on the surgically excised specimen.\n",
      "for each patient, the operating surgeon conducted an en bloc surgi-\n",
      "cal tumor resection procedure (achieved by tors-electrocautery instruments), and the\n",
      "resulting excised specimen was sent to a surgical pathology room for grossing.\n",
      "the tissue\n",
      "specimenwasseriallysectionedtogeneratetissueslices,whichwerethenformalin-ﬁxed,\n",
      "parafﬁn-embedded, sectioned, and stained to create hematoxylin & eosin (h&e) slides\n",
      "for pathologist interpretation (see fig. 1).\n",
      "to\n",
      "validate optical measurements to pathology labels (e.g., benign tissue vs. residual tumor),\n",
      "pathology labels from the excision margins were digitally annotated by a pathologist on\n",
      "each h&e section.\n",
      "the aggregate of h&e sections was correspondingly labeled on\n",
      "the ex vivo specimen at the cut lines where the tissue specimen was serially sectioned.\n",
      "this\n",
      "process enables the direct validation of flim measurements to the pathology status of\n",
      "the electrocauterized surgical margins (see table 1).\n",
      "2.3\n",
      "flim preprocessing\n",
      "the raw flim waveform contains background noise, instrument artifacts, and other\n",
      "types of interference, which need to be carefully processed and analyzed to extract\n",
      "meaningful information (i.e., the ﬂuorescence signal decay characteristics).\n",
      "to retrieve the ﬂuorescence\n",
      "function, we used a non-parametric model based on a laguerre expansion polynomi-\n",
      "als and a constrained least-square deconvolution with the instrument impulse response\n",
      "function as previously described [17].\n",
      "due to its robust performance, we chose the generalized one-class\n",
      "discriminative subspaces (gods) classiﬁcation model\n",
      "the gods\n",
      "is a pairwise complimentary classiﬁer deﬁned by two separating hyperplanes to min-\n",
      "imize the distance between the two classiﬁers, limiting the healthy flim data within\n",
      "the smallest volume and maximizing the margin between the hyperplanes and the data,\n",
      "thereby avoiding overﬁtting while improving classiﬁcation robustness.\n",
      "η − max\n",
      "\u0004\n",
      "w t\n",
      "2 xi + b2\n",
      "\u0005\u00062\n",
      "∔\n",
      "(1)\n",
      "where w1, w2 are the orthonormal frames,\n",
      "min\n",
      "w∈sk\n",
      "d ,b\n",
      "is the stiefel manifold, η is the\n",
      "sensitivity margin, and was set η = 0.4 for our experiments.\n",
      "ν denote a penalty factor on\n",
      "these soft constraints, and b is the biases.\n",
      "[21].\n",
      "592\n",
      "m. a. hassan et al.\n",
      "2.5\n",
      "classiﬁer training and evaluation\n",
      "the novelty detection model used for detecting residual cancer is evaluated at the point-\n",
      "measurement level to assess the diagnostic capability of the method over an entire tissue\n",
      "surface.\n",
      "we used grid search to optimize the\n",
      "hyper-parameters and features used in each model and are tabulated in the supplemen-\n",
      "tary section table s1.\n",
      "the sensitivity, speciﬁcity, and accuracy were used as evaluation\n",
      "metrics to assess the performance of classiﬁcation models in the context of the study.\n",
      "results of a binary classiﬁcation model using svm are also shown in the supplementary\n",
      "section table s2.\n",
      "2.6\n",
      "classiﬁer augmented display\n",
      "theclassiﬁer augmentationdepends onthreeindependent processingsteps: aimingbeam\n",
      "localization, motion correction, and interpolation of the point measurements.\n",
      "a detailed\n",
      "description of implementing the augmentation process is discussed in [23].\n",
      "the inter-\n",
      "polation consists of ﬁtting a disk to the segmented aiming beam pixel location for each\n",
      "point measurement and applying a color map (e.g., green: healthy and red: cancer) for\n",
      "each point prediction.\n",
      "individual pixels from overlapping disks are averaged to produce\n",
      "the overall classiﬁcation map and augmented to the surgical ﬁeld as a transparent overlay.\n",
      "3\n",
      "results\n",
      "table 2 tabulates the classiﬁcation performance comparison of novelty detection models\n",
      "for classifying residual cancer vs. healthy on in vivo flim scans in the cavity.\n",
      "the gods reported\n",
      "the best classiﬁcation performance with an average sensitivity of 0.75 ± 0.02 (see fig. 2).\n",
      "the oc-svm\n",
      "and robust covariance reported a high standard deviation, indicating that the performance\n",
      "of the classiﬁcation model is inconsistent across different patients.\n",
      "we also observed that\n",
      "changing the hyper-parameter, such as the anomaly factor, biased the model toward a\n",
      "single class indicating overﬁtting (see supplementary section fig.\n",
      "the gods uses two separating hyperplanes to minimize the distance between the\n",
      "two classiﬁers by learning a low-dimensional subspace containing flim data properties\n",
      "flim-based in vivo classiﬁcation of residual cancer\n",
      "593\n",
      "table 2. classiﬁcation performance comparison of novelty detection models for classifying resid-\n",
      "ual cancer vs. healthy on in vivo flim scans in the cavity, mean (sd).\n",
      "we observed that the gods with the flim decay\n",
      "curves in the cdt space achieve the best classiﬁcation performance compared to other\n",
      "novelty detection models with a mean accuracy of 0.76 ± 0.02.\n",
      "this is mainly due to the\n",
      "robustness of the model, the ability to handle high-dimensional data, and the contrast in\n",
      "the flim decay curves.\n",
      "the proposed model can resolve residual tumor at the point-measurement\n",
      "level over a tissue surface.\n",
      "this enhances surgical precision for tors procedures otherwise limited to\n",
      "visual inspection of the cavity, palpation of the excised specimen, and ifsa.\n",
      "combining the\n",
      "proposed approach and ifsa could lead to an image-guided frozen section analysis to\n",
      "help surgeons achieve negative margins in a more precise manner.\n",
      "the columns represent each patient, and the rows depict the ground truth labels,\n",
      "thepoint-predictionoverlay,andtheaugmentedsurgicalview.fpr-falsepositiverate,fnr-false\n",
      "negative rate.\n",
      "accounted for by the interpolation approach used for the classiﬁer augmentation (refer\n",
      "to supplementary section fig.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf:\n",
      "automatic segmentation of colonoscopic intestinal lesions is\n",
      "essential for early diagnosis and treatment of colorectal cancers.\n",
      "cur-\n",
      "rent deep learning-driven methods still get trapped in inaccurate colono-\n",
      "scopic lesion segmentation due to diverse sizes and irregular shapes of\n",
      "diﬀerent types of polyps and adenomas, noise and artifacts, and illumi-\n",
      "nation variations in colonoscopic video images.\n",
      "this work proposes a new\n",
      "deep learning model called cascade transformer encoded boundary-aware\n",
      "multibranch fusion networks for white-light and narrow-band colorectal\n",
      "lesion segmentation.\n",
      "it further introduces a boundary-aware multibranch fusion mech-\n",
      "anism as a decoder that can enhance blurred lesion edges and extract\n",
      "salient features, and simultaneously suppress image noise and artifacts\n",
      "and illumination changes.\n",
      "such a newly designed encoder-decoder archi-\n",
      "tecture can preserve lesion appearance feature details while aggregating\n",
      "the semantic global cues at several diﬀerent feature levels.\n",
      "addition-\n",
      "ally, a hybrid spatial-frequency loss function is explored to adaptively\n",
      "concentrate on the loss of important frequency components due to the\n",
      "inherent bias of neural networks.\n",
      "we evaluated our method not only on\n",
      "an in-house database with four types of colorectal lesions with diﬀerent\n",
      "pathological features, but also on four public databases, with the exper-\n",
      "imental results showing that our method outperforms state-of-the-art\n",
      "network models.\n",
      "in particular, it can improve the average dice similarity\n",
      "coeﬃcient and intersection over union from (84.3%, 78.4%) to (87.0%,\n",
      "80.5%).\n",
      "intestinal lesions, particularly polyps\n",
      "and adenomas, are usually developed to crc in many years.\n",
      "therefore, diagnosis\n",
      "and treatment of colorectal polyps and adenomas at their early stages are essen-\n",
      "tial to reduce morbidity and mortality of crc.\n",
      "however\n",
      "these lesions in colonoscopic images are easily omitted and wrongly classiﬁed\n",
      "due to limited knowledge and experiences of surgeons.\n",
      "automatic and accurate\n",
      "segmentation is a promising way to improve colorectal examination.\n",
      "many researchers employ u-shaped network\n",
      "[7,13,18] for colonoscopic polyp\n",
      "segmentation.\n",
      "unlike a family of u-net driven segmentation\n",
      "methods, numerous papers have been worked on boundary constraints to seg-\n",
      "ment colorectal polyps.\n",
      "both polyp boundary-aware segmentation\n",
      "methods work well but still introduce much false positive.\n",
      "[6] removed the attention mechanism and replaced\n",
      "res2net50 by hardnet to build hardnet-mseg that can achieve faster segmen-\n",
      "tation.\n",
      "[9] modiﬁed pranet to construct uacanet with\n",
      "parallel axial attention and uncertainty augmented context attention to compute\n",
      "uncertain boundary regions.\n",
      "[10] introduced task-relevant feature replenishment networks for cross-\n",
      "center polyp segmentation, while tian et al.\n",
      "to\n",
      "address these issues mentioned above, we explore a new deep learning archi-\n",
      "tecture called cascade transformer encoded boundary-aware multibranch fusion\n",
      "(ctbmf) networks with cascade transformers and multibranch fusion for polyp\n",
      "and adenoma segmentation in colonoscopic white-light and narrow-band video\n",
      "images.\n",
      "first,\n",
      "we construct cascade transformers that can extract global semantic and subtle\n",
      "boundary features at diﬀerent resolutions and establish weighted links between\n",
      "global semantic cues and local spatial ones for intermediate reasoning, providing\n",
      "long-range dependencies and a global receptive ﬁeld for pixel-level segmentation.\n",
      ", we built a new colonoscopic lesion image database and will make it publicly\n",
      "available, while this work also conducts a thorough evaluation and comparison\n",
      "on our new database and four publicly available ones (fig. 2).\n",
      "fig.\n",
      "let x0 and xi be input patches and the feature map at stage i, respec-\n",
      "tively.\n",
      "overlapping patch embedding (ope) separates an image into ﬁxed-size\n",
      "patches and linearly embeds them into tokenized images while making adja-\n",
      "cent windows overlap by half of a patch.\n",
      "either key ki or value vi is the input\n",
      "cascade transformer encoded boundary-aware multibranch fusion\n",
      "721\n",
      "sequence of linear spatial reduction (lsr) that implements layer normalization\n",
      "(ln) and average pooling (ap) to reduce the input dimension:\n",
      "lsr(ki) = ap(reshape(ln(ki ⊕ ω(ki), ri)wki)\n",
      "(1)\n",
      "where ω(·) denotes the output parameters of position embedding, ⊕ is the\n",
      "element-wise addition, wki indicates the parameters that reduces the dimen-\n",
      "sion of ki or vi, and ri is the reduction ratio of the attention layers at stage i.\n",
      "as the output of lsr is fed into multihead attention, we can obtain attention\n",
      "feature map aj\n",
      "i from head j (j = 1, 2, · · · , n, n is the head number of the\n",
      "attention layer) at stage i:\n",
      "aj\n",
      "i = attention(qw j\n",
      "qi, lsr(ki)w j\n",
      "ki, lsr(vi)w j\n",
      "vi)\n",
      "(2)\n",
      "where attention(·) is calculated as the original transformer [14].\n",
      "eventually, the output feature map xi of the pyramid\n",
      "transformer at stage i can be represented by\n",
      "xi = reshape(ψ ⊕ cff(qi, ki, vi))\n",
      "(6)\n",
      "2.2\n",
      "boundary-aware multibranch fusion decoding\n",
      "boundary-aware attention module.\n",
      "given the feature map xi with semantic cues and rough appearance details,\n",
      "we perform convolution (conv) on it and obtain ˜xi = conv(xi), which is further\n",
      "augmented by channel and spatial attentions.\n",
      "(7)\n",
      "722\n",
      "a. wang et al.\n",
      "where ⊗ indicates the elementwise product.\n",
      "we subtract the feature map ˜xi from\n",
      "the enhanced map zi to obtain the augmented boundary attention map bi,\n",
      "and also establish the correlation between the neighbor layers xi+1 and xi to\n",
      "generate multilevel boundary map gi:\n",
      "bi = zi ⊖ ˜xi, i = 1, 2, 3, 4\n",
      "gi = ˜xi ⊖ us( ˜xi+1), i = 1, 2, 3\n",
      "(9)\n",
      "where ⊖ and us indicate subtraction and upsampling.\n",
      "we obtain the fused feature representation map mi (i = 1, 2, 3, 4) from the\n",
      "elementwise addition or summation of mi+1, di, and the residual feature ˜xi by\n",
      "mi = conv( ˜xi ⊕ di ⊕ us(mi+1)), i = 1, 2, 3\n",
      "m4 = conv(b4)\n",
      "(11)\n",
      "eventually, the output m1 of the boundary-aware multibranch fusion decoder\n",
      "is represented by the following equation:\n",
      "m1 = conv( ˜x1 ⊕ d1 ⊕ us(m2))\n",
      "(12)\n",
      "which precisely combines global semantic features with boundary or appearance\n",
      "details of colorectal lesions.\n",
      "2.3\n",
      "hybrid spatial-frequency loss\n",
      "this work proposes a hybrid spatial-frequency loss function hl to train our\n",
      "network architecture for colorectal polyp and adenoma segmentation:\n",
      "the frequency-domain loss fl can be computed by [8]\n",
      "fl = λ\n",
      "1\n",
      "wh\n",
      "w −1\n",
      "\u0002\n",
      "u=0\n",
      "h−1\n",
      "\u0002\n",
      "v=0\n",
      "γ(u, v)|g(u, v) − p(u, v)|2\n",
      "(14)\n",
      "cascade transformer encoded boundary-aware multibranch fusion\n",
      "723\n",
      "where w ×h is the image size, λ is the coeﬃcient of fl, g(u, v) and p(u, v) are\n",
      "a frequency representation of ground truth g and prediction p using 2-d dis-\n",
      "crete fourier transform.\n",
      "3\n",
      "experiments\n",
      "our clinical in-house colonoscopic videos were acquired from various colonoscopic\n",
      "procedures under a protocol approved by the research ethics committee of the\n",
      "university.\n",
      "these white-light and narrow-band colonoscopic images contain four\n",
      "types of colorectal lesions with diﬀerent pathological features classiﬁed by sur-\n",
      "geons: (1) 268 cases of hyperplastic polyp, (2) 815 cases of inﬂammatory polyp,\n",
      "(3) 1363 cases of tubular adenoma, and (4) 143 cases of tubulovillous adenoma.\n",
      "we implemented ctbmf on pytorch and trained it with a single nvidia\n",
      "rtx3090 to accelerate the calculations for 100 epochs at mini-batch size 16.\n",
      "factors λ (eq.\n",
      "3. visual comparison of the segmentation results of using the four diﬀerent meth-\n",
      "ods tested on those in-house and public datasets.green and blue show ground truth\n",
      "and prediction.\n",
      "(color ﬁgure online)\n",
      "table 1. results and computational time of using ﬁve databases(our in-house and four\n",
      "public databases)\n",
      "average\n",
      "dsc\n",
      "iou\n",
      "fβ\n",
      "in-house\n",
      "public\n",
      "average\n",
      "pranet\n",
      "the momentum and weight decay were\n",
      "set as 0.9 and 0.0005.\n",
      "further, we resized input images to 352 × 352 for training\n",
      "and testing and the training time was nearly 1.5 h to achieve the convergence.\n",
      "we employ three metrics to evaluate the segmentation: dice similarity coeﬃcient\n",
      "(dsc), intersection over union (iou), and weighted f-measure (fβ).\n",
      "fig.\n",
      "public data segmented results of our ablation study\n",
      "modules dsc\n",
      "iou\n",
      "fβ\n",
      "d1\n",
      "0.681 0.593 0.634\n",
      "d2\n",
      "0.822 0.753 0.798\n",
      "d3\n",
      "0.820 0.748 0.793\n",
      "residual 0.828 0.757 0.802\n",
      "fl\n",
      "0.834 0.764 0.804\n",
      "4\n",
      "results and discussion\n",
      "figure 3 visually compares the segmentation results of the four methods tested on\n",
      "our in-house and public databases.\n",
      "our method can accurately segment polyps\n",
      "in white-light and narrow-band colonoscopic images under various scenarios,\n",
      "and ctbmf can successfully extract small, textureless and weak boundary and\n",
      "colorectal lesions.\n",
      "the segmented boundaries of our method are sharper and\n",
      "clear than others especially in textureless lesions that resemble intestinal lining.\n",
      "cascade transformer encoded boundary-aware multibranch fusion\n",
      "725\n",
      "figure 4 shows the dsc-boxplots to evaluate the quality of segmented polyps and\n",
      "adenomas, which still demonstrate that our method works much better than the\n",
      "others.\n",
      "furthermore, we also summarizes the average three\n",
      "metrics computed from all the ﬁve databases (the in-house dataset and four\n",
      "public datasets).\n",
      "our method attains much higher average dsc and iou of\n",
      "(0.870, 0.805) than the others on the ﬁve databases.\n",
      "fig.\n",
      "each module\n",
      "can improve the segmentation performance.\n",
      "particularly, the boundary-aware\n",
      "attention module critically improves the average dsc, iou, and fβ.\n",
      "first, the cascade-transformer encoder can extract local\n",
      "and global semantic features of colorectal lesions with diﬀerent pathological\n",
      "characteristics due to its pyramid representation and linear spatial reduction\n",
      "attention.\n",
      "while the pyramid operation extracts multiscale local features, the\n",
      "attention mechanism builds global semantic cues.\n",
      "additionally, the hybrid spatial-frequency loss was also contributed to the\n",
      "improvement of colorectal lesion segmentation.\n",
      "5\n",
      "conclusion\n",
      "this work proposes a new deep learning model of cascade pyramid transformer\n",
      "encoded boundary-aware multibranch fusion networks to automatically segment\n",
      "diﬀerent colorectal lesions of polyps and adenomas in colonoscopic imaging.\n",
      "while such an architecture employs simple and convolution-free cascade trans-\n",
      "formers as an encoder to eﬀectively and accurately extract global semantic fea-\n",
      "tures, it introduces a boundary-aware attention multibranch fusion module as a\n",
      "decoder to preserve local and global features and enhance structural and bound-\n",
      "ary information of polyps and adenomas, as well as it uses a hybrid spatial-\n",
      "frequency loss function for training.\n",
      "the thorough experimental results show\n",
      "that our method outperforms the current segmentation models without any pre-\n",
      "processing.\n",
      "in particular, our method attains much higher accuracy on colono-\n",
      "scopic images with small, illumination changes, weak-boundary, textureless, and\n",
      "motion blurring lesions, improving the average dice similarity coeﬃcient and\n",
      "intersection over union from (89.5%, 84.1%) to (90.3%, 84.4%) on our in-house\n",
      "database, from (78.9%, 72.6%) to (83.4%, 76.5%) on the four public databases,\n",
      "and from (84.3%, 78.4%) to (87.0%, 80.5%) on the ﬁve databases.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_50.pdf:\n",
      "deep learning-based image segmentation for radiotherapy is\n",
      "intended to speed up the planning process and yield consistent results.\n",
      "however, most of these segmentation methods solely rely on distribution\n",
      "and geometry-associated training objectives without considering tumor\n",
      "control and the sparing of healthy tissues.\n",
      "to incorporate dosimetric\n",
      "eﬀects into segmentation models, we propose a new training loss func-\n",
      "tion that extends current state-of-the-art segmentation model training\n",
      "via a dose-based guidance method.\n",
      "we hypothesized that adding such a\n",
      "dose-guidance mechanism improves the robustness of the segmentation\n",
      "with respect to the dose (i.e., resolves distant outliers and focuses on loca-\n",
      "tions of high dose/dose gradient).\n",
      "we demonstrate the eﬀectiveness of\n",
      "the proposed method on gross tumor volume segmentation for glioblas-\n",
      "toma treatment.\n",
      "the obtained dosimetry-based results show reduced\n",
      "dose errors relative to the ground truth dose map using the proposed\n",
      "dosimetry-segmentation guidance, outperforming state-of-the-art distri-\n",
      "bution and geometry-based segmentation losses.\n",
      "keywords: segmentation · radiotherapy · dose guidance · deep\n",
      "learning\n",
      "1\n",
      "introduction\n",
      "radiotherapy (rt) has proven eﬀective and eﬃcient in treating cancer patients.\n",
      "however, its application depends on treatment planning involving target lesion\n",
      "and radiosensitive organs-at-risk (oar) segmentation.\n",
      "hence, this manual segmentation step is very time-consuming and must\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43996-4 50.\n",
      "studies\n",
      "have shown that the manual segmentation task accounts for over 40% of the\n",
      "treatment planning duration\n",
      "hence, deep learning-based (dl) segmenta-\n",
      "tion is essential for reducing time-to-treatment, yielding more consistent results,\n",
      "and ensuring resource-eﬃcient clinical workﬂows.\n",
      "nowadays, training of dl segmentation models is predominantly based on\n",
      "loss functions deﬁned by geometry-based (e.g., softdice loss [15]), distribution-\n",
      "based objectives (e.g., cross-entropy), or a combination thereof [13].\n",
      "speciﬁcally, the\n",
      "dice loss, allegedly the most popular segmentation loss function, has been shown\n",
      "to have a tendency to yield overconﬁdent trained models and lack robustness\n",
      "in out-of-distribution scenarios\n",
      "in the ﬁeld of rt planning for brain tumor patients, the recent study\n",
      "of [17] shows that current dl-based segmentation algorithms for target struc-\n",
      "tures carry a signiﬁcant chance of producing false positive outliers, which can\n",
      "have a considerable negative eﬀect on applied radiation dose, and ultimately,\n",
      "they may impact treatment eﬀectiveness.\n",
      "therefore, we postulate that training dl-based\n",
      "segmentation models for rt planning should consider this clinical objective.\n",
      "in this paper, we propose an end-to-end training loss function for dl-based\n",
      "segmentation models that considers dosimetric eﬀects as a clinically-driven learn-\n",
      "ing objective.\n",
      "our contributions are: (i) a dosimetry-aware training loss function\n",
      "for dl segmentation models, which (ii) yields improved model robustness, and\n",
      "(iii) leads to improved and safer dosimetry maps.\n",
      "in\n",
      "addition, we report results comparing the proposed loss function, called dose-\n",
      "segmentation loss (doselo), with models trained with a combination of\n",
      "binary cross-entropy (bce) and softdice loss functions.\n",
      "a segmentation\n",
      "model (u-net\n",
      "[20]) is trained to output target segmentation predictions for the\n",
      "gross tumor volume (gtv) based on patient mri sequences.\n",
      "predicted segmen-\n",
      "tations and their corresponding ground-truth (gt) are fed into a dose predictor\n",
      "model, which outputs corresponding dose predictions (denoted as \u0002dp and dp in\n",
      "fig.\n",
      "a pixel-wise mean squared error between both dose predictions is then\n",
      "dose guidance for radiotherapy-oriented deep learning segmentation\n",
      "527\n",
      "fig.\n",
      "a segmentation model (u-net\n",
      "[20]) is trained to output target segmentation predic-\n",
      "tions (\u0002st ) for the gross tumor volume (gtv) based on patient mri sequences imr.\n",
      "predicted (\u0002st ) and ground-truth segmentations (st ) are fed into the dose predictor\n",
      "model along with the ct-image (ict ), and oar segmentation (sor).\n",
      "2.1\n",
      "deep learning-based dose prediction\n",
      "recent dl methods based on cascaded u-nets have demonstrated the feasibility\n",
      "of generating accurate dose distribution predictions from segmentation masks,\n",
      "approximating analytical dose maps generated by rt treatment planning sys-\n",
      "tems\n",
      "this good level of\n",
      "performance, along with its ability to yield near-instant dose predictions, enables\n",
      "us to create a training pipeline that guides learned features to be dose-aware.\n",
      "following [12], the dose predictor model consists of a cascaded u-net (i.e., the\n",
      "input to the second u-net is the output of the ﬁrst concatenated with the input\n",
      "to the ﬁrst u-net) trained on segmentation masks, ct images, and reference\n",
      "dose maps.\n",
      "the model’s input is a normalized ct volume and segmentation\n",
      "masks for target volume and oars.\n",
      "as output, it predicts a continuous-valued\n",
      "dose map of the same dimension as the input.\n",
      "we refer the reader to [9,12] for further implementation\n",
      "details.\n",
      "we remark that the dose predictor model was also trained with data\n",
      "augmentation, so imperfect segmentation masks and corresponding dose plans\n",
      "are included.\n",
      "this allows us in this study to use the dose predictor to model the\n",
      "interplay between segmentation variability and dosimetric changes.\n",
      "formally, the dose prediction model md receives as inputs: segmentations\n",
      "masks for the gtv st ∈ zw ×h and the oars sor ∈ zw ×h, the ct image\n",
      "(used for tissue attenuation calculation purposes in rt) ict ∈ rw ×h, and\n",
      "outputs md(st , sor, ict ) \u0003→\n",
      "2.2\n",
      "dose segmentation loss (doselo)\n",
      "during the training of the segmentation model, we used the dose predictor model\n",
      "to generate pairs of dose predictions for the model-generated segmentations and\n",
      "the gt segmentations.\n",
      "the diﬀerence between these two predicted dose maps is\n",
      "used to guide the segmentation model.\n",
      "the intuition behind this is to guide the\n",
      "segmentation model to yield segmentation results being dosimetrically consistent\n",
      "with the dose maps generated via the corresponding gt segmentations.\n",
      "formally, given a set of n pairs of labeled training images {(imr, sp )i : 1 ≤\n",
      "i ≤ n}, imr ∈ rd (with d : {t1, t1c, t2, flair} mri clinical sequences),\n",
      "and corresponding gt segmentations of the gtv st ∈ zh×w , a dl segmen-\n",
      "tation model ms(imr) \u0003→ \u0002st is commonly updated by minimizing a standard\n",
      "loss term, such as the bce loss (lbce).\n",
      "to guide the training process with dosimetry information stemming from seg-\n",
      "mentation variations, we propose to use the mean squared error (mse) between\n",
      "dose predictions for the gt segmentation (st ) and the predicted segmentation\n",
      "(\u0002st ), and construct the following dose-segmentation loss,\n",
      "ldsl =\n",
      "1\n",
      "h × w\n",
      "h×w\n",
      "\u0003\n",
      "i\n",
      "(di\n",
      "p − \u0002di\n",
      "p )2\n",
      "(1)\n",
      "dp = md(st , sor, ict )\n",
      "the ﬁnal loss is then,\n",
      "ltotal = lbce + λldsl,\n",
      "(4)\n",
      "dose guidance for radiotherapy-oriented deep learning segmentation\n",
      "529\n",
      "where λ is a hyperparameter to weigh the contributions of each loss term.\n",
      "we\n",
      "remark that during training we use standard data augmentations including spa-\n",
      "tial transformations, which are also subjected to dose predictions, so the model is\n",
      "informed about relevant segmentation variations producing dosimetry changes.\n",
      "3\n",
      "experiments and results\n",
      "3.1\n",
      "data and model training\n",
      "we divide the descriptions of the two separate datasets used for the dose pre-\n",
      "diction and segmentation models.\n",
      "this includes ct imaging data, segmentation masks of 13 oars, and the gtv.\n",
      "gtvs were deﬁned according to the estro-acrop guidelines [16].\n",
      "we refer the reader to [9] for further\n",
      "details.\n",
      "segmentation models:\n",
      "to develop and test the proposed approach, we\n",
      "employed a separate in-house dataset (i.e., diﬀerent cases than those used to\n",
      "train the dose predictor model) of 50 cases from post-operative gmb patients\n",
      "receiving standard rt treatment.\n",
      "all cases comprise a planning ct\n",
      "registered to the standard mri images (t1-post-contrast (gd), t1-weighted,\n",
      "t2-weighted, flair), and gt segmentations containing oars as well as the\n",
      "gtv.\n",
      "we note that for this ﬁrst study, we decided to keep the dose prediction\n",
      "model ﬁxed during the training of the segmentation model for a simpler presen-\n",
      "tation of the concept and modular pipeline.\n",
      "hence, only the parameters of the\n",
      "segmentation model are updated.\n",
      "baselines and implementation details: we employed the same u-net\n",
      "[20]\n",
      "architecture for all trained segmentation models, with the same training param-\n",
      "eters but two diﬀerent loss functions, to allow for a fair comparison.\n",
      "as a strong\n",
      "comparison baseline, we used a combo-loss formed by bce plus softdice, which\n",
      "is also used by nnunet and recommended by its authors [8].\n",
      "our method1 was implemented in pytorch 1.13 using\n",
      "adam optimizer [10] with β1 = 0.9, β2 = 0.999, batch normalization, dropout\n",
      "set at 0.2, learning rate set at 10−4, 2 · 104 update iterations, and a batch size of\n",
      "16.\n",
      "the input image size is 256 × 256 pixels with an isotropic spacing\n",
      "of 1 mm.\n",
      "3.2\n",
      "evaluation\n",
      "to evaluate the proposed doselo, we computed dose maps for each test case\n",
      "using a standardized clinical protocol with eclipse (varian medical systems inc.,\n",
      "palo alto, usa).\n",
      "we calculated dose maps for segmentations using the state-\n",
      "of-the-art bce+softdice and the proposed doselo.\n",
      "[12], which is the mean absolute error between\n",
      "the reference dose map (dst ) and the dose map derived from the corresponding\n",
      "segmentation result (d\u0002st , where \u0002st ∈ {bce+softdice, doselo}), and set it\n",
      "relative to the reference dose map (dst ) (see eq. 5).\n",
      "rmae =\n",
      "1\n",
      "h × w\n",
      "h×w\n",
      "\u0003\n",
      "i\n",
      "|dst − d \u0003\n",
      "st |\n",
      "dst\n",
      "(5)\n",
      "although it has been shown that geometric-based segmentation metrics\n",
      "poorly correlate with the clinical end-goal in rt\n",
      "[4,11,18,23], we report in sup-\n",
      "plementary material dice and hausdorﬀ summary statistics as well (supplemen-\n",
      "tary table 3).\n",
      "this signiﬁcant dose error reduc-\n",
      "tion shows the ability of the proposed approach to yield segmentation results in\n",
      "better agreement with dose maps obtained using gt segmentations than those\n",
      "obtained using the state-of-the-art bce+softdice combo-loss.\n",
      "table 1 shows results for the ﬁrst and most signiﬁcant four cases from a\n",
      "rt point of view (due to space limitations, all other cases are shown in sup-\n",
      "plementary material).\n",
      "dose guidance for radiotherapy-oriented deep learning segmentation\n",
      "531\n",
      "fig.\n",
      "2. relative mean absolute dose errors/diﬀerences (rmae) between the reference\n",
      "dose map and dose maps obtained using the predicted segmentations.\n",
      "across all tested cases and folds we observe a large rmae reduction for dose maps\n",
      "using the proposed doselo (average rmae reduction of 42.5%).\n",
      "maps.\n",
      "for case no. 3\n",
      "the tumor presents a non-convex shape alongside the skull’s parietal lobe, which\n",
      "was not adequately modeled by the training dataset used to train the segmen-\n",
      "tation models.\n",
      "indeed, we remark that both models failed to yield acceptable\n",
      "segmentation quality in this area.\n",
      "in case no. 4, both models failed to segment the\n",
      "diﬀuse tumor area alongside the skull; however, as shown in fig.\n",
      "case no. 5 (shown in supple-\n",
      "mentary material) is an interesting case called butterﬂy gbm, which is a rare\n",
      "type of gbm (around 2% of all gbm cases [3]), characterized by bihemispheric\n",
      "involvement and invasion of the corpus callosum.\n",
      "although we are aware that classical segmentation metrics poorly correlate\n",
      "with dosimetric eﬀects [18], we report that the proposed method is more robust\n",
      "than the baseline bce+softdice loss function, which yields outliers with haus-\n",
      "dorﬀ distances: 64.06 ± 29.84 mm vs 28.68 ± 22.25 mm (–55.2% reduction) for\n",
      "the proposed approach.\n",
      "as pointed out by [17], segmentation outliers can have a\n",
      "detrimental eﬀect on rt planning.\n",
      "it can be seen that\n",
      "doselo yields improved dose maps, which are in better agreement with the reference\n",
      "dose maps (dose map color scale: 0 (blue) - 70gy (red)).\n",
      "case\n",
      "input image\n",
      "dose simulation\n",
      "reference\n",
      "|ref.-(bce+sd)| |ref. -\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf:\n",
      "imitation learning has shown its eﬃcacy in\n",
      "learning skills from expert demonstrations, but it faces challenges in pre-\n",
      "dicting uncertain future movements and generalizing to various surgical\n",
      "scenes.\n",
      "experimental results demonstrate that our solution outperforms sota\n",
      "imitation learning methods on our formulated task.\n",
      "imitation learning has been widely studied in various domains [11,16,18]\n",
      "with its good ability to learn complex skills, but it still needs adaptation and\n",
      "improvement when being applied to learn dissection trajectory from surgical\n",
      "data.\n",
      "supervised learning such as behavior cloning (bc) [3] tends to average all pos-\n",
      "sible prediction paths, which leads to inaccurate predictions.\n",
      "in addition, the model performance can be sensitive to data distribution\n",
      "and the noise in training data would result in unstable trajectory predictions.\n",
      "to eﬀectively model the surgeon’s\n",
      "behaviors and handle the large variation of surgical scenes, we leverage implicit\n",
      "modeling to express expert dissection skills.\n",
      "to address the limitations of inef-\n",
      "ﬁcient training and unstable performance associated with ebm-based implicit\n",
      "policies, we formulate the implicit policy using an unconditional diﬀusion model,\n",
      "which demonstrates remarkable ability in representing complex high-dimensional\n",
      "data distribution for videos.\n",
      "for experi-\n",
      "mental evaluation, we collected a surgical video dataset of esd procedures, and\n",
      "preprocessed 1032 short clips with dissection trajectories labelled.\n",
      "results show\n",
      "that our method achieves superior performances in diﬀerent contexts of surgical\n",
      "scenarios compared with representative popular imitation learning methods.\n",
      ", it},\n",
      "it ∈ rh×w ×3 and the output is an action distribution of a sequence of 2d\n",
      "coordinates a = {yt+1, yt+2, ..., yt+n}, yt ∈ r2 indicating the future dissection\n",
      "trajectory projected to the image space.\n",
      "in order to obtain the demonstrated dissection trajectories from the expert\n",
      "video data, we ﬁrst manually annotate the dissection trajectories on the video\n",
      "frame according to the moving trend of the instruments observed from future\n",
      "frames, then create a dataset d = {(s, a)i}m\n",
      "i=0 containing m pairs of video clip\n",
      "(state) and dissection trajectory (action).\n",
      "by rep-\n",
      "resenting the data using a continuous thermodynamics diﬀusion process, which\n",
      "can be discretized into a series of gaussian transitions, the diﬀusion model is\n",
      "498\n",
      "j. li et al.\n",
      "able to express complex high-dimensional distribution with simple parameter-\n",
      "ized functions.\n",
      "in addition, the diﬀusion process also serves as a form of data\n",
      "augmentation by adding a range of levels of noise to the data, which guarantees\n",
      "a better generalization in high-dimensional state space.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "experimental dataset and evaluation metrics\n",
      "dataset.\n",
      "the input state is a\n",
      "1.5-s length video clip containing 3 consecutive frames, and the expert dissection\n",
      "trajectory is represented by a 6-point polyline indicating the tool’s movements\n",
      "in future 3 s. we totally annotated 1032 video clips, which contain 3 frames for\n",
      "each clip.\n",
      "experiment setup.\n",
      "to evaluate the performance of the proposed approach,\n",
      "we adopt several metrics, including commonly used evaluation metrics for trajec-\n",
      "tory prediction as used in [23,26], including average displacement error (ade),\n",
      "the lower is the better.\n",
      "method in-the-context\n",
      "out-of-the-context\n",
      "ade\n",
      "fde\n",
      "fd\n",
      "ade\n",
      "fde\n",
      "fd\n",
      "bc\n",
      "10.43 (±5.52) 16.68\n",
      "(±10.59) 24.92\n",
      "(±14.59) 13.67\n",
      "(±6.43) 17.03\n",
      "(±12.50) 29.23\n",
      "(±16.56)\n",
      "ibc[5]\n",
      "15.54 (±4.79) 22.66\n",
      "(±8.06)\n",
      "35.26\n",
      "(±11.78) 15.81\n",
      "(±4.66) 19.66\n",
      "(±7.56)\n",
      "31.66\n",
      "(±9.14)\n",
      "mid[8]\n",
      "9.90\n",
      "(±0.66) 15.26\n",
      "(±1.35)\n",
      "23.78\n",
      "(±1.73)\n",
      "12.42\n",
      "(±1.89) 16.32\n",
      "(±3.11)\n",
      "27.04\n",
      "(±4.79)\n",
      "ours\n",
      "9.47\n",
      "(±1.66) 13.85 (±2.01)\n",
      "21.43 (±3.89)\n",
      "10.21 (±3.17) 14.14 (±3.63)\n",
      "21.56 (±5.97)\n",
      "which respectively reports the overall deviations between the predictions and the\n",
      "ground truths, and final displacement error (fde) describing the diﬀerence\n",
      "from the moving target by computing the l2 distance between the last trajec-\n",
      "tory points.\n",
      "pixel errors are used\n",
      "as units for all metrics, while the input images are in 128 × 128 resolution.\n",
      "3.2\n",
      "comparison with state-of-the-art methods\n",
      "to evaluate the proposed approach, we have selected popular baselines and state-\n",
      "of-the-art methods for comparison.\n",
      "we have chosen the fully supervised method,\n",
      "behavior cloning, as the baseline, which is implemented using a cnn-mlp\n",
      "network.\n",
      "[5], the performance did\n",
      "not meet our expectations and was even surpassed by the baseline.\n",
      "according to the bar charts in fig. 3, the explicit diﬀusion policy\n",
      "shows a performance drop for both evaluation sets on ade compared with the\n",
      "implicit form.\n",
      "3, the implicit diﬀusion policy\n",
      "beneﬁts more from the forward-diﬀusion guidance in the “in-the-context” scenes,\n",
      "achieving an improvement of 0.33 on ade.\n",
      "when encountered with the unseen\n",
      "scenarios in “out-of-the-context” data, the performance improvement of such\n",
      "inference strategy is marginal.\n",
      "value of synthetic data.\n",
      "3 shows the synthetic data is useful as the augmented data for\n",
      "downstream task learning.\n",
      "3. left: ablation study of key method components; middle: visualization of\n",
      "reverse processes of unconditional/conditional sampling from implicit policy; right:\n",
      "performance of bc trained with synthetic data v.s. our method on ade.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf:\n",
      "image-guided surgery requires fast and accurate registration to align\n",
      "preoperative imaging and surgical spaces.\n",
      "rigid registration fails to account for nonrigid soft\n",
      "tissue deformations, and biomechanical modeling approaches like ﬁnite element\n",
      "simulations can be cumbersome in implementation and computation.\n",
      "computing a displacement ﬁeld using this method does not\n",
      "require mesh discretization or large matrix assembly and inversion conventionally\n",
      "associated with ﬁnite element or mesh-free methods.\n",
      "we solve for the optimal\n",
      "superposition of regularized kelvinlet functions that achieves registration of the\n",
      "medical image to simulated intraoperative geometric point data of the breast.\n",
      "we\n",
      "present registration performance results using a dataset of supine mr breast imag-\n",
      "ing from healthy volunteers mimicking surgical deformations with 237 individual\n",
      "targets from 11 breasts.\n",
      "to demonstrate application, we perform\n",
      "registration on a breast cancer patient case with a segmented tumor and compare\n",
      "performance to other image-to-physical and image-to-image registration methods.\n",
      "we show comparable accuracy to a previously proposed image-to-physical reg-\n",
      "istration method with improved computation time, making regularized kelvinlet\n",
      "functions an attractive approach for image-to-physical registration problems.\n",
      "keywords: deformation · registration · elasticity · image-guidance · breast ·\n",
      "ﬁnite element · kelvinlet\n",
      "© the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14228, pp.\n",
      "https://doi.org/10.1007/978-3-031-43996-4_33\n",
      "regularized kelvinlet functions to model linear elasticity\n",
      "345\n",
      "1\n",
      "introduction\n",
      "image-to-physical registration is a necessary process for computer-assisted surgery to\n",
      "align preoperative imaging to the intraoperative physical space of the patient to in-form\n",
      "surgical decision making.\n",
      "most intraoperatively utilized image-to-physical regis-trations\n",
      "are rigid transformations calculated using ﬁducial landmarks [1].\n",
      "this has made image-guided surgery\n",
      "more tractable for soft tissue organ systems like the liver, prostate, and breast\n",
      "image-to-physical registration methods that accurately model an elastic\n",
      "soft-tissue environment while also complying with intraoperative data constraints is an\n",
      "active ﬁeld of research.\n",
      "determining correspondences between imaging space and geo-\n",
      "metric data is required for image-to-physical registration, but it is often an inexact and\n",
      "ill-posed problem.\n",
      "deep learning image registra-\n",
      "tion methods like voxelmorph have also been used for this purpose [10].\n",
      "other\n",
      "non-learning image-to-physical registration strategies include [11] which utilized a coro-\n",
      "tational linear-elastic ﬁnite element method (fem) combined with an iterative closest\n",
      "point algorithm.\n",
      "similarly, the registration method introduced in [12] iteratively updated\n",
      "the image-to-physical correspondence between surface point clouds while solving for\n",
      "an optimal deformation state.\n",
      "both [11] and [12] leverage fem, which uses a 3d mesh to solve for\n",
      "unique deformation solutions.\n",
      "the element-free galerkin method is a mesh-free method that requires\n",
      "only nodal point data and uses a moving least-squares approximation to solve for a\n",
      "solution [13].\n",
      "in this work, we propose an image-to-physical registration method that uses regu-\n",
      "larized kelvinlet functions as a novel deformation basis for nonrigid registration.\n",
      "finally, our approach is validated on an exem-\n",
      "plar breast cancer case with a segmented tumor by comparing performance to previously\n",
      "proposed registration methods.\n",
      "(1), where e is young’s modulus, ν is poisson’s ratio, u(x)\n",
      "is the displacement vector, and f(x) is the forcing function.\n",
      "analytical displacement\n",
      "solutions to eq.\n",
      "the closed-form displacement solution for eq.\n",
      "(3)\n",
      "regularized kelvinlet functions to model linear elasticity\n",
      "347\n",
      "becomes numerically problematic in discretized problems because the displacement\n",
      "and displacement gradient become indeﬁnite as x approaches x0.\n",
      "= f\n",
      "\u0002\n",
      "15ε4\n",
      "8π\n",
      "1\n",
      "r7ε\n",
      "\u0003\n",
      "(4)\n",
      "uε,grab(r) =\n",
      "\u0002\n",
      "a−b\n",
      "rε i + b\n",
      "r3ε rrt + a\n",
      "2\n",
      "ε2\n",
      "r3ε i\n",
      "\u0003\n",
      "f = kgrab(r)f\n",
      "(5)\n",
      "the second type of regularized kelvinlet functions represent “twist” deformations\n",
      "which are derived by expanding the previous formulation to accommodate locally afﬁne\n",
      "loads instead of displacement point sources.\n",
      "the pure twist displacement ﬁeld\n",
      "response uε,twist(r) to the forcing matrix in eq.\n",
      "then, the\n",
      "348\n",
      "m. ringel et al.\n",
      "f grab and f twist vectors are optimized to solve for a displacement ﬁeld that minimizes\n",
      "distance error between geometric data inputs.\n",
      "for a predetermined conﬁguration of regularized kelvinlet “grab” and “twist” func-\n",
      "tions centered at different x0 control point locations, an elastically deformed state can\n",
      "be represented as the summation of all regularized kelvinlet displacement ﬁelds where\n",
      "∼u (x) is the superposed displacement vector and k = kgrab + ktwist in eq.\n",
      "an objective function is formulated to minimize misalignment between the moving\n",
      "space xmoving and ﬁxed space xﬁxed through geometric data constraints.\n",
      "for the breast\n",
      "imaging datasets in this work, we used simulated intraoperative data features that real-\n",
      "istically could be collected in a surgical environment visualized in fig.\n",
      "these data feature designations are consistent with\n",
      "implementations in previous work [16, 17].\n",
      "for a given deformation state, each data feature contributes to the total error measure.\n",
      "ese is the average strain energy density within the breast geometry,\n",
      "and it is computed for each β at every iteration.\n",
      "the optimal\n",
      "state β is iteratively solved using levenberg-marquardt optimization terminating at\n",
      "|(β)|<10–12.\n",
      "\b(β) =\n",
      "1\n",
      "npoint\n",
      "npoint\n",
      "\u0006\n",
      "i=1\n",
      "(ei\n",
      "point)2 +\n",
      "1\n",
      "nsurface\n",
      "nsurface\n",
      "\u0006\n",
      "i=1\n",
      "(ei\n",
      "surface)2 + wse(ese)2\n",
      "(11)\n",
      "3\n",
      "experiments and results\n",
      "in this section, two experiments are conducted.\n",
      "the second\n",
      "validates the registration method in a breast cancer patient and compares registration\n",
      "accuracy and computation time to previously proposed methods.\n",
      "3.1\n",
      "hyperparameters sensitivity analysis\n",
      "this dataset consists of supine breast mr images simulating surgical deformations of\n",
      "11 breasts from 7 healthy volunteers.\n",
      "volunteers (ages 23–57) were enrolled in a study\n",
      "approved by the institutional review board at vanderbilt university.\n",
      "mr images (0.391 × 0.391 ×\n",
      "1 mm3 or 0.357 × 0.357 × 1 mm3) were acquired with the volunteers’ arms placed by\n",
      "their sides.\n",
      "this image was used as the xmoving space.\n",
      "a second\n",
      "mr image in the deformed state was acquired to create simulated intraoperative physical\n",
      "data and to use for validation.\n",
      "this second image was used as the xﬁxed space.\n",
      "the breast in xmoving was segmented at the boundary between the chest wall and\n",
      "breast parenchyma to create a 3d model.\n",
      "the skin ﬁducials and intra-ﬁducial surface point clouds were\n",
      "labeled in both images as data features.\n",
      "subsurface\n",
      "anatomical targets were labeled in both images and used to compute target error after\n",
      "registration.\n",
      "outliers\n",
      "are noted as (x) and are 1.5•iqr.\n",
      "3.2\n",
      "registration methods comparison\n",
      "this dataset consists of supine breast mr images simulating surgical deformations from\n",
      "one breast cancer patient.\n",
      "skin ﬁducial placement, image acquisition, arm placement, and\n",
      "preprocessing steps followed the same protocol detailed in sect.\n",
      "the tumor was\n",
      "segmented in both images by a subject matter expert, and a 3d tumor model was created\n",
      "to evaluate tumor overlap metrics after registration.\n",
      "regularized kelvinlet function registration was compared to 3 other registration\n",
      "methods: rigid registration, an fem-based image-to-physical registration method, and an\n",
      "image-to-image registration method.\n",
      "the\n",
      "fem-based image-to-physical registration method, detailed in [12] and implemented\n",
      "in breast in [16], utilizes the same optimization scheme as this method but with an\n",
      "fem-generated basis.\n",
      "the image-to-image registration method was a symmetric diffeomorphic method\n",
      "with explicit b-spline regularization publicly available in the advanced normalization\n",
      "toolkit (ants) repository [19, 20].\n",
      "image-to-image registration would not be possi-\n",
      "ble for intraoperative registration in most surgical settings.\n",
      "the rigid and image-to-physical\n",
      "registrations were performed on a single thread of a 3.6 ghz amd ryzen 7 3700x\n",
      "cpu.\n",
      "image-to-image registration was multithreaded on 2.3 ghz intel xeon (e5–4610\n",
      "v2) cpus.\n",
      "registration results for the 4 methods are shown in table 1.\n",
      "the regularized kelvinlet\n",
      "methodaccuracywascomparable(ifnotslightlyimproved)tothefem-basedmethodfor\n",
      "thisexamplecase.runtimefortheregularizedkelvinletmethodwasimprovedcompared\n",
      "to the fem-based method.\n",
      "as expected, registration without deformable correction was\n",
      "poor, and image-to-image registration had the best accuracy.\n",
      "4.\n",
      "table 1. registration performance for 4 methods.\n",
      "rigid\n",
      "image-to-physical\n",
      "image-to-image\n",
      "fem\n",
      "r. kelvinlets\n",
      "point\n",
      "metrics\n",
      "fiducial error (mm)\n",
      "7.4 ± 2.0\n",
      "0.7 ± 0.5\n",
      "1.4 ± 0.6\n",
      "2.0 ± 1.7\n",
      "target error (mm)\n",
      "6.1 ± 1.4\n",
      "3.3 ± 1.1\n",
      "3.0 ± 1.1\n",
      "2.3 ± 1.5\n",
      "tumor\n",
      "overlap\n",
      "metrics\n",
      "dice coefﬁcient\n",
      "2.3%\n",
      "32.7%\n",
      "49.5%\n",
      "85.8%\n",
      "centroid distance (mm)\n",
      "7.3\n",
      "4.4\n",
      "3.5\n",
      "1.3\n",
      "modiﬁed hd (mm)\n",
      "4.1\n",
      "2.2\n",
      "1.7\n",
      "0.6\n",
      "runtime (seconds)\n",
      "< 1\n",
      "188\n",
      "14\n",
      "15,942\n",
      "fig.\n",
      "orange – image-to-image registered xmoving tumor.\n",
      "in this work, we demonstrated the use of regularized kelvinlet functions for image-\n",
      "to-physical registration of the breast.\n",
      "we believe that this approach is gen-\n",
      "eralizable to other soft-tissue organ systems and is well-suited for improving navigation\n",
      "during image-guided surgeries.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf:\n",
      "endoscopic radio-guided cancer detection and\n",
      "resection has recently been evaluated whereby a novel tethered laparo-\n",
      "scopic gamma detector is used to localize a preoperatively injected radio-\n",
      "tracer.\n",
      "this can both enhance the endoscopic imaging and complement\n",
      "preoperative nuclear imaging data.\n",
      "initial failed attempts used segmentation or geometric meth-\n",
      "ods, but led to the discovery that it could be resolved by leveraging high-\n",
      "dimensional image features and probe position information.\n",
      "to demon-\n",
      "strate the eﬀectiveness of this solution, we designed and implemented a\n",
      "simple regression network that successfully addressed the problem.\n",
      "through intensive experimentation, we demonstrated that\n",
      "our method can successfully and eﬀectively detect the sensing area, estab-\n",
      "lishing a new performance benchmark.\n",
      "keywords: laparoscopic image-guided intervention · minimally\n",
      "invasive surgery · detection of sensing area\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43996-4 25.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "surgery is\n",
      "one of the main curative treatment options for cancer.\n",
      "1. (a) hardware set-up for experiments, including a customized portable stereo\n",
      "laparoscope system and the ‘sensei’ probe, a rotation stage, a laparoscopic lighting\n",
      "source, and a phantom; (b) an example of the use of the ‘sensei’ probe in mis.\n",
      "leverages the cancer-targeting ability of nuclear\n",
      "agents typically used in nuclear imaging to more accurately identify cancer intra-\n",
      "operatively from the emitted gamma signal (see fig.\n",
      "geometrically, the sensing\n",
      "area is deﬁned as the intersection point between the gamma probe axis and\n",
      "the tissue surface in 3d space, but projected onto the 2d laparoscopic image.\n",
      "in this study, in order to provide sensing area visual-\n",
      "ization ground truth, we modiﬁed a non-functional ‘sensei’ probe by adding\n",
      "a miniaturized laser module to clearly optically indicate the sensing area on\n",
      "the laparoscopic images - i.e. the ‘probe axis-surface intersection’.\n",
      "our system\n",
      "consists of four main components: a customized stereo laparoscope system for\n",
      "capturing stereo images, a rotation stage for automatic phantom movement, a\n",
      "shutter for illumination control, and a daq-controlled switchable laser module\n",
      "(see fig.\n",
      "2\n",
      "related work\n",
      "laparoscopic images play an important role in computer-assisted surgery and\n",
      "have been used in several problems such as object detection\n",
      "[9], image segmenta-\n",
      "tion [23], depth estimation\n",
      "a 3d displacement module was explored in [21] and 3d geometric consistency\n",
      "was utilized in [8] for self-supervised monocular depth estimation.\n",
      "[19] presented a spatiotemporal vision transformer-based method and a self-\n",
      "supervised generative adversarial network was introduced in [7] for depth esti-\n",
      "mation of stereo laparoscopic images.\n",
      "however, acquiring per-pixel ground\n",
      "truth depth data is challenging, especially for laparoscopic images, which makes\n",
      "it diﬃcult for large-scale supervised training [8].\n",
      "laparoscopic segmentation is another important task in computer-assisted\n",
      "surgery as it allows for accurate and eﬃcient identiﬁcation of instrument posi-\n",
      "tion, anatomical structures, and pathological tissue.\n",
      "for instance, a uniﬁed\n",
      "framework for depth estimation and surgical tool segmentation in laparoscopic\n",
      "images was proposed in [5], with simultaneous depth estimation and segmen-\n",
      "tation map generation.\n",
      "in [12], self-supervised depth estimation was utilized to\n",
      "regularize the semantic segmentation in knee arthroscopy.\n",
      "[16]\n",
      "introduced a multi-task convolutional neural network for event detection and\n",
      "semantic segmentation in laparoscopic surgery.\n",
      "the dual swin transformer u-net\n",
      "was proposed in [11] to enhance the medical image segmentation performance,\n",
      "which leveraged the hierarchical swin transformer into both the encoder and the\n",
      "decoder of the standard u-shaped architecture, beneﬁting from the self-attention\n",
      "computation in swin transformer as well as the dual-scale encoding design.\n",
      "[3] has been commonly used as the encoder to extract the\n",
      "image features and geometric information of the scene.\n",
      "in particular, in [21],\n",
      "concatenated stereo image pairs were used as inputs to achieve better results,\n",
      "and such stereo image types are also typical in robot-assisted minimally invasive\n",
      "surgery with stereo laparoscopes.\n",
      "hence, stereo image data was also adopted in\n",
      "this paper.\n",
      "we note that the standard illumination image from the laparoscopic probe\n",
      "is also captured with the same setup when the laser module is on.\n",
      "therefore, we\n",
      "can establish a dataset with an image pair (rgb image and laser image) that\n",
      "shares the same intersection point ground truth with the laser image (see fig.\n",
      "the assumptions made are that the probe’s 3d pose when projected\n",
      "into the two 2d images is the observed 2d pose, and that the intersection point\n",
      "is located on its axis.\n",
      "(a)\n",
      "(b)\n",
      "(c)\n",
      "laser spot\n",
      "laser spot\n",
      "(d)\n",
      "left\n",
      "image\n",
      "left\n",
      "image\n",
      "right\n",
      "image\n",
      "right\n",
      "image\n",
      "fig.\n",
      "(a) standard illumination left rgb image; (b) left image with\n",
      "laser on and laparoscopic light oﬀ; same for (c) and (d) but for right images.\n",
      "the accompanying api\n",
      "allowed for automatic image acquisition, exposure time adjustment, and white\n",
      "balancing.\n",
      "we acquired the dataset on a silicone tissue phantom which was 30 × 21 × 8\n",
      "cm and was rendered with tissue color manually by hand to be visually realistic.\n",
      "the phantom was placed on a rotation stage that stepped 10 times per revolution\n",
      "to provide views separated by a 36-degree angle.\n",
      "at each position, stereo rgb\n",
      "images were captured i) under normal laparoscopic illumination with the laser\n",
      "oﬀ; ii) with the laparoscopic light blocked and the laser on; and iii) with the\n",
      "laparoscopic light blocked and the laser oﬀ. subtraction of the images with laser\n",
      "on and oﬀ readily allowed segmentation of the laser area and calculation of its\n",
      "central point, i.e. the ground truth probe axis-surface intersection.\n",
      "all data acquisition and devices were controlled by python and labview\n",
      "programs, and complete data sets of the above images were collected on visually\n",
      "realistic phantoms for multiple probe and laparoscope positions.\n",
      "therefore, our ﬁrst newly acquired dataset, named jerry, contains 1200 sets\n",
      "of images.\n",
      "– depth estimation: corresponding ground truth will be released.\n",
      "– tool segmentation: corresponding ground truth will be released.\n",
      "4\n",
      "probe axis-surface intersection detection\n",
      "4.1\n",
      "overview\n",
      "the problem of detecting the intersection point is trivial when the laser is on and\n",
      "can be solved by training a deep segmentation network.\n",
      "however, segmentation\n",
      "requires images with a laser spot as input, while the real gamma probe produces\n",
      "no visible mark and therefore this approach produces inferior results.\n",
      "however, marker-based tracking and pose\n",
      "estimation methods have sterilization implications for the instrument, and the\n",
      "sfm method requires the surgeon to constantly move the laparoscope, reducing\n",
      "the practicality of these methods for surgery.\n",
      "furthermore, this sim-\n",
      "ple methodology facilitated an average inference time of 50 frames per second,\n",
      "enabling real-time sensing area map generation for intraoperative surgery.\n",
      "(a) the input rgb image, (b) the estimated line using\n",
      "pca for obtaining principal points, (c) the image with laser on that we used to detect\n",
      "the intersection ground truth.\n",
      "images\n",
      "resblock1\n",
      "....\n",
      "resblock4\n",
      "fc\n",
      "concatenation\n",
      "mlp1\n",
      "mlp2\n",
      "loss\n",
      "feature1\n",
      "point\n",
      "feature\n",
      "visual\n",
      "feature\n",
      "feature2\n",
      "principal points\n",
      "fig.\n",
      "4. an overview of our approach using resnet and mlp.\n",
      "4.2\n",
      "intersection detection as segmentation\n",
      "we utilized diﬀerent deep segmentation networks as a ﬁrst attempt to address\n",
      "our problem [10,18].\n",
      "please refer to the supplementary material for the imple-\n",
      "mentation details of the networks.\n",
      "we observed that when we do not use images\n",
      "with the laser, the network was not able to make any good predictions.\n",
      "(a) and (c) are standard illumination images and (b) and (d)\n",
      "are\n",
      "images with laser on and laparoscopic light oﬀ. the predicted intersection point\n",
      "is shown in blue and the green point indicates the ground truth, which are further\n",
      "indicated by arrows for clarity.\n",
      "(color ﬁgure online)\n",
      "understandable as the red laser spot provides the key information for the seg-\n",
      "mentation.\n",
      "therefore the network does not have any visual information to make\n",
      "predictions from images of the gamma probe.\n",
      "we note that to enable real-world\n",
      "applications, we need to estimate the intersection point using the images when\n",
      "the laser module is turned oﬀ.\n",
      "4.3\n",
      "intersection detection as regression\n",
      "problem formulation.\n",
      "formally, given a pair of stereo images il, ir, n points\n",
      "{pl\n",
      "1, pl\n",
      "2, ..., pl\n",
      "n} were sampled along the principal axis of the probe, pl\n",
      "i ∈ r2\n",
      "from the left image.\n",
      "the same process was repeated for the right image.\n",
      "unlike the segmentation approach, the intersection\n",
      "point was directly predicted using a regression network.\n",
      "the images fed to the\n",
      "network were ‘laser oﬀ’ stereo rgb, but crucially, the intersection point for these\n",
      "images was known a priori from the paired ‘laser on’ images.\n",
      "the raw image\n",
      "resolution was 4896×3680 but these were binned to 896×896.\n",
      "[15] was used to extract the central axis of the probe and\n",
      "50 points were sampled along this axis as an extra input dimension.\n",
      "a network\n",
      "was designed with two branches, one branch for extracting visual features from\n",
      "the image and one branch for learning the features from the sequence of principal\n",
      "points using resnet [3] and vision transformer (vit)\n",
      "detecting the sensing area of a laparoscopic probe in cancer surgery\n",
      "267\n",
      "4.4\n",
      "implementation\n",
      "evaluation metrics.\n",
      "implementation details.\n",
      "the networks were implemented in pytorch [17],\n",
      "with an input resolution of 896 × 896 and a batch size of 12.\n",
      "we partitioned the\n",
      "jerry dataset into three subsets, the training, validation, and test set, consisting\n",
      "of 800, 200, and 200 images, respectively, and the same for the coﬀbee dataset.\n",
      "6.9\n",
      "8.2\n",
      "16.7 21.3\n",
      "7.0\n",
      "3d median\n",
      "6.0\n",
      "5.9\n",
      "7.1\n",
      "5.3\n",
      "6.2\n",
      "5\n",
      "results\n",
      "quantitative results on the released datasets are shown in table 1 and table 2\n",
      "with diﬀerent backbones for extracting image features, resnet and vit.\n",
      "for\n",
      "the 2d error on two datasets, among the diﬀerent settings, the combination of\n",
      "resnet and mlp gave the best performance with a mean error of 70.5 pixels\n",
      "268\n",
      "b.\n",
      "comparing\n",
      "the table 1 and table 2, we found that the resnet backbone was better than the\n",
      "vit backbone in the image processing task, while mlp was better than lstm in\n",
      "probe pose representation.\n",
      "resnet processed the input images as a whole, which\n",
      "was better suited for utilizing the global context of a uniﬁed scene composed of\n",
      "the tissue and the probe, compared to the vit scheme, which treated the whole\n",
      "scene as several patches.\n",
      "it is worth noting that the results from stereo inputs exceeded\n",
      "those from mono inputs, which can be attributed to the essential 3d information\n",
      "included in the stereo image pairs.\n",
      "for the 3d error, the resnet backbone still gave generally better performance\n",
      "than the vit backbone while under the resnet backbone, lstm and mlp gave\n",
      "competitive results and they are all in sub-milimeter level.\n",
      "this ﬁgure illustrates that our proposed method successfully detected the inter-\n",
      "section point using solely standard rgb laparoscopic images as the input.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_29.pdf:\n",
      "it is\n",
      "based on a siamese architecture, including a recurrent neural network\n",
      "that leverages the ultrasound image features and the optical ﬂow to\n",
      "estimate the relative position of frames.\n",
      "in addition, despite the\n",
      "predominant non-linearity motion in our context, our method achieves\n",
      "a good reconstruction with ﬁnal and average drift rates of 23.11% and\n",
      "28.71% respectively.\n",
      "keywords: intraoperative ultrasound · liver surgery · volume\n",
      "reconstruction · recurrent neural networks\n",
      "1\n",
      "introduction\n",
      "liver cancer is the most prevalent indication for liver surgery, and although there\n",
      "have been notable advancements in oncologic therapies, surgery remains as the\n",
      "only curative approach overall [20].\n",
      "liver laparoscopic resection has demonstrated fewer complications compared\n",
      "to open surgery [21], however, its adoption has been hindered by several reasons,\n",
      "such as the risk of unintentional vessel damage, as well as oncologic concerns such\n",
      "as tumor detection and margin assessment.\n",
      "https://doi.org/10.1007/978-3-031-43999-5_29\n",
      "304\n",
      "s. el hadramy et al.\n",
      "performing ious during laparoscopic liver surgery poses signiﬁcant chal-\n",
      "lenges, as laparoscopy has poor ergonomics and narrow ﬁelds of view, and on the\n",
      "other hand, ious demands skills to manipulate the probe and analyze images.\n",
      "at the end, and despite its real-time capabilities, ious images are inter-\n",
      "mittent and asynchronous to the surgery, requiring multiple iterations and\n",
      "repetitive steps (probe-in −→ instruments-out −→ probe-out −→ instruments-in).\n",
      "therefore, any method enabling a continuous and synchronous us assessment\n",
      "throughout the surgery, with minimal iterations required would signiﬁcantly\n",
      "improve the surgical workﬂow, as well as its eﬃciency and safety.\n",
      "to overcome these limitations, the use of intravascular ultrasound (ivus)\n",
      "images has been proposed, enabling continuous and synchronous inside-out\n",
      "imaging during liver surgery [19].\n",
      "with an intravascular approach, an overall\n",
      "view and full-thickness view of the liver can quickly and easily be obtained\n",
      "through mostly rotational movements of the catheter, while this is constrained\n",
      "to the lumen of the inferior vena cava, and with no interaction with the tissue\n",
      "(contactless, a.k.a. standoﬀ technique) as illustrated in fig.\n",
      "1. left: ivus catheter positioned in the lumen of the inferior vena cava in the\n",
      "posterior surface of the organ, and an example of the lateral ﬁring and longitudinal\n",
      "beam-forming images; middle: anterior view of the liver and the rotational move-\n",
      "ments of the catheter providing full-thickness images; right: inferior view showing the\n",
      "rotational us acquisitions\n",
      "however, to beneﬁt from such a technology in a computer-guided solution,\n",
      "the diﬀerent us images would need to be tracked and possibly integrated into\n",
      "a volume for further processing.\n",
      "this information is then used to register the 3d ultrasound image with\n",
      "the patient’s anatomy.\n",
      "with the recent advances in deep learning, recent\n",
      "trackerless volume reconstruction from intraoperative ultrasound images\n",
      "305\n",
      "works have proposed to learn a higher order nonlinear mapping between adjacent\n",
      "frames and their relative spatial transformation.\n",
      "[9] ﬁrst demon-\n",
      "strated the eﬀectiveness of a convolution neural network to learn the relative\n",
      "motion between a pair of us images.\n",
      "[13] leverages past and future frames to esti-\n",
      "mate the relative transformation between each pair of the sequence; they used\n",
      "the consistency loss proposed in [14].\n",
      "recent work [15,16] proposed to exploit the acceleration and\n",
      "orientation of an inertial measurement unit (imu) to improve the reconstruc-\n",
      "tion performance and reduce the drift error.\n",
      "we use a\n",
      "siamese architecture based on a sequence to vector(seq2vec) neural network\n",
      "that leverages image and optical ﬂow features to learn relative transformation\n",
      "between a pair of images.\n",
      "to achieve this goal, we propose a siamese architecture that leverages\n",
      "the optical ﬂow in the sequences in addition to the frames of interest in order to\n",
      "provide a mapping with the relative frames spatial transformation.\n",
      "then, gaussian heatmaps are used to describe the\n",
      "motion of the m points in an image-like format(see sect.\n",
      "we use the pyramidal\n",
      "implementation of lucas-kanade method proposed in [4] to solve the equation.\n",
      "+ 2}\n",
      "we only keep the ﬁrst and last position of each point, which corresponds to the\n",
      "trackerless volume reconstruction from intraoperative ultrasound images\n",
      "307\n",
      "(a) first frame in the sequence\n",
      "(b) last frame in the sequence\n",
      "fig.\n",
      "∈ rh×w with\n",
      "the same dimension as the ultrasound frames to encode these points, they are\n",
      "more suitable as input for the convolutional networks.\n",
      "these pairs\n",
      "concatenated with the ultrasound ﬁrst and last frames form the recurrent neural\n",
      "network sequential input of size (m + 1, h, w, 2), where m + 1 is the number\n",
      "of channels (m heatmaps and one ultrasound frame), h and w are the height\n",
      "and width of the frames and ﬁnally 2 represents the temporal dimension.\n",
      "2.3\n",
      "network architecture\n",
      "the siamese architecture is based on a sequence to vector network.\n",
      "our network\n",
      "maps a sequence of two images having m + 1 channel each to a six degrees of\n",
      "freedrom vector (three translations and three rotation angles).\n",
      "the network\n",
      "takes as input a sequence of two images with m + 1 channel each, m heatmaps and\n",
      "an ultrasound frame.\n",
      "||t(1,k+2)− ˆt(1,k+2)||2+||t(k+2,2k+3)− ˆt(k+2,2k+3)||2+||t(1,2k+3)− ˆt(1,2k+3)||2\n",
      "(3)\n",
      "3\n",
      "results and discussion\n",
      "3.1\n",
      "dataset and implementation details\n",
      "to validate our method, six tracked sequences were acquired from an ex vivo\n",
      "swine liver.\n",
      "a manually manipulated ivus catheter was used (8 fr lateral ﬁring\n",
      "acunavtm 4–10 mhz) connected to an ultrasound system (acuson s3000\n",
      "helx touch, siemens healthineers, germany), both commercially available.\n",
      "frames were cropped to\n",
      "remove the patient and probe characteristics, then down-sampled to a size of\n",
      "128 × 128 with an image spacing of 0.22 mm per pixel.\n",
      "first and end stages of the\n",
      "sequences were removed from the six acquired sequences, as they were considered\n",
      "to be largely stationary, and aiming to avoid training bias.\n",
      "clips were created by\n",
      "sliding a window of 7 frames (corresponding to a value of k = 2) with a stride of 1\n",
      "trackerless volume reconstruction from intraoperative ultrasound images\n",
      "309\n",
      "over each continuous sequence, yielding a data set that contains a total of 13734\n",
      "clips.\n",
      "the number of\n",
      "heatmaps m and the frame jump k were experimentally chosen among 0, 2, 4, 6.\n",
      "our\n",
      "method is implemented in pytorch1 1.8.2, trained and evaluated on a geforce\n",
      "rtx 3090.\n",
      "the model with the best\n",
      "performance on the validation data was selected and used for the testing.\n",
      "average drift rate (adr): the average cumulative drift of all frames\n",
      "divided by the length from the frame to the starting point of the sequence.\n",
      "(color ﬁgure online)\n",
      "trackerless volume reconstruction from intraoperative ultrasound images\n",
      "311\n",
      "4\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf:\n",
      "imaging patients with inﬂammatory bowel disease (ibd) can\n",
      "be especially problematic, owing to involuntary bowel movements and\n",
      "diﬃculties with long breath-holds during acquisition.\n",
      "therefore, this\n",
      "paper proposes a deep adversarial super-resolution (sr) reconstruction\n",
      "approach to address the problem of multi-task degradation by utilizing\n",
      "cycle consistency in a staged reconstruction model.\n",
      "we leverage a low-\n",
      "resolution (lr) latent space for motion correction, followed by super-\n",
      "resolution reconstruction, compensating for imaging artefacts caused by\n",
      "respiratory motion and spontaneous bowel movements.\n",
      "this alleviates\n",
      "the need for semantic knowledge about the intestines and paired data.\n",
      "learned image reconstruction approaches are believed to\n",
      "occasionally hide disease signs.\n",
      "we investigate this hypothesis by evalu-\n",
      "ating a downstream task, automatically scoring ibd in the area of the\n",
      "terminal ileum on the reconstructed images and show evidence that our\n",
      "method does not suﬀer a synthetic domain bias.\n",
      "keywords: abdominal mr · motion correction · super-resolution ·\n",
      "deep learning\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43999-5 12.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "one of its manifestations, crohn’s\n",
      "disease, often exhibits symptoms such as abdominal pain, diarrhoea, fatigue, and\n",
      "cramping pain, which can be accompanied by severe complications [3].\n",
      "although\n",
      "crohn’s disease cannot be completely cured, early diagnosis can signiﬁcantly\n",
      "reduce treatment costs and permanent physical damage [4].\n",
      "as a result, many patients’ images are degraded by respiration, involuntary\n",
      "movements and peristalsis.\n",
      "furthermore, due to technical limitations, it is dif-\n",
      "ﬁcult to acquire hr images in all scan orientations.\n",
      "this limits the assessment\n",
      "of the complete volume in 3d.\n",
      "despite these challenges, mc and sr are crucial because corrupted mr\n",
      "images can lead to inaccurate interpretation and diagnosis\n",
      "manual correction\n",
      "or enhancement of these volumes is not feasible.\n",
      "contribution: our method (mocosr) alleviates the need for semantic knowl-\n",
      "edge and manual paired-annotation of individual structures and the requirement\n",
      "for acquiring multiple image stacks from diﬀerent orientations, e.g., [8].\n",
      "there are several methodological contributions of our work: (1) first, to\n",
      "account for non-isotropic voxel sizes of abdominal images, we reconstruct spa-\n",
      "tial resolution from corrupted bowel mr images by enforcing cycle consistency.\n",
      "the complementary spatial information from unpaired quality images\n",
      "is exploited via cycle regularisation to provide an explicit constraint.\n",
      "(3) experimental evaluation and analysis show that our mocosr is able to\n",
      "generate high-quality mr images and performs favourably against other, alter-\n",
      "mocosr\n",
      "123\n",
      "native methods.\n",
      "furthermore, we explore conﬁdence in the generated data and\n",
      "improvements to the diagnostic process.\n",
      "(4) experiments with existing models\n",
      "for predicting the degree of small bowel inﬂammation in crohn’s disease patients\n",
      "show that mocosr can retain diagnostically relevant features and maintain the\n",
      "original hr feature distribution for downstream image analysis tasks.\n",
      "joint optimization of mc and sr remains challenging\n",
      "because of the high-dimensionality of hr image space, and lr latent space has\n",
      "been introduced in order to alleviate this issue.\n",
      "recent studies on sr joint with\n",
      "other tasks (e.g., reconstruction, denoising) have demonstrated improvements in\n",
      "the lr space [11,19,20].\n",
      "in the ﬁeld of machine learning and gastroin-\n",
      "testinal disease, [21] used random forests to segment diseased intestines, which is\n",
      "the ﬁrst time that image analysis support has been applied to bowel mri.\n",
      "how-\n",
      "ever, this technique requires radiologists to label and evaluate diseased bowel\n",
      "segments, and patients’ scan times are long.\n",
      "during inference, only the ﬁrst pair of lr encoder eclr and sr\n",
      "decoder dsr will be utilized to generate high-quality, motion-free, and super-resolved\n",
      "images.\n",
      "a pair of task-speciﬁc\n",
      "discriminators is used to improve the performance of each task-related encoder\n",
      "and decoder.\n",
      "loss functions: rather than aiming to reconstruct motion-free and hr images\n",
      "in high dimensional space with paired data, we propose to regularize in low\n",
      "dimensional latent space to obtain a high quality lr feature that can be used\n",
      "for upscaling.\n",
      "a lmc between ˜zq downsampled from qhr and zq cycled after\n",
      "llr and clr, deﬁnes in an unpaired manner as follows:\n",
      "+ ladv( ˜y )\n",
      "(4)\n",
      "the corresponding two task-speciﬁc discriminators ldmc and ldsr for dis-\n",
      "criminating between corrupted and quality images followed are used for the\n",
      "mocosr\n",
      "125\n",
      "purpose of staged reconstruction zq and ˆy of mc at latent space and sr at\n",
      "spatial space, respectively.\n",
      "the residual\n",
      "output is then added to the input using a residual connection to obtain a staged\n",
      "output.\n",
      "the model implements the extraction of local features while integrating\n",
      "all previous features through the connected blocks and compression layers.\n",
      "3\n",
      "experiments\n",
      "data degradation: we use 64 × 64 × 64 patches.\n",
      "for downsampling and the\n",
      "degradation associated with mri scanning, (1) gaussian noise with a standard\n",
      "deviation of 0.25 was added to the image.\n",
      "the\n",
      "simulated motion includes the inﬂuence of environmental factors that cause the\n",
      "respiratory intensity and frequency to ﬂuctuate within a certain range.\n",
      "this lead\n",
      "to the presence of inter-slice misalignment in image domains.\n",
      "(b) the staged reconstruction process involving application\n",
      "of pmrm, mc result, and ground truth (gt) on tcga-lihc dataset.\n",
      "we applied simulated motion to tcga mri-abdomen series to generate the\n",
      "motion-corrupted dataset with respiration-induced shift.\n",
      "ibd data set, inﬂammatory bowel disease:\n",
      "mri sequences obtained\n",
      "include axial t2 images, coronal t2 images and axial postcontrast mri data\n",
      "on a philips achieva 1.5 t mr scanner.\n",
      "abdominal 2d-acquired images exhibit\n",
      "motion shifts between slices and ﬁbrillation artefacts due to the diﬃculty of\n",
      "holding one’s breath/body movement and suppressing random organ motion for\n",
      "extended periods.\n",
      "the abnormal crohn’s disease sample cases, which\n",
      "could contain more than one segment of terminal ileum and small bowel crohn’s\n",
      "disease, were deﬁned based on the review of clinical endoscopic, histological, and\n",
      "radiological images and by unanimous review by the same two radiologists (this\n",
      "criterion has been used in the recent metric trial investigating imaging in\n",
      "crohn’s disease [24]).\n",
      "setting:\n",
      "we compare with interpolation of bicubic and bilinear techniques,\n",
      "rapid and accurate mr image sr (raisr-mr) with hashing-based learning [25],\n",
      "which we use as the representative of the model-based methods, and mri\n",
      "sr with various generative adversarial networks (mresr [17], cmrsr\n",
      "various methods were included in the quantitative evaluation,\n",
      "including single forward wgan (s-wgan) and cycle rdb (c-rdb) for abla-\n",
      "tion experiments on the cycle consistency framework and the glrb setting.\n",
      "there is a performance gap between interpolation and gan-\n",
      "based methods, and the sr method based on learning has a signiﬁcant mapping\n",
      "advantage for complex gastrointestinal images.\n",
      "mocosr achieves the best per-\n",
      "formance among all evaluation metrics.\n",
      "cmrsr and mresr cannot guarantee\n",
      "the output quality of mapping from hr back to lr, resulting in poor perfor-\n",
      "mance on complex 3d bowel data.\n",
      "our evaluation is based on the\n",
      "average possibility of normal and abnormal small bowel inﬂammation on mri.\n",
      "complete results including lr degraded image, sr image reconstructed by\n",
      "mresr, cmrsr, and mocosr, are shown in table 3.\n",
      "the ideal sr data can achieve classiﬁcation results as close as possible to\n",
      "hr data with lower requirements.\n",
      "the results of mresr\n",
      "are volatile but present an unexpected improvement on healthy samples.\n",
      "combining multi-\n",
      "scale image information in the feature space of diﬀerent resolution image domains\n",
      "yields better results than inter-domain integration.\n",
      "this leads to a distribution shift in the samples, which makes\n",
      "the disease prediction biased as shown in fig.\n",
      "(b)\n",
      "the comparison method results in the loss and concealment of discriminative features.\n",
      "crohn’s disease classiﬁcation performance on diﬀerent 3d data.\n",
      "mocosr has\n",
      "a negligible eﬀect on the downstream classiﬁcation task as shown by high p-values in\n",
      "contrast to lr, mresr, and cmrsr which produce signiﬁcantly lower performance.\n",
      "the present sensitivity study is lim-\n",
      "ited to the automatic classiﬁcation from single domain and down-stream task\n",
      "framework, and future extensions will explore model-based and learning segmen-\n",
      "tation tasks across data domains and acquisitions.\n",
      "mocosr\n",
      "is evaluated extensively and compared to the various image sr reconstruction\n",
      "algorithms on a public abdominal dataset, simulating diﬀerent degrees of respira-\n",
      "tory motion, and an ibd dataset with inherent motion.\n",
      "mocosr demonstrated\n",
      "superior performance.\n",
      "to test if our learned reconstruction preserves clinically\n",
      "relevant features, we tested on a downstream disease scoring method and found\n",
      "no decrease in disease prediction performance with mocosr.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf:\n",
      "therefore, we propose an ldct image super-resolution\n",
      "network consisting of a dual-guidance feature distillation backbone for\n",
      "elaborate visual feature extraction, and a dual-path content communica-\n",
      "tion head for artifacts-free and details-clear ct reconstruction.\n",
      "the dgfm guides the network to concentrate the feature repre-\n",
      "sentation of the 3d inter-slice information in the region of interest (roi)\n",
      "by introducing the average ct image and segmentation mask as comple-\n",
      "ments of the original ldct input.\n",
      "furthermore, the heads\n",
      "with the same function share the parameters so as to eﬃciently improve\n",
      "the reconstruction performance by reducing the amount of parameters.\n",
      "the experiments compared with 6 state-of-the-art methods on 2 public\n",
      "datasets prove the superiority of our method.\n",
      "keywords: low-dose computed tomography · image denoising ·\n",
      "image super-resolution · deep learning\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14229, pp.\n",
      "https://doi.org/10.1007/978-3-031-43999-5_10\n",
      "low-dose ct image super-resolution network\n",
      "99\n",
      "1\n",
      "introduction\n",
      "following the “as low as reasonably achievable” (alara) principle\n",
      "[18] and cancer screening [28]. to\n",
      "balance the high image quality and low radiation damage compared to normal-\n",
      "dose ct (ndct), numerous algorithms have been proposed for ldct super-\n",
      "resolution [3,4].\n",
      "in the past decades, image post-processing techniques attracted much atten-\n",
      "tion from researchers because they did not rely on the vendor-speciﬁc parameters\n",
      "[2] like iterative reconstruction algorithms\n",
      "image post-processing super-resolution (sr) methods\n",
      "could be divided into 3 categories: interpolated-based methods [16,25], model-\n",
      "based methods [13,14,24,26] and learning-based methods [7–9,17].\n",
      "and model-based methods often involved time-consuming\n",
      "optimization processes and degraded quickly when image statistics were biased\n",
      "from the image prior\n",
      "[6].\n",
      "with the development of deep learning (dl), various learning-based meth-\n",
      "ods have been proposed, such as edsr [20], rcan [31], and swinir [19].\n",
      "those\n",
      "methods optimized their trainable parameters by pre-degraded low-resolution\n",
      "(lr) and high-resolution (hr) pairs to build a robust model with generaliza-\n",
      "tion and ﬁnally reconstruct sr images.\n",
      "[11] introduced a deep alternating network (dan) which\n",
      "estimated the degradation kernels and corrected those kernels iteratively and\n",
      "reconstructed results following the inverse process of the estimated degradation.\n",
      "more recently, aiming at improving the quality of medical images further, huang\n",
      "et al.\n",
      "to\n",
      "accurately reconstruct hr ct images from lr ct images, hou et al.\n",
      "the aforementioned methods still have drawbacks: (1) they treated the\n",
      "regions of interest (roi) and regions of uninterest equally, resulting in the extra\n",
      "cost in computing source and ineﬃcient use for hierarchical features.\n",
      "(2) most of\n",
      "them extracted the features with a ﬁxed resolution, failing to eﬀectively lever-\n",
      "age multi-scale features which are essential to image restoration task [27,32].\n",
      "1(a), we propose an ldct image\n",
      "sr network with dual-guidance feature distillation and dual-path content com-\n",
      "100\n",
      "j. chi et al.\n",
      "fig.\n",
      "avg ct is the average image among adjacent ct slices\n",
      "of each patient.\n",
      "munication.\n",
      "(2) we propose a sampling attention block (sab)\n",
      "which consists of sampling attention module (sam), channel attention module\n",
      "(cam) and elaborate multi-depth residual connection aiming at the essential\n",
      "multi-scale features by up-sampling and down-sampling to leverage the features\n",
      "in ct images.\n",
      "we ﬁrst calculate the\n",
      "average ct image of adjacent ct slices of each patient to provide the 3d spatial\n",
      "structure information of ct volume.\n",
      "meanwhile, the roi mask is obtained by\n",
      "a pre-trained segmentation network to guide the network to concentrate on the\n",
      "focus area or tissue area.\n",
      "then those guidance images and the input ldct image\n",
      "low-dose ct image super-resolution network\n",
      "101\n",
      "are fed to the dual-guidance feature distillation backbone to extract the deep\n",
      "features.\n",
      "finally, the proposed dual-path architecture consisting of parameter-\n",
      "shared sr heads and denoising heads leverages the deep visual features obtained\n",
      "by our backbone to build the connection between the sr task and the denoising\n",
      "task, resulting in noise-free and detail-clear reconstructed results.\n",
      "to decrease the redundant\n",
      "computation and make full use of the above-mentioned extra information, we\n",
      "design a dual-guidance feature distillation backbone consisting of a dual-guidance\n",
      "fusion module (dgfm) and sampling attention block(sab).\n",
      "firstly, we use a 3 × 3 convolutional layer to extract the shallow features of\n",
      "the three input images.\n",
      "considering the indicative function of roi, we calculate the correlation\n",
      "matrix between ldct and its mask and then acquire the response matrix\n",
      "between the correlation matrix and the average ct image by multi-heads atten-\n",
      "tion mechanism:\n",
      "g represent the shallow\n",
      "features of the input roi mask and the average ct image respectively.\n",
      "furthermore, to take advantage of the multi-scale information which is essen-\n",
      "tial for obtaining the response matrix containing the connections between dif-\n",
      "ferent levels of features, as shown in fig.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "datasets and experiment setup\n",
      "datasets.\n",
      "two widely-used public ct image datasets, 3d-ircadb\n",
      "we choose 1663 ct images from 16 patients for training, 226\n",
      "ct images from 2 patients for validation and 185 ct images from 2 patients for\n",
      "testing.\n",
      "similarly, the pancreas dataset is used for pancreas segmentation\n",
      "which consists of 19328 512 × 512 ct ﬁles from 82 patients.\n",
      "we choose 5638\n",
      "ct images from 65 patients for training, 668 ct images from 8 patients for\n",
      "validation and 753 ct images from 9 patients for testing.\n",
      "and we use bicubic interpolation to degrade\n",
      "the hr images to 256 × 256 lr images and 128 × 128 lr images.\n",
      "low-dose ct image super-resolution network\n",
      "103\n",
      "table 1. ablation experiments on pancreas dataset with the scale factor of 2 and 4\n",
      "(a) ablation experiments for dual-guidance on the pancreas dataset with the scale factor of\n",
      "2 and 4\n",
      "avg ct\n",
      "mask\n",
      "×2\n",
      "×4\n",
      "psnr\n",
      "ssim\n",
      "psnr\n",
      "ssim\n",
      "×\n",
      "×\n",
      "30.0282 ± 2.9426\n",
      "0.8948 ± 0.0431\n",
      "28.5120 ± 2.2875\n",
      "0.8643 ± 0.0508\n",
      "✓\n",
      "×\n",
      "29.9600 ± 3.2378\n",
      "0.8950 ± 0.0419\n",
      "28.1490 ± 2.3284\n",
      "0.8592 ± 0.0543\n",
      "×\n",
      "✓\n",
      "30.2991 ± 3.1391\n",
      "0.8960 ± 0.0413\n",
      "28.6589 ± 2.2497\n",
      "0.8639 ± 0.0522\n",
      "✓\n",
      "✓\n",
      "30.4047 ± 3.1558\n",
      "0.8974 ± 0.0383\n",
      "28.7542 ± 2.2728\n",
      "0.8672 ± 0.0412\n",
      "the best quantitative performance is shown in bold and the second-best in underlined.\n",
      "(b) ablation experiments for shared heads mechanism on the pancreas dataset with the scale\n",
      "factor of 2 and 4\n",
      "heads\n",
      "param (m) ×2\n",
      "×4\n",
      "×2/×4\n",
      "psnr\n",
      "ssim\n",
      "psnr\n",
      "ssim\n",
      "sr only 5.748/5.896 30.2904 ± 3.0620\n",
      "0.8948 ± 0.0431 28.4422 ± 2.3707\n",
      "0.8628 ± 0.0523\n",
      "unshared 6.009/6.304 30.3257 ± 3.2504\n",
      "0.8940 ± 0.0442 28.5675 ± 2.2540\n",
      "0.8645 ± 0.0529\n",
      "shared\n",
      "5.795/5.934 30.4047 ± 3.1558 0.8974 ± 0.0383 28.7542 ± 2.2728 0.8672 ± 0.0412\n",
      "experiment setup.\n",
      "all experiments are implemented on ubuntu 16.04.12\n",
      "with an nvidia rtx 3090 24g gpu using pytorch 1.8.0 and cuda 11.1.74.\n",
      "we augment the data by rotation and ﬂipping ﬁrst and then randomly crop\n",
      "them to 128 × 128 patches.\n",
      "peak signal-to-noise (psnr) and\n",
      "structural similarity (ssim) are used as the quantitative indexes to evaluate the\n",
      "performance.\n",
      "3.2\n",
      "ablation study\n",
      "table 1a shows the experimental result of the dual-guidance ablation study.\n",
      "introducing the average ct image guidance alone degrades performance com-\n",
      "pared with the model without guidance for both the scale factor of 2 and 4.\n",
      "and introducing mask guidance alone could improve the reconstruction eﬀect.\n",
      "when the average ct image guidance and the mask guidance are both embed-\n",
      "ded, the performance will be promoted further.\n",
      "the experimental result proves that\n",
      "introducing the proposed dual-path architecture could promote the reconstruc-\n",
      "tion performance and the model with shared heads is superior than that without\n",
      "them in both reconstruction ability and parameter amount.\n",
      "(a)\n",
      "is the hr image and its red rectangle region displays the liver and its lateral issues.\n",
      "(color ﬁgure online)\n",
      "3.3\n",
      "comparison with state-of-the-art methods\n",
      "we compare the performance of our proposed method with other state-of-the-\n",
      "art methods, including bicubic interpolation\n",
      "all methods enhance the image quality to diﬀer-\n",
      "ent extents compared with bicubic interpolation.\n",
      "similarly, our method outperforms the second-best methods\n",
      "low-dose ct image super-resolution network\n",
      "105\n",
      "fig.\n",
      "(a) is the\n",
      "hr image and its red rectangle region shows the pancreas and kidney.\n",
      "those quantitative superiorities conﬁrm our qualitative observa-\n",
      "tions.\n",
      "106\n",
      "j. chi et al.\n",
      "4\n",
      "conclusion\n",
      "in this paper, we propose an ldct image sr network with dual-guidance fea-\n",
      "ture distillation and dual-path content communication.\n",
      "especially, the dgfm could fuse the average\n",
      "ct image to take the advantage of the 3d spatial information of ct volume and\n",
      "the segmentation mask to focus on the roi, which provides pixel-wise shallow\n",
      "information and deep semantic features for our backbone.\n",
      "the sab leverages\n",
      "the essential multi-scale features to enhance the ability for feature extraction.\n",
      "the experiments compared with 6 state-of-\n",
      "the-art methods on 2 public datasets demonstrate the superiority of our method.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_48.pdf:\n",
      "we propose a new approach to 3d reconstruction from\n",
      "sequences of images acquired by monocular endoscopes.\n",
      "we demonstrate excellent accuracy on phantom\n",
      "imagery.\n",
      "remarkably, the watertight prior combined with illumination\n",
      "decline, allows to complete the reconstruction of unseen portions of the\n",
      "surface with acceptable accuracy, paving the way to automatic quality\n",
      "assessment of cancer screening explorations, measuring the global per-\n",
      "centage of observed mucosa.\n",
      "unfortunately, we do\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43999-5 48.\n",
      "this would usher exciting new developments, such as post-intervention\n",
      "diagnosis, measuring polyps and stenosis, and automatically evaluating explo-\n",
      "ration thoroughness in terms of the surface percentage that has been observed.\n",
      "it has been shown that the colon\n",
      "3d shape can be estimated from single images acquired during human colono-\n",
      "scopies\n",
      "however, to model large sections of it while increasing the recon-\n",
      "struction accuracy, multiple images must be used.\n",
      "as most endoscopes contain\n",
      "a single camera, the natural way to do this is to use video sequences acquired\n",
      "by these cameras in the manner of structure-from-motion algorithms.\n",
      "an impor-\n",
      "tant ﬁrst step in that direction is to register the images from the sequences.\n",
      "and occlusions, specularities, varying albedos, and speciﬁcities of\n",
      "endoscopic lighting make it a challenging one.\n",
      "to overcome these diﬃculties, we rely on two properties of endoscopic images:\n",
      "– endoluminal cavities such as the gastrointestinal tract, and in particular the\n",
      "human colon, are watertight surfaces.\n",
      "to take advantage of these speciﬁcities, we build on the success of neural implicit\n",
      "surfaces (neus)\n",
      "[25] that have been shown to be highly eﬀective at deriving sur-\n",
      "face 3d models from sets of registered images.\n",
      "[15] that inspired them, they were designed to operate on regular images\n",
      "taken around a scene, sampling fairly regularly the set of possible viewing direc-\n",
      "tions.\n",
      "neus training selects a pixel from an image and samples points along its\n",
      "projecting ray.\n",
      "earlier methods [3] have reported similar accu-\n",
      "racies but only on very few synthetic images and on short sections of the colon.\n",
      "this makes us the ﬁrst to show\n",
      "accurate results of extended 3d watertight surfaces from monocular endoscopy\n",
      "images.\n",
      "2\n",
      "related works\n",
      "3d reconstruction from endoscopic images.\n",
      "unfortunately, many state-of the-\n",
      "art slam techniques based on feature matching [5] or direct methods [6,7]\n",
      "are impractical for dense endoscopic reconstruction due to the lack of texture\n",
      "and the inconsistent lighting that moves along with the camera.\n",
      "nevertheless,\n",
      "sparse reconstructions by classical structure-from-motion (sfm) algorithms can\n",
      "be good starting points for reﬁnement and densiﬁcation based on shape-from-\n",
      "shading (sfs)\n",
      "in contrast, for the same environments, we\n",
      "propose the watertight prior coded by implicit sdf representations.\n",
      "recent methods for dense reconstruction rely on neural networks to predict\n",
      "per-pixel depth in the 2d space of each image and fuse the depth maps by\n",
      "using multi-view stereo (mvs)\n",
      "however, holes in\n",
      "the reconstruction appear due to failures in triangulation and inaccurate depth\n",
      "estimation or in areas not observed in any image.\n",
      "[27] show the\n",
      "potential of neural rendering in reconstruction from medical images, although\n",
      "they use a binocular static camera with ﬁxed light source, which is not feasible\n",
      "in endoluminal endoscopy.\n",
      "lightneus: neural surface reconstruction in endoscopy\n",
      "507\n",
      "3.2\n",
      "endoscope photometric model\n",
      "apart from illumination decline, there are several signiﬁcant diﬀerences between\n",
      "the images captured by endoscopes and those conventionally used to train nerfs\n",
      "and neus: ﬁsh-eye lenses, strong vignetting, uneven scene illumination, and post-\n",
      "processing.\n",
      "hence, we also modiﬁed the original neus implementation to support these\n",
      "models.\n",
      "(3) is not\n",
      "constant for all image pixels.\n",
      "the post-processing software of medical endoscopes is designed to always\n",
      "display well-exposed images, so that physicians can see details correctly.\n",
      "an\n",
      "adaptive gain factor g is applied by the endoscope’s internal logic and gamma\n",
      "correction is also used to adapt to non-linear human vision, achieving better\n",
      "contrast perception in mid tones and dark areas.\n",
      "endoscope manufacturers know\n",
      "the post-processing logic of their devices, but this information is proprietary and\n",
      "not available to users.\n",
      "again, gamma correction can be calibrated assuming it is\n",
      "constant [3], and the gain change between successive images can be estimated,\n",
      "for example, by sparse feature matching.\n",
      "thus,\n",
      "our photometric loss is computed using a normalized image:\n",
      "i′ =\n",
      "\u0002 iγ\n",
      "leg\n",
      "\u00031/γ\n",
      "(5)\n",
      "4\n",
      "experiments\n",
      "we validate our method on the c3vd dataset [4], which covers all diﬀerent sec-\n",
      "tions of the colon anatomy in 22 video sequences.\n",
      "the images were recorded inside a phantom, a model of a human colon made of\n",
      "silicone.\n",
      "in the ﬁrst rows of table 1, we report median (medae), mean (mae), and\n",
      "root mean square (rmse) values of these distances for all vertices seen in at\n",
      "least one image.\n",
      "(e) we managed to reconstruct a\n",
      "curved section of the colon.\n",
      "(f) our method plausibly estimates the wall of the colon\n",
      "at the right of camera (b), although it was never seen in the images.\n",
      "2 and additional ones in the supple-\n",
      "mentary material.\n",
      "3. this ability to “ﬁll in” observation gaps may be useful in providing\n",
      "the endoscopist with an estimate of the percentage of unsurveyed area during a\n",
      "procedure.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_73.pdf:\n",
      "we evaluate the quantitative performance of structuregnet for head\n",
      "and neck cancer between 3d ct scans and 2d histopathological slides,\n",
      "enabling pixel-wise mapping of low-quality radiologic imaging to gold-\n",
      "standard tumor extent and bringing biological insights toward homoge-\n",
      "nized clinical guidelines.\n",
      "additionally, our method can be used in radia-\n",
      "tion therapy by mapping 3d planning ct into the 2d mr frame of the\n",
      "treatment day for accurate positioning and dose delivery.\n",
      "keywords: multimodal · registration · 2d-3d · histopathology ·\n",
      "radiology\n",
      "1\n",
      "introduction\n",
      "2d-3d registration refers to the highly challenging process of aligning an input 2d\n",
      "image to its corresponding slice inside a given 3d volume [4].\n",
      "it has received growing\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43999-5 73.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14229, pp.\n",
      "https://doi.org/10.1007/978-3-031-43999-5_73\n",
      "772\n",
      "a. leroy et al.\n",
      "attention in medical imaging due to the various contexts where it applies, like image\n",
      "fusion between 2d real-time acquisitions and either pre-operative 3d images for\n",
      "guided interventions or reference planning volumes for patient positioning in radi-\n",
      "ation therapy (rt).\n",
      "indeed, mri or ct scans are the baseline source\n",
      "ofinformationforcancertreatmentbutfailtoprovideanaccurateassessmentofdis-\n",
      "ease proliferation, leading to high variability in tumor detection\n",
      "on the\n",
      "other hand, high-resolution digitized histopathology, called whole slide imaging\n",
      "(wsi),providescell-levelinformationonthetumorenvironmentfromthesurgically\n",
      "resected specimens.\n",
      "however, the registration process is substantially diﬃcult due\n",
      "to the visual characteristics, resolution scale, and dimensional diﬀerences between\n",
      "thetwomodalities.inaddition,histologicalpreparationinvolvestissueﬁxationand\n",
      "slicing,leadingtoseverecollapseandout-of-planedeformations.(semi-)automated\n",
      "methodshavebeendevelopedtoavoidtime-consumingandbiasedmanualmapping,\n",
      "including protocols with 3d mold or landmarks [10,22], volume reconstruction to\n",
      "perform 3d registration [2,18,19,23], or optimization algorithms for direct multi-\n",
      "modalcomparison[3,15].morerecently,deeplearning(dl)hasbeenintroducedbut\n",
      "is limited to 2d/2d and requires prior plane selection\n",
      "onepromisingsolutionistorelyonrigidstructuresthataresupposedlymorerobust\n",
      "duringthepreparation.structuralinformationtoguideimageregistrationhasbeen\n",
      "studiedwiththehelpofsegmentationsintothetrainingloop[11],orbylearningnew\n",
      "image representations for reﬁned mapping [12].\n",
      "in this paper, we propose to leverage the structural features of tissue and\n",
      "more particularly the rigid areas to guide the registration process with two dis-\n",
      "tinct contributions: (1) a cascaded rigid alignment driven by stiﬀ regions and\n",
      "coupled with recursive plane selection, and (2) an improved 2d/3d deformable\n",
      "motion model with distance ﬁeld regularization to handle out-of-plane deforma-\n",
      "tion.\n",
      "we also use the cyclegan for image translation\n",
      "and direct monomodal signal comparison [25].\n",
      "like [14,24], we combine regis-\n",
      "tration with modality translation and integrate the two aforementioned compo-\n",
      "nents.\n",
      "the modality transfer is a 2d image-to-image translation\n",
      "problem deﬁned as follows: given a sequence of n slices h = {h1, ..., hn} and a\n",
      "volume considered as a full stack of m axial slices ct = {ct1, ..., ctm}, we build\n",
      "a cyclegan with two generators and two discriminators gh→ct , gct →h, dh\n",
      "and dct .\n",
      "with a symmetric situation for gct →h, gh→ct outputs a synthetic\n",
      "ct image, which is then processed by dct along with randomly sampled orig-\n",
      "inal input slices with an associated adversarial loss ladv.\n",
      "the cyclical pattern\n",
      "lies in the similarity between the original images and the reconstructed samples\n",
      "gct →h ◦ gh→ct (hi) through a pixel-wise cycle loss lcyc.\n",
      "finally, we employ\n",
      "two additional metrics: an identity loss lid to encourage modality-speciﬁc fea-\n",
      "ture representation when considering hi being the input for gct →h with an\n",
      "expected identity synthesis; and a structure consistency mind loss from [7] to\n",
      "ensure style transfer without content alteration.\n",
      "these losses are the classical\n",
      "implementations for cyclegan and are detailed in the supplementary material.\n",
      "2.2\n",
      "recursive cascaded plane selection\n",
      "we replace the volume reconstruction step with a recursive dual model.\n",
      "for rigid initialization, the hypothesis is that the histological specimen is\n",
      "cut with an unknown spacing and angle, but the latter is supposed constant\n",
      "between wsis.\n",
      "a rigid alignment is thus suﬃcient to reorient moving ct onto\n",
      "ﬁxed h. based on a theoretical axial slice sequence z = (z1, ..., zm), we deﬁne\n",
      "774\n",
      "a. leroy et al.\n",
      "fig.\n",
      "2. cascaded alignment through rigid structure-aware warping followed by recur-\n",
      "sive plane selection.\n",
      "because soft tissues undergo too large out-of-plane deformations, we\n",
      "leverage the rigid structures which are supposed not to be distorted or shrunk\n",
      "during the histological process.\n",
      "we extract their segmentation masks mct, mh\n",
      "for both modalities (see preprocessing in sect.\n",
      "we then introduce a sequence alignment\n",
      "problem, the objective being to update the slice sequence z of sct by mapping\n",
      "it to a corresponding sequence j of 2d images from ct.\n",
      "[zi − 2, zi + 2]. based on\n",
      "these rigid registration and plane selection blocks, we build a cascaded module\n",
      "to iteratively reﬁne the alignment where the intermediate warping becomes the\n",
      "new input.\n",
      "this\n",
      "dual model is crucial for initialization but does not take into account out-of-\n",
      "plane deformations and a perfect alignment is not accessible yet.\n",
      "the deformable\n",
      "framework bridges this gap by focusing on irregular displacements caused by\n",
      "tissue manipulation and reﬁning the rigid warping.\n",
      "2.3\n",
      "deformable 2d-3d registration\n",
      "given one ﬁxed multi-slice sct ′ and a moving rigidly warped r(ct) from\n",
      "the previous module, we adopt an architecture close to voxelmorph [1].\n",
      "both latent representations are element-wise sub-\n",
      "tracted.\n",
      "a decoder is connected to both encoders and generates a displacement\n",
      "ﬁeld φ the same size as input images but with (x, y, z)-channels corresponding\n",
      "to the displacement in each spatial coordinate.\n",
      "soft tissues away from bones\n",
      "and cartilage are more subject to shrinkage or disruption, so we harness the\n",
      "information from the cartilage segmentation mask of ct to generate a distance\n",
      "transform map δ deﬁned as δ(v) = minm∈mct ||v−m||2.\n",
      "we can then\n",
      "control the displacement ﬁeld, with close tissue being more highly constrained\n",
      "than isolated areas: φ′ = φ⊙(δ+ϵ), where ⊙ is the hadamard product and ϵ is\n",
      "a hyperparameter matrix allowing small displacement even for cartilage areas for\n",
      "which distance transform is null.\n",
      "= \u0004\n",
      "v∈r3 ||∇φ′(v)||2 on the volume to constrain spatial gradients and\n",
      "thus encourage smooth deformation, which is essential for empty slices which\n",
      "are excluded from ldefo.\n",
      "3. deformable 2d-3d registration pipeline, made of two encoders and a shared\n",
      "decoder, with regularization applied on the displacement ﬁeld φ thanks to the distance\n",
      "map from ct.\n",
      "3\n",
      "experiments\n",
      "dataset and preprocessing.\n",
      "two expert radiation oncologists on ct delineated both the thy-\n",
      "roid and cricoid cartilages for structure awareness and the gross tumor volume\n",
      "(gtv) for clinical validation, while two expert pathologists did the same on\n",
      "wsis.\n",
      "we ended up with images of size 256×256\n",
      "(×64 for 3d ct) of 1 mm isotropic grid space.\n",
      "to demon-\n",
      "strate the performance of our model on another application, we also retrieved\n",
      "the datasets from [14] for pelvis 3d ct/2d mr.\n",
      "all masks were provided by\n",
      "the authors and were originally segmented by internal experts.\n",
      "hyperparameters.\n",
      "we drew our code from cyclegan and voxelmorph imple-\n",
      "mentations with modiﬁcations explained above, and we thank the authors of\n",
      "msv-regsynnet for making their code and data available to us\n",
      "a\n",
      "detailed description of architectures and hyperparameters can be found in the\n",
      "supplementary material.\n",
      "we implemented our model with pytorch1.13 frame-\n",
      "work and trained for 600 (800 for mr/ct) epochs with a batch size of 8 (4 for\n",
      "mr/ct) patients parallelized over 4 nvidia gtx 1080 tis.\n",
      "evaluation.\n",
      "next, we implemented the modality translation-based msv-\n",
      "structuregnet: 2d-3d multimodal registration\n",
      "777\n",
      "regsyn-net and modiﬁed it for our application to measure the importance of\n",
      "joint structure-aware initialization and regularization.\n",
      "1. from a qualitative per-\n",
      "spective, the densities of the diﬀerent tissues are well reconstructed, with rigid\n",
      "structures like cartilage being lighter than soft tissues or tumors.\n",
      "the general\n",
      "shape of the larynx also complies with the original radiologic images.\n",
      "therefore, the cascaded rigid initialization is\n",
      "crucial and helps the modality translation module in getting more similar pairs\n",
      "of images for eased synthesis on the next pass.\n",
      "4. the initialization enables an accurate plane\n",
      "selection as proved by the similar shape of cartilages in (b).\n",
      "(a) original ct, (b) warped ct after rigid ini-\n",
      "tialization and plane selection, (c) warped ct after deformable registration, (d) origi-\n",
      "nal histology with landmarks from pathologist (black) and warped projected landmarks\n",
      "from radiologists (yellow), (e) overlaid cartilage masks after registration of histology\n",
      "(ﬁlled blue) and radiology (red for our method, yellow for msv-regsynnet), (f) over-\n",
      "laid contours between warped ct (gtv, red) and wsi (true tumor extent, blue).\n",
      "dev. registration performance in terms of dice score (%),\n",
      "hausdorﬀ distance (mm) and landmark error (mm).\n",
      "method\n",
      "dice\n",
      "hausdorﬀ\n",
      "landmark\n",
      "runtime\n",
      "voxelmorph 3d\n",
      "68.4 ± 0.6\n",
      "8.53 ± 0.32\n",
      "6.71 ± 0.16\n",
      "1.3\n",
      "voxelmorph 2/3d 71.9 ± 1.7\n",
      "7.19 ± 0.24\n",
      "5.99 ± 0.22\n",
      "1.4\n",
      "msv-regsynnet\n",
      "76.3 ± 1.4\n",
      "6.88 ± 0.28\n",
      "4.98 ± 0.15\n",
      "2.1\n",
      "ours (no init)\n",
      "77.9 ± 1.9\n",
      "6.91 ± 0.19\n",
      "4.73 ± 0.31\n",
      "2.1\n",
      "ours (no regu)\n",
      "85.1 ± 0.8\n",
      "4.23 ± 0.27\n",
      "3.71 ± 0.19\n",
      "2.8\n",
      "ours\n",
      "86.9 ± 1.3 3.81 ± 0.20\n",
      "3.28 ± 0.16 2.9\n",
      "3d ct/2d mr\n",
      "0.35t truefisp → 3d ct 1.5t t2 → 3d ct\n",
      "dice\n",
      "hausdorﬀ\n",
      "dice\n",
      "hausdorﬀ\n",
      "msv-regsynnet\n",
      "84.6 ± 0.9\n",
      "7.25 ± 0.05\n",
      "86.1 ± 1.0\n",
      "5.84 ± 0.15\n",
      "ours\n",
      "84.8 ± 1.1 7.12 ± 0.08\n",
      "87.9 ± 1.2\n",
      "5.21 ± 0.09\n",
      "diﬃculties inherent to the histological process like a cut larynx, the model suc-\n",
      "cessfully maps both cartilage and soft tissue without completely tearing the ct\n",
      "image thanks to regularization (c-d-e).\n",
      "for quantitative assessment, we computed\n",
      "the dsc as well as the hausdorﬀ distance between cartilages, and the average\n",
      "distance between characteristic landmarks disposed before registration(table 1).\n",
      "the superior performance of msv-regsynnet advocates for a modality\n",
      "translation-based method compared to a direct multimodal similarity criterion.\n",
      "we\n",
      "also compared against msv-regsynnet on its own validation dataset for gener-\n",
      "alization assessment: we yielded comparable results for the ﬁrst cohort and sig-\n",
      "niﬁcantly better ones for the second, which proves that structuregnet behaves\n",
      "well on other modalities and that the structure awareness is an essential asset for\n",
      "better registration, as pelvis is a location where organs are moving.\n",
      "visuals of\n",
      "registration results are displayed in the supplementary material.\n",
      "eventually, an\n",
      "important clinical endpoint of our study is to compare the gtv delineated on\n",
      "ct with gold-standard tumor extent after co-registration to highlight system-\n",
      "atic errors and better understand the biological environment from the radiologic\n",
      "signals.\n",
      "the\n",
      "typical error cases are the inclusion of cartilage or edema, which highlights the\n",
      "structuregnet: 2d-3d multimodal registration\n",
      "779\n",
      "limitations and variability of radiology-based examinations, leading to increased\n",
      "toxicity or untreated areas in rt.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_66.pdf:\n",
      "we propose an unsupervised deep learning method to recon-\n",
      "struct a 3d tomographic image from biplanar x-rays, to reduce the num-\n",
      "ber of required projections, the patient dose, and the acquisition time.\n",
      "we optimize the latent vectors of the generative model to recover a vol-\n",
      "ume that both integrates this prior knowledge and ensures consistency\n",
      "between the reconstructed image and input projections.\n",
      "keywords: image reconstruction · inverse problem · sparse\n",
      "sampling · deep generative model · ct\n",
      "1\n",
      "introduction\n",
      "tomographic imaging estimates body density using hundreds of x-ray projec-\n",
      "tions, but it’s slow and harmful to patients.\n",
      "this can improve image-guided therapies and preoperative planning, espe-\n",
      "cially for radiotherapy, which requires precise patient positioning with minimal\n",
      "radiation exposure.\n",
      "however, this task is an ill-posed inverse problem: x-ray measurements are\n",
      "the result of attenuation integration across the body, which makes them very\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43999-5_66.\n",
      "feed-forward methods do not manage to\n",
      "predict a detailed and matching tomographic volume from a few projections.\n",
      "in\n",
      "other words, many 3d volumes may have generated such projections a priori.\n",
      "these\n",
      "non-learning methods show good results when the number of input projections\n",
      "remains higher than a dozen but fail when very few projections are provided, as\n",
      "our experiments in sect.\n",
      "to do this, we\n",
      "leverage the potential of generative models to learn a low-dimensional manifold\n",
      "of the target body part.\n",
      "3d ct reconstruction from biplanar x-rays with deep structure prior\n",
      "701\n",
      "compared to other 3d gans, it is proven to provide the best disentanglement\n",
      "of the feature space related to semantic features [2].\n",
      "compared to nerf-based methods, our method exploits prior\n",
      "knowledge from many patients to require only two projections.\n",
      "we perform several experiments to com-\n",
      "pare our method with a feed-forward-based method [30] and a recent nerf-based\n",
      "method [23], which are the previous state-of-the-art methods for the very few or\n",
      "few projections cases, respectively.\n",
      "to\n",
      "summarize, our contributions are two-fold: (i) a new paradigm for 3d recon-\n",
      "struction with biplanar x-rays: instead of learning to invert the measurements,\n",
      "we leverage a 3d style-based generative model to learn deep image priors of\n",
      "anatomic structures and optimize over the latent space to match the input pro-\n",
      "jections; (ii) a novel unsupervised method, fast and robust to sampling ratio,\n",
      "source energy, angles and geometry of projections, all of which making it general\n",
      "for downstream applications and imaging systems.\n",
      "we ﬁrst learn the low-\n",
      "dimensional manifold of ct volumes of a target body region.\n",
      "at inference, we\n",
      "estimate the maximum a posteriori (map) volume on this manifold given very\n",
      "few projections: we ﬁnd the latent vectors that minimize the error between the\n",
      "synthetic projections from the corresponding volume on the manifold and the\n",
      "real ones.\n",
      "in this section, we formalize the problem, describe how we learn the\n",
      "manifold, and detail how we optimize the latent vectors.\n",
      "we ﬁrst learn the low-dimensional manifold of 3d structures\n",
      "using a generative model.\n",
      "2.2\n",
      "manifold learning\n",
      "to regularize the domain space of solutions, we leverage a style-based generative\n",
      "model to learn deep priors of anatomic structures.\n",
      "the map-\n",
      "ping network learns to disentangle the initial latent space relatively to semantic\n",
      "features which is crucial for the inverse problem.\n",
      "we implement adaptive discriminator augmentation from stylegan-ada\n",
      "[14]\n",
      "to improve learning of the model’s manifold with limited medical imaging data.\n",
      "(3)\n",
      "note that by contrast with [18] for example, we optimize on the noise vectors\n",
      "n as well: as we discovered in our early experiments, the n are also useful to\n",
      "embed high-resolution details.\n",
      "= − \u0004\n",
      "i,j log m(θi,j|0, κ) encourages the wi vectors to be\n",
      "collinear so to keep the generation of coarse-to-ﬁne structures coherent.\n",
      "3\n",
      "experiments and results\n",
      "3.1\n",
      "dataset and preprocessing\n",
      "manifold learning.\n",
      "we focused\n",
      "ct scans on the head and neck region above shoulders, with a resolution of\n",
      "80 × 96 × 112, and centered on the mouth after automatic segmentation using a\n",
      "pre-trained u-net [22].\n",
      "planning ct scans were obtained for dose preparation, and cbct\n",
      "scans were obtained at each treatment fraction for positioning with full gantry\n",
      "acquisition.\n",
      "3 and the supplementary material, all these\n",
      "cases are challenging as there are large changes between the original ct scan\n",
      "and the cbct scans.\n",
      "2.3.\n",
      "3.2\n",
      "implementation details\n",
      "manifold learning.\n",
      "we used pytorch to implement our model, based on style-\n",
      "gan2\n",
      "we also\n",
      "used 8 fully-convolutional layers with dimension 512 and an input latent vec-\n",
      "tor of dimension 512, with tanh function as output activation.\n",
      "[15] and style mixing [15], and added a 0.2\n",
      "probability for generating images without gaussian noise to focus on embedding\n",
      "the most information.\n",
      "we augmented the discriminator with vertical and depth-\n",
      "oriented ﬂips, rotation, scaling, motion blur and gaussian noise at a probability\n",
      "of 0.2.\n",
      "we\n",
      "perform 100 optimization steps starting from the mean of the mapped latent\n",
      "space, which takes 25 s, enabling clinical use.\n",
      "3.3\n",
      "results and discussion\n",
      "manifold learning.\n",
      "we tested our model’s ability to learn the low-dimensional\n",
      "manifold.\n",
      "this may be due to\n",
      "a more complex architecture, discriminator augmentation, or simpler anatomy.\n",
      "we compared our method against the main feed-forward method\n",
      "x2ct-gan [30] and the neural radiance ﬁelds with prior image embedding\n",
      "method nerp\n",
      "method\n",
      "1 projection\n",
      "2 projections\n",
      "psnr (db)↑ ssim↑\n",
      "psnr (db)↑ ssim↑\n",
      "nerp (w/o prior volume) 14.8 (±2.7)\n",
      "0.12 (±0.10)\n",
      "18.4 (±3.8)\n",
      "0.17 (±0.10)\n",
      "nerp (w/ prior volume)\n",
      "22.5 (±3.2)\n",
      "0.29 (±0.07)\n",
      "23.5 (±3.5)\n",
      "0.30 (±0.06)\n",
      "x2ct-gan\n",
      "20.7 (±2.4)\n",
      "0.57 (±0.07)\n",
      "21.8 (±2.5)\n",
      "0.72 (±0.08)\n",
      "ours\n",
      "23.2 (±2.8)\n",
      "0.79 (±0.09) 25.8 (±3.2)\n",
      "0.85 (±0.10)\n",
      "4 projections\n",
      "8 projections\n",
      "nerp (w/o prior volume) 19.9 (±2.6)\n",
      "0.21 (±0.04)\n",
      "20.0 (±2.5)\n",
      "0.23 (±0.05)\n",
      "nerp (w/ prior volume)\n",
      "24.2 (±2.7)\n",
      "0.32 (±0.05)\n",
      "24.9 (±4.9)\n",
      "0.34 (±0.08)\n",
      "ours\n",
      "28.2 (±3.5)\n",
      "0.89 (±0.10) 30.1 (±3.9)\n",
      "0.92 (±0.11)\n",
      "improvements compared to x2ct-gan [30] and have similar constraints to\n",
      "feed-forward methods.\n",
      "additionally, no public implementation is available.\n",
      "to evaluate our method’s performance with biplanar pro-\n",
      "jections, we focused on positioning imaging for radiotherapy.\n",
      "even when initialised with a previous\n",
      "ct volume, nerp often fails to converge to the correct volume and introduces\n",
      "many artifacts when few projections are used.\n",
      "we used quantitative metrics (psnr and ssim) to evaluate reconstruction\n",
      "error and human perception, respectively.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf:\n",
      "multimodal image registration is a challenging but essential\n",
      "step for numerous image-guided procedures.\n",
      "experiments\n",
      "on three diﬀerent datasets demonstrate that our approach generalizes\n",
      "well beyond the training data, yielding a broad capture range even on\n",
      "unseen anatomies and modality pairs, without the need for specialized\n",
      "retraining.\n",
      "keywords: image registration · multimodal · metric learning ·\n",
      "diﬀerentiable · deformable registration\n",
      "1\n",
      "introduction\n",
      "multimodal imaging has become increasingly popular in healthcare due to its\n",
      "ability to provide complementary anatomical and functional information.\n",
      "how-\n",
      "ever, to fully exploit its beneﬁts, it is crucial to perform accurate and robust\n",
      "registration of images acquired from diﬀerent modalities.\n",
      "multimodal image reg-\n",
      "istration is a challenging task due to diﬀerences in image appearance, acquisition\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43999-5 72.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "while simple similarity measures directly based on the images’ intensities\n",
      "such as sum of absolute (l1) or squared (l2) diﬀerences and normalized cross-\n",
      "correlation (ncc)\n",
      "essentially, it abstracts the problem to the\n",
      "statistical concept of information theory and optimizes image-wide alignment\n",
      "statistics.\n",
      "as an alternative to directly assessing similarity on the original images, var-\n",
      "ious groups have proposed to ﬁrst compute intermediate representations, and\n",
      "then align these with conventional l1 or l2 metrics [5,20].\n",
      "[5], which is\n",
      "based on image self-similarity and has with minor adaptations (denoted mind-\n",
      "ssc for self-similarity context) also been applied to us problems [7].\n",
      "yet, such\n",
      "feature descriptors are not expressive enough to cope with complex us artifacts\n",
      "and exhibit many local optima, therefore requiring closer initialization.\n",
      "some of these methods involve the utiliza-\n",
      "tion of convolutional neural networks (cnn) to extract segmentation volumes\n",
      "from the source data, transforming the problem into the registration of label\n",
      "maps\n",
      "it has furthermore been proposed in the past to utilize\n",
      "cnns as a replacement for a similarly metric.\n",
      "in [3,17], the two images being\n",
      "registered are resampled into the same grid in each optimizer iteration, concate-\n",
      "nated and fed into a network for similarity evaluation.\n",
      "while such a measure\n",
      "can directly be integrated into existing registration methods, it still suﬀers from\n",
      "similar limitations in terms of runtime performance and modality dependance.\n",
      "this approach combines ml and classical multimodal image\n",
      "disa: universal multimodal registration\n",
      "763\n",
      "registration techniques in a novel way, avoiding the common limitations of ml\n",
      "approaches: ground truth registration is not required, it is diﬀerentiable and\n",
      "computationally eﬃcient, and generalizes well across anatomies and imaging\n",
      "modalities.\n",
      "2\n",
      "approach\n",
      "we formulate image registration as an optimization problem of a similarity met-\n",
      "ric s between the moving image m and the ﬁxed image f with respect to the\n",
      "parameters α of a spatial transformation tα :\n",
      "tα the deformed image, the optimization target can be\n",
      "expressed in the following way:\n",
      "f(α) =\n",
      "\u0002\n",
      "p∈ω\n",
      "w(p) s(f[p], m ◦ tα[p]),\n",
      "(1)\n",
      "where w(p) is the weight assigned to the point p, s(·, ·) deﬁnes a local similarity\n",
      "and the [·] operator extracts a patch (or a pixel) at a given spatial location.\n",
      "the core idea of our method is to approximate the similarity metric s(p1, p2)\n",
      "of two image patches with a dot product ⟨φ(p1), φ(p2)⟩ where φ(·) is a function\n",
      "that extracts a feature vector, for instance in r16, from its input patch.\n",
      "our experiments show that\n",
      "this assumption (implicitly made also by other descriptors like mind) does not\n",
      "present any practical impediment.\n",
      "our method exhibits a large capture range\n",
      "and can converge over a wide range of rotations and deformations.\n",
      "advantages.\n",
      "in contrast to many existing methods, our approach doesn’t\n",
      "require any ground truth registration and can be trained using patches from\n",
      "unregistered pairs of images.\n",
      "each heatmap shows\n",
      "the similarity of the marked point on the source image to every point in the target\n",
      "image.\n",
      "that the cnn has a negligible computational cost and can generalize well across\n",
      "anatomies and modalities: a single network can be used for all types of images\n",
      "and does not need to be retrained for a new task.\n",
      "3\n",
      "method\n",
      "we train our model to approximate the three-dimensional lc2 similarity, as it\n",
      "showed good performance on a number of tasks, including ultrasound [2,22].\n",
      "in order to reduce the sensitivity on the scale, our target is actually the\n",
      "average lc2 over diﬀerent radiuses of 3, 5, and 7.\n",
      "in order to be consistent with\n",
      "the original implementation of lc2 we use the same weighting function w based\n",
      "on local patch variance.\n",
      "our neural network is trained using patches from the “gold atlas\n",
      "- male pelvis - gentle radiotherapy” [14] dataset, which is comprised of 18\n",
      "patients each with a ct, mr t1, and mr t2 volumes.\n",
      "as lc2 requires the usage of gradient magnitude in one of the\n",
      "modalities, we randomly pick it from either ct or mr.\n",
      "we would like to report that, initially, we also made use of a proprietary\n",
      "dataset including us volumes.\n",
      "we do not use any normalization layer, as this\n",
      "resulted in a reduction in performance.\n",
      "the architecture\n",
      "consists of ten layers and a total of 90,752 parameters, making it notably smaller\n",
      "than many commonly utilized neural networks.\n",
      "augmentation on the training data is used to make the model as robust as\n",
      "possible while leaving the target similarity unchanged.\n",
      "the training converges to\n",
      "an average patch-wise l2 error of 0.0076 on the training set and 0.0083 on the\n",
      "validation set.\n",
      "4\n",
      "experiments and results\n",
      "we present an evaluation of our approach across tasks involving diverse modali-\n",
      "ties and anatomies.\n",
      "notably, the experimental data utilized in our analysis diﬀers\n",
      "signiﬁcantly from our model’s training data in terms of both anatomical struc-\n",
      "tures and combination of modalities.\n",
      "fre is the average of ﬁducial errors in millimeters across all cases, while fre25,\n",
      "fre50, and fre75 refer to the 25th, 50th, and 75th percentiles.\n",
      "method\n",
      "mode\n",
      "avg.\n",
      "fre fre25 fre50 fre75\n",
      "mind-ssc rigid\n",
      "5.05\n",
      "1.69\n",
      "2.20\n",
      "3.31\n",
      "mind-ssc aﬃne 2.01\n",
      "1.44\n",
      "1.84\n",
      "2.29\n",
      "lc2\n",
      "rigid\n",
      "1.71\n",
      "1.31\n",
      "1.56\n",
      "1.72\n",
      "lc2\n",
      "aﬃne 1.73\n",
      "1.32\n",
      "1.67\n",
      "1.89\n",
      "disa-lc2\n",
      "rigid\n",
      "1.82\n",
      "1.37\n",
      "1.65\n",
      "1.80\n",
      "disa-lc2\n",
      "aﬃne 1.74\n",
      "1.33\n",
      "1.58\n",
      "1.73\n",
      "table 2. results on the abdomen mr-ct task of the learn2reg challenge 2021.\n",
      "in all experiments, we use a wilcoxon signed-rank test with p-value\n",
      "10−2 to establish the signiﬁcance of our results.\n",
      "4.3)\n",
      "our method obtains a signiﬁcantly larger capture range, opening new possibilities\n",
      "for tackling this challenging problem.\n",
      "4.1\n",
      "aﬃne registration of brain us-mr\n",
      "in this experiment, we evaluate the performance of diﬀerent methods for\n",
      "estimating aﬃne registration of the retrospective evaluation of cerebral\n",
      "tumors (resect) miccai challenge dataset\n",
      "in conclusion, our experiments demonstrate that the proposed\n",
      "disa-lc2, combined with a simple optimization strategy, is capable of achieving\n",
      "equivalent performance to manually tuned lc2.\n",
      "4.2\n",
      "deformable registration of abdominal mr-ct\n",
      "our second application is the abdomen mr-ct task of the learn2reg challenge\n",
      "2021\n",
      "we\n",
      "estimate dense deformation ﬁelds using the methodology outlined in [6] (without\n",
      "inverse consistency) which ﬁrst estimates a discrete displacement using explicit\n",
      "search and then iteratively enforces global smoothness.\n",
      "segmentation maps of\n",
      "anatomical structures are used to measure the quality of the registration.\n",
      "the hyperparameters of the reg-\n",
      "istration algorithm have been manually optimized for each approach.\n",
      "table 2\n",
      "shows that our method obtains signiﬁcantly better results than mind-scc on\n",
      "the dsc metrics while being not signiﬁcantly better on hd95.\n",
      "4.3\n",
      "deformable registration of abdominal us-ct and us-mr\n",
      "as the most challenging experiment, we ﬁnally use our method to achieve\n",
      "deformable registration of abdominal 3d freehand us to a ct or mr volume.\n",
      "between\n",
      "4 and 9 landmark pairs (vessel bifurcations, liver gland borders, gall bladder,\n",
      "kidney) were manually annotated by an expert.\n",
      "note that this registration problem is much more challenging than the prior\n",
      "two due to diﬃcult ultrasonic visibility in the abdomen, strong deformations,\n",
      "and ambiguous matches of liver vasculature.\n",
      "therefore, to the best of our knowl-\n",
      "edge, these results present a signiﬁcant leap towards reliable and fully automatic\n",
      "fusion, doing away with cumbersome manual landmark placements.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_6.pdf:\n",
      "medical image synthesis is a challenging task due to the\n",
      "scarcity of paired data.\n",
      "several methods have applied cyclegan to lever-\n",
      "age unpaired data, but they often generate inaccurate mappings that shift\n",
      "theanatomy.thisproblemisfurtherexacerbatedwhentheimagesfromthe\n",
      "sourceandtargetmodalitiesareheavilymisaligned.recently,currentmeth-\n",
      "ods have aimed to address this issue by incorporating a supplementary seg-\n",
      "mentation network.\n",
      "extensive experiments demonstrate that maskgan outper-\n",
      "formsstate-of-the-artsynthesismethodsonachallengingpediatricdataset,\n",
      "where mr and ct scans are heavily misaligned due to rapid growth in\n",
      "children.speciﬁcally,maskganexcelsinpreservinganatomicalstructures\n",
      "withouttheneedforexpertannotations.thecodeforthispapercanbefound\n",
      "at https://github.com/hieuphan33/maskgan.\n",
      "mri and ct pro-\n",
      "duce diﬀerent tissue contrast and are often used in tandem to provide comple-\n",
      "mentary information.\n",
      "while mri is useful for visualizing soft tissues (e.g. muscle,\n",
      "acknowledgement: this study was supported by channel 7 children’s research foun-\n",
      "dation of south australia incorporated (crf).\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43999-5 6.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "unfortunately, ct imaging\n",
      "exposes patients to ionizing radiation, which can damage dna and increase\n",
      "cancer risk [9], especially in children and adolescents.\n",
      "given these issues, there\n",
      "are clear advantages for synthesizing anatomically accurate ct data from mri.\n",
      "despite the superior perfor-\n",
      "mance, supervised methods require a large amount of paired data, which is\n",
      "prohibitively expensive to acquire.\n",
      "several unsupervised mri-to-ct synthesis\n",
      "methods [4,6,14], leverage cyclegan with cycle consistency supervision to elim-\n",
      "inate the need for paired data.\n",
      "unfortunately, the performance of unsupervised\n",
      "ct synthesis methods [4,14,15] is inferior to supervised counterparts.\n",
      "due to\n",
      "the lack of direct constraints on the synthetic outputs, cyclegan [20] struggles\n",
      "to preserve the anatomical structure when synthesizing ct images, as shown in\n",
      "fig.\n",
      "this problem is particularly relevant\n",
      "in brain scanning, where both the pixel-wise correlation and noise statistics in\n",
      "mr and ct images are diﬀerent, as a direct consequence of the signal acqui-\n",
      "sition technique.\n",
      "the alternative shape-wise consistency methods [3,4,19] aim\n",
      "58\n",
      "v. m. h. phan et al.\n",
      "to preserve the shapes of major body parts in the synthetic image.\n",
      "notably,\n",
      "shape-cyclegan [4] segments synthesized ct and enforces consistency with the\n",
      "ground-truth mri segmentation.\n",
      "however, these methods rely on segmentation\n",
      "annotations, which are time-consuming, labor-intensive, and require expert radi-\n",
      "ological annotators.\n",
      "a recent natural image synthesis approach, called attention-\n",
      "gan [12], learns attention masks to identify discriminative structures.\n",
      "atten-\n",
      "tiongan implicitly learns prominent structures in the image without using the\n",
      "ground-truth shape.\n",
      "comparisons of diﬀerent shape-aware image synthesis.\n",
      "method\n",
      "mask\n",
      "supervision\n",
      "human\n",
      "annotation\n",
      "structural\n",
      "consistency\n",
      "shape-cyclegan [4]\n",
      "precise mask\n",
      "yes\n",
      "yes\n",
      "attentiongan [12]\n",
      "not required\n",
      "no\n",
      "no\n",
      "maskgan (ours) coarse mask\n",
      "no\n",
      "yes\n",
      "in this paper, we propose maskgan, a novel unsupervised mri-to-ct\n",
      "synthesis method, that preserves the anatomy under the explicit supervision of\n",
      "coarse masks without using costly manual annotations.\n",
      "unlike segmentation-\n",
      "based methods [4,18], maskgan bypasses the need for precise annotations,\n",
      "replacing them with standard (unsupervised) image processing techniques, which\n",
      "can produce coarse anatomical masks.\n",
      "such masks, although imperfect, provide\n",
      "suﬃcient cues for maskgan to capture anatomical outlines and produce struc-\n",
      "turally consistent images.\n",
      "maskgan is the ﬁrst framework that maintains shape consis-\n",
      "tency without relying on human-annotated segmentation.\n",
      "3) extensive experiments show that our\n",
      "method outperforms state-of-the-art methods by using automatically extracted\n",
      "coarse masks to eﬀectively enhance structural consistency.\n",
      "the cycle shape consistency (csc) loss lcsc minimizes the l1\n",
      "distance between the masks learned by the mri and ct generators, promoting con-\n",
      "sistent anatomy segmentation across domains.\n",
      "intuitively, each channel ai in the mask tensor a focuses on diﬀerent anatom-\n",
      "ical structures in the medical image.\n",
      "2.2\n",
      "cyclegan supervision\n",
      "the two generators, gmr and gct , map images from mri domain (x) and\n",
      "ct domain (y ), respectively.\n",
      "two discriminators, dmr and dct , are used to\n",
      "distinguish real from fake images in the mri and ct domains.\n",
      "the adversarial\n",
      "loss for training the generators to produce synthetic ct images is deﬁned as\n",
      "lct(gmr, dct, x, y) = ey∼pdata(y)\n",
      "\u0003\n",
      "log dct(y)\n",
      "\u0004\n",
      "+ ex∼pdata(x)\n",
      "\u0003\n",
      "log(1 − dct(gmr(x)))\n",
      "\u0004\n",
      ".\n",
      "(2)\n",
      "the adversarial loss lmr for generating mri images is deﬁned in a similar\n",
      "manner.\n",
      "we extract the coarse mask b using\n",
      "basic image processing operations.\n",
      "speciﬁcally, we design a simple but robust\n",
      "algorithm that works on both mri and ct scans, with a binarization stage\n",
      "followed by a reﬁnement step.\n",
      "in the binarization stage, we normalize the inten-\n",
      "sity to the range\n",
      "in the\n",
      "post-processing stage, we reﬁne the binary image using morphological operations,\n",
      "speciﬁcally employing a binary opening operation to remove small artifacts.\n",
      "we introduce a novel mask supervision loss that penalizes the diﬀerence\n",
      "between the background mask an learned from the input image and the ground-\n",
      "truth background mask b in both mri and ct domains.\n",
      "previous shape-aware methods [4,18] use a pre-trained u-net [10]\n",
      "segmentation network to enforce shape consistency on the generator.\n",
      "u-net is\n",
      "pre-trained in a separate stage and frozen when the generator is trained.\n",
      "hence,\n",
      "structure-preserving synthesis: maskgan for unpaired mr-ct translation\n",
      "61\n",
      "any errors produced by the segmentation network cannot be corrected.\n",
      "our loss penalizes the discrepancy between the background atten-\n",
      "tion mask amr\n",
      "n\n",
      "learned from the input mri image and the mask ˜act\n",
      "n\n",
      "learned\n",
      "from synthetic\n",
      "3\n",
      "experimental results\n",
      "3.1\n",
      "experimental settings\n",
      "data collection.\n",
      "we targeted the age group from 6–24 months\n",
      "since pediatric patients are more susceptible to ionizing radiation and experience\n",
      "a greater cancer risk (up to 24% increase) from radiation exposure [7]. further-\n",
      "more, surgery for craniosynostosis, a birth defect in which the skull bones fuse\n",
      "too early, typically occurs during this age [5,16].\n",
      "following [13],\n",
      "we conducted experiments on sagittal slices.\n",
      "all models are trained using\n",
      "1 ethics approval was granted by southern adelaide clinical human research ethics\n",
      "committee.\n",
      "to provide a quantitative evaluation of methods, we com-\n",
      "pute the same standard performance metrics as in previous works\n",
      "the scope of the\n",
      "paper centers on theoretical development; clinical evaluations such as dose cal-\n",
      "culation and treatment planning will be conducted in future work.\n",
      "3.2\n",
      "results and discussions\n",
      "comparisons with state-of-the-art.\n",
      "we compare the performance of our\n",
      "proposed maskgan with existing state-of-the-art image synthesis methods,\n",
      "including cyclegan [20], attentiongan [12], structure-constrained cyclegan\n",
      "(sc-cyclegan)\n",
      "[4]. shape-cyclegan requires anno-\n",
      "tated segmentation to train a separate u-net.\n",
      "for a fair comparison, we imple-\n",
      "ment shape-cyclegan using our extracted coarse masks based on the authors’\n",
      "oﬃcial code.\n",
      "as better mri synthesis leads to improved\n",
      "ct synthesis, we also report the model’s performance on mri synthesis.\n",
      "the improvement of maskgan over all compared methods is statistically signiﬁcant.\n",
      "unlike shape-cyclegan, which underperforms when trained with coarse\n",
      "segmentations, our method obtains consistently higher results.\n",
      "3. visual comparison of synthesized ct images by diﬀerent methods on two sam-\n",
      "ples.\n",
      "under this challenging setting, unpaired image syn-\n",
      "thesis can have non-optimal visual results and ssim scores.\n",
      "yet, our maskgan\n",
      "achieves the highest quality, indicating its suitability for pediatric image synthe-\n",
      "sis.\n",
      "the combination of both mask and\n",
      "cycle shape consistency losses results in the largest improvement, demonstrating\n",
      "the complementary contributions of our two losses.\n",
      "we compare the performance\n",
      "of our approach with shape-cyclegan [4] using deformed masks that simulate\n",
      "human errors during annotation.\n",
      "to alter object shapes, we employ random\n",
      "elastic deformation, a standard data augmentation technique [10] that applies\n",
      "random displacement vectors to objects.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_5.pdf:\n",
      "multi-sequence mri is valuable in clinical settings for reli-\n",
      "able diagnosis and treatment prognosis, but some sequences may be unus-\n",
      "able or missing for various reasons.\n",
      "recent deep learning-based methods have achieved\n",
      "good performance in combining multiple available sequences for missing\n",
      "sequence synthesis.\n",
      "despite their success, these methods lack the ability\n",
      "to quantify the contributions of diﬀerent input sequences and estimate\n",
      "region-speciﬁc quality in generated images, making it hard to be practical.\n",
      "hence, we propose an explainable task-speciﬁc synthesis network, which\n",
      "adapts weights automatically for speciﬁc sequence generation tasks and\n",
      "provides interpretability and reliability from two sides: (1) visualize and\n",
      "quantify the contribution of each input sequence in the fusion stage by\n",
      "a trainable task-speciﬁc weighted average module; (2) highlight the area\n",
      "the network tried to reﬁne during synthesizing by a task-speciﬁc atten-\n",
      "tion module.\n",
      "we conduct experiments on the brats2021 dataset of 1251\n",
      "subjects, and results on arbitrary sequence synthesis indicate that the pro-\n",
      "posed method achieves better performance than the state-of-the-art meth-\n",
      "ods.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43999-5 5.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "[16], lesion segmentation [17], treatment prognosis\n",
      "however, some\n",
      "acquired sequences are unusable or missing in clinical settings due to incorrect\n",
      "machine settings, imaging artifacts, high scanning costs, time constraints, con-\n",
      "trast agents allergies, and diﬀerent acquisition protocols between hospitals [5].\n",
      "without rescanning or aﬀecting the downstream pipelines, the mri synthesis\n",
      "technique can generate missing sequences by leveraging redundant shared infor-\n",
      "mation between multiple sequences\n",
      "many studies have demonstrated the potential of deep learning methods for\n",
      "image-to-image synthesis in the ﬁeld of both nature images [8,11,12] and medical\n",
      "images [2,13,19].\n",
      "most of these works introduce an autoencoder-like architecture\n",
      "for image-to-image translation and employ adversarial loss to generate more\n",
      "realistic images.\n",
      "unlike these one-to-one approaches, mri synthesis faces the\n",
      "challenge of fusing complementary information from multiple input sequences.\n",
      "recent studies about multi-sequence fusion can speciﬁcally be divided into two\n",
      "groups: (1) image fusion and (2) feature fusion.\n",
      "the image fusion approach is\n",
      "to concatenate sequences as a multi-channel input.\n",
      "image-level fusion is\n",
      "simple and eﬃcient but unstable – zero-padding inputs for missing sequences\n",
      "lead to training unstable and slight misalignment between images can easily\n",
      "cause artifacts.\n",
      "in contrast, eﬀorts have been made on feature fusion, which can\n",
      "alleviate the discrepancy across multiple sequences, as high-level features focus\n",
      "on the semantic regions and are less aﬀected by input misalignment compared\n",
      "to images.\n",
      "more importantly, recent studies only focus on\n",
      "proposing end-to-end models, lacking quantifying the contributions for diﬀerent\n",
      "sequences and estimating the qualities of generated images.\n",
      "specially, this framework\n",
      "can be easily extended to other tasks, such as segmentation.\n",
      "our primary con-\n",
      "tributions are as follows: (1) we propose a ﬂexible network to synthesize the\n",
      "target mri sequence from an arbitrary combination of inputs; (2) the network\n",
      "explainable task-speciﬁc fusion network\n",
      "47\n",
      "e\n",
      "e\n",
      "e\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "placeholder\n",
      "flair\n",
      "t2\n",
      "t1\n",
      "concat\n",
      "task-specific code\n",
      "task-specific attention\n",
      "g\n",
      "tsem\n",
      "t1gd\n",
      "task-specific \n",
      "weighted average\n",
      "code\n",
      "spatial \n",
      "attention\n",
      "channel \n",
      "attention\n",
      "fig.\n",
      "finally, the fused features are decoded to the\n",
      "target sequence by g. furthermore, to explain the mechanism of multi-sequence\n",
      "fusion, our network can quantify the contributions of diﬀerent input sequences\n",
      "with the task-speciﬁc weighted average module and visualize the tsem with\n",
      "the task-speciﬁc attention module.\n",
      "to leverage shared information between sequences, we use e and g from\n",
      "seq2seq [10], which is a one-to-one synthetic model that integrates arbitrary\n",
      "sequence synthesis into single e and g. they can reduce the distance between\n",
      "diﬀerent sequences at the feature level to help more stable fusion.\n",
      "to fuse mul-\n",
      "tiple sequences at the feature level, we ﬁrst encode images and concatenate the\n",
      "features as ⃗f = {e(xi)|i = 1, ..., n}.\n",
      "the multi-sequence fusion module includes: (1)\n",
      "a task-speciﬁc weighted average module for the linear combination of available\n",
      "features; (2) a task-speciﬁc attention module to reﬁne the fused features.\n",
      "task-speciﬁc weighted average.\n",
      "the weighted average is an intuitive fusion\n",
      "strategy that can quantify the contribution of diﬀerent sequences directly.\n",
      "+ ϵ\n",
      "(1)\n",
      "where w and b are weights and bias for the fc layer, ϵ = 10−5 to avoid dividing\n",
      "0 in the following equation.\n",
      "ω = ω0 · csrc\n",
      "⟨ω0, csrc⟩\n",
      "(2)\n",
      "where · refers to the element-wise product and ⟨·, ·⟩ indicates the inner product.\n",
      "it\n",
      "demonstrates that the designed ω can help the network excellently inherit the\n",
      "synthesis performance of pre-trained e and g. in this work, we use ω to quantify\n",
      "the contribution of diﬀerent input combinations.\n",
      "task-speciﬁc attention.\n",
      "1, channel attention and spatial attention can provide adap-\n",
      "tive feature reﬁnement guided by the task-speciﬁc code c to generate residual\n",
      "attentional fused features fa.\n",
      "λr and\n",
      "λp are weight terms and are experimentally set to be 10 and 0.01.\n",
      "2.2\n",
      "task-speciﬁc enhanced map\n",
      "as fa is a task-speciﬁc contextual reﬁnement for fused features, analyzing it can\n",
      "help us understand more what the network tried to do.\n",
      "many studies focus on\n",
      "visualizing the attention maps to interpret the principle of the network, especially\n",
      "for the transformer modules [1,6].\n",
      "thus, we proposed the\n",
      "tsem by subtracting the reconstructed target sequences with and without fa,\n",
      "which has the same resolution as the original images and clear interpretation for\n",
      "speciﬁc tasks.\n",
      "tsem = |x′\n",
      "a − x′|\n",
      "(6)\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "dataset and evaluation metrics\n",
      "we use brain mri images of 1,251 subjects from brain tumor segmentation 2021\n",
      "(brats2021)\n",
      "all the images are intensity normalized to [−1, 1] and central\n",
      "cropped to 128×192×192.\n",
      "the synthesis performance is quantiﬁed using the metrics of peak signal noise\n",
      "rate (psnr), structural similarity index measure (ssim), and learned perceptual\n",
      "image patch similarity (lpips)\n",
      "[21], which evaluate from intensity, structure,\n",
      "and perceptual aspects.\n",
      "3.2\n",
      "implementation details\n",
      "the models are implemented with pytorch and trained on the nvidia geforce\n",
      "rtx 3090 ti gpu.\n",
      "the initial convolutional layer is responsible for encoding intensities to\n",
      "features, while the second and third convolutional layers downsample images by a\n",
      "factor of four.\n",
      "[12] (average)\n",
      "26.2 ± 2.7\n",
      "0.834 ± 0.054\n",
      "15.84 ± 6.05\n",
      "mm-gan [18]\n",
      "28.0 ± 2.3\n",
      "0.878 ± 0.037\n",
      "10.33 ± 3.58\n",
      "diamondgan\n",
      "[10] (average) 28.2 ± 2.2\n",
      "0.879 ± 0.035\n",
      "11.11 ± 3.72\n",
      "tsf-seq2seq (w/o fa) 28.0 ± 2.4\n",
      "0.875 ± 0.039\n",
      "9.89 ± 3.63\n",
      "tsf-seq2seq\n",
      "28.3 ± 2.4 0.882 ± 0.038 9.48 ± 3.58\n",
      "3\n",
      "pix2pix\n",
      "[12] (average)\n",
      "26.6 ± 2.5\n",
      "0.842 ± 0.041\n",
      "15.77 ± 5.08\n",
      "mm-gan\n",
      "[10] (average) 28.5 ± 2.3\n",
      "0.880 ± 0.038\n",
      "11.61 ± 3.87\n",
      "tsf-seq2seq (w/o fa) 28.3 ± 2.6\n",
      "0.876 ± 0.044\n",
      "9.61 ± 4.00\n",
      "tsf-seq2seq\n",
      "28.8 ± 2.6 0.887 ± 0.042\n",
      "the e\n",
      "and g from seq2seq are pre-trained using the adam optimizer with an initial\n",
      "learning rate of 2 × 10−4 and a batch size of 1 for 1,000,000 steps, taking about\n",
      "60 h. then we ﬁnetune the tsf-seq2seq with the frozen e using the adam\n",
      "optimizer with an initial learning rate of 10−4 and a batch size of 1 for another\n",
      "300,000 steps, taking about 40 h.\n",
      "3.3\n",
      "quantitative results\n",
      "we compare our method with one-to-one translation, image-level fusion,\n",
      "and feature-level fusion methods.\n",
      "[10]. image-level fusion methods consist of mm-\n",
      "gan [18], diamondgan\n",
      "table 1 reports the sequence synthesis performance for comparison meth-\n",
      "ods organized by the diﬀerent numbers of input combinations.\n",
      "note that, for\n",
      "multiple inputs, one-to-one translation methods synthesize multiple outputs sep-\n",
      "arately and average them as one.\n",
      "as shown\n",
      "in table 1, the proposed method achieves the best performance in diﬀerent input\n",
      "combinations.\n",
      "3.4\n",
      "ablation study\n",
      "we compare two components of our method, including (1) task-speciﬁc weighted\n",
      "average and (2) task-speciﬁc attention, by conducting an ablation study between\n",
      "seq2seq, tsf-seq2seq (w/o fa), and tsf-seq2seq.\n",
      "as shown in\n",
      "table 1, when only one sequence is available, our method can inherit the perfor-\n",
      "mance of seq2seq and achieve slight improvements.\n",
      "for multi-input situations,\n",
      "the task-speciﬁc weighted average can decrease lpips to achieve better percep-\n",
      "tual performance.\n",
      "3.5\n",
      "interpretability visualization\n",
      "the proposed method not only achieves superior synthesis performance but also\n",
      "has good interpretability.\n",
      "3, both t1 and t1gd contribute greatly to\n",
      "the sequence synthesis of each other, which is expected because t1gd are t1-\n",
      "weighted scanning after contrast agent injection, and the enhancement between\n",
      "these two sequences is indispensable for cancer detection and diagnosis.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf:\n",
      "the computed tomography (ct) for diagnosis of lesions in\n",
      "human internal organs is one of the most fundamental topics in med-\n",
      "ical imaging.\n",
      "the proposed method can capture the local\n",
      "coherence of adjacent images by optical ﬂow, which yields signiﬁcant\n",
      "improvements in the precision and stability of the constructed images.\n",
      "we evaluate our proposed method on real datasets and the experimental\n",
      "results suggest that it can outperform existing state-of-the-art recon-\n",
      "struction approaches signiﬁcantly.\n",
      "keywords: ct reconstruction · low-dose · generative adversarial\n",
      "networks · local coherence · optical ﬂow\n",
      "1\n",
      "introduction\n",
      "computed tomography (ct) is one of the most widely used technologies in\n",
      "medical imaging, which can assist doctors for diagnosing the lesions in human\n",
      "internal organs.\n",
      "however, when the\n",
      "dose is low together with the issues like sparse-view or limited angles, it becomes\n",
      "quite challenging to reconstruct high-quality ct images.\n",
      "the high-quality ct\n",
      "images are important to improve the performance of diagnosis in clinic [27].\n",
      "+ δ,\n",
      "(1)\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43999-5 50.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al. (eds.): miccai 2023, lncs 14229, pp.\n",
      "https://doi.org/10.1007/978-3-031-43999-5_50\n",
      "solving low-dose ct reconstruction via gan with local coherence\n",
      "525\n",
      "where xr ∈ rd denotes the unknown ground-truth picture, y ∈ rm denotes\n",
      "the received measurement, and δ is the noise.\n",
      "[11] can produce serious detrimental artifact.\n",
      "another popular line\n",
      "for learning the regularizers comes from deep learning [13,17]; the advantage\n",
      "of the deep learning methods is that they can achieve an end-to-end recov-\n",
      "ery of the true image xr from the measurement y\n",
      "recent researches\n",
      "reveal that convolutional neural networks (cnns) are quite eﬀective for image\n",
      "denoising, e.g., the cnn based algorithms\n",
      "[10,34] can directly learn the recon-\n",
      "structed mapping from initial measurement reconstructions (e.g., fbp) to the\n",
      "ground-truth images.\n",
      "the dual-domain network that combines the sinograms\n",
      "with reconstructed low-dose ct images were also proposed to enhance the gen-\n",
      "eralizability\n",
      "a major drawback of the aforementioned reconstruction methods is that they\n",
      "deal with the input ct 2d slices independently (note that the goal of ct recon-\n",
      "struction is to build the 3d model of the organ).\n",
      "namely, the neighborhood\n",
      "correlations among the 2d slices are often ignored, which may aﬀect the recon-\n",
      "struction performance in practice.\n",
      "in the ﬁeld of computer vision, “optical ﬂow”\n",
      "is a common technique for tracking the motion of object between consecutive\n",
      "frames, which has been applied to many diﬀerent tasks like video generation [35],\n",
      "prediction of next frames [22] and super resolution synthesis [5,31]. to estimate\n",
      "the optical ﬂow ﬁeld, existing approaches include the traditional brightness gra-\n",
      "dient methods [2] and the deep networks [7].\n",
      "the idea of optical ﬂow has also\n",
      "been used for tracking the organs movement in medical imaging [16,20,33].\n",
      "when a patient is\n",
      "located in a ct equipment, a set of consecutive cross-sectional images are gen-\n",
      "erated.\n",
      "so we apply optical ﬂow, though\n",
      "there exist several technical issues waiting to solve for the design and imple-\n",
      "mentation, to capture the local coherence of adjacent ct images for reducing\n",
      "the artifacts in low-dose ct reconstruction.\n",
      "we introduce the “local coherence” by characterizing the correlation of con-\n",
      "secutive ct images, which plays a key role for suppressing the artifacts.\n",
      "2. together with the local coherence, our proposed generative adversarial net-\n",
      "works (gans) can yield signiﬁcant improvement for texture quality and sta-\n",
      "bility of the reconstructed images.\n",
      "3. to illustrate the eﬃciency of our proposed approach, we conduct rigorous\n",
      "experiments on several real clinical datasets; the experimental results reveal\n",
      "the advantages of our approach over several state-of-the-art ct reconstruc-\n",
      "tion methods.\n",
      "as mentioned in sect.\n",
      "1, optical ﬂow can capture the tem-\n",
      "poral coherence of object movements, which plays a crucial role in many video-\n",
      "related tasks.\n",
      "[9].\n",
      "solving low-dose ct reconstruction via gan with local coherence\n",
      "527\n",
      "based on these assumptions, the brightness of optical ﬂow can be described by\n",
      "the following equation:\n",
      "∇iw · vw + ∇ih · vh + ∇it = 0,\n",
      "(4)\n",
      "where v = (vw, vh) represents the optical ﬂow of the position (w, h) in the image.\n",
      "∇i = (∇iw, ∇ih) denotes spatial gradients of image brightness, and ∇it denotes\n",
      "the temporal partial derivative of the corresponding region.\n",
      "in practice, the brightness of\n",
      "adjacent ct images often has very tiny diﬀerence, due to the inherent continu-\n",
      "ity and structural integrity of human body.\n",
      "therefore, we introduce the “local\n",
      "coherence” that indicates the correlation between adjacent images of a tissue.\n",
      "namely, adjacent ct images often exhibit signiﬁcant similarities within a certain\n",
      "local range along the vertical axis of the human body.\n",
      "the scanning window of x-\n",
      "ray slides from the position of the left image to the position of the right image.\n",
      "the left and\n",
      "right images share the local coherence and thus the optical ﬂows are small.\n",
      "(color\n",
      "ﬁgure online)\n",
      "3\n",
      "gans with local coherence\n",
      "in this section, we introduce our low-dose ct image generation framework with\n",
      "local coherence in detail.\n",
      "the proposed framework comprises three\n",
      "components, including a generator g, a discriminator d and an optical ﬂow esti-\n",
      "mator f. the generator is the core component, and the ﬂow estimator provides\n",
      "auxiliary warping images for the generation process.\n",
      "suppose we have a sequence of measurements y1, y2, · · · , yn; for each yi,\n",
      "1 ≤\n",
      "i ≤ n, we want to reconstruct its ground truth image xr\n",
      "i as the eq.\n",
      "before\n",
      "performing the reconstruction in the generator g, we apply some prior knowledge\n",
      "in physics and run ﬁlter backward projection on the measurement yi in eq.\n",
      "then the network has two input components,\n",
      "i.e., the initial backward projected image si that serves as an approximation of\n",
      "the ground truth xr\n",
      "i , and a set of neighbor ct slices n(si) = {si−1, si+1}1 for\n",
      "preserving the local coherence.\n",
      "the discriminator d assigns the label “1” to real standard-\n",
      "dose ct images and “0” to generated images.\n",
      "the goal of d is to maximize the\n",
      "separation between the distributions of real images and generated images:\n",
      "1 if i = 1, n(si) = {s2}; if i = n, n(si) = {sn−1}.\n",
      "+ log(1 − d(xg\n",
      "i ))),\n",
      "(5)\n",
      "where xg\n",
      "i is the image generated by g (the formal deﬁnition for xg\n",
      "i will be intro-\n",
      "duced below).\n",
      "we use the generator g to reconstruct the high-quality ct image\n",
      "for the ground truth xr\n",
      "i from the low-dose image si.\n",
      "the generated image is\n",
      "obtained by\n",
      "xg\n",
      "i = g(si, w(n(xg\n",
      "i )));\n",
      "n(xg\n",
      "i ) = g(n(si)),\n",
      "(6)\n",
      "where w(·) is the warping operator.\n",
      "sub-\n",
      "sequently, according to the optical ﬂow f(n(si), si), we warp the reconstructed\n",
      "images n(xg\n",
      "i ) to align with the current slice by adjusting the brightness values.\n",
      "(7)\n",
      "in (7), “lpixel” is the loss measuring the pixel-wise mean square error of the\n",
      "generated image xg\n",
      "i with respect to the ground-truth xr\n",
      "i .\n",
      "“ladv” represents the\n",
      "adversarial loss of the discriminator d, which is designed to minimize the dis-\n",
      "tance between the generated standard-dose ct image distribution pxg and the\n",
      "real standard-dose ct image distribution px.\n",
      "through capturing the high frequency diﬀerences in ct images, lpercept can\n",
      "enhance the sharpness for edges and increase the contrast for the reconstructed\n",
      "images.\n",
      "530\n",
      "w. liu and h. ding\n",
      "4\n",
      "experiment\n",
      "datasets.\n",
      "first, our proposed approaches are evaluated on the “mayo-clinic\n",
      "low-dose ct grand challenge” (mayo-clinic) dataset of lung ct images [19].\n",
      "the dataset contains 2250 two dimensional slices from 9 patients for training,\n",
      "and the remaining 128 slices from 1 patient are reserved for testing.\n",
      "the low-\n",
      "dose measurements are simulated by parallel-beam x-ray with 200 (or 150) uni-\n",
      "in order to further verify the denoising ability of our\n",
      "approaches, we add the gaussian noise with standard deviation σ = 2.0 to the\n",
      "sinograms after x-ray projection in 50% of the experiments.\n",
      "the proposed networks were implemented in the pytorch\n",
      "framework and trained on nvidia 3090 gpu with 100 epochs.\n",
      "baselines and evaluation metrics.\n",
      "following most of the previous articles on\n",
      "3d ct reconstruction, we evaluate the experimental performance by two met-\n",
      "rics: the peak signal-to-noise ratio (psnr) and the structural similarity index\n",
      "(ssim)\n",
      "psnr measures the pixel diﬀerences of two images, which is nega-\n",
      "tively correlated with mean square error.\n",
      "ssim measures the structure similarity\n",
      "between two images, which is related to the variances of the input images.\n",
      "the\n",
      "methods fbp and uar are very sensitive to noise; the performance of lpd\n",
      "is relatively stable but with low reconstruction accuracy.\n",
      "experimental results for mayo-clinic dataset.\n",
      "experimental results for rider dataset.\n",
      "due to the bias in the\n",
      "datasets collected from diﬀerent facilities, the performances of all the models are\n",
      "declined to some extents.\n",
      "to illustrate the reconstruction performances more clearly, we also show the\n",
      "reconstruction results for testing images in fig.\n",
      "3. we can see that our network\n",
      "can reconstruct the ct image with higher quality.\n",
      "due to the space limit, the\n",
      "experimental results of diﬀerent views nv and more visualized results are placed\n",
      "in our supplementary material.\n",
      "“ground truth” is the standard-dose ct\n",
      "image.\n",
      "by considering the inherent\n",
      "continuity of human body, local coherence can be captured through optical ﬂow,\n",
      "which is small deformations and structural diﬀerences between consecutive ct\n",
      "slices.\n",
      "the experimental results on real datasets demonstrate the advantages\n",
      "of our proposed network over several popular approaches.\n",
      "in future, we will\n",
      "evaluate our network on real-world ct images from local hospital and use the\n",
      "reconstructed images to support doctors for the diagnosis and recognition of lung\n",
      "nodules.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf:\n",
      "respiratory correlated cone beam computed tomography\n",
      "(4dcbct) is a technique used to address respiratory motion artifacts\n",
      "that aﬀect reconstruction quality, especially for the thorax and upper-\n",
      "abdomen.\n",
      "4dcbct sorts the acquired projection images in multiple\n",
      "respiratory correlated bins.\n",
      "this technique results in the emergence of\n",
      "aliasing artifacts caused by the low number of projection images per bin,\n",
      "which severely impacts the image quality and limits downstream use.\n",
      "using a funda-\n",
      "mental property of the fdk reconstruction algorithm, and prior results\n",
      "from the literature, we prove mathematically the ability of the method\n",
      "to work and specify the underlying assumptions.\n",
      "we apply the method to a public dataset and to an in-house dataset\n",
      "and show that it matches the performance of a supervised approach and\n",
      "outperforms it when measurement noise is present in the data.\n",
      "image\n",
      "guided rt (igrt) is a technique to capture the anatomy of the day using in\n",
      "room imaging in order to align the treatment beam with the tumor location [1].\n",
      "cone beam ct (cbct) is the most widely used imaging modality for igrt.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43999-5 46.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "https://doi.org/10.1007/978-3-031-43999-5_46\n",
      "482\n",
      "s. papa et al.\n",
      "a major challenge especially for cbct imaging of the thorax and upper-\n",
      "abdomen is the respiratory motion that introduces blurring of the anatomy,\n",
      "reducing the localization accuracy and the sharpness of the image.\n",
      "however, only 20 to 60 respiratory peri-\n",
      "ods are imaged.\n",
      "additionally, the projections are aﬀected by stochastic mea-\n",
      "surement noise caused by the ﬁnite imaging dose used, which further degrades\n",
      "the quality of the reconstruction even when all projections are used.\n",
      "however, the method cannot reduce measurement\n",
      "noise because it is still present in the images used as targets during training.\n",
      "a diﬀerent method, called noise2inverse, uses an unsupervised approach to\n",
      "reduce measurement noise in the traditional ct setting [4].\n",
      "in this case, the motion artifacts that blur the image will appear\n",
      "again, as noise2inverse requires averaging the sub-reconstructions to obtain a\n",
      "clean reconstruction.\n",
      "we explore diﬀerent dataset sizes to understand their eﬀects on the\n",
      "reconstructed images.\n",
      "2\n",
      "theoretical background\n",
      "in this section, we will introduce the concepts and the notation necessary to\n",
      "understand the method and the choices made during implementation.\n",
      "given input-target pairs x, y ∈\n",
      "r we can deﬁne the regression problem in the one-dimensional setting as ﬁnding\n",
      "noise2aliasing\n",
      "483\n",
      "f ∗ : r → r which satisﬁes the following:\n",
      "f ∗ = arg min\n",
      "f\n",
      "ex,y\n",
      "\u0002\n",
      "∥f(x)\n",
      "[5], input-target pairs are two samples of the same image that\n",
      "only diﬀer because of some independent mean-zero noise (x + δ1, x + δ2) with\n",
      "eδ2\n",
      "[x + δ2|x + δ1] = x. then f ∗ will recover the input image without any noise:\n",
      "f ∗(x + δ1) =\n",
      "during a ct scan, a volume x\n",
      "is imaged by acquiring projections y = ax using an x-ray source and a detector\n",
      "placed on the opposite side of the volume.\n",
      "− ˆxj∥2\n",
      "2 + e∥ˆxj − ˜xj∥2\n",
      "2,\n",
      "(4)\n",
      "where j is a random variable that picks subsets of projections at random and j′\n",
      "is its complementary.\n",
      "given eq. 2, we observe that function f ∗ which minimizes l is:\n",
      "f ∗(˜xj′)\n",
      "(5)\n",
      "when using reconstructions from a subset of noisy projections as input and\n",
      "reconstructions from their complementary as its output, a neural network will\n",
      "learn to predict the expected reconstruction without the noise.\n",
      "property of expectation over subsets of projections using fdk.\n",
      "[2] that reconstructs a volume of dimensionality\n",
      "dv from projections j each with dimensionality dd (geometrical details on the\n",
      "exact setup are not relevant).\n",
      "the fdk uses, as its fundamental step, the dual\n",
      "484\n",
      "s. papa et al.\n",
      "radon transform\n",
      "}, the fdk reconstruction\n",
      "algorithm r, and the noisy projections ˜y = ax+ϵ with ϵ mean-zero element-wise\n",
      "independent noise.\n",
      "− ˜xj1∥2\n",
      "2.\n",
      "(8)\n",
      "additionally, j1, j2 are disjoint, the noise is mean-zero element-wise, and we\n",
      "are using the fdk reconstruction algorithm which deﬁnes a linear operator r.\n",
      "these allow us to use eq. 5 to ﬁnd that the function f ∗ that minimizes l is the\n",
      "following:\n",
      "f ∗(˜xj) = ej1,j2(ˆxj1|˜xj2 = ˜xj).\n",
      "(9)\n",
      "this is suﬃcient to reduce stochastic noise but we need to further manipulate this\n",
      "expression to address view aliasing.\n",
      "the reconstructions will dis-\n",
      "play organs in their average position and, therefore, have the same underlying\n",
      "structure.\n",
      "coincidentally, a previously proposed subset selection\n",
      "method utilized for supervised aliasing reduction ﬁts all these requirements and\n",
      "will, therefore, be used in this work [4].\n",
      "4\n",
      "experiments\n",
      "first, we used the spare varian dataset to study whether noise2aliasing can\n",
      "match the performance of the supervised baseline and if it can outperform it\n",
      "when adding noise to the projections.\n",
      "then, we use the internal dataset to explore\n",
      "the requirements for the method to be applied to an existing clinical dataset.\n",
      "the projections obtained during\n",
      "a scan are sub-sampled according to the pseudo-average subset selection method\n",
      "described in [6] and then used to obtain 3d reconstructions.\n",
      "given two volumes\n",
      "(x, y), the training pairs (xi(k), yi(k)) are the same i-th slice along the k-th dimen-\n",
      "sion of each volume chosen to be the axial plane.\n",
      "the spare varian dataset was used to provide performance results on pub-\n",
      "licly available patient data.\n",
      "to more closely resemble normal respiratory\n",
      "motion per projection image, the 8 min scan has been used from each patient\n",
      "(ﬁve such scans are available in the dataset).\n",
      "the msd makes use of\n",
      "dilated convolutions to process features at all scales of the image.\n",
      "in the\n",
      "supervised approach, the model is trained by using as input reconstructions\n",
      "obtained from subsets deﬁned with pseudo-average subset selection while the\n",
      "targets use all of the projections available.\n",
      "inference speed with the nvidia a100 gpu averages 600ms\n",
      "per volume made of 220 slices.\n",
      "in the low-\n",
      "noise setting, both supervised and noise2aliasing outperform fdk with very\n",
      "similar results, often within a single standard deviation.\n",
      "noise2aliasing successfully matches the performance of the supervised base-\n",
      "line.\n",
      "noise2aliasing and the supervised method produce very similar images\n",
      "in the low-noise case.\n",
      "from fig. 1 and table 1, the supervised approach repro-\n",
      "duces the noise that was seen during training, while noise2aliasing manages to\n",
      "remove it consistently, outperforming the supervised approach, especially in the\n",
      "soft tissue area around the lungs, where the noise aﬀects attenuation coeﬃcients\n",
      "the most.\n",
      "when applied to a clinical dataset, noise2aliasing beneﬁts from more\n",
      "patients being included in the dataset, however, qualitatively good performance\n",
      "is already achieved with 5 patients.\n",
      "we have empirically demonstrated its performance on\n",
      "a publicly available dataset and on an internal clinical dataset.\n",
      "noise2aliasing\n",
      "noise2aliasing\n",
      "489\n",
      "outperforms a supervised approach when stochastic noise is present in the pro-\n",
      "jections and matches its performance on a popular benchmark.\n",
      "the method removes noise more reliably when the\n",
      "dataset size is increased, however further analysis is required to establish a good\n",
      "quantitative measurement of this phenomenon.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_3.pdf:\n",
      "conventional ac techniques require additionally-acquired com-\n",
      "puted tomography (ct) or magnetic resonance (mr) images to calculate\n",
      "attenuation coeﬃcients, which increases imaging expenses, time costs, or\n",
      "radiation hazards to patients, especially for whole-body scanners.\n",
      "in this\n",
      "paper, considering technological advances in acquiring more anatomi-\n",
      "cal information in raw pet images, we propose to conduct attenuation\n",
      "correction to pet by itself.\n",
      "to achieve this, we design a deep learn-\n",
      "ing based framework, namely anatomical skeleton-enhanced generation\n",
      "(aseg), to generate pseudo ct images from non-attenuation corrected\n",
      "pet images for attenuation correction.\n",
      "experiments on four public pet/ct\n",
      "datasets demonstrate that our aseg outperforms existing methods by\n",
      "achieving better consistency of anatomical structures in generated ct\n",
      "images, which are further employed to conduct pet attenuation correc-\n",
      "tion with better similarity to real ones.\n",
      "this work veriﬁes the feasibility of\n",
      "generating pseudo ct from raw pet for attenuation correction without\n",
      "acquising additional images.\n",
      "the associated implementation is available\n",
      "at https://github.com/yongshengpan/aseg-for-pet2ct.\n",
      "keywords: pet · attenuation correction · ct · image generation\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43999-5_3.\n",
      "c\n",
      "⃝ the author(s), under exclusive license to springer nature switzerland ag 2023\n",
      "h. greenspan et al.\n",
      "according to the pet imaging principle, radioactive tracers injected\n",
      "into the body involve in the metabolism and produce γ decay signals externally.\n",
      "however, due to photoelectric absorption and compton scattering, the decay\n",
      "signals are attenuated when passing through human tissues to external receivers,\n",
      "resulting in incorrect tracer distribution reasoning (see non-attenuation corrected\n",
      "pet (nac-pet) in fig.\n",
      "to obtain correct tracer distribution (see ac-\n",
      "pet in fig.\n",
      "traditional ac accompanies additional costs caused by the simultaneously\n",
      "obtained mr or ct images which are commonly useless for diagnosis.\n",
      "to reduce the costs,\n",
      "including expense, time, and radiation hazards, some studies proposed to con-\n",
      "duct ac by exploiting each pet image itself.\n",
      "researchers have been motivated\n",
      "to generate pseudo ct images from nac-pet images [2,7], or more directly,\n",
      "26\n",
      "y. pan et al.\n",
      "to generate ac-pet images from nac-pet images [5,11].\n",
      "since pseudo ct is\n",
      "convenient to be integrated into conventional ac processes, generating pseudo\n",
      "ct images is feasible in clinics for ac.\n",
      "the pseudo ct images should satisfy two-fold requests.\n",
      "firstly, the pseudo\n",
      "ct images should be visually similar in anatomical structures to correspond-\n",
      "ing actual ct images.\n",
      "secondly, pet images corrected by pseudo ct images\n",
      "should be consistent with that corrected by actual ct images.\n",
      "however, cur-\n",
      "rent techniques of image generation tend to produce statistical average values\n",
      "and patterns, which easily erase signiﬁcant tissues (e.g., bones and lungs).\n",
      "therefore, special techniques should be\n",
      "investigated to guarantee the ﬁdelity of anatomical structures in these generated\n",
      "pseudo ct images.\n",
      "aseg focuses more on the ﬁdelity of tissue distribu-\n",
      "tion, i.e., anatomical skeleton, in pseudo ct images.\n",
      "g1 devotes to delineating the anatomical skele-\n",
      "ton from a nac-pet image, thus producing a prior tissue distribution map\n",
      "to g2, while g2 devotes to rendering the tissue details according to both the\n",
      "skeleton and nac-pet image.\n",
      "we regard g1 as a segmentation network that\n",
      "is trained under the combination of cross-entropy loss and dice loss and out-\n",
      "puts the anatomical skeleton.\n",
      "experiments on four\n",
      "publicly collected pet/ct datasets demonstrate that our aseg outperforms\n",
      "existing methods by preserving better anatomical structures in generated pseudo\n",
      "ct images and achieving better visual similarity in corrected pet images.\n",
      "g2 then devotes to rendering the tissue details in the ct pattern exploit-\n",
      "ing both the skeleton and nac-pet images.\n",
      "specially, the general dice loss and cross-entropy loss [16]\n",
      "revealing anatomical structures in pet\n",
      "27\n",
      "are employed to guarantee g1 for the ﬁdelity of tissue distributions while general\n",
      "mean absolute error and feature matching losses are utilized to guarantee g2 for\n",
      "potential coarse-to-ﬁne semantic constraint.\n",
      "to improve the ﬁdelity of anatomi-\n",
      "cal structures, we further propose the anatomical consistency loss to encourage\n",
      "g2 to generate ct images that are consistent in tissue distributions with actual\n",
      "ct images in particular.\n",
      "let xnac and xac denote the nac-pet and ac-pet\n",
      "images, and y be the actual ct image used for ac.\n",
      "since ct image is highly\n",
      "crucial in conventional ac algorithms, they generally have a relationship as\n",
      "xac = f(xnac, y ),\n",
      "(1)\n",
      "under an ac algorithm f. to avoid scanning an additional ct image, we attempt\n",
      "to predict y from xnac as an alternative in ac algorithm.\n",
      "(2)\n",
      "this results in a pioneering ac algorithm that requires only a commonly reusable\n",
      "mapping function g for all pet images rather than a corresponding ct image\n",
      "y for each pet image.\n",
      "as veriﬁed in some previous studies [1,2,7], g can be assigned by some\n",
      "image generation techniques, e.g. gans and cnns.\n",
      "however, since these general\n",
      "techniques tend to produce statistical average values, directly applying them may\n",
      "lead to serious brightness deviation, for those tissues with large intensity ranges.\n",
      "to avoid annotating the ground truth, yas can be derived from the actual ct\n",
      "image by a segmentation algorithm (denoted as s : yas = s(y )).\n",
      "herein, we ﬁrst smooth each non-normalized ct\n",
      "image with a small recursive gaussian ﬁlter to suppress the impulse noise, and\n",
      "then threshold this ct image to four binary masks according to the hounsﬁeld\n",
      "scale of tissue density\n",
      "as mentioned above, two generative modules {g1, g2}\n",
      "work for two tasks, namely the skeleton prediction and tissue rendering, respec-\n",
      "tively.\n",
      "it is generally known that ct images can provide\n",
      "anatomical observation because diﬀerent tissues have a distinctive appearance\n",
      "in hounsﬁeld scale (linear related to attenuation coeﬃcients).\n",
      "therefore, it is\n",
      "crucial to ensure the consistency of tissue distribution in the pseudo ct images,\n",
      "tracking which we propose to use the tissue distribution consistency to guide the\n",
      "network learning.\n",
      "based on the segmentation algorithm s, both the actual and\n",
      "generated cts {y, ˆy } can be segmented to anatomical structure/tissue distribu-\n",
      "tion masks {s(y ), s( ˆy )}, and their consistency can then be measured by dice\n",
      "coeﬃcient.\n",
      "(6)\n",
      "during the inference phase, only the nac-pet image of each input subject\n",
      "is required, where the pseudo ct image is derived by ˆy ≈ g2(g1(xnac), xnac).\n",
      "revealing anatomical structures in pet\n",
      "29\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "materials\n",
      "the data used in our experiments are collected from the cancer image archive\n",
      "(tcia)\n",
      "in our experiments, we re-sampled all of them\n",
      "to a voxel spacing of 2×2×2 and re-scaled the intensities of nac-pet/ac-pet\n",
      "images to a range of [0, 1], of ct images by multiplying 0.001.\n",
      "to achieve full-fov output, the consecutive outputs of each sample\n",
      "are composed into a single volume where the overlapped regions are averaged.\n",
      "3.2\n",
      "comparison with other methods\n",
      "we compared our aseg with three state-of-the-art methods, including (i) a\n",
      "u-net based method\n",
      "[3] that directly learns a mapping from nac-pet to\n",
      "ct image with mae loss (denoted as u-net), (ii) a conventional gan-based\n",
      "method [1,2] that uses the u-net as the backbone and employ the style-content\n",
      "loss and adversarial loss as an extra constraint (denoted as cgan), and (iii)\n",
      "an auxiliary gan-based method [10] that uses the ct-based segmentation (i.e.,\n",
      "the simple thresholding s) as an auxiliary task for ct generation (denoted as\n",
      "agan).\n",
      "for a fair comparison, we implemented these methods by ourselves in a\n",
      "tensorflow platform with an nvidia 3090 gpu.\n",
      "and follow the same experimental settings.\n",
      "as the most import application of ct that\n",
      "is to display the anatomical information, we propose to measure the anatom-\n",
      "ical consistency between the pseudo ct images and actual ct images, where\n",
      "the dice coeﬃcients on multiple anatomical regions that extracted from the\n",
      "pseudo/actual ct images are calculated.\n",
      "to avoid excessive self-referencing in\n",
      "evaluating anatomical consistency, instead of employing the simple threshold-\n",
      "ing segmentation (i.e., s), we resort to the open-access totalsegmentator\n",
      "[15]\n",
      "to ﬁnely segment the actual and pseudo ct images to multiple anatomical\n",
      "structures, and compose them to nine independent tissues for simplifying result\n",
      "30\n",
      "y. pan et al.\n",
      "table 1. comparison of pseudo ct images generated by diﬀerent methods.\n",
      "firstly, u-net and cgan generate ct images with\n",
      "slightly better global intensity similarity but worse anatomical consistency in\n",
      "some tissues than agan and aseg.\n",
      "this indicates that the general constraints\n",
      "(mae and perceptual feature matching) cannot preserve the tissue distribution\n",
      "since they tend to produce statistical average values or patterns, particularly in\n",
      "these regions with large intensity variants.\n",
      "as the pseudo ct images gener-\n",
      "ated from nac-pet are expected to be used in ac, it is necessary to further\n",
      "evaluate the eﬀectiveness of pseudo ct images in pet ac.\n",
      "to evaluate the pseudo ct images, we simply use\n",
      "them to take place of the actual ct. four metrics, including the peak signal to\n",
      "noise ratio (psnr), mean absolute error (mae), normalized cross correla-\n",
      "tion (ncc), and ssim, are used to measure acgan with pseudo ct images\n",
      "on test datasets (nsclc, tcga-hnsc, and tcga-luda).\n",
      "the results are\n",
      "reported in table 1(b), where the fourth column list the acgan results with\n",
      "actual ct images.\n",
      "meanwhile, we also report the results of direct mapping nac-\n",
      "pet to ac-pet without ct images in the third column (“no ct”), which is\n",
      "trained from scratch and independent from acgan.\n",
      "it can be observed from table 1(b) that: (1) acgan with actual ct images\n",
      "can predict images very close to the actual ac-pet images, thus is qualiﬁed to\n",
      "simulate the ac process; (2) with actual or pseudo ct images, acgan can\n",
      "predict images closer to the actual ac-pet images than without ct, demon-\n",
      "strating the necessity of ct images in process of pet ac; (3) these pseudo cts\n",
      "cannot compare to actual cts, reﬂecting that there exist some relative informa-\n",
      "tion that can hardly be mined from nac-pet; (4) the pseudo cts generated\n",
      "by aseg achieve the best in three metrics (mae, psnr, ncc) and second\n",
      "in the other metric (ssim), demonstrating the advance of our aseg.\n",
      "2. visualization of diﬀerent pseudo ct images (top) and their ac eﬀect (bottom).\n",
      "4\n",
      "conclusion\n",
      "in this paper, we proposed the anatomical skeleton-enhance generation (aseg)\n",
      "to generate pseudo ct images for pet attenuation correction (ac), with the\n",
      "goal of avoiding acquiring extra ct or mr images.\n",
      "experiments on a collection of public\n",
      "datasets demonstrate that our aseg outperforms existing methods by achiev-\n",
      "ing advanced performance in anatomical consistency.\n",
      "our study support that\n",
      "aseg could be a promising and lower-cost alternative of ct acquirement for\n",
      "ac.\n",
      "our future work will extend our study to multiple pet tracers.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_21.pdf:\n",
      "contrast-enhanced ultra-sound (ceus) has become a\n",
      "viable method for non-invasive, dynamic visualization in medical diag-\n",
      "nostics, yet ultrasound localization microscopy (ulm) has enabled\n",
      "a revolutionary breakthrough by oﬀering ten times higher resolution.\n",
      "to date, delay-and-sum (das) beamformers are used to render ulm\n",
      "frames, ultimately determining the image resolution capability.\n",
      "to take\n",
      "full advantage of ulm, this study questions whether beamforming is\n",
      "the most eﬀective processing step for ulm, suggesting an alternative\n",
      "approach that relies solely on time-diﬀerence-of-arrival (tdoa) infor-\n",
      "mation.\n",
      "keywords: ultrasound · microbubble · localization · microscopy ·\n",
      "geometry · parallax · triangulation · trilateration · multilateration ·\n",
      "time-of-arrival\n",
      "1\n",
      "introduction\n",
      "ultrasound localization microscopy (ulm) has revolutionized medical imaging\n",
      "by enabling sub-wavelength resolution from images acquired by piezo-electric\n",
      "transducers and computational beamforming.\n",
      "the discovery of ulm has recently surpassed the diﬀraction-limited spatial\n",
      "resolution and enabled highly detailed visualization of the vascularity [8]. ulm\n",
      "borrows concepts from super-resolution ﬂuorescence microscopy techniques to\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43999-5_21.\n",
      "https://doi.org/10.1007/978-3-031-43999-5_21\n",
      "218\n",
      "c. hahne and r. sznitman\n",
      "precisely locate individual particles with sub-pixel accuracy over multiple frames.\n",
      "by the accumulation of all localizations over time, ulm can produce a super-\n",
      "resolved image, providing researchers and clinicians with highly detailed repre-\n",
      "sentation of the vascular structure.\n",
      "[8] initially demonstrated the potential of\n",
      "ulm by successfully localizing contrast agent particles (microbubbles) using a\n",
      "2d point-spread-function model.\n",
      "1. comparison of ulm processing pipelines: classical ulm (top) employs compu-\n",
      "tational beamforming from n channels and image ﬁlters to localize microbubbles.\n",
      "as\n",
      "a reﬁnement step, ellipse intersections are fused via clustering (right).\n",
      "for example, a recent\n",
      "study has shown that ultrasound image segmentation can be learned from radio-\n",
      "frequency data and thus without beamforming [13].\n",
      "[3], optimization of\n",
      "the point-spread function (psf) poses high demands on the transducer array,\n",
      "data storage, and algorithm complexity.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Relevant sentences from /Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_24.pdf:\n",
      "sparse-view computed tomography (ct) is a promising solu-\n",
      "tion for expediting the scanning process and mitigating radiation expo-\n",
      "sure to patients, the reconstructed images, however, contain severe streak\n",
      "artifacts, compromising subsequent screening and diagnosis.\n",
      "recently,\n",
      "deep learning-based image post-processing methods along with their\n",
      "dual-domain counterparts have shown promising results.\n",
      "however, exist-\n",
      "ing methods usually produce over-smoothed images with loss of details\n",
      "due to i) the diﬃculty in accurately modeling the artifact patterns in\n",
      "the image domain, and ii) the equal treatment of each pixel in the loss\n",
      "function.\n",
      "to address these issues, we concentrate on the image post-\n",
      "processing and propose a simple yet eﬀective frequency-band-aware\n",
      "and self-guided network, termed freeseed, which can eﬀectively remove\n",
      "artifacts and recover missing details from the contaminated sparse-view\n",
      "ct images.\n",
      "speciﬁcally, we ﬁrst propose a frequency-band-aware arti-\n",
      "fact modeling network (freenet), which learns artifact-related frequency-\n",
      "band attention in the fourier domain for better modeling the globally\n",
      "distributed streak artifact on the sparse-view ct images.\n",
      "we then intro-\n",
      "duce a self-guided artifact reﬁnement network (seednet), which lever-\n",
      "ages the predicted artifact to assist freenet in continuing to reﬁne the\n",
      "severely corrupted details.\n",
      "extensive experiments demonstrate the supe-\n",
      "rior performance of freeseed and its dual-domain counterpart over the\n",
      "state-of-the-art sparse-view ct reconstruction methods.\n",
      "source code is\n",
      "made available at https://github.com/masaaki-75/freeseed.\n",
      "supplementary information the online version contains supplementary material\n",
      "available at https://doi.org/10.1007/978-3-031-43999-5 24.\n",
      "sparse-view ct is one of the eﬀective solutions, which reduces the\n",
      "radiation by only sampling part of the projection data for image reconstruction.\n",
      "nevertheless, images reconstructed by the conventional ﬁltered back-projection\n",
      "(fbp) present severe artifacts, thereby compromising their clinical value.\n",
      "existing learning-based approaches\n",
      "mainly include image-domain methods [2,4,18] and dual-domain ones [7,13,16],\n",
      "both involving image post-processing to restore a clean ct image from the\n",
      "low-quality one with streak artifacts.\n",
      "for the image post-processing, residual\n",
      "learning [3] is often employed to encourage learning the artifacts hidden in\n",
      "the residues, which has become a proven paradigm for enhancing the perfor-\n",
      "mance [2,4,6,16].\n",
      "unfortunately, existing image post-processing methods may\n",
      "fail to model the globally distributed artifacts within the image domain.\n",
      "they\n",
      "can also produce over-smoothed images due to the lack of diﬀerentiated super-\n",
      "vision for each pixel.\n",
      "in this paper, we advance image post-processing to beneﬁt\n",
      "both classical image-domain methods and the dominant dual-domain ones.\n",
      "motivation.\n",
      "we view the sparse-view ct image reconstruction as a two-step\n",
      "task: artifact removal and detail recovery.\n",
      "while fourier domain band-pass maps help capture the pattern of the arti-\n",
      "facts, restoring the image detail contaminated by strong artifacts may still be\n",
      "diﬃcult due to the entanglement of artifacts and details in the residues.\n",
      "sequently, we propose a self-guided artifact reﬁnement network (seednet) that\n",
      "provides supervision signals to aid freenet in reﬁning the image details con-\n",
      "taminated by the artifacts.\n",
      "freeseed achieves promising results with only image data and can be further\n",
      "enhanced once the sinogram is available.\n",
      "252\n",
      "c. ma et al.\n",
      "our contributions can be summarized as follows: 1) a novel frequency-band-\n",
      "aware network is introduced to eﬃciently capture the pattern of global artifacts\n",
      "in the fourier domain among diﬀerent sparse-view scenarios; 2) to promote the\n",
      "restoration of heavily corrupted image detail, we propose a self-guided artifact\n",
      "reﬁnement network that ensures targeted reﬁnement of the reconstructed image\n",
      "and consistently improves the model performance across diﬀerent scenarios; and\n",
      "3) quantitative and qualitative results demonstrate the superiority of freeseed\n",
      "over the state-of-the-art sparse-view ct reconstruction methods.\n",
      "first row: sparse-view ct images (left half) and the corresponding artifacts\n",
      "(right half); second row: real fourier amplitude maps of artifacts (left half) and the\n",
      "learned band-pass attention maps (right half, with inner radius and bandwidth respec-\n",
      "tively denoted by d0 and w. values greater than 0.75 are bounded by red dotted line)\n",
      "given diﬀerent number of views nv.\n",
      "(color ﬁgure online)\n",
      "2\n",
      "methodology\n",
      "2.1\n",
      "overview\n",
      "given a sparse-view sinogram with projection views nv, let is and if denote\n",
      "the directly reconstructed sparse- and full- view images by fbp, respectively.\n",
      "in\n",
      "this paper, we aim to construct an image-domain model to eﬀectively recover is\n",
      "with a level of quality close to if.\n",
      "2, which mainly\n",
      "consists of two designs: freenet that learns to remove the artifact and is built\n",
      "with band-pass fourier convolution blocks that better capture the pattern of the\n",
      "artifact in fourier domain; and seednet as a proxy module that enables freenet\n",
      "to reﬁne the image detail under the guidance of the predicted artifact.\n",
      "(4)\n",
      "2.3\n",
      "self-guided artifact reﬁnement network\n",
      "areas heavily obscured by the artifact should be given more attention, which\n",
      "is hard to achieve using only freenet.\n",
      "concretely, given sparse-\n",
      "view ct images is, freenet predicts the artifact \ta and restored image \ti =\n",
      "is − \ta; the latter is fed into seednet to produce targeted reﬁned result \n",
      "i. to\n",
      "guide the network on reﬁning the image detail obscured by heavy artifacts, we\n",
      "design the transformation t that turns \ta into a mask m using its mean value\n",
      "as threshold:\n",
      "the pseudo-code for the training process and\n",
      "the exploration on the selection of α can be found in our supplementary material.\n",
      "to further enhance the image reconstruc-\n",
      "tion quality, we extend freeseed to the dominant dual-domain framework by\n",
      "adding the sinogram-domain sub-network from dudonet\n",
      "the sinogram-\n",
      "domain sub-network involves a mask u-net that takes in the linearly interpo-\n",
      "lated sparse sinogram ss, where a binary sinogram mask m s that outlines the\n",
      "unseen part of the sparse-view sinogram is concatenated to each stage of the\n",
      "u-net encoder.\n",
      "3. overview of dual-domain counterpart of freeseed.\n",
      "3\n",
      "experiments\n",
      "3.1\n",
      "experimental settings\n",
      "we conduct experiments on the dataset of “the 2016 nih-aapm mayo clinic\n",
      "low dose ct grand challenge”\n",
      "[8], which contains 5,936 ct slices in 1 mm\n",
      "image thickness from 10 anonymous patients, where a total of 5,410 slices from\n",
      "9 patients, resized to 256 × 256 resolution, are randomly selected for training\n",
      "and the 526 slices from the remaining one patient for testing without patient\n",
      "overlap.\n",
      "specifying the distance from the x-ray source to the\n",
      "rotation center as 59.5 cm and the number of detectors as 672, we generate\n",
      "sinograms from full-dose images with multiple sparse views nv ∈ {18, 36, 72, 144}\n",
      "uniformly sampled from full 720 views covering [0, 2π].\n",
      "the models are implemented in pytorch\n",
      "experi-\n",
      "ments are conducted on a single nvidia v100 gpu using the same setting.\n",
      "[15].\n",
      "256\n",
      "c. ma et al.\n",
      "3.2\n",
      "overall performance\n",
      "we compare our models (freeseed and freeseeddudo) with the following recon-\n",
      "struction methods: direct fbp, ddnet\n",
      "[13]. fbpconv and ddnet are image-domain methods, while\n",
      "dudonet and dudotrans are state-of-the-art dual-domain methods eﬀective for\n",
      "ct image reconstruction.\n",
      "not surprisingly, we ﬁnd that the performance of conventional image-domain\n",
      "methods is inferior to the state-of-the-art dual-domain method, mainly due to\n",
      "the failure of removing the global artifacts.\n",
      "note that when the sinogram data are available,\n",
      "dual-domain counterpart freeseeddudo gains further improvements, showing the\n",
      "great ﬂexibility of our model.\n",
      "3.3\n",
      "ablation study\n",
      "table 2 presents the eﬀectiveness of each component in freeseed, where seven\n",
      "variants of freeseed are: (1) fbpconv upon which freenet is built (baseline); (2)\n",
      "freenet without band-pass attention maps nor seednet guidance lmask (baseline\n",
      "+ fourier); (3) fbpconv trained with lmask (baseline + seednet); (4) freenet\n",
      "frequency-band-aware and self-guided network for sparse-view ct\n",
      "257\n",
      "trained without lmask (freenet); (5) freenet trained with simple masked loss\n",
      "l1+mask = ∥(af − \ta) ⊙ (1 + m)∥2 (freenet1+mask); (6) freenet trained with\n",
      "lmask using ℓ1 norm (freeseedℓ1); and (7) freenet trained with lmask using ℓ2\n",
      "norm, i.e., the full version of our model (freeseed).\n",
      "by comparing the ﬁrst two rows of table 2, we ﬁnd that simply applying ffc\n",
      "provides limited performance gains.\n",
      "interestingly, we observe that the advantage\n",
      "of band-pass attention becomes more pronounced given more views, which can be\n",
      "seen in the last row of fig.\n",
      "1 where the attention maps are visualized by averaging\n",
      "all inner radii and bandwidths in diﬀerent stages of freenet and calculating the\n",
      "map following eq.\n",
      "= 36, 72, 144 where\n",
      "artifacts are less entangled with the image content and present a banded shape\n",
      "in the frequency domain.\n",
      "visually, clinical details in the image that are obscured by\n",
      "the heavy artifacts can be further reﬁned by freenet; please refer to fig.\n",
      "s1 in\n",
      "our supplementary material for more examples and ablation study.\n",
      "we also ﬁnd\n",
      "that freenet1+mask does not provide stable performance gains, probably because\n",
      "directly applying a mask on the pixel-wise loss leads to the discontinuous gradient\n",
      "that brings about sub-optimal results, which, however, can be circumvented with\n",
      "the guidance of seednet.\n",
      "we ﬁnd that ℓ1 norm does not ensure\n",
      "stable performance gains when ffc is used.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keywords = ['demographic', 'ethics', 'ethical considerations', 'bias mitigation', \n",
    "            'gender', 'sex', 'female', 'woman', 'women', 'men', 'male', 'man', \n",
    "            'age', 'race', 'ethnicity', 'bias', 'biases', 'fairness', 'equity', 'transparency']\n",
    "\n",
    "\n",
    "# Extract relevant sentences from the selected papers\n",
    "for pdf_path in selected_papers_paths:\n",
    "    path = pdf_path[0]\n",
    "    text = extract_text(path)\n",
    "    # Preprocessing can be optional based on your requirement for extracting sentences\n",
    "    #preprocessed_text = preprocess_text(text)\n",
    "    #relevant_sentences = extract_relevant_sentences(preprocessed_text, keywords)\n",
    "    relevant_sentences = extract_relevant_sentences(text, keywords)\n",
    "    print(f\"Relevant sentences from {path}:\")\n",
    "    for sentence in relevant_sentences:\n",
    "        print(sentence)\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate over each volume and search for keywords\n",
    "\n",
    "def process_papers(folder_path, keywords):\n",
    "    for i in range(1, 11):  # Volumes 1 to 10\n",
    "        folder_name = folder_path\n",
    "    \n",
    "        \n",
    "        for pdf in os.listdir(folder_path):\n",
    "            if pdf.endswith(\".pdf\"):\n",
    "                pdf_path_ = os.path.join(folder_path, pdf)\n",
    "                full_text = extract_text(pdf_path_)\n",
    "                if find_keywords_section(full_text, keywords):\n",
    "                    selected_papers.append(os.path.join(folder_name, pdf))\n",
    "\n",
    "\n",
    "print(f\"Extracted titles from {len(titles)} selected papers.\")\n",
    "print(f\"With the keyword(s) being {keywords}, {len(selected_papers)} papers were selected as relevant to cancer research.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
