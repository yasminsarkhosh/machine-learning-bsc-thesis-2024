Paper Title,Header Number,Header Title,Text
Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,1.0,Introduction,"Deep learning-based systems show remarkable predictive performance in many computer vision tasks, including medical image analysis, and are often comparable to human performance. However, the complexity of this technique makes it challenging to extract model knowledge and understand model decisions. This limitation is being addressed by the field of Explainable AI, in which significant progress has been made in recent years. An important line of research is the use of inherently explainable models, which circumvent the need for indirect, errorprone on-top explanations  A promising approach for interpretability is the use of Privileged Information, i.e. information that is only available during training  Prototype Networks are another line of research implementing the idea that the representations of images cluster around a prototypical representation for each class  Our method addresses the limitations of privileged information-based and prototype-based explanation by combining case-based visual reasoning through exemplary representation of high-level attributes to achieve explainability and high-performance. The proposed method is an image classifier that satisfies explainable-by-design with two elements: First, decisive intermediate results of a high-performance CNN are trained on human-defined attributes which are being predicted during application. Second, the model provides prototypical natural images to validate the attribute prediction. In addition to the enhanced explainability offered by the proposed approach, to our knowledge the proposed method outperforms existing studies on the LIDC-IDRI dataset. The main contributions of our work are: -A novel method that, for the first time to our knowledge, combines privileged information and prototype learning to provide increased explanatory power for medical classification tasks. -A prototype network architecture based on a capsule network that leverages the benefits of both techniques. -An explainable solution outperforming state-of-the-art explainable and nonexplainable methods on the LIDC-IDRI dataset. We provide the code with the model architecture and training algorithm of Proto-Caps on GitHub."
Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,2.0,Methods,"The idea behind our approach is to combine the potential of attribute and prototype learning for a powerful and interpretable learning system. For this, we use a capsule network of attribute capsules from which the target class is predicted. As the attribute prediction can also be susceptible to error, we use prototypes to explain the predictions made for each attribute. Based on  The backbone of our approach is a capsule network consisting of three layers: Features of the input image of size 1×32×32 are extracted by a 2D convolutional layer containing 256 kernels of size 9×9. We decided not to use 3D convolutional layers, as preliminary experiments showed only marginal differences (within std. dev. of results), but required significantly more computing time. The primary capsule layer then segregates low-level features into 8 different capsules, with each capsule applying 256 kernels of size 9 × 9. The final dense capsule layer consists of one capsule for each attribute and extracts high-level features, overall producing eight 16-dimensional vectors. These vectors form the starting point for the different prediction branches. The target head, a fully connected layer, combines the capsule encodings. The loss function for the malignancy prediction was chosen according to LaLonde et al.  where Y a is the ground truth mean attribute score by the radiologists, O a is the network score prediction for the a-th attribute, and b is a random binary mask allowing semi-supervised attribute learning. Two prototypes are learned per possible attribute class, resulting in 8-12 prototypes per attribute (i.e. capsule). During the training, a combined loss function encourages a training sample to be close to a prototype of the correct attribute class and away from prototypes dedicated to others, similar to existing approaches  In order to clearly distinguish between different attribute specifications, a separation loss is applied to increase the distance to the capsule prototypes that do not have the correct specification, limited by a maximum distance: Prototype optimization begins after 100 epochs. In addition to fitting the prototypes with the loss function, each prototype is replaced every 10 epochs by the most similar latent vector of a training sample. The original image of the training sample is stored and used for prototype visualization. During inference, the predicted attribute value is set to the ground truth attribute value of the closest prototype, ignoring the learned dense layers in the attribute head at this stage. The overall loss function is the following weighted sum, where λ recon = 0.512 was chosen according to "
Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,3.0,Experiments,"Data. The proposed approach is evaluated using the publicly available LIDC-IDRI dataset consisting of 1018 clinical thoracic CT scans from patients with Non-Small Cell Lung Cancer (NSCLC)  Experiment Designs. To ensure comparability with previous work  The algorithm was implemented using the PyTorch framework version 1.13 and CUDA version 11.6. A learning rate of 0.5 was chosen for the prototype vectors and 0.02 for the other learnable parameters. The batch size was set to 128 and the optimizer was ADAM  Besides pure performance, the effect of reduced availability of attribute annotations was investigated. This was done by using attribute information only for a randomly selected fraction of the nodules during the training. To investigate the effect of prototypes on the network performance, an ablation study was performed. Three networks were compared: Proto-Caps (proposed) including learning and applying prototypes during inference, Proto-Caps w/o use where prototypes are only learned but ignored for inference, and Proto-Caps w/o learn using the proposed architecture without any prototypes."
Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,4.0,Results,"Qualitative. Figure  During application, these discrepancies between the prototypes and the sample nodule raise suspicion, and help to assess the malignancy prediction. A quantitative evaluation of the relationship between correctness in attribute and in target prediction using logistic regression analysis shows a strong relationship between both with an accuracy of 0.93/0.1. Quantitative. Table  Table "
Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,5.0,Discussion and Conclusion,"We propose a new method, named Proto-Caps, which combines the advantages of privileged information, and prototype learning for an explainable network, achieving more than 6 % better accuracy than the state-of-the-art explainable method. As shown by qualitative results (Fig.  The experiments demonstrate that it outperforms state-of-the-art methods that provide less explainability. Our data reduction studies show that the proposed solution is robust to the number of annotated examples, and good results are obtained even with a 90% reduction in privileged information. This opens the door for application to other datasets by reducing the additional annotation overhead. While we did see a reduction in performance with too few labels, our results suggest that this is mainly due to inhomogeneous coverage of individual attribute values. In this respect, it would be interesting to find out how a specific selection of the annotated samples, e.g. with extremes, affects the accuracies, especially since our results show that the overall performance is robust even when the attributes are not explicitly trained, i.e. without additional privileged information. Another area of research would be to explore other types of privileged information that require less extra annotation effort, such as medical reports, to train the attribute capsules. It would also be worth investigating more sophisticated 3D-based capsule networks. In conclusion, we believe that the approach of leveraging privileged information with comprehensible architectures and prototype learning is promising for various high-risk application domains and offers many opportunities for further research."
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,1.0,Introduction,"Consider a longitudinal brain connectome study where each participant goes through imaging protocol multiple times over the study period. Given a population of such subjects, analyzing them can be posed as a spatio-temporal graph analysis where each sample in a dataset is given as a set of longitudinal graphs of different cardinality. An exemplar sample corresponding to this task is shown in Fig.  There are several practical bottlenecks to extract meaningful results from the longitudinal brain connectome and region-wise imaging measures. The data are temporally sparse, i.e., the participants pay a different number of visits which can be very few. Also, each brain network has a different structure of white-matter fiber connections unlike regular lattice structure in images. Last but importantly, most neuroimaging datasets suffer from lack of samples as the data are expensive to acquire and process. These spatio-temporal heterogeneities and sample-size issue make longitudinal analyses of the brain network challenging, but it must be investigated to characterize the progressive disease-relevant variations. Therefore, it is necessary to develop an efficient prediction model for a ""set of longitudinal graphs"" and corresponding regional measures (i.e., node features) over sparse time-points. Most graph neural networks are designed for a fixed template graph for predicting node values  We tackle the aforementioned issues by designing a flexible architecture with a ""mix"" of Multi-layer Perceptron (MLP) "
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,,Contributions of,"Our Work: our model 1) can be trained efficiently compared to existing spatio-temporal graph deep models with a significantly reduced number of parameters, 2) flexibly incorporates irregular space and time into prediction, 3) yields interpretable results that quantify the contribution of each node   contains F node features for N nodes and E i ∈ R N ×N is a weighted adjacency matrix whose elements denote connection strength between two nodes. Given a population of G m with C classes, STGMLP aims to classify the label of each G m by leveraging both temporal and spatial variations of the graph set from different groups. Note that the label of each sample (i.e., longitudinal graph set) is consistent over time."
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,,Overview of STGMLP. STGMLP mixes graph features across space and time,"with Graph Spatial Mixer (GSM) and Graph Temporal Mixer (GTM), respectively. GSM performs a per-graph operation (i.e., node-mixing) and GTM accounts for cross-temporal operation (i.e., graph-mixing) between multiple graphs in a G m . Figure  The overall structure of STGMLP is shown in Fig.  Graph Spatial Mixer. GSM encodes node features and a graph structure of a graph G i with graph convolution. The node-mixing MLP (R F → R F ) acts on rows of X i , and it is shared across G m for all N × T m nodes. Let f (•) be an operation of node-mixing MLP which takes Ẽi and X i as inputs, where Ẽi is a normalized E i to ensure unbiased strength of the connectivity. It includes selfconnections I N (i.e., identity matrix) and computed as Ẽi = D-  where W 0 and W 1 are trainable weights and LN(•) is a layernorm function. The output (X i ) j ∈ R F accounts for a latent vector of local graph structure at node j. Stacking (X i ) j s up to the number of nodes N , an outcome X i ∈ R N ×F is derived for an input G i . In this way, a set of whole outputs from T m GSMs is derived as {X i } Tm i=1 ∈ R Tm×N ×F . Notice that the GSM can be stacked D times by iteratively taking the X i as an updated input to encode a wider range of local graph structures. After performing max pooling on T m and F dimensions of {X i } Tm i=1 , the condensation of spatial features across G m = {G 1 , ..., G Tm } is obtained as a N -dimensional vector S. Graph Temporal Mixer. GTM performs a cross-temporal operation on multiple ""pairs"" of graphs. This graph-mixing encodes the relations between graphs of different time-points. Given T m graphs from a subject, P pairs of graphs, each pair as a set {G i1 , G i2 }, are selected where P is a user parameter. For each pair for {i 1 , i 2 }, an averaged connectivity Ẽp =( Ẽi1 + Ẽi2 )/2 and X p ∈ R N ×2F as a concatenation of X i1 and X i2 are inputted into the graph-mixing MLP g(•). In our work, we choose to input pairs of temporally adjacent graphs together with the first-and-last graph pair to encode a temporal sequence. The g(•) acts on rows of X p , mapping R 2F → R F . It transforms the features of node j (i.e., (X p ) j ) into F -dimensional latent vector, and the projection is performed across the whole node pairs in parallel. Similar to the node-mixing MLP, graph-mixing MLP contains two FC layers with weights W 2 and W 3 and a GELU σ(•) as (2) As in the GSM, each (X p ) j is stacked N times to be a X p . For P GTMs, an output {X p } P p=1 ∈ R P ×N ×F is obtained and reduced into T ∈ R N by max pooling on P pairs and F node features. Note that, unlike GSMs, Ẽp is used only once in Eq. (  where W 4 and W 5 are weight matrices and σ(•) is a nonlinearity. With this STM, irregular space and time components can be flexibly integrated into a prediction. Longitudinal Graph Classifier. To take a full advantage from the extracted features, S and T are combined together with F via a skip connection. These features collected from diverse branching paths contain both low and high-level information extracted from the graphs, and their integration provides strong ensemble-like results  where W 6 is a set of trainable weights of the FC layer for class prediction. Given the ground truth Y , the cross-entropy loss is defined with 2 -regularization as where W is a set of trainable parameters and λ controls a regularization strength."
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,3.0,Experiments,"In this section, we evaluate STGMLP on two independent datasets, i.e., ADNI and ABCD, whose demographics are given in Table "
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,3.1,Materials and Setup,"ADNI Dataset. The ADNI is the largest public AD dataset providing longitudinal and multimodal images such as magnetic resonance imaging (MRI) and positron emission tomography (PET). As node features, cortical thickness from MRI, standardized uptake value ratio (SUVR) from FDG-PET and Amyloid-PET at all ROIs were measured. Structural brain networks were obtained by inhouse probabilistic tractography on diffusion tensor images (DTI) on Destrieux ABCD Dataset. The ABCD dataset (v4.0) contains two timepoints with multivariate features: baseline data for children aged 8-10 and their 2-year follow-up measurements such as fractional anisotropy, mean diffusivity, and cortical thickness obtained via DTI and MRI. Morphometric similarity network  Setup. As baselines, we adopt various graph convolutional networks (GCNs) for spatio-temporal graph analysis such as ST-GCN  To implement STGMLP, the learning rate, weight decay (λ), dropout rate, and depth D of GSM were set to 1e-2, 5e-4, 5%, and 3, respectively. For GTMs on the ADNI, total P =T m pairs are selected: T m -1 pairs for adjacent graphs in time, and one pair for the first and last (i.e., end-to-end) timepoints. For the ABCD, P is set to 1 as T m =2 for all samples. Note that, the combination of timepoints can be flexibly selected to include domain knowledge."
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,3.2,Evaluation and Discussions on the Results,"Quantitative results (i.e., mean accuracy, precision, and recall) of all experiments and the number of trainable parameters are compared in Table  Preclinical vs. MCI vs. AD on ADNI Dataset. Here, STGMLP achieved 71.3% accuracy with 7.8∼24.0%p margin over baselines. Here, we provide clinically interpretable results by analyzing nodal contributions to classify each class via class-averaged Grad-CAM  Below-Poverty vs. Non-poverty on ABCD Dataset. As shown in Table "
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,3.3,Temporal Analysis on AD-Specific Activation,In Fig. 
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,3.4,Ablation Study on Hyperparamters,"Ablation studies on each module (i.e., GSM, GTM, and STM) and pooling methods were performed on ADNI. The results, reported in Table "
Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis,4.0,Conclusion,"In this work, we proposed a novel longitudinal graph mixer to investigate longitudinal variations of spatio-temporal graphs. The idea was driven by mixing features temporally and spatially along the topology of brain networks, and its structure was designed to deal with the heterogeneity of data with a significantly reduced number of parameters compared to deep graph convolutional models. Experiments validate the superiority of our framework, successfully identifying key ROIs in classifying different classes, suggesting a significant potential to be deployed for other longitudinal connectome analyses of various brain disorders."
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,1.0,Introduction,"Alzheimer's disease (AD) is an irreversible neurodegenerative disorder that affects millions of people worldwide  In the literature, various methods have been proposed to brain imaging genetics analysis  In this paper, we propose a novel attentive deep canonical correlation analysis (ADCCA) model for diagnosing AD disease and discovering biomarkers using multimodal brain imaging genetics data. As illustrated in Fig.  Through extensive experiments on the real-world ADNI dataset with three imaging modalities (VBM-MRI, FDG-PET, and AV45-PET) and genetic SNP data, we show that our model achieves outstanding performance for classifying AD vs. HC, AD vs. MCI, and MCI vs. HC groups. Also, it is demonstrated that the model explanation can reveal disorder-specific biomarkers coinciding with neuroscience findings. Last, we show that the combination of classification and correlation models can boost disease prediction performance."
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,2.0,Method,"Suppose that the problem includes N subjects with M modalities. Let X m ∈ R N ×dm denote the m-th modality data, where d m represents the dimension of features in the m-th modality, the label information of all subjects. In this work, we seek to learn a disease prediction model that estimates Ŷ from {X m } M m=1 by making full use of all M modalities, as well as identify disease-specific biomarkers for clinical interpretation. The proposed ADCCA aims to combine the strengths of DNN, attention mechanism, and CCA to integrate and exploit the complementary information from multiple data modalities (Fig.  Mathematically, the self-attention representation of f m (X m ) can be calculated as: Third, following  where U 1 , • • • , U 4 , U y are projection matrices for each modality and label information, respectively. I denotes the identity matrix. According to Eq. (  Thus, the label Y can be approximated as follows: , where U † y denotes the pseudo-inverse of U y . Then, we substitute self-attention representations that are more representative of each modality into the above equation and let Further, the conventional supervised crossentropy loss  CrossEntropy Y, Softmax( Ŷm ) . ( The final label prediction of ADCCA can be obtained using the following soft voting of the label presentation of each modality: Ŷ = Softmax(( M m=1 Ŷm )/M ). Overall, our final training objective can be defined as: where L cls is the supervised cross-entropy disease prediction loss, L cor is the correlation loss, and λ is a tunable hyperparameter that scales the numerical value of each loss item to the same order of magnitude to balance their influence. The solution on loss function L is similar to the SGDCCA method except for substituting the outputs of DNN models to their self-attention representations. 3 Experiments and Results"
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,3.1,Data Acquisition and Preprocessing,"Brain imaging genetic data used in this study were obtained from the public ADNI database  For genetic SNP data, according to the AlzGene database"
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,3.2,Evaluation of Disease Classification Performance,"In our experiments, the whole data were separated into three groups, including AD vs. HC, AD vs. MCI, and MCI vs. HC. To quantitatively evaluate the performance of different methods, we considered four commonly-used evaluation metrics: accuracy (ACC), F1-score (F1), area under receiver operating characteristic curve (AUC), and Matthews correlation coefficient (MCC)  Table "
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,3.3,The Most Discriminative Brain Regions and SNPs,"Identifying the most discriminative brain regions (i.e., ROIs) and SNPs is crucial for AD diagnosis. Here, we employed the integrated gradients interface provided by Captum "
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,3.4,Ablation Study,"The proposed ADCCA is trained using both correlation and classification losses. To understand the impact of each loss on classification, we conducted ablation studies by evaluating the performance of two additional models: the ADCCA model trained without the correlation loss (w/o L cor ) and without the classification loss (w/o L cls ). The results presented in Table "
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,3.5,Hyperparameter Analysis,"We investigated the impact of two important hyperparameters in the ADCCA model: λ, which appears in the loss function to balance the classification and correlation losses, and the dimension of the shared representation G. In order to explore the effects of these hyperparameters on the performance of the model, we conducted experiments using different values of λ and the shared representation dimensionality. Due to the space limit, we only report the classification results in AD vs. HC group, as shown in Fig. "
Attentive Deep Canonical Correlation Analysis for Diagnosing Alzheimer’s Disease Using Multimodal Imaging Genetics,4.0,Conclusion,"In this work, we propose a novel deep canonical correlation analysis method for multimodal Alzheimer's disease diagnosis that leverages attention mechanisms to enhance interpretability and multimodal feature learning. Experimental results on the real-world imaging-genetics dataset demonstrate that our approach achieves better classification performance than the existing state-of-theart methods in terms of both classification accuracy and correlation between the modalities. In an exploratory analysis, we further show that the biomarkers identified by our model are closely associated with Alzheimer's disease. Our proposed approach is applicable to other diseases with multimodal data available. However, the limited size of medical datasets may restrict the effectiveness and generalization ability of such deep learning models. To address this issue, a potential future direction is to employ pre-training and transfer learning techniques that facilitate learning across datasets."
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,1.0,Introduction,"One important task for the neuroscience community is to study atypical alterations in cortices associated with brain development, degeneration, or disorders. For this aim, recent approaches, namely interpretable and explainable deep learning, rely on the training of diagnostic or predictive deep learning models  Although explainable deep learning methods are being actively studied in the machine learning community, they have two challenges when applying to neuroimaging data. First, existing methods typically adopt post-hoc techniques to explain a deep network  This paper presents an explainable geometric deep network, called NeuroExplainer, with applications to uncover altered infant cortical development patterns associated with preterm birth. NeuroExplainer adopts high-resolution cortical attributes as the input to develop a hierarchical attention-decoding architecture working in the sperical space. Distinct to existing post-hoc methods, the NeuroExplainer is constructed as an end-to-end framework, where finegrained explanation factors can be identified in a fully learnable fashion. Our network take advantage of the explainability to boost classification for the highdimensional neuroimaging data. Specifically, in the framework of weakly supervised discriminative localization, our NeuroExplainer is trained by minimizing general classification losses coupled with a set of constraints designed according to prior knowledge regarding brain development. These targeted regularizers drive the network to implicitly optimize the explainability metrics from multiple aspects (i.e., fidelity, sparsity, and stability), thus capturing fine-grained explanation factors to explicitly improve classification accuracies. Experimental results on the public dHCP benchmark suggest that our NeuroExplainer led to quantitatively reliable explanation results that are qualitatively consistent with representative neuroimaging studies, implying that it could be a practically useful AI tool for other related cortical surface-based neuroimaging studies."
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,2.0,Method,As the schematic diagram shown in Fig. 
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,2.1,Spherical Attention Encoding,"The starting components of the encoding branch are four spherical convolution blocks (i.e., EB-1 to EB-4 in Fig.  Specifically, let F l and F r ∈ R 162×M0 be the vertex-wise representations (produced by EB-4) for the left and right hemispheres, respectively. We first concatenate them as a 324 × M 0 matrix, on which a self-attention operation  (1) where s o [i] (i = 0 or 1) in our study denote the prediction scores of preterm and fullterm, respectively, and 1 is an unit vector having the same row size with the subsequent matrix. Finally, we define the hemispheric attentions as , respectively, with values spatially varying and depending on the relevance to subject's category."
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,2.2,Hierarchically Spherical Attention Decoding,"The explanation factors captured by the encoding branch are relatively coarse, as the receptive field of a cell on the downsampled surfaces (with 162 vertices after three pooling operations) is no smaller than a hexagonal region of 343 cells on the input surfaces (with 10, 242 vertices). To tackle this challenge, we design a spherical attention decoding strategy to hierarchically propagate coarse attentions (from lower-resolution spheres) onto higher-resolution spheres, based on which fine-grained attentions are finally produced to improve classification. Specifically, NeuroExplainer contains three consecutive decoding blocks (i.e., DB-1 to DB-3 in Fig.  Fr in , respectively, where each row of Fin has M in channels, and denotes element-wise dot product. We first upsample F l G and F r G to the spatial resolution of the current DB, by using hexagonal transposed convolutions  where C θ (•) denotes 1-ring conv parameterized by θ, and ⊕ stands for channel concatenation. In terms of F D , the attention mechanism described in (1) is further applied to producing refined spherical attentions and classification scores. Finally, as shown in Fig. "
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,2.3,Domain Knowledge-Guided Explanation Enhancement,"To perform task-oriented learning of explanation factors, we design a set of targeted regularization strategies by considering fundamental domain knowledge regarding infant brain development. Specifically, according to existing studies, we assume that human brains in infancy have generally consistent developments, while the structural/functional discrepancies between different groups (e.g., preterm and term-born) are typically rationalized  Explanation Fidelity-Aware Contrastive Learning. Given the spherical attention block at a specific resolution, we have A + i and A - j ∈ R V ×1 as the output attentions for a positive and negative subjects (i.e., preterm and fullterm infants in our study), respectively, and F + i and F - j ∈ R V ×M are the corresponding representation matrices. Based on the prior knowledge regarding infant brain development, it is reasonable to assume that A + i highlights atypically-developed cortical regions caused by preterm birth. In contrast, the remaining part of the cerebral cortex of a preterm infant (corresponding to 1 -A + i ) still growths normally, i.e., looking globally similar to the cortex of a term-born infant. Accordingly, as the illustration shown in Fig.  be the holistic feature vector and its inverse for the ith (positive) sample, respectively. Similarly, denotes the holistic feature vector for the compared jth (negative) sample. By pushing f + i away from both f + i and f - j , while pulling f + i close to f - j , we define the respective loss as where i and j indicate any a pair of positive and negative cases from totally N training samples, and m is a margin setting as 1 in our implementation. Explanation Sparsity-Aware Regularization. According to the specified prior knowledge regarding infant brain development, the attention maps produced by our NeuroExplainer should have two featured properties in terms of sparsity. That is, the attention map for a preterm infant (e.g., A + i ) should be sparse, considering that altered cortical developments are assumed to be localized. In contrast, the attention map for a healthy term-born infant (e.g., A - j ) should not be spatially informative, as all brain regions growth typically without abnormality. To this end, we design a straightforward entropy-based regularization to enhance results' explainability, such as where i and j indicate a positive and a negative cases from totally N training samples, respectively, and 1 is an unit vector to sum up the values of all vertices. Explanation Stability-Aware Regularization. We enhance the explanation stability of our NeuroExplainer from two aspects. First, we require the spherical attention mechanisms to robustly decode from complex cortical-surface data finegrained explanation factors to produce accurate predictions. To this end, we randomize the surface coarsening step by quantifying a vertex's cortical attributes (on the downsampled surface) as the average of a random subset of the vertices from the respective hexagonal region of the highest-resolution surface, such as the examples summarized in Fig.  Second, as described in Sect. 2.2, we design a cross-scale consistency regularization to refine the decoding branch. Specifically, let A l i and A h i be the spherical attentions from two different DB blocks. We simply minimize which encourages spherical attentions at different resolutions to be consistent. Implementation Details. In our implementation, the feature representations produced by EB-1 to EB-4 in Fig.  where the tuning parameters were empirically set as λ 1 = 0.2, λ 3 = 0.5, and λ 3 = 0.1. The network parameters were updated by using Adam optimizer for 500 epochs, with the initial learning rate setting as 0.001 and bath size as 20."
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,3.0,Experiments,"Dataset and Experimental Setup. We conducted experiments on the dHCP benchmark  On the other hand, the explanation performance of our NeuroExplainer was compared with two representative feature-based explanation approaches, i.e., CAM  Classification Results. The classification results obtained by different competing methods are summarized in Table  Explanation Results. The quantitative explanation results are summarized in Table  Finally, we compared the individualized preterm-altered cortical development patterns uncovered by our NeuroExplainer with representative group-wise multimodal (dMRI and sMRI) quantitative analyses presented in "
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,4.0,Conclusion,"In the paper, we have proposed an geometric deep network, i.e., NeuroExplainer, to learn fine-grained explanation factors from complex cortical-surface data to boost discriminative representation extraction and accurate classification model construction. On the benchmark dHCP database, our NeuroExplainer achieved better performance than existing post-hoc approaches in terms of both explainability and prediction accuracy, in uncovering preterm-altered infant cortical development patterns. The proposed method could be a promising AI tool applied to other similar cortical surface-based neuroimage and neuroscience studies."
NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants,,Competing Mehtods,ACC AUC SEN SPE SphericalCNN 
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,1.0,Introduction,"Accurate segmentation of a variety of anatomical structures is a crucial prerequisite for subsequent diagnosis or treatment  In real-world scenarios, clinical databases are often sequentially constructed from various clinical sites with varying imaging protocols  At present, satisfactory methods applied in the realistic HSI setting are largely unavailable. F irst, recent structure-incremental works cannot deal with domain shift. Early attempts  In this work, we propose a unified HSI segmentor evolving framework with a divergence-aware decoupled dual-flow (D 3 F) module, which is adaptively optimized via HSI pseudo-label distillation using a momentum MixUp decay (MMD) scheme. To explicitly avoid the overwriting of previously learned parameters, our D 3 F follows a ""divide-and-conquer"" strategy to balance the old and new tasks with a fixed rigidity branch and a compensated learnable plasticity branch, which is guided by our novel divergence-aware continuous batch renormalization (cBRN). The complementary knowledge can be flexibly integrated with the model re-parameterization  Our main contributions can be summarized as follow: • To our knowledge, this is the first attempt at realistic HSI segmentation with both incremental structures of interest and diverse domains. • We propose a divergence-aware decoupled dual-flow module guided by our novel continuous batch renormalization (cBRN) for alleviating the catastrophic forgetting under domain shift scenarios. • The adaptively constructed HSI pseudo-label with self-training is developed for efficient HSI knowledge distillation. We evaluated our framework on anatomical structure segmentation tasks from different types of MRI data collected from multiple sites. Our HSI scheme demonstrated superior performance in segmenting all structures with diverse data distributions, surpassing conventional class-incremental methods without considering data shift, by a large margin."
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,2.0,Methodology,"For the segmentation model under incremental structures of interest and domain shift scenarios, we are given an off-the-shelf segmentor f θ 0 : X 0 → Y 0 parameterized with θ 0 , which has been trained with the data {x 0 n , y 0 n } N 0 n=1 in an initial source domain D 0 = {X 0 , Y 0 }, where x 0 n ∈ R H×W and y 0 n ∈ R H×W are the paired image slice and its segmentation mask with the height of H and width of W , respectively. There are T consecutive evolving stages with heterogeneous target domains D t = {X t , S t } T t=1 , each with the paired slice set {x t n } N t n=1 ∈ X t and the current stage label set {s t n } N t n=1 ∈ S t , where x t n , s t n ∈ R H×W . Due to heterogeneous domain shifts, X t from different sites or modalities follows diverse distributions across all T stages. Due to incremental anatomical structures, the overall label space, across the previous t stages, Y t is expanded from Y t-1 with the additional annotated structures for delineating all of the structures Y T seen in T stages."
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,2.1,cBRN Guided Divergence-Aware Decoupled Dual-Flow,"To alleviate the forgetting through parameter overwriting, caused by both new structures and data shift, we propose a D 3 F module for flexible decoupling and integration of old and new knowledge. Specifically, we duplicate the convolution in each layer initialized with the previous model f θ t-1 to form two branches as in  However, under the domain shift, it can be sub-optimal to directly average the parameters, since f r θ t may not perform well to predict Y t-1 on X t . It has been demonstrated that batch statistics adaptation plays an important role in domain generalizable model training  Of note, as a default block in the modern convolutional neural networks (CNN)  The recent BRN  where η ∈ [0, 1] is applied to balance the global statistics and the current batch. In addition, γ = σB σ and β = μB -μ σ are used in both training and testing. Therefore, BRN renormalizes zi = zi-μ σ to highlight the dependency on the global statistics {μ, σ} in training for a more generalizable model, while limited to the static learning. In this work, we further explore the potential of BRN in the continuously evolving HSI task to be general for all of domains involved. Specifically, we extend BRN to cBRN across multiple consecutive stages by updating {μ c , σ c } along with all stages of training, which is transferred as shown in Fig.  For testing, the two branches in final model f θ T can be merged for the lightweight implementation: Therefore, f T θ does not introduce additional parameters for deployment (Fig. "
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,2.2,HSI Pseudo-label Distillation with Momentum MixUp Decay,"The training of our developed f θ t with D 3 F is supervised with the previous model f θ t-1 and current stage data {x t n , s t n } N t n=1 . In conventional class incremental learning, the knowledge distillation  In this work, we construct a complementary pseudo-label ŷt n ∈ R H×W with a MixUp decay scheme to adaptively exploit the knowledge in the old segmentor for the progressively learned new segmentor. In the initial training epochs, f θ t-1 could be a more reliable supervision signal, while we would expect f θ t can learn to perform better on predicting Y t-1 . Of note, even with the rigidity branch, the integrated network can be largely distracted by the plasticity branch in the initial epochs. Therefore, we propose to dynamically adjust their importance in constructing pseudo-label along with the training progress. Specifically, we MixUp the predictions of f θ t-1 and f θ t w.r.t. Y t-1 , i.e., f θ t (•)[: t -1], and control their pixel-wise proportion for the pseudo-label ŷt n with MMD: where i indexes each pixel, and λ is the adaptation momentum factor with the exponential decay of iteration I. λ 0 is the initial weight of f θ t-1 (x t n:i ), which is empirically set to 1 to constrain λ ∈ (0, 1]. Therefore, the weight of old model prediction can be smoothly decreased along with the training, and f θ t (x t n:i ) gradually represents the target data for the old classes in [: t  In training, the overall optimization loss is formulated as follows: where α is used to balance our HSI distillation and SE minimization terms, and I max is the scheduled iteration. Of note, strictly minimizing the SE can result in a trivial solution of always predicting a one-hot distribution "
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,3.0,Experiments and Results,We carried out two evaluation settings using the BraTS2018 database 
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,3.1,Cross-Subset Structure Incremental Evolving,"In our cross-subset setting, three structures were sequentially learned across three stages: (CoreT with BraTS2013) → (EnhT with TCIA) → (ED with CBICA). Of note, we used a CoreT segmentator trained with BraTS2013 as our off-the-shelf segmentor in t = 0. Testing involved all subsets and anatomical structures. We compared our framework with the three typical structureincremental (SI-only) segmentation methods, e.g., PLOP  For the ablation study, we denote HSI-D 3 F as our HSI without the D 3 F module, simply fine-tuning the model parameters. HSI-cBRN used dual-flow to avoid direct overwriting, while the model was not guided by cBRN for more generalized prediction on heterogeneous data. As shown in Table "
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,3.2,Cross-Modality Structure Incremental Evolving,"In our cross-modality setting, three structures were sequentially learned across three stages: (CoreT with T1) → (EnhT with T2) → (ED with T2 FLAIR). Of note, we used the CoreT segmentator trained with T1 modality as our off-the-shelf segmentor in t = 0. Testing involved all MRI modalities and all structures. With the hyperparameter validation, we empirically set η = 0.01 and α 0 = 10. In Table "
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,4.0,Conclusion,"This work proposed an HSI framework under a clinically meaningful scenario, in which clinical databases are sequentially constructed from different sites/imaging protocols with new labels. To alleviate the catastrophic forgetting alongside continuously varying structures and data shifts, our HSI resorted to a D 3 F module for learning and integrating old and new knowledge nimbly. In doing so, we were able to achieve divergence awareness with our cBRN-guided model adaptation for all the data involved. Our framework was optimized with a self-entropy regularized HSI pseudo-label distillation scheme with MMD to efficiently utilize the previous model in different types of MRI data. Our framework demonstrated superior segmentation performance in learning new anatomical structures from cross-subset/modality MRI data. It was experimentally shown that a large improvement in learning anatomic structures was observed."
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,1.0,Introduction,"Colonoscopy is a critical tool for identifying adenomatous polyps and reducing rectal cancer mortality. Deep learning methods have shown powerful abilities in automatic colonoscopy analysis, including polyp segmentation "
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,,Diffusion Sampler,"Fig.  Despite recent progress in these methods for medical image analysis, existing models face two major challenges when applied to colonoscopy image analysis. Firstly, the foreground (polyp) of colonoscopy images contains rich pathological information yet is often tiny compared with the background (intestine wall) and can be easily overwhelmed during training. Thus, naive generative models may generate realistic colonoscopy images but those images seldom contain polyp regions. In addition, in order to generate high-quality annotated samples, it is crucial to maintain the consistency between the polyp morphologies in synthesized images and the original masks, which current generative models struggle to achieve. To tackle these issues and inspired by the remarkable success achieved by diffusion models in generating high-quality CT or MRI data  In summary, our contributions are three-fold: (1) Adaptive Refinement SDM: Based on the standard semantic diffusion model "
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,2.0,Method,"Background. Denoising diffusion probabilistic models (DDPMs)  where q (x 0 ) is the original data distribution with x 0 ∼ q (x 0 ), x 1:T are latents with the same dimension of x 0 and β t is a variance schedule. The reverse process is aiming to learn a model to reverse the forward process that reconstructs the original input data, which is defined as: (2) where p (x T ) is the noised Gaussian transition from the forward process at timestep T . In this case, we only need to use deep-learning models to represent μ θ with θ as the model parameters. According to the original paper "
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,,Re-Weighting Module,"Diffusion Loss ℒ Thus, instead of training the model μ θ to predict μt , we can train the model θ to predict ˜ , which is easier for parameterization and learning."
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,,Refinement Loss ℒ,"In this paper, we propose an adaptive refinement semantic diffusion model, a variant of DDPM, which has three key parts, i.e., mask conditioning, adaptive loss re-weighting, and prediction-guided sample refinement. The overall illustration of our framework is shown in Fig. "
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,2.1,Mask Conditioning,"Unlike the previous generative methods, our work aims to generate a synthetic image with an identical segmentation mask to the original annotation. To accomplish this, we adapt the widely used conditional U-Net architecture  where ∼ N (0, I) , α t := 1β t and ᾱt := t s=1 α s . It will be fed into the encoder E of the U-Net, and its corresponding mask annotation c 0 ∈ R H×W will be injected into the decoder D. The model output can be formulated as: Thus, the U-Net model θ in Eq. 3 becomes θ (x t , t, c 0 ), and the loss function in Eq. 3 is changed to: Algorithm 1: One training iteration of ArSDM 6 Take gradient descent step on ∇ θ L total"
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,2.2,Adaptive Loss Re-weighting,"The polyp regions in the colonoscopy images differ from the background regions, which contain more pathological information and should be adequately treated to learn a better model. However, training the diffusion models using the original loss function ignores the difference between different regions, where each pixel shares the same weights when calculating the loss. This would lead to the model generating more background-like polyps since the large background region will easily overwhelm the small foreground polyp regions during training. A simple way to alleviate this problem is to apply a weighted loss function that assigns the polyp and background regions with different weights. However, most polyps vary a lot in size and shape. Thus assigning constant weights for all polyps exacerbated the imbalance problem. In this case, to tackle this problem, we propose an adaptive loss function that vests different weights according to the size ratio of the polyp over the background. Specifically, we define a pixel-wise weights matrix W λ ∈ R H×W with each entry w λ i,j to be: where p = 1 means the pixel p at (h, w) belongs to the polyp region and p = 0 means it belongs to the background region. Thus, the loss function becomes:"
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,2.3,Prediction-Guided Sample Refinement,"The downstream tasks of polyp segmentation and detection require rich semantic information on polyp regions to train a good model. Through extensive experiments, we found inaccurate sample images with coarse polyp boundary that is not aligned properly with the original masks may introduce large biases and noises to the datasets. The model can be confused by several conflicting training images with the same annotation. To this end, we design a refinement strategy that uses the prediction of a pre-trained segmentation model on the sampled images to guide the training process and restore the proper polyp boundary information. Specifically, at each iteration of training, the output ˜ = θ (x t , t, c 0 ) will go into the sampler to generate sample image x0 . Then, we take the sample image as the input of the segmentation model to predict the pseudo masks c0 . We propose the following refinement loss based on IoU loss and binary cross entropy (BCE) loss between c0 and c 0 . The refinement loss is: where L = L IoU + L BCE is the sum of the IoU loss and BCE loss, c0 is the collection of the three side-outputs ( c3 , c4 , c5 ) and the global map cg as described in  3 Experiments"
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,3.1,ArSDM Experimental Settings,We conducted our experiments on five public polyp segmentation datasets: EndoScene 
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,3.2,Downstream Experimental Settings,"We conduct the evaluation of our methods and the state-of-the-art counterparts on polyp segmentation and detection tasks. We generated the same number of samples as the diffusion training set using the original masks, and then combined them to create a new downstream training set. We employed PraNet "
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,3.3,Quantitative Comparisons,The experimental results presented in Table  Ablation Study. We conducted an ablation study to assess the importance of each proposed component. Table 
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,3.4,Qualitative Analyses,"To further investigate the generative performance of our approach, we present visualization results in Fig. "
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,4.0,Conclusion,"Automatic generation of annotated data is essential for colonoscopy image analysis, where the scale of existing datasets is limited by the expertise and time required for manual annotation. In this paper, we propose an adaptive refinement semantic diffusion model (ArSDM) for generating colonoscopy images while preserving annotations by introducing innovative adaptive loss re-weighting and prediction-guided sample refinement mechanisms. To evaluate our approach comprehensively, we conduct polyp segmentation and detection experiments on five widely used datasets, where experimental results demonstrate the effectiveness of our approach, in which model performances are greatly enhanced with little synthesized data."
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_32.
Synthetic Augmentation with Large-Scale Unconditional Pre-training,1.0,Introduction,"The recent advancements in medical image recognition systems have greatly benefited from deep learning techniques  Although annotated data is typically hard to acquire for medical images, unannotated data is often more accessible. To mitigate the issue existed in current cGAN-based synthetic augmentation methods "
Synthetic Augmentation with Large-Scale Unconditional Pre-training,2.0,Methodology,Figure 
Synthetic Augmentation with Large-Scale Unconditional Pre-training,2.1,Diffusion Models,"Diffusion models (DM)  where variances β t are constants. If β t are small, the posterior q(z t-1 |z t ) can be well approximated by diagonal Gaussian  (2) The DM reverse process (also known as sampling) then generates samples z 0 ∼ p θ (z 0 ) by initiating a Markov chain with Gaussian noise z T ∼ N(0, I) and progressively decreasing noise in the chain of z T -1 , z T -2 , . . . , z 0 using the learnt p θ (z t-1 |z t ). To learn p θ (z t-1 |z t ), Gaussian noise is added to z 0 to generate samples z t ∼ q(z t |z 0 ), then a model θ is trained to predict using the following mean-squared error loss: where time step t is uniformly sampled from {1, . . . , T }. Then μ θ (z t ) and Σ θ (z t ) in Eq. 2 can be derived from θ (z t , t) to model p θ (z t-1 |z t ) "
Synthetic Augmentation with Large-Scale Unconditional Pre-training,2.2,HistoDiffusion,"Model Architecture. Our proposed HistoDiffusion is built on Latent Diffusion Models (LDM)  Unconditional Large-scale Pre-training. To ensure the latent space Z can cover features of various data types, we first pre-train our proposed His-toDiffusion on large-scale unlabeled datasets. Specifically, we gather unlabeled images from M different sources to construct a large-scale set of datasets S = {S 1 , S 2 , . . . , S M }. We then train an LAE using the data from S with the following self-reconstruction loss to learn a powerful latent space Z that can describe diverse features:  After training the LAE, we fixed the trained encoder E and then train a DM with the loss L DM in Eq. 3 to model E's latent space Z. Here z 0 = E(x) in Eq. 3. Once the DM is trained, we can use denoising model θ in the DM reverse sampling process to synthesize a novel latent z0 ∈ R h×w×c and employ the trained decoder D to generate a new image x = D(z 0 ), which should satisfy the similar distribution as the data in S. Conditional Small-scale Fine-tuning. Using the LAE and DM pretrained on S, we can only generate the new image x following the similar distribution in S. To generalize our HistoDiffusion to the small-scale labeled dataset S collected from a different source (i.e., S ⊂ S), we further fine-tune HistoDiffusion using the labeled data from S . Let y be the label of image x in S . To minimize the training cost, we fix both the trained encoder E and trained DM model θ to keep latent space Z unchanged. Then we only fine-tune the decoder D using labeled data (x, y) from S with the following loss function: LD = Lrec(x, x) + λCELCE(ϕ(x), y) , "
Synthetic Augmentation with Large-Scale Unconditional Pre-training,,Classifier-guided Conditional Synthesis.,"To enable conditional image generation with our HistoDiffusion, we further apply the classifier-guided diffusion sampling proposed in  where g is the guidance scale. Then the DM reverse process in HistoDiffusion can finally generate a novel latent z0 satisfying the class condition y through a Markov chain starting with a standard Gaussian noise z T ∼ N(0, I) using p θ,φ (z t-1 |z t , y) defined as follows: p θ,φ (zt-1|zt, y) = N (zt-1; μθ (zt|y), Σ θ (zt)) .  Selective Augmentation. To further improve the efficacy of synthetic augmentation, we follow "
Synthetic Augmentation with Large-Scale Unconditional Pre-training,3.0,Experiments,"Datasets. We employ three public datasets of histopathology images during the large-scale pre-training procedure. The first one is the H&E breast cancer dataset  To replicate a scenario where only a small annotated dataset is available for training, we have opted to utilize a subset of 5,000 (5%) samples for finetuning. This subset has been carefully selected through an even sampling without replacement from each tissue type present in the train set. It is worth noting that the labels for these samples have been kept, which allows the fine-tuning process to be guided by labeled data, leading to better predictions on the specific task or domain being trained. By ensuring that the fine-tuning process is representative of the entire dataset through even sampling from each tissue type, we can eliminate bias towards any particular tissue type. We evaluate the fine-tuned model on the official test set. The related data use declaration and acknowledgment can be found in our supplementary materials. Evaluation Metrics. We employ Fréchet Inception Distance (FID) score  Model Implementation. All the patches are resized to 256 × 256 × 3 before being passed into the models. Our implementation of HistoDiffusion basically follows the LDM-4  We use the same architecture for the auxiliary image classifier ϕ. For downstream evaluation, we implement the classifier using the ViT-B/16 architecture  Comparison to State-of-the-Art. We compare our proposed HistoDiffusion with the current state-of-the-art cGAN-based method "
Synthetic Augmentation with Large-Scale Unconditional Pre-training,4.0,Conclusions,"In this study, we have introduced a novel synthetic augmentation technique, termed HistoDiffusion, to enhance the performance of medical image recognition systems. HistoDiffusion leverages multiple unlabeled datasets for large-scale, unconditional pre-training, while employing a labeled dataset for small-scale conditional fine-tuning. Experiment results on a histopathology image dataset excluded from the pre-training demonstrate that given limited labels, HistoDiffusion with image selection remarkably enhances the classification performance of the baseline model, and can potentially handle any future incoming small dataset for augmented training using the same pre-trained model."
Synthetic Augmentation with Large-Scale Unconditional Pre-training,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 71.
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,1.0,Introduction,"A must-have ingredient for training a deep neural network (DNN) is a large number of labelled data that is not always available in real-world applications. This challenge of data annotation becomes even worse for medical image segmentation tasks that require pixel-level annotation by experts. Data augmentation (DA) is a recognized approach to tackle this challenge. Common DA strategies create new samples by using predefined transformations such as rotation, translation, and colour jitter to existing data, where the performance gains heavily relies on the choice of augmentation operations and parameters  To mitigate this reliance, recent efforts have focused on learning optimal augmentation operations for a given task and dataset  However, to date, all existing approaches to learning deformable registrationbased DA assume a perfect alignment of image pairs to learn the transformations. In other words, the deformation-based transformations are learned globally for the image. This assumption is restrictive and associated with several challenges. First, the learning of a global image-level transformation requires image alignment that may be non-trivial in many scenarios, such as the alignment of tumours that can appear at different locations of an image, or alignment of images from different modalities. The learning of transformations itself is also complicated by the presence of other objects in the image and is best suited when the object of interest is always in the same (and often centre) location in all the images, i.e., images are globally aligned a priori  Intuitively, object-centric transformations and augmentations have the potential to overcome the challenges associated with global image-level transformations. Recently, an object-centric augmentation method termed as TumorCP  Similarly, other existing works on object-level augmentation of lesions have mostly focused on position, orientation, and random transformations of the lesion on different backgrounds  In this paper, we present a novel approach for learning and transferring object-centric deformations for DA in medical image segmentation tasks. As illustrated in Fig.  -A generative model of object-centric deformations -constrained to C1 diffeomorphism for better DNN training -to describe shape variability learned from paired patches of objects of interest. This allows the learning to focus on the shape variations of an object regardless of its positions and sizes in the image, thus bypassing the requirement for image alignment. -An online augmentation strategy to sample transformations from the generative model and to augment the objects of interest in place without distorting the surrounding content in the image. This allows us to add shape diversity to the objects of interest in an image regardless of their positions or sizes, eventually facilitating transferring the learned variations across datasets. We demonstrated the effectiveness of the presented object-centric diffeomorphic augmentation in kidney tumour segmentation, including using shape variations of kidney tumours learned from the same dataset (KiTS "
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,2.0,Methods,We focus on DA for tumour segmentation because tumours can occur at different locations of an organ with substantially different orientations and sizes. It thus presents a challenging scenario where global image-level deformable transformations cannot apply. mentation approach comprises as outlined in Below we describe the two key methodological elements.
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,2.1,Object-Centric Diffeomorphism as a Generative Model,"The goal of this element is to learn to generate diffeomorphic transformation parameters θ that describe shape variations -in the form of deformable transformations T θ -that are present within training instances of tumour x. To realize this, we train a generative model G(.) for θ such that, when given two instances of tumours (x src , x tgt ), it is asked to generate θ from the encoded latent representations z in order to deform x src through T θ (x src ) to x tgt . Transformations: In order to model shape deformations between x src and x tgt , we need highly expressive transformations to capture rich shape variations in tumour pairs. We assume a spatial transformation T θ in the form of pixel-wise displacement field u as T θ (x) = x + u. Inspired from  where the integration can be done via a specialized solver  Generative Modeling: The data generation process can be described as: where z is the latent variable assumed to follow an isotropic Gaussian prior, p φ (θ|z) is modeled by a neural network parameterized by φ, and p(x tgt |θ, x src ) follows the deformable transformation as described in Equation (  We define variational approximations of the posterior density as q ψ (z|x src , x tgt ), modeled by a convolutional neural network that expects two inputs x src and x tgt . Passing a tuple of x src and x tgt as the input helps the latent representations to learn the spatial difference between two tumour samples. Alternatively, the generative model as described can be considered as a conditional model where both the generative and inference model is conditioned on the source tumour sample x src . Variational Inference: The parameters ψ and φ are optimized by the modified evidence lower bound (ELBO) of the log-likelihood log p(x tgt |x src ): where the first term in the ELBO takes the form of similarity loss: L 2 norm on the difference between x tgt and xsrc = T θ (x src ) synthesized using the θ from G(z). The second KL term constrains our approximated posterior q ψ (z|x src , x tgt ) to be closer to the isotropic Gaussian prior p(z), and its contribution to the overall loss is scaled by the hyperparameter β. To further ensure that xsrc looks realistic, we discourage G(z) from generating overly-expressive transformations by adding a regularization term over the L 2 norm of the displacement field u with a tunable hyperparameter λ reg . The final objective function becomes: Object-Centric Learning: To learn object-centric spatial transformations, x src and x tgt are in the forms of image patches that solely contain tumours. Given an image and its corresponding tumour segmentation mask (X, Y ), we first extract a bounding box around the tumour by applying skimage.measure.regionprops from the scikit-image package to Y . We then use this bounding box to carve out the tumour x from the image X, masking out all the regions within the bounding box that do not belong to the tumour. All the tumour patches are then resized to the same scale, such that tumours of different sizes can be described by the same tesselation resolution. When pairing tumour patches, we pair each tumour with its K nearest neighbour tumours based on their Euclidean distance -this again avoids learning overly expressive transformation when attempting to deform between significantly different tumour shapes."
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,2.2,Online Augmentations with Generative Models,"The goal of this element is to sample random object-centric transformations of T θ from G(z), to generate diverse augmentations of different instances of tumours in place. However, if we only transform the tumour and keep the rest of the image identical, the transformed tumour may appear unrealistic and out of place. To ensure that the entire transformed image appears smooth, we use a hybrid strategy to construct a deformation field for the entire image X that combines tumour-specific deformations with an identity transform for the rest of the image. Specifically, we fill a small region around the tumour with displacements of diminishing magnitudes, achieved by propagating the deformations from the boundaries of the deformation fields from G(z) to their neighbours with reduced magnitudes. Repeating this process ensures that the change at the boundaries is smooth and that the transformed region appears naturally as part of the image. "
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,3.0,Experiments and Results,"We used two publicly available datasets, LiTS "
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,,Model:,"The encoder of G(z) consisted of five convolutional layers and three fully connected layers, with a latent dimension of 12 for z. The decoder consisted of five fully connected layers to output the parameters θ for T θ . We trained the G(z) for a total of 400 epochs and a batch size of 16. We also implemented early stopping if the validation loss does not improve for 20 epochs. We used Adam optimizer  Results: We evaluated G(z) with two criteria. First, the model needs to be able to reconstruct x tgt by generating θ to transform x src . Second, the model needs to be able to generate diverse transformed tumour samples for a given tumour sample. Figure "
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,3.2,Deformation-Based da for Kidney Tumour Segmentation,"Data: We then used G(z) to generate deformation-based augmentations to increase the size and diversity of training samples for kidney tumour segmentation on KiTS. To assess the effect of augmentations on different sizes of labelled data, we considered training using 25%, 50%, 75%, and 100% of the KiTS training set. We considered two DA scenarios: augment with transformations learned from KiTS (within-data augmentation) versus from LiTS (cross-data augmentation). Models: For the base segmentation network, we adopted nnU-net  Results: We use Sørensen-Dice Coefficient (Dice) to measure segmentation network performance. Dice measures the overlap between prediction and ground truth. As summarized in Table "
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,4.0,Discussion and Conclusions,"In this work, we presented a novel diffeomorphism-based object-centric augmentation that can be learned and used to augment the objects of interest regardless of their position and size in an image. As demonstrated by the experimental results, this allows us to not only introduce new variations to unfixed objects like tumours in an image but also transfer the knowledge of shape variations across datasets. An immediate next step will be to extend the presented approach to learn and transfer 3D transformations for 3D segmentation tasks, and to enrich the shape-based transformation with appearance-based transformations. In the long term, it would be interesting to explore ways to transfer knowledge about more general forms of variations across datasets."
Few Shot Medical Image Segmentation with Cross Attention Transformer,1.0,Introduction,"Automatic segmentation of medical images is a fundamental step for a variety of medical image analysis tasks, such as diagnosis, treatment planning, and disease monitoring  To address the challenge of manual annotation, various label-efficient techniques have been explored, such as self-supervised learning  Most few-shot segmentation methods follow the learning-to-learn paradigm, which aims to learn a meta-learner to predict the segmentation of query images based on the knowledge of support images and their respective segmentation labels. The success of this paradigm depends on how effectively the knowledge can be transferred from the support prototype to the query images. Existing fewshot segmentation methods mainly focus on the following two aspects: (1) how to learn the meta-learner "
Few Shot Medical Image Segmentation with Cross Attention Transformer,2.2,Network Overview,The Overview of our CAT-Net is illustrated in Fig. 
Few Shot Medical Image Segmentation with Cross Attention Transformer,2.3,Mask Incorporated Feature Extraction,"The Mask Incorporate Feature Extraction (MIFE) sub-net takes query and support images as input and generates their respective features, integrated with the support mask. A simple classifier is then used to predict the segmentation for the query image. Specifically, we first employ a feature extractor network (i.e., ResNet-50) to map the query and support image pair I q and I s into the feature space, producing multi-level feature maps F q and F s for query and support image, respectively. Next, the support mask is pooled with F s and then expanded and concatenated with both F q and F s . Additionally, the segmentation mask of query image in MIFE is further concatenated with the query feature to strengthen the correlation between query and support features via a pixel-wise similarly map. Finally, the query feature is processed by a simple classifier to get the query mask. Further details of the MIFE architecture can be found in the supplementary material."
Few Shot Medical Image Segmentation with Cross Attention Transformer,2.4,Cross Masked Attention Transformer,"As shown in Fig.  Self-Attention Module. To capture the global context information of every pixel in the query feature F q 0 and support features F s 0 , the initial features are first flattened into 1D sequences and fed into two identical self-attention modules. Each self-attention module consists of a multi-head attention (MHA) layer and a multi-perceptron (MLP) layer. Given an input sequence S, the MHA layer first projects the sequence into three sequences K, Q, and V with different weights. The attention matrix A is then calculated as: where d is the dimension of the input sequence. The attention matrix is then normalized by a softmax function and multiplied by the value sequence V to get the output sequence O: The MLP layer is a simple 1×1 convolution layer that maps the output sequence O to the same dimension as the input sequence S. Finally, the output sequence O is added to the input sequence S and normalized using layer normalization (LN) to obtain the final output sequence X. The output feature sequence of the selfattention alignment encoder is represented by X q ∈ R HW ×D and X s ∈ R HW ×D for query and support features, respectively. Cross Masked Attention Module. We utilize cross masked attention to incorporate query features and support features with respect to their foreground information by constraining the attention region in attention matrix with support and query masks. Specifically, given the query feature X q and support features X s from the aforementioned self-attention module, we first project the input sequence into three sequences K, Q, and V using different weights, resulting in K q , Q q , V q , and K s , Q s , V s , respectively. Taking the support features as an example, the cross attention matrix is calculated by: We expand and flatten the binary query mask M q to limit the foreground region in attention map. The masked cross attention (MCA) map is computed as: Similar to self-attention, the support feature is processed by MLP and LN layer to get the final enhanced query features F s 1 . Similarly, the enhanced query feature F q 1 is obtained with foreground information from the query feature. Prototypical Segmentation Module. Once the enhanced query and support features are obtained, the prototypical segmentation is used to obtain the final prediction. First, a prototype of class c is built by masked average pooling of the support feature F s 1 as follows: where K is the number of support images, and m s (k,x,y,c) is a binary mask that indicates whether pixel at the location (x, y) in support feature k belongs to class c. Next, we use the non-parametirc metric learning method to perform segmentation. The prototype network calculates the distance between the query feature vector and the prototype P = {P c |c ∈ C}. Softmax function is applied to produce probabilistic outputs for all classes, generating the query segmentation: M q i,(x,y) = softmax αcos(F q i,(x,y) , p c ) • softmax(αcos(F q i,(x,y) , p c )) where cos(•) denotes cosine distance, α is a scaling factor that helps gradients to back-propagate in training. In our work, α is set to 20, same as in  1, M q i,(x,y) > τ 0, M q i,(x,y) < τ"
Few Shot Medical Image Segmentation with Cross Attention Transformer,2.5,Iterative Refinement Framework,"As explained above, the CMAT module is designed to refine the query and support features, as well as the query segmentation mask. Thus, it's natural to iteratively apply this sub-net to get the enhanced features and refine the mask, resulting in a boosted segmentation result. The result after the i-th iteration is represented by: The subdivision of each step can be specifically expressed as: where CMA(•) indicates the self-attention and cross masked attention module, and Proto(•) represents the prototypical segmentation module."
Few Shot Medical Image Segmentation with Cross Attention Transformer,3.1,Dataset and Evaluation Metrics,"We evaluate the proposed method on three public datasets, i.e., Abd-CT  To ensure a fair comparison, all the experiments are conducted under the 1-way 1-shot scenario using 5-fold cross-validation. We follow "
Few Shot Medical Image Segmentation with Cross Attention Transformer,3.2,Implementation Details,"The proposed method is implemented using PyTorch. Each 3D scan is sliced into 2D slices and reshaped into 256×256 pixels. Common 3D image pre-processing techniques, such as intensity normalization and resampling, are applied to the training data. We apply episode training with 20k iterations. SGD optimizer is adopted with a learning rate of 0.001 and a batch size of 1. Each episode training takes approximately 4 h using a single NVIDIA RTX 3090 GPU. "
Few Shot Medical Image Segmentation with Cross Attention Transformer,3.3,Comparison with State-of-the-Art Methods,"We compare the proposed CAT-Net with state-of-the-art (SOTA) methods, including SE-Net "
Few Shot Medical Image Segmentation with Cross Attention Transformer,3.4,Ablation Study,We conduct an ablation study to investigate the effectiveness of each component in CAT-Net. All ablation studies are conducted on Abd-MRI under setting II.
Few Shot Medical Image Segmentation with Cross Attention Transformer,,Effectiveness of CMAT Block:,"To demonstrate the importance of our proposed CAT-Net in narrowing the information gap between the query and supporting images and obtaining enhanced features, we conducted an ablation study. Specifically, we compared the results of learning foreground information only from the support (S →Q) or query image (Q→S ) and obtaining a single enhanced feature instead of two (S ↔Q). It can be observed that using the enhanced query feature (S →Q) achieves 66.72% in Dice, outperforming only using the enhanced support feature (Q→S ) by 0.74%. With our CMAT block, the mutual boosted support and query feature (S ↔Q) could improve the Dice by 1.90%. Moreover, the iteration refinement framework consistently promotes the above three variations by 0.96%, 0.56%, and 2.26% in Dice, respectively (Table "
Few Shot Medical Image Segmentation with Cross Attention Transformer,4.0,Conclusion,"In this paper, we propose CAT-Net, Cross Attention Transformer network for few-shot medical image segmentation. Our CAT-Net enables mutual interaction between the query and support features by the cross masked attention module, enhancing the representation abilities for both of them. Additionally, the proposed CMAT module can be iteratively applied to continually boost the segmentation performance. Experimental results demonstrated the effectiveness of each module and the superior performance of our model to the SOTA methods. In the future, we plan to extend our CAT-Net from 2D to 3D networks, explore the application of our model to other medical image segmentation tasks, as well as the extension of our model to other clinical applications, such as rare diseases and malformed organs, where data and annotations are scarce and costly."
Few Shot Medical Image Segmentation with Cross Attention Transformer,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_22.
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,1.0,Introduction,"Federated Learning (FL)  Based on the availability of labeled data, the existing SSFL studies can be classified into two main scenarios: (a) labels-at-client, with each client having some labeled and some unlabeled data  The classic federated averaging scheme aggregates weights of all labeled and unlabeled client models trained in parallel. The labeled clients typically use cross-entropy-based loss functions while the unlabeled clients primarily use consistency regularization loss  To address this question, we present a novel SSFL algorithm which we call IsoFed that effectively improves client training by isolating the model aggregation of labeled and unlabeled client groups while still leveraging one group of models to improve another. In summary, the primary contributions of this paper are: 1. We propose IsoFed, a novel SSFL framework, that realizes isolated aggregation of labeled and unlabeled client models in the server followed by federated self-supervised pretraining of the global model in each individual site. 2. This is the first work to reformulate model aggregation for fully labeled and fully unlabeled clients under SSFL settings. To the best of our knowledge, we are the first to isolate the aggregation of labeled and unlabeled client models while switching between the two client groups. 3. This work bridges the gap between Federated Learning and Transfer Learning (TL) "
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,2.1,Problem Description,"Assume a federated learning setting with m fully labeled clients denoted as {C 1 , C 2 , ..., C m } each possessing a labeled dataset D l = {(X l i , y l i )} N l i=1 and n fully unlabeled clients defined as {C m+1 , C m+2 , ..., C m+n } each possessing an unlabeled dataset D u = {(X u i )} N u i=1 . Our objective is to learn a global model θ glob via decentralized training."
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,2.2,Local Training,"We adopt mean-teacher-based semi-supervised learning  t,j where p t,i and pt,i denote each element in p t before and after sharpening, respectively. τ denotes the temperature parameter. The student model is trained on the local data (D u ) via consistency regularization with the teacher model output. The consistency regularization loss is defined as L MSE = pt -p s 2 2 where pt and p s are teacher and student predictions, respectively. . 2  2 denotes L2-norm. The student model weights are optimized via backpropagation whereas the teacher model weights are updated by exponential moving averaging (EMA) after each local iteration, as in Eq. 1: where α denotes momentum parameter. We optimize cross-entropy loss for local training on labeled clients defined as L CE = -y i log p i , where y i denotes labels."
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,2.3,Isolated Federated Aggregation,"In this section, we explain the proposed isolated aggregation of labeled and unlabeled client models. Each communication round is composed of two consecutive substeps. First, the server initializes the global model W t glob and sends it to unlabeled clients (U i ). The global model is used to initialize the teacher model W t in each client. At this stage, only the unlabeled clients perform local training on the global model by minimizing the consistency regularization loss. The updated semi-supervised models obtained after running the local epochs are then uploaded to the server. We adopt a dynamically weighted Federated Averaging scheme  where K is the total number of clients. n k is the number of samples in each client. The client models are then dynamically scaled using coefficients c k designed as functions of the individual distances from the averaged model as denoted in Eq. 3. The global model (W glob ) is updated by re-aggregating the client weights scaled by new coefficients c k . In Eq. 3, λ c is a hyperparameter. The updated global model parameters are then communicated to each labeled client which initializes its models using these weights and trains the local model via minimization of the standard cross-entropy loss. After a predefined number of local epochs, each labeled client uploads its local model to the server. The server then aggregates all the supervised models employing the aforementioned weighting scheme and the resultant global model W t+1 glob is then sent to each unlabeled client at the beginning of the next round. "
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,2.4,Client-Adaptive Pretraining,"Motivated by the recent success of continued pretraining in Natural Language Processing  For self-supervised pretraining, we jointly learn the client-invariant features and client-specific classifier by optimizing an information-theoretic metric called information maximization (IM) loss denoted as L inf in Eq. 4. It acts as an estimate of the expected misclassification error of the global model for each client. Optimizing the IM loss makes the global model output predictions that are individually certain but collectively diverse. With the help of a diversity preserving regularizer (first component in Eq. 4), IM avoids the trivial solution of entropy minimization where all unlabeled data collapses to the same one-hot encoding. The joint optimization is done by reducing the entropy of the output probability distribution of global model (p i ) in conjunction with maximizing the mutual information between the data distribution and the estimated output distribution yielded by the global model. where N is the number of classes. x denotes any instance belonging to a dataset D. The entropy minimization leads to the least number of confused predictions whereas the regularizer avoids the degenerate solution where every data sample is assigned to the same class "
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,3.1,Datasets and FL Settings,"To evaluate the performance and generalisability of the proposed method, we conduct experiments on four publicly available medical image benchmark datasets with different modalities  PathMNIST has 107,180 images and has 9 types of tissues. PneumoniaMNIST is a collection of 5,856 images and the task is binary classification (diseased vs normal). OrganAMNIST is comprised of 58,850 images and the task is multi-class classification of 11 body organs. We split each training dataset between 4 clients to mimic a practical collaborative setting in healthcare. To testify the versatility of the models, we study two challenging non-IID data partition strategies with 0.5 and 0.8-Dirichlet (γ). As a result, the number of samples per class and per client widely vary from each other. Additionally, we show the impact of varying the proportion of labeled clients (75%, 50%, 25%) on model performance. See Suppl. Sec 1 for more details."
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,3.2,Implementation and Training Details,"For all datasets, we employ a simple CNN comprising of two 5 × 5 convolution layers, a 2 × 2 max-pooling layer, and two fully-connected layers as the feature extraction backbone followed by a two-layer MLP and a fully-connected layer as the classification network. Our model is implemented with PyTorch. We follow the settings prescribed for a training RSCFed to enable a fair comparison. See Suppl. Sec 2 for more training details."
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,3.3,Results and Discussion,"We use the standard metrics -accuracy, area under a ROC curve (AUC), Precision, and Recall to evaluate performance. We observe that the dynamically weighted version of Fed-Avg (discussed in Sect. 2.3) outperforms standard Fed-Avg and hence use it as a baseline in this paper instead of vanilla Fed-Avg. In order to fairly evaluate IsoFed, we compare with the following state-of-theart SSFL benchmarks: (a) MT+wFed-Avg: a combination of Mean Teacher and dynamically weighted Fed-Avg, (b) RSCFed: Random sampling consensusbased FL  Table "
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,3.4,Ablation Study,"Owing to space constraints, we show ablation experiments only on OrganAM-NIST, which provides the most challenging classification task, to evaluate the impact of IsoFed components. (More results in Suppl. Sec 2). Table "
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,4.0,Conclusion,"We have introduced a novel SSFL framework called IsoFed, an isolated federated learning technique, to address joint training of labeled and unlabeled clients in the context of decentralized semi-supervised learning. It opens a new research direction in learning across domains by unifying two dominant approaches -Federated Learning (among labeled or unlabeled clients) and Transfer Learning (between labeled and unlabeled clients). Our results challenge the conventional strategy of co-training fully labeled and fully unlabeled clients in SSFL. Experimental results on 4 different medical imaging datasets with varied proportion of labeled clients (25, 50, 75%) and varied non-IID distribution (0.5 & 0.8-Dirichlet) show that IsoFed achieves a considerable boost compared to current state-of-the-art SSFL method. IsoFed can be easily incorporated into other federated learning-based aggregation schemes as well as used in conjunction with any other semi-supervised learning framework in federated learning setting."
Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 39.
FedContrast-GPA: Heterogeneous Federated Optimization via Local Contrastive Learning and Global Process-Aware Aggregation,1.0,Introduction,"Recently, federated learning has emerged as a promising strategy for performing privacy-preserving, distributed learning for medical image segmentation. Among various methods, FedAvg  Different from the above-mentioned methods, in this paper, we aim to tackle the ""client drift"" problem by exploring a unified latent feature space for different clients in a privacy-preserving manner and by enhancing the feature discriminability of each client. Concretely, we propose to extract local prototypes to represent the feature distribution at each client. Since local prototypes are statistical characteristics, we can share them among different clients without the concern of privacy issues. Then performing cross-client pixel to local prototype matching can help not only to perceive the global feature distribution but also to explicitly align the cross-client features, leading to a more unified latent feature space. Besides, by performing pixel to local prototype matching at each client, we can directly shape and enhance the discriminability of the learned feature space at each client. Another well-acknowledged concern of federated networks is the ""straggler"" problem caused by system-level heterogeneity. FedAvg and some of its variants  The contributions of our method can be summarized as follows: -We propose a novel FedContrast-GPA framework to simultaneously alleviate both data-level and system-level heterogeneity issues in federated optimization. -We propose an intra-client and inter-client local prototype based contrastive learning scheme, which not only enhances the feature discriminability of each client, but also explicitly performs cross-client feature distribution perception and alignment in a privacy-preserving manner. -We introduce a simple yet effective process-aware weighting scheme to suppress the influence of ""stragglers"" in global model aggregation. 2 Method A typical federated learning process consists of two stages: local update at each client and global aggregation at the server side. In this paper, we propose the FedContrast-GPA framework (as shown in Fig.  Assuming there exist K clients in the federated network, we denote client k as S k . Then private data set on the k-th client can be denoted as "
FedContrast-GPA: Heterogeneous Federated Optimization via Local Contrastive Learning and Global Process-Aware Aggregation,2.1,Intra-and Inter-client Local-Prototype based Contrastive Learning,"Local Prototype Learning. Denote the bottleneck features of class c on S k as where N c k represents the number of pixels belonging to class c in the intermediate feature maps. In order to model the feature distribution of S k from a statistical view, we propose to generate the class-specific local prototypes to capture semantic-aware feature distribution. Considering that the spatial coverage and visual changes may vary dramatically across different classes, we extend the method introduced in  where T c refers to the number of sub-clusters for class c), and the pixel-to-local-prototype mapping as  from all the semantic classes, where C is the total class number. Then contrastive learning is introduced to enforce compactness within a subcluster and separation among different sub-clusters. Specifically, the intra-client pixel-to-local-prototype contrastive loss is calculated as, where s   where <, > denotes the cosine similarity function. Please note, visual compactness is only imposed at the sub-cluster granularity, which means the local features should distribute faraway from not only sub-clusters of the other semantic classes, but also other sub-clusters of the same semantic class. Apart from the contrastive loss term, in Intra-LPCL, we also explicitly maximize the feature similarities between local features and their assigned local prototypes as, Then the final Intra-LPCL loss is calculated as, Inter-client Local-Prototype Based Contrastive Learning (Inter-LPCL) for Feature Alignment. The aim of Inter-LPCL is to perform distributed feature alignment across different clients in a privacy-preserving manner, such that the aggregated global model can generalize well across clients. Given the i-th local feature of class c from S k (i.e., f c,i k ), and the prototypes pool we don't know the cross-client pixel-to-prototype assignments. Thus, instead of imposing strict restrictions on sub-cluster compactness as done in Intra-LPCL, we loosen the alignment restrictions to category level. Specifically, the local features from S k are supposed to distribute closer to one of the sub-clusters belonging to the same class in S k , and faraway from sub-clusters of the other semantic classes. Mathematically, the inter-LPCL loss is calcualted as, where max(•) returns the maximum value in the set, {s c ,t k,k , t ∈ {1, • • • , T c }} denotes the similarity set calculated between f c,i k and all the local prototypes from class c in S k , which is formulated as, The final Inter-LPCL loss of S k is then calculated by averaging over k in L inter k,k , which is formally defined as, Overall Objective for Local Update. The overall loss function for updating local model from S k is formulated as, where λ 1 , λ 2 are the hyper-parameters, L seg k is the segmentation loss, where CE(•) denotes the cross entropy loss."
FedContrast-GPA: Heterogeneous Federated Optimization via Local Contrastive Learning and Global Process-Aware Aggregation,2.2,Process-Aware Global Model Aggregation,"During each federated communication, FedAvg updates the global model as weighted average over local models, where α k is the aggregation weight for S k , which is commonly set as N k k N k (N k is the number of images in S k ). Instead of weighting the local models by its data amount ratio, in this paper, we argue that the aggregation weights should reflect the training process of each local model (i.e., well-trained model that generates good segmentation results should contribute more during aggregation). Specifically, denote the mean Dice Similarity Coefficient obtained on the training and validation data of S k as DSC k , then the normalized weights in our method are calculated as, By introducing the process-aware aggregation scheme, we can effectively detect the straggler, improving the robustness of aggregated global model."
FedContrast-GPA: Heterogeneous Federated Optimization via Local Contrastive Learning and Global Process-Aware Aggregation,3.0,Experiments and Results,"Datasets and Implementation Details. We validate our method on the challenging task of prostate segmentation from 3D MR images. T2-weighted MRI images used in our study are collected from 6 different data sources  where each source is treated as a client in our study. We follow  Ablation Study on the Effectiveness of Each Component: We denote the process-aware global aggregation as GA p , then the detailed analysis on component effectiveness is presented in Table  To further demonstrate the advantage of our federated learning strategy, we conduct experiments to analyze the performance of centralized training and separate training, where centralized training is trained by updating the global model sequentially using private data from each client, while separate training is trained by updating each local client with only private data and no global communication. The average Dice performance for each client in centralized training is 73%, 77%, 84%, 72%, 86%, and 76%, respectively, while the average Dice performance for each client in separate training is 85%, 79%, 86%, 73%, 91%, and 27%, respectively. We can see that directly putting data together in centralized training does not bring performance gain due to data heterogeneity. Besides, in separate training, we can see a severe performance drop in some clients without enough learning data and knowledge from others. Analysis on the Straggler Mitigation Effect. To demonstrate the effectiveness of our method in straggler mitigation, we compare the client-specific DSC and HD95 performance between the baseline (FedAvg) and Ours. For clarity, we first define the ""stragglers"" in a federated learning network as follows: the clients whose performance from the baseline (FedAvg) rank among the worst half of all the clients. Note that the ""stragglers"" are recognized according to the performance of FedAvg, since our method aims to address the ""straggler"" problem in FedAvg. As shown in Fig.  From above analysis, we can see that the proposed method can achieve effective straggler mitigation by bringing larger performance gains over stragglers, and slightly boost the performance over the 'non-stragglers"". Comparison with State-of-the-Art Methods. We compare the performance of our method with five state-of-the-art (SOTA) methods, including FedAvg  To analyse the performance of our proposed FedContrast-GPA framework, we report the DSCs for all the distributed clients (i.e., S 1 -S 6 in Table "
FedContrast-GPA: Heterogeneous Federated Optimization via Local Contrastive Learning and Global Process-Aware Aggregation,4.0,Conclusions,"In this paper, we proposed a novel FedContrast-GPA framework to simultaneously address both the data-level heterogeneity and the system-level heterogeneity issues in federated networks. Extensive ablation studies and comparisons with the SOTA methods demonstrated the effectiveness of the proposed method."
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,1.0,Introduction,Deep learning methods provide state-of-the-art (SOTA) performance for a variety of medical image analysis tasks such as diabetic retinopathy grading  Previous works on GZSL in medical images have focused on the single class scenario where an image is assigned a single disease class  GZSL for natural images  1. We propose a novel feature disentanglement method where a given image is decomposed into class-specific and class agnostic features. This enables better feature learning of different classes and subsequently contributes to better feature synthesis in the multi-label scenario. 2. We use text embedding similarities to learn the semantic relationships between different labels. This contributes to more accurate learning of multilabel interactions at a global scale and guide feature generation to synthesize feature vectors that are realistic and preserve the multi-label relationship between disease labels. 3. We solve the GZSL classification problem in terms of cluster assignment. Class specific feature disentanglement performs better for multi-label classification  Prior Work: GZSL's objective is to recognize images from known and unknown classes. Many works have shown promising results using GANs 
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,2.0,Method,Method Overview: Given training data with seen classes we: 1) create a dictionary from the text embedddings; 2) disentangle the image into class specific and class agnostic features; 3) use class specific features to generate features of seen and unseen classes using the Mixup approach 
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,,Embeddings:,We generate embeddings of image class labels using BioBERT 
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,2.1,Feature Disentanglement,"Our feature disentanglement method is inspired from  Reconstruction Loss: L Rec , is the commonly used image reconstruction loss: It is a sum of the reconstruction losses from the class specific autoencoders. We train different autoencoders for images of each class in order to obtain class specific features and refer to them as 'Classspecific autoencoders'. Class Specific Loss: For given class l the class specific component z spec l i will have high similarity with samples from the same class and low similarity with the z spec k i of other classes k. These two conditions are incorporated as follows: where . denotes cosine similarity. The sum is calculated for all classes indexed by l and over all samples indexed by i, j. Class Agnostic Loss: Class agnostic features of different classes have similar semantic content and have high cosine similarity. L agn is defined as We want class specific and class agnostic features of same-class samples to be mutually complementary and have minimal overlap in semantic content, i.e., Since the above loss terms are minimized it helps us achieve our stated objectives.  Feature Generation Network: After disentangling the different seen class samples into their class specific components we create a distribution of each seen class feature. We generate synthetic class specific features of unseen classes using the following approach inspired by Mixup  where z specU k is the class specific synthetic vector for unseen classes k( = l), z specS l is a feature sampled from the distribution of seen class l, Λ l is a random number drawn from a beta distribution. ŷ is a one-hot encoded vector and is a sum of the one-hot label vectors of individual classes. Hence we do not need a weight when combining the label vectors. The weights Λ l are such that l Λ l = 1. Generating unseen class features through Mixup without additional constraints can generate unrealistic features. We use the dictionary of text embeddings to guide the feature generation process. As synthetic features of the seen and unseen classes are generated we cluster them using the online self supervised learning based SwAV method  where Cent All refers to the changing matrix of cluster centroid similarities for all seen and unseen classes. N is the total number of classes. The final loss term for clustering all class samples is L Clust = L(x s , x t ) + λ 4 L ML-Cluster where L(x s , x t ) is the SwAV loss function defined in  Training, Inference and Implementation: For a given test image we use the pre-trained L class specific autoencoders to get the class specific features. An input 256 × 256 image is passed through the Encoder having 3 convolution layers (64, 32, 32 3 × 3 filters ) each followed by max pooling. The Decoder is symmetric to the Encoder. z agn and z spec are 256-dimension vectors. We then calculate the cosine similarity of the class specific features with the corresponding class centroids. If the cosine similarity is above 0.5 then the sample is assigned to the class. Following standard practice for GZSL, average class accuracies are calculated for the seen (Acc S ) and unseen (Acc U ) classes, and also the harmonic mean H = 2×AccU ×AccS AccU +AccS ."
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,3.0,Experimental Results,Dataset Description. We demonstrate our method's effectiveness on the following chest xray datasets for multi-label classification tasks: 1.NIH Chest X-ray Dataset 
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,3.1,Generalized Zero Shot Learning Results,Classification results for medical images in Table 
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,3.2,Ablation Studies,"Table  We also investigate the influence of L ML-Cluster (Eq. 6) in the clustering process. The numbers in Table  Realism of Synthetic Features. We reconstruct the xray images from the synthetic feature vectors using the feature disentanglement autoencoders' decoder part. We select 1000 such synthetic images from 14 classes of the NIH dataset and ask two trained radiologists, having 12 and 14 years experience in examining chest xray images for abnormalities, to identify whether the images are realistic or not. Each radiologist was blinded to the other's answers. Results for ML-GZSL show one radiologist (RAD 1) identified 912/1000 (91.2%) images as realistic while RAD 2 identified 919 (91.9%) generated images as realistic. Both of them had a high agreement with 890 common images (89.0%) identified as realistic. Considering both RAD 1 and RAD 2 feedback, a total of 941 (94.1%) unique images were identified as realistic and 59/1000 (5.9%) images were not identified as realistic by any of the experts. ML-GZSL showed the highest agreement between RAD 1 and RAD 2."
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,4.0,Conclusion,"Our experiments demonstrate that our approach of multi label GZSL is more accurate than using conventional approaches that solve the single-label scenario. We propose a novel feature disentanglement approach that obtains class specific and class agnostic features from the training images. Additionally, the relationship between text embeddings of disease labels is used to create a dictionary that guides clustering and feature synthesis. Classification results on multiple publicly available chest xray datasets demonstrate the improved performance obtained by using class specific features. The synthetic features obtained by our method are realistic since a major percentage of the corresponding reconstructed images are validated as realistic by trained clinicians."
Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 26.
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,1.0,Introduction,"Chest X-ray (CXR) plays a vital role in screening and diagnosis of thoracic diseases  Recently, CLIP  In this paper, we propose a training method, CXR-CLIP, that integrates image-text data with image-label data using class-specific prompts made by radiologists. Our method does not depend on a rule-based labeler and can be applied to any image-label data. Also, inspired by DeCLIP  The main contributions of this paper are summarized as follows. 1) We tackle the lack of data for VLP in CXR by generating image-text pairs from image-label datasets using prompt templates designed by radiologists and utilizing multiple images and texts in a study. 2) Two additional contrastive losses are introduced to learn discriminate features of image and text, improving image-text retrieval performances. 3) Performance of our model is validated on diverse datasets with zero-shot and few-shot settings."
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,2.0,Related Work,Data Efficient VLP. Recent studies  Self-supervision Within CXR Study. A CXR study could include several images in different views and two report sections: 'findings' and 'impression'. The impression section includes the differential diagnosis inferred from the findings section. BioVIL  Leveraging Image-Label Data in VLP. MedCLIP 
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,3.0,Method,"CXR-CLIP samples image-text pairs from not only image-text data but also image-label data, and learns study-level characteristics with two images and two texts per study. The overview of the proposed method is illustrated in Fig. "
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,3.1,Data Sampling,"We define a CXR study as s = {X, T }, where X is a set of images, and T is a set of ""findings"" and ""impression"" sections. The study of image-label dataset has a set of image labels Y instead of T . For the image-label dataset, we make promptbased texts T = Concat({p ∼ P (y)} y∈Y ), where p is a sampled prompt sentence, P (y) is a set of prompts given the class name and value y, and Concat(•) means concatenating texts. The set of prompts is used to generate sentences such as actual clinical reports, taking into account class labels and their values (positive, negative, etc.), unlike the previous prompt  We sample two images (x 1 , x 2 ) in X if there are multiple images. Otherwise, we use augmented image A i (x 1 ) as x 2 , where A i is image augmentation. To leverage various information from different views in CXR (AP, PA, or lateral), we sample images from two distinct views as possible. Similarly, we sample two texts (t 1 , t 2 ) in T if there are both ""findings"" and ""impression"". Otherwise, we use augmented text A t (t 1 ) as t 2 , where A t is text augmentation. For the imagelabel data, we sample two prompt sentences as t 1 and t 2 from the constructed T = Concat({p ∼ P (y)} y∈Y )."
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,3.2,Model Architecture,"We construct image encoder E i and text encoder E t to obtain global representations of image and text, and a projection layer f i and f t to match the size of final embedding vectors. Image Encoder. We have tested two different image encoders; ResNet-50 "
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,3.3,Loss Function,"In this section, we first describe CLIP loss  (1) where τ is a learnable temperature to scale logits. In DeCLIP  (2) The goal of ICL and TCL is to learn modality-specific characteristics in terms of image and text respectively. We design ICL and TCL as same as CLIP loss, but the input embeddings are different. ICL only uses image embeddings; ICL pulls image embeddings from the same study and pushes image embeddings from the different studies, so that, the image encoder can learn study-level diversity. Similarly, TCL pulls embeddings of ""findings"" and ""impression"" in the same study or diverse expressions of prompts from the same label and pushes the other studies' text embeddings, so that the text encoder can match diverse clinical expressions on the same diagnosis. Thereby, the final training objective consists of three contrastive losses balanced each component by λ I and λ T , formulated by"
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,4.1,Datasets,"We used three pre-trained datasets and tested with various external datasets to test the generalizability of models. The statistics of the datasets used are summarized in Table  MIMIC-CXR  CheXpert  RSNA pneumonia  SIIM Pneumothorax VinDR-CXR  Open-I [3] is an image-text dataset. From each study, one of the report sections and one frontal-view image were sampled and used for image-to-text retrieval."
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,4.2,Implementation Details,"We used augmentations A i and A t to fit medical images and reports. For A i , we resize and crop with scale [0.8, 1.1], randomly adapt CLAHE "
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,4.3,Comparison with State-of-the-Arts,"Zero-Shot and Few-Shot Classification. Table  Model Name Pre-Train Dataset CheXpert5x200 MIMIC-CXR Open-I Total RSUM R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 shot classification fairly, we used evaluation prompts suggested from previous works  Image-to-Text Retrieval. We evaluated image-to-text retrieval computed by R@K, the recall of the exact report in the top K retrieved reports for a given image. (Table "
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,,Method,CheXpert 5x200 MIMIC-CXR Total RSUM ACC R@1 R@5 R@10 R@1 R@5 R@10 Vanila CLIP 58.9 4. 
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,4.4,Ablations,"For the ablation study, models with ResNet-50  In the second ablation study, CXR-CLIP was compared to DeCLIP "
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,5.0,Conclusion,"We presented a framework enlarging training image-text pair by using imagelabel datasets as image-text pair with prompts and utilizing multiple images and report sections in a study. Adding image-label datasets achieved performance gain in classification tasks including zero-shot and few-shot settings, on the other hand, lost the performance of retrieval tasks. We also proposed loss functions ICL and TCL to enhance the discriminating power within each modality, which effectively increases image-text retrieval performance. Our additional loss functions are designed to efficiently learn CXR domain knowledge along with image-text contrastive learning."
CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_10.
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,1.0,Introduction,"Model generalizability is one of the main challenges of AI, especially in high stake applications such as healthcare. While NN models achieve state-of-the-art (SOTA) performance in disease classification  Standard interpretable by design method  Our Contributions. This paper proposes a novel data-efficient interpretable method that can be transferred to an unseen domain. Our interpretable model is built upon human-interpretable concepts and can provide sample-specific explanations for diverse disease subtypes and pathological patterns. Beginning with a BB in the source domain, we progressively extract a mixture of interpretable models from BB. Our method includes a set of selectors routing the explainable samples through the interpretable models. The interpretable models provide First-order-logic (FOL) explanations for the samples they cover. The remaining unexplained samples are routed through the residuals until they are covered by a successive interpretable model. We repeat the process until we cover a desired fraction of data. Due to class imbalance in large CXR datasets, early interpretable models tend to cover all samples with disease present while ignoring disease subgroups and pathological heterogeneity. We address this problem by estimating the class-stratified coverage from the total data coverage. We then finetune the interpretable models in the target domain. The target domain lacks concept-level annotation since they are expensive. Hence, we learn a concept detector in the target domain with a pseudo labeling approach "
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,2.0,Methodology,"Notation. Assume f 0 : X → Y is a BB, trained on a dataset X ×Y ×C, with X , Y, and C being the images, classes, and concepts, respectively; f 0 = h 0 •Φ, where Φ and h 0 is the feature extractor and the classifier respectively. Also, m is the number of class labels. This paper focuses on binary classification (having or not having a disease), so m = 2 and Y ∈ {0, 1}. Yet, it can be extended to multiclass problems easily. Given a learnable projection  three functions: (1) a set of selectors (π : C → {0, 1}) routing samples to an interpretable model or residual, (2) a set of interpretable models (g : C → Y), and (3) the residuals. The interpretable models are called ""experts"" since they specialize in a distinct subset of data defined by that iteration's coverage τ as shown in SelectiveNet "
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,2.1,Distilling BB to the Mixture of Interpretable Models,"Handling Class Imbalance. For an iteration k, we first split the given coverage τ k to stratified coverages per class as , where w m denotes the fraction of samples belonging to the m th class; N m and N are the samples of m th class and total samples, respectively. Learning the Selectors. At iteration k, the selector π k routes i th sample to the expert (g k ) or residual (r k ) with probability π k (c i ) and 1 -π k (c i ) respectively. For coverages {τ k m , ∀m}, we learn g k and π k jointly by solving the loss: where θ * s k , θ * g k are the optimal parameters for π k and g k , respectively. R k is the is the empirical mean of samples of m th class selected by the selector for the associated expert g k . We define L k (g k ,π k ) in the next section. The selectors are neural networks with sigmoid activation. At inference time, π k routes a sample to g k if and only if π k (.) ≥ 0.5."
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,,Learning the Experts.,"For iteration k, the loss L k (g k ,π k ) distills the expert g k from f k-1 , BB of the previous iteration by solving the following loss: is the cumulative probability of the sample covered by the residuals for all the previous iterations from 1, • • • , k -1 (i.e., k-1 j=1 1 -π j (c i ) ) and the expert g k at iteration k (i.e., π k (c i )). Learning the Residuals. After learning g k , we calculate the residual as, of logits). We fix Φ and optimize the following loss to update h k to specialize on those samples not covered by g k , effectively creating a new BB f k for the next iteration (k + 1): We refer to all the experts as the Mixture of Interpretable Experts (MoIE-CXR). We denote the models, including the final residual, as MoIE-CXR+R. Each expert in MoIE-CXR constructs sample-specific FOLs using the optimization strategy and algorithm discussed in "
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,2.2,Finetuning to an Unseen Domain,"We assume the MoIE-CXR-identified concepts to be generalizable to an unseen domain. So, we learn the projection t t for the target domain and compute the pseudo concepts using SSL "
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,3.0,Experiments,"We perform experiments to show that MoIE-CXR 1) captures a diverse set of concepts, 2) does not compromise BB's performance, 3) covers ""harder"" instances with the residuals in later iterations resulting in their drop in performance, 4) is finetuned well to an unseen domain with minimal computation. Experimental Details. We evaluate our method using 220,763 frontal images from the MIMIC-CXR dataset  Baseline. We compare our method with 1) end-to-end CEM  MoIE-CXR Captures Diverse Explanations. Figure  Table "
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,,Model,Effusion  
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,,MoIE-CXR does not Compromise BB's Performance. Analysing MoIE-CXR:,Table 
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,,Analysing MoIE-CXR+R:,"To compare the performance on the entire dataset, we additionally report MoIE-CXR+R, the mixture of interpretable experts with the final residual in Table  Identification of Harder Samples by Successive Residuals. Figure  Applying MoIE-CXR to the Unseen Domain. In this experiment, we utilize Algorithm 1 to transfer MoIE-CXR trained on MIMIC-CXR dataset to Stanford Chexpert "
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,4.0,Conclusion,"This paper proposes a novel iterative interpretable method that identifies instance-specific concepts without losing the performance of the BB and is effectively fine-tuned in an unseen target domain with no concept annotation, limited labeled data, and minimal computation cost. Also, as in the prior work, MoIEcaptured concepts may not showcase a causal effect that can be explored in the future."
Distilling BlackBox to Interpretable Models for Efficient Transfer Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 59.
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,1.0,Introduction,"The absence of highly accurate and noninvasive diagnostics for risk-stratifying benign vs malignant solitary pulmonary nodules (SPNs) leads to increased anxiety, costs, complications, and mortality "
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,,Present Work.,"In this work, we jointly learn from longitudinal medical imaging, demographics, billing codes, medications, and lab values to classify SPNs. We converted 9195 non-imaging event streams from the EHR to longitudinal curves to impute cross-sections and synchronize across modalities. We use Independent Component Analyses (ICA) to disentangle latent clinical signatures from these curves, with the hypothesis that the disease mechanisms known to be important for SPN classification can also be captured with probabilistic independence. We leverage a transformer-based encoder to fuse features from both longitudinal imaging and clinical signature expressions sampled at intervals ranging from weeks to up to five years. Due to the importance of time dynamics in SPN classification, we use the time interval between samples to scale self-attention with the intuition that recent observations are more important to attend to than older observations. Compared with imaging-only and a baseline that aggregates longitudinal data into bins, our approach allowed us to incorporate additional modalities from routinely collected EHRs, which led to improved SPN classification."
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,2.0,Methods,"Latent Clinical Signatures via Probabilistic Independence. We obtained event streams for billing codes, medications, and laboratory tests across the full record of each subject in our EHR cohorts (up to 22 years). After removing variables with less than 1000 events and mapping billing codes to the SNOMED-CT ontology  We use an ICA model to estimate a linear decomposition of the observed curves from the EHR-Pulmonary cohort to independent latent sources, or clinical signatures. Formally, we have dataset D EHR-Pulmonary = {L k | k = 1, . . . , n} with longitudinal curves denoted as L k = {l i |i = 1, . . . , 9195}. We randomly sample l i ∀i ∈  Longitudinal Multimodal Transformer (TDSig). We represent our multimodal datasets D Image-EHR and }, where T is the maximum sequence length. We set T = 3 and added a fixed padding embedding to represent missing items in the sequence. Embeddings that incorporate positional and segment information are computed for each item in the sequence (Fig.  Time-Distance Self-attention. Following  This is a flipped sigmoid function that monotonically decreases with the relative time from the most recent observation. Its slope of decline and decline offset are governed by learnable non-negative parameters b and c respectively. A separate TEM is instantiated for each attention head, with the rationale that separate attention heads can learn to condition on time differently. The transformer encoder computes query, key, and value matrices as linear transformations of input embedding H = { Ê Ĝ} at attention head p TEM-scaled self-attention is computed via element-wise multiplication of the query-key product and R: where M is the padding mask [31] and d is the dimension of the query and key matrices. ReLU gating of the query-key product allows the TEM to adjust the attention weights in an unsigned direction. Baselines. We compared against a popular multimodal strategy that aggregates event streams into a sequence of bins as opposed to our method of extracting instantaneous cross-sectional representations. For each scan, we computed a TF-IDF [27] weighted vector from all billing codes occurring up to one year before the scan acquisition date. We passed this through a published Word2Vec-based medical concept embedding "
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,3.0,Experimental Setup,"Datasets. This study used an imaging-only cohort from the NLST [28] and three multimodal cohorts from our home institution with IRB approval (Table  Reclassification Analysis. We performed a reclassification analysis of low, medium, and high-risk tiers separated by thresholds of 0.05 and 0.65, which are the cutoffs used to guide clinical management. Given a baseline comparison, our approach reclassifies a subject correctly if it predicts a higher risk tier than the baseline in cases, or a lower risk tier than the baseline in controls (Fig. "
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,4.0,Results,The significant improvement with TDSig over CSSig demonstrates the advantage of longitudinally in the context of combining images and clinical signatures (Table 
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,5.0,Discussion and Conclusion,"This work presents a novel transformer-based strategy for integrating longitudinal imaging with interpretable clinical signatures learned from comprehensive multimodal EHRs. We demonstrated large performance gains in SPN classification compared with baselines, although calibration of our models is needed to assess clinical utility. We evaluated on clinically-billed SPNs, meaning that clinicians likely found these lesions difficult enough to conduct a clinical workup. In this setting, we found that adding clinical context increased the performance gap between longitudinal data and single cross-sections. Our clinical signatures incorporated longitudinality and additional modalities to build a better representation of clinical context than binned embeddings. We release our implementation at https://github.com/MASILab/lmsignatures. The lack of longitudinal multimodal datasets has long been a limiting factor  Our approach of sampling cross-sections where clinical decisions are likely to be made scales well with long, multi-year observation windows, which may not be true for BERT-based embeddings "
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,1.0,Introduction,"In recent years, Deep Learning (DL) based methods have achieved considerable success in the medical domain for tasks including disease diagnosis and clinical feature segmentation  Ophthalmologists use the segmentation of ocular Optical Coherence Tomography (OCT) images to diagnose, and treatment of eye diseases such as Diabetic Retinopathy (DR) and Diabetic Macular Edema (DME) "
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,2.0,Related Work,"Active learning is a cost-effective strategy that selects the most informative samples for annotation to improve model performance based on uncertainty  Many AL methods have been adopted for segmentation tasks  However, viewpoint information is not always available in medical imaging. We leverage edge information as a prior for AL sampling based on previous studies where edge information has improved the performance of segmentation tasks  There has not been sufficient work other than "
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,3.0,Methodology,Figure 
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,3.1,Segmentation Network,"We trained our OCT semantic segmentation model using a randomly selected small portion of the labeled data D s , seed set, keeping the rest for oracle imitation. We choose Y-net-gen-ffc (YN * ) without pre-retrained weight initialization as our primary architecture due to its superior performance "
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,3.2,Uncertainty in Prediction,"EdgeAL seeks to improve the model's performance by querying uncertain areas on unlabeled data D u after training it on a seed set D s . To accomplish this, we have created a novel edge-based uncertainty measurement method. We compute the edge entropy score and edge divergence score -to assess the prediction ambiguity associated with the edges. Figure  Entropy Score on Edges. Analyzing the edges of raw OCT inputs yields critical information on features and texture in images. They may look noisy, but they summarize all the alterations in a picture. The Sobel operator can be used to identify edges in the input image  To determine the probability that each pixel in an image belongs to a particular class c, we use the output of our network, denoted as P (m,n) i (c). We adopt Monte Carlo (MC) dropout simulation for uncertainty sampling and average predictions over |D| occurrence from  Following Zhao et al.  We name φ m,n i (c) as contextual probability and define our edge entropy by following entropy formula of  Divergence Score on Edges. In areas with strong edges/gradients, edge entropy reflects the degree of inconsistency in the network's prediction for each input pixel. However, the degree of this uncertainty must also be measured. KLdivergence is used to measure the difference in inconsistency between P (m,n) i and φ (m,n) i for a pixel (m, n) in an input image based on the idea of self-knowledge distillation I i  where measures the difference between model prediction probability and contextual probability for pixels belonging to edges of the input (Fig. "
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,3.3,Superpixel Selection,"Clinical images have sparse representation, which can be beneficial for active learning annotation  We compute mean edge entropy EE r i and mean edge divergence ED d i for a particular area r within a superpixel. Where |r| is the amount of pixels in the superpixel region. We use regional entropy to find the optimal superpixel for our selection strategy and pick the one with the most significant value based on the literature  (i, r) = arg max (j,s) EE s j  (p, q) = arg max After each selection, we remove the superpixels from R. The selection process runs until we have K amount of superpixels being selected from R. After getting the selected superpixel maps, we receive the matching ground truth information for the selected superpixel regions from the oracle. The model is then freshly trained on the updated labeled dataset for the next active learning iteration."
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,4.0,Experiments and Results,"This section will provide a detailed overview of the datasets and architectures employed in our experiments. Subsequently, we will present the extensive experimental results and compare them with other state-of-the-art methods to showcase the effectiveness of our approach. We compare our AL method with nine well-known strategies: softmax margin (MAR) "
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,4.1,Datasets and Networks,"To test EdgeAL, we ran experiments on Duke "
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,4.2,Comparisons,"Figure  Other AL methods, CEAL, RMCDR, CORESET, and MAR, do not perform consistently in all three datasets. We used the same segmentation network YN * and hyperparameters (described in Sect. 3) for a fair comparison. Our 5-fold CV result in Table "
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,5.0,Conclusion,"EdgeAL is a novel active learning technique for OCT image segmentation, which can accomplish results similar to full training with a small amount of data by utilizing edge information to identify regions of uncertainty. Our method can reduce the labeling effort by requiring only a portion of an image to annotate and is particularly advantageous in the medical field, where labeled data can be scarce. EdgeAL's success in OCT segmentation suggests that a significant amount of data is not always required to learn data distribution in medical imaging. Edges are a fundamental image characteristic, allowing EdgeAL to be adapted for other domains without significant modifications, which leads us to future works."
EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 8.
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,1.0,Introduction,"Automatic multi-organ segmentation (MOS) plays a vital role in computeraided diagnosis and treatment planning. Recently, deep learning based methods have made remarkable progress in solving MOS tasks. However, they typically require a large amount of expert-level accurate, densely-annotated data for training, which is laborious and time consuming to collect. Therefore, existing fully labeled datasets (termed as FLDs) are very few and often low in sample size  Although witnessed great progress in PSMOS, existing methods are faced with the following challenges: 1) Shortage in sufficiently labeled samples for supervised learning, since voxel-level labels are only available for a subset of organs in PLDs; 2) Significant cross-site appearance variations caused by different imaging protocols or subject cohorts. Different from existing methods, we propose a novel framework to explicitly tackle the above-mentioned challenges. To handle the label-scarcity problem in PLDs, we propose a novel Affinityaware Consistency Learning (ACL) scheme to incorporate voxel-to-organ affinity in the embedding space into consistency learning. Although consistency learning is frequently used for leveraging unlabeled data in label-efficient learning  To tackle the data discrepancy problem  -We propose a novel affinity-aware consistency learning scheme to incorporate voxel-to-organ affinity in the embedding space into a consistency learning framework, which can capture semantic context in the latent feature space. -We design a novel cross site feature alignment module to calibrate feature distributions of PLDs with distribution priors learned from a small-sized FLD, alleviating the cross-site data discrepancy. -We demonstrate on five datasets collected from different sites that our method can effectively learn a unified MOS model from multi-source datasets, achieving superior performance over the state-of-the-art (SOTA) methods. A schematic illustration of our framework. ""Aug"" refers to perturbations with data augmentations. In the CSFA module, hollow shapes refer to the features belonging to unlabeled organs in the PLDs, while solid ones refer to labeled organs. The affinity matrix is calculated according to Eq. 10 and Eq. 11. Lseg is the segmentation loss."
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,2.0,Methodology,"To learn a unified model from a small-sized FLD and a number of PLDs, we propose a novel framework to address the issues of label-scarcity and cross-site data discrepancy. The overall workflow of our method is presented in Fig.  During training, in each batch, we sample 3D patches from both the FLD and one of the PLDs, where the teacher-student scheme "
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,2.1,Preliminaries,"Denote Y full as the full label set, i.e., Y full = {0, 1, 2, • • • , C}, where 0 refers to the background class, and {1, • • • , C} are one-to-one mappings to the target organs, C is the number of target organs. Given a small-sized FLD D f and a number of PLDs where N is the number of PLDs. Each dataset can then be formally defined as either , where I f j,i is the i-th pixel of the j-th image in the FLD D f , and y f j,i is its corresponding label. Similarly, (I n j,i , y n j,i ) is the i-th pixel-label pair of the j-th image in the n-th PLD D n p . Please note that each D n p contains only a subset of the full label set, i.e., Y n p = unique({y n j,i }) Y full , where unique(•) returns the unique values in the label set. The task of PSMOS aims to learn the mapping function ϕ = f • g to project the 3D image patch I j ∈ R h×w×z to its corresponding semantic labels, where f is the feature extractor, g is the segmentation head, and • means sequentially executing f and g, (h, w, z) are the 3D patch size. Since foreground organ in one PLD may be labeled as background in another dataset, such a background ambiguity brings challenges to joint training on multiple PLDs. To address this issue, we follow "
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,2.2,Prototype Generation,"In our proposed framework, the calculation of both the pixel-to-prototype predictions (in ACL) and the feature alignment loss (in CSFA) are based on organspecific prototypes. In each mini-batch, denote the organ-specific prototypes for the FLD as {q c }, c ∈ {0, • • • , C} and prototypes for the n-th PLD as {q n c }, c ∈ {0, • • • , C}, then they are generated as follows. On the FLD, we generate the prototypes in an exponential moving average scheme. Specifically, the feature prototype of the t-th iteration is calculated as (for brevity, we omit the iteration superscript t), where q update c is the average feature of the c-th class in current mini-batch of the FLD and α is the weighting coefficient. Given the feature maps F = {f i } and their related labels {y i }, where f i represents the i-th pixel in the feature maps of current mini-batch, the feature center of the c-th class is then calculated as, where Z c is the number of pixels belonging to the c-th class in current mini-batch. On the n-th PLD, we directly adopt the feature centers calculated in each mini-batch as the organ-specific prototypes. In specific, for the labeled organs, the prototypes {q n c }, c ∈ Y n p are calculated according to Eq. 2, with feature maps generated on 3D patches from the n-th PLD. While on the unlabeled organs, only reliable features are used for calculating the pseudo feature centers as, where p n i is the normalized prediction score generated from the teacher model, y i denotes the corresponding pseudo label, τ is the confidence threshold, Z c is the number of reliable predictions in class c, and 1[•] returns 1 if the inside condition is True, otherwise, returns 0."
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,2.3,Affinity-Aware Consistency Learning,"In this paper, we propose to incorporate the voxel-to-organ affinity into consistency learning. Specifically, instance-to-prototype matching is calculated to capture the voxel-to-organ affinity. The affinities are then transformed into normalized scores for calculating the consistency constraint on two perturbed inputs. We adopt the teacher-student scheme  where < •, • > calculates the cosine similarity between the two terms. Similarly, in the student branch, denote ψ i as the i-th feature Since p t,i , p s,i model the voxel-to-organ affinities in the embedding space, constraining consistency on them introduces rich context information for training on the unlabeled data, which is formulated as, where 1 is the normalization factor to get the mean KL-Divergence in the feature embedding space. Denote ϕ tea = f tea • g tea , ϕ stu = f stu • g stu as the teacher and student segmentation model respectively, the logits from the student and the teacher branch can be calculated as l s,i = ϕ stu (I s,i ), l t,i = ϕ tea (I t,i ). Then the consistency loss in the label space is calculated as, where 1 is the normalization factor. The overall affinity-aware consistency loss is finally formulated as,"
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,2.4,Cross-Site Feature Alignment (CSFA) Module,"The CSFA module is proposed to calibrate feature distributions across different sites. Specifically, given the learned prototypes from current mini-batch of the n-th PLD ({q n c }, c ∈ {0, • • • , C}), they can be regarded as the organ-specific cluster centers in the embedding space. Then, compactness loss is introduced to calibrate D n p with the cluster centers learned from the FLD as, where |Y n p | returns the number of labeled organs in D n p . To further take into consideration the inter-organ affinity relationships during feature distribution alignment, we first model inter-organ affinity relationships on the FLD by calculating the affinity matrix A = {a ij } ∈ R (C+1)×(C+1) as shown in Fig.  Similarly, we can obtain the affinity matrix Then the affinity relationship aware feature alignment loss is calculated as, where a c , a n p,c refer to the c-th row of A and A n p respectively. The overall cross-site alignment loss is then calculated as the sum of the compactness loss and the affinity relationship aware calibration loss, The overall training objective is finally formulated as, where λ a is the tradeoff parameter."
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,3.0,Experiments and Results,"Datasets and Implementation Details. We use five abdominal CT datasets (MALBCVWC  Comparison with the State-of-the-Art (SOTA) Methods. We compare with four SOTA methods, including PaNN "
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,4.0,Conclusion,"In this paper, we designed a novel Affinity-aware Consistency Learning scheme (ACL) to model voxel-to-organ affinity context in the feature embedding space into consistency learning. Meanwhile, the CSFA module was designed to perform feature distribution alignment across different sites, where both organ-specific cluster centers and the inter-organ affinity relationships were propagated from the small-sized FLD to PLDs for cross-site feature alignment. Extensive ablation studies validated effectiveness of each component in our method. Quantitative and Qualitative comparison results with other SOTA methods demonstrated superior performance of our method."
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,1.0,Introduction,"There is a growing interest in leveraging brain imaging data to predict non-brain-imaging phenotypes in individual participants, since brain functional activity could intrinsically serve as an ""objective"" observer of a subject given that the emergence of behavior and cognition were widely attributed to the orchestration of local and remote cortical areas by means of a densely connected brain network  However, in recent works  Integration of multimodal data has been realized by using a graph to present the relation between subjects, where subjects are defined as nodes, node feature is fMRI connectivity, and edge is estimated by thresholding the similarity of behaviors, including eye movement  In the following sections, we firstly introduce the dataset and preprocessing steps. Basics of CensNet and SENet, and proposal of A-CensNet are introduced followed by its application to our task. Comparative and ablation studies regarding to prediction accuracy (AUC) were presented in the Results to demonstrate the effectiveness of multiple channel integration strategy, and the better performance of A-CensNet than others."
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,2.1,Dataset,In the Human Connectome Project (HCP) 7T release 
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,2.2,Preprocessing,"FMRI data have been preprocessed by the minimal preprocessing pipeline for the Human Connectome Project  In this study, subcortical regions are not our major interest and not included. Destrieux atlas  For eye tracking data, time stamps are used to extract the effective data points and synchronize the eye behavior features across subjects. Blink session is not considered. Since many phenotypic measures within a domain could be correlated with one another, we perform principal components analysis (PCA) to measures classified in ""cognition"" domains "
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,2.3,Classification of Population via Attention-CensNet,"Basics of CensNet. Supposing we have a dataset of M subjects, our objective is to assign each subject a cognitive group label l. We construct a graph G = {V, E, A} to represent the entire cohort as shown in Fig.  For spectral graph convolution, normalized graph Laplacian of a graph G = {V, E, A} is computed: L = I N -D -1/2 AD -1/2 where I N is the identity matrix and D is the diagonal degree matrix. One of the important steps is the layer-wise propagation rule based on an approximated graph spectral kernel as follows: where Ã = A + I N and D is the degree matrix, H l and W l are the hidden feature matrix and learnable weight of the l th layer. On this basis, the CensNet is proposed to have both node and edge convolution layers. For convenience, the graph (white box) in Fig.  where e , T ∈ R Nv×Ne is a binary matrix that indicates whether an edge connects a node. P e is a learnable weight vector, denotes the diagonalization operation. denotes the element-wise product. The loss function is defined as: where Y L is the subset of nodes with labels, M is the softmax results of the last node layer where node feature map has F dimensions. Squeeze-and-Excitation Attention Block. The SENet  It is important to note that previous works "
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,3.1,Implementation Details,"It was demonstrated in He et al., 2020 that the behavior score prediction accuracy via regression drops dramatically when subject number is below 100 (no more than r = 0.1 via Spearman correlation). Since we only have 81 subjects, a low regression accuracy cannot be a trustworthy to be used to compare with state-of-the-arts. As a compromise solution, we adopted classification scheme to demonstrate the effectiveness of our proposed framework. In our application, subjects are divided to four cognitive groups (around 20 subjects in each one, a total of 81 subjects). Then, we randomly split the dataset to training, validation and testing sets, respectively. We randomly selected 10 subjects from each group, a total of 40 subjects (about 50% of the total) to form the training set. Among the remaining 41 people, we randomly selected 20 people to be the verification group and 21 people to be the testing group (each accounting for 25% of the total). Such a random division of training/validation/testing subsets was repeated 100 times, independently. The results (AUCs) were the average of 100 independent replicates. We experiment on preserving {10%, 15%, 20%} top graph and edges by their weights, and find that preserving 10% node feature and 10% edges yields the best prediction performance. We try different settings of learning rate from {0.05, 0.01, 0.005, 0.001}, dropout {0.2, 0.3, 0.4, 0.5}, hidden {16, 32, 64, 128, 512, 1024}, and found that the best performance is yielded by learning rate to 0.005, dropout to 0.2, hidden to 1024. We implement the A-CensNet structure by adding the Attention mechanism based on CensNet, which empirically produce the best performance. AUC is adopted for each independent replication experiment to evaluate the prediction performance."
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,3.2,Ablation Study,"The prediction accuracy measured by AUC of 100 repeated experiments are reported in Table  In the non-attention algorithms, CensNet on movie2 dataset ({mfMRI2, Pupil2}) yields the best performance. Concatenation of node feature or edge feature from two movie datasets even decreases the accuracy. When attention module is added (Attention-Middle), two-channel models (gray rows) do not significantly increase the accuracy. Within one movie dataset, when node feature is fixed and eye trajectory and pupil size variation are taken as two channels, the performance is not better than that on single-channel model on movie3 dataset. When two movie datasets are considered (car-neose&green rows), they are integrated by means of channels when models with attention are used. In contrast, in non-attention models, the two movie datasets are integrated by means of feature concatenation. It is seen that the channel integration outperforms feature concatenation when pupil size is used as edges (green). But this does not hold when eye trajectory was used as edges (carneose). When both trajectory and pupil size are both considered as two channels, but features from two dataset are concatenated (white rows), the prediction performance is worst (46.21 ± 0.55) in all attention models. Finally, when both trajectory and pupil size from two dataset are used as four channels (blue row) in our algorithm, the best performance was yielded. These comparisons suggest that integration of edge features by channel attention does not always improves the performance than single use of an edge (gray rows), while integration of datasets by channel attention significantly outperforms the integration by feature concatenation. This observation still holds for ""Attention-Before"" models. But their accuracy is not as high as that via ""Attention-Middle"" models (see the crosscomparison between blue rows and gray rows), suggesting that a node-edge embedding could yield latent features more sensitive to individual variations, such that a channel attention works better at this deep feature space than being applied immediately after the original shallow features (""Attention-Before""). Finally, in addition to the 4-group classification, we evenly divided the subjects into 6 and 8 groups, respectively. AUCs via our model are 56.34 ± 0.65 and 56.95 ± 0.48, demonstrating the robustness of the algorithm to class numbers."
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,3.3,Comparison with State-of-the-Arts,We compare our results with the ones by state-of-the-art methods listed in Table 
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,,Models,"Data AUC Edge Node Linear [8]  -mfM RI2 41.94±0.81 GCN [16]  Trj2 mfM RI2 48.51±0.94 CensNet [17]  mfM RI2 49.75±0.65 CensNet [17]  Trj2 mfM RI2 50.36±0.71 Ppl2 52.91±0.73 FNN [9]  - All nonlinear deep neural networks yield significant improvement in contrast to the linear method (41.94 ± 0.81). A concatenation of mfMRI features (mfMRI2 +3 rows) does not improve the prediction accuracy in contrast to that on a single dataset, suggesting the importance of the strategy selection for concatenating features from multiple datasets. Within a single movie dataset (unshaded rows), models integrating mfMRI and eye tracking outperform the models (FNN and BrainNetCNN) that used single modality (mfMRI). After integrating mfMRI and eye behavior from multiple datasets, our results outperform all the-state-of-arts. These results demonstrate the effectiveness of integration of brain activity and eye behavior to one framework in cognition prediction. Also, given the limited subjects, multiple loads of stimuli integrated via attention modules could significantly improve the prediction performance."
Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet,4.0,Conclusion,"We propose A-CensNet to predict subjects' cognitive scores, with subjects taken as nodes, mfMRI derived functional connectivity as node feature, different eye tracking features are used to compute similarity between subjects to construct heterogeneous graph edges. These graphs from different dataset are all taken as different channels. The proposed model integrates graph embeddings from multiple channels into one. This model outperforms the one using single modality, single channel and state-of-the-art methods. Our results suggest that the brain functional activity patterns and the behavior patterns might complement each other in interpreting trait-like phenotypes, and might provide new clues to studies of diseases with cognitive abnormality "
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,1.0,Introduction,"Globally, cancer is a leading cause of death and the burden of cancer incidence and mortality is rapidly growing  Recently, many computational tools have shown to be effective in analyzing pathology images  In this study, we propose a centroid-aware feature recalibration network (CaFeNet) for accurate and robust cancer grading in pathology images. CaFeNet is built based upon three major components: 1) a feature extractor, 2) a centroid update (Cup) module, and 3) a centroid-aware feature recalibration (CaFe) module. The feature extractor is utilized to obtain the feature representation of pathology images. Cup module obtains and updates the centroids of class labels, i.e., cancer grades. CaFe module adjusts the input embedding vectors with respect to the class centroids (i.e., training data distribution). Assuming that the classes are well separated in the feature space, the centroid embedding vectors can serve as reference points to represent the data distribution of the training data. This indicates that the centroid embedding vectors can be used to recalibrate the input embedding vectors of pathology images. During inference, we fix the centroid embedding vectors so that the recalibrated embedding vectors do not vary much compared to the input embedding vectors even though the data distribution substantially changes, leading to improved stability and robustness of the feature representation. In this manner, the feature representations of the input pathology images are re-calibrated and stabilized for a reliable cancer classification. The experimental results demonstrate that CaFeNet achieves the state-of-the-art cancer grading performance in colorectal cancer grading datasets. The source code of CaFeNet is available at https://github.com/col in19950703/CaFeNet."
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,2.0,Methodology,The overview of the proposed CaFeNet is illustrated in Fig. 
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,2.1,Centroid-Aware Feature Recalibration,"Let {x i , y i } N i=1 be a set of pairs of pathology images and ground truth labels where N is the number of pathology image-ground truth label pairs, x i ∈ R h×w×c is the i th pathology image, y i ∈ {C 1 , . . . , C M } represents the corresponding ground truth label. h, w, and c denote the height, width, and the number of channels, respectively. M is the cardinality of the class labels. Given x i , a deep neural network f maps x i into an embedding space, producing an embedding vector e i ∈ R d . The embedding vector e i is fed into 1) a centroid update (Cup) module and 2) a centroid-aware feature recalibration (CaFe) module. Cup module obtains and updates the centroid of the class label in the embedding space E C ∈ R M ×D . CaFe module adjusts the embedding vector e i in regard to the embedding vectors of the class centroids and produces a recalibrated embedding vector e R i . e i and e R i are concatenated together and is fed into a classification layer to conduct cancer grading.  from the centroid embedding vectors E C by using a linear layer. Then, attention scores are computed via a dot product between Q E and K C followed by a softmax operation. Multiplying the attention scores by V C , we obtain the recalibrated feature representation E R for the input embedding vectors E. The process can be formulated as follows: Finally, CaFe concatenates E and E R and produces them as the output."
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,2.2,Network Architecture,We employ EfficientNet-B0 
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,3.1,Datasets,Two publicly available colorectal cancer datasets 
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,3.2,Comparative Experiments,"We conducted a series of comparative experiments to evaluate the effectiveness of CaFeNet for cancer grading, in comparison to several existing methods: 1) three DCNNbased models: ResNet "
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,3.3,Implementation Details,"We initialized all models using the pre-trained weights on the ImageNet dataset, and then trained them using the Adam optimizer with default parameter values (β 1 = 0.9, β 2 = 0.999, ε = 1.0e-8) for 50 epochs. We employed cosine anneal warm restart schedule with initial learning rates of 1.0 e -3 , η min = 1.0 e -3 , and T 0 = 20. After data augmentation, all patches, except for those used in ViT "
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,3.4,Result and Discussions,"We evaluated the performance of colorectal cancer grading by the proposed CaFeNet and other competing models using five evaluation metrics, including accuracy (Acc), precision, recall, F1-score (F1), and quadratic weighted kappa (κ w ). Table  Were the best performing models. Among DCNN-based models, ResNet was superior to other DCNN-based models. Metric learning was able to improve the classification performance. EffcientNet was the worst model among them, but with the help of triplet loss (Triplet) or supervised contrastive loss (SC), the overall performance increased by ≥2.8% Acc, ≥0.023 precision, ≥0.001 recall, ≥0.010 F1, and ≥0.047 κ w . Among the transformer-based models, Swin was one of the best performing models, but ViT showed much lower performance in all evaluation metrics. Moreover, we applied the same models to C TestII to test the generalizability of the models. We note that C TestI originated from the same set with C Train and C Validation and C TestII was obtained from different time periods and using a different slide scanner. Table  We conducted ablation experiments to investigate the effect of the CaFe module on cancer classification. The results are presented in Table "
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,4.0,Conclusions,"Herein, we propose an attention mechanism-based deep neural network, called CaFeNet, for cancer classification in pathology images. The proposed approach proposes to improve the feature representation of deep neural networks by re-calibrating input embedding vectors via an attention mechanism in regard to the centroids of cancer grades. In the experiments on colorectal cancer datasets against several competing models, the proposed network demonstrated that it has a better learning capability as well as a generalizability in classifying pathology images into different cancer grades. However, the experiments were only conducted on two public colorectal cancer datasets from a single institute. Additional experiments need to be conducted to further verify the findings of our study. Therefore, future work will focus on validating the effectiveness of the proposed network for other types of cancers and tissues in pathology images."
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,1.0,Introduction,"Artificial intelligence (AI) research has evolved rapidly, and unprecedented breakthroughs have been made in many fields. Applications of AI products can be witnessed in our daily life, such as autonomous driving, computer-aided diagnosis, automatic voice customer service, etc. The development of AI is undoubtedly a revolution in the course of human history. The most effective and commonly used AI model is the one based on deep neural networks  XAI is an old area in AI research, but was named relatively recently  In this study, we developed and presented an original XAI approach that can reveal the learned functions of groups of neurons in a neural network, which we call ""functional explanations"" and define as explanations of the model behavior by a combination of functions, as opposed to the visualization of a pattern to which a neuron responds. To our knowledge, there is no XAI method that offers functional explanations. Thus, our method is a post-hoc method that offers both instance-based and model-based functional explanations. We applied our XAI method to an MTANN model to emphasize the explainability and trustability of the MTANN, so that users can trust the MTANN."
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,2.1,MTANN Deep Learning,"In the field of image processing, supervised nonlinear filters and edge enhancers based on an artificial neural network (ANN)  An MTANN is a deep learning model consisting of linear-output artificial neural network regression model that directly operates on pixels in an input image, as shown in Fig. "
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,2.2,Sensitivity-Based Structure Optimization,"The numbers of hidden layers and their units in an MTANN model are adjustable hyperparameters. A relatively large structure is used to ensure that the model performs well on a specific task. A trained large model, however, may contain redundant units, and functions of neurons for the task would be ""distributed and diluted"" in many neurons in the model. This makes the analysis of the functions of neurons very difficult  To address this issue, we applied our sensitivity-based structure optimization algorithm  (Initialize the index of the hidden layer until in the -th hidden layer) for in do (Go through each hidden layer) if do (This layer has only one unit which cannot be removed) Skip to the next iteration for in do (Go through each hidden unit in the -th hidden layer) Remove the -th hidden unit in the -th hidden layer from temporarily the loss value of on if do Put the -th hidden unit in the -th hidden layer back to ← (Copy current model's weights and structure) Remove the -th hidden unit in the -th hidden layer from permanently return With the proposed optimization algorithm, the hidden units of MTANN could be gradually removed until the performance drops greatly when any of the rest unit is deleted."
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,2.3,Calculation of Weighted Function Maps,"After applying the structure optimization algorithm, every hidden unit in the compact model is expected to have an essential function for the target task. To understand the functions of the hidden units, function maps were obtained by performing the MTANN convolution of a hidden unit over a given input image. For better discrimination between enhancement and suppression, the function maps were normalized and then multiplied by the sign of the weight between the hidden units and the output unit. Weighted function maps were finally generated by shifting the range of the function map by 0.5. Namely, for a given hidden unit, in the weighted function map, a pixel value >0.5 means enhancement of patterns in the input image, whereas a pixel value <0.5 means suppression."
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,2.4,Unsupervised Hierarchical Clustering,"To group similar functions of the hidden units of the MTANN, we applied an unsupervised hierarchical clustering algorithm  where SSIM is the structural similarity index, and NMRSE is the normalized root mean square error. With the unsupervised hierarchical clustering algorithm, we visualize the function maps of the hidden units group by group to explain the behavior of each group of the hidden units."
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,3.1,Dynamic Contrast-Enhanced Liver CT,"Our XAI technique was applied to explain the MTANN model's decision in a liver tumor segmentation task  In addition, since most liver tumors' shape is ellipsoidal, the liver tumors can also be enhanced by the Hessian-based method and utilized in the model to improve the performance  Seven cases and 24 cases in the dynamic contrast-enhanced CT scans dataset were used for training and testing, respectively. 10,000 patches were randomly selected from the liver mask region in each case, summing up to a total of 70,000 training samples for training. The number of input units in the MTANN model with one hidden layer was 250. The structure optimization process started with 80 hidden units in the hidden layer. The binary cross-entropy (BCE) loss function was used to train the model. The MTANN model classified the input patches into tumor or non-tumor classes, and the output pixels represented the probability of being a tumor class. During the structure optimization process, the F1 score on the training patches and the Dice coefficient on the training images were also calculated as the reference to select a suitable compact model that performed equivalently to the original large model. As observed in the four evaluation metric curves in Fig.  Then, we applied the unsupervised hierarchical clustering algorithm to the weighted function maps from the optimized compact model with 9 hidden units. Figure "
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,4.0,Conclusion,"In this study, we proposed a novel XAI approach to explain the functions and behavior of an MTANN model for semantic segmentation of liver tumors in CT. Our structure optimization algorithm refined the structure and made every hidden unit in the model have a clear, meaningful function by removing redundant hidden units and ""condensing"" the functions into fewer hidden units, which solved the issue of unstable XAI results with conventional XAI methods. The unsupervised hierarchical clustering algorithm in our XAI approach grouped the hidden units with a similar function into one group so as to explain their functions by group. Through the experiments, we successfully proved that the MTANN model was explainable by functions."
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,1.0,Introduction,"Vision Transformers (ViTs) are self-attention based neural networks that have achieved state-of-the-art performance on various medical imaging tasks  Federated Learning (FL) enables clients to collaboratively learn a global model by aggregating locally trained models  Recent studies  In this work, we build upon the FeSTA framework  To overcome the above limitations, we propose a framework called Federated Split learning of Vision transformer with Block Sampling (FeSViBS). Our primary novelty is the introduction of a block sampling module, which randomly selects an intermediate transformer block for each client in each training round, extracts intermediate features, and distills these features into a pseudo cls token using a shared projection network. The proposed approach has two key benefits: (i) it effectively leverages intermediate ViT features, which are completely ignored in FeSTA, and (ii) sampling these intermediate features from different blocks, rather than relying solely on an individual block's features or the final cls token, serves as a feature augmentation strategy for the network, enhancing its generalization. The contributions of this work can be summarized as follows: i. We propose the FeSViBS framework, a novel federated and split learning framework that leverages the features learned by intermediate ViT blocks to enhance the performance of the collaborative system. ii. We introduce block sampling at the server level, which acts as a feature augmentation strategy for better generalization."
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,2.0,Methodology,"We first describe the working of a typical split vision transformer before proceeding to describe FeSViBS. , where N c is the number of training samples available at client c, x represents the input data, and y is the class label. Following  Here, Φ l represents the parameters of the l th transformer block and denotes the complete set of parameters of the transformer body. During training, the client performs a forward pass of the input data through the head to produce an embedding h c = H θc (x c ) ∈ R 768×M of its local data, which is typically organized as M patch tokens representing different patches of the input image. These embeddings (smashed representations) are then sent to the server. The ViT appends an additional token called the class token (cls ∈ R 768×1 ) and utilizes the self-attention mechanism to obtain a representation b c = B Φ (h c ) ∈ R 768×1 , which is typically the cls token resulting from the last transformer block. This cls token is returned to the client for further processing. The tail at each client projects the received class token representation b c into a class probability distribution to get the final prediction ŷc = T ψc (b c ). This marks the end of the forward pass. Subsequently, the backpropagation starts with computing loss c (y c , ŷc ), where c (.) represents the client's loss function between the true labels y c and predicted labels ŷc . The gradient of this loss is propagated back in the reverse order from the client's tail, server's body, to the client's head. We refer to this setting as Split Learning of Vision Transformer (SLViT), where each client optimizes the following objective in each round: In FeSTA "
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,2.1,FeSViBS Framework,"The proposed FeSViBS method is illustrated in Fig.  where z c,l ∈ R 768×M . The server then projects the extracted intermediate features into a lower dimension using a projection network R (shared across all blocks) to obtain the final representation b c,l = R π (z c,l ), where b c,l ∈ R 768×1 . This final representation b c,l can be considered as a pseudo class token and the role of the projection network is to distill the discriminative information contained in the intermediate features into this pseudo class token. The primary motivation for block sampling is to effectively leverage intermediate ViT features that are better at capturing local texture information (but are lost when only the final cls token is used). Stochasticity in the block selection serves as a feature augmentation strategy, thereby aiding the generalization performance. The architecture of the projection network is shown in Fig.  In the FeSViBS framework, the heads and tails of all the clients are assumed to have the same network architecture. Within each collaboration round, all the clients perform the forward and backward passes. While the parameters of the relevant head and tail as well as the shared projection network are updated after every backward pass, the parameters of the ViT body are updated only at the end of a collaboration round after aggregating updates from all the clients. The above protocol until this step is referred to as SViBS, because there is still no federation of the heads and tails. Similar to FeSTA, we also perform aggregation of the local heads and tails periodically in unifying rounds, resulting in the final FeSViBS framework. While in SViBS, the clients can initialize their heads and tails independently, FeSviBS requires a common initialization by the server and sharing of aggregated head and tail parameters after a unifying round."
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,3.0,Experimental Setup,"Datasets. We conduct our experiments on three medical imaging datasets. The first dataset is HAM10000  end if 21: end for categories of pigmented lesions; we randomly perform 80%/20% split for training and testing, respectively. The second dataset  Server's Network. For the server's body, we chose the ViT-B/16 model from timm library "
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,4.0,Results and Analysis,Following 
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,5.0,Ablation Study,"Set of ViT Blocks. To study the impact of ViT blocks from which the intermediate features are sampled on the overall performance of FeSViBS, we carry out experiments choosing different sets of blocks. The results depicted in Fig.  FeSViBS with Differential Privacy. Differential Privacy (DP)  Number of Unifying Rounds. We investigated the impact of reducing communication rounds (unifying rounds) on FeSViBS performance. However, our results showed that performance was maintained even with decreasing the number of communication rounds. Computational and Communication Overhead. Except for MOON and SCAFFOLD, all methods in Table "
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,6.0,Conclusion and Future Directions,"We proposed a novel Federated Split Learning of Vision Transformer with Block Sampling (FeSViBS), which utilizes FL, SL and sampling of ViT blocks to enhance the performance of the collaborative system. We evaluate FeSViBS framework under IID and non-IID settings on three real-world medical imaging datasets and demonstrate consistent performance. In the future, we aim to (i) extend our work and evaluate the privacy of FeSViBS under the presence of malicious clients/server, (ii) evaluate FeSViBS in the context of natural images and (iii) extend the current framework to multi-task settings."
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,1.0,Introduction,"The human brain is a complex and economically organized system, consisting of interconnected regions that form a hierarchical brain network  In network science, hub nodes are often classified as provincial or connector hubs based on the information of network modules  To address these challenges, we propose a novel learning-based spatiotemporal hub identification method. Unlike existing approaches that treat each temporal network as an independent static network, our method jointly identifies a set of temporal hub nodes using a dynamic graph embedding. Specially, due to dynamic graph embedding vectors "
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,2.1,Dynamic Graph Embedding Learning,"Dynamic Brain Network. Suppose we observe a time-varying brain network consisting of N brain regions, we define the network over time as a dynamic graph G = (V, E, T ), where V = {V(t)} t∈T is the node set over time, E = {E(t)} t∈T is a collection of edges over time, and T is the time span. For each temporal point t ∈ T = [0, T ], there is a graph snapshot G t of N nodes with the node-to-node connectivity degrees encoded in an N × N adjacency matrix, denoted as Temporal Hub Identification. Given a temporal network G t , our goal is to find K hubs in each temporal network. The locations of these temporal hubs are indexed by a binary diagonal matrix , where s i = 0 indicates the i th node is a hub, and s i = 1 otherwise. To achieve this, we require the latent temporal graph embedding F(t) ∈ R N ×P (P < N , F(t) T F(t) = I P×P , ) for each temporal network W(t) should yield a distinct separation between connector hub nodes and the peripheral nodes. To link the learning of F(t) with the optimization of hub selection indicator s(t), we adopt the following objective function: arg min where is the diagonal matrix. The trace norm, , measures the smoothness of graph embedding in the context of the network topology governed by L s  Physics-Based Network Evolution Model. Since a temporal network evolves from the previous temporal state, the physics network-to-network evolution model can be mathematically described as follows: where τ is the time interval and ∂F(t) ∂t denotes the network-to-network evolution rate. In the context of neurobiological signals, the evolution of network connectivity is a smooth process rather than a mutational change. This means that as the time interval between two consecutive network states approaches zero, the 2-norm difference between two consecutive networks (F(t) and F(t -τ )) approaches a finite value, i.e.,  which includes two terms: the identification of temporal hub sets S(t) over time using the learned dynamic graph embedding F(t), and the regularization term that enforces the smoothness of F(t) evolution over time. The scalar parameter α controls the trade-off between the two terms."
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,2.2,Optimization on Grassmannian Manifold,"Equation (  Since Eq. (  Optimizing Dynamic Graph Embedding on Grassmannian Manifold. According to  signifies the evolving variation from the temporal state t m-1 to t m , see Fig. "
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,,"(b). In the Grassmannian manifold space, F(t m ) 2 2 can be accurately measured by the squared geodesic distance Log F(t m ) (F(t m-1 )) = P-tr(F(t m )F(t m ) T F(t m-1 )F(t m-1 ) T ).","Therefore, the optimization of Eq. (  ) onto the tangent space via the orthogonal projection  Given F(t m ) , we update the modified F(t m ) using an exponential mapping operation  Optimizing Temporal Hub Node Set. After updatingF(t m ), the energy function of Eq. (  where represents the distance between the temporal graph embedding F(t m ) at the i th and j th nodes. The optimal set of temporal hub nodes s at the t m temporal point can be achieved using the convex optimization scheme proposed in "
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,3.0,Experiments and Results,"We assess the performance of our spatiotemporal hub identification method on both simulated and real network data. Our proposed method, named Dynamic-Graph-Embedding-based method, not only learns the topological features of each temporal network but also jointly learns the evolving consistency pattern on the entire dynamic sequence. We compare our method with conventional approaches: the classic sortingbased hub identification method that uses nodal betweenness centrality "
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,3.1,Accuracy and Robustness on Synthesized Network Data,"Data Preparation. A set of synthesized time-variant networks were generated by the folling steps: (1) initialize a network with a specified number of nodes and connections; (2) set an evolution ratio (updated/total) to simulate the gradual process of network evolution. This involves keeping a fixed proportion of connections in the final state, while updating the remaining connections to generate the evolved network. Figure "
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,3.2,Evaluation of Hub Identification on Real Brain Networks,"Data Preparation. A total of 125 subjects consisting of 63 normal control (NC) and 62 obsessive-compulsive disease (OCD) were selected from an obsessive-compulsive  Consistency of Hub Topology. We employed conventional engineering methods and our proposed spatiotemporal hub identification technique on each sliding window across subjects, as depicted in Fig.  To further quantify the similarity, we calculated the covariance of the count histogram between the last and current temporal states, and our method exhibited the highest similarity (Fig. "
Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold,4.0,Conclusion,This paper introduces a novel spatiotemporal hub identification method. Our approach integrates the evolution model of network connectivity to ensure the consistency of dynamic graph embedding over time. The results on both simulated and real data are promising and suggest the great potential for investigating the role of hubs in the evolution of both task-based and resting-state-based networks.
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,1.0,Introduction,"Liver cancer is one of the most deadly cancers and has the second highest fatality rate  A single-phase lesion annotation means the annotation of both lesion position and its class. In hospitals, collected multi-phase CTs are normally grouped by patients rather than lesions, which makes single-phase lesion annotation insufficient for feature fusion learning. However, the number of lesions inside a single patient can vary from one to dozens and they can be of different types in realistic cases. Multi-phase CTs are also not co-registered in most cases, therefore, it is necessary to make sure the lesions extracted from different phases are somehow aligned for feature fusion, which is called as multi-phase lesion annotation. Moreover, while most works have attached much importance to liver lesion segmentation  Self-attention based transformers  In this paper, we construct a hybrid framework with ViT backbone for liver lesion classification, TransLiver. We design a pre-processing unit to reduce the annotation cost, where we obtain lesion area on multi-phase CTs from annotations marked on a single phase. To alleviate the limitations of pure transformers, we propose a multi-stage pyramid structure and add convolutional layers to the original transformer encoder. We use additional cross phase tokens at the last stage to complete a multi-phase fusion, which can focus on cross-phase communication and improve the fusion effectiveness as compared with conventional modes. While most multi-phase liver lesion classification studies use datasets with no more than three phases (without DL phase for its difficulty of collection) or no more than six lesion classes, we validate the whole framework on an in-house dataset with four phases of abdominal CT and seven classes of liver lesions. Considering the disproportion of axial lesion slice number and the relatively small scale of the dataset, we adopt a 2-D network in classification part instead of 3-D in pre-processing part and achieve a 90.9% accuracy."
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,2.0,Method,"Figure  For each phase, a convolutional encoder extracts preliminary lesion features on axial slices. As the backbone of the whole framework, transformer encoder employs a 4-stage pyramid structure extracting multi-scale features, with each stage connected by a convolutional down-sampler. There are two types of transformer blocks, single-phase liver transformer block (SPLTB) and multi-phase liver transformer block (MPLTB). The former is phase-specific, while the latter is in charge of multi-phase fusion. Extracted features from different phases are averaged and classified by two successive fully connected networks. A voting strategy about slices is applied to decide the classification results. "
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,2.1,Pre-processing Unit,"The single-phase annotated lesion has the position and class labels in all phases but they are not aligned, so we could have difficulty finding out which lesions in different phases are the same with 2 or more lesions in one patient. To reduce errors caused by unregistered data and address the situation that one patient has multiple lesions of different types, we pre-process the multi-phase liver CTs registered and grouped by lesions. The registration network is based on Voxelmorph  After registration, a lesion matcher finds the same lesions in different phases. We generate a minimum circumscribed cuboid with padding as the lesion window for each lesion to keep the surrounding information. The windows are then converted to 0-1 masks to calculate Dice coefficient. Lesions with the maximal window Dice coefficient that is no less than a set threshold are considered the same. Only lesions completely found in all phases will be used in the following classification network."
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,2.2,Convolutional Encoder and Convolutional Down-Sampler,"In pure vision transformer, input images are converted to tokens by patch embedding and added with positional encoding to keep the positional information. Patch embedding consists of a linear connected layer or a convolutional layer, which does not enable to construct local relation  So, we construct a convolutional encoder without absolute positional encoding to replace the original embedding layer. For an input image X ∈ R B×H×W ×1 , B is the batch size, and H × W is the size of the input. The module contains four convolutional layers playing different roles. The first layer, Conv 1 , with a kernel size of 3, stride of 2, and output channels of 32, reduces the size to H 2 × W 2 . Next two layers, Conv 2 and Conv 3 , each with a kernel size of 3, stride of 1, and the same output channel as Conv 1 , extract local information. Conv 1 , Conv 2 , and Conv 3 each is followed by a GeLU activation layer and a batch normalization. Considering the design of PVTv2  to finish the tokenization of transformer. We add convolutional down-samplers between stages of transformer encoder so that they can produce hierarchical representation like CNN structure. Each convolutional down-sampler contains a residual structure with a 3×3 depthwise convolution to increase the locality of our model. We also utilize a convolutional layer with a kernel size of 2 and stride of 2, which halves the image resolution and doubles the number of channels."
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,2.3,Single-Phase Liver Transformer Block,"Vision Transformers can get excellent performance on large-scale datasets such as ImageNet  where Q, K, V are the same with original ViT, d h is the head dimension, and P is the relative positional encoding. Spatial reduction SR consists of a k × k depthwise convolution with a stride of k and a batch normalization, where k is the spatial reduction ratio set in each stage. Feed forward network (FFN) is designed for a better capacity of representation. We use the module designed in "
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,2.4,Multi-phase Liver Transformer Block,"Single-phase liver transformer block (SPLTB) is phase-specific, which means the model parameters of each phase are independent. It can fully extract features in different phases before fusion. Inspired by  where X l i is phase tokens of the ith phase and the lth layer and t l is cross phase tokens of the lth layer. Because of the phase-specific SPLTB, t l i represents the corresponding cross phase tokens output of the ith phase in the lth layer. Cross phase tokens need negligible extra cost and can force the information to concentrate inside the tokens "
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,3.1,Liver Lesion Classification,"Dataset. The employed single-phase annotated dataset is collected from Sir Run Run Shaw Hospital (SRRSH), affiliated with the Zhejiang University School of Medicine, and has received the ethics approval of IRB. The collection process can be found in supplementary materials. The size of each CT slice is decreased to 224×224 using cubic interpolation. After the pre-processing unit with window Dice threshold of 0.3, we screen 761 lesions from 444 patients with four phases of CTs, seven types of lesions (13.2% of HCC, 5.3% of HM, 11.3% of ICC, 22.6% of HH, 31.1% of HC, 8.7% of FNH, and 7.8% of HA), and totally 4820 slices. To handle the imbalance of dataset, we randomly select 586 lesions as the training and validation set with no more than 700 axial slices in each lesion type, and the rest 175 lesions constitute the test set. Lesions from the same patient are either assigned to the training and validation set or the test set, but not both. Implementations. The training and validation set is randomly divided with a 4:1 ratio. The data is augmented by flip, rotation, crop, shift, and scale. We initialize the backbone network using pre-trained weights of CMT-S  Results. We first compare the class-wise accuracy of our model against other advanced methods applying different architectures in multi-phase liver lesion classification with more than four lesion types  Because the sources of data are different among the methods compared above and to the best of our knowledge, no relevant study based on transformers was found, we further train some SOTA normal classification models on our dataset. Considering the fairness, all the models below are initialized with pre-trained weights and adopt 2-D structures using the same slice-level classification strategy. For completeness, we concatenate the multi-phase features to execute the fusion. As illustrated in Table "
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,3.2,Ablation Study,"To verify the improvement of our modules, we conduct three baseline experiments for comparison. Here convolutional encoder, convolutional down-sampler, and SPLTB as a whole is called c-SPLTB. Baseline 0 does not use c-SPLTB or cross phase tokens in MPLTB but replaces them with pure vision transformer and output feature concatenation respectively. Baseline 1 adds the c-SPLTB and Baseline 2 adds the cross phase tokens. A 3-D version of Baseline 2 utilizing 3-D patch embedding is also studied in Baseline 3 to validate the advantage of our 2-D model. The result shown in Fig.  We also evaluate the model performance under different phase combinations by cutting the branch of certain phases. It shows that information from various phases can significantly influence the classification performance. A missing phase can cause an accuracy drop of about 10% and complete four-phase model outperforms single-phase model by nearly 20%. Figure "
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,4.0,Conclusion,"We have presented a hybrid architecture for multi-phase liver lesion classification in this paper. The lesion features are extracted by transformer backbone with several auxiliary convolutional modules. Then, we fuse the features from different phases through cross phase tokens to enhance their information exchange. To handle the issues in realistic cases, we design a pre-processing unit to acquire multi-phase annotated lesions from single-phase annotated ones. We report performance of an overall 90.9% classification accuracy on a four-phase seven-class dataset through quantitative experiments and show obvious improvement compared with SOTA classification methods. In future work, we will extend classification to instance segmentation and provide an end-to-end effective model for liver lesion diagnosis."
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 31.
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,1.0,Introduction,"Semantic segmentation on histological whole slide images (WSIs) allows precise detection of tumor boundaries, thereby facilitating the assessment of metastases  We use region-based active learning (AL)  This work focuses on region selection methods, a topic that has been largely neglected in literature until now, but which we show to have a great impact on AL sampling efficiency (i.e., the annotated area required to reach the full annotation performance). We discover that the sampling efficiency of the aforementioned standard method decreases as the AL step size (i.e., the annotated area at each AL cycle, determined by the multiplication of the region size and the number of selected regions per WSI) increases. To avoid extensive AL step size tuning, we propose an adaptive region selection method with reduced reliance on this AL hyperparameter. Specifically, our method dynamically determines an annotation region by first identifying an informative area with connected component detection and then detecting its bounding box. We test our method using a breast cancer metastases segmentation task on the public CAMELYON16 dataset and demonstrate that determining the selected regions individually provides greater flexibility and efficiency than selecting regions with a uniform predefined shape and size, given the variability in histological tissue structures. Results show that our method consistently outperforms the standard method by providing a higher sampling efficiency, while also being more robust to AL step size choices. Additionally, our method is especially beneficial for settings where a large AL step size is desirable due to annotator availability or computational restrictions."
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,2.1,Region-Based Active Learning for WSI Annotation,"We are given an unlabeled pool U = {X 1 . . . X n }, where X i ∈ R Wi×Hi denotes the i th WSI with width W i and height H i . Initially, X i has no annotation; regions are iteratively selected from it and annotated across AL cycles. We denote the j th annotated rectangular region in , where (c ij x , c ij y ) are the center coordinates of the region and w ij , h ij are the width and height of that region, respectively. In the standard region selection method, where fixedsize square regions are selected, w ij = h ij = l, ∀i, j, where l is predefined. Figure  The selection of k new regions from X i is performed in two steps based on the model prediction P i = g(X i ). First, P i is converted to a priority map M i using a per-pixel informativeness measure. Second, k regions are selected based on M i using a region selection method. The informativeness measure is not the focus of this study, we therefore adopt the most commonly used one that quantifies model uncertainty (details in Sect. 3.2). Next we describe the four region selection methods evaluated in this work."
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,2.2,Region Selection Methods,"Random. This is the baseline method where k regions of size l × l are randomly selected. Each region contains at least 10% of tissue and does not overlap with other regions. Standard  . This size range is chosen to be comparable to the other three methods, which select regions of size l 2 . Note that Standard (non-square) can be understood as an ablation study of the proposed method Adaptive to examine the effect of variable region shape by maintaining constant region size."
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,2.3,WSI Semantic Segmentation Framework,"This section describes the breast cancer metastases segmentation task we use for evaluating the AL region selection methods. The task is performed with patch-wise classification, where the WSI is partitioned into patches, each patch is classified as to whether it contains metastases, and the results are assembled. Training. The patch classification model h(x, w) : R d×d -→ [0, 1] takes as input a patch x and outputs the probability p(y = 1|x, w) of containing metastases, where w denotes model parameters. Patches are extracted from the annotated regions at 40× magnification (0.25 µm px ) with d = 256 pixels. Following "
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,3.1,Dataset,We used the publicly available CAMELYON16 Challenge dataset 
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,3.2,Implementation Details,"Training Schedules. We use MobileNet v2  Active Learning Setups. Since the CAMELYON16 dataset is fully annotated, we perform AL by assuming all WSIs are unannotated and revealing the annotation of a region only after it is selected during the AL procedure. We divide the WSIs in U randomly into five stratified subsets of equal size and use them sequentially. In particular, regions are selected from WSIs in the first subset at the first AL cycle, from WSIs in the second subset at the second AL cycle, and so on. This is done because WSI inference is computationally expensive due to the large patch amount, reducing the number of predicted WSIs to one fifth helps to speed up AL cycles. We use an informativeness measure that prioritizes pixels with a predicted probability close to 0.5 (i.e., M i = 1-2|P i -0.5|), following  Evaluations. We use the CAMELYON16 challenge metric Free Response Operating Characteristic (FROC) score  To evaluate the WSI segmentation performance directly, we use mean intersection over union (mIoU). For comparison, we follow "
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,3.3,Results,"Full Annotation Performance. To validate our segmentation framework, we first train on the fully-annotated data (average performance of five repetitions reported). With a patch extraction stride s = 256 pixels, our framework yields an FROC score of 0.760 that is equivalent to the Challenge top 2, and an mIoU (Tumor) of 0.749, which is higher than the most comparable method in "
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,4.0,Discussion and Conclusion,"We presented a new AL region selection method to select annotation regions on WSIs. In contrast to the standard method that selects regions with predetermined shape and size, our method takes into account the intrinsic variability of histological tissue and dynamically determines the shape and size for each selected region. Experiments showed that it outperforms the standard method in terms of both sampling efficiency and the robustness to AL hyperparameters. Although the uncertainty map was used to demonstrate the efficacy of our approach, it can be seamlessly applied to any priority maps. A limitation of this study is that the annotation cost is estimated only based on the annotated area, while annotation effort may vary when annotating regions of equal size. Future work will involve the development of a WSI dataset with comprehensive documentation of annotation time to evaluate the proposed method and an investigation of potential combination with self-supervised learning."
Aneurysm Pose Estimation with Deep Learning,1.0,Introduction,"Intracranial aneurysms are abnormal focal dilations of cerebral blood vessels. Their rupturing accounts for 85% of Subarachnoid Hemorrhages (SAH), and is associated with high morbidity and mortality rates  Automated methods for detecting UIAs range from traditional Computer-Aided Detection (CAD) systems using image filtering techniques  Estimating the pose of organs has been investigated in the literature as a slice positioning problem. A set of slices must be optimally selected relative to the pose of the knee  In this paper, we introduce a novel one-stage method to simultaneously localize, and estimate the size and the orientation of aneurysms from 3D TOF-MRA images. A fast and approximate annotation is used. To address the class imbalance problem, a small patch approach is combined with dedicated data sampling and generation strategies. We follow a landmark approach to estimate the aneurysm pose, while avoiding rotation discontinuity problems associated with Euler angles and quaternions "
Aneurysm Pose Estimation with Deep Learning,2.1,Datasets and Data Annotation,"In this work, two TOF-MRA aneurysm datasets were used. The first dataset includes 132 exams (75 female, 57 male) collected at our medical institution between 2015 and 2021 according to the following inclusion criteria: diagnosed unruptured saccular aneurysms smaller than 20 mm, no pre-treated aneurysm or fusiform aneurysm. A single exam was included per patient (i.e. no follow-up exams). All images were acquired using a 3T scanner (GE Discovery MR750w). Acquisition parameters included TR = 28 ms, TE = 3.4 ms, slice thickness= 0.8 mm, and 4 slabs (54 slices/slab), resulting in 512 × 512 × 254 volumes with a 0.47×0.47×0.4mm 3 voxel size. Each DICOM data was anonymized on the clinical site before processing. As per the charter of our university hospital, the anonymous use of imaging data acquired in clinical practice is authorized for research purposes, in accordance with the principle of non-opposition of the patient. Each image contained from one (84/132) to five aneurysms (4 subjects), totaling 206 aneurysms with a mean diameter of 3.97 ± 2.32 mm (range: 0.96-19.63 mm). Most aneurysms were small, with 81 below 3 mm and 77 between 3-5 mm. The second dataset is the public aneurysm dataset  Previous works on aneurysm detection and segmentation relied on voxel-wise labeling, which is time-consuming and susceptible to intra-and inter-rater variability. To address these limitations, weak annotations using spheres have been recently investigated "
Aneurysm Pose Estimation with Deep Learning,2.2,Data Sampling and Generation,"Accurate modeling of aneurysm and background properties is crucial for pose estimation tasks. We use small 96 × 96 × 96 voxel patches with an isotropic voxel size of 0.4 mm, resulting in 38.4 mm side length patches. This approach is computationally efficient compared to larger patch methods, such as nnDetection "
Aneurysm Pose Estimation with Deep Learning,2.3,Neural Network Architecture,"Inspired by YOLO  To encode the input patch into feature maps, we use residual convolutional blocks and down-sampling operations. The Localization and Orientation Head splits the encoded feature maps into a grid of 1728 cells using two consecutive convolutional blocks followed by three parallel convolutions. The first convolution generates a confidence probability score indicating whether the cell contains an aneurysm center. For positive cells (i.e. containing an aneurysm center), the second convolution, followed by sigmoid function, predicts the aneurysm center coordinates C = (C x , C y , C z ) relative to the cell size, while the third convolution estimates the aneurysm size and its orientation by calculating the axis vector "
Aneurysm Pose Estimation with Deep Learning,2.4,Loss Function,"Due to the grid-based nature of our architecture, there is a high imbalance between the number of negative cells and a very small number of positive cells. Inspired by  To optimize the detection confidence, we used the binary cross-entropy (BCE) loss function for both positive (BCE P ) and negative (BCE N ) cells. To prioritize identifying aneurysms over background, we weighted the negative cell term by half the number of positive cells (#P ) in the batch (Eq. 1). Aneurysm localization and dimensions are assessed using mean squared error (MSE) (Eq. 2). Orientation estimation is enforced through the cosine similarity of v (Eq. 3). These last two terms are only computed on positive cells with a weight of 5 to account for the limited number of such cells."
Aneurysm Pose Estimation with Deep Learning,2.5,Implementation Details,"We implemented our method using PyTorch framework (1.10.0). The model has approximately 28 million parameters, that were optimized using the stochastic gradient descent algorithm. The hyper-parameters were determined using a subset of the in-house dataset: 200 epochs, balanced batch sampling technique between negative and positive patches, batch size of 32, and initial learning rate of 10 -2 . Each input volume was normalized using z-score normalization. Training and inference were performed on an NVIDIA RTX A6000 GPU with 48 GB of memory. During inference, a patch reconstruction technique is used to predict the location and orientation of aneurysms in the entire volume. The original volume is split into patches with an isotropic voxel resolution of 0.4 mm. To mitigate border effects caused by convolutions, a 16 voxel overlap is considered between adjacent patches. Predictions are made for each patch and converted back to the original volume resolution: a pose is kept only if the predicted center is inside the central 64 × 64 × 64 part of the patch. Non-Maximum Suppression (NMS) is used to eliminate overlapping predictions, considered as spheres (see Sect. 2.3)."
Aneurysm Pose Estimation with Deep Learning,2.6,Evaluation Metrics,"For the pose estimation task, our method was evaluated based on two standard metrics. First, the Euclidean distance (in mm) was measured between the predicted aneurysm center (C) and its corresponding ground truth (GT). The second metric computes the angular difference (in degrees) between the predicted aneurysm orientation vector ( v) and its corresponding GT. For the detection task, our evaluation was based on the Intersection-over-Union (IoU) between the predicted and GT spheres at a threshold of 10% "
Aneurysm Pose Estimation with Deep Learning,3.1,Pose Estimation,"We conducted 5-fold cross-validation separately on two large datasets (see Sect. 2.1) to evaluate the performance of our method for aneurysm pose estimation. Each dataset was randomly split into five subsets, with 25 or 26 patients per subset for the in-house dataset and 54 patients per subset for dataset  The results on both datasets are shown in Table "
Aneurysm Pose Estimation with Deep Learning,3.2,Object Detection,"We also evaluated the effectiveness of our method on the classical detection task by comparing it with two public and fully-automated state-of-the-art baselines, nnDetection "
Aneurysm Pose Estimation with Deep Learning,,Methods,AP0.1 (%) Sensitivity0.5 (%) FPs/case0. and Faster RCNN  As shown in Table 
Aneurysm Pose Estimation with Deep Learning,4.0,Conclusion,"In this paper, we proposed a novel one-stage deep learning approach for aneurysm pose estimation from TOF-MRA images, which can also be used for the classical detection task. It was evaluated using two large datasets, including a public one  In the pose estimation task, our method achieved good and similar performance on both datasets, accurately estimating the pose of aneurysms with diverse shapes and sizes. Rare errors in orientation were primarily due to small aneurysms and sometimes complex aneurysm shapes, leading to weak and uncertain GT annotations. Specifically, on the public dataset  In the aneurysm detection task, our proposed method exhibited promising performance compared to two state-of-the-art baselines, nnDetection  Our method represents a promising step towards automated aneurysm pose estimation and detection, offering several advantages over existing approaches. It demonstrated multi-task learning capabilities by simultaneously localizing, and estimating the size and the orientation of aneurysms in a single forward pass. Preliminary qualitative tests are hopeful indicators for its clinical utility."
Localized Questions in Medical Visual Question Answering,1.0,Introduction,"Visual Question Answering (VQA) models are neural networks that answer natural language questions about an image  Recent work on medical VQA has primarily focused on building more effective model architectures  To this day, few works have addressed the ability to include location information in VQA models. In  To overcome these limitations, we propose a novel VQA architecture that alleviates the mentioned issues. At its core, we hypothesize that by allowing the VQA model to access the entire images and properly encoding the region of interest, this model can be more effective at answering questions about regions. To achieve this, we propose using a multi-glimpse attention mechanism "
Localized Questions in Medical Visual Question Answering,2.0,Method,"Our method extends a VQA model to answer localized questions. We define a localized question for an image x as a tuple (q, m), where q is a question, and m is a binary mask of the same size as x that identifies the region to which the question pertains. Our VQA model p θ , depicted in Fig.  ( The model proceeds in three stages to produce its prediction: input embedding, localized attention, and final classification. Input Embedding. The question q is first processed by an LSTM  Localized Attention. An attention mechanism uses the embedding to determine relevant parts of the image to answer the corresponding question. Unlike previous attention methods, we include the region information that the mask defines. Our localized attention module (Fig.  where the index :hw indicates the feature vector at location (h, w), W (x) ∈ R C ×C , W (q) ∈ R C ×Q , and W (g) ∈ R G×C are learnable parameters of linear transformations, and is the element-wise product. In practice, the transformations W (x) and W (g) are implemented with 1 × 1 convolutions and all linear transformations include a dropout layer applied to its input. The image feature maps x are then weighted with the attention map and masked with m as, where c and g are the indexes over feature channels and glimpses, respectively, (h, w) is the index over the spatial dimensions, and m ↓ H×W denotes a binary downsampled version of m with the spatial size of x. This design allows the localized attention module to compute the attention maps using the full information available in the image, thereby incorporating context into them before being masked to constrain the answer to the specified region. Classification. The question descriptor q and the weighted feature maps x from the localized attention are vectorized and concatenated into a single vector of size C • G + Q and then processed by a multi-layer perceptron classifier to produce the final probabilities. Training. The training procedure minimizes the standard cross-entropy loss over the training set updating the parameters of the LSTM encoder, localized attention module, and the final classifier. The training set consists of triplets of images, localized questions, and the corresponding ground-truth answers. As in "
Localized Questions in Medical Visual Question Answering,3.0,Experiments and Results,We compare our model to several baselines across three datasets and report quantitative and qualitative results. Additional results are available in the supplementary material.
Localized Questions in Medical Visual Question Answering,3.1,Datasets,We evaluate our method on three datasets containing questions about regions which we detail here. The first dataset consists of an existing retinal fundus VQA dataset with questions about the image's regions and the entire image. The second and third datasets are generated from public segmentation datasets but use the method described in 
Localized Questions in Medical Visual Question Answering,,DME-VQA,"We automatically generated binary questions with the structure ""is there [instrument] in this region?"" and corresponding masks as rectangular regions with random locations and sizes. Based on the ground-truth label maps, the binary answers were labeled ""yes"" if the region contained at least one pixel of the corresponding instrument and ""no"" otherwise. The questions were balanced to maintain the same amount of ""yes"" and ""no"" answers. Figure "
Localized Questions in Medical Visual Question Answering,3.2,Baselines and Metrics,"We compare our method to four different baselines, as shown in Fig.  No mask: no information is provided about the region in the question. Region in text  Crop region  We evaluated the performance of our method using accuracy for the DME-VQA dataset and the area under the receiver operating characteristic (ROC) curve and Average Precision (AP) for the RIS-VQA and INSEGCAT-VQA datasets. Implementation Details: Our VQA architecture uses an LSTM  We use the ResNet-152 "
Localized Questions in Medical Visual Question Answering,3.3,Results,"Our method outperformed all considered baselines on the DME-VQA (Table  When analyzing mistakes made by our model, we observe that they tend to occur when objects or background structures in the image look similar to the object mentioned in the question (Fig. "
Localized Questions in Medical Visual Question Answering,4.0,Conclusions,"In this paper, we proposed a novel VQA architecture to answer questions about regions. We compare the performance of our approach against several baselines and across three different datasets. By focusing the model's attention on the region after considering the evidence in the full image, we show how our method brings improvements, especially when the complete image context is required to answer the questions. Future works include studying the agreement between answers to questions about concentric regions, as well as the agreement between questions about images and regions."
Localized Questions in Medical Visual Question Answering,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 34.
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,1.0,Introduction,"Carotid intima-media (CIM) segmentation has been widely applied in clinical practice, providing a diagnostic basis for atherosclerotic disease (one of the complications of obesity). To identify the contour of the intima-media, i.e., the structure between the lumen-intima (LI) and the media-adventitia (MA), one of the available solutions is deep learning-based medical image segmentation for CIM. Currently, this CIM segmentation approach faces the challenges of lack of largequantity images, high-quality labels from ultrasound experts, and a mixture of clear and ambiguous CIM areas in carotid ultrasound images. Semi-supervised learning recently applies novel frameworks to a general segmentation task  In this work, we propose pseudo-label divergence-based active learning (PLD-AL) to obtain accurate CIM segmentation contributing to the clinical diagnosis of obesity and atherosclerotic disease. As shown in Fig.  Our contributions are as follows: we propose PLD-AL, which aims to train segmentation models using a gradually enlarged and refined labeled pool. First, we automatically select and annotate large divergence data between the current and previous AI models, facilitating fast convergence of the AL model to most sound data in the unlabeled pool. Second, we propose a strategy to refine the labels in the labeled pool alternatingly with the proposed label-divergence-based AL algorithm, which improves the robustness compared to the conventional AL approach. We conducted experiments to demonstrate that our method yielded competitive performance gains over other AL methods. Finally, we applied the trained model to a real-world in-house hospital dataset with noisy labels and obtain accurate CIM segmentation results. We release our code at https:// github.com/CrystalWei626/PLD AL."
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,2.0,Method,"Section 2.1 establishes mathematical formulation on the main task of CIM segmentation in our AL framework. Our proposed AL approach has two loops: in Sect. 2.2, the outer loop implements progressive annotation on the automatically selected unlabeled pool; in Sect. 2.3, the inner loop trains the neural networks on the labeled pool and subsequently refines it through a feedback routine."
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,2.1,Mathematical Notations and Formulation,"Denote x ∈ R I×J a carotid ultrasound image and y ∈ R I×J the corresponding CIM mask. Let D L = X L ×Y L and X U be the initial labeled and unlabeled pools, where X L is the carotid ultrasound image set, and Y L is the corresponding label set. We aim to improve generalization ability of the AI model by selecting the most informative data in X U and delivering them to an expert for annotation. We propose a novel AL framework: PLD-AL for CIM segmentation, as illustrated in Fig. "
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,,Algorithm 1: PLD-AL,"1 Input: Initial labeled pool DL = XL × YL; Unlabeled pool XU ; Judgment threshold τ ; Refining threshold λ; 2 Initialize θS and θT ; Fit the mIoU curve In each AL iteration, we use a mean-teacher architecture as the backbone of AL. The student and the teacher networks, respectively parameterized by θ S and θ T , share the same neural network architecture F , which maps the carotid ultrasound image x ∈ R I×J to the extended three-dimensional CIM mask probability p ∈ R I×J×2 , whose 3rd-dimensional component p ij ∈ R 2 denotes the softmax probability output for binary classification at the pixel (i, j). We use the divergence between pseudo-labels generated by student and teacher networks to assist in selecting data for the expert to annotate."
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,2.2,Outer Loop: Divergence Based AL,"The outer loop is an AL cycle that selects data for the expert to annotate according to the divergence between the predictions of the student and teacher networks. First, we initialize θ S and θ T . We complete the inner loop proposed in Sect. 2.3, and obtain the trained parameters for the student and teacher networks. Then, we select n data from X U for the expert to annotate. We suggest using the Kullback-Leibler (KL) divergence to assist in selecting data, as shown in Eq. (  We consider data prediction uncertainty as a decisive metric for data selection. It is deduced that the KL divergence between the output of the primary and the auxiliary models in a dual-decoder architecture can approximate the prediction uncertainty "
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,2.3,Inner Loop: Network Optimization and Label Refinement,"The inner loop trains the neural networks by the labeled pool and refines noisy labels through a feedback routine. In the k th epoch of the inner loop, we first use the last labeled pool D L to optimize the training parameter θ (k) S by minibatch stochastic gradient descent. The loss function consists of a supervised loss L sup between labels and predictions of the student model, and a consistency loss L con between the predictions of the student and the teacher models. These can be implemented using the cross-entropy loss and the mean squared error loss, respectively. Then, we update θ (k) T by exponential moving average (EMA) with a decay rate α as Eq. (2): We refine noisy labels based on the idea that the fitness soars sharply at first but slows down after the model begins to fit noise  where a, b, and c are the fitting parameters to be determined by least squared estimate. Then we calculate the ratio of change of the model fitness γ k via the derivative of the mIoU curve M (k): When training stops at this epoch k satisfying γ k < τ (τ is a judgment threshold), we lastly predict the CIM mask probability p ij = F (x(i, j)|θ T ) via the teacher network for each pixel at (i, j) and update the noisy label y(i, j) in Y L if max p ij > λ (λ is a refining threshold)."
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,3.1,Experiment Settings,Implementation Details. We used the PyTorch platform (version 1.13.1) to implement our method. And we adapted the same UNet++  Dataset. We employed the publicly available Carotid Ultrasound Boundary Study (CUBS) dataset Table 
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,3.2,Performance Comparison,"We evaluated the performance of AL methods on the CIM segmentation task using the CUBS dataset. Baselines. We compared our method to other AL methods, including AL methods with query strategy based on random selection (Random), entropy increase (Entropy)  Table "
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,3.3,Ablation Study,We conducted ablation study on the CUBS dataset to demonstrate the importance of the label refinement module proposed in Sect. 2.3. We canceled the label refinement module and substituted the label refinement module with confidence learning (CL) for noise label correction  Table 
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,3.4,Application on In-house Dataset,We applied the teacher network trained in Sect. 3.2 to the in-house dataset acquired at a pediatric hospital. Figure 
PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images,4.0,Conclusion,"We propose a novel AL framework PLD-AL, by training segmentation models using a gradually enlarged and refined labeled pool to obtain accurate and efficient CIM segmentation. Compared with other AL methods, it achieves competitive performance gains. Furthermore, we applied the trained model to an in-house hospital dataset and obtained accurate CIM segmentation results. In the future, we will extend our approach to subsequently calculate CIM thickness and roughness for clinical evaluation of obesity or atherosclerotic disease. We will also investigate the robustness of the proposed method in terms of inter-expert variations and noisy annotation labels. Our approach merely involves one expert in the loop, which may potentially be sensitive to the expert's experience. Multiple experts may consider minimizing inter-reader differences during human-AI interactive labeling "
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,1.0,Introduction,"In the past decade, numerous deep learning-based methods for DR staging have been explored and achieved promising results  Federated learning (FL) is a collaborative learning framework that enables training a model without sharing data between institutions, thereby ensuring data privacy  To address the issues, we propose a novel FL paradigm, named FedUAA, that employs a personalized structure to handle collaborative DR staging among multiple institutions with varying DR staging criteria. We utilize uncertainty to evaluate the reliability of each client's contribution. While uncertainty is a proposed measure to evaluate the reliability of model predictions "
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,2.0,Methodology,"Figure  where £ is the total loss for optimizing the model, f i is the model of i -th client, while X i and Y i are the input and label of i -th client. Different from previous personalized FL paradigms "
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,2.1,Temperature-Warmed Evidential Uncertainty Head,"To make the model more reliable without sacrificing DR staging performance, we propose a novel temperature-warmed evidence uncertainty head (TWEU), which can directly generate DR staging results with uncertainty score based on the features from the encoder. The framework of TWEU is illustrated in Fig.  , where b i ≥ 0 is the probability of i -th category, while u represent the overall uncertainty score. Specifically, as shown in Fig.  is the Dirichlet intensities. Therefore, the probability assigned to category k is proportional to the observed evidence for category k. Conversely, if less total evidence is obtained, the greater the uncertainty score will be. As shown in Fig. "
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,2.2,Uncertainty-Aware Weighting Module,"Most existing FL paradigms aggregate model parameters by assigning a fixed weight to each client, resulting in limited performance on those clients with large heterogeneity in their data distributions. To address this issue, as shown in Fig.  where P i and Y i are the final prediction result and ground truth of i -th sample in local dataset. Based on U and U GT , we can find the optimal uncertainty score θ, which can well reflect the reliability of the local client. To this end, we calculate the ROC curve between U and U GT , and obtain all possible sensitivity (Sens) and specificity (Spes) values corresponding to each uncertainty score (u) used as a threshold. Then, Youden index (J)  More details about Youden index are given in Supplementary B. Finally, the optimal uncertainty scores Θ = [θ 1 , ..., θ N ] of all clients are sent to the server, and a Softmax function is introduced to normalize Θ to obtain the weights for model aggregation as w i = e θi / N i=1 e θi . Therefore, the weights for model aggregation are proportional to the optimal threshold of the client. Generally, local dataset with larger uncertainty distributions will have a higher optimal uncertainty score θ, indicating that it is necessary to improve the feature learning capacity of the client model to further enhance its confidence in the feature representation, and thus higher weights should be assigned during model aggregation."
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,3.0,Loss Function,"As shown in Fig.  where   where Γ (•) is the gamma function, while α = y + (1y) α represents the adjusted parameters of the Dirichlet distribution which aims to avoid penalizing the evidence of the ground-truth class to 0. In summary, the loss function L Uce for the model optimization based on the features that were parameterized by Dirichlet concentration is as follows: where λ is the balance factor for L KL . To prevent the model from focusing too much on KL divergence in the initial stage of training, causing a lack of exploration for the parameter space, we initialize λ as 0 and increase it gradually to 1 with the number of training iterations. And, seen from Sect. 2.1, Dirichlet concentration alters the original feature distribution of F v , which may reduce the model's confidence in the category-related evidence features, thus potentially leading to a decrease in performance. Aiming at this problem, as shown in Fig. "
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,4.0,Experimental Results,"Dataset and Implementation: We construct a database for federated DR staging based on 5 public datasets, including APTOS (3,662 samples) We conduct experiments on the Pytorch with 3090 GPU. The SGD with a learning rate of 0.01 is utilized. The batch size is set to 32, the number of epochs is 100, and the temperature coefficient τ is empirically set to 0.05. To facilitate training, the images are resized to 256 × 256 before feeding to the model. Performance for DR Staging: Table  Reliability Analysis: Providing reliable evaluation for final predictions is crucial for AI models to be deployed in clinical practice. As illustrated in Fig.  Ablation Study: We also conduct ablation experiments to verify the effectiveness of the components in our FedUAA. In this paper, the pre-trained ResNet50  For training model with SingleSet, as shown in Table "
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,5.0,Conclusion,"In this paper, focusing on the challenges in the collaborative DR staging between institutions with different DR staging criteria, we propose a novel FedUAA by combining the FL with evidential uncertainty theory. Compared to other FL methods, our FedUAA can produce reliable and robust DR staging results with uncertainty evaluation, and further enhance the collaborative DR staging performance by dynamically aggregating knowledge from different clients based on their reliability. Comprehensive experimental results show that our FedUAA addresses the challenges in collaborative DR staging across multiple institutions, and achieves a robust and reliable DR staging performance."
Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_21.
Debiasing Medical Visual Question Answering via Counterfactual Training,1.0,Introduction,"Medical visual question answering (Med-VQA) has attracted considerable attention in recent years. It seeks to discover the plausible answer by evaluating the visual information of a medical image and a clinical query regarding the image. The Med-VQA technology can considerably enhance the efficiency of medical professionals and fulfill the growing demand for medical resources  Therefore, we propose a novel unbiased and interpretable Med-VQA model and preliminarily construct a bias-sensitive Med-VQA dataset to address the problems mentioned above. First, with the aim of forcing the model to focus on clinic objects rather than superficial correlations, we prepare the counterfactual samples by masking clinic words with ""[MASK]"" tokens and meanwhile assign the irrelevant answers for implicit bias-weaken. Further, for explicitly reducing the linguist bias, we treat the language bias as the causal effect of the clinic Train What organ system is pictured? What organ system is pictured? Baseline: Chest DeBCF: Neck Answer: Chest Fig.  Additionally, we conduct a bias-sensitive Med-VQA dataset Semantically-Labeled Knowledge-Enhanced-Changing Priors (SLAKE-CP) for evaluating the ability of disentangling the memorized linguist priors and semantic visual information. Qualitative and quantitative experimental results illustrate that our proposed model is superior to the state-of-the-art Med-VQA models on the two public benchmarks and can obtain more obvious improvements on the newly constructed SLAKE-CP."
Debiasing Medical Visual Question Answering via Counterfactual Training,2.0,Methodology,Figure 
Debiasing Medical Visual Question Answering via Counterfactual Training,2.1,Counterfactual Training Data Preparation,"To implicitly weaken the language bias, we follow CSS  where P (a|q, v) represents the probability of predicting answer a through Med-VQA model with image v and question q, ∇ is the gradient operator, S is the cosine similarity, and 1 is the all-ones vector. The top-K clinic words with the highest importance s are defined as critical words. Then, we construct counterfactual samples Q -by replacing the critical words with ""[MASK]"". We also assign the Q -with an answer A -, and the detailed assigning procedure is as follows. We first generate the probability of predicting answer P + (a) with the question Q + which replaces the marginal words with ""[MASK]"" (all but the question type labels and the critical words), and then pick up top-N candidate answers with the highest probability as A + . The rest answers are denoted as"
Debiasing Medical Visual Question Answering via Counterfactual Training,2.2,Counterfactual Cause Effect Training Procedure,"For explicitly subtracting the language priors, following  where M is the mediator between the variables X and Y . Note that the total effect can be composed of the natural direct effect (NDE) and total indirect effect(TIE). Between them, the NDE concentrates the exclusive effect of X = x on Y and prohibit the effect through M : Thus TIE can reflect the reduction of language bias by subtracting the NDE from the TE: Based on the above definition, we translate the Med-VQA task into a causal effect graph as Fig.  Through subtracting NDE of Q = q on A from the TE of V = v, Q = q and K = k on the answer, we can explicitly capture language bias and remove it via TIE, which is defined below. In the inference stage, we choose the answer with the maximum TIE as the prediction. where k * = K(V = v * , Q = q * ), v * and q * is the counterfactual situation where model is without v, q as inputs. The Y q ,v ,k = log σ(Z q + Z v + Z k ), where the  where L V QA , L QA and L V A are corss-entropy losses over Y q ,v ,k , Z q and Z v . The complete objective of our method is optimized to minimize the L DeBCF which combines the L cls over both the original and the counterfactual data: where α is the hyperparameter which control the ratio of counterfactual samples."
Debiasing Medical Visual Question Answering via Counterfactual Training,3.0,SLAKE-CP: Construction and Analysis,"For further evaluating the debiasing ability of Med-VQA, we follow VQA-CP  Grouping. We first construct all image-question-answer samples of train-set and test-set in SLAKE  Re-Splitting. We re-split the SLAKE "
Debiasing Medical Visual Question Answering via Counterfactual Training,4.1,Datasets and Implementation Details,"Datasets. SLAKE  Implementation Details. For implementation, we apply Pytorch library with 6 NVIDIA TITAN 24 GB Xp GPUs. We employ the MEVF "
Debiasing Medical Visual Question Answering via Counterfactual Training,4.2,Experimental Results,Comparison with State-of-the-Art Methods. We compare our DeBCF with 10 state-of-the-art Med-VQA models on the SLAKE  Discussion of SLAKE-CP. Table  Ablation Analysis. Table 
Debiasing Medical Visual Question Answering via Counterfactual Training,,Influence of Hyperparameters.,"The influence results of the top-K and the hyperparameter α are conducted in Table  Quantitative Analysis. As Fig.  Given the same question but different medical images and answers, the proposed model correctly predict the various answers while the MEVF+BAN "
Debiasing Medical Visual Question Answering via Counterfactual Training,5.0,Conclusion,"In this paper, we propose a novel debiasing Med-VQA model that prepares the counterfactual data by masking critical clinic words and combines it into the counterfactual training stage which subtracting the causal effect of language priors directly, aiming to migrate the linguistic-bias in Med-VQA. Additionally, we construct a linguistic-bias sensitive Med-VQA dataset SLAKE-CP by disintegrating the language priors from training. Experimental results demonstrate the superior debiasing and interpretive performance of the proposed model. It's the first attempt to construct a preliminary bias-sensitive Med-VQA dataset, which will be elaborated in our future work. The codes will be released."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,1.0,Introduction,"Federated learning (FL), allowing decentralized data sources to train a unified deep learning model collaboratively without data sharing, has drawn great attention in medical imaging due to its privacy-preserving properties  Several FL frameworks have been proposed to tackle imbalanced data  In this paper, we formulate the effect of class imbalance in FL into the attribute bias and the class bias  The main contributions are summarized as follows. (1) A new viewpoint of realistic medical FL scenarios where global training data is class-imbalanced. (2) A novel privacy-preserving framework FedIIC for balanced federated learning. (3) Superior performance in dealing with class imbalance under both real-world and simulated multi-source decentralized settings."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,2.1,Preliminaries and Overview,"Considering a typical FL scenario for multi-class image classification with K participants, each participant is assumed to own a private dataset where N k is the data amount of D k , and denote each image-label pair as ). The goal of FL is to Assuming each image has two kinds of latent attributes, i.e. Z c and Z a , representing the class-specific attributes (determining the category of the image, e.g. texture, color, etc.) and the variant background attributes (e.g. brightness, contrast, etc.) respectively  where the last two items represent the attribute bias and the class bias respectively, which widely exist in class-imbalanced data and affect the posterior probability. For robust FL with class-imbalanced data, the key idea is to alleviate the two biases simultaneously, instead of focusing on the latter as "
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,2.2,Intra-Client Contrastive Learning,"Limited local data affects data diversity (i.e., limited (Z c , Z a ) combinations), especially for minority classes, making Z c less distinguishable. To emphasize more on the learning of Z c , supervised contrastive learning (SCL), proven to be effective for representation learning  The basic loss function of SCL can be formulated as where I denotes the index set of the multi-view batch generated by different augmentations (e.g. the two views in Fig.  . In the multi-view batch, L SCL keeps the embeddings of the same class closer while pushing the embeddings of different classes further away, which helps the model learn better Z c of each class due to richer Z a . However, SCL can not perfectly address class imbalance as the majority classes would benefit more from Eq. 2 following traditional training losses (e.g. the cross entropy loss). To overcome this problem, we propose to employ a dynamic temperature τ := P τ = (p i p j ) t τ in Eq. 2 inspired by  named intra-client contrastive learning. Through P , sample pairs of the minority classes are up-weighted compared to those of the majority classes, leading to better balance."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,2.3,Inter-client Contrastive Learning,"Given limited local data under FL, the effectiveness of intra-client contrastive learning may be bounded. How to better utilize cross-client data from the global perspective is crucial for further performance improvement. Inspired by learning from prototypes  where y i is the label of sample i. When minimizing L Inter , the embedding of each sample will get closer to the prototype of the same class while farther from the prototypes of different classes, encouraging local models to learn common attributes (i.e. class-specific attributes) for samples with the same classes. To this end, how to produce high-quality prototypes is the key to interclient contrastive learning. In previous studies, one common method to generate prototypes is uploading and aggregating local information. For example, Mu et al.  In this way, the cosine similarity of any ( v i , v j ) pair in V is minimized to be equal, resulting in V with lower inter-class similarity. This operation is called orthogonalization. Finally, the class-wise prototypes V are defined as the element-wise l 2 -normalization of V and are sent to clients for inter-client contrastive learning."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,2.4,Difficulty-Aware Logit Adjustment,"After calibrating the feature extractor g(•), one common method to calibrate the linear classifier f (•) is logit adjustment (LA)  where δ y denotes the positive per-class margin and is inversely proportional to the local class frequency p(y). In this way, during local training, the logits of minority classes will increase to compensate for the item, which in turn trains the model to emphasize more on minority classes. However, the frequency-dependent margin may not be appropriate for medical data. For instance, some disease types/classes may have large intra-class variations and are difficult to diagnose even with a large amount of data, which may result in even smaller per-class margins. To address this, in FedIIC, the per-class margin is calculated based on not only the class frequency but also difficulties inspired by  where k 1 and k 2 are trade-off hyper-parameters. After minimizing L during the local training phase of each client, the global model is updated by FedAvg "
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,3.0,Experiments,"Datasets. Three FL scenarios with class-imbalanced global data are used for evaluation, which are described as follows: 1. Real Multi-Source Dermoscopic Image Datasets (denoted as Real ) consisting of five data sources from three datasets, including PH 2 "
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,3.0,Skin Lesion Classification (denoted as ISIC ).,The training data of ISIC 2019  Data distributions of the three training settings are illustrated in Fig.  Implementation Details. EfficientNet-B0 
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,3.1,Comparison with State-of-the-Art Methods,"Ten related approaches are included for comprehensive comparison, including FedAvg "
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,3.2,Ablation Study,"To validate the effectiveness of each component in FedIIC, a series of ablation studies are conducted on ISIC and ICH following the same experimental details described in Sect. 3. Quantitative results are summarized in Table  Ablation studies of hyper-parameters in FedIIC are conducted on ISIC as stated in Table "
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,4.0,Conclusion,"This paper discusses a more realistic federated learning (FL) setting in medical scenarios where global data is class-imbalanced and presents a novel framework FedIIC. The key idea behind FedIIC is to calibrate both the feature extractor and the classification head to simultaneously eliminate attribute biases and class biases. Specifically, both intra-and inter-client contrastive learning are introduced for balanced feature learning, and difficulty-aware logit adjustment is deployed to balance decision boundaries across classes. Experimental results on both real-world and simulated medical FL scenarios demonstrate FedIIC's superiority against the state-of-the-art FL approaches. We believe that this study is helpful to build real-world FL systems for clinical applications."
FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_65.
Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,1.0,Introduction,"When using a Machine Learning (ML) model during intraoperative tissue characterisation, it is vital that the surgeon is able to assess how reliable a model's prediction is  Despite being speedy, easy to deploy and able to localise semantic features, the above methods lack trustworthiness due to the training strategy of their underlying model. Deep learning (DL) models trained with empirical risk minimisation (ERM) are overconfident in prediction  In this paper, we propose the first approach which incorporates risk estimation into a PA method. A classification model is trained with Dropout and a PA method is used to generate a PA map. At test time, the classification model is employed with the Dropout enabled. In this work, we propose to repeat this process for a number of iterations creating a volume of PA maps. This volume is used to generate a pixel-wise distribution of PA values from which we can infer risk. More specifically, we introduce a method to generate an enhanced PA map by estimating the expectation values of the pixel-wise distributions. In addition, the coefficient of variation (CV) is used to estimate pixel-wise risk of this enhanced PA map. This provides an improved explanation of the model's prediction by clearly presenting to the surgeon which salient areas to trust in the model's enhanced PA map. In this work, we focus on the explainability of the classification of brain tumours using probe-based Confocal Laser Endomicroscopy (pCLE) data. Performance evaluation on pCLE data shows that our improved explainability method outperforms the SOTA. "
Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,2.0,Methodology,"The aim of the proposed method is to produce an improved PA map of a classification model, while providing risk estimation of the model's explainability to enhance trustworthiness in decision making during intraoperative tissue characterisation. In our method, any CNN classification model trained with Dropout can be used. Let Ŷ be the output logits of the CNN model, where Dropout is enabled at test time, with input image X ∈ R height×width×channels . Any PA method can be used to generate a PA map using the output logits S = f s ( Ŷ ) ∈ R height×width where f s (.) is the PA method. We propose to repeat the above process for T iterations to create a volume of PA maps S = {S 1 , ..., S T } ∈ R height×width×T . A visual representation of how the volume is generated is show in Fig.  where, i, j represent the pixel's row and column coordinates, respectively. The expectation E(S i,j ) of each pixel (i, j) is used to generate an enhanced PA map of size height × width. The intuition is that the above distribution of PA values can produce less noisy and risky estimations of a pixel's contribution to the final explainability map compared to a single estimate. As well as advancing SOTA PA methods, our method also estimates the trustworthiness of the enhanced PA map generated above. For risk estimation, it is important to consider that different pixels in the PA map correspond to different semantic features which contribute differently to the output logits. This makes the pixel-wise distributions have different scales. For this reason, the coefficient of variation (CV) is used to estimate pixel-wise risk, as it allows us to compare pixel-wise variances despite their different scales. This is mathematically defined as: Our proposed method improves trustworthiness of explainability as it allows visualisation of both the explainability of the classification model (provided by the enhanced PA map) together with the pixel-wise risk of this map (provided by the CV map). For instance, salient areas on the PA map should not be trusted unless the CV values are low. An example of the enhanced PA and risk maps generated with the proposed method are shown in Fig. "
Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,3.0,Experiments and Analysis,"Dataset. The developed explainability framework has been validated on an in vivo and ex vivo pCLE dataset of meningioma, glioblastoma and metastases of an invasive ductal carcinoma (IDC). All studies on human subjects were performed according to the requirements of the local ethic committee and in agreement with the Declaration of Helsinki (No. CLE-001 Nr: 2014480). The Cellvizio c by Mauna Kea Technologies, Paris, France has been used in combination with the mini laser probe CystoFlex c UHD-R. The distinguishing characteristic of the meningioma is the psammoma body with concentric circles that show various degrees of calcification. Regarding glioblastomas, the pCLE images allow for the visualization of the characteristic hypercellularity, evidence of irregular nuclei with mitotic activities or multinuclear appearance with irregular cell shape. When examining metastases of an IDC, the tumor presents as egg-shaped cells with uniform evenly spaced nuclei. Our dataset includes 38 meningioma videos, 24 glioblastoma and 6 IDC. Each pCLE video represents one tumour type and corresponds to a different patient. The data has been curated to remove noisy images and similar frames. This resulted in a training dataset of 2500 frames per class (7500 frames in total) and a testing dataset of the same size. The dataset is split into a training and testing subset, with the division done on the patient level. Implementation. To implement the DL models we use the open-source framework PyTorch  Evaluation Metrics. Evaluating a PA method is not a trivial task because a PA map may not need to be inline with what a human deems ""reasonable""  where, X = X f s ( Ŷ (X). The above equation measures the effect on the output score of the classification model if we only include the pixels which the PA method scored highly. A minimum average drop is desired. As average drop was found to not be sufficient on its own, the unified method ADCC  Coherency is the Pearson Correlation Coefficient which ensures that the remaining pixels after dropping are still important, defined as: where, Cov(., .) is the covariance and σ is the standard deviation. A higher coherency is better. Complexity is the L1 norm of the output PA map. Complexity is used to measure how cluttered a PA map is. For a good PA map, complexity should be a minimum. As it has been shown in the literature, the metrics in Eqs. ( "
Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,4.0,Conclusion,"In this work we have introduced the first combination of risk in an explainability method. Using our proposed framework we not only improve on all the tested SOTA PA method's ADCC performances but also produce an estimation of risk on the output PA values. The proposed method can clearly present to the surgeon areas of the explainability map that are more trustworthy. From this work we hope to encourage trust between the surgeon and DL models. For future work, we plan to reducing the computation time of our method and deploy the proposed framework for use in surgery."
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,1.0,Introduction,"Lifelong learning  In a lifelong learning scenario, maintaining a robust embedding space and preserving geometrical structure is crucial to mitigate performance degradation and catastrophic forgetting of old tasks  In this paper, we propose to perform distillation in a Reproducing Kernel Hilbert Space (RKHS), constructed from the embedding space of multiple fixed-curvature spacess. This approach is inspired by the ability of kernel methods to yield rich representations in higher-dimensional RKHS  -To the best of our knowledge, this is the first attempt to study mixed-curvature space for the continual medical image classification task. -We propose a novel knowledge distillation strategy to maintain a similar geometric structure for continual learning by minimizing the distance between new embedding and subspace constructed using old embedding in RKHS. -Quantitative analysis shows that our proposed distillation strategy is capable of preserving complex geometrical structure in embedding space resulting in significantly less degradation of the performance of continual learning and superior performance compared to state-of-the-art baseline methods on BloodMNIST, PathMNIST, and OrganaMNIST datasets."
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,2.0,Preliminaries,"Lifelong Learning (L3). L3 consists of a series of T tasks T t ∈ {T . We assume that a fixedsize memory M is available to store a subset of previously seen samples to mitigate catastrophic forgetting in L3. Mixed-Curvature Space. Mixed-Curvature space is formulated as the Cartesian product of fixed-curvature spaces and represented as Here, M i can be a Euclidean (zero curvature), hyperbolic (constant negative curvature), or spherical (constant positive curvature) space. Furthermore, × denotes the Cartesian product, and d i is the dimensionality of fixed-curvature space M i with curvature c i . The distance in the mixed-curvature space can be decomposed as d M (x, y) := C i=1 d Mi (x i , y i ). Hyperbolic Poincaré Ball. Hyperbolic space is a Riemannian manifold with negative curvature. The Poincare ball with curvature -c, c > 0, D n c = {x ∈ R n : c x < 1} is a model of n-dimensional hyperbolic geometry. To perform vector operations on H n , Möbius Gyrovector space is widely used. Möbius addition between x ∈ D n c and y ∈ D n c is defined as follows Using Möbius addition, geodesic distance between two input data points, x and y in D n c is computed using the following formula. Tangent space of data point x ∈ D n c is the inner product space and is defined as which comprises the tangent vector of all directions at x. Mapping hyperbolic embedding to Euclidean space and vice-versa is crucial for performing operations on D n . Consequently, a vector x ∈ T x D n c is embedded onto the Poincaré ball D n c with anchor x using the exponential mapping function and the inverse process is done using the logarithmic mapping function log c v that maps x ∈ D n c to the tangent space of v as follows where λ c v is conformal factor that is defined as In practice, anchor v is set to the origin. Therefore, the exponential mapping is expressed as"
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,3.0,Proposed Method,"In our approach, we emphasize on modeling complex latent structure of medical data by combining embedding representation of zero-curvature Euclidean and negativecurvature hyperbolic space. To attain richer representational power of RKHS "
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,,Definition 1. Positive Definite Kernel,"Popular kernel functions (e.g., the Gaussian RBF) operate on flat-curvature Euclidean spaces. In R n , the Gaussian RBF kernel method is defined as However, using the geodesic distance in a hyperbolic space along with an RBF function similar to Eq. (4) (i.e., replacing z i -z j 2 with the geodesic distance) does not lead to a valid positive definite kernel. Theoretically, a valid RBF kernel is impossible to obtain for hyperbolic space using geodesic distance  to embed hyperbolic data to RKHS via the following valid pd kernel (see  Now, in L3 setting, we have two models h t and h t-1 at our hand at time t. We aim to improve h t while ensuring the past knowledge incorporated in h t-1 is kept within h t . Assume Z t and Z t-1 are the extracted feature vectors for input X using current and old feature extractor, h t feat and h t-1 feat , respectively. Unlike other existing distillation methods, we employ an independent 2-layer MLP for each fixed-curvature space to project extracted features to a new lower-dimensional embedding space on which we perform further operations. This has two benefits, (i) it relaxes the strong constraint directly applied on Z t and Z t-1 and (ii) reduce the computation cost of performing kernel method. Since we are interested in modeling embedding structure in zero-curvature Euclidean and negative-curvature hyperbolic spaces, we have two MLP as projection modules attached to feature extractors, namely g e and g h . Our Idea. Our main idea is that, for a rich and overparameterized representation, the data manifold is low-dimensional. Our algorithm makes use of RKHS, which can be intuitively thought of as a neural network with infinite width. Hence, we assume that the data manifold for the model at time t-1 is well-approximated by a low-dimensional hyperplane (our data manifold assumption). Let  = min In Eq. (  In Eq. (  Here, β is a hyper-parameter that controls the weight of distillation between the Euclidean and hyperbolic spaces. We can employ Eq. ( "
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,3.1,Classifier and Exemplar Selection,"We employ herding based exemplar selection method that selects examples that are closest to the class prototype, following iCARL  Constant-Curvature and Mixed-Curvature Space. Constant-curvature spaces have been successfully used in the literature to realize the intrinsic geometrical orientation of data for various downstream tasks in machine learning. Flat-curvature Euclidean space is suitable to model grid data  L3 Using Regularization with Distillation. Regularization-based approaches impose constraints on updating weights of L3 model to maintain the performance on old tasks. LwF mimics the prediction of the old model into the current model but struggles to maintain consistent performance in the absence of a task identifier. Rebuff et al. in "
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,5.0,Experimental Details,"Datasets. In our experiments, we use four datasets (e.g., BloodMNIST  Evaluation Metrics. We rely on average accuracy and average forgetting to quantitatively examine the performances of lifelong learning methods as used in previous approaches  , where at task t, forgetting on task i is defined as the maximum difference value previously achieved accuracy and current accuracy on task i."
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,6.0,Results and Discussion,"In our comparison, we consider two regularization-based methods (i.e., EWC "
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,7.0,Conclusion,"In this paper, we propose a novel distillation strategy, L3DMC on mixed-curvature space to preserve the complex geometric structure of medical data while training a DNN model on a sequence of tasks. L3DMC aims to optimize the lifelong learning model by minimizing the distance between new embedding and old subspace generated using current and old models respectively on higher dimensional RKHS. Extensive experiments show that L3DMC outperforms state-of-the-art L3 methods on standard medical image datasets for disease classification. In future, we would like to explore the effectiveness of our proposed distillation strategy on long-task and memory-free L3 setting."
L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space,,"Machine Learning -Explainability,","Bias, and Uncertainty I"
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,1.0,Introduction,"Gaze information is a widely used behavioral measure to study attentional focus  Recently, Electroencephalogram (EEG) has been explored as an alternative method to estimate eye movements by recording electrical activity from the brain non-invasively with high temporal resolution  Attention mechanisms have recently emerged as a powerful tool for processing sequential data, including time-series data in various fields such as natural language processing, speech recognition, and computer vision  In this study, we introduce a new deep learning framework for analyzing EEG signals applying attention mechanisms. For the method evaluation, we used the EEGEyeNet dataset and benchmark "
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,2.1,Motivation,"In this study, our primary goal was to build a model sensitive to different electrodes. The motivation for this goal is two-fold. Firstly, with regards to interpreting the model, the electrodes can be considered the smallest entity as they record signals from specific regions of the brain. Therefore, the electrodebased explanation is a reasonable approach considering human understanding. Second, in the context of model learning, incorporating adaptive weighting of electrodes within a neural network can potentially enhance the accuracy and reliability of gaze estimation systems. This is because electrodes are functionally connected to cognitive behaviors. Specifically, in tasks such as gaze estimation, electrodes positioned near the eyes can capture electrical signals from the orbicularis oculi muscles  (1) emphasizing the reliable electrodes and diminishing the influence of suspicious electrodes, while simultaneously (2) providing explanations for each prediction."
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,2.2,Attention-CNN,"Following the idea from the previous section, we propose the Attention-CNN model, where the attention blocks are used as the electrode-sensitive component. As shown in Fig.  Squeeze and Excitation Block: the SE block involves two principle operations. The Squeeze operation compresses features u ∈ R T ×J into electrodewise vectors z ∈ R J by using global average pooling. Here, T denotes the feature size, and J is the number of electrodes. More precisely, the j-th element of z is calculated by The Excitation operation first computes activation s by employing the gating mechanism with sigmoid activation: , where σ refers to the sigmoid function, δ represents the ReLU "
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,,Self Attention Block:,"The self-attention mechanism  , where U denotes the input of self-attention block and φ(•, •) represents linear transformation. Then, Attention Weights are computed using Query and Key: where d k stands for the dimensions of the Key, and √ d k works as a scaling factor. The softmax function was applied to adjust the range of the value in attention weights (M att ) to [0, 1]. Unlike the transformer model, the attention weights are first compressed into a one-dimensional vector by a layer of global average pooling (ψ) and normalized by a sigmoid function. More precisely, we compute Z att = sigmoid(ψ(M att )). Finally, the output of SA Block X is computed by : X = κ(Z att , V ), where κ denotes the electrode-wise production."
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,3.1,Materials and Experimental Settings,"EEGEyeNet Dataset: For our experiments, we utilized the EEGEyeNet dataset "
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,,Implementation Details:,"The experiments are implemented with PyTorch  Evaluation: For Position task, Euclidean distance is applied as the evaluation metric in both pixels and visual angles. Compared to pixel distance, visual angles depend on both object size on the screen and the viewing distance, thus enabling the comparison across varied settings. The performance of Direction Task is measured by the square root of the mean squared error (RMSE) for the angle (in radians) and the amplitude (in pixels) of saccades. In order to avoid the error caused by the repeatedness of angles in the plane (i.e. 2π and 0 rad represents the same direction), atan(sin(α), cos(α)) is applied, just like in angle loss."
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,3.2,Performance of the Attention-CNN,"Table  We can conclude that the CNN model with both attention blocks consistently outperforms the CNN model alone by 5 to 10 percent across all tasks, indicating that electrode-wise attention assists in the learning process of the models.  "
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,3.3,Model Interpretability by Case Studies,"To provide a more detailed analysis of the interpretability of our proposed Attention-CNN model, as well as to further investigate the underlying reasons for the observed accuracy improvement, we conducted a visual analysis of the model performance, with a particular focus on the role of the attention block. Our analysis yielded two key findings, which are as follows: Firstly, the attention blocks were able to detect the electrical difference between the right and left pre-frontal area in case of longer saccades, i.e. rapid eye movements from one side of the screen to the other; see the saccades (d) and (e) in Fig.  Additionally, the attention block effectively learned to circumvent the interference caused by noisy electrodes and redirected attention towards the frontal region. Figure "
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,3.4,Explainability Quantification,"We further examine the validity in explainability of the proposed method by comparing the distribution of learned attention of noisy and non-noisy electrodes in the Direction Task. The attention block's effectiveness is demonstrated by its ability to assign lower weights to these noisy electrodes in contrast to the non-noisy ones. Within all samples in the Direction Task that feature at least one noisy electrode, only 19% of the non-noisy electrodes had normalized attention weights below 0.05. In contrast, 42% of the noisy electrodes exhibited this trait, implying the attention block's ability to reduce weights of abnormal electrodes. We direct readers to the Supplementary materials for a distribution plot showcasing the difference between noisy and non-noisy electrodes, along with additional details. It's important to note that quantifying explainability methods for signal-format data, such as EEG, presents a significant challenge and has limited existing research. Therefore, additional investigations in this field are anticipated in future studies."
An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography,4.0,Conclusion,"In this study, we aimed to address the issue of the lack of interpretability in deep learning models for EEG-based tasks. Our approach was to leverage the fact that EEG signal noise or artifacts are often localized to specific electrodes. We accomplished this by incorporating attention modules as electrode-sensitive components within a neural network architecture. These attention blocks were used to emphasize the importance of specific electrodes, resulting in more accurate predictions and improved interpretability through the use of scaling. Moreover, our proposed approach was less susceptible to noise. We conducted comprehensive experiments to evaluate the performance of our proposed Attention-CNN model. Our results demonstrate that this model can accurately classify EEG and eye-tracking data while also providing insights into the quality of the recorded EEG signals. This contribution is significant as it can lead to the development of new decoding techniques that are less sensitive to noise. In summary, our study underscores the importance of incorporating attention mechanisms into deep learning models for analyzing EEG and eye-tracking data. This approach opens up new avenues for future research in this area and has the potential to provide valuable insights into the neural basis of cognitive processes."
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,1.0,Introduction,Deep learning methods for automatic lung lesion segmentation from CT volumes have the potential to alleviate the burden on clinicians in assessing lung damage and disease progression in COVID-19 patients 
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,,Related Work:,"Interactive segmentation methods for Artificial Intelligence (AI) assisted annotation have shown promising applications in the existing literature  Due to their quick adaptability and efficiency, a number of existing online likelihood methods have been applied as interactive segmentation methods  MONet enables human-in-the-loop online learning to perform AI-assisted annotations and should not be mistaken for an end-to-end segmentation model. We perform expert evaluation which shows that adaptively learned MONet outperforms existing state-of-the-art, achieving 5.86% higher Dice score with 24.67% less perceived NASA-TLX workload score evaluated."
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,2.0,Method,"Given an input image volume, I, a pre-trained CNN segmentation model generates an automatic segmentation C with associated probabilities P . When using data from a new domain, the automated network may fail to properly segment foreground/background objects. To improve this, the user provides scribblesbased interaction indicating corrected class labels for a subset of voxels in the image I. Let S = S f ∪ S b represent these set of scribbles, where S f and S b denote the foreground and background scribbles, respectively, and S f ∩ S b = ∅. Figure "
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,2.1,Multi-scale Online Likelihood Network,"Our proposed multi-scale online likelihood network (MONet), shown in Fig. "
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,2.2,Adaptive Loss for Online Learning,"The scribbles S only provide sparse information for online learning. However, these corrections are likely also applicable to neighboring voxels with similar appearance features, thereby providing an extended source of training information. Concurrently, the initial automated segmentation C will often provide reliable results away from the scribbles. To extend the influence of the scribbles S while preserving the quality of the initial segmentation C, we propose a spatially-varying adaptive online loss: where i is a voxel index, L C and L S are individual loss terms for learning from the automated segmentation C and the user-provided correction scribbles S respectively. W are spatially-varying interaction-based weights defined using the geodesic distance D between voxel i and the scribbles S: where the temperature term τ controls the influence of W in I. The geodesic distance to the scribbles is defined as D(i, S, I) = min j∈S d(i, j, I) where d(i, j, I) = min p∈Pi,j 1 0 ∇I(p(x)) • u(x) dx and P i,j is the set of all possible differentiable paths in I between voxels i and j. A feasible path p is parameterized by x ∈ [0, 1]. We denote u(x) = p (x)/ p (x) the unit vector tangent to the direction of the path p. We further let D = ∞ for S = ∅. Dynamic Label-Balanced Cross-Entropy Loss: User-scribbles for online interactive segmentation suffer from dynamically changing class imbalance  where α and β are class weights for labels C and scribbles S that are defined by labels and scribbles distributions during online interaction as: The patch-based training approach from "
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,2.3,Improving Efficiency with Probability-Guided Pruning,"MONet is applied as an online likelihood learning method, where the online training happens with an expert human annotator in the loop, which makes online training efficiency critical. We observe that the automatic segmentation models provide dense labels C which may significantly impact online training and inference performance. C may contain ambiguous predictions for new data, and a number of voxels in C may provide redundant labels. To improve online efficiency while preserving accuracy during training, we prune labels as C * = M C where: M i is set to 1 if P i ≥ ζ and U i ≥ η and 0 otherwise. ζ ∈ [0, 1] is the minimum confidence to preserve a label, U i ∈ [0, 1] is a uniformly distributed random variable, and η ∈ [0, 1] is the fraction of samples to prune."
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,3.0,Experimental Validation,"Table  We utilize τ = 0.3 for MONet, MIDeepSegTuned and MONet-NoMS. We use GraphCut regularization, where λ = 2.5 and σ = 0.15 "
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,3.1,Quantitative Comparison Using Synthetic Scribbler,We employ the synthetic scribbler method from 
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,3.2,Performance and Workload Validation by Expert User,This experiment aims to compare the performance and perceived subjective workload of the proposed MONet with the best performing comparison method MIDeepSegTuned based on 
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,4.0,Conclusion,"We proposed a multi-scale online likelihood network (MONet) for scribblesbased AI-assisted interactive segmentation of lung lesions in CT volumes from COVID-19 patients. MONet consisted of a multi-scale feature extractor that enabled extraction of relevant features at different scales for improved accuracy. We proposed an adaptive online loss that utilized adaptive weights based on user-provided scribbles that enabled adaptive learning from both an initial automated segmentation and user-provided label corrections. Additionally, we proposed a dynamic label-balanced cross-entropy loss that addressed dynamic class imbalance, an inherent challenge for online interactive segmentation methods. Experimental validation showed that the proposed MONet outperformed the existing state-of-the-art on the task of annotating lung lesions in COVID-19 patients. Validation by an expert showed that the proposed MONet achieved on average 5.86% higher Dice while achieving 24.67% less perceived NASA-TLX workload score than the MIDeepSegTuned method  This project utilized scribbles-based interactive segmentation tools from opensource project MONAI Label (https://github.com/Project-MONAI/MONAILabel) "
Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 53.
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,1.0,Introduction,"Deep neural networks have shown expert-level performance in various disease diagnoses  Multiple approaches have been proposed to alleviate the catastrophic forgetting issue. One approach aims to determine part of the model parameters which are crucial to old knowledge and tries to keep these parameters unchanged during learning new knowledge  In this study, inspired by recent advances in transfer learning in natural language processing "
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,2.0,Method,"This study aims to improve continual learning performance of an intelligent diagnosis system. At each learning round, following previous studies "
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,2.1,Overall Framework,"We propose an Adapter-based Continual Learning framework called ACL with a multi-head training strategy. With the motivation to make full use of readily available pretrained CNN models and slow down the speed of model expansion that appears in some state-of-the-art continual learning methods (e.g., DER "
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,2.2,Task-Specific Adapters,"State-of-the-art continual learning methods try to preserve old knowledge by either combining old fixed feature extractors with the newly learned feature extractor  where the adapter-tuned output ẑt,k will be used as input to the (k +1)-th stage. The light-weight adapter can be flexibly designed. In this study, a simple twolayer convolution module followed by a global scaling is designed as the adapter (Fig.  Transformer model in natural language processing. Different from Delta tuning which is used as a transfer learning strategy to adapt a pretrained model for any individual downstream task, the proposed task-specific adapter is used as a continual learning strategy to help a model continually learn new knowledge over multiple rounds (i.e., multiple tasks) in image processing, with each round corresponding to a specific set of adapters. Also note that the proposed adapter differs from existing adapters in CLIP-Adapter (CA) [9] and Tip-Adapter (TA) "
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,2.3,Task-Specific Head,"Task-specific head is proposed to alleviate the potential feature fusion issue in current state-of-the-art methods  However, training the task-specific classifier head without considering its relationship with existing classifier heads of previously learned tasks may cause the head selection issue during model inference. For example, a previously learned old classifier head may consider an input of latterly learned class as one of the old classes (correspondingly the 'others' output from the old head will be low). In other words, the 'others' outputs from multiple classifier heads cannot not be reliably compared (i.e., not calibrated) with one another if each classifier head is trained individually. In this case, if all classifier heads consider a test input as 'others' class with high confidence or multiple classifier heads consider a test input as one of their classes, it would become difficult to choose an appropriate classifier head for final prediction. To resolve the head selection issue, after initial training of the current task's adapters and classifier head, all the tasks' heads are fine-tuned together such that all 'others' outputs from the multiple heads are comparable. In short, at the t-th round of continual learning, the t task-specific classifier heads can be fine-tuned by minimizing the loss function L, where L c s is the cross-entropy loss for the s-th classifier head. Following the finetuning step in previous continual learning studies  Once the multi-head classifier is fine-tuned at the t-th round of continual learning, the classifier can be applied to predict any test data as one of all the learned classes so far. First, the task head with the smallest 'others' output probability (among all t 'others' outputs) is selected, and then the class with the highest output from the selected task head is selected as the final prediction result. Although unlikely selected, the 'others' class in the selected task head is excluded for the final prediction."
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,3.1,Experimental Setup,"Four datasets were used to evaluate the proposed ACL (Table  In continual learning, the classifier sequentially learned multiple tasks, with each task a small number of new classes (e.g., 2, 10, 20). After learning each task, the mean class recall (MCR) over all classes learned so far is used to measure the classifier's performance. Note that MCR is equivalent to classification accuracy for class-balanced test set. For each experiment, the order of classes is fixed, and all methods were executed three times with different initialization. The mean and standard deviation of MCRs over three runs were reported."
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,3.2,Result Analysis,"Effectiveness Evaluation: In this section, we compare ACL against stateof-the-art baselines, including iCaRL  Ablation Study: An ablation study was performed to evaluate the performance gain of each proposed component in ACL. Table "
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,4.0,Conclusion,"Here we propose a new adapter-based strategy for class-incremental learning of new diseases. The learnable light-weight and task-specific adapters, together with the pretrained and fixed feature extractor, can effectively learn new knowledge of diseases and meanwhile keep old knowledge from catastrophic forgetting. The task-specific heads with the special 'out-of-distribution' output neuron within each head helps keep extracted features discriminative between different tasks. Empirical evaluations on multiple medical image datasets confirm the efficacy of the proposed method. We expect such adapter-based strategy can be extended to other continual learning tasks including lesion detection and segmentation."
Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 7.
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,1.0,Introduction,"Differential privacy (DP) has emerged as a promising technique to safeguard the privacy of sensitive data in federated learning (FL)  For critical medical applications requiring low error tolerance, such performance degradation makes the rigorous privacy guarantee diminish  Several studies have examined the trade-off in the centralized scenario. For instance, Li et al.  To improve the trade-off of privacy protection and performance, the key point is to mitigate the noise added to the client during gradient updates. Our idea is inspired by the observation in DP-FedAvg  In this paper, we present a novel adaptive intermediary method to optimize the privacy-performance trade-off. Our approach is based on the interplay relationships among noise levels, training diversities, and the number of clients. Specifically, we observe a reciprocal correlation between the noise level and the number of intermediaries, as well as a linear correlation between the training diversity and the intermediary number. To determine the optimal number of intermediaries, we introduce a new term called intermediary ratio, which quantifies the ratio of noise level and training diversity. Our theoretical analysis demonstrates that splitting the original clients into more intermediaries achieves DP with the same privacy budget and DP failure probability. Furthermore, we show that when sample-level DP and client-level DP have equivalent noise levels, the variance of the difference between noisy and original model diverges exponentially with more training steps, leading to poor performance. We evaluate our method on both classification and segmentation tasks, including the intracranial hemorrhage diagnosis with 25,000 CT slices, and the prostate MRI segmentation with heterogeneous data from different hospitals. Our method consistently outperforms various DP optimization methods on both tasks and can serve as a lightweight add-on with good compatibility. In addition, we conduct comprehensive analytical studies to demonstrate the effectiveness of our method."
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,2.1,Preliminaries,"In this work, we consider client-level differential privacy. We first introduce the definition of DP as follows: Definition 1. (( , δ)-Differential Privacy  where denotes the privacy budget, and δ represents the probability that -DP fails in this mechanism. Note that the smaller the value is, the more private the mechanism is. Our aim of applying DP is to protect the collection of ""datasets"" X , which are client model updates in every communication round in the context of FL. The protection can be done by incorporating a DP-preserving randomization mechanism into the learning process. One commonly used method is the Gaussian mechanism, which involves bounding the contribution (l 2 -norm) of each client update followed by adding Gaussian noise proportional to that bound onto the aggregate  where C is the gradient clipping threshold, and z is the noise multiplier determined by the privacy accountant with given , δ, and training steps. The noise multiplier z indicates the amount of noise required to reach a particular privacy budget. To privatize the participation of clients in FL, the noise added for client-level DP typically correlates with the number of clients. This incurs a large magnitude of noise in cross-silo FL in the medical field, which can significantly deteriorate the final server model performance."
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,2.2,Adaptive Intermediary for Improving Client-Level DP,"The key to optimizing the privacy-performance trade-off lies in mitigating the effects of noise without compromising privacy protection. Based on the noise calculation in Eq. (  Feasibility. We demonstrate the feasibility by showing the use of intermediary preserves privacy. For X the collection of possible datasets from extant clients, denote D i ∈ X the dataset of client i, we randomly split D i into v disjoint subsets D i,1 , ..., D i,v , so that j D i,j = D i . We define the dataset D i,j of client i as the intermediary j. Then we show that partitioning extant clients into multiple intermediaries is capable of maintaining DP. Denote the collection of all possible datasets formed by the intermediaries as Y, and note that X ⊆ Y. We have: This indicates that partitioning the original client into intermediaries keeps the same DP regime. The proof can be found in Appendix C. We also analyze the reverse relation in the appendix section to complete the overall relationship. Privacy-Performance Trade-Off Analysis. With the above basis, we further investigate the privacy-performance trade-off by varying the number of intermediaries. According to the noise calculation of σ = zC /N, we can reduce noise by splitting clients into intermediaries to increase N . However, increasing the number of intermediaries causes each intermediary to hold fewer samples. This may affect the aggregation direction and harms final performance consequently. There is a trade-off behind intermediary splitting. To investigate the trade-off, we design and study two highly related metrics, i.e., noise level ξ and client update diversity level ϕ. Denoting clipped gradients as Δi , we define the noise level and diversity level as: By varying the number of intermediaries, we obtain different values for noise levels and diversities (see Fig.  where ξ v and ϕ v denote the value when each client is split into v intermediaries. By defining the intermediary ratio as λ = ξ /ϕ, we can use this ratio to quantify the relations between noise level and diversity, which helps identify the optimal number of intermediaries to generate. Adaptive Intermediary Generation. We can generate the intermediary based on the defined intermediary ratio λ. We experimentally investigated the relationships between the final performance and the number of intermediaries and found the optimal ratio lies in the range of 1 /N. Therefore, for each client, the number of intermediaries is v = 2 N • ξ /ϕ. Considering the extreme case of lim N →∞ ξ = 0, we can also infer the ratio λ = 0, which further validates the rationality and consistency with our empirical findings. For the practical application, we can initialize the number of intermediaries via the first round results. Then, for each round, we will re-calculate the ratio using ξ and ϕ from the last round, and then adaptively split clients to make sure the new ratio lies around 1 /N."
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,2.3,Cumulation of Sample-Level DP to Client-Level,"We further investigate the relationships between client-level DP and sample-level DP, by cumulating sample-level DP mechanism to a client level. In DP-SGD  Note that z can take different forms, the form provided by moment accountant  On the Client-Level. For client-level noise, we can compute the standard deviation as σ c = z( c , δ c )C, where C is the clip bound of client update. The clip bound is typically set to the median among l2-norms of all client updates. Assuming an identical distribution across clients and samples, we have C = O(T c). As a result, we have z c = O(z T ), indicating that the cumulation of sample-level noise in DP-SGD gives the same DP level up to a constant, which is equivalent to adding noise directly to the client level through the moment accountant. Regarding the performance, we note that by leveraging the noisy models from several clients that hold identically distributed datasets, we can reduce the probability of getting a significantly drifted model without additional privacy leakage."
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,3.1,Experimental Setup,"Datasets. We evaluate our method on two tasks: 1) intracranial hemorrhage (ICH) classification, and 2) prostate MRI segmentation. For ICH classification, we use the RSNA-ICH dataset  Table "
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,,Method,"No Privacy z = 0.5 z = 1.0 z = 1.5 Privacy Setup. We use the Opacus'  Implementation Details. We use Adam optimize, set the local update epoch to 1, and set total communication rounds to 100. We use DenseNet121 "
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,3.2,Empirical Evaluation,"First, we present experimental results using different global optimizers on the server with client-level DP. Then, we demonstrate how our adaptive intermedi-ary strategy benefits privacy-performance trade-offs. We consider four popular private server optimizers: DP-FedAvg  We perform validation with different noise multiplier values. Non-private FL is also provided as a performance upper bound. Note that our method has the same performance ascompared methods in non-private settings, because there are no noises to harmonize. As can be observed from Table "
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,3.3,Analytical Studies,Effects of Optimizing Privacy-Performance Trade-Offs. We present the dynamic behavior of our method regarding variations of the intermediary ratio λ across different rounds in Fig.  Client Scalability Analysis. As the noise level is highly dependent on client numbers (see Eq. ( 
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,,Stability of Adaptive Intermediary,"Estimation. Finally, we analyze the historical variation of our adaptive intermediary strategy in Fig. "
Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging,4.0,Conclusion,"In this paper, we propose a novel adaptive intermediary method to promote privacy-performance trade-offs in the context of client-level DP in FL. We have comprehensively studied the relations among number of intermediaries, noise levels and training diversities in our work. We also investigate relations between sample-level and client-level DP. Our proposed method outperforms compared methods on both medical image diagnosis and segmentation tasks and shows good compatibility with existing DP optimizers. For future work, it is promising to investigate our method for clients with imbalanced class distributions, where the intermediary may not have all labels."
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,1.0,Introduction,"Semantic segmentation of organs, anatomical structures, or anomalies in medical images (e.g. CT or MRI scans) remains one of the fundamental tasks in medical image analysis. Volumetric medical image segmentation (MIS) helps healthcare professionals to diagnose conditions more accurately, plan medical treatments, and perform image-guided procedures. Although deep neural networks (DNNs) have shown remarkable improvements in performance for different vision tasks, A model trained on voxel-domain adversarial attacks is vulnerable to frequency-domain adversarial attacks. In our proposed adversarial training method, we generate adversarial samples by perturbing their frequency-domain representation using a novel module named ""Frequency Perturbation"". The model is then updated while minimizing the dice loss on clean and adversarially perturbed images. Furthermore, we propose a frequency consistency loss to improve the model performance. including volumetric MIS, their real-world deployment is not straightforward particularly due to the vulnerabilities towards adversarial attacks  Ensuring the adversarial robustness of the models involved in safety-critical applications such as, medical imaging and healthcare is of paramount importance because a misdiagnosis or incorrect decision can result in life-threatening implications. Moreover, the weak robustness of deep learning-based medical imaging models will create a trust deficit among clinicians, making them reluctant to rely on the model predictions. The adversarial robustness of the medical imaging models is still an open and under-explored area  In the context of 2D natural images, it has been recently observed that frequency-domain based adversarial attacks are more effective against the defenses that are primarily designed to ""undo"" the impact of pixel-domain adversarial noise in natural images  -We propose an approach with a min-max objective for adversarial training of volumetric MIS model in the frequency domain. In the maximization step, we introduce a volumetric adversarial frequency attack (VAFA) that is specifically designed for volumetric medical data to achieve higher fooling rate. Further, we introduce a volumetric adversarial frequency-domain training (VAFT) based on a frequency consistency loss in the minimization step to produce a model that is robust to adversarial attacks. -We conduct experiments with two different hybrid CNN-transformers based volumetric medical segmentation methods for multi-organ segmentation. Related Work: There are three main types of popular volumetric MIS model architectures: CNN "
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,2.0,Frequency Domain Adversarial Attack and Training,"We aim to train a model for volumetric medical segmentation that is robust against adversarial attacks. Existing adversarial training (AT) approaches rely on min-max optimization  maximizing the model loss (e.g., dice loss in segmentation). The loss function is then minimized on such adversaries to update the model parameters. In this work, we propose a frequency-domain adversarial attack that takes into account the 3D nature of the volumetric medical data and performs significantly better than the other voxel-domain as well as 2D frequency domain attacks (Table  3D Medical Segmentation Framework: Deep learning-based 3D medical segmentation generally uses encoder-decoder architectures "
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,2.1,Volumetric Adversarial Frequency Attack (VAFA),"Generally, adversarial attacks operate in the voxel domain by adding an imperceptible perturbation to the input data. In contrast, our attack perturbs the 3D-DCT coefficient to launch a frequency-domain attack for 3D medical image segmentation. Our Frequency Perturbation Module (FPM) transforms voxel-domain data into frequency-domain by using discrete cosine transforms (DCTs) and perturbs the DCT coefficients using a learnable quantization. It then takes an inverse DCT of the perturbed frequency-domain data and returns voxel-domain image. We keep the model in a ""frozen"" state while maximizing the dice loss  We represent a 3D (volumetric) single channel clean sample by X ∈ R 1×H×W ×D and its ground-truth binary segmentation mask by Y ∈ {0, 1} NumClass×H×W ×D , where ""NumClass"" is the number of classes. We split X into n 3D patches i.e. X → {x i } n i=1 , where We apply our frequency perturbation module to each of these patches. Frequency Perturbation Module: We apply a 3D discrete cosine transform (DCT), represented as D(•), to each patch x i . The resulting DCT coefficients are then processed through a function ϕ(•), which performs three operations: quantization, differentiable rounding (as described in  x → D(x) → ϕ(D(x), q) quantization, rounding and de-quantization We repeat the above mentioned sequence of transformations for all patches and then merge {x i } n i=1 to form adversarial image X ∈ R H×W ×D . Quantization Constraint: We learn quantization table q by maximizing the L dice while ensuring that q ∞ ≤ q max . Quantization threshold q max controls the Algorithm 1. Volumetric Adversarial Frequency Attack (VAFA)"
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,,". , n}","Initialize all quantization tables with ones. 5: for t ← 1 to T do 6: Merge all adversarial patches to form X 9: L(X, X , Y) = L dice (M θ (X ), Y) -Lssim(X, X ) 10: end for 13: end function 14: Return X Algorithm 2. Volumetric Adversarial Frequency Training (VAFT) Freq. Attack on clean images. 7: Backward pass and update M θ 9: end for 10: end for 11: M ← M θ AT robust model after training completion. 12: Return M extent to which DCT coefficients are perturbed. The higher the value of q max , the more information is lost. The drop in perception quality of the adversarial sample and the accuracy of the model are directly proportional to the value of q max . To increase the perceptual quality of adversarial samples, we also minimize the structural similarity loss  where L ssim (X, X ) = 1 -1 n n i=1 SSIM(x i , x i ) is structural similarity loss "
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,2.2,Volumetric Adversarial Frequency Training (VAFT),"The model parameters are then updated by minimizing the segmentation loss on both clean and adversarial samples (Eq. 3). Since our attack disrupts the frequency domain to find adversaries, we develop a novel frequency consistency loss (Eq. 4) to encourage frequency domain representation of the model's output (segmentation logits) for the clean sample close to the adversarial sample. Our frequency consistency loss not only boosts the robustness of the model against adversarial attacks but also improves/retains the performance of the robust model on clean images (Sect. 3). We present our volumetric adversarial frequency training (VAFT) in Algorithm 2. where X = VAFA(X, Y) and D(•) is 3D DCT function. An overview of the adversarial training can be found in minimization step of Fig. "
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,3.0,Experiments and Results,"Implementation Details: We demonstrate the effectiveness of our approach using two medical segmentation models: UNETR  Results: For each evaluation metric, we take mean across all classes (including background) and test images. In each table (where applicable), green values show DSC and HD95 on clean images. Table "
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,4.0,Conclusion,"We present a frequency-domain based adversarial attack and training for volumetric medical image segmentation. Our attack strategy is tailored to the 3D nature of medical imaging data, allowing for a higher fooling rate than voxelbased attacks while preserving comparable perceptual similarity of adversarial samples. Based upon our proposed attack, we introduce a frequency-domain adversarial training method that enhances the robustness of the volumetric segmentation model against both voxel and frequency-domain based attacks. Our training strategy is particularly important in medical image segmentation, where the accuracy and reliability of the model are crucial for clinical decision making."
Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_43.
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,1.0,Introduction,"The brain consumes copious amounts of energy to sustain its activity, resulting in a skewed energetic budget per mass compared to the rest of the body (about 25% utilized by about 3%, see  Neurovascular coupling is a cornerstone of proper brain function and also underpins the ability to observe and study the human brain in action. Imaging methods based on blood oxygenated level dependent (BOLD) approaches rely on it  The initial characterization of the hemodynamic response function (HRF) was performed at the system level, where system refers to large cortical regions encompassing tens of thousands of neurons of different types, without taking into account the fine details of different vascular compartments (see  Our model needs to combine neuron firing and blood vessel data and employs a multi-modal transformer. There are three types of multi-modal transformers: (i) a multi-modal Transformer where the two modalities are concatenated and separated by the [SEP] token  Our results show that the new transformer model can predict the state of blood vessels better than the baseline models. The utility of neuronal data in the prediction is demonstrated by an ablation study. By analyzing the learned model, we obtain insights into the link between neuronal and vascular activities."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,2.0,Data,"All procedures were approved by the Ethics Committee of Tel Aviv University for Animal Use and Welfare and followed pertinent Institutional Animal Care and Use Committee (IACUC) and local guidelines. Neuronal activity was monitored in female C57BL/6J transgenic mice expressing Thy1-GCaMP6s. Vascular dynamics were tracked using a Texas Red fluorescent dye, which was conjugated to a large polysaccharide moiety (2 mega Dalton dextran) and retro-orbitally bolus injected under brief isoflurane sedation at the beginning of the imaging day. 425 quasi-linear vascular segments and 50 putative neuronal cell bodies were manually labeled within a volume of 490×500×300μm 3 , which was continuously imaged across two consecutive 1850-second long sessions at an imaging rate of 30.03 volumes per second  For vascular diameter estimation we used the Radon transform,  This unique ability to rapidly track neuronal and vascular interactions across a continuous brain volume bears several important advantages "
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,3.0,Method,"The HRF learning problem explored in this work is defined as the prediction of current blood flow rates at different vessel segments, given the previous neuronal spikes as well as previous blood flow rates. We propose to design a parameterized deep neural network f θ for scalar regression of blood flow rates at different vessel segments, such that at a given time t we have where the matrix S t ∈ R ts×n denotes the n neurons' spikes at the t s previous samples, while the matrix F t ∈ R tv×m denotes the blood flow of the m vessel segment at the previous t v time samples. X S ∈ R n×3 and X F ∈ R m×3 are the three-dimensional positions of the neurons and vessel segments, respectively. HRF predictions should satisfy fundamental symmetries and invariance of physiological priors and of experimental bias, such as invariance to rigid spatial transformation (rotation and translation). Therefore, a positional input X u is transformed to inter-elements Euclidean distances Thus, the learning problem is refined as , where D S , D F , D SF represents the Euclidean distance matrix between neurons, vessel segments, and neurons to vessel segments, respectively. We do not include any further auxiliary features or prior in the input. We model f θ using a new variant of the Transformer family. The proposed model consists of an encoder and a decoder. The encoder embeds the neurons at both spatial and temporal levels. The decoder predicts vessel segment flow by utilizing both the past flow values and the spatial information of the vessel segments, along with the neuronal activity via the cross-attention mechanism."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,,Transformers.,"The self-attention mechanism introduced by Transformers  where Q ∈ R N ×d , K ∈ R k×d and V ∈ R k×d represent the packed N queries, k keys and values tensors respectively. Keys, queries and values are obtained using linear transformations of the sequence's elements. A multi-head self-attention layer is defined by extending the self-attention mechanism using h attention heads, i.e. h self-attention functions applied to the input, reprojected to values via a dh × D linear layer."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,,Neuronal Encoding.,"To obtain the initial Spatio-Temporal Encoding, for the prediction at time t, we project each neuron to a high d dimensional embedding φ s t ∈ R ts×n×d by modulating it with its spike value such that , where W ∈ R d denotes the neuronal encoding. The embedding is modulated by the magnitude of the spike, such that higher neuronal activities are projected farther in the embedding space. The temporal encoding is defined using sinusoidal encoding  In order to incorporate the spatial information of the neurons, we propose to insert spatial encoding by importing the pairwise information directly into the self-attention layer. For this, we multiply the distance relation by the similarity tensor as follows with denoting the Hadamard product, and ψ S (D S ) : R + → R + an elementwise learnable parameterized similarity function. This way, the similarity function scales the self-attention map according to the distance between the elements (in our case the neurons)."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,,Vascular Decoding.,"The spatio-temporal encoding of the vascular data is similar to the embedding performed by the encoder. The information on each vascular segment is embedded in a high-dimensional vector φ F t ∈ R tv×m×d to be further projected by the temporal encoding. The spatial geometric information is incorporated via the pairwise vascular segments' distance matrix D F via the decoder's self-attention module A F . The most important element of the decoder is the cross-attention module, which incorporates neuronal information for vascular prediction. Given the final neuronal embeddings φ s t , the cross-attention module performs cross-analysis of the neuronal embeddings such that where Q F and K S represent the affine transform of φ F t and φ s t , respectively. Here also, the (non-square) cross-attention map is modulated by the neuronvessel distance matrix D SF . The spatio-temporal map is of dimensions A SF ∈ R tv×ts×h×m×n where h denotes the number of attention heads. Thus, we perform aggregation by averaging over the neuronal time dimension, in order to remain invariant to the temporal neuronal embedding and to gather all past neuronal influence on blood flow rates. This way, one can observe that the proposed method is not limited to any spatial or time constraint. The model can be deployed in different spatiotemporal settings at test time, thanks to both the geometric spatial encoding and the Transformer's sequential processing ability. Finally, the output module reprojects the last time vessel embedding into the prediction space."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,,Architecture and Training.,"The initial encoding defines the model embedding dimension d = 64. The encoder and the decoder are defined as the concatenation of L = 3 layers, each composed of self-attention and feed-forward layers interleaved with normalization layers. The decoder also contains N additional cross-attention modules. The output layer is defined by a fully connected layer that projects the last vascular time embedding into the objective dimension m. An illustration of the model is given in Fig.  The dimension of the feed-forward network is four times that of the embedding  The training objective is the Mean Squared Error loss The Adam optimizer "
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,4.0,Experiments,"We compare the proposed method, dubbed Hemodynamic Response Function Transformer (HRFT), with several popular statistical and machine-learning models: (i) naive persistence model, which predicts the previous time step's vascular input, (ii) linear regression, which concatenates all the input (blood flow and neuronal data) from all times stamps before performing the regression, and (iii) a Recurrent Neural Network composed of two stacked GRU  We present both MSE and Normalized Root MSE. Because of computational constraints, we randomly subsample 55 vessel segments among the 425. We trained the model with temporal windows of size t s = t v = 10, equivalent to 300 ms according to the original data acquisition's 30.03 Hz sampling rate. In addition to the original sampling rate, we also present results for prediction based on lower frequencies, in order to check the ability of the models to capture longer-range dependencies. We note that the error in these cases is expected to be larger, since the time gap between the last measurement and the required prediction is larger. In order to check the generalization abilities of the methods, we test the trained models on a second dataset obtained 30 min after the sampling of the original dataset (that includes training, validation, and the first test set). The results are presented in Table  To gain insights into the HRF, we examine the HRFT model. The learned distance function ψ SF of the 1st cross attention layer is depicted in Fig. "
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,5.0,Conclusions and Future Work,"We present the first local HRF model. While for the baseline methods, the performance is at the same level with and without neuronal data (omitted from the tables), the transformer we present supports an improved prediction capability using neuronal firing rates (ablation) and also gives rise to interesting insights regarding the behavior of the hemodynamic response function. Limitations. Our main goal is to verify the ability to model HRF by showing that using neuronal data helps predict blood flow beyond the history of the latter. The next challenge is to scale the model in order to be able to model more vessels (without subsampling) and longer historical sequences (larger t v , t s ). With transformers being used for very long sequences, this is a limitation of our resources and not of our method."
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,,Acknowledgements,. The authors thank 
Reconstructing the Hemodynamic Response Function via a Bimodal Transformer,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 35.
Weakly Supervised Medical Image Segmentation via Superpixel-Guided Scribble Walking and Class-Wise Contrastive Regularization,1.0,Introduction,"Accurately segmenting cardiac images is crucial for diagnosing and treating cardiovascular diseases. Recently, deep learning methods have greatly advanced cardiac image segmentation. However, most state-of-the-art segmentation models require a large scale of training samples with pixel-wise dense annotations, which are expensive and time-consuming to obtain. Thus, researchers are active in exploring other labour-efficient forms of annotations for effective training. For example, semi-supervised learning (SSL)  In this paper, we propose SC-Net, a new scribble-supervised approach that combines Superpixel-guided scribble walking with Class-wise contrastive regularization. The basic framework is built upon the recent dual-decoder backbone design "
Weakly Supervised Medical Image Segmentation via Superpixel-Guided Scribble Walking and Class-Wise Contrastive Regularization,2.1,Preliminaries and Basic Framework,"In the scribble-supervised setting, the dataset includes images and their corresponding scribble annotations. We denote an image as X with the scribble annotation S = {(s r , y r )}, where s r is the pixel of scribble r, and y r ∈ {0, 1, ..., C -1} denotes the corresponding label with C possible classes at pixel s r . As shown in Fig.  where p c 1(i) and p c 2(i) are the predicted probability of pixel i belonging to class c from the two decoders θ Dec1 and θ Dec2 , respectively. For the self-training loss, this dual-decoder framework randomly mix the predictions from the two different decoders to generate the ensemble pseudo label as: ŷML = argmax[α × p 1 + (1α) × p 2 , where α = random(0, 1). Such dynamically mixing scheme can increase the diversity of pseudo labels, which helps to prevent the model from memorizing its own single prediction and falling into a trivial solution during optimization  ( Despite its effectiveness, this framework still overlooks the aforementioned fundamental limitations of sparse scribble supervision: (i) although the mixed pseudo labels provide dense supervision, they still stems from the initial sparse guidance, making it difficult to provide accurate local structural information. Thus, we propose superpixel-guided scribble walking strategy (Sect. 2.2) to enrich structural priors for the initial supervision itself. (ii) Extremely sparse supervision inevitably leads to less-compact class feature distributions, resulting in poor generalizability to unseen test data. Thus, we further propose class-wise contrastive regularization (Sect. 2.3) to enhance the compactness of class embeddings."
Weakly Supervised Medical Image Segmentation via Superpixel-Guided Scribble Walking and Class-Wise Contrastive Regularization,2.2,Superpixel-Guided Scribble Walking,"In order to enhance the structural information in our initial supervision, we utilize the superpixel of the image as a guide for propagating scribble annotations to unlabeled pixels, considering that it effectively groups pixels with similar characteristics within the uniform regions of an image and helps capture the class boundaries  Next, it iteratively assigns each pixel to the nearest seed point based on its color similarity and spatial proximity (distance). This process is repeated until the clustering converges or reaches a predefined number of iterations. Finally, the algorithm updates the location of the seed points to the centroid of the corresponding superpixel, and repeats until convergence. As such, the image is coarsely segmented into K clusters. To balance accuracy and computational efficiency, the number of iterations is empirically set to 10. K is set to 150. An example of superpixel is depicted in Fig.  where N is the number of label-containing pixels. ŷsp is converted to one-hot representation. p 1(i) and p 2(i) are the predicted probabilities of pixel i from θ Dec1 and θ Dec2 , respectively. Following "
Weakly Supervised Medical Image Segmentation via Superpixel-Guided Scribble Walking and Class-Wise Contrastive Regularization,2.3,Class-Wise Contrastive Regularization,"When using extremely sparse supervision, it is difficult for the model to learn compact class feature distributions, leading to poor generalizability. To address this, we propose a class-wise contrastive regularization term that leverages prototype contrastive learning to disconnect the feature manifolds of different classes, as illustrated in Fig.  where w ij is obtained by normalizing the learnable attention weights (detailed in  Overall, the final loss of our SC-Net is summarized as: where λ MLS , λ sN R and λ CR are the trade-off weights. λ MLS is set to 0.5, following "
Weakly Supervised Medical Image Segmentation via Superpixel-Guided Scribble Walking and Class-Wise Contrastive Regularization,3.0,Experiments and Results,"Dataset. We evaluate our method on the public ACDC dataset  Implementation and Evaluation Metrics. The framework is implemented with PyTorch using an NVIDIA RTX 3090 GPU. We adopt UNet  Comparison Study. We compare our proposed SC-Net with recent state-ofthe-art alternative methods for annotation-efficient learning. Table  Ablation Study. We perform an ablation study to investigate the effects of the two key components of our SC-Net. The results are also shown in Table  We found that the two components need to work together. When we remove L sN R , the performance degrades to some extent. This may be because it is difficult for the model to generate high-quality local pseudo-labels with only sparse supervision provided by scribbles, and class-wise contrastive regularization relies heavily on pseudo labels to separate class features. When we remove L CR , the performance also drops slightly. This is mainly because the generated superpixels inevitably contain errors, which can misguide the scribble walking. Yet, using L CR can regularize the feature distribution between classes, reducing the impact of these errors. Meanwhile, the structure prior strengthened by superpixel guidance helps to provide higher-quality local pseudo labels to assist class-wise contrastive regularization. The two components complement each other, resulting in the best performance of our complete SC-Net. Sensitivity Analysis. The superpixel-guided scribble walking plays an important role in our SC-Net. Thus, we conduct further assessments on the sensitivity of λ sN R , which is used to weight L sN R , and the cluster number K used for superpixel generation. The results obtained by five-fold cross validation are presented in Fig. "
Weakly Supervised Medical Image Segmentation via Superpixel-Guided Scribble Walking and Class-Wise Contrastive Regularization,4.0,Conclusion,"In this work, we proposed the SC-Net towards effective weakly supervised medical image segmentation using scribble annotations. By combining superpixelguided scribble walking with class-wise contrastive regularization, our approach alleviates two inherent challenges caused by sparse supervision, i.e., the lack of structural priors during training and less-compact class feature distributions. Comprehensive experiments on the public cardiac dataset ACDC demonstrated the superior performance of our method compared to recent scribble-supervised and semi-supervised methods with similar labeling efforts."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,1.0,Introduction,"Humans inherently learn in an incremental manner, acquiring new concepts over time without forgetting previous ones. In contrast, deep learning models suffer from catastrophic forgetting  The medical domain faces a similar problem: the ability to dynamically extend a model to new classes is critical for multiple organ and tumor segmentation, wherein the key obstacle lies in mitigating 'forgetting.' A typical strategy involves retaining some previous data. For instance, Liu et al.  Therefore, we identify two main open questions that must be addressed when designing a multi-organ and tumor segmentation framework. Q1: Can we relieve the forgetting problem without needing previous data and annotations? Q2: Can we design a new model architecture that allows us to share more parameters among different continual learning steps? To tackle the above questions, in this paper, we propose a novel continual multi-organ and tumor segmentation method that overcomes the forgetting problem with little memory and computation overhead. First, inspired by knowledge distillation methods in continual learning  We focus on organ/tumor segmentation because it is one of the most critical tasks in medical imaging  segment liver tumors in the LiTS dataset. On the private dataset, the learning trajectory is to first segment 13 organs, followed by continual segmentation of three gastrointestinal tracts and four cardiovascular system structures. In our study, we review and compare three popular continual learning baselines that apply knowledge distillation to predictions "
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,2.0,Methodology,"We formulate the continual organ segmentation as follows: given a sequence of partially annotated datasets {D 1 , D 2 , . . . , D n } each with organ classes {C 1 , C 2 , . . . , C n }, we learn a single multi-organ segmentation model sequentially using one dataset at a time. When training on the i-th dataset D t , the previous datasets {D 1 , . . . , D t-1 } are not available. The model is required to predict the accumulated organ labels for all seen datasets {D 1 , . . . , D t }: (1) where j is a voxel index, X is an image from D t , P is the probability function that the model learns and Ŷ is the output segmentation mask."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,2.1,Pseudo Labels for Multi-organ Segmentation,"In the context of continual organ segmentation, the model's inability to access the previous dataset presents a challenge as it often results in the model forgetting the previously learned classes. In a preliminary experiment, we observed that a segmentation model pre-trained on some organ classes will totally forget the old classes when fine-tuned on new ones. We found the use of pseudo-labeling can largely mitigate this issue and preserve the existing knowledge. Specifically, we leverage the output prediction from the previous learning step t -1, denoted as Ŷt-1 , which includes the old classes C t-1 , as the pseudo label for the current step's old classes. For new classes, we still use the ground truth label. Formally, the label Lc t for class c in current learning step t can be expressed as: where L c t represents the ground truth label for class c in step t obtained from dataset D t . By utilizing this approach, we aim to maintain the original knowledge and prevent the model from forgetting the previously learned information while learning the new classes. The following proposed model is trained only with pseudo labeling of old classes without any other distillation or regularization."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,2.2,The Proposed Multi-organ Segmentation Model,"In the following, we introduce the proposed multi-organ segmentation model for continual learning. Figure  Backbone Model: For continual learning, ideally, the model should be able to learn a sufficiently general representation that would easily adapt to new classes. We use Swin UNETR "
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Image-Aware Organ-Specific Heads:,"The vanilla Swin UNETR has a Softmax layer as the output layer that predicts the probabilities of each class. We propose to replace the output layer with multiple image-aware organ-specific heads. We first use a global average pooling (GAP) layer on the last encoder features to obtain a global feature f of the current image X. Then for each organ class k, a multilayer perceptron (MLP) module is learned to map the global image feature to a set of parameters θ k : where E(X) denotes the encoder feature of image X. An output head for organ class k is a sequence of convolution layers that use parameters θ k as convolution kernel parameters. These convolution layers are applied to the decoder features, which output the segmentation prediction for organ class k: where E is the encoder, D is the decoder, σ is the Sigmoid non-linear layer and P (Y k j = 1) denotes the predicted probability that pixel j belongs to the organ class k. The predictions for each class are optimized by Binary Cross Entropy loss. The separate heads allow independent probability prediction for newly introduced and previously learned classes, therefore minimizing the impact of new classes on old ones during continual learning. Moreover, this design allows multi-label prediction for cases where a pixel belongs to more than one class (e.g., a tumor on an organ)."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Text Driven Head Parameter Generation:,"We further equip the segmentation heads with semantic information about each organ class. With the widespread success of large-scale vision-language models, there have been many efforts that apply these models to the medical domain  where ω k is the text embedding for organ class k. CLIP embeddings carry highlevel semantic meanings and have the ability to connect correlated concepts. Therefore, it guides the MLP module to generate better convolution parameters for each organ class. More importantly, the fixed-length CLIP embedding allows us to adapt the pre-trained model to open-vocabulary segmentation and extend to novel classes. "
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Difference from Universal Model,Unlike Liu et al. 
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,2.3,Computational Complexity Analysis,"Another key contribution of our work is the reduction of computational complexity in continual segmentation. We compare our proposed model's FLOPs (floating-point operations per second) with the baseline model, Swin UNETR "
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,3.0,Experiment and Result,"Datasets: We empirically evaluate the proposed model under two data settings: in one setting, both training and continual learning are conducted on the inhouse JHH dataset. It has multiple classes annotated, which can be categorized into three groups: the abdominal organs (in which seven classes are learned in step 1: spleen, right kidney, left kidney, gall bladder, liver, postcava, pancreas), the gastrointestinal tract (in which three classes are learned in step 2: stomach, colon, intestine), and other organs (in which four classes are learned in step 3: aorta, portal vein and splenic vein, celiac truck, superior mesenteric artery). The categorization is in accordance with TotalSegmentator "
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Baselines and Metrics:,"For a fair comparison, all the compared methods use the same Swin UNETR "
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Implementation Details:,"The proposed model architecture is trained on new classes with pseudo labeling of old classes. No other distillation techniques are used. We use a lightweight design for the image-aware organ-specific heads. Each head consists of three convolution layers. The number of kernels in the first two layers is 8, and in the last layer is 1. All the compared models are trained using the AdamW optimizer for 100 epochs with a cosine learning rate scheduler. We use a batch size of 2 and a patch size of 96 × 96 × 96 for the training. The initial "
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Results:,"The continual segmentation results using the JHH dataset and public datasets are shown in Tables  To evaluate the proposed model designs, we also conduct the ablation study on the JHH dataset, shown in Table "
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,4.0,Conclusion,"In this paper, we propose a method for continual multiple organ and tumor segmentation in 3D abdominal CT images. We first empirically verified the effectiveness of high-quality pseudo labels in retaining previous knowledge. Then, we propose a new model design that uses organ-specific heads for segmentation, which allows easy extension to new classes and brings little computational cost in the meantime. The segmentation heads are further strengthened by utilizing the CLIP text embeddings that encode the semantics of organ or tumor classes. Numerical results on an in-house dataset and two public datasets demonstrate that the proposed method outperforms the continual learning baseline methods in the challenging multiple organ and tumor segmentation tasks."
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 4.
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,1.0,Introduction,"Black-box neural network classifiers offer enormous potential for computer-aided diagnosis and prediction in medical imaging applications but, unfortunately, they also have a strong tendency to learn spurious correlations in the data  Spurious correlations arise when the training data are confounded by additional variables that are unrelated to the diagnostic information we want to predict. For instance, older patients in our training data may be more likely to present with a disease than younger patients. A classifier trained on this data may inadvertently learn to base its decision on image features related to age rather than pathology. Crucially, such faulty behavior cannot be identified using classification performance metrics such as area under the ROC curve (AUC) if the testing data contains the same confounding information as the training data, since the classifier predicts the right thing, but for the wrong reason. If undetected, however, such spurious correlations may lead to serious safety implications after deployment. Fig.  Interpretable ML approaches may be used as a powerful tool to detect spurious correlations during development or after deployment of an ML system. Currently, the most widely used explanation modality are visual explanations, which highlight the pixels in the input image that are responsible for a particular decision. Common strategies include methods which leverage the gradient of the prediction with respect to the input image  The majority of visual explanation methods are post-hoc techniques, meaning a heuristic is applied to any trained model (e.g. a ResNet  Inherently interpretable visual explanation approaches are much less widely explored than post-hoc techniques, but there has recently been an increased interest in the topic. Two recently proposed methods in this category are the attribution network (Attri-Net) "
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,,Related Work on Comparing Explanation Techniques.,"A number of works have studied the quality of post-hoc explanation techniques. The vast majority of work focuses exclusively on gradient-based approaches (e.g.  In their landmark study, Adebayo et al.  A small number of works specifically investigate explanations' sensitivity to spurious correlations. In closely related work to ours, Adebayo et al.  Contributions. We present a rigorous evaluation of post-hoc explanations and inherently interpretable techniques for the identification of spurious correlations in a medical imaging task. Specifically, we focus on the task of diagnosing cardiomegaly from chest x-ray data with three types of synthetically generated spurious correlations (see Fig. "
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,2.0,A Framework for Evaluating Explanation Techniques,"In the following, we introduce our evaluation strategy and proposed evaluation metrics, the studied confounders, as well as the evaluated explanation techniques. The strategy and evaluation metrics are generic and can also be applied to different problems. The confounders are engineered to correspond to realistic image artifacts that can appear in chest x-ray imaging"
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,2.1,Evaluation Strategy,"We assume a setting in which the development data for a binary neural network based classifier contains an unknown spurious correlation with the target label. To quantitatively study this setting, we create training data with artificial spurious correlations by adding a confounding effect (e.g. a hospital tag) in a percentage of the cases with a positive label, where we vary the percentage p ∈ {0, 20, 50, 80, 100}. E.g., for p = 100% all of the positive images in the training set will have an artificial confounder, and for p = 0% there is no spurious signal. With increasing p the reliance on a spurious signal becomes more likely. The images with a negative label remain untouched. In the evaluation, we consider a scenario in which the test data contain the same confounder type with the same proportion p used in the respective trainings. In this case, we can not tell if a classifier relies on the confounded features from classification performance. Our aim, therefore, is to investigate whether explanation techniques can identify that the classifier predicts the right thing for the wrong reason. We perform all experiments on chest x-ray images from the widely used CheXpert dataset "
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,2.2,Studied Confounders,"We study three types of confounders inspired by real-world artefacts. Firstly, we investigate a hospital tag placed in the lower left corner of the image (see Fig. "
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,2.3,Evaluation Metrics for Measuring Confounder Detection,We propose two novel metrics which reflect an explanation's ability to correctly identify spurious correlations.
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,,Confounder Sensitivity (CS).,"Firstly, the explanations should be able to correctly attribute the confounder if classifier bases its decision on it. We assess this property by summing the number of true positive attributions divided by the total number of confounded pixels for each test image. We consider a pixel a true positive if it is part of the pixels affected by the confounder and in the top 10% attributed pixels according to a visual explanation. Thus the maximum sensitivity of 1 is obtained if all confounded pixels are in the top 10% of the attributions. Note that we do not penalise attributions outside of the confounding label as those can still also be correct. To guarantee that we only evaluate on samples for which the prediction is actually influenced by the confounder, we only include images for which the prediction with and without the confounding label is of the opposite class. To reduce computation times we use a maximum of 100 samples for each evaluation. An optimal explanation methods should obtain a CS score of 0 if the data contains p = 0% confounded data points, since in that case the spurious signal should not be attributed. For increasing p the confounder sensitivity should increase, i.e. the explanation should reflect the classifiers increasing reliance on the confounder."
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,,Sensitivity to Prediction Changes via Explanation NCC.,"Secondly, the explanations should not be invariant to changes in classifier prediction. That is, if the classifier's prediction for a specific image changes when adding or removing a confounder, then the explanations should also be different. We measure this property using the average normalised cross correlation (NCC) between explanations of test images when confounders were either present or absent.Again, we only evaluate on images for which the prediction changes when adding the confounder as in these cases, we know the classifier is relying on confounders, and we evaluate a maximum of 100 samples. An optimal explanation method should obtain a high NCC score if the training data contains p = 0% confounded data points, since in that case the explanation with and without the confounder should be similar. For increasing p the NCC score should decrease to reflect the classifiers increasing reliance on the confounder."
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,2.4,Evaluated Explanation Methods,We evaluated five post-hoc techniques with representative examples from the approaches mentioned in the introduction: Guided Backpropgation 
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,3.0,Results,"We first established the classifiers' performance in the presence of confounders, then compared all techniques in their ability to identify such confounders. Classification Performance. Both investigated classifiers, the ResNet50 and the inherently interpretable Attri-Net, performed similarly in terms of classification AUC (first row of Fig.  Explanations. We analysed the explanations' ability to identify confounders by reporting confounder sensitivity (CS, middle row in Fig.  Figures "
Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?,4.0,Discussion,"In this paper, we proposed an evaluation strategy to assess the ability of visual explanations to correctly identify a classifier's reliance on a spurious signal. We specifically focused on the scenario where the classifier is predicting the right thing, but for the wrong reason, which is highly significant for the safe development of ML-basd diagnosis and prediction systems. Using this strategy, we assessed the performance of five post-hoc explanation techniques and one inherently interpretable technique with three realistic confounding signals. We found that the inherently interpretable Attri-Net technique, as well as the post-hoc SHAP technique performed the best, with Attri-Net yielding the most balanced performance. Both techniques are suitable for finding false reliance on a spurious signals. We also observed that the variation in the explanations' sparsity makes them perform differently in detecting spurious signals of different sizes and   shapes. In agreement with prior work, we found that gradient based techniques performed less robustly in our experiments. From our experiments we draw two main conclusions. Firstly, practitioners looking to check for spurious correlations in a trained black-box model such as a ResNet should give preference to SHAP which provided the best performance out of the post-hoc techniques in our experiments. Secondly, an inherently interpretable technique, namely Attri-Net, performed the best in our experiments providing evidence to the supposition by Rudin et al.  A major limitation of our study is the limited number of techniques we examined. Thus a primary focus of future work will be to scale our experiments to a wider range of techniques. Future work will also focus on human-in-the-loop experiments, as we believe, this will be the ultimate assessment of the usefulness of different explanation techniques."
Efficient Subclass Segmentation in Medical Images,1.0,Introduction,"In recent years, the use of deep learning for automatic medical image segmentation has led to many successful results based on large amounts of annotated training data. However, the trend towards segmenting medical images into finergrained classes (denoted as subclasses) using deep neural networks has resulted in an increased demand for finely annotated training data  Here, the primary challenge is to effectively leverage superclass annotations to facilitate the learning of fine-grained subclasses. To solve this problem, several works have proposed approaches for recognizing new subclasses with limited subclass annotations while utilizing the abundant superclass annotations in classification tasks  However, to the best of our knowledge, there has been no work specifically exploring learning subclasses with limited subclass and full superclass annotations in semantic segmentation task. Previous label-efficient learning methods, such as semi-supervised learning  In this study, we focus on the problem of efficient subclass segmentation in medical images, whose goal is to segment subclasses under the supervision of limited subclass and sufficient superclass annotations. Unlike previous works such as  Our main approach is to utilize the hierarchical structure of categories to design network architectures and data generation methods that make it easier for the network to distinguish between subclass categories. Specifically, we propose 1) a Prior Concatenation module that concatenates predicted logits from the superclass classifier to the input feature map before subclass segmentation, serving as prior knowledge to enable the network to focus on recognizing subclass categories within the current predicted superclass; 2) a Separate Normalization module that aims to stretch the intra-class distance within the same superclass, facilitating subclass segmentation; 3) a HierarchicalMix module inspired by GuidedMix "
Efficient Subclass Segmentation in Medical Images,2.0,Method,"Problem Definition. We start by considering a set of R coarse classes, denoted by Y c = {Y 1 , ..., Y R }, such as background and brain tumor, and a set of N training images, annotated with Y c , denoted by D c = {(x l , y l )|y l i ∈ Y c } N l=1 . Each pixel i in image x l is assigned a superclass label y l i . To learn a finer segmentation model, we introduce a set of fine subclass kR }, such as background, enhancing tumor, tumor core, and whole tumor. We assume that only a small subset of n training images have pixel-wise subclass labels z ∈ Y f denoted by Our goal is to train a segmentation network f (x l ) that can accurately predict the subclass labels for each pixel in the image x l , even when n N . Without specification, we consider R = 2 (background and foreground) and extend the foreground class to multi subclass in this work. Prior Concatenation. One direct way to leverage the superclass and subclass annotations simultaneously is using two 1×1×1 convolution layers as superclass and subclass classification heads for the features extracted from the network. The superclassification and subclassification heads are individually trained by superclass P c (x l ) labels and subclass labels P f (x l ). With enough superclass labels, the feature maps corresponding to different superclasses should be well separated. However, this coerces the subclassification head to discriminate among K subclasses under the mere guidance from few subclass annotations, making it prone to overfitting. Another common method to incorporate the information from superclass annotations into the subclassification head is negative learning  To make use of superclass labels without affecting the training of the subclass classification head, we propose a simple yet effective method called Prior Concatenation (PC): as shown in Fig.  Separate Normalization. Intuitively, given sufficient superclass labels in supervised learning, the superclassification head tends to reduce feature distance among samples within the same superclass, which conflicts with the goal of increasing the distance between subclasses within the same superclass. To alleviate this issue, we aim to enhance the internal diversity of the distribution within the same superclass while preserving the discriminative features among superclasses. To achieve this, we propose Separate Normalization(SN) to separately process feature maps belonging to hierarchical foreground and background divided by superclass labels. As a superclass and the subclasses within share the same background, the original conflict between classifiers is transferred to finding the optimal transformations that separate foreground from background, enabling the network to extract class-specific features while keeping the features inside different superclasses well-separated. Our framework is shown in Fig.  HierarchicalMix. Given the scarcity of subclass labels, we intend to maximally exploit the existent subclass supervision to guide the segmentation of coarsely labeled samples. Inspired by GuidedMix  As shown in Fig.  Next, we adopt image mixup by cropping the bounding box of foreground pixels in x , resizing it to match the size of foreground in x, and linearly overlaying them by a factor of α on x. This semantically mixed image x mix has subclass labels z = resize(α • z ) from the fine-labeled image x . Then, we pass it through the network to obtain a segmentation result f (x mix ). This segmentation result is supervised by the superposition of the pseudo label map z pse and subclass labels z, with weighting factor α: The intuition behind this framework is to simultaneously leverage the information from both unlabeled and labeled data by incorporating a more robust supervision from transform-invariant pseudo labels. While mixing up only the semantic foreground provides a way of exchanging knowledge between similar foreground objects while lifting the confirmation bias in pseudo labeling "
Efficient Subclass Segmentation in Medical Images,3.0,Experiments,"Dataset and Preprocessing. We conduct all experiments on two public datasets. The first one is the ACDC Implementation Details and Evaluation Metrics. To augment the data during training, we randomly cropped the images with a patch size of 256 × 256 for the ACDC dataset and 96 × 96 × 96 for the BraTS2021 dataset. The model loss L is set by adding the losses from Cross Entropy Loss and Dice Loss. The weighing factor α in HierarchicalMix section is chosen to be 0.5, while τ linearly decreases from 1 to 0.4 during the training process. We trained the model for 40,000 iterations using SGD optimizer with a 0.9 momentum and a linearly decreasing learning rate that starts at 0.01 and ends with 0. We used a batch size of 24 for the ACDC dataset and 4 for the BraTS2021 dataset, where half of the samples are labeled with subclasses and the other half only labeled with superclasses. More details can be found in the supplementary materials. To evaluate the segmentation performance, we used two widely-used metrics: the Dice coefficient (DSC) and 95% Hausdorff Distance (HD 95 ). The confidence factor τ mentioned in HierarchicalMix starts at 1 and linearly decays to 0.4 throughout the training process, along with a weighting factor α sampled according to the uniform distribution on [0.5, 1]. Performance Comparison with Other Methods. To evaluate the effectiveness of our proposed method, we firstly trained two U-Net models  Table "
Efficient Subclass Segmentation in Medical Images,4.0,Conclusion,"In this work, we proposed an innovative approach to address the problem of efficient subclass segmentation in medical images, where limited subclass annotations and sufficient superclass annotations are available. To the best of our knowledge, this is the first work specifically focusing on this problem. Our approach leverages the hierarchical structure of categories to design network architectures and data generation methods that enable the network to distinguish between subclass categories more easily. Specifically, we introduced a Prior Concatenation module that enhances confidence in subclass segmentation by concatenating predicted logits from the superclass classifier, a Separate Normalization module that stretches the intra-class distance within the same superclass to facilitate subclass segmentation, and a HierarchicalMix model that generates high-quality pseudo labels for unlabeled samples by fusing only similar superclass regions from labeled and unlabeled images. Our experiments on the ACDC and BraTS2021 datasets demonstrated that our proposed approach outperformed other compared methods in improving the segmentation accuracy. Overall, our proposed method provides a promising solution for efficient fine-grained subclass segmentation in medical images."
Efficient Subclass Segmentation in Medical Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_25.
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,1.0,Introduction,"Federated learning (FL) has emerged as a promising methodology for harnessing the power of private medical data without necessitating centralized data governance  In this regard, we study a practical problem of enhancing personalization and generalization jointly in cross-silo FL for medical image classification when faced data heterogeneity. To this end, we aim to address the following two questions in FL: What could be the causes that result in local and global trade-off ? and How to achieve better local and global trade-off ? First, we provide a new angle to understand the trade-off. We reveal that over-personalization in FL can cause overfitting on local data and trap the model into a sharp valley of loss landscape (highly sensitive to parameter perturbation, see detailed definition in Sec. 2.2), thus limiting its generalizability. An effective strategy for avoiding sharp valleys in the loss landscape is to enforce models to obtain flat minima. In the centralized domain, weight interpolation has been explored as a means of seeking flat minima as its solution is moved closer to the centroid of the high-performing models, which corresponds to a flatter minimum  With the above basis, we propose to track both local and global models during the federated training and perform model interpolation to seek the optimal balance. Our insight is drawn from the model soup method  In this paper, we propose a novel federated model soup method (FedSoup) to produce an ensembled model from local and global models that achieve better local-global trade-off. We refer the 'soup' as a combo of different federated models. Our proposed FedSoup includes two key modules. The first one is temporal model selection, which aims to select suitable models to be combined into one. The second module is Federated model patching "
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,2.1,Problem Setup,"Consider a cross-silo FL setting with N clients. Let D := {D i } N i=1 be a set of N training domain, each of which is a distribution over the input space X . For each client, we have access to n training data points in the form of (x i j , y i j ) n j=1 ∼ D i , where y i j denotes the target label for input x i j . We also define a set of unseen target domains T := {T i } N i in a similar manner, where N is the number of target domains and is typically set to one. The goal of personalization (local performance) is to find a model f ( "
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,2.2,Generalization and Flat Minima,"In practice, ERM in deep neural networks, i.e., arg min θ E D (θ), can yield multiple solutions that offer comparable training loss, but vastly different levels of generalizability  One common reason for failures in ERM is the presence of variations in the data distribution (D i = D), which can cause a shift in the loss landscape. As illustrated in Fig.  From the domain generalization formalization in  where the sup A |P Di (A) -P T (A)| is a divergence between domain D i and T , A is the set of measurable subsets under D i and T , and ξ is the confidence bound term related to the radius and the number of the training samples. From the Equation (1), we can infer that minimizing sharpness and seeking flat minima is directly related with the generalization performance on the unseen target domain."
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,2.3,Our Solution: FedSoup,"After analyzing the aforementioned relationship between sharpness and generalization, we expound on the distinctive challenges of seeking flat minima and mitigating the trade-off between local and global performance in FL. Consequently, we introduce two refined modules as the ingredient of our proposed FedSoup solution. FedSoup only needs to modify the training method of the client, and the algorithm implementation is shown in Algorithm 1. Temporal Model Selection. Stochastic Weight Averaging (SWA)  where θ g is global model, θ l is local model, k is the number of selected global models. This update rule corresponds to Algorithm 1 Line 9. Algorithm 1. FedSoup It is important to note that our proposed FedSoup algorithm requires only one carefully tuned hyper-parameter, namely the interpolation start epoch. To mitigate the risk of having empty global model soups when the start epoch is too late and to prevent potential performance degradation when the start epoch is too early, we have set the default interpolation start epoch to be 75% of the total training epochs, aligning with the default setting of SWA. Furthermore, it is worth mentioning that the modified model soup and model patching modules in our proposed FedSoup framework are interdependent. Model patching, which is a technique based on our modified model soup algorithm, provides an abundance of models to explore flatter minima and enhance performance."
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,3.1,Experimental Setup,"Dataset. We validate the effectiveness of our proposed method, FedSoup, on two medical image classification tasks. The first task involved the classification of pathology images from five different sources using Camelyon17 dataset  To assess the generalization ability and personalization of our model, we have constructed both local and global testing sets. Following "
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,,Models and Training,"Hyper-Paramters. We employ the ResNet-18 architecture as the backbone model. Our approach initiates local-global interpolation at the 75% training phase, consistent with the default hyper-parameter setting of SWA. We utilize the Adam optimizer with learning rate of 1e-3, momentum coefficients of 0.9 and 0.99 and set the batch size to 16. We set the local training epoch to 1 and perform a total of 1, 000 communication rounds."
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,3.2,Comparison with State-of-the-Art Methods,We compare our method with seven common FL and state-of-the-art PFL methods. Results in Table  Trade-off at Different Personalized Levels. Following the evaluation in 
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,3.3,Unseen Domain Generalization,"We show the additional benefits of FedSoup on unseen domain generalization. Setup. To evaluate the generalization of our method beyond the participating domains, we utilize one domain that did not take part in the distributed training and used its data as the evaluation set for unseen domain generalization. To this end, we perform leave-one-out cross-validation by having one client as the tobe-evaluated unseen set each time. To ensure a reliable results of unseen domain generalization, we conducted experiments on the Camelyon17 dataset, which has a larger number of samples. Results. Overall, our proposed method demonstrates an advantage in terms of unseen domain generalization capabilities (see Fig. "
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,4.0,Conclusion,"In this paper, we demonstrate the trade-off between personalization and generalization in the current FL methods for medical image classification. To optimize this trade-off and achieve flat minima, we propose the novel FedSoup method. By maintaining personalized global model pools in each client and interpolating weights between local and global models, our proposed method enhances both generalization and personalization. FedSoup outperforms other PFL methods in terms of both generalization and personalization, without incurring any additional inference or memory costs."
FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 30.
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,1.0,Introduction,"Predictive modeling of the individual-level cognitive development during infancy is of great importance in advancing our understanding of the subject-specific relationship between the cognitive ability and early brain structural and functional development and their underlying neural mechanisms. It is also critical for early identifying cognitive delays and developing more effectively and timely personalized therapeutic interventions for at-risk infants. However, this is a very challenging task due to the complex and rapid development of brain structure, function and cognition during the first years of life  Recently, a few methods have been explored for predicting infant cognition using either resting-state functional MRI (rs-fMRI)  To address the above issues, we propose a novel cortical surface-based multimodal learning framework (CSML), to enable learning of the fine-grained spatial patterns and complementary information from structural and functional MRI data for precise prediction of the individual-level cognitive development. Specifically, 1) to learn detailed spatial patterns of both functional connectivity and structural information, we propose to leverage the strong feature learning and representation ability of spherical surface networks "
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,2.0,Method,"In this section, we present the details of CSML (Fig. "
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,2.1,Surface-Based Fine-Grained Information Representation,"The input of the network framework consists of two branches for encoding cortical structural information and functional connectivity information, respectively. To integrate multi-modal MRI data together for cognition development prediction, we map all modality data to a common space, i.e., the cortical surface registered to UNC 4D infant surface atlas "
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,2.2,Modality-Specific Encoder,"For the multi-modality input, we employ two modality-specific encoders E s and E f to describe its feature representation, respectively. To be specific, we regard each modality comprised of multiple feature channels, while each channel could be interpreted as an observation of the data from a certain view. Therefore, we process each view separately as I is the number of morphological features we used; j [1, J ], J is the number of parcels we used in building FC maps. We implement E s and E f as the Spherical Res-Net  ∼ where z s is the aggregated representation for the structural data, Q s (•), K s (•), U s (•), and W s (•) are four multi-layer perceptrons (MLP), const is a constant for normalization. Similarly, we can obtain the functional-related variable z f by feeding x f and the functional view-related representations {v"
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,2.3,Modality-Fusion Block,"To better learn the complementary information between the two modalities, we further decompose the modality-specific latent variables z s and z f into two parts: Com(z n ) and Spe(z n ), where n {s, f }, standing for the structure (s) and function (f ) related variables, respectively. Com(z n ) is the common code representing the shared information among modalities, while Spe(z n ) is the specific code representing the complementary information that differentiates one modality from the other. The basic requirements of this disentanglement are: (1) The concatenation of Com(z n ) and Spe(z n ) equals z n ; (2) Com(z s ) and Com(z f ) should be as similar as possible; (3) Spe(z s ) differs from Spe(z f ) as much as possible. Accordingly, L 1 Disen is defined as: Since the latent variable of each modality has been disentangled into Com(z n ) and Spe(z n ), the combined information is formed as the concatenation of the common code and specific codes as follows: z s,f = Spe(z s ), Common, Spe(z f ) , where Common = 0.5(Com(z s ) + Com(z f ))."
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,2.4,Cognitive Scores Prediction,"Given the combined multimodal information z s,f , it is intuitive to regress the cognitive scores directly. However, considering that cognitive functions develop rapidly during the first years of life  (2) Age(m s,f ) is capable of age estimation through an age predictor P a ; (3) Ind (m s,f ) is incapable of age estimation through P a . Accordingly, L 2  Disen is defined as: where t is the ground truth of age. Then, we can use the identity-related features Ind (m s,f ) containing subject-specific structure-function profile to predict the cognitive scores through a cognitive score predictor P c under the guidance of the corresponding age feature Age(m s,f ). The loss function to train P c is defined as: where y is the ground truth of cognitive scores. Specifically, we implement P a and P c as two sets of MLP. Finally, the overall objective function to optimize the neural network is written as: where λ 1 and λ 2 are trade-off parameters to balance the multiple losses."
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,3.1,Dataset,"We verified the effectiveness of the proposed CSML model for infant cognition development prediction on a public high-resolution dataset including 318 pairs of sMRI and rs-fMRI scans acquired at different ages ranging from 88 to 1040 days in the UNC/UMN Baby Connectome Project  To quantify the cognition development level of each participant, four Mullen cognitive scores "
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,3.2,Experimental Settings,"In order to validate our methods, a 5-fold cross-validation strategy is employed, and each fold consists of 190 training samples, 64 validating samples, and 64 testing samples. To quantitatively evaluate the performance, the Pearson's correlation coefficient (PCC) and root mean square error (RMSE) between the ground truth and predicted values were calculated. In the testing phase, the mean and standard deviation of the 5-fold results were reported. The encoders E s and E f in CSML constitutes 5 Res-blocks with the dimensions of {32, 32, 64, 64, 128}, respectively. The modality fusion block F, age predictor P a , and cognitive score predictor P c were designed as two-layer MLP with the ReLU activation function and the dimension of {192, 128}, {64, 1}, and {128, 1}, respectively. We implemented the model with PyTorch and used Adam as optimizer with the weight decay of 10 -4 and the learning rate cyclically tuned within [10 -6 , 10 -3 ]. The batch size was set to 1. The maximum training epoch is 500. After comparison, we empirically set the hyperparameters as λ 1 =0.05 and λ 2 =0.01."
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,3.3,Results,"We first show the results of some ablated models of our method in Table  We also comprehensively compared with various traditional and state-of-the-art functional connectivity-based methods, including KNN, random forest (RF), SVR, gaussian process regression (GPR), GCN "
Prediction of Infant Cognitive Development with Cortical Surface-Based Multimodal Learning,4.0,Conclusion,"In this work, we develop an innovative cortical surface-based multimodal learning framework (CSML) to address the infant cognition prediction problem. Specifically, we unprecedentedly propose to explicitly leverage the surface-based feature representations to preserve the fine-grained, spatially detailed multimodal information for cognition prediction. In addition, by disentangling the modality-shared and complementary information, our model successfully captures the individualized cognition development patterns underlying the dramatic brain development. With its superior performance compared to state-of-the-art methods, our proposed CSML suggests that the informative clues for brain-cognitive relationship are hidden in the multimodal fine-grained details and validates itself as a potentially powerful framework for simultaneously learning effective representations from sMRI and rs-fMRI data."
Category-Independent Visual Explanation for Medical Deep Network Understanding,1.0,Introduction,"Medical application is a field that has high requirements of model reliability, trustworthiness, and interpretability. According to the act proposed by the European Commission on AI system regulation  The state-of-art algorithms mostly focus on providing visualization during training where the categorical label is available. It becomes problematic at product deployment stage when no label is available. Without supplying the ground truth categorical labels, the false categorical labels would mislead the visualization algorithm to highlight wrong regions for cues. A sample is shown in Fig. "
Category-Independent Visual Explanation for Medical Deep Network Understanding,2.0,Related Works,"The visual explanation for deep networks is an essential task for researchers to interpret and debug deep networks where an ROI heat map is one of the most popular tools. This field is pioneered by Oquab et al.  Medical applications have high requirements for model reliability, trustworthiness, and interpretability. The visualization tools such as GradCAM and Fig.  GradCAM++ are widely applied to medical applications such as retina imaging "
Category-Independent Visual Explanation for Medical Deep Network Understanding,3.0,Method,"Our algorithm generates an ROI heatmap to indicate the region on the image that the deep learning algorithms focus on when making classification decisions. Our method does not require any modification or additional training on target deep networks. The overview flow of our algorithm is illustrated in Fig.  It is well known that the Hessian matrix appears in the expansion of gradient about a point in parameter space  where ω is a point in parameter space, Δω is a perturbation of ω, Δω is the gradient and H is the hessian matrix. In order to approximate the Hessian matrix H, we let Δω = rv, where v is the identity matrix, and r is a small number which leads the O(r) term to become insignificant. So we can further simplify the equation into: Our goal is to apply the Hessian matrix as a weighting function to indicate the significance of each activation function output in the CNN. So we applied an L2 normalization on the Hv, here v is an identity matrix, so we can get the normalized Hessian matrix Ĥ = |Hv| Hv 2 . In the CNN we denote A k as the feature activation map from the kth convolution layer. Ĥk denotes the normalized Hessian matrix in the kth layer. We calculate the Hadamard product between Ĥk and A k , then apply ReLU to obtain the new activation map. n is the depth of the activation map. The ROI heatmap L H = n k=1 ReLu( Ĥk A k ). The ROI heatmap L H can be noisy, we follow Muhammad's approach  One drawback of Muhammad's approach "
Category-Independent Visual Explanation for Medical Deep Network Understanding,4.1,Experiment Setup,"We conduct experiments on lung disease classification Chestx-ray8  x-ray images with 19 disease labels. It is a significantly imbalanced dataset with some categories having as few as 7 images. To demonstrate our visualization techniques, we simplified the dataset by selecting 6 diseases with a higher number of images. After the selection, our training set contains images from atelectasis (3135 images), effusion (2875 images), infiltration (6941 images), mass (1665 images), nodule (2036 images), and pneumothorax (1485 images). Additionally, we randomly selected 7000 images from healthy people. 20% of images in the training set were set aside as validation sets for parameter tuning. This dataset contains 881 test images with bounding boxes that indicate the location of the diseases which 644 images were in the 6 diseases we selected. We utilize the pre-trained ResNet50 "
Category-Independent Visual Explanation for Medical Deep Network Understanding,4.2,Quantitative Evaluation,"The algorithm is evaluated following the method proposed by Cao et al.  To simulate the deployment scenario, we conduct two sets of evaluations. In the first evaluation, the prediction results (our ResNet model delivers 42.6% prediction accuracy) from the deep learning model are used as a label feed into the visualization methods. One drawback of this approach is the prediction result is not always reliable and the incorrect prediction could mislead the algorithm to output the wrong ROI. As a comparison, in the second evaluation, we supply ground truth labels to visualization methods. The quantitative evaluation of different visualization methods is shown in Table  Three groups of visualization algorithms are evaluated in our experiment. The gradient based group contains the algorithm that relies on the gradient from the label to generate the ROI. In this group, GradCAM achieved the highest at 0.175 IoU at the 75% threshold. The pertubation based group makes small perturbations in the input image or activation weight to find the ROI that has the highest impact. In this group, the ScoreCAM achieved the highest 0.168 IoU at the 75% threshold. The category independent group contains algorithms that do not require a label to generate ROI. Our method scored the highest IoU at 0.271 IoU at the 75% threshold. When ground truth labels are supplied, the IoU for gradient based methods is improved in the range of 5% to 20%. For perturbation based methods, supplying ground truth data reduced the performance of AblationCAM and had minimal impact on RISE. Next, we split the test set into two categories which are samples with wrong and correct predictions (shown in Fig.  To further investigate the efficiency of our algorithm, we extract the IoU on each disease (Fig. "
Category-Independent Visual Explanation for Medical Deep Network Understanding,4.3,Qualitative Evaluation,Sample images comparing our algorithm with five state-of-art algorithms are shown in Fig. 
Category-Independent Visual Explanation for Medical Deep Network Understanding,4.4,Clinical Application,Our algorithm has the potential to apply to many clinical applications. We conducted an additional experiment on the glaucoma retinal image database 
Category-Independent Visual Explanation for Medical Deep Network Understanding,5.0,Conclusion,"In this study, we propose a novel category-independent deep learning visualization algorithm that does not rely on categorical labels to generate visualizations. Our evaluation demonstrates that our algorithm outperforms seven state-of-theart algorithms by a significant margin on a multi-disease classification task using X-ray images. This indicates that our algorithm has the potential to enhance model explainability and facilitate its deployment in medical applications. Additionally, we demonstrate the flexibility of our algorithm by showing a clinical use case on retinal image glaucoma detection. Overall, our proposed Hessian-CIAM algorithm represents a promising tool for improving our understanding of deep learning models and enhancing their interpretability, particularly in medical applications."
Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos,1.0,Introduction,"Carotid stenosis grading (CSG) represents the severity of carotid atherosclerosis, which is highly related to stroke risk  Achieving accurate automatic CSG with US videos is challenging. First, the plaque clips often have extremely high intra-class variation due to changeable plaque echo intensity, shapes, sizes, and positions (Fig.  A typical approach for this video classification task is CNN-LSTM  There are several types of 3D networks that have been widely used in visual tasks: (1) Pure 3D convolution neural networks (3D CNNs) refer to capturing local ST features using convolution operations  Recently, Wang et al.  In this study, we present the first video classification framework based on 3D Convolution-Transformer design for CSG (named CSG-3DCT). Our contribution is three-fold. First, we propose a novel and effective video classification network for weakly-supervised CSG, which can avoid the need of laborious and unreliable mask annotation. Second, we adopt an inflation strategy to ease the model training, where pre-trained 2D convolution weights can be adapted into the 3D counterpart. In this case, our network can implicitly gain the pre-trained weights of existing large models to achieve an effective warm start. Third, we propose a novel play-and-plug attention-guided multi-dimension fusion (AMDF) transformer encoder to integrate global dependencies within and across ST dimensions. Two lightweight cross-dimensional attention mechanisms are devised in AMDF to model ST interactions, which merely use class (CLS) token "
Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos,2.0,Methodology,"Figure  3D Mix-Architecture for Video Classification. CNN and Transformer have been validated that they specialize in extracting local and global features, respectively. Besides, compared to the traditional 2D video classifiers, 3D systems have shown the potential to improve classification accuracy due to their powerful capacity of encoding multi-dimensional information. Thus, in CSG-3DCT, we propose to leverage the advantages of both CNN and Transformer and extend the whole framework to a 3D version. The meta-architecture of our proposed CSG-3DCT follows the 2D Convolution-Transformer (Conformer) model  Inflation Strategy for 3D CNN Encoder. We devise an inflation strategy for the 3D CNN encoder to relieve the model training and enhance the representation ability. For achieving 2D-to-3D inflation, a feasible scheme is to expand all the 2D convolution kernels at temporal dimension with t>1  Transformer Encoder with Play-and-plug AMDF Design. Simply translating the 2D transformer encoder into the 3D standard version mainly has two limitations: (1) It blindly compares the similarity of all ST tokens by selfattention, which tends to inaccurate predictions. Moreover, such video-based computation handles t× tokens simultaneously compared to image-based methods, leading to much computational cost. (  Before the transformer encoder, we first decompose the feature maps X produced by the stem module into t × n 2 embeddings without overlap. A CLS token X cls ∈ R d is then added in the start position of X to obtain merged embeddings Z ∈ R d×(t×n 2 +1) . n 2 and d denote the number of spatial patch tokens and hidden dimensions, respectively. Then, the multiple AMDF Trans blocks in the transformer encoder drive Z to produce multi-dimensional enhanced representations. Specifically, the AMDF block has the following main components. 1) Intra-dimension ST Learning Module. Different from the cascade structure in  2) Inter-dimension ST Fusion Module. To boost interactions between S and T dimensions, we build the inter-dimension fusion module after the intradimension learning module. The only difference between the two types of modules is the calculation mode of attention. As shown in Fig.  To improve computing efficiency in CA, Chen et al.  3) Learnable Mechanism for Adaptive Updating. Multi-dimensional features commonly have distinct degrees of contribution for prediction. For example, supposing the size of carotid plaque does not vary significantly in a dynamic segment, the spatial information may play a dominant role in making the final diagnosis. Thus, we introduce a learnable parameter to make the network adaptively adjust the weights of different branches and learn the more important features (see Fig. "
Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos,3.0,Experimental Results,"Dataset and Implementations. We validated the CSG-3DCT on a large inhouse carotid transverse US video dataset. Approved by the local IRB, a total Table "
Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos,,Methods,"Accuracy F1-score Precision Recall I3D  In this study, we implemented CSG-3DCT in Pytorch, using an NVIDIA A40 GPU. Unless specified, we trained our model using 8-frame input plaque clips. All frames were resized to 256 × 256. The learnable weights of QKV projection and LayerNorm weights in spatial dimension branch of intra-dimension ST learning module were initialized with those from transformer branch in Conformer  Figure  Ablation Study. We performed ablation experiments in the last 6 rows of Table  1) Effects of Different Key Components of Our Model Design. We compared CSG-3DCT with three variants (i.e., -Base, -SWA * , and -CA * ) to analyze the effects of different key components. Compared with -Base, each of our proposed modules and their combination can help improve the accuracy. We adopt CA in our final model for its good performance. 2) Effects of Plaque Clip Length. We only investigated the effects of our model on 8-frame and 16-frame input clips due to limited GPU memory. We can find in Table  3) Effectiveness of Initialization with ImageNet. We evaluated the value of training models starting from ImageNet-pretrained weights compared with scratch. It can be seen in Table "
Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos,4.0,Conclusion,"We propose a novel and effective video classification network for automatic weakly-supervised CSG. To the best of our knowledge, this is the first work to tackle this task. By adopting an inflation strategy, our network can achieve effective warm start and make more accurate predictions. Moreover, we develop a novel AMDF Transformer encoder to enhance the feature discrimination of the video with reduced computational complexity. Experiments on our large inhouse dataset demonstrate the superiority of our method. In the future, we will explore to validate the generalization capability of CSG-3DCT on more large datasets and extend two-grade classification to four-grade of carotid stenosis."
Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_48.
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,1.0,Introduction,"Despite the success of artificial intelligence (AI) in aiding diagnosis, its application to medical education remains limited. Trainee physicians require several years of experience with a diverse range of clinical cases to develop sufficient skills and expertise. However, designing educational materials solely based on real-world data poses several challenges. For example, although small but significant disease characteristics (e.g., depth of cancer invasion) can sometimes alter diagnosis and treatment, collecting pairs with and without these characteristics is cumbersome. Another major challenge is longitudinal tracking of pathological progression over time (e.g., from the early stage of cancer to the advanced stage), which is difficult to understand because medical images are often snapshots. Privacy is also a concern since images of educational materials are widely distributed. Therefore, medical image editing that allows users to generate their intended disease characteristics is useful for precise medical education  Image editing can synthesize low-or high-level image contents  Several types of image editing techniques for medical imaging have been introduced, mainly using generative adversarial networks [5] and, more recently, diffusion models  Here, we propose a novel framework for image editing called U3-Net that allows the generation of anatomical elements with precise conditions. The core technique is self-supervised segmentation, which aims to achieve pixel-wise clustering without manually annotated labels "
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,,Contributions: Our contributions are as follows:,"-We propose a novel image-editing algorithm, U3-Net, to synthesize images for medical education via self-supervised segmentation. -U3-Net can faithfully synthesize intended anatomical elements according to the editing operation on the segmentation labels. -Evaluation by five expert physicians showed that the edited images were natural as medical images with the intended features."
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,2.0,Methodology,"U3-Net consists of three neural networks: encoder, decoder, and discriminator (see Fig. "
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,2.1,First Training Stage for Self-supervised Segmentation,"The training process for U3-Net is two-stage. First, we train the encoder and decoder (excluding the discriminator) to conduct K-class self-supervised segmentation. To achieve pixel-wise clustering that is consistent between two transformed views of the input images, we introduce four constraints: intra-cluster pull force, inter-cluster push force, cross-view consistency, and reconstruction loss."
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,,Random Image Transformation:,"We consider a sequence of image transformations [t 1 , . . . , t n ] specified by the type (e.g., image rotation) and magnitude (e.g., degree of rotation) of each transformation:  Cluster Assignment and Update: In the CL module, K-means clustering in the first iteration initializes K mean vectors µ k ∈ R D . Then, the embedding vector of the i-th pixel e i∈{1,...,H×W } ∈ R D in the embedding maps, E 1 and E 2 , is assigned to the cluster with the nearest mean vector as follows: , where y i is the cluster index of the i-th pixel. By replacing embedding vectors with their respective mean vectors, quantized embedding maps, E q1 and E q2 , are generated g(E) = E q = [µ y1 , . . . , µ yH×W ] ∈ R D×H×W . The cluster indices form the segmentation maps S = [y 1 , . . . , y H×W ] ∈ R H×W , S 1 and S 2 . The mean vectors µ k are updated by using the exponential moving average [9]. Intra-cluster Pull Force: For transformation-invariant pixel-wise clustering, we define four loss terms. The first term, cluster loss, forces the embedding vectors to adhere to the associated mean vector (see Fig.  Inter-cluster Push Force: The second term, distance loss, pushes the distance between the mean vectors above a margin parameter m (see Fig.  where k A and k B indicate two different cluster indices."
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,,Cross-view Consistency:,"The segmentation maps from the different views, S 1 and S 2 , should overlap after re-transforming to align the coordinates. Such a re-transform is composed of inverse and forward geometric transformations: The inverse transformations of the photometric transformations are not considered. Using the re-transformed segmentation maps, we impose a third term, cross-view consistency loss, which forces the embedding vectors of one view to match the mean vector of the other (see Fig.  Reconstruction Loss: Without user editing, the decoder reconstructs the input images from quantized embedding maps h(E q ) = R ∈ R C×H×W . We thus employ reconstruction loss, which minimizes the mean squared error between the reconstructed and input images. Learning Objective: The weighted sum of the loss functions is set to be minimized:"
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,2.2,Second Training Stage for Faithful Image Synthesis,"In the second stage, we train the decoder and discriminator (excluding the encoder) to produce naturally appearing images from the quantized embedding maps. Learning Objective: We impose generator loss L gen for the decoder to produce more faithful images by deceiving the discriminator, and discriminator loss L dis for the discriminator to judge the real or fake of the images as the perpixel feedback "
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,2.3,Inference Stage for Medical Image Editing,"After training, the encoder can output a segmentation map from a testing image. As shown in Fig. "
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,4.0,Conclusion,"In this study, we propose a medical image-editing framework to edit fine-grained anatomical elements. The self-supervised segmentation extracted low-to midlevel content of medical images, which corresponded well to the clinically meaningful substructures of organs and diseases. The majority of the edited images with intended characteristics were perceived as natural medical images by several expert physicians. Our medical image editing method can be applied to medical education, which has been overlooked as an application of AI. Future challenges include improving scalability with fewer manual operations, validating segmentation maps from a more objective perspective, and comparing our proposed algorithm with existing methods, such as those based on superpixels  Data use declaration and acknowledgment: The pelvic MRI and chest CT datasets were collected from the National Cancer Center Hospital. The study, data use, and data protection procedures were approved by the Ethics Committee of the National Cancer Center, Tokyo, Japan (protocol number 2016-496). Our implementation and all synthesized images will be available here: https:// github.com/Kaz-K/medical-image-editing."
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_38.
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,3.0,Experiments and Results,"Implementation and Datasets: All neural networks were implemented in Python 3.8 using the PyTorch library 1.10.0 [12] on an NVIDIA Tesla A100 GPU running CUDA 10.2. The encoder, decoder, and discriminator were implemented based on U-Net  Self-supervised Medical Image Segmentation: We began by optimizing the hyperparameters to achieve self-supervised segmentation. Appropriate transformations were selected from six candidate functions: t 1 , Random HorizontalFlip, t 2 , RandomAffine, t 3 , ColorJitter, t 4 , RandomGaussianBlur, t 5 , RandomPosterize, t 6 , RandomGaussianNoise. Because anatomical elements, including the substructures of organs and diseases, are too detailed for human annotators to segment, it was difficult to create ground-truth labels. Therefore, the training configuration was selected based on the consensus of two expert radiologists with domain knowledge. By comparing different settings on the pelvic MRI training dataset (see Supplementary Information), the number of segmentation classes of 10, the combination of t 1 , t 2 , and t 3 with moderate magnitude, the weakly imposed reconstruction loss, and a certain value of the margin parameter were considered suitable for self-supervised segmentation. In particular, we found that reconstruction loss is essential for obtaining segmentation maps corresponding to anatomical elements, although such a loss term was not included in previous studies "
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,,Evaluation of the Synthesized Images:,"We measured the quality of image reconstruction using mean square error (MSE), structural similarity (SSIM), and peak signal-to-noise ratio (PSNR). The mean ± standard deviations of MSE,"
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,1.0,Introduction,"Multi-organ segmentation is a crucial step in medical image analysis that enables physicians to perform diagnosis, prognosis, and treatment planning. However, manual segmentation of large volume computed tomography (CT) and magnetic resonance (MR) images is time-consuming and prone to high inter-rater variability  In computer vision, current self-supervised learning methods can be broadly divided into discriminative modeling and generative modeling. In earlier times, discriminative self-supervised pretext tasks are designed as rotation prediction  Our proposed framework leverages Felzenszwalb's algorithm  -We conduct extensive experiments on three multi-organ segmentation benchmarks and demonstrate that our method consistently outperforms current supervised and unsupervised pre-training approaches."
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,2.0,Methodology,Figure 
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,2.1,Pre-training Stage,"In the pre-training stage, for each batch an image x q is randomly chosen from B images as a query sample, and the rest of the images x n ∈ {x 1 , x 2 , ..., x B } are considered as negative key samples, where n = q. To formulate a positive key sample x p , elastic transforms are performed on the query sample x q . Global Contrast. To explore global contextual information, we train a latent encoder E g following the contrastive protocol in  , where τ g is the global temperature hyper-parameter per "
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,,Local Region,"Contrast. Unlike global contrast, positive and negative pairs for local contrast are only generated from input image x q and its transform x p . We differentiate local regions and formulate the positive and negative pairs by using Felzenszwalb's algorithm. For an input image x, Felzenszwalb's algorithm provides K local regions R = {r 1 , r 2 , .., r K }, where r k is the k-th local region cluster for image x. We then perform elastic transform for both the query image x q and its local regions R q so that we have the augmented image x p = T e (x q ) and its local regions R p = {r 1 p , r 2 p , .., r Kp p }, where r k p = T e (r k q ). Note that K q = K p always holds since R p is a one-to-one mapping from R q . Following the widely used U-Net  , where f k,n q is the n-th vector sampled from feature map f q within the k-th local region r k q . Our sampling strategy is straightforward: we sample random points with replacement following a uniform distribution. We simply refer to this as ""random sampling"". Similarly, for feature map f p , its sample mean f k p can be provided following the same random sampling process. Each local region pair of f k q and f k p is considered a positive pair. For the negative pairs, we sample both f q and f p from the rest of the local regions {r 1 , r 2 , ..., r k-1 , r k+1 , ..., r K }. The local contrastive loss can be defined as follows: where τ l is the local temperature hyper-parameter. Compared to the global contrast branch, in local contrastive learning, we pre-train both E l and D l ."
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,2.2,Fine-Tuning Stage,"In the former pre-training stages, E g , E l , and D l are pre-trained with global and local contrast strategy accordingly, with a large number of unlabelled images. In the fine-tuning stage, we fine-tune the model with a limited number of labelled images x f ∈ {x 1 , x 2 , ..., x F }, where F is the size of the fine-tuning dataset. Besides the two pre-trained encoders and one decoder, a randomly initialized decoder D g is appended to the pre-trained E g to ensure that the embeddings have the same dimensions prior to concatenation. We combine local and global contrast models by concatenating the output of D g and D l 's last convolutional layer, and fine-tune on the target dataset in an end-to-end fashion. Different levels of feature maps from encoders are concatenated with corresponding layers of decoders through skip connections to provide alternative paths for the gradient. Dice loss is applied as in usual multi-organ segmentation tasks."
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.1,Pre-training Dataset,"During both global and local pre-training stages, we pre-train the encoders on the Abdomen-1K  Although segmentation masks for liver, kidney, spleen, and pancreas are provided in this dataset, we ignore these labels during pre-training since we are following the self-supervised protocol."
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.2,Fine-Tuning Datasets,"During the fine-tuning stage, we perform extensive experiments on three datasets with respect to different regions of the human body. ABD-110 is an abdomen dataset from  Thorax-85 is a thorax dataset from  HaN is from "
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.3,Implementation Details,"All images are re-sampled to have spacing of 2.5 mm × 1.0 mm × 1.0 mm, with respect to the depth, height, and width of the 3D volume. In the pre-training stage, we apply elastic transform to formulate positive samples. In the global contrast branch, we use the SGD optimizer to pre-train a ResNet-50 "
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.4,Quantitative Results,In Table 
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.5,Qualitative Results,In Fig. 
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.7,Ablation Study,"Effect of Additional Parameters. Additional parameters do bring performance enhancement in machine learning. However, in Number of Samples N . In Table "
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,4.0,Conclusion,"In this paper, we propose a contrastive learning framework, which integrates a novel localized contrastive sampling loss and enables the learning of finegrained representations that are crucial for accurate segmentation of complex structures. Through extensive experiments on three multi-organ segmentation datasets, we demonstrated that our approach consistently boosts current supervised and unsupervised pre-training methods. LRC provides a promising direction for improving the accuracy of medical image segmentation, which is a crucial step in various clinical applications. Overall, we believe that our approach can significantly benefit the medical image analysis community and pave the way for future developments in self-supervised learning for medical applications."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,1.0,Introduction,"Parkinson's Disease (PD) is an age-related neurodegenerative disease with complex symptomology that significantly impacts the quality of life, with nearly 90,000 people diagnosed each year in North America  Graph Neural Networks (GNNs) have been highly successful in inferring neural activity patterns in resting-state fMRI (rs-fMRI)  Addressing the problem of identifying brain functional network alterations related to the severity of gait impairments presents several challenges: (i) clinical datasets are often sparse or highly imbalanced, especially for severely impaired disease states; (ii) although substantial progress has been made in modeling functional connectomes using graph theory, few studies exist that capture the individual variability in disease progression and they often fall short of generating clinically relevant explanations that are symptom-specific. In this work, we propose a novel, explainable, geometric weighted-graph attention network (xGW-GAT) that embeds functional connectomes in a learnable, graph structure that encodes discriminative edge attributes used for attention-based, transductive classification tasks. We train the model to predict a gait impairment rating score (MDS-UPDRS Part 3.10) for each PD participant. To mitigate limited clinical data across all different classes of gait impairment and data imbalance (challenge i), we propose a stratified, learning-based sample selection method that leverages non-Euclidean, centrality features of connectomes to sub-select training samples with the highest predictive power. To provide clinical interpretability (challenge ii), xGW-GAT innovatively produces individual and global attention-based, explanation masks per gait category and soft assigns nodes to functional, resting-state brain networks. We apply the proposed framework on our dataset of 35 clinical participants and compare it with existing methods. We observe significant improvements in classification accuracy while enabling adequate clinical interpretability. In summary, our contributions are: "
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,,Problem definition.,"Assume a set of functional connectomes, G n ∈ R d×d , G 2 , . . . , G N are given, where N is the number of samples and d is the number of ROIs. Each connectome is represented by a weighted, undirected graph G = (V, E, W), where V = {v i } d i=1 is the set of nodes, E ⊆ V × V is the edge set, and W ∈ R |V|×|V| denotes the matrix of edge weights. The weight w ij of an edge e ij ∈ E represents the strength of the functional connection between nodes v i and v j , i.e., the Pearson correlation coefficient of the time series of the pair of the nodes. Each G n contains node attributes X n and edge attributes H n . We develop a model that predicts a gait impairment score, Y n and outputs an individual explanation mask M c ∈ R d×d per class c to assign ROIs to functional brain networks."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,2.1,Connectomes in a Riemannian Manifold,"Functional connectivity matrices belong to the manifold of symmetric positivedefinite (SPD) matrices  To calculate the geodesic distance between two SPD matrices S i and S j ∈ Sym + d , we adopt the Log-Euclidean Riemannian Metric (LERM)  where • 2 F is the Frobenius norm. LERM is invariant to similarity transformations (scaling and orthogonality) and is computationally efficient for highdimensional data. See the Supplementary Material for results with other distance metrics."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,2.2,Stratified Learning-Based Sample Selection,"Data availability and dataset imbalance are re-occurring challenges with realworld clinical datasets, often leading to bias and overfitting during model train-ing. We address this by expanding a learning-based sample selection method "
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,2.3,Dynamic Graph Attention Layers,"Attention. We employ Graph Attention Network version 2 (GATv2)  where a (l) ∈ R 2F and Θ (l) are trainable parameters and learned, h i ∈ R F is the embedding for node i, σ represents a non-linearity activation function, and denotes vector concatenation. As conventional graph attention mechanisms for transductive tasks typically do not incorporate edge attributes, we introduce an attention-based, message-passing mechanism incorporating edge weights, similar to  where MLP 1 is a Multi-Layer Perceptron. Accordingly, an update of each ROI representation is influenced by its neighboring regions weighted by their connectivity strength. After stacking L layers, a readout function summarizing all node embeddings is employed to obtain a graph-level embedding g: Loss Function. xGW-GAT layers (Fig.  where r q is the rescaling weight for the q-th class, y pq is the q-th element of the true label vector y p for the p-th sample, and ŷpq is the predicted label vector."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,2.4,Individual-And Global-Level Explanations,"We define an attention explanation mask for each sample, n ∈ 1, 2, . . . , N and for each class c ∈ 1, 2, . . . , C that identifies the most important node/ROI connections contributing to the classification of subjects. We return a set of attention coefficients α n = [α n 1 , α n 2 , . . . , α n S ] for each sample n, where S is the number of attention heads. We aggregate trained, attention coefficients per sample used for predicting each ŷ using a max operation that returns α n max ∈ R d×d . An explanation mask per class, M c , or per sample, M n , can be derived using the max attention coefficients, α max (Fig.  M can be soft-thresholded to retain the top-L most positively attributed attention weights to the mask as follows: where Top-L(M) represents the set of top-L elements in M."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,3.0,Experiments,"Dataset. We obtained data from a private dataset (n = 35, mean age 69 ± 7.9) defined in "
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,,Software.,"All experiments were implemented in Python 3.10 and ran on Nvidia A100 GPU runtimes. We used PyTorch Geometric  Setup. We used the mean, connectivity profile, i.e., W "
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,3.1,Results,We perform a multi-class classification task of Slight 
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,,Method,Pre Rec F1 AUC GCN*  The results (Table 
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,4.0,Discussion,"Brain Networks Mapping. As shown in Fig.  We observe that rich interactions decrease significantly for the Mild class, Fig. "
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,5.0,Conclusion,"This study showcases a novel benchmark for using an explainable, geometric-weighted graph attention network to discover patterns associated with gait impairment. The framework innovatively integrates edge-weighted attention encoding and explanations to represent neighborhood interactions in functional brain connectomes, providing interpretable functional network clustering for neurological analysis. Despite a small sample size and imbalanced settings, the lightweight model offers stable results for quick inference on categorical PD neuromotor states. Future work includes new experiments, an expanded, multi-modal dataset, and sensitivity and specificity analysis to discover subtypes associated with the severity of PD gait impairment."
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,,Acknowledgements,. This work was partially supported by 
An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 68.
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,1.0,Introduction,"Segmentation is among the most common medical image analysis tasks and is critical to a wide variety of clinical applications. To date, data-driven deep learning (DL) methods have shown prominent segmentation performance when trained on fully-annotated datasets  Cold-start AL is highly relevant to many practical scenarios. First, cold-start AL aims to study the general question of constructing a training set for an organ that has not been labeled in public datasets. This is a very common scenario (whenever a dataset is collected for a new application), especially when iterative AL is not an option. Second, even if iterative AL is possible, a better initial set has been found to lead to noticeable improvement for the subsequent AL cycles  Despite its importance, very little effort has been made to address the coldstart problem, especially in medical imaging settings. The existing cold-start AL techniques are mainly based on the two principles of the traditional AL strategies: (1) Uncertainty sampling  In this paper, we introduce the COLosSAL benchmark, the first cold-start active learning benchmark for 3D medical image segmentation by evaluating on six popular cold-start AL strategies. Specifically, we aim to answer three important open questions: (1) compared to random selection, how effective are the uncertainty-based and diversity-based cold-start strategies for 3D segmentation tasks?  Our contributions are summarized as follows: • We offer the first cold-start AL benchmark for 3D medical image segmentation. We make our code repository, data partitions, and baseline results publicly available to facilitate future cold-start AL research. • We explore the impact of the budget and the extent of the 3D ROI on the cold-start AL strategies. "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.0,COLosSAL Benchmark Definition,"Formally, given an unlabeled data pool of size N , cold-start AL aims to select the optimal m samples (m ≪ N ) without access to any prior segmentation labels. Specifically, the optimal samples are defined as the subset of 3D volumes that can lead to the best validation performance when training a standard 3D segmentation network. In this study, we use m = 5 for low-budget scenarios."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.1,3D Medical Image Datasets,We use the Medical Segmentation Decathlon (MSD) collection 
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.2,Cold-Start AL Scenarios,"In this study, we investigate the cold-start AL strategies for 3D segmentation tasks in three scenarios, as illustrated in Fig.  1. With a low budget of 5 volumes (except for Heart, where 3 volumes are used because of the smaller dataset and easier segmentation task), we assess the performance of the uncertainty-based and diversity-based approaches against the random selection. 2. Next, we explore the impact of budgets for different cold-start AL schemes by allowing a higher budget, as previous work shows inconsistent effectiveness of AL schemes in different budget regimes  Evaluation Metrics. To evaluate the segmentation performance, we use the Dice similarity coefficient and 95% Hausdorff distance (HD95), which measures the overlap between the segmentation result and ground truth, and the quality of segmentation boundaries by computing the 95 th percentile of the distances between the segmentation and the ground truth boundary points, respectively."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.3,Baseline Cold-Start Active Learners,"We provide the implementation for the baseline approaches: random selection, two variants of an uncertainty-based approach named ProxyRank  Random Selection. As suggested by prior works  Uncertainty-Based Selection. Many traditional AL methods use uncertainty sampling, where the most uncertain samples are selected using the uncertainty of the network trained on an initial labeled set. Without such an initial labeled set, it is not straightforward to capture uncertainty in the cold-start setting. Recently, Nath et al.  However, this approach  As in  Diversity-Based Selection. Unlike uncertainty-based methods which require a warm start, diversity-based methods can be used in the cold-start setting. Generally, diversity-based approaches consist of two stages. First, a feature extraction network is trained using unsupervised/self-supervised tasks to represent each unlabeled data as a latent feature. Second, clustering algorithms are used to select the most diverse samples in latent space to reduce data redundancy. The major challenge of benchmarking the diversity-based methods for 3D tasks is to have a feature extraction network for 3D volumes. To address this issue, we train a 3D auto-encoder on the unlabeled training data using a self-supervised task, i.e., image reconstruction. Specifically, we represent each unlabeled 3D volume as a latent feature by extracting the bottleneck feature maps, followed by an adaptive average pooling for dimension reduction  , where X c = {x 1 , x 2 , ...x j } is the feature set in a cluster and cosine similarity is used as sim(⋅). 3. TypiClust  . K is set as 20 in the original paper but that is too high for our application. Instead, we use all the samples from the same cluster to calculate typicality."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,2.4,Implementation Details,"In our benchmark, we use the 3D U-Net as the network architecture. For uncertainty estimation, 20 Monte Carlo simulations are used with a dropout rate of 0.2. As in  A variety of augmentation techniques as in  For the global vs. local experiments, the local ROIs are created by extracting the 3D bounding box from the ground truth mask and expanding it by five voxels along each direction. We note that although no ground truth masks are accessible in the cold-start AL setting, this analysis is still valuable to determine the usefulness of local ROIs. It is only worth exploring automatic generation of these local ROIs if the gold-standard ROIs show promising results."
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,3.0,Experimental Results,"Impact of Selection Strategies. In Fig.  Our results explain why random selection remains a strong competitor for 3D segmentation tasks in cold-start scenarios, as no strategy evaluated in our benchmark consistently outperforms the random selection average performance. However, we observe that TypiClust (shown as orange) achieves comparable or superior performance compared to random selection across all tasks in our benchmark, whereas other approaches can significantly under-perform on certain tasks, especially challenging ones like the liver dataset. Hence, Typi-Clust stands out as a more robust cold-start selection strategy, which can achieve at least a comparable (sometimes better) performance against the mean of random selection. We further note that TypiClust largely mitigates the risk of 'unlucky' random selection as it consistently performs better than the low-performing random samples (red dots below the dashed line). Impact of Different Budgets. In Fig.  Impact of Different ROIs. In Fig.  Limitations. For the segmentation tasks that include tumors (  th columns on Fig. "
Medical Image Computing and Computer Assisted Intervention – MICCAI 2023,4.0,Conclusion,"In this paper, we presented the COLosSAL benchmark for cold-start AL strategies on 3D medical image segmentation using the public MSD dataset. Comprehensive experiments were performed to answer three important open questions for cold-start AL. While cold-start AL remains an unsolved problem for 3D segmentation, important trends emerge from our results; for example, diversitybased strategies tend to benefit more from a larger budget. Among the compared methods, TypiClust "
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,1.0,Introduction,"The demand for precise medical data analysis has led to the widespread use of deep learning methods in the medical field. However, accompanied by the promulgation of data acts and the strengthening of data privacy, it has become increasingly challenging to train models in large-scale centralized medical datasets. As one of the solutions, federated learning provides a new way out of the dilemma and attracts significant attention from researchers. Federated learning (FL)  The primary contributions of this paper can be summarized as: "
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,2.1,Overview,"Suppose K clients with private data cooperate to train a global model and share the same neural network structure, 3D-Unet "
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,2.2,FedGrav,"Model Affinity. Inspired by the law of universal gravitation, we assume that there is similar gravitation between any two local models. We define it as model affinity in federated learning. It can be described that the affinity between two local models is proportional to the sample size of the client corresponding to the local model, and inversely proportional to the distance between two models. The equation for model affinity takes the form: where A ik is the affinity between i-th and k-th local models, n i and n k are the sample size of i-th and k-th client, and d ik is the distance between two local models, which is quantified from the perspective of neural network topology and will be described in the following section. M is the affinity constant, it can be simplified in the subsequent analysis, so this paper will not set specific values for it. The model affinity depicts the internal correlation between two local models, which lays the foundation for accurate aggregation weights. Graph Distance. The distance is defined to quantify model differences. The differences in local models reflect the discrepancies in the distribution of client data to a certain extent. If the differences in local models can be accurately measured, the more appropriate aggregation weights will be assigned to local models to aggregate a better global model. The key motivation is to measure the internal correlations of local models as accurately as possible. We explore the model distance from the perspective of neural network topology in this paper and define it as model graph distance. In FedGrav, the computation of graph distance goes through the following steps: (1) Graph Mapping. Suppose the server has received local models trained by local data, and we map them into the topological graph. Inspired by  It can be mapped into a graph whose structure is similar to the full connection layer after the scalarization of the convolutional layer. Given a 3×3×3×C in ×C out convolutional layer, the dimensions of its input and output are C in and C out respectively. So, we obtain a weight matrix W t ∈R Cin×Cout after averaging or summing the weights of convolution kernel. We take the C in and C out as the number of nodes, and the weight summation w sum is the edge weight. (2) Graph Pruning. The server collects local models from clients and makes the graph mapping on them to get K graphs which have the same structure except for the edge weights. These graphs contain all the information of local models, including the part of universality and the part of characteristics of the client data. To make the graphs more distinctive, the graph pruning is conducted. In detail, we differentiated these graphs by setting an adaptive threshold δ, where the edge will be removed if the weight difference of each layer between the local models and global model in the last round is less than the threshold, otherwise, the edge will exist. It can be simplified as: otherwise. (3) where in Eq. 3, 0 denotes the edge is removed, w t kj denotes edge weight of the j-th layer from the k-th graph in the t-th round, also the weight summation of the j-th layer from the k-th local model in the t-th round, w t-1 gj is the weight summation of the j-th layer from the global model in (t -1)-th round. The threshold δ varies adaptively with the weights of local models, and λ is the pruning ratio which is responsible for adjusting the degree of pruning. After that we get K discriminative graphs (3) Graph Comparison. In order to measure the degree of correlation between two graphs, we measure the similarity between pairs of graphs by computing matching between their sets of embeddings, where the Pyramid Match Graph Kernel  Aggregation Weights. According to the above process, the Affinity Matrix A is obtained, which reports the correlation among local models and is symmetric. The element A ik in matrix A denotes the affinity of G i and G k . The elements in Table "
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,,Method Accuracy (%),"FedAvg  In federated learning, clients send the updated local models back to the server each round. In round t, α k is represented as α t k . The global model w t+1 g is aggregated by the server: then, the server assigns the global model w t g to all clients. Repeat and until T rounds or other limits."
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,3.1,Datesets and Settings,CIFAR-10. The first dataset to verify the validity of our algorithm is CIFAR-10. We partition the training set into 8 clients with heterogeneous data by sampling from a Dirichlet distribution (α = 0.5) as in 
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,,MICCAI FeTS2021,"Training Data. The real-world dataset used in experiments is provided by the FeTS Challenge organizer, which is the training set of the whole dataset about brain tumor segmentation. In order to evaluate the performance of FedGrav, we partition the dataset composed of 341 data samples "
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,3.2,Results,"Experiment Results on the CIFAR-10. We first validate the proposed method on the CIFAR-10 dataset. Table  Experiment Results on MICCAI FeTS2021 Training Dataset. In order to verify the robustness of our method and its performance in real-world data, we conduct the experiment on the MICCAI FeTS2021 Training dataset. We evaluate the performance of our algorithm by comparing six indicators: the Dice Similarity Coefficient(DSC) and Hausdorff Distance-95th percentile(HD95) of whole tumor(WT), enhancing tumor(ET), and tumor core(TC). As is shown in Table  The visualization results are shown in Fig. "
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,3.3,Ablation Study,"To evaluate the effectiveness and find the better configuration of FedGrav, we conduct the ablation study on the FeTS datasets, and the results are shown in Fig. "
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,4.0,Conclusion,"In this paper, we introduced FedGrav, a novel aggregation strategy inspired by the law of universal gravitation in physics. FedGrav improves local model aggregation by considering both the differences in sample size and discrepancies among local models. It can adaptively adjust the aggregation weights and explore the internal correlations of local models more effectively. We evaluated our method on CIFAR-10 and real-world MICCAI Federated Tumor Segmentation Challenge (FeTS) datasets, and the superior results demonstrated the effectiveness and robustness of our FedGrav."
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,1.0,Introduction,"The electrocardiogram (ECG), is one of the most widely used methods to analyze cardiac morphology and function, by measuring the electrical signal from the heart with multiple electrodes. ECG data is used by clinicians for both diagnostic and monitoring purposes in various cardiac syndromes. A 12-lead ECG is routinely obtained in patients to diagnose and monitor disease development. However, for the interpretation of the ECG signal, the knowledge of an expert is required. Physicians usually analyze the ECG through the recognition of specific patterns, known to be associated with disease. This however requires substantial expertise, and potentially additional relevant information exists in a 12-lead ECG missed by human interpretation. Deep learning has already proven its usefulness in the interpretation of the ECG signal in multiple classification challenges  VAEs and in particular β-VAEs have been used as unsupervised explainable ECG feature generators in the explainable AI algorithms mentioned above  The aim of this paper is to explore further improvement of the latent features by improving their explainability and prediction performance. This is clinically relevant but unexplored for the post myocardial infarction setting. We propose to improve explainability by reducing the dimension of the latent space to a level more manageable for human assessment, while encouraging outcome specific information to be captured in a small part of the latent space, and while maintaining ECG reconstruction performance for visual assessment. To achieve this, we propose a novel method to jointly optimize the β-VAE with a combination of a task specific prediction loss for a subset of the latent space, and KL-divergence and reconstruction loss for the entire latent space. The task chosen to optimize here is left ventricular function (LVF), one of the most important determinants of prognosis in patients with cardiac disease. Current assessment of LVF requires advanced imaging methods and interpretation by a trained professional. The ECG, on the other hand, can be obtained by a patient at home. In combination with automated analysis this would facilitate remote monitoring of LVF in patients."
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,2.1,Data,"To train the models for both reconstruction and LVF prediction, two datasets were used: i) A non-labeled dataset consisting of 119,886 raw 10 s 12-lead ECG signals taken at 500 Hz from 7255 patients diagnosed with acute coronary syndrome between 2010 and 2021 at the Leiden University Medical Center, the Netherlands; ii) A labeled dataset of 33,610 ECGs from 2736 patients of the same cohort. This dataset was labeled by visual assessment of an echocardiogram performed within 3 days before or after the ECG. The label categories, normal, mild, moderate and severe impairment were binarized for model training. When the ECG was taken within two weeks after cardiac intervention a 1-day margin was used. If a cardiac intervention was performed between ECG and echocardiography, the case was excluded. 11.5% of the ECGs were labeled with a moderate to severe impaired LVF. The institutional review board approved the study protocol (nWMODIV2 2022006) and waived the obligation to obtain informed consent. The raw ECG signals were first split into separate heartbeats (400ms before and after the R-peak, the largest peak in the ECG, that represents depolarization of the ventricles) with a peak detection method inspired by RPNet, a U-Net structured CNN with inception blocks, that was trained on manually labeled peak locations "
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,2.3,Model Overview,To investigate a general improvement to the VAE feature extraction pipeline 
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,2.4,Model Training,"The β-VAE was first pretrained in a self-supervised manner with the mean heartbeats of all filtered ECG signals, minimizing i) the mean squared reconstruction error (MSE) between the input and output ECG, and ii) the KL-divergence between the output of the encoder and the standard normal distribution. The KL-divergence loss was weighted with a β factor, like in the original paper "
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,2.5,Feature Evaluation,"The differences between the features from the task naive and task specific VAEs, were compared w.r.t. reconstruction and prediction. For reconstruction, both MSE and correlation between input and output ECG, and for prediction the Area Under the Receiver Operator Characteristic Curve (AUROC) and the macroaveraged F1 score were used. Significant difference between AUROC scores was calculated as proposed in "
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,2.6,Baseline Methods,"As a baseline method, a principal component analysis (PCA) was performed on the preprocessed ECGs, to extract features. PCA can be considered an ordered task naive linear feature extractor that focuses on the axis of the largest variance, in contrast to the VAEs which are non-ordered non-linear feature extractors, that are optimized for reconstruction. A logistic regression predictor with just sex and age as input was used as an additional baseline."
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,3.1,Experiments,"The proposed pipeline contains several hyper-parameters, of which the latent space size L was optimized in this study. The influence of the β parameter was also briefly addressed. L was optimized for its importance in the explainability and the reconstruction and prediction quality of the model. A higher L increases the complexity of the model, and consequently decreases its explainability. An L that is too low, on the other hand, restricts the capacity of the model for reconstruction and prediction. The PCA baseline method was considered to give an upper bound of L, since the number of principal components, the PCA analog for L, indicates how many values would be needed to capture sufficient information."
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,3.2,Hyperparameter Optimization,The influences of γ on prediction and reconstruction performance was small and was therefore fixed to 500. The influence of L on prediction quality can be seen in Fig. 
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,3.3,Results on the Test Set,Table 
Joint Optimization of a $$\beta $$-VAE for ECG Task-Specific Feature Extraction,4.0,Discussion,"Joint optimization of a β-VAE successfully generated features that contain more information about LVF, without hampering reconstruction of the ECG signal. We hypothesize that the β-VAEs have multiple optima for ECG reconstruction of which only some generate features that are relevant for LVF prediction. This study shows that joint optimization will favor this desired subset of optima, and that this is true for different architectures. In addition, we showed that jointly optimizing only a subset of the latent space features for prediction, results in aggregation of the predictive information, thereby improving explainability. The AUROC score of the FactorECG VAE prediction is similar when compared to van der Leur et al. (2022)  The F1 score is considered more robust than the AUROC score with data imbalance, which is the case here "
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,1.0,Introduction,"Coronary artery segmentation is crucial for clinical coronary artery disease diagnosis and treatment  Label-efficient learning algorithms have garnered considerable interest and research efforts in natural and medical image processing  Different types of supervision are utilized according to the specific tasks. The application of various types of weak supervision are inhibited in coronary artery segmentation on CCTA images by the following challenges. 1) Difficult labeling (Fig.  Given the above, we propose partial vessels annotation (PVA) (Fig.  2) PVA provides flexibility for clinicians. Given that clinical diagnosis places greater emphasis on the trunks rather than the branches, PVA allows clinicians to focus their labeling efforts on vessels of particular interest. Therefore, our proposed PVA is well-suited for clinical use. In this paper, we further propose a progressive weakly supervised learning framework for PVA. Our proposed framework, using PVA (only 24.29% vessels labeled), achieved better performance than the competing weakly supervised methods, and comparable performance in trunk continuity with the full annotation (100% vessels labeled) supervised baseline model. The framework works in two stages, which are local feature extraction (LFE) stage and global structure reconstruction (GSR) stage. 1) LFE stage extracts the local features of coronary artery from the limited labeled vessels in PVA, and then propagates the knowledge to unlabeled regions. 2) GSR stage leverages prediction consistency during the iterative self-training process to correct the errors, which are introduced inevitably by the label propagation process. The code of our method is available at https://github.com/ZhangZ7112/PVA-CAS. To summarize, the contributions of our work are three-fold: -To the best of our knowledge, we proposed partial vessels annotation for coronary artery segmentation for the first time, which is in accord with clinical use. First, it balances efficiency and informativity. Second, it provides flexibility for clinicians to annotate where they pay more attention. -We proposed a progressive weakly supervised learning framework for partial vessels annotation-based coronary artery segmentation. It only required 24.29% labeled vessels, but achieved comparable performance in trunk continuity with the baseline model using full annotation. Thus, it shows great potential to lower the label cost for relevant clinical and research use. -We proposed an adaptive label propagation unit (LPU) and a learnable plugand-play feature prototype analysis (FPA) block in our framework. LPU integrates the functions of pseudo label initialization and updating, which dynamically adjusts the updating weights according to the calculated confidence level. FPA enhances vessel continuity by leveraging the similarity between feature embeddings and the feature prototype."
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,2.0,Method,"As shown in Fig.  2) The GSR stage (Sect. 2.2) utilizes pseudo labels to conduct self-training, and leverages prediction consistency to improve the pseudo labels. In our proposed framework, we also designed an adaptive label propagation unit (LPU) and a learnable plug-and-play feature prototype analysis (FPA) block. LPU initialize and update the pseudo labels; FPA block learns before testing and improves the final output during testing. "
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,2.1,Local Feature Extraction Stage,"In LFE stage, our hypothesis is that the small areas surrounding the labeled regions hold valid information. Based on this, a light segmentation model S l is trained to learn vessel features locally, with small patches centering around the labeled regions as input and output. In this manner, the negative impact of inaccurate supervision information in unlabeled regions is also reduced. Pseudo Label Initialization in LPU. After training, S l propagates the learned knowledge of local feature to unlabeled regions. For each image of shape H × W × D, the corresponding output logit ŷ1 ∈ [0, 1] H×W ×D of S l provides a complete estimate of the distribution of vessels, albeit with some approximation. Meanwhile, the PVA label y P V A ∈ {0, 1} H×W ×D provides accurate information on the distribution of vessels, but only to a limited extent. Therefore, LPU integrate ŷ1 and y P V A to initialize the pseudo label y P L (Eq. 1), which will be utilized in GSR stage and updated during iterative self-training. (1)"
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,2.2,Global Structure Reconstruction Stage,"The GSR stage mainly consists of three parts: 1) The segmentation model S g to learn the global tree-like structure; 2) LPU to improve pseudo labels; 3) FPA block to improve segmentation results at testing. Through initialization (Eq. 1), the initial pseudo label y (t=0) P L contains the information of both PVA labels and the knowledge of local features in S l . Therefore, at the beginning of this stage, S g learns from y (t=0) P L to warm up. After this, logits of S g are utilized to update the pseudo labels during iterative self-training. Pseudo Label Updating in LPU. The principle of this process is that more reliable logit influences more the distribution of the corresponding pseudo label. Based on this principle, first we calculate the confidence degree η (t) ∈ [0, 1] for ŷ(t) 2 . Defined by Eq. 2, η (t) numerically equals to the average of the logits in labeled regions. This definition makes sense since the expected logit equals to ones in vessel regions and zeros in background regions. The closer ŷ(t) 2 gets to the expected logit, the higher η (t) (confidence degree) will be. Then, a quality control test is performed to avoid negative optimization as far as possible. As the confidence degree η (t) assesses the quality of predictions, which means low-confidence predictions are more likely to generate errors, our quality control test rejects low-confidence predictions to reduce the risk of error accumulation. If η (t) is higher than all elements in the set {η (i) } t-1 i=1 , the current logit is trustworthy to pass the test to improve the pseudo label. Then, y (t) P L is updated by the exponentially weighted moving average (EWMA) of the logits and the pseudo labels (Eq. 3). This process is similar to prediction ensemble  Feature Prototype Analysis Block. Inspired by  In the penultimate layer of the network, which is followed by a 1 × 1 × 1 convolutional layer to output logits, we parallelly put the feature map Z ∈ R C×H×W ×D into FPA. The output similarity map O ∈ R 1×H×W ×D is calculated by Eq. 5, where Z(h, w, d) ∈ R C denotes the feature embeddings of voxel (h, w, d), and ρ θ ∈ R C the kernel parameters of FPA. The learning process of FPA block is before testing, during which the whole model except FPA gets frozen. To reduce the additional overhead, ρ θ is initialized by one-time calculated ρ c and fine-tuned with loss L fpa (Eq. 6), where only labeled voxels will take effect in updating the kernel. 3 Experiments and Results"
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,3.1,Dataset and Evaluation Metrics,"Experiments are implemented on a clinical dataset, which includes 108 subjects of CCTA volumes (2:1 for training and testing). The volumes share the size of 512 × 512 × D, with D ranging from 261 to 608. PVA labels of the training set are annotated by clinicians, where only 24.29% vessels are labeled. The metrics used to quantify the results include both integrity and continuity assessment indicators. Integrity assessment indicators are Mean Dice Coefficient (Dice), Relevant Dice Coefficient (RDice) "
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,3.2,Implementation Details,3D U-Net 
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,3.3,Comparative Test,"To verify the effectiveness of our proposed method, it is compared with both classic segmentation models (3D U-Net  The qualitative visual results verify that our proposed method outperforms the competing methods under PVA. Three cases are shown in Fig. "
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,3.4,Ablation Study,Ablation experiments were conducted to verify the importance of the components in our proposed framework (summarized in Table 
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,4.0,Conclusion,"In this paper, we proposed partial vessels annotation (PVA) for coronary artery segmentation on CCTA images. The proposed PVA is convenient for clinical use for the two merits, providing flexibility as well as balancing efficiency and informativity. Under PVA, we proposed a progressive weakly supervised learning framework, which outperforms the competing methods and shows comparable performance in trunk continuity with the full annotation supervised baseline model. In our framework, we also designed an adaptive label propagation unit (LPU) and a learnable plug-and-play feature prototype analysis(FPA) block. LPU integrates the functions of pseudo label initialization and updating, and FPA improves vessel continuity by leveraging the similarity between feature embeddings and the feature prototype. To conclude, our proposed framework under PVA shows great potential for accurate coronary artery segmentation while requiring significantly less annotation effort."
Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_28.
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,1.0,Introduction,"Magnetic Resonance Imaging (MRI) of the brain is an essential imaging modality to accurately diagnose various neurological diseases ranging from inflammatory T. Pinetz and A. Effland-are funded the German Research Foundation under Germany's Excellence Strategy -EXC-2047/1 -390685813 and -EXC2151 -390873048 and R. Haase is funded by a research grant (BONFOR; O-194.0002.1). T. Pinetz and E. Kobler-contributed equally to this work. lesions to brain tumors and metastases. For accurate depictions of said pathologies, gadolinium-based contrast agents (GBCA) are injected intravenously to highlight brain-blood barrier dysfunctions. However, these contrast agents are expensive and may cause nephrogenic systemic fibrosis in patients with severely reduced kidney function  Driven by this recommendation, several research groups have recently published dose-reduction techniques focusing on maintaining image quality. Complementary to the development of higher relaxivity contrast agents  In recent years, generative models have been used to overcome data scarcity in the computer vision and medical imaging community. Frequently, generative adversarial networks (GANs)  Learning conditional distributions between images can be accomplished by additionally feeding a condition (additional scans, dose level, etc.) into both the generator and discriminator. In particular, for image-to-image translation tasks, these conditional GANs have been successfully applied using paired  With this in mind, the contributions of this work are as follows: -synthesis of GBCA behavior at various doses using conditional GANs, -loss enabling interpolation of dose levels present in training data, -noise-preserving content loss function to generate realistic synthetic images."
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,2.0,Methodology,"Given a native image x NA (i.e. without any contrast agent injection) and a CE standard-dose image x SD , our conditional GAN approach synthesizes CE lowdose images xLD for selected dose levels d ∈ D ⊂ [0, 1] from a uniform noise image z ∼ N (0, Id), see Fig.  For training and evaluation, we consider samples (x NA , x SD , y LD , d, B) of a dataset DS, where y LD = x LD -x NA is the residual image of a real CE low-dose scan x LD with dose level d ∈ D and B ∈ {1.5, 3} is the field-strength in Tesla of the used scanner. To simplify learning of the contrast accumulation behavior, we adapt the preprocessing pipeline of BraTS "
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,2.1,Conditional GANs for Contrast Signal Synthesis,"Our approach is built on the insight that contrast enhancement is an inherently local phenomenon and the necessary information for the synthesis task can be extracted from a local neighborhood within an image. Therefore, we use as generator g θ a convolutional neural network (CNN) that is based on the U-Net  To learn this generator, a convolutional discriminator f φ is used, which is in turn trained to distinguish the generated residual images ŷLD with random dose level d from the real residual images y LD with the associated real dose level d. To make this a non-trivial task, label smoothing on the metadata is used, i.e., the real dose is augmented by zero-mean additive Gaussian noise with standard deviation 0.05. The discriminator architecture essentially implements the encoding side of the generator, however, no local attention layers are used as suggested by  For training of the generator θ and discriminator φ, we consider the loss which consists of a Wasserstein GAN loss L GAN , a gradient penalty loss L GP , and a content loss L C that are relatively weighted by scalar non-negative factors λ GP and λ C . In detail, the Wasserstein GAN loss reads as   Lipschitz continuous with factor 1 in its arguments as required by Wasserstein GANs "
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,2.2,Noise-Preserving Content Loss,"To generate realistic CE images, it is also important to retain the original noise characteristics. Therefore, we introduce a novel loss that accounts for deviations in local statistics using optimal transport between empirical distributions of paired patches, as illustrated in Fig.  where 1 is the vector of ones of size n 3 . In contrast to the element-wise difference penalization of the 1 -distance, the Wasserstein distance accounts for mismatches between distributions. To illustrate this, let us, for instance, assume that both patches are Gaussian distributed (x ∼ N (μ, σ), x ∼ N (μ, σ)), which is a coarse simplification of real MRI noise  To efficiently solve problem (2), we use the inexact proximal point algorithm  where P p extracts a local n × n × n patch at location p ∈ P = {0, n, 2n, . . .} 3 using periodic boundary conditions. Note that we compute the expectation over offsets o ∈ O = {0, 1, . . . , n 2 } 3 to avoid patching artifacts. In the numerical implementation, only a single offset is sampled for time and memory constraints."
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,3.0,Numerical Results,"In this section, we evaluate the proposed conditional GAN approach with a particular focus on different content loss distance functions. All synthesis models were trained on 250 samples acquired on 1.5T and 3T Philips Achieva scanners and evaluated on 193 test cases, all collected at site 1 . Further details of the dataset, model and training can be found in the supplementary. In our experiments, we observed that the choice of the content loss distance function C (ŷ, y) strongly influences the performance. Thus, we consider the different cases: Following Johnsen et al.  A qualitative comparison of the different distance functions C is visualized in Fig.  To highlight the generalization capabilities, we depict in the bottom row of Fig.  For completeness, a quantitative ablation of the considered distance functions on the test images of site 1 is shown in Table  Table  To determine the effectiveness of the LD synthesis models at different settings, we acquired 160 data samples from 1.5T and 3T Philips Ingenia scanners at site 2 . This site used the GBCA gadoterate, which has a lower relaxivity compared to gadobutrol used at site 1  Finally, Fig. "
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,4.0,Conclusions,"In this work, we used conditional GANs to synthesize contrast-enhanced images using non-standard GBCA doses. To this end, we introduced a novel noisepreserving content loss motivated by optimal transport theory. Numerous numerical experiments showed that our content loss improves the faithful synthesis of low-dose images. Further, the performance of virtual contrast models increases if training data is extended by synthesized images from our GAN model trained by the noise-preserving content loss."
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_57.
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,1.0,Introduction,"Deep learning has achieved remarkable success in medical image segmentation when the training and test data are independent and identically distributed (i.i.d)  Domain generalization (DG) methods exploit the diversity of source domains to improve the model generalization  TTA methods have also been recently applied in medical image applications. Ma et al.  In this paper, we present Semantic-Aware Test-Time Adaptation (SATTA) for cross-domain medical image segmentation, aiming to perform individual domain adaptation for each semantic category at test time. SATTA first utilizes an uncertainty estimation module to effectively measure the discrepancies of different semantic categories in domain shift. Based on the estimated discrepancies, a semantic adaptive learning rate is then developed to achieve a personalized degree of adaptation for each semantic category. Lastly, a semantic proxy contrastive loss is proposed to individually adjust the model parameters with the semantic adaptive learning rate. Our SATTA is evaluated on retinal fluid segmentation based on spectral-domain optical coherence tomography (SD-OCT) images, and the experimental results show superior performance than other state-of-the-art TTA methods."
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,2.1,Test-Time Adaptation Review,"Given a labeled source-domain dataset S = {(x s n , y s n )} N s n=1 , model parameters θ are pre-trained on S by supervised risk minimization: where L sup is the supervised loss for model optimization, such as the crossentropy loss. However, for an unlabeled target-domain dataset that has different domain distributions with S, the model F θ s may have an obvious performance degeneration. To make the model F θ s adapt to the targetdomain distributions, an unsupervised TTA loss L tta (such as rotation prediction loss  where η is learning rate, θ t 0 is initialized with θ s . The final prediction on x t n can be given by y"
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,2.2,Semantic Adaptive Learning Rate,"Pseudo-labeling for Semantic Aggregation.  where each category proxy w c can be regarded as the high-dimensional representative of the category. Category predictor measures the similarities between pixel embeddings and all category proxies. We represent the classification process of pixel p i as: where f (•) is the feature extractor, C is the category number, h and w are the height and width of images. Since pixel-wise labels are not available for targetdomain samples at test time, we are hard to obtain the semantic information of all pixel embeddings directly. To address this problem, we assign pseudo labels to all pixel embeddings by passing them through the category predictor and then aggregate all pixel embeddings into C semantic clusters as according to their pseudo labels. Semantic Uncertainty Estimation. After performing semantic aggregation by pseudo-labeling, we need to estimate the varying discrepancies of domain shift on categories. Here, we employ Monte Carlo Dropout  where M(•) is a mapping network that maps the pixel embedding e i into an additional probability output. Then we estimate the standard deviation of the L outputs as the uncertainty score of e i : Here we take a single pixel as an example to show the uncertainty score computation, it should be noted that all pixels are performed parallel computation in the semantic segmentation model. Therefore, the computation cost does not increase with the number of pixels. For category c, its semantic uncertainty score can be calculated by: where N Ωc is the number of pixel embeddings in Ω c . U c captures the unique semantic domain discrepancy over category c. Later, semantic adaptive learning rate η c of category c for TTA is obtained directly based on the semantic domain discrepancy U c : where α is a scale factor. In this work, α could be set as the learning rate used for the model pre-training with the source-domain dataset. Each semantic category has its own individual learning rate in each iteration."
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,2.3,Semantic Proxy Contrastive Learning,"General contrastive losses focus on exploring rich sample-to-sample relations, but they are hard to learn specific semantic information from samples. Proxy contrastive loss can model semantic relations by category proxies, as category proxies are more robust intuitively to noise samples  Projection Heads. We regard each category proxy as the anchor and consider all proxy-to-sample relations. Since proxy-based methods converge very easily, we consider applying projection heads to map both pixel embeddings and category proxies to a new feature space where proxy contrastive loss is applied. We use a three-layer MLP H 1 (•) for projecting pixel embeddings and one-layer MLP H 2 (•) for projecting category proxy weights. The new pixel embedding and category proxy weight can be given by z i = H 1 (e i ) and v c = H 2 (w c ). Top-K Selection. Pixel embeddings with high uncertainty scores contribute little to semantic proxy contrastive learning. Besides, the computation cost is huge for all pixel embeddings. To address this problem, we select K pixel embeddings with the highest confidence from each semantic cluster Ω c . Specifically, for each semantic cluster Ω c , we first order all pixel embeddings in it from smallest to largest according to their uncertainty scores. Then we select the first K pixel embeddings as the new semantic cluster Ω c for the next proxy contrastive loss by Ω c = T opK(Order(Ω c )). Semantic Proxy Contrastive Loss. For an anchor category cluster Ω c , we associate all pixel embeddings in it with category proxy weight v c to form the positive pairs. We ignore the sample-to-sample positive pairs and only consider the sample-to-sample negative pairs. The semantic proxy contrastive loss for category c can be given by: where {z i } K i=1 are obtained by x and C is the number of categories appearing in samples."
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,2.4,Training and Adaptation Procedure,"The overview of our SATTA is shown in Fig.  , the model parameters θ are pre-trained by the combination of supervised cross-entropy loss and semantic proxy contrastive loss: At test time, for a target-domain sample at time step n, we perform a forward pass to obtain semantic clusters and uncertainty scores and calculate the semantic adaptive learning rate of each category to serve for semantic proxy contrastive loss. For category c, the model parameters are updated to achieve desired adaptation by: The updated model parameters are stored in a memory bank and will be loaded for the next domain adaptation of category c. If category c does not appear in the test sample by pseudo-labeling, we ignore the update of model parameters θ c n-1 . We only update the parameters of the feature extractor and freeze the parameters of the category predictor."
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,3.1,Materials,Our SATTA was evaluated on retinal fluid segmentation based on RETOUCH challenge 
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,3.2,Comparison with State-of-the-Arts,"We compare our SATTA with four state-of-the-art TTA methods, including TENT "
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,3.3,Ablation Study,"We conduct ablation studies to analyze the key factors regarding our SATTA. We first explore the effect of K value in the Top-K selection strategy. The Top-K selection strategy aims to select pixel embeddings with high confidence scores to improve the semantic proxy contrastive learning and reduce the computation cost significantly. Figure  We also investigate the effect of the initial learning rate. We select different initial learning rates from the set {5e-3, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5} for TTA, and Fig. "
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,4.0,Conclusion,"In this paper, we present the SATTA method for cross-domain medical image segmentation. Aiming at the problem that the domain shift has different effects on the semantic categories, our SATTA provides a semantic adaptive parameter optimization scheme at test time. Although our SATTA shows superior crossdomain segmentation performance than other state-of-the-art methods, it still has a limitation. Since SATTA adjusts the model for each semantic category, it is not quite suitable for the samples with too many semantic categories due to high computation costs."
SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_14.
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,1.0,Introduction,"Ultrasound imaging is a very effective technique for breast lesion diagnosis, which has high sensitivity. Automatically detecting breast lesions is a challenging problem with a potential to aid in improving the efficiency of radiologists in ultrasound-based breast cancer diagnosis  Most existing breast lesion detection methods can be categorized into imagebased  To address the aforementioned issues, we propose a spatial-temporal deformable attention based network, named STNet, for detecting the breast lesions in ultrasound videos. Within our STNet, we introduce a spatial-temporal deformable attention module to fuse multi-scale spatial-temporal information among different frames, and further integrate it into each layer of the encoder and decoder. In this way, different from the recent CVA-Net, our proposed STNet performs both deep and local feature fusion. In addition, we introduce multiframe prediction with encoder feature shuffle operation that shares the backbone and encoder features, and only perform multi-frame prediction in the decoder. This enables us to significantly accelerate the detection speed of the proposed approach. We conduct extensive experiments on a public breast lesion ultrasound video dataset, named BLUVD-186 "
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,2.0,Method,"Here, we describe our proposed spatial-temporal deformable attention based framework, named STNet, for detecting breast lesions in the ultrasound videos. Figure "
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,2.1,Spatial-Temporal Deformable Attention,"Given a reference point, deformable attention  proposed STDA. Let F t = F l t L l=1 represent the set of multi-scale feature maps at frame t, where F l t ∈ R C×H l ×W l is the feature map at level l. Given the query features p q and corresponding reference points z q , the spatio-temporal multiscale attention is given as: where m represents multi-head index and k is sampling point index. W m is a linear layer, A tlqk indicates attention weight of sampling point, and Δp tlqk indicates sample offset of sampling point. φ l normalizes the coordinates p q by the scale of feature map F l t . The sampling offset Δp tlqk is predicted by the query feature z q with a linear layer. The attention weight A tlqk is predicted by feeding query feature z q to a linear layer and a softmax layer. As a result, the sum of attention weights is equal to one as ( Compared to the standard deformable attention, the proposed spatial-temporal deformable attention fully exploits spatial information within frame and temporal information across frames."
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,2.2,Spatial-Temporal Deformable Attention Based Encoder and Decoder,"Here, we integrate the proposed spatial-temporal deformable attention (STDA) into encoder and decoder (called ST-Encoder and ST-Decoder). As shown in Fig.  , where the query corresponds to each pixel in multi-scale feature maps. Then, the fused feature map goes through a feed-forward network (FFN) to generate the output feature maps Similar to the original deformable DETR, we adopt cascade structure to stack six STDA and FFN layers in ST-Encoder. The ST-Decoder takes the output feature maps E k-1 , E k , E k+1 , E r1 , E r2 , E r3 and a set of learnable queries Q ∈ R N ×C as inputs. The learnable queries first go through a self-attention layer. Afterwards, STDA performs cross-attention operation between these feature maps and the queries, where the key elements are these output feature maps of ST-Encoder. Then, we employ a FFN layer to generate the prediction features P k ∈ R N ×C . We also stack six self-attention, STDA, and FFN layers in ST-Decoder for deep feature extraction."
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,2.3,Multi-frame Prediction with Encoder Feature Shuffle,"As discussed above, the proposed STNet adopts six frames to predict the results of one frame. Although STNet fully exploits temporal information for improved breast lesion detection, it becomes time-consuming for multi-frame prediction. To accelerate the detection speed, we introduce multi-frame prediction with encoder feature shuffle during inference. Instead of going through the entire network several times, we first share deep multi-scale feature maps before encoder and second perform the decoder several times for multi-frame prediction. To perform multi-frame prediction only in the decoder, we propose the encoder feature shuffle operation shown in Fig. "
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,3.1,Dataset and Implementation Details,Dataset. We conduct the experiments on the public BLUVD-186 dataset 
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,,Method,"Type Backbone AP AP50 AP75 GFL  DFF  Evaluation Metrics. Three commonly-used metrics are employed for performance evaluation of breast lesion detection methods on the ultrasound videos, namely average precision (AP), AP 50 , and AP 75 . Implementation Details. We employ the ResNet-50 "
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,3.2,State-of-the-Art Comparison,"Our proposed approach is compared with eleven state-of-the-art methods, comprising image-based and video-based methods. We report the detection performance of these state-of-the-art methods generated by CVA-Net  Qualitative Comparisons. Figure "
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,3.3,Ablation Study,"Effectiveness of STDA: To show the efficacy of our proposed STDA, we perform different ablation studies. The first baseline network, referred as ""Baseline + Single-frame"", uses the original deformable DETR and takes a single frame as input. The second baseline network, referred as ""Baseline + Multi-frame"", uses modified deformable DETR with multi-head attention module to fuse six input frames. For the third study, labeled ""ST-Encoder + DA-Decoder"", we retain the encoder with STDA in our model but replace the STDA in the decoder with the conventional deformable attention. Similarly, in the fourth study, labeled ""DA-Encoder + ST-Decoder"", we retain the decoder with STDA in our model but replace the STDA in the encoder with the conventional deformable attention. As shown in Table "
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,4.0,Conclusion,"We propose a novel breast lesion detection approach for ultrasound videos, termed as STNet, which performs local spatial-temporal feature fusion and deep feature aggregation in each stage of both encoder and decoder using our spatial-temporal deformable attention module. Additionally, we introduce the encoder feature shuffle strategy that enables multi-frame prediction during inference, thereby enabling us to accelerate the inference speed while maintaining better detection performance. The experiments conducted on a public breast lesion ultrasound video dataset show the efficacy of our STNet, resulting in a superior detection performance while operating at a fast inference speed. We believe STNet presents a promising solution and will help further promote future research in the direction of efficient and accurate breast lesion detection in videos."
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,1.0,Introduction,"Deep Neural Networks (DNNs) have successfully been applied in research and industry for a multitude of complex tasks. This includes various medical F. Pahde and M. Dreyer-Contributed equally. Fig.  applications for which DNNs have even shown to be superior to medical experts, such as with Melanoma detection  The field of XAI brings light into the black boxes of DNNs and provides a better understanding of their decision processes. As such, local XAI methods reveal (input) features that are most relevant to a model, which, for image data, can be presented as heatmaps. In contrast, global XAI methods (e.g.,  To that end, we propose Reveal to Revise (R2R), an iterative XAI life cycle requiring low amounts of human interaction that consists of four phases, illustrated in Fig. "
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,2.0,Related Work,"Among other methods, e.g., leveraging auxiliary information  Most model correction methods require dense annotations, such as labels for artifactual samples or artifact localization masks, which are either crafted heuristically or by hand  Existing works for model correction measure the performance on the original or clean test set, with corrected models often showing an improved generalization "
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,3.0,Reveal to Revise Framework,"Our Reveal to Revise (R2R) framework comprises the entire XAI life cycle, including methods for (1) the identification of model bias, (2) artifact labeling and localization, (3) the correction of detected misbehavior, and (4) the evaluation of the improved model. To that end, we now describe the methods used for R2R."
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,3.1,Data Artifact Identification and Localization,"The identification of spurious data artifacts using CRP concept visualizations or SpRAy clusters is firstly described, followed by our artifact localization approach. CRP Concept Visualizations. CRP  Explanation Outliers Through SpRAy. Alternatively, SpRAy  Artifact Localization. We automate artifact localization by training a Concept Activation Vector (CAV) h l to model the artifact in latent space of a layer l, representing the direction from artifactual to non-artifactual samples obtained from a linear classifier. The artifact localization is given by a modified backward pass on the biased model with LRP for an artifact sample x, where we initialize the relevances R l (x) at layer l as with activations a l and element-wise multiplication operator •. This is equivalent to explaining the output from the linear classifier given as a l (x) • h l . Using a threshold, the resulting CAV heatmap can be further processed into a binary mask to crop out the artifact from any corrupted sample, as illustrated in Fig. "
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,3.2,Methods for Model Correction,"In the following, we present the methods used for mitigating model biases. ClArC for Latent Space Correction. Methods from the ClArC framework correct model (mis-)behavior w.r.t. an artifact by modeling its direction h in latent space using CAVs  with perturbation strength γ(x) dependent on input x. Parameter γ(x) is chosen such that the activation in direction of the CAV is as high as the average value over non-artifactual or artifactual samples for p-ClArC or a-ClArC, respectively. RRR and CDEP for Correction Through Prior Knowledge. Model correction using RRR  with a binary mask M prior (x) localizing an artifact and class label y true . Alternatively, CDEP  (4)"
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,4.0,Experiments,"The experimental section is divided into the two parts of (1) identification, mitigation and evaluation of spurious model behavior with various correction methods and (2) showcasing the whole R2R framework in an iterative fashion. "
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,4.1,Experimental Setup,We train VGG-16 
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,4.2,Revealing and Revising Spurious Model Behavior,"Revealing Bias: In the first step of the R2R life cycle, we can reveal the use of several artifacts by the examined models, including the well-known band-aid, ruler and skin marker  Revising Model Behavior: Having revealed spurious behavior, we now revise the models, beginning with model correction. Specifically, we correct for the band-aid, ""L"" markings as well as synthetic artifacts. The skin marker and ruler  "
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,4.3,Iterative Model Correction with R2R,Showcasing the full R2R life cycle (as shown in Fig. 
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,5.0,Conclusion,"We present R2R, an XAI life cycle to reveal and revise spurious model behavior requiring minimal human interaction via high automation. To reveal model bias, R2R relies on CRP and SpRAy. Whereas SpRAy automatically points out Clever Hans behavior by analyzing large sets of attribution data, CRP allows for a finegrained investigation of spurious concepts learned by a model. Moreover, CRP is ideal for large datasets, as the concept space dimension remains constant. By automatically localizing artifacts, we successfully perform model revision, thereby reducing attention on the artifact and leading to improved performance on corrupted data. When applying R2R iteratively, we did not find the emergence of new biases, which, however, might happen if larger parts of the model are finetuned or retrained to correct strong biases. Future research directions include the application to non-localizable artifacts, and addressing fairness issues in DNNs."
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,,Acknowledgements,. This work was supported by the 
Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models,,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 56. We evaluate the effectiveness of model corrections based on two metrics: the attributed fraction of relevance to artifacts and prediction performance on both the original and a poisoned test set (in terms of F1-score and accuracy). Whereas in the synthetic case, we simply insert the artifact into all samples to poison the test set, data-intrinsic artifacts are cropped from random artifactual samples using our artifact localization strategy. Note that artifacts might overlap clinically informative features in poisoned samples, limiting the comparability of poisoned and original test performance. As shown in Tab. 1 (ISIC 2019) and Appendix A.2 (Bone Age), we are generally able to improve model behavior with all methods. The only exception is the synthetic artifact for VGG-16, where only RRR mitigates the bias to a certain extent, indicating that the artifact signal is too strong for the model. Here, fine-tuning only the last layer is not sufficient to learn alternative prediction strategies. Interestingly, despite successfully decreasing the models' output sensitivity towards artifacts,"
A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis,1.0,Introduction,"Deep learning for medical image analysis is a key tool to facilitate precision medicine and support clinical decision making. However, it has become increasingly evident that biases in the training data can lead to obstacles for clinical implementation. In this work, we define bias as a property of the data (e.g., class/attribute imbalance, spurious correlations) used for training a model that can lead to shortcut learning and/or failure to adequately represent data subgroups, which may lead to reduced generalizability and/or fairness when applied in real-world scenarios. For instance, such biases have been shown to lead to poor generalization capabilities of models evaluated on cohorts with sociodemographic population statistics different to those that it was trained on  Due to these problems, a plethora of research has recently gone towards bias mitigation  Current methods that have been proposed for fully controlled simulation of features in deep learning datasets, where generating factors can be fully disentangled and are well known in advance, are largely limited to toy problems or MNIST-like scenarios  In this work, we simulate brain magnetic resonance (MR) images with regionspecific morphology variations representing disease and bias effects. We also introduce global morphological variation representing distinct synthetic subjects. This option facilitates bridging the relationship between understanding the impacts of biases alone, and how biases combine with real-world variation when training deep learning models. We utilize neuroimaging data as an initial use case and focus on morphological biases in this work. However, the proposed modular framework is not limited to neuroimaging problems and could be modified to introduce other bias effects, such as gray value effects caused by acquisition parameters or pathologies. Ultimately, this framework can serve as a tool for generating datasets to facilitate analysis of how deep learning models handle various sources of bias. With complete control over the number of samples, types of bias, number of subgroups with different biases, intersections of biased subgroups, and strength and proportion of bias in target classes, datasets generated with this framework can be used as a tool for evaluating how proposed or state-of-the-art models are affected by biases in terms of performance, explainable AI, uncertainty, etc., as well as for benchmarking bias mitigation and data harmonization strategies on a wide range of realistic, controlled scenarios. The contributions of this paper are summarized as follows: "
A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis,2.0,Methods,"The purpose of the proposed framework is to generate a dataset for a multi-class classification problem consisting of synthetic T1-weighted MR images, with N images I i and associated labels corresponding to m disease classes. For simplicity, in this description of the methods, we focus on the binary classification task (m=2) with disease labels corresponding to disease (D) and no disease (ND). All images are derived by applying non-linear diffeomorphic transformations to a template image I T , which represents an average brain morphology. More specifically, we consider three types of transformations: (1) ϕ S , a subject morphology, (2) ϕ D , a disease (target) effect, and (3) ϕ B , a bias effect. ϕ S is a global nonlinear transformation that deforms I T into a (simulated) subject morphology. In contrast, ϕ D and ϕ B are spatially localized deformations that only modify I T locally to introduce an effect (ϕ D ) that can be used to differentiate disease classes, and a bias effect (ϕ B ). In our setup, each synthetic image is generated by sampling the transformations ϕ S , ϕ D , and optionally ϕ B from dedicated generative models (Fig.  Principal Component Analysis-Based Generative Models for Simulating Effects/Variability. To apply anatomically realistic morphological deformations to our template neuroimaging dataset in this work, we fit a principal component analysis (PCA) to the velocity fields of real T1-weighted MR images of different healthy subjects, which were non-linearly registered to the template image I T . We treat the resulting low-dimensional affine subspace model as a generative model and sample velocity fields representing a range of real anatomical variation from it. For region-specific effects (ϕ D and ϕ B ), the real T1-weighted MR image velocity fields within the regions defined by a label atlas are masked prior to PCA fitting, whereas the full brain is used in the PCA model for simulating subject morphology (ϕ S ). Thus, by sampling velocity fields v D , v B , and v S from the latent space of the respective subspace models, we can model disease, bias, and subject morphology as variations within an expected extent of human neuroanatomy. Disease and Bias Effects. We model disease (ϕ D ) and bias (ϕ B ) effects as morphological deformations localized to specific brain regions. We also assume that datasets belonging to each disease class have these localized effects sampled from respective distributions in a bimodal Gaussian mixture model within the PCA subspace of the disease effect model. We assume that bias effects are introduced as an additional morphological deformation in a separate brain region, and that these effects are sampled from a Gaussian distribution within the PCA subspace of the bias effect model. In general, an arbitrary number of target classes and bias groups can be introduced to the datasets in a similar sampling procedure. Subject Morphology. To better emulate anatomical variation in clinical data and warrant the use of deep learning models, global morphological variation representing distinct subjects (ϕ S ) are applied to the entire anatomy within I T . These are also sampled from a Gaussian distribution within the PCA subspace of the dedicated subject morphology model. Introducing Effects to the Template Image. The sparsely defined velocity fields for the disease and bias effects, v D and v B , are densified using the scattered grid B-spline method "
A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis,3.0,Experiments and Results,"To evaluate our synthetic datasets in a deep learning pipeline, we trained a CNN to predict whether images from biased datasets belong to the disease (D) or no disease (ND) class. More precisely, we evaluated (1) how the proportion of datasets containing bias features within the D class, and (2) how the spatial proximity between the disease region and bias region affect the performance and explainability (XAI) results of a CNN trained to classify D from ND cases (Fig. "
A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis,,Model and Training.,"A CNN was used as a model for predicting whether datasets belonged to the D or ND class. The model consisted of 5 blocks each containing a convolutional layer with (3×3×3) kernel, batch normalization, sigmoid activation, and (2×2×2) max pooling. The convolutional filter sizes were 32, 64, 128, 256, and 512 for each respective block. The sixth block contained average pooling, dropout (rate=0.2), and a dense classification layer with softmax activation. Binary cross entropy loss, Adam optimizer (learning rate = 1e -4 ), and batch size 4 with early stopping based on validation loss (patience=30) were used for training. Each experiment simulated and used 500 datasets of voxel dimensions (173×211×155) with a 55%/15%/30% train/validation/test split, stratified by disease and bias labels. Evaluation. Model performance was evaluated using accuracy, sensitivity, and specificity computed for the aggregate test set, as well as separately for the bias (B) and no bias (NB) groups. Results are reported as the mean ± standard deviation of the models with 5 different weight initialization seeds on the same train/validation/test splits, following  Results and Discussion. The results of our evaluation are summarized in Fig. "
A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis,4.0,Conclusion,"In this work, we presented a flexible and modular framework for simulating bias in medical imaging datasets using realistic morphological effects in neuroimaging as a use case. By sampling brain region-specific morphological variation representing the disease state and bias features from generative models in a controlled manner, we can generate synthetic datasets of arbitrary size and composition, which enables the investigation of a vast range of dataset bias scenarios and corresponding impacts on deep learning pipelines. Directions for future work with this framework are extensive and could include the analysis of more variations of bias proportions and proximities on alternate model architectures (e.g., vision transformers), evaluation of state-of-the-art bias mitigation strategies on various dataset compositions, as well as assessing other potential limitations of explainability methods as a tool for investigating bias. We believe that our work provides a strong foundation for advancing understanding of bias in deep learning for medical image analysis and consequently developing responsible models and methods for clinical use."
A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 46.
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,1.0,Introduction,"Quantitative Magnetic Resonance Imaging (QMRI) is used to identify tissue's intrinsic physical properties, including the spin-lattice magnetic relaxation time (T1), and the spin-spin magnetic relaxation time (T2)  Despite various benefits, most QMRI approaches suffer from slow imaging speeds, and usually provide only a single intrinsic property at a time (e.g., quantification of T1 alone, followed by T2 alone), resulting in low throughput. Magnetic Resonance Fingerprinting (MRF) provides an alternative QMRI framework to achieve multi-property quantification simultaneously  In this work, we propose an MRI physics-regularized deep learning model for fast and robust MRF, called BlochNet as shown in Fig.  The rationale underlying the design is that domain knowledge such as wellfounded physics principles bring additional useful constraints that can effectively reduce the solution space of an inverse problem. This contributes to an optimized solution, in particular, for an (ill-posed) inverse problem  -The proposed BlochNet incorporates an MRI physics model to the decoding mechanism, which plays a role to regularize the training of the encoder. We expect that such a physics-based design can provide useful training signals for the encoder to better solve the MRF problems. -We improve the efficiency of the implementation of the Bloch equations, which reduce the computation overhead such that the MRI physics-based decoding model can be used directly as a differentiable module and trained end-to-end in neural networks (e.g., the decoder in BlochNet). -Compared to existing methods, BlochNet shows consistently better generalization performance across synthetic, phantom and real MRF data, and across different types of RF pulse sequences."
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,2.0,Background and Related Works,"Since tissue properties lead to magnetic responses according to the MRI dynamics, quantifying tissue's properties via QMRI/MRF is a typical inverse problem, also an anti-causal task. The core idea of MRF is based on the fact that for each specific tissue, a pseudo-random pulse sequence leads to a unique magnetic response (i.e., magnetization along the temporal dimension) which can serve as an identifiable signal signature, analogous to a ""fingerprint"" for the corresponding tissue. Once the unique identifiable magnetic responses are obtained, the estimation of tissue properties reduces to a pattern recognition problem. Various approaches have been developed to solve the MRF problem, using either model-based techniques, e.g., dictionary matching (DM), compressive sensing, or learning-based / data-driven techniques. Model-Based Approaches. In the original MRF work  Learning-Based Approaches. To address some shortcomings of model-based methods, learning-based approaches have been proposed for fast MRF by replacing the dictionary with a compact neural network. In particular, motivated by the success of deep learning in a number of tasks, there is an emerging trend  Physics-Informed Learning. Another highly related line of research is modelbased learning "
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,3.1,BlochNet: Regularized Networks by Physics-Based Decoding,"The proposed BlochNet adopts a supervised encoder-decoder framework where the encoder solves the primary inverse problem that predicts tissue properties from input magnetic responses, while the decoder solves an auxiliary task that reconstructs the inputs from the estimated tissue properties using the Bloch equations-based MRI physics model. We highlight that a sophisticated MRI physics model is tailored and exploited as the decoder. The rationale behind such a design lies in the fact that the data generation mechanism represented by MRI physics is a useful constraint that can effectively reduce the solution space of the inverse problem. Therefore, the physics-based decoder will act as a strong regularizer that can provide informative feedback and contribute to the training of a better encoder. We expect the physics prior can introduce a better and generalizable inductive bias to the encoder. Similar ideas have been explored in some other domains  Specifically, in the proposed approach, the encoder uses a three-layer fully connected neural network to address the inverse problem that predicts T1, T2 tissue properties from input magnetic responses. Given an enquiry magnetic response X n ∈ C L for the n-th voxel where L denotes the length of each magnetic response, e.g. L = 1000 in our experiments, the encoder E outputs predicted tissue properties Θn = { T 1 n , T 2 n } ∈ R p where p = 2 denotes the number of tissue properties to be predicted. Note that, the estimation of tissue properties Θ from magnetic responses X requires long enough sequences L > p to create identifiable signal evolutions that distinguish different tissues. Hence, this operation nonlinearly maps the magnetic responses from a high-dimensional manifold to a low-dimensional manifold. In contrast, the decoder reconstructs the input magnetic responses from the estimated tissue properties Θn by solving the Bloch equations  where M = [M x , M y , M z ] denotes the magnetization vector. M 0 denotes the equilibrium magnetization; B denotes the magnetic field; and γ denotes the gyromagnetic ratio. (More details of Bloch equations are provided in the Appendix.)"
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,3.2,Fast EPG for Solving Bloch Equations,"Since there is no general analytic solution to the Bloch equations, numerical solutions such as EPG formalism are often adopted. However, a limitation of the released EPG implementation  Instead, we adapt the EPG implementation "
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,3.3,Loss Function,"The loss function consists of two parts: the mean squared error (MSE) between the ground truth and the predicted tissue properties, referred to as embedding loss, and the MSE between the input and the reconstructed signatures, referred to as reconstruction loss,"
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,4.0,Experiment Results,"In this section, we perform an evaluation of the proposed method and conduct a comparison with other state-of-the-art MRF methods. We evaluate the generalization performance of all models across different data distributions and different RF pulse sequences. The evaluation metric is the MSE in log-scale, and therefore, the unit is the squared millisecond in log-scale. For our model, we use 3-layer encoder-decoder with varying hidden units, Adam optimizer (lr=1e-3), and maximum epochs of 100 with early stopping based on the validation set on GTX 1080 Ti. We performed ten independent trials, the results of which are presented in Table "
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,4.1,Data Settings and Baseline Methods,"We exploit three types of data including synthetic data, phantom MRI data, and anatomical MRI data. In particular, synthetic data (around 80,000 samples) is used for training, while phantom data (85,645 samples) and anatomical data (7,499 samples) is for evaluation. More details about the three datasets are provided in the Appendix. We compare our approach with 6 representative state-of-the-art MRF methods, including dictionary matching (DM) "
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,4.2,Experiments of Evaluating Generalization Performance,"We evaluate the generalization performance of various models on two types of experiment settings: 1) across different data distributions, including synthetic, phantom and anatomical MRF data; 2) across different RF pulse sequences with different flip angles. In addition, a series of ablation studies were conducted via comparison with other methods that use different types of decoders, as shown in Table  Generalization Across Different Data Distributions. Due to limited anatomical data with ground truth T1, T2 values, it is common practice to use a large amount of synthetic data to train models to avoid potential overfitting, and then perform validation on anatomical data  Figure  In spite of degraded performance for all models under different train and test RF pulse sequences, the results clearly show the advantage of autoencoder (FC-FC or RNN-RNN) models over non-autoencoder models(FC or RNN), which confirms the benefits of incorporating a decoder to derive the reconstruction loss as additional regularization. Furthermore, the proposed BlochNet demonstrates significant gains over competing methods in such challenging cases on both phantom and anatomical MRF data. In Fig. "
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,5.0,Discussion,We present statistical significance tests on 10 trials for pairwise group comparisons using Tukey HSD test after the Normality Test and repeated ANOVA. For results in Table 
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,6.0,Conclusion,"We propose BlochNet, a novel MRI physics-informed learning model, which consistently outperforms competing methods with better robustness and generalizability in MRF problems. In future work, we will consider k-space subsampling and incorporating spatial information for faster and more efficient QMRI/MRF."
Physics-Based Decoding Improves Magnetic Resonance Fingerprinting,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 42.
DeDA: Deep Directed Accumulator,1.0,Introduction,"Over the past decade, we have observed the substantial success of Convolutional Neural Networks (CNNs) in a multitude of grid-based medical imaging applications, such as magnetic resonance imaging (MRI) reconstruction  In this study, we strive to answer this question by addressing the identification problem associated with a specific type of multiple sclerosis (MS) lesion, referred to as a chronic active lesion, or rim+ lesion. Histopathology studies characterize rim+ lesions by an iron-rich rim of activated macrophages and microglia  Given the limited amount of data and high class imbalance, it's more advantageous to explicitly incorporate domain knowledge into the network as priors. As illustrated in Fig.  In this work, we introduce the Deep Directed Accumulator (DeDA), a novel image processing operation. DeDA, symmetric to grid sampling within a neural network's forward-backward framework, explicitly encodes the aforementioned prior information. Given a feature map and sampling grids, DeDA creates an accumulator space, quantizes it into finite intervals, and accumulates feature values. DeDA can also be viewed as a generalized discrete Radon transform, as it accumulates values between two discrete feature spaces. Our contributions are twofold: Firstly, we present DeDA, a simple yet powerful method that augments neural networks' representation capacity by explicitly incorporating domain-specific priors. Secondly, our experimental results on rim+ lesion identification demonstrate a notable improvement of 10.1% in partial area under the receiver operating characteristic curve (pROC AUC) and a 10.2% improvement in area under the precision recall curve (PR AUC), outperforming existing state-of-the-art methods."
DeDA: Deep Directed Accumulator,2.0,Methodology,"Numerous signal processing techniques, including the Fourier transform, Radon transform, and Hough transform, map discrete signals from image space to another functional space. We call this new space accumulator space, where each cell's value in the new space constitutes a weighted sum of values from all cells in the original image space. For our purposes, an appealing feature of the accumulator space is that local convolutions within it, like those in Hough and sinogram spaces, result in global aggregation of structural features, such as lines, in the feature map space. This proves beneficial for incorporating geometric priors into neural networks. Differing from attention-based methods, this convolution in accumulator space explicitly captures long-range information through direct geometric prior parameterization."
DeDA: Deep Directed Accumulator,2.1,Differentiable Directed Accumulation,"The process of transforming an image to an accumulator space involves a critical step, directed accumulation (DA), in which a cell from the accumulator space is pointed by multiple cells from the image space. Figure  Grid Sampling: Given a source feature map U ∈ R C×H×W , a sampling grid G ∈ R 2×H ×W = (G x , G y ) specifying pixel locations to read from U, and a kernel function K() defining the image interpolation, then the output value of a particular position (i, j) at the target feature map V ∈ R C×H ×W can be written as follows: where the kernel function K() can be replaced with any other specified kernels, e.g. integer sampling kernel δ( Here x + 0.5 rounds x to the nearest integer and δ() is the Kronecker delta function. The gradients with respect to U and G for back propagation can be defined accordingly  the number of grids), and a kernel function K(), the output value of a particular position (i, j) at the target feature map V can be written as follows: It is worth noting that the spatial dimension of the grid G[k] should be the same as that of U, but the first dimension of G[k] can be an arbitrary number as long as it aligns with the number of spatial dimensions of V, e.g. if given Basically, the DeDA operation in Eq. (  To allow back propagation for training networks with DeDA, the gradients with respect to U are derived using the chain rule as follows: The gradient tensor with respect to V is A. The structure of Eq. (  where and N = max(H, W ). Now the DeDA-based transformation of Rim (DA-TR) can be formulated as V s = D(S, G; K) and V u = D(U, G; K), where the integer sampling kernel is used. It is worth noting that feature and gradient magnitude values are accumulated separately due to differences of image intensity and gradients between rim+ and rim-lesions (see Fig. "
DeDA: Deep Directed Accumulator,,Network Layer for DA-TR:,"To gain more representation ability and capture long-range contextual information, DA-TR is applied to both intermediate feature maps and original images. As can be seen from Fig. "
DeDA: Deep Directed Accumulator,3.0,Experiments and Results,"For fair and consistent comparison, the dataset applied in the previous work "
DeDA: Deep Directed Accumulator,3.1,Comparator Methods and Implementation Details,"Comparator Methods: Three methods have been developed so far for rim+ lesion identification, of which APRL  Implementation Details: A stratified five-fold cross-validation procedure was applied to train and validate the performance, and all experiments including ablation study were carried out within this setting. Each lesion was cropped into patches with a fixed size of 32 × 32 × 8 voxels. Random flipping, random affine transformation and random Gaussian blurring were used to augment our data. More details of the training procedure can be found out in the supplementary materials."
DeDA: Deep Directed Accumulator,3.2,Results and Ablation Study,"Lesion-wise Results: To evaluate the performance of each method and produce clinically relevant results, pROC curves with false positive rates (FPRs) in the range of (0, 0.1) and PR curves of the different validation folds were interpolated using piece-wise constant interpolation and averaged to show the overall performance at the lesion level. For each curve, AUC was computed directly from the interpolated and averaged curves. The binary indicators of rim+/rimlesions were generated by thresholding the model probabilities to maximize the F 1 score, where F 1 = 2• precision • sensitivity precision + sensitivity . In addition, accuracy, F1 score, sensitivity, specificity, and precision were used to characterize the performance of each method. Table "
DeDA: Deep Directed Accumulator,,Subject-wise Results:,We also evaluated the performance at the subjectlevel. Pearson's correlation coefficient was used to measure the correlation model predicted count and human expert count. Mean Squared Error (MSE) was also used to measure the averaged accuracy for the model predicted count. Figure 
DeDA: Deep Directed Accumulator,,Ablation Study:,"We conducted an ablation study to investigate the effects of each component accompanied with DA-TR. First, we examined the effects of applying the proposed DA-TR to the latent feature maps and raw images. Second, we examined the effects of using V u and V s , because rim+ lesions differ from rim-lesions in both gradient magnitudes and values at the edge of the lesion. We then investigated how multi-radius rim parameterization can affect the results, as the size of rim+ lesions vary greatly with a radius from 5 to 15 among different subjects. Results from models #1, #2 and #4 show that the rim parametrization DA-TR is useful for rim+ identification, and DA-TR used in the latent feature map space performs even better. Comparing model #3 and #4, one can see that accumulating both gradient magnitudes and feature values is beneficial. The consistent performance improvement from model #4 to #5 and from model #5 to #6 has demonstrated the effectiveness of applying multi-radius rim parameterization. More results on backbone networks can be found in the appendix."
DeDA: Deep Directed Accumulator,3.3,Discussions,"Medical images often require processing of a primary target or region of interest (ROI), such as rims, left ventricles, or tumors. These ROIs frequently exhibit distinct geometric structures  While the study focuses on rim+ lesion identification, the proposed DeDA can be extended to other applications. These include the utilization of polar transformation for skin lesion recognition/segmentation, symmetric circular transformation for cardiac image registration "
DeDA: Deep Directed Accumulator,4.0,Conclusions,"We present DeDA, an image processing operation that helps parameterize rim and effectively incorporates prior information into networks through a value accumulation process. The experimental results demonstrate that DeDA surpasses existing state-of-the-art methods in all evaluation metrics by a significant margin. Furthermore, DeDA's versatility extends beyond lesion identification and can be applied in other image processing applications such as Hough transform, bilateral grid, and Polar transform. We are excited about the potential of DeDA to advance numerous medical applications and other image processing tasks."
DeDA: Deep Directed Accumulator,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 72.
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,1.0,Introduction,"Deep learning techniques have achieved unprecedented success in the field of medical image classification, but this is largely due to large amount of annotated data  Active learning (AL) is an effective approach to address this issue from a data selection perspective, which selects the most informative samples from an unlabeled sample pool for experts to label and improves the performance of the trained model with reduced labeling cost  Recently, Ning et al.  In this paper, we propose a novel AL framework under an open-set scenario, and denote it as OpenAL, which cannot only query as many target class samples as possible but also query the most informative samples from the target classes. OpenAL adopts an iterative query paradigm and uses a two-stage sample selection strategy in each query. In the first stage, we do not rely on a detection network to select target class samples and instead, we propose a feature-based target sample selection strategy. Specifically, we first train a feature extractor using all samples in a self-supervised learning manner, and map all samples to the feature space. There are three types of samples in the feature space, the unlabeled samples, the target class samples labeled in previous iterations, and the non-target class samples queried in previous iterations but not being labeled. Then we select the unlabeled samples that are close to the target class samples and far from the non-target class samples to form a candidate set. In the second stage, we select the most informative samples from the candidate set by utilizing a model-based informative sample selection strategy. In this stage, we measure the uncertainty of all unlabeled samples in the candidate set using the classifier trained with the target class samples labeled in previous iterations, and select the samples with the highest model uncertainty as the final selected samples in this round of query. After the second stage, the queried samples are sent for annotation, which includes distinguishing target and non-target class samples and giving a fine-grained label to every target class sample. After that, we train the classifier again using all the fine-grained labeled target class samples. We conducted two experiments with different matching ratios (ratio of the number of target class samples to the total number of samples) on a public 9-class colorectal cancer pathology image dataset. The experimental results demonstrate that OpenAL can significantly improve the query quality of target class samples and obtain higher performance with equivalent labeling cost compared with the current state-of-the-art AL methods. To the best of our knowledge, this is the first open-set AL work in the field of pathology image analysis."
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,2.0,Method,"We consider the AL task for pathology image classification in an open-set scenario. The unlabeled sample pool P U consists of K classes of target samples and L classes of non-target samples (usually, K < L). N iterative queries are performed to query a fixed number of samples in each iteration, and the objective is to select as many target class samples as possible from P U in each query, while selecting as many informative samples as possible in the target class samples. Each queried sample is given to experts for labeling, and the experts will give fine-grained category labels for target class samples, while only giving a ""non-target class samples"" label for non-target class samples. "
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,2.1,Framework Overview,Figure 
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,2.2,Feature-Based Target Sample Selection,"Self-supervised Feature Representation. First, we use all samples to train a feature extractor by self-supervised learning and map all samples to the latent feature space. Here, we adopt DINO  Sample Scoring and Selection in the Feature Space. Then we define a scoring function on the base of the distribution of unlabeled samples, labeled target class samples and non-target class samples queried in previous iterations. Every unlabeled sample in the current iteration is given a score, and a smaller score indicates that the sample is more likely to come from the target classes. The scoring function is defined in Eq. 1. where s i denotes the score of the unlabeled sample x U i . s ti measures the distance between x U i and the distribution of features derived from all the labeled target class samples. The smaller s ti is, the closer x U i is to the known sample distribution of the target classes, and the more likely x U i is from a target class. Similarly, s wi measures the distance between x U i and the distribution of features derived from all the queried non-target class samples. The smaller s wi is, the closer x U i is from the known distribution of non-target class samples, and the less likely x U i is from the target class. After scoring all the unlabeled samples, we select the top ε% samples with the smallest scores to form the candidate set. In this paper, we empirically take twice the current iterative labeling budget (number of samples submitted to experts for labeling) as the sample number of the candidate set. Below, we give the definitions of s ti and s wi . Distance-Based Feature Distribution Modeling. We propose a category and Mahalanobis distance-based feature distribution modeling approach for calculating s ti and s wi . The definitions of these two values are slightly different, and we first present the calculation of s ti , followed by that of s wi . For all labeled target class samples from previous iterations, their fine-grained labels are known, so we represent these samples as different clusters in the feature space according to their true class labels, where a cluster is denoted as C L t (t = 1, . . . , K). Next, we calculate the score s ti for z U i using the Mahalanobis distance (MD) according to Eq. 2. MD is widely used to measure the distance between a point and a distribution because it takes into account the mean and variance of the distribution, which is very suitable for our scenario. where D(•) denotes the MD function, μ t and Σ t are the mean and covariance of the samples in the target class t, and Nom(•) is the normalization function. It can be seen that s ti is essentially the minimum distance of the unlabeled sample x U i to each target class cluster. For all the queried non-target class samples from previous iterations, since they do not have fine-grained labels, we first use the K-means algorithm to cluster their features into w classes, where a cluster is denoted as C L w (w = 1, . . . , W ). W is set to 9 in this paper. Next, we calculate the score s wi for z U i using the MD according to Eq. 4. ) where μ w and Σ w are the mean and covariance of the non-target class sample features in the wth cluster. It can be seen that s wi is essentially the minimum distance of z U i to each cluster of known non-target class samples. The within-cluster selection and dynamic cluster changes between rounds significantly enhance the diversity of the selected samples and reduce redundancy."
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,2.3,Model-Based Informative Sample Selection,"To select the most informative samples from the candidate set, we utilize the model-based informative sample selection strategy in Stage 2. We measure the uncertainty of all unlabeled samples in the candidate set using the classifier trained in the last iteration and select the samples with the highest model uncertainty as the final selected samples. The entropy of the model output is a simple and effective way to measure sample uncertainty "
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,3.1,"Dataset, Settings, Metrics and Competitors","To validate the effectiveness of OpenAL, we conducted two experiments with different matching ratios (the ratio of the number of samples in the target class to the total number of samples) on a 9-class public colorectal cancer pathology image classification dataset (NCT-CRC-HE-100K)  Metrics. Following  As defined in Eq. 5, precision is the proportion of the target class samples among the total samples queried in each query and recall is the ratio of the number of the queried target class samples to the number of all the target class samples in the unlabeled sample pool. where k m denotes the number of target class samples queried in the mth query, l m denotes the number of non-target class samples queried in the mth query, and n target denotes the number of target class samples in the original unlabeled sample pool. Obviously, the higher the precision and recall are, the more target class samples are queried, and the more effective the trained target class classifier will be. We measure the final performance of each AL method using the accuracy of the final classifier on the test set of target class samples. Competitors. We compare the proposed OpenAL to random sampling and five AL methods, LfOSA "
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,3.2,Performance Comparison,Figure 
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,3.3,Ablation Study,"To further validate the effectiveness of each component of OpenAL, we conducted an ablation test at a matching ratio of 33%. Figure  It can be seen that the distance modeling of both the target class samples and the non-target class samples is essential in the FTSS strategy, and missing either one results in a decrease in performance. Although the MISS strategy does not significantly facilitate the selection of target class samples, it can effectively help select the most informative samples among the samples in the candidate set, thus further improving the model performance with a limited labeling budget. In contrast, when the samples are selected based on uncertainty alone, the performance decreases significantly due to the inability to accurately select the target class samples. The above experiments demonstrate the effectiveness of each component of OpenAL."
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,4.0,Conclusion,"In this paper, we present a new open-set scenario of active learning for pathology image classification, which is more practical in real-world applications. We propose a novel AL framework for this open-set scenario, OpenAL, which addresses the challenge of accurately querying the most informative target class samples in an unlabeled sample pool containing a large number of non-target samples. OpenAL significantly outperforms state-of-the-art AL methods on real pathology image classification tasks. More importantly, in clinical applications, on one hand, OpenAL can be used to query informative target class samples for experts to label, thus enabling better training of target class classifiers under limited budgets. On the other hand, when applying the classifier for future testing, it is also possible to use the feature-based target sample selection strategy in the OpenAL framework to achieve an open-set classifier. Therefore, this framework can be applied to both datasets containing only target class samples and datasets also containing a large number of non-target class samples during testing."
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,1.0,Introduction,"Deep neural networks have greatly advanced medical image analysis in recent years  To this end, we study a practical and challenging domain adaptation problem which explores transferable knowledge from multiple source domains to target domain with only pre-trained source models rather than the source data, namely multi-source model adaptation (MSMA). Although MSMA methods  To address this problem, we develop a novel Transferability-Guided Model Adaptation (TGMA) model, which represents the first attempt to solve MSMA in medical image segmentation. Specifically, a label-free transferability metric (LFTM) is designed to evaluate the relevance between source and target domain without access to the source data. Based on the designed LFTM, we can compute instance-level transferability matrix (ITM) to achieve pseudo-label correction for precise supervision, and domain-level transferability matrix (DTM) to accomplish model selection for better target initialization. To this end, we can achieve adaptation to unlabeled target domain with clean pseudo label and proper model initialization. The main contributions are summarized as: "
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,2.0,Method,"In MSMA scenario, we address the problem of jointly adapting multiple segmentation models, trained on a variety domains, to a new unlabeled target domain. Formally, let us consider we have a set of source models {F sj } M j=1 , where the j th model {F sj } is a segmentation model learned using the source dataset , with N j data points, where x i sj and y i sj denote the i-th source image and the corresponding segmentation label respectively. Now, given a target unlabeled dataset D t = {x t i } Nt i=1 , the problem is to learn a segmentation model F t , using only the learned source models, without any access to the source dataset. Figure  To eliminate negative transfer by domain-dissimilar source models, we design a label-free transferability metric to evaluate the transferability of source models in an unsupervised manner for the first time. Before target training, an instancelevel transferability matrix (ITM) is computed to rectify target pseudo labels, and a domain-level transferability matrix (DTM) is calculated to achieve model selection for better model initialization. Based on the rectified pseudo labels and selected models, target segmentation model is trained with dice loss to achieve model adaptation."
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,2.1,Label-Free Transferability Metric,"Most of multi-source model adaptation approaches  Given unlabeled target data D t = {x i t } N i=1 and a pre-trained source segmentation model F s k , we import them to the LFTM estimator and compute the transferability metric LF T M (x t , F s k ) with only twice forwards as shown in Fig.  where M (x t ) is the masking operation to randomly remove pixels. In the second forward process, the masked target sample x m k t is passed into the source model Then we calculate the dice score between these two predictions as transferability metric: The larger the LFTM is, the more stable the source model is on the target sample. With M source models and N t target samples, we can compute the instance-level transferability matrix (ITM) T instance ∈ R M ×Nt , which can be utilized to correct target pseudo labels. Averaging T instance on the domain-space can generate domain-level transferability matrix (DTM) T domain ∈ R M ×1 , which represents the contribution of each source model to the target domain. The detailed process is illustrated in Transferability Matrix Estimation of Fig. "
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,2.2,Transferability-Guided Model Adaptation,"The basic pipeline for target training needs accurate pseudo labels and suitable model initialization. While there are multiple pseudo labels and source models, simply averaging them as target supervision and model initialization is trivial solution, which ignores the contribution differences of these source domains. To tackle this problem, we propose a transferability-guided model adaptation (TGMA) framework on the basis of LFTM, which consists of two modules: Label Correction and Model Selection. Based on the instance-level transferability matrix T instance , we re-weight the pseudo labels generated by multiple source models to achieve pseudo label correction. With the domain-level transferability matrix T domain , we select the most portable source model as the main model initialization and make full use of other source models by weighted optimization strategy. Transferability-Guided Label Correction. In MSMA, we generate pseudo labels as supervision because no target ground truth is available. However, with multiple pseudo labels predicted by source models for a target sample, prior works  Taking a target sample x t for example, we pass this sample to source models {F s1 , F s2 , ..., F sM } to obtain corresponding predictions {P s1 , P s2 , ..., P sM }. We take argmax operation on these predictions to generate one-hot pseudo labels {y s1 , y s2 , ..., y sM }, where y = argmax(P ). The instance-level transferability matrix T instance is applied on these pseudo labels to achieve noise correction by contribution re-weighting: where each pseudo label is weighted by the corresponding LFTM score for better combination. This strategy largely prevents the negative transfer problem caused by noisy labels of those domain-irrelevant source models. Transferability-Guided Model Selection. Previous MSMA methods  where L dice is calculated on the combined target prediction and corresponding pseudo label. This loss optimizes model parameter W main * F main + W aux * F aux . The model selection strategy choose optimal source model while makes full use of those sub-optimal source models for better model initialization, thus avoiding the negative transfer by those domain-irrelevant domains."
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,3.1,Dataset,Extensive experiments are conducted to verify the effectiveness of our proposed framework on Prostate MR (PMR) dataset which is collected and labeled from six different public data sources for prostate segmentation 
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,3.2,Implementation Details and Evaluation Metrics,The framework is implemented with Pytorch 1.7.0 using an NVIDIA RTX 2080Ti GPU. Following 
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,3.3,Comparison with State-of-the-Arts,"We compare our methods to several domain adaptation frameworks, including the single source-free domain adaptation (SFDA)  We observe that our model with transferability guidance can well eliminate the negative transfer interference by some domain-irrelevant domains. "
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,3.4,Ablation Analysis,"The performance improvement mainly comes from our designed LFTM to detect negative transfer. There are some other unsupervised metrics that can evaluate model stability, such as entropy, rotation-consistency and crop-consistency. To better evaluate the effectiveness of LFTM, we apply these unsupervised metrics to estimate ITM and DTM for label correction and model selection. The comparison results are shown in Table  Removing dilation operation leads to 1.71% performance degradation on Average Dice, revealing the effect of receptive field."
Transferability-Guided Multi-source Model Adaptation for Medical Image Segmentation,4.0,Conclusion,"In this paper, we study a practical domain adaptation problem, named multisource model adaptation where only multiple pre-trained source segmentation models rather than the source data are provided for adaptation to unlabeled target domain. To eliminate the negative transfer by domain-dissimilar source models, we design a label-free transferability metric based on the attentive masking consistency to evaluate the transferability of each source segmentation model with only target images. Using this metric, we calculate two types of transferability matrices: an instance-level matrix to adjust the target pseudo label, and a domain-level matrix to choose an optimal subset for improved model initialization."
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,1.0,Introduction,"One-shot federated learning (FL) allows a global model to be trained through a single communication round without sharing data between clients  To prevent global model overfitting, we attempt to leverage random noise as a training source for KD (see Fig.  The contributions are as follows: (i) We propose one-shot FL leveraging image synthesis with client model adaptation. This allows to transfer knowledge from client models to the global model with synthesized images ranging from random noise to realistic images and contributes to preventing overfitting. (ii) We employ noise-adapted client models using AdaBN to produce a better KD signal for random noise. (iii) Comprehensive experiments on five small-and three large-scale medical image classification datasets consisting of microscopy, dermatoscopy, oct, histology, x-ray, and retinal images reveal that our method outperforms state-of-the-art one-shot FL methods. Related Work. Due to the challenges of one-shot FL, prior methods were trained on public data "
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,2.0,Method,"The overall training processes are shown in Fig.  k with respect to data D k , the objective of FL is to train a global model W g , which represents all data D = {D 1 , . . . , D k }. Motivated by  where L CE , L BN , and L T V are cross-entropy, BN, and total variation losses  Noise Adaptation. BN  where α represents momentum and x is a sample stored in memory. Initially, μ and σ2 are set to μ and σ 2 . The samples in memory ranging from characteristic images for D to N (0, 1) by gradually adjusting μ and σ2 towards N (0, 1) through Eq. 2 for I steps. With this in mind, we now describe how to train the global model. "
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,,Global Model Training. KD allows to train a global model with multiple client models,"where λ denotes a noise level stored in memory. L KD (x; W c , W g ) denotes the Kullback-Leibler divergence between p(x; W c ) and p(x; W g ) where p(•) is an ensemble (averaging) prediction of given models with a temperature on softmax inputs "
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,3.0,Experiments,"Datasets. For evaluation, we use five small-scale (28×28) medical image classification datasets i.e., Blood, Derma, Oct, Path, and Tissue from MedMNIST  Experimental Settings. We explore three scenarios i.e., (i) data heterogeneity levels, (ii) impact on large-scale datasets, and (iii) model heterogeneity i.e., each client has different architectures. In (i), Blood, Derma, Oct, Path, and Tissue datasets are used with Independent and Identically Distributed (IID) clients and Dirichlet distributed  Comparison Methods. We employ three one-shot FL methods: FedAvg  Implementation Details. We used ResNet18 "
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,3.1,Main Results,Table  FedISCA outperforms all one-shot FL methods across all datasets regardless of the level of heterogeneity. In Table 
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,4.0,Conclusion,"We present a novel one-shot FL framework that uses image synthesis and client model adaptation with KD. We demonstrate that (i) random noise significantly reduces the risk of overfitting, resulting in robust global model training; (ii) noiseadapted client models enhance the KD signal leading to high accuracy; and (iii) through experiments on eight datasets, our method outperforms the state-ofthe-art one-shot FL methods on medical data. Further investigation into severe heterogeneity in clients will be a topic of future research."
One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_49.
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,1.0,Introduction,"Non-nutritive sucking (NNS) is an infant oral sucking pattern characterized by the absence of nutrient delivery  NNS occurs in bursts of 6-12 sucks at 2 Hz per suck, with bursts happening a few times per minute during high activity periods  Our contributions address the fine-grained NNS action recognition problem of classifying 2.5 s video clips, and the NNS action segmentation problem of detecting NNS activity in minute-long video clips. The action recognition method uses convolutional long short-term memory networks for spatiotemporal learning. We address data scarcity and reliability issues in real-world baby monitor footage using tailored infant pose state estimation, focusing on the face and pacifier region, and enhancing it with dense optical flow. The action segmentation method aggregates local NNS recognition signals from sliding windows. We present two new datasets in our work: the NNS clinical in-crib dataset, consisting of 183 h of nighttime in-crib baby monitor footage collected from 19 infants and annotated for NNS activity and pacifier use by our interdisciplinary team of behavioral psychology and machine learning researchers, and the NNS in-the-wild dataset, consisting of 10 naturalistic infant video clips annotated for NNS activity. Figure  Our main contributions are (1) creation of the first infant video datasets manually annotated with NNS activity; (2) development of an NNS classification system using a convolutional long short-term memory network, aided by infant domain-specific face localization, video stabilization, and customized signal enhancement, and (3) successful NNS segmentation on longer clips by aggregating local NNS recognition results across sliding windows."
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,2.0,Related Work,"Current methods for measuring the NNS signal are limited to the pressured transducer approach  Action recognition is the task of identifying the action label of a short video clip from a set of predetermined classes. In our case, we wish to classify short infant clips based on the presence or absence of NNS. As with many action recognition algorithms, our core model is based on extending 2D convolutional neural networks to the temporal dimension for spatiotemporal data processing. In particular, we make use of sequential networks (such as long short-term memory (LSTM) networks) after frame-wise convolution to enhance mediumrange temporal dependencies  Action segmentation is the task of identifying the periods of time during which specified events occur, often from longer untrimmed videos containing mixed activities. We follow an approach to segmentation common in limiteddata contexts, patching together signals from a local low-level layer-our NNS action recognition-to obtain a global segmentation result "
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,3.1,NNS Action Recognition,"The core of our model is the NNS action recognition system shown in Fig.  Preprocessing Module. Our frame-based preprocessing module applies the following transformations in sequence. All three steps are used to produce training data for the subsequent spatiotemporal classifier, but during inference, the data augmentation step is not applicable and is omitted. Smooth Facial Crop. The RetinaFace face detector  Optical Flow. After trimming and augmentation, we calculate the short-time dense optical flow  Spatiotemporal-Based Action Classifier. Finally, the optical flow video is processed by a spatiotemporal model that outputs an action class label (NNS or non-NNS). Two-dimensional convolutional neural networks extract spatial representations from static images, which are then fed in sequence to a temporal convolution network for spatiotemporal processing. The final classification outcome is the output of the last temporal convolution network unit."
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,3.2,NNS Action Segmentation,"To segment NNS actions in mixed videos with transitions between NNS and non-NNS activity, we applied NNS recognition in 2.5 s sliding windows and aggregated results to predict start and end timestamps. This window length provides fine-grained resolution for segmentation while being long enough (26 frames at a 10 Hz frame rate) for consistent human and machine detection of NNS behavior. To address concerns about the coarseness of this resolution, we tested the following window-aggregation configurations, the latter two of which have finer 0.5 s effective resolutions: Tiled: 2.5 s windows precisely tile the length of the video with no overlaps, and the classification outcome for each window is taken directly to be the segmentation outcome for that window. Sliding: 2.5 s windows are slid across with 0.5 s overlaps, and the classification outcome for each window is assigned to its (unique) middle-fifth 0.5 s segment as the segmentation outcome. Smoothed: 2.5 s windows are slid across with 0.5 s overlaps, the classification confidence score for each window is assigned to its middle-fifth 0.5 s segment, a 2.5 s moving average of these confidence scores are taken, then the averaged confidence scores are thresholded for the final segmentation outcome. Tens of thousands of timestamps for NNS and pacifier activity were placed, by two trained behavioral coders per video. For NNS, the definition of an event segment was taken to be an NNS burst: a sequence of sucks with <1 s gaps between. We restrict our subsequent study to NNS during pacifier use, which was annotated more consistently. Cohen κ annotator agreement of NNS events during pacifier use (among 10 pacifier-using infants) averaged 0.83 in 10 s incidence windows, indicating strong agreement by behavioral coding standards, but we performed further manual selection to increase precision for machine learning use, as detailed below From each of these two datasets, we extracted 2.5 s clips for the classification task and 60 s clips for the segmentation task. In the NNS clinical in-crib dataset, we restricted our attention to six infant videos containing enough NNS activity during pacifier use for meaningful clip extraction. From each of these, we randomly drew up to 80 2.5 s clips consisting entirely of NNS activity and 80 2.5 s clips containing non-NNS activity for classification, for a total of 960; and five 60 s clips featuring transitions between NNS and non-NNS activity for segmentation, for a total of 30; redrawing if available when annotations were not sufficiently accurate. In the NNS in-the-wild dataset, we restricted to five infants exhibiting sufficient NNS activity during pacifier use, from which we drew 38 2.5 s clips each of NNS and no NNS activity for classification, for a total of 76; and from two to 26 60 s clips of mixed activity from each infant for segmentation, for a total of 39; again redrawing in cases of poor annotations. The 2.5 s clips for classification are equally balanced for NNS and non-NNS activity to support machine learning training; the 60 s mixed clips intended for segmentation intentionally over-represent NNS compared to its natural incidence rate (see Supp. Table "
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,4.2,NNS Recognition Implementation and Results,"For the spatiotemporal core of our NNS action recognition, we experimented with four configurations of 2D convolutional networks, a 1-layer CNN, ResNet18, ResNet50, and ResNet101  We trained and tested this method with NNS clinical in-crib data from six infant subjects under a subject-wise leave-one-out cross-validation paradigm. Action recognition accuracies are reported on the top left of Table  Optical Flow Ablation. Performance of all models with raw RGB input replacing optical flow frames can be found on the right side of Table "
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,4.3,NNS Segmentation Results,"Adopting the best ResNet18-LSTM recognition model, we tested the three configurations of the derived segmentation method on the 60 s mixed activity clips, under the same leave-one-out cross-validation paradigm on the six infants. In addition to the default classifier threshold of 0.5 used by our recognition model, we tested a 0.9 threshold to coax higher precision, as motivated in Sect. 1. We use the standard evaluation metrics of average precision AP t and average recall AR t based on hits and misses defined by an intersection-over-union (IoU) with threshold t, across common thresholds t ∈ {0.1, 0.3, 0.5} The metrics reveal strong performance from all methods and both confidence thresholds on both test sets. Generally, as expected, setting a higher confidence threshold or employing the more tempered tiled or smoothed aggregation methods favours precision, while lowering the confidence threshold or employing the more responsive sliding aggregation method favours recall. The results are excellent at the IoU threshold of 0.1 but degrade as the threshold is raised, suggesting that while these methods can readily perceive NNS behavior, they are still limited by the underlying ground truth annotator accuracy. The consistency of the performance of the model across both cross-validation testing in the clinical in-crib dataset and the independent testing on the NNS in-the-wild dataset suggests strong generalizability. Figure "
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,5.0,Conclusion,"We present our novel computer vision method for the detection of non-nutritive sucking from videos, with a spatiotemporal action recognition model for classifying short video clips and a segmentation model for determining event timestamps in longer videos. Our work is grounded in our methodological collection and annotation of infant video data from varied settings. We use domain-specific techniques such as dense optical flow and infant state tracking to detect subtle sucking movements and ameliorate a relative scarcity of data. Future work could improve the robustness these methods in challenging examples of NNS activity, such as more ambiguous sucking or sucking while moving. This would require more precisely and reliably annotated data to train and evaluate, which in our experience could be difficult to obtain. An alternative approach would be to aim for more robust but less exacting, split-second results. Beyond improvements to the core NNS detection algorithms, algorithmic extraction of NNS signal characteristics, such as individual suck frequency, strength, duration, and temporal pattern, could further NNS research and one day aid in clinical care."
A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_55.
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,1.0,Introduction,"People perceive the world with signals from different modalities, which often carry complementary information about varying aspects of an object or event of interest. Therefore, collecting and utilizing multimodal information is crucial for Artificial Intelligence to understand the world around us. Data collected from various sensors (e.g., microphones, cameras, motion controllers) are used to identify human activity  In practical application, however, modality missing is a common scenario. Wirelessly connected sensors may occasionally disconnect and temporarily be unable to send any data  Currently, existing fusion strategies to tackle this challenge can be broadly grouped into three categories: the arithmetic strategy, the selection strategy and the convolution strategy. As shown in Fig.  Transformer has achieved success in the field of computer vision, demonstrating that self-attention mechanism has the ability to capture the latent correlation of image tokens. However, no work has explored the effectiveness of self-attention mechanism on the N-to-One fusion, where N is variable during training, rather than fixed. Furthermore, the calculation of self-attention does not require a fixed number of tokens as input, which represents a potential for handling missing data. Therefore, we propose a self-attention based fusion block (SFusion) to tackle the problems of the above fusion strategies. As shown in Fig.  The contributions of this work are: -We propose SFusion, which is a data-dependent fusion strategy without impersonating missing modalities. It can learn the latent correlations between different modalities and builds a shared representation adaptively. -The SFusion is not limited to specific deep learning architectures. It takes inputs from any kind of upstream processing model and serves as the input of the downstream decision model, which enables applying the SFusion to various backbone networks for different tasks. -We provide qualitative and quantitative performance evaluations on activity recognition with the SHL "
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,2.1,Method Overview,"For multiple modalities, let k ∈ K ⊆ {1, 2, . . . , S} index a specific modality, within the available modality set of K, where S is the number of all possible modalities. Given an input f k ∈ R B×C×R f , B and C denote the batch size and the number of channels, respectively. R f represents the shape of feature representation extracted from the k-th modality of a sample data, which can be 1D (L), 2D (H×W), 3D (D×H×W) or higher-dimensional. In addition, I = {f k |k ∈ K} denotes the input set of feature representations from all the available modalities. Our goal is to learn a fusion function F that can project I into a shared feature representation f s , denoted as F (I) → f s . To achieve the goal, we design an N-to-One fusion block, SFusion. The architecture is shown in Fig. "
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,2.2,Correlation Extraction,"Given the feature representation Then, we obtain the concatenation of all the tokens z 0 ∈ R B×T ×C , where T = R × |K|, and |K| denotes the number of available modalities. Given z 0 , the stack of eight self-attention layers (SAL) are introduced to learn the latent multimodal correlations. Each layer includes a multi-head attention (MHA) block and a fully connected feed-forward network (FFN)  (1) Therefore, we get z l ∈ R B×T ×C , which is the last SAL output. By reverting z l to the size of |K| × B × C × R f , we obtain the output I = {f k |k ∈ K} of CE as: where r(•) and split(•) are the reshape and split operations, and I is the set of calculated feature representations f k ∈ R B×C×R f which contains multimodal correlations and has the same size as the original input f k ."
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,2.3,Modal Attention,"Given the calculated feature representations set I , the weight map m k is generated with the modal attention mechanism. Feature representations extracted from different modalities are expected to have different weights for fusion at the voxel level. Therefore, we introduce a modal-wise and voxel-level softmax function to generate the weight maps from I , as shown in Fig.  By element-wise multiplying input feature map f k with the corresponding weight map m k and summing all the modalities, we can obtain a fused feature map f s as: Since the sum of m i 1 , . . . m i |K| is 1, the value range of fused feature representation f s remains stable to improve the robustness for variable input modalities. Moreover, the relative sizes of v i 1 , . . . v i |K| (contain the latent multi-modal correlations learned from the CE module) are retained in the corresponding weights. In particular, when only one modality is available, all the values of the weight map are 1, which means f s = f k (k ∈ K, |K| = 1). In this case, the input feature representation remains unchanged. It enables the backbone network (the upstream processing model and the downstream decision model) to enhance its capability to encode and decode information from different modalities rather than relying on a particular one. It is crucial for variable multimodal data analysis."
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,3.1,Datasets,SHL2019. The SHL (Sussex-Huawei Locomotion) Challenge 2019  BraTS2020. The BraTS2020 
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,3.2,Baseline Methods,"EmbraceNet. In the experiments on activity recognition, we compare SFusion with EmbraceNet "
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,,GFF.,"In the experiments on brain tumor segmentation, we compare SFusion with a gated feature fusion block (GFF)  Our implementations are on an NVIDIA RTX 3090(24G) with PyTorch 1.8.1."
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,3.3,Results,"Activity recognition. We compare SFusion with the EmbraceNet  Brain Tumor Segmentation. The quantitative segmentation results are shown in Table  Ablation Experiments. The correlation extraction (CE) module and the modal attention (MA) module are two key components in SFusion. We evaluate the SFusion without CE and MA, respectively. SFusion without CE denotes that feature representations are directly fed into the MA module (Fig. "
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,4.0,Conclusion,"In this paper, we propose a self-attention based N-to-One fusion block SFusion to tackle the problem of multimodal missing modalities fusion. As a data-dependent fusion strategy, SFusion can automatically learn the latent correlations between different modalities and builds a shared feature representation. The entire fusion process is based on available data without simulating missing modalities. In addition, SFusion has compatibility with any kind of upstream processing model and downstream decision model, making it universally applicable to different tasks. We show that it can be integrated into existing backbone networks by replacing their fusion operation or block to improve activity recognition and achieve brain tumor segmentation performance. In particular, by integrating with SFusion, SF_FDGF achieves the state-of-the-art performance. In the future, we will explore other tasks related to variable multimodal fusion with SFusion."
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,1.0,Introduction,"Automated image segmentation is a fundamental task in many medical imaging applications, such as diagnosis  To address these limitations, few-shot segmentation (FSS) methods have been proposed  We propose a novel volume-informed self-supervised approach for Few-Shot 3D Segmentation (VISA-FSS). Generally, VISA-FSS aims to exploit information beyond 2D image slices by learning inter-slice information and continuous shape changes that intrinsically exists among consecutive slices within a 3D image. To this end, we introduce a novel type of self-supervised tasks (see Sect. 2.2) that builds more varied and realistic self-supervised FSS tasks during training. Besides of generating synthetic queries (like "
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,2.0,Methodology,"In this section, we introduce our proposed VISA-FSS for 3D medical image segmentation. Our method goes beyond 2D image slices and exploits intra-volume information during training. To this end, VISA-FSS designs more varied and realistic self-supervised FSS tasks (support-query pairs) based on two types of transformations: 1) applying a predefined transformation (e.g., geometric and intensity transformation as used in "
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,2.1,Problem Setup,"In FSS, a training dataset D tr = {(x i , y i (l))} Ntr i=1 , l ∈ L tr , and a testing dataset D te = {(x i , y i (l))} Nte i=1 , l ∈ L te are available, where (x i , y i (l)) denotes an imagemask pair of the binary class l. L tr and L te are the training and testing classes, respectively, and L tr ∩ L te = ∅. The objective is to train a segmentation model on D tr that is directly applicable to segment an unseen class l ∈ L te in a query image, x q ∈ D te , given a few support set {(x j s , y j s (l))} p j=1 ⊂ D te . Here, q and s indicate that an image or mask is from a query or support set. To simplify notation afterwards, we assume p = 1, which indicates the number of support images. During training, a few-shot segmenter takes a support-query pair (S, Q) as the input data, where Q = {(x i q , y i q (l))} ⊂ D tr , and S = {(x j s , y j s (l))} ⊂ D tr . Then, the model is trained according to the cross-entropy loss on each support-query pair as follows: L(θ) = -log p θ (y q |x q , S). In this work, we model p θ (y q |x q , S) using the prototypical network introduced in "
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,2.2,Self-supervised Task Generation,"There is a large level of information in a 3D medical image over its 2D image slices, while prior FSS methods  Realistic Tasks. To make the second type of task, we take 2m adjacent slices of the support image x i s , as our query images {x j q } j∈N (i) , where N (i) = {i-m, ..., i-1, i+ 1, i+ m}. These query images can be considered as real deformations of the support image. This encourages the few-shot segmenter to learn intra-volume information contrary to the first type of task. Importantly, pseudo-label generation for consecutive slices is the main challenge. To solve this problem, we introduce a novel strategy called SPPS that propagates the pseudo-label of the support image into query ones. Specifically, we consecutively apply flow field vectors that exist between adjacent image slices on y i s (l) to generate pseudo-label y j q (l) as follows: ) for j > m, and ) for j < m, where φ(x i , x j ) is the flow field vector between x i and x j , which can be computed by deformably registering the two images using VoxelMorph  Loss Function. The network is trained end-to-end in two stages. In the first stage, we train the few-shot segmenter on both types of synthetic and realistic tasks using the segmentation loss employed in  , which is applied on a random query x i q (formed by synthetic or realistic transformations) to predict the segmentation mask ŷi q (l), where l ∈ L tr . The regularization loss L reg is defined to segment the class l in its corresponding support image x i s , as follows: Overall, in each iteration, the loss function during the first-stage training is In the second stage of training, we aim to exploit information beyond 2D image slices in a volumetric image by employing realistic tasks. To this end, we define the 2.5D loss function, L 2.5D , which enforces mask continuity among the prediction of adjacent queries. The proposed L 2.5D profits the Dice loss  Specifically, the loss function compares the predicted mask of a query slice with the predicted mask of its adjacent slice and penalizes any discontinuities between them. This helps ensure that the model produces consistent and coherent segmentation masks across multiple slices, improving the overall quality and accuracy of the segmentation. Hence, in the second-stage training, we train the network only on realistic tasks using the loss function: , where λ 1 is linearly increased from 0 to 0.5 every 1000th iteration during training. Finally, after self-supervised learning, the few-shot segmenter can be directly utilized for inference on unseen classes."
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,2.3,Volumetric Segmentation Strategy,"During inference, the goal is to segment query volumes based on a support volume with only a sparse set of human-annotated slices, while the few-shot segmenter is trained with 2D images. To evaluate 2D segmentation on 3D volumetric images, we take inspiration from  In fact, we first find the pseudo-mask of (x c q ) k using the 2D few-shot segmenter and consequently consider this pseudo-annotated slice as the support for all other slices in X k q . It is worth mentioning that our task generation strategy discussed in Sect. 2.2 is capable of handling such intra-volume tasks. Further details of the VSPS algorithm are brought in the Supplementary Materials."
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,3.1,Experimental Setup,"To unify experiment results, we follow the evaluation protocol established by  Dataset. Following "
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,3.2,Results and Discussion,Comparison with Existing Approaches. Table 
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,4.0,Conclusion,"This work introduces a novel framework called VISA-FSS, which aims to perform few-shot 3D segmentation without requiring any manual annotations during training. VISA-FSS leverages inter-slice information and continuous shape changes that exist across consecutive slices within a 3D image. During training, it uses consecutive slices within a 3D volume as support and query images, as well as support-query pairs generated by applying geometric and intensity transformations. This allows us to exploit intra-volume information and introduce a 2.5D loss function that penalizes the model for making predictions that are discontinuous among adjacent slices. Finally, during inference, a novel strategy for volumetric segmentation is introduced to employ the volumetric view even during the testing time."
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_11.
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,1.0,Introduction,"Cardiovascular diseases continue to be the primary cause of death worldwide. Imaging of myocardial fibrosis/scar provides both diagnostic and prognostic information. Cardiac magnetic resonance (CMR) late gadolinium enhancement (LGE) is the gold standard for myocardial scar evaluation in ischemic and non-ischemic heart disease  Recently, deep learning (DL) based methods have been proposed to limit gadolinium use by creating virtual LGE-like images using short-axis (sax) cine "
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,,Contribution:,"In this study, we develop a novel end-to-end deep spatiotemporal residual attention neural network (ST-RAN) for scar detection using whole heart imaging in ischemic and non-ischemic heart diseases. The proposed model leverages spatial information to capture changes in contrast and temporal information to capture WMA to detect scars, in a large heterogeneous dataset. To achieve this, we propose a novel efficient Conv3Plus1D layer that deploys a factorized 4D (3D+time) receptive field, to simultaneously extract hierarchical spatial features and deep temporal features (comprehensive spatiotemporal features), distinguishing between patients with and without a scar. We introduce a multi-scale residual attention block that learns global and local motions to detect significant and subtle changes, the latter more present in patients with small scar sizes and nearly preserved wall motion. We validate our proposed model on a large cohort of patients with and without scars, showing the robustness of the model, outperforming state-of-the-art methods. Architecture overview. Our model takes an input a set of short-axis cine images of the whole heart, consisting of 20 phases, which are fed to a novel Conv3Plus1D layer, to extract spatial and temporal features. After batch normalization and nonlinear transformation, the feature maps are fed to a series of residual attention blocks (RAB) at different scales to extract global and local features, subtle to changes due to myocardial scar. After the RAB, a global average pooling followed by a fully connected are used to predict presence of a scar."
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,2.0,Methods,As illustrated in Fig. 
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,2.1,Spatiotemporal Decomposition Using 4D(3D+time) Layers,"In sax cine, the data is in 4-dimension consisting of a 3D volume (stack of sax slices) with a temporal dimension. Therefore, effective representation of spatiotemporal features is crucial for accurate analysis. Inspired by Squeeze & Excitation network  where W c1 and W c2 are the weights for the fully connected layers 1 and 2 in the spatial attention module. The temporal convolution W t is applied across a volume input D ti across X × Y × T where the feature map output is where i = 1....m and m is the number of feature maps in the Z direction. The spatiotemporal attention module is trained to assign an attention score a F k for each feature map F ti and patch K in X × Y × T direction given as: where W c3 and W c4 are the weights for the fully connected layers 1 and 2 in the spatiotemporal attention module. To simplify the Eqs. ( "
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,2.2,Residual Attention Blocks,"The motion patterns of the heart can evolve over time and scale. The residual attention network builds a stack of attention modules that generate attentionaware features. As the layers deepen, attention-aware features scale adaptively, enabling the detection of spatial and temporal subtle changes to be enhanced. This enhancement is crucial in accurately detecting small scar sizes. By aggregating information from tissues and motion across multiple scales, the attention module is able to learn and assign relative importance to each region with regard to the presence of a scar. The feature maps at a scale i where i = 1....4, are input to two fully-connected layers to encode spatial-wise and temporal-wise dependencies defined as G = W Ri1 * ReLU (V i * W Ri2 ), with W Ri1 being the weights for the first fully connected layer at scale i, and W Ri2 being the weights for the second fully connected layer at scale i. The output G is then passed through the softmax activation to obtain the spatiotemporal residual weights, which will be applied to the input map V i to extract the spatiotemporal features at scale i."
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,2.3,Network Details,"The proposed network applies first a Conv3Plus1D layer followed by a batch normalization to mitigate internal covariate shift in the data, applies a small regularization effect, and a non-linear activation function ReLU. The output feature map is then resized through a customized resize frames layer that downsamples the size of the input by a factor of 2. This helps in increasing the batch size (×2 folds) and accelerates training (×12 folds) and testing (×4 folds) while maintaining the same high performance. This is followed by four attention residual blocks. Each block consists of residual attention applied to the input feature map and two sets of Conv3Plus1D layers, each followed by a layer normalization and ReLU. When layer normalization is located inside the residual blocks, it allows to speed-up convergence without a need for a learning rate warm-up stage "
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,3.0,Materials and Implementation Details,"Data Aacquisition. Cine images were collected breath-hold electrocardiogramgated balanced steady-state free precession sequence of 10 sax slices. The data were acquired from institution anonymous from 2016 to 2020 using multivendor (GE Healthcare, Siemens Healthineers) and different field strengths (1.5 T, 3 T). The institutional review board approved the use of CMR data for research with a consent waiver. Patient information was handled in compliance with the Health Insurance Portability and Accountability Act. Patients were referred for a clinical CMR for different cardiac indications, resulting in a heterogeneous patient cohort, necessary for better evaluation of the model performance. In total, 3000 patients (1697 males, 54 ± 18 years) were used for training and evaluation. The data were split into training (n=2000, 762 scar+), validation (n=500, 169 scar+), and testing (n=500, 199 scar+). All images were cropped at the center to a size of 128 × 128 and normalized to a fixed intensity range (from 0 to 1). Implementation Details. The model's optimization was performed using a mini-batch stochastic gradient descent of 64 with an initial learning rate of 0.001 and a weight decay of 0.0001 when the validation loss plateaus. The model was trained for a maximum of 500 epochs with an early stopping of 70. The binary cross-entropy loss function and binary accuracy metric for both training and validation were monitored to avoid overfitting and underfitting. All models were implemented using TensorFlow version 2.4.1 and trained on an NVIDIA DGX-1 system equipped with 8 T V100 graphics processing units (each with 32 GB memory and 5120 cores). All selected hyperparameters were optimized experimentally. DeLong's test was used to compare the AUC of the different models. All tests were two-sided with a significance level = 0.05."
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,4.0,Experiments and Results,"Ablation Study on the Impact of Different Components Design. We first perform an ablation study to evaluate each component's contribution to our proposed model. We test the effect of having spatial kernels only (S-CNN), temporal kernels only (T-CNN), and a combination of both (ST-CNN). We test the impact of spatial and temporal attention (AST-CNN) and residual attention (ST-RAN) on the performance of our network. To this end, we train five variants of our model. We can observe from Table "
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,,Comparison with State-of-the-Art Methods.,"We then compare our model with state-of-the-art methods trained and tested on the same dataset for myocardial scar detection, including 3D (2D + time) spatiotemporal CNN (3D-STCNN) "
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,5.0,Discussion,"Recent works using deep learning have shown the promise of contrast-free shortaxis cine images to detect scars based on WMA in ischemic patients. However, these methods have limitations in detecting scar in non-ischemic heart diseases. Moreover, the large heterogeneous number of patients without scar in the dataset and with WMA has degraded these models' performance in detecting scar in ischemic patients. In contrast, our approach utilizes both spatial and temporal information to detect scar. Our model demonstrates superior performance over To overcome the complexity of 4D convolution, we propose an effective training and inference strategy based on spatiotemporal factorization 4D (3D+time). This approach allows for a reduction in model parameters by a factor of 32 while maintaining high performance. The proposed layer extracts both spatial and temporal features, while enhancing attention on features in both directions, to detect subtle differences in left ventricle myocardial texture, as well as in cardiac motion. In future work, we will investigate multimodality learning, incorporating other sequences such as T 1 maps, to enhance the model's precision to an even greater degree."
Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization,6.0,Conclusion,"We propose a spatiotemporal residual attention neural network for myocardial scar detection, and we tackled the challenging non-ischemic patients. We showed that our model works on ischemic heart disease as well. Our results demonstrate the potential of our model in unmasking hidden information in native sax cine images, and allows for better detection of patients with a high likelihood of having a myocardial scar. These results indicate the potential of our proposed model in screening patients with and without a scar, thus, saving patients from unnecessary gadolinium based contrast agent administration, reducing costs, and environmental pollution. Finally, our proposed network has potential applications in various clinical contexts that require 4D processing."
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,1.0,Introduction,"Skin cancer is one of the most common cancers all over the world. Serious skin diseases such as melanoma can be life-threatening, making early detection and treatment essential  accuracy and robustness requirements in applications, which is hard to suffice due to the long-tailed occurrence of diseases in the real-world. Long-tailed problem is usually caused by differences in incidence rate and difficulties in data collection. Some diseases are common while others are rare, making it difficult to collect balanced data  To tackle the challenge of learning unbiased classifiers with imbalanced data, many previous works focus on three main ideas, including re-sampling data  Recently, contrastive learning (CL) methods pose great potential for representation learning when trained on imbalanced data  To address the above issues, we propose a class-Enhancement Contrastive Learning (ECL) method for skin lesion classification, differences between SCL and ECL are illustrated in Fig.  Our contributions can be summarized as follows: "
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,2.0,Methods,The overall end-to-end framework of ECL is presented in Fig. 
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,2.1,Hybrid-Proxy Model,"The proposed hybrid-proxy model consists of a set of class-dependent proxies  In this way, the tail classes have more proxies while head classes have less, thus alleviating the imbalanced problem in a mini-batch. As we know, a gradient descent algorithm will generally be executed to update the parameters after training a mini-batch of samples. However, when dealing with an imbalanced dataset, tail samples in a batch contribute little to the update of their corresponding proxies due to the low probability of being sampled. So how to get better representative proxies? Here we propose a cycle update strategy for the optimization of the parameters. Specifically, we introduce the gradient accumulation method into the training process to update proxies asynchronously. The proxies are updated only after a finished epoch that all data has been processed by the framework with the gradients accumulated. With such a strategy, tail proxies can be optimized in a view of whole data distribution, thus playing better roles in class information enhancement. Algorithm 1 presents the details of the training process."
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,2.2,Balanced-Hybrid-Proxy Loss,"To tackle the problem that SCL loss pays more attention on head classes, we introduce BCL and propose balanced-hybrid-proxy loss to treat classes equally. Given a batch of samples B = (x be the feature embeddings in a batch and B denotes the batch size. For an anchor sample z i ∈ Z in class c, we unify the positive image set as z + = {z j |y j = y i = c, j = i}. Also for an anchor proxy p c i , we unify all positive proxies as p + . The proposed balanced-hybrid-proxy loss pulls points (both samples and proxies) in the same class together, while pushes apart samples from different classes in embedding space by using dot product as a similarity measure, which can be formulated as follows: where B c means the sample number of class c in a batch, τ is the temperature parameter. In addition, we further define Z c and P c as a subset with the label c of Z and P respectively. The average operation in the denominator of balancedhybrid-proxy loss can effectively reduce the gradients of the head classes, making an equal contribution to optimizing each class. Note that our loss differs from BCL as we enrich the learning of relations between samples and proxies. Sampleto-sample, proxy-to-sample and proxy-to-proxy relations in the proposed loss have the potential to promote network's representation learning. Moreover, as the skin datasets are often small, richer relations can effectively help form a high-quality distribution in the embedding space and improve the separation of features."
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,2.3,Balanced-Weighted Cross-Entropy Loss,"Taking both ""imbalanced data"" and ""imbalanced diagnosis difficulty"" into consideration, we design a curriculum schedule and propose balanced-weighted cross-entropy loss to train an unbiased classifier. The training phase are divided into three stages. We first train a general classifier, then in the second stage we assign larger weight to tail classes for ""imbalanced data"". In the last stage, we utilize the results on the validation set as the diagnosis difficulty indicator of skin disease types to update the weights for ""imbalanced diagnosis difficulty"". The loss is given by: where w denotes the weight and ỹ denotes the network prediction. We assume f e c is the evaluation result of class c on validation set after epoch e and we use f1-score in our experiments. The network is trained for E epochs, E 1 and E 2 are hyperparameters for stages. The final loss is given by Loss = λL BHP +μL BW CE where λ and μ are the hyperparameters which control the impact of losses."
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,3.1,Dataset and Implementation Details,Dataset and Evaluation Metrics. We evaluate the ECL on two publicly available dermoscopic datasets ISIC2018  Implementation Details. The proposed algorithm is implemented in Python with Pytorch library and runs on a PC equipped with an NVIDIA A100 GPU. We use ResNet50 
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,3.2,Experimental Results,"Quantitative Results. To evaluate the performance of our ECL, we compare our method with 10 advanced methods. Among them, focal loss "
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,4.0,Conclusion,"In this work, we present a class-enhancement contrastive learning framework, named ECL, for long-tailed skin lesion classification. Hybrid-proxy model and balanced-hybrid-proxy loss are proposed to tackle the problem that SCL-based methods pay less attention to the learning of tail classes. Class-dependent proxies are generated in hybrid-proxy model to enhance information of tail classes, where rich relations between samples and proxies are utilized to improve representation learning of the network. Furthermore, balanced-weighted cross-entropy loss is designed to help train an unbiased classifier by considering both ""imbalanced data"" and ""imbalanced diagnosis difficulty"". Extensive experiments on ISIC2018 and ISIC2019 datasets have demonstrated the effectiveness and superiority of ECL over other compared methods."
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 23.
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,1.0,Introduction and Related Work,"Ultrasound (US) is one of the most common imaging techniques in medical practice, with applications to fetal imaging, cardiac imaging, sports medicine, and more. With the rise of US for routine clinical care, there is a growing interest in applying computer vision techniques to automate or enhance the analysis of US imagery  To design an efficient US recognition architecture, it is necessary to consider the space of US recognition tasks and evaluate the algorithmic structures needed to efficiently capture the semantics in those settings. We posit that many of these tasks amount to the identification of specific visual characteristics at key moments in the clip. The identification of the standard plane in fetal head US depends on recognizing key structures in fetal brain tissue  A large body of work has addressed video recognition tasks, including object tracking  Multi-instance learning (MIL) describes the situation where labels apply to bags of instances rather than to individual instances. Instances within a bag are assumed to be unordered and, conditional on the bag label, independent from one another  There is growing interest in applying action recognition techniques to medical US video with applications to fetal "
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,2.1,USVN,"Architecture. Our video recognition architecture, shown in Fig.  Frames are first embedded into 2048-dimensional vectors using a CNN encoder. This encoder is initialized via ImageNet pretraining and fine-tuned during training. Rather than learn N a projections from scratch for the attention weighting, we simply partition the frame representations into N a vectors h t i each of size d a = 2048/N a and rely on the final convolutional layers of the CNN to adapt. We then compute the un-normalized attention scores via dot product with the global query vectors: The resulting scores are normalized resulting in N a attention vectors, a i = softmax(λ i ), where the arrow notation represents vectorization in time. The video-level representation from the i th head is then simply H i = a i • h i , and the full video representation is the concatenation H = concat([H 1 , H 2 , . . . , H Na ]). The video-level prediction can then be computed using a shallow fully-connected network, y = f (H). Augmentation by Frame Sampling. Because USVN treats all frames independently, it is not necessary to use contiguous spans of frames during training. Instead, we randomly sample fixed-size sets of frames from each video. This can have a regularizing effect by using novel frames for each training epoch. During evaluation we use all video frames. We accommodate the varying numbers of frames in each video by zero padding and masked attention. Model Interpretability. We identify prototype frames for each attention head. These prototypes produce embedding subspace vectors h t i that are closely aligned with the corresponding query vector q i . These prototype images can then be qualitatively evaluated by the clinical specialist (see Supplemental Material)."
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,2.2,Benchmark Implementations,"A simple and common approach for video recognition is to use fixed pooling functions to aggregate the frame-level representations across time, treating each element of the representation as a channel. We evaluate this approach using max and average pooling functions. Our attention-based method can implement average pooling by assigning equal weight to all frames for each attention head. Neglecting potential optimization challenges, this suggests that attention-based pooling should be at least as good as average pooling. On the other hand, our model can only approximate max pooling in the N a = 2048 case by assigning very large, positive values to the single-element query vectors causing the attentions to become sharply concentrated at one time step. However, this solution pushes the softmax over time into regions with very small gradients. We conclude that max pooling can learn video representations that cannot be expressed by USVN (and vice versa). R(2+1)D is a 3D CNN video recognition architecture that decomposes the spatial and temporal convolution into two successive steps "
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,3.1,Datasets,"Patent Ductus Arteriosus (PDA). PDA is an opening between the aorta and pulmonary artery that, in severe cases, can cause heart failure shortly after birth. Ultrasound imaging is the primary diagnostic tool for detecting and characterizing PDA. Specifically, doppler US imaging can visualize the motion of the blood through the PDA opening. This motion appears as a characteristic blob of color in the region of the PDA. Physicians are trained to recognize the color and shape of the blob as well as where it appears in relation to other visible anatomy. Superficially, this recognition task makes no reference to the dynamics of the video. We therefore expect that temporal features are not required for accurate PDA recognition. For this dataset we train USVN to predict whether or not an image indicates the presence of PDA. The model output, y, is therefore a single number interpreted as the log-odds of PDA. We retrospectively collected a set of 1,145 doppler US clips from 165 distinct examinations involving 66 distinct patients. Each clip was labeled to indicate the presence (661 clips) or absence (484 clips) of PDA. Patients were divided into training (44), validation  The large variation in the number of videos in the validation and test sets results from the fact that patients have a variable number of examinations ranging from 1 to 10.  The echocardiograms were obtained by registered sonographers and level 3 echocardiographers. For each of these videos, a masking and cropping transformation was performed to remove text and instrument information from the scanning area. For this dataset, we train USVN to predict ejection fraction. Rather than predict EF directly, we output a tuple of real numbers (y 1 , y 2 ) and insert them in place of ESV and EDV in Eq. ( "
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,3.2,Results,"Model Performance. Table  For PDA, R(2+1)D underperforms the time-independent methods, and the gap is larger for smaller numbers of training patients (see Fig. "
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,3.3,Implementation Details,"For the fixed pooling methods and USVN, we use an ImageNet-pretrained ResNet50 image encoder provided through the timm library  To reproduce the results of R(2+1)D on Echonet Dynamic Dataset by Ouyang et al. "
On the Relevance of Temporal Features for Medical Ultrasound Video Recognition,4.0,Conclusions and Discussion,"The field of video recognition has been driven by large human action recognition datasets. Unlike videos of human actions, the accurate recognition of medical ultrasound images often only requires identifying key pieces of information at any point in the video and does not make reference to the sequence of events. The contrast between results for the PDA task (where USVN excels) and the EchoNet task (where USVN suffers) demonstrates the importance of tailoring the model architecture to the task at hand in data-constrained settings. Our results suggest that models developed for human action recognition are not optimal in some practical scenarios involving medical ultrasound and that models that assume temporal independence have better sample efficiency. We introduce an architecture, USVN, that is tailored to the medical ultrasound context and demonstrate a situation where the inductive prior of time independence leads to significant sample efficiency benefits. We also present a situation where temporal features are relevant and show that, even for very small datasets, USVN produces no efficiency benefits. Practitioners of deep learning who work with medical ultrasound in the low-data regime should take care to match the architecture choice to the nature of the recognition task."
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,1.0,Introduction,"Myocardial infarction (MI) is the deadliest cardiovascular disease in the developed world  In this work, we propose the multi-objective point cloud autoencoder as a novel geometric deep learning approach for interpretable MI prediction, based on 3D cardiac shape information. Its specialized multi-branch architecture allows for the direct and efficient processing of high resolution 3D point cloud representations of the multi-class cardiac anatomy at multiple time points of the cardiac cycle, while simultaneously predicting future MI events. Crucially, a lowdimensional latent space vector captures task-specific 3D shape information as an orderly multivariate probability distribution, offering pathology-specific separability and allowing for a straightforward visual analysis of associations between 3D structure and latent encodings. The resulting high explainability considerably boosts the method's clinical applicability and sets it apart from previous black-box deep learning approaches for MI classification "
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,2.1,Dataset and Preprocessing,"We select the cine MRI acquisitions of 470 subjects of the UK Biobank study as our dataset in this work  (incident MI) as indicated by UK Biobank field IDs 42001 and 42000. The other 50% of subjects are considered as normal control cases. They were chosen to be free of any cardiovascular disease and other pathologies frequently observed in the UK Biobank study, following a similar selection as previous works "
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,2.2,Network Architecture,"The architecture of the multi-objective point cloud autoencoder consists of three task-specific branches, namely an encoder, a reconstruction, and a prediction branch, which are connected by a low-dimensional latent space vector (Fig.  Concatenated multi-class point clouds at the ED and ES phases of the cardiac cycle with shape (2 * p)×4 are first fed into the encoder branch as network inputs where (2 * p) represents the number of points p in the ED and ES point clouds and 4 are the x, y, z coordinate values in 3D space and a class label to encode the three cardiac substructures, namely left ventricular (LV) endocardium, LV epicardium, and right ventricular (RV) endocardium. The inputs are then passed through the point cloud-specific encoder, which is composed of two connected PointNet-style "
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,2.3,Loss and Training,"The loss function of the multi-objective point cloud autoencoder consists of the sum of three subloss terms, each representing a different training objective in the multi-task setting, and weighted by two parameters β and γ. ( The first loss term, L reconstruction , encourages the network to accurately reconstruct input anatomies and thereby capture important shape information. It contains two subloss terms and a weighting parameter α. ( Here, C and T refer to the number of cardiac substructures and phases respectively. We use C = 3 and T = 2 in this work. The L coarse and L dense loss terms compare the respective coarse and dense output predictions of the network with the same input point cloud using the symmetric Chamfer distance (CD). The weighting parameter α is increased stepwise from smaller (0.01) to larger (2.0) values during training in a monotonic annealing schedule to encourage the network to first focus on a good global reconstruction and gradually put more emphasis on a high local accuracy as training progresses. The second loss term in Eq. ( "
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,3.1,Input Shape Reconstruction,"In our first experiment, we evaluate whether the multi-objective point cloud autoencoder is able to accurately reconstruct the ED and ES input anatomies. To this end, we pass all anatomies of the test dataset through the trained network and visualize both the input and corresponding predicted point clouds of three sample cases in Fig.  Next, we quantify the reconstruction performance by calculating the symmetric Chamfer distances between the respective input and reconstructed point clouds of all subjects in the test dataset separately for each cardiac substructure and phase (Table "
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,3.2,Myocardial Infarction Prediction,"We next evaluate the performance of the network for incident MI prediction as its second task. To this end, we first obtain both the gold standard MI outcomes and the MI predictions of our pre-trained network for all cases in the test dataset and quantify its performance using five common binary classification metrics (Table "
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,3.3,Task-Specific Latent Space Analysis,"In addition to validating the reconstruction and prediction performance of our network, we also investigate the ability of its latent space to store high-resolution 3D shape data in an interpretable and pathology-specific manner. To this end, we first pass the anatomy point clouds of both normal and MI cases through the encoder branch of the pre-trained network to obtain their respective latent space encodings. We then apply the Laplacian eigenmap  We observe a clear differentiation between the encoded normal and MI cases in the eigenmap. Furthermore, the sample anatomies positioned in the area of normal subjects typically exhibit noticeably different shape patterns to the ones located in the MI cluster. For example, the left middle MI subject shows much smaller volume and myocardial thickness changes between ED and ES anatomies than the three normal cases. "
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,3.4,Ablation Study,"In order to assess the contributions of important parts of our network architecture, we next ablate multiple key components and study their effects on prediction performance. More specifically, we individually remove the dropout layer, the KL loss term, and the reconstruction branch, retrain each of the three ablated networks, and report their MI prediction results on the test dataset in Table "
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,4.0,Discussion and Conclusion,"In this paper, we have presented the multi-objective point cloud autoencoder as a novel geometric deep learning approach for interpretable MI prediction. The network is able to reconstruct input point clouds with high accuracy and only small localized smoothness artifacts despite the difficult multi-task setting. This shows the suitability of its architecture for efficient multi-scale feature extraction and its ability to effectively capture important 3D shape information in its latent space. Furthermore, the network can simultaneously process all three cardiac substructures at both ED and ES, indicating high flexibility and a potential for further extensions to the full cardiac cycle or other cardiac substructures. In addition, it also allows for more complex 3D shape-based biomarkers to be learned based on inter-temporal and inter-anatomical relationships. All these results are achieved directly on point cloud data, which offers a considerably more efficient storage of anatomical surface information than widely used voxelgrid-based deep learning approaches. Furthermore, the method is fast, fully automatic, and can be readily incorporated into a 3D shape analysis pipeline with cine MRI inputs. The network also outperforms both machine learning techniques based on widely used clinical biomarkers as well as other deep learning approaches for MI prediction. On the one hand, this corroborates previous findings on the increased utility of full 3D shape information compared to single-valued or 2D biomarkers for MI assessment  The network achieves these results based on a highly interpretable latent space with a clear differentiation between normal and MI subject encodings. Furthermore, the observed associations between encodings and 3D shapes demonstrate that the latent space is not only discriminative but also that the differentiation is based on clinically plausible 3D shape differences, such as reduced myocardial thinning between ED and ES in MI subjects which is indicative of impaired contraction ability of the heart. This greatly improves the explainability and applicability of the approach, as new subject phenotypes can be quickly and easily compared to other ones with similar encodings. Furthermore, the latent map not only shows well known associations of EF and MI but also a clear differentiation between some normal and MI cases with similar EF values. This indicates that the network is able to capture more intricate biomarkers that go beyond ejection fraction and to successfully utilize them in its MI prediction task while retaining high interpretability. Finally, we show in our ablation studies that all major components of the architecture improve predictive accuracy. We hypothesize that the dropout layer, KL divergence term, and reconstruction branch introduce useful constraints, which have a positive regularizing effect and aid generalization. The multiobjective training procedure accounts for the largest performance gain. This is likely due to the exploited synergies of multiple tasks, which we also believe to be the primary reason for the high separability in the latent space."
Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 50.
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,1.0,Introduction,"The past years have witnessed a rapid growth of applying deep learning methods in medical imaging  There are two groups of methods to tackle unfairness. The first group proceeds implicitly with fairness through unawareness  It is natural to consider whether it is possible to inherit the advantages from both worlds, that is, learning a single model on the whole dataset yet still with explicit modeling of sensitive attributes. Therefore, we propose a framework with a powerful adapter termed Fair Adaptive Batch Normalization (FairAd-aBN). Specifically, FairAdaBN is designed to mitigate task disparity between subgroups captured by the neural network. It integrates the common information of different subgroups dynamically by sharing part of network parameters, and enables the differential expression of feature maps for different subgroups, by adding only a few parameters compared with backbones. Thanks to FairAdaBN, the proposed architecture can minimize statistical differences between subgroups and learn subgroup-specific features for unfairness mitigation, which improves model fairness and reserves model precision at the same time. In addition, to intensify the models' ability for balancing performance and fairness, a new loss function named Statistical Disparity Loss (L SD ), is introduced to optimize the statistical disparity in mini-batches and specify fairness constraints on network optimization. L SD also enhances information transmission between subgroups, which is rare for independent models. Finally, a perfect model should have both higher precision and fairness compared to current well-fitted models. However, most of the existing unfairness mitigation methods sacrifice overall performance for building a fairer model  To sum up, our contributions are as follows: "
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,2.0,Related Work,"According to  Pre-processing. Pre-processing methods focus on the quality of the training set, by organizing fair datasets via datasets combination  Post-processing. Although calibration has been widely used in unfairness mitigation in machine learning tasks, medical applications prefer to use pruning strategies. For example, Wu et. al  In-Processing. In-processing methods mainly consist of two folds. Some studies mitigate unfairness by directly adding fairness constraints to the cost functions "
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,3.0,FairAdaBN,"Problem Definition. We assume a medical imaging dataset D = {d 1 , d 2 , ..., d N } with N samples, the i-th sample d i consists of input image X i , sensitive attributes A i and classification ground truth label Y i . i.e. A is a binary variable (e.g., skin tone, gender), which splits the dataset into the unprivileged group, D A=0 , which has a lower average performance than the overall performance, and the privileged group, D A=1 , which has a higher average performance than the overall performance. Using accuracy as the performance metric for example, for a neural network f θ (•), our goal is to minimize the accuracy gap between D A=0 and D A=1 by finding a proper θ.  In this paper, we propose FairAdaBN, which replaces normalization layers in vanilla models with adaptive batch normalization layers, while sharing other layers between subgroups. The overview of our method is shown in Fig.  Batch normalization (BN) is a ubiquitous network layer that normalizes minibatch features using statistics  where μ(x), σ(x) is the mean and standard deviation of the feature map computed in the mini-batch, β and γ denotes the learnable affine parameters. We implant the attribute awareness into BN, named FairAdaBN, by parallelizing multiple normalization blocks that are carefully designed for each subgroup. Specifically, for subgroup D A=a , its adaptive affine parameter γ a and β a are learnt by samples in D A=a . Thus, the adaptive BN function for subgroup D A=a is given by Eq. 3. where a is the index of the sensitive attribute corresponding to the current input image, μ α , σ α are computed across subgroups independently. The FairAdaBN acquires subgroup-specific knowledge by learning the affine parameter γ and β. Therefore, the feature maps of subgroups can be aligned and the unfair representation between privileged and unprivileged groups can be mitigated. By applying FairAdaBN on vanilla backbones, the network can learn subgroup-agnostic feature representations by the sharing parameters of convolution layers, and subgroup-specific feature representations using respective BN parameters, resulting in lower fairness criteria. The detailed structure of FairAdaBN is shown in Fig.  In this paper, we aim to retain skin lesion classification accuracy and improve model fairness simultaneously. The loss function consists of two parts: (i) the cross-entropy loss, L CE , constraining the prediction precision, and (2) the statistical disparity loss L SD as in Eq. 4, aiming to minimize the difference of prediction probability between subgroups and give extra limits on fairness. where N cg means the number of classification categories. The overall loss function is given by the sum of the two parts, with a hyperparameter α to adjust the degree of constraint on fairness."
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,4.1,Evaluation Metrics,"Lots of fairness criteria are proposed including statistical parity  However, these metrics only evaluate the level of fairness while do not consider the trade-off between fairness and accuracy. Therefore, inspired by  where F C can be one of EOpp0, EOpp1, EOdd. ACC denotes accuracy. The subscript m and b denote the mitigation model and baseline model, respectively. λ is a weighting factor that adjusts the requirements for fairness pre-defined by the user considering the real application, here we define λ = 1.0 for simplification. A model obtains a higher FATE if it mitigates unfairness and maintains accuracy. Note that FATE should be combined with utility metrics and fairness metrics, rather than independently."
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,4.2,Dataset and Network Configuration,We use two well-known dermatology datasets to evaluate the proposed method. The Fitzpatrick-17k dataset 
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,4.3,Results,"We compare FairAdaBN with Vanilla (ResNet-152), Resampling  Results on Fitzpatrick-17k Dataset. Table "
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,,Results on ISIC 2019 Dataset.,Table  Hyper-parameter α. Our experiments show that α = 1.0 has the best fairness scores and FATE compared to α = 0.1 and α = 2.0. Therefore we select α = 1.0 as our final setting.
FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification,5.0,Conclusion,"We propose FairAdaBN, a simple but effective framework for unfairness mitigation in dermatological disease classification. Extensive experiments illustrate that the proposed framework can mitigate unfairness compared to models without fair constraints, and has a higher fairness-accuracy trade-off efficiency compared with other unfairness mitigation methods. By plugging FairAdaBN into several backbones, its generalization ability is proved. However, the current study only evaluates the effectiveness of FairAdaBN on dermatology datasets, and its generalization ability on other datasets (chest X-Ray, brain MRI) or tasks (segmentation, detection), where unfairness issues also exist, needs to be evaluated in the future. We also plan to explore the unfairness mitigation effectiveness for other universal models "
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,1.0,Introduction,"Deep learning has achieved promising performance in computer-aided diagnosis  Transferring pre-trained models to downstream tasks is an effective solution for addressing the label-limited problem  However, previous prompt tuning research  Therefore, this paper proposes the first framework for selective labeling and prompt tuning (SLPT), combining model-centric and data-centric methods to improve performance in medical label-limited scenarios. We make three main contributions: "
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,2.0,Methodology,"Given a task-agnostic pre-trained model and unlabeled data for an initial medical task, we propose SLPT to improve model performance. SLPT consists of three components, as illustrated in Fig. "
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,2.1,Prompt-Based Visual Model,"The pre-trained model, learned by supervised or unsupervised training, is a powerful tool for improving performance on label-limited downstream tasks. Finetuning a large pre-trained model with limited data may be suboptimal and prone to overfitting  To incorporate FPU into a pre-trained model, we consider the model comprising N modular M i (i = 1, ..., N ) and a head output layer. After each M i , we insert an F P U i . Given the input F in i-1 and prompt P i-1 , we have the output feature F i , updated prompt P i and prediction Y as follows: where input X = F 0 , FPU and Head are tuned while M i is not tunable."
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,2.2,Diversified Visual Prompt Tuning,"Inspired by multi-prompt learning  through K different upsampling and convolution operations UpConv k . P M is initialized from the statistical probability map of the foreground category, similar to  To enhance prompt diversity, we introduce a prompt diversity loss L div that regularizes the cosine similarity between the generated prompts and maximizes their diversity. This loss is formulated as follows: where P k1 and P k2 represent the k 1 -th and k 2 -th generated prompts, respectively, and || • || 2 denotes the L2 norm. By incorporating the prompt diversity loss, we aim to generate a set of diverse prompts for our visual model. In NLP, using multiple prompts can produce discrepant predictions  where k = 1, ..., K, M F P U is the pre-trained model with FPU, CE is the crossentropy loss, and λ 1 = λ 2 = λ 3 = 1 weight each loss component. Y represents the ground truth and L is the total loss."
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,2.3,Tandem Selective Labeling,"Previous studies overlook the critical issue of data selection for downstream tasks, especially when available labels are limited. To address this challenge, we propose a novel strategy called TESLA. TESLA consists of two tandem steps: unsupervised diversity selection and supervised uncertainty selection. The first step aims to maximize the diversity of the selected data, while the second step aims to select the most uncertain samples based on diverse prompts. Step 0: Unsupervised Diversity Selection. Since we do not have any labels in the initial and our pre-trained model is task-agnostic, we select diverse samples to cover the entire dataset. To achieve this, we leverage the pre-trained model to obtain feature representations for all unlabeled data. Although these features are task-independent, they capture the underlying relationships, with similar samples having closer feature distances. We apply the k-center method from Coreset  Step 1: Supervised Uncertainty Selection. After prompt tuning with the initial dataset, we obtain a task-specific model that can be used to evaluate data value under supervised training. Since only prompt-related parameters can be tuned while others are frozen, we assess prompt-based uncertainty via diverse prompts, considering inter-prompts uncertainty and intra-prompts uncertainty. In the former, we compute the multi-prompt-based divergence map D, given K probability predictions Y k through K diverse prompts P k , as follows: where KL refers to the KL divergence  In the latter, we evaluate intra-prompts uncertainty by computing the mean prediction of the prompts and propose to estimate prompt-based gradients as the model's performance depends on the update of prompt parameters θ p . However, for these unlabeled samples, computing their supervised loss and gradient directly is not feasible. Therefore, we use the entropy of the model's predictions as a proxy for loss. Specifically, we calculate the entropy-based prompt gradient score S g for each unlabeled sample as follows: To avoid manual weight adjustment, we employ multiplication instead of addition. We calculate our uncertainty score S as follows: where max(•) finds the maximum value. We sort the unlabeled data by their corresponding S values in ascending order and select the top B data to annotate."
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,3.1,Experimental Settings,"Datasets and Pre-trained Model. We conducted experiments on automating liver tumor segmentation in contrast-enhanced CT scans, a crucial task in liver cancer diagnosis and surgical planning  Collecting large-scale data from our hospital and training a new model will be expensive. Therefore, we can use the model trained from them as a starting point and use SLPT to adapt it to our hospital with minimum cost. We collected a dataset from our in-house hospital comprising 941 CT scans with eight categories: hepatocellular carcinoma, cholangioma, metastasis, hepatoblastoma, hemangioma, focal nodular hyperplasia, cyst, and others. It covers both major and rare tumor types. Our objective is to segment all types of lesions accurately. We utilized a pre-trained model for liver segmentation using supervised learning on two public datasets  Metrics. We evaluated lesion segmentation performance using pixel-wise and lesion-wise metrics. For pixel-wise evaluation, we used the Dice per case, a commonly used metric  Training Setup. We conducted the experiments using the Pytorch framework on a single NVIDIA Tesla V100 GPU. The nnUNet "
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,3.2,Results,"Evaluation of Prompt Tuning. Since we aim to evaluate the efficacy of prompt tuning on limited labeled data in Table  Evaluation of Selective Labeling. We conducted steps 0 (unsupervised selection) and 1 (supervised selection) from the unlabeled 752 data and compared our approach with other competing methods, as shown in Table  Ablation Studies. We conducted ablation studies on S d and S g in TESLA. As shown in Table "
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,4.0,Conclusions,"We proposed a pipeline called SLPT that enhances model performance in labellimited scenarios. With only 6% of tunable prompt parameters, SLPT outperforms fine-tuning due to the feature-aware prompt updater. Moreover, we presented a diversified visual prompt tuning and a TESLA strategy that combines unsupervised and supervised selection to build annotated datasets for downstream tasks. SLPT pipeline is a promising solution for practical medical tasks with limited data, providing good performance, few tunable parameters, and low labeling costs. Future work can explore the potential of SLPT in other domains."
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,1.0,Introduction,"With the increasing demand for accurate and efficient medical image analysis, Semi-supervised segmentation methods offer a viable solution to tackle the problems associated with scarce labeled data and mitigate the reliance on manual expert annotation. It is often not feasible to annotate all images in a dataset. By exploring the information contained in the unlabeled data, semi-supervised learning  Consistency constraint is a widely-used solution in semi-supervised segmentation to improve performance by making the prediction and/or intermediate features remain consistent under different perturbations. However, it's challenging to obtain universal and appropriate perturbations (e.g., augmentation  To put it briefly, prior research has not fully addressed the robustness and variability of prediction results in response to perturbations. To address this, unlike the global prototypes in  The main contributions of this paper can be summarized as: "
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,2.0,Method,"In the semi-supervised segmentation task, the training set is divided into the labeled set C×H×W , where H, W , and C are the height, width, and class number, respectively. Our objective is to enhance the segmentation performance of the model by extracting additional knowledge from the unlabeled dataset D u ."
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,2.1,Self-cross Prototypical Prediction,"The prototype in segmentation refers to the aggregated representation that captures the common characteristics of some pixel-wise features from a particular object or class. Let p c k (i) denote the probability of pixel i belonging to class c, f k ∈ R D×H×W represent the feature map of sample k. The class-wise prototypes q c k is defined as follows: Let B denote the batch size. In the iterative training process, one mini-batch contains B × C prototypes for sample k = 1 and other samples with index j = 2, 3, • • • , B. Then, feature similarity is calculated according to the selfaware prototype q c k or cross-sample prototypes q c j to form multiple segmentation probability matrices. Specifically, ŝc kk is the self-aware prototypical similarity map via calculating the cosine similarity between the feature map f k and the prototype vector q c k as Eq. 2: Then, sof tmax function is applied to generate the self-aware probability prediction pkk ∈ R C×H×W based on ŝkk ∈ R C×H×W . Since q c k is aggregated in sample k itself, which can align f k with more homologous features, ensuring the intra-class consistency of prediction. Similarly, we can obtain B -1 cross-sample prototypical similarity maps ŝc kj following Eq. 3: This step ensures that features are associated and that information is exchanged in a cross-image manner. To enhance the reliability of prediction, we take the multiple similarity estimations ŝkj ∈ R C×H×W into consideration and integrate them to get the cross-sample probability prediction pko ∈ R C×H×W : "
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,2.2,Prototypical Prediction Uncertainty,"To effectively evaluate the predication consistency and training stability in semisupervised settings, we propose a prototypical prediction uncertainty estimation method based on the similarity matrices ŝkk and ŝkj . First, we generate B binary represented mask mkn ∈ R C×H×W via argmax operation and one-hot encoding operation, where n = 1, 2, • • • , B. Then, we sum all masks mkn and dividing it by B to get a normalized probability pnorm as: And a normalized entropy is estimated from pnorm , denoted as e k ∈ R H×W : where e k serves as the overall confidence of multiple prototypical predictions, and a higher entropy equals more prediction uncertainty. Then, we use e k to adjust the pixel-wise weight of labeled and unlabeled samples, which will be elaborated in next subsection."
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,2.3,Unsupervised Prototypical Consistency Constraint,"To enhance the prediction diversity and training effectiveness in consistency learning and mitigate the negative effect of noisy predictions in pkk and pkj , we propose two unsupervised prototypical consistency constraints (PCC) in SPC-Net benefiting from the self-aware prototypical prediction pkk , cross-sample prototypical prediction pkj , and the corresponding uncertainty estimation e k . Self-aware Prototypical Consistency Constraint (SPCC). To boost the intra-class compactness of segmentation prediction, we propose a SPCC method which applies pkk as pseudo-label supervision. Therefore, the loss function of SPCC is formulated as: Cross-sample Prototypical Consistency Constraint (CPCC). To derive dependable knowledge from other training samples, we propose a dual-weighting method for CPCC. First, we take the uncertainty estimation e k into account, which reflects the prediction stability. A higher value of e k indicates that pseudo labels with greater uncertainty may be more susceptible to errors. However, these regions provide valuable information for segmentation performance. To reduce the influence of the suspicious pseudo labels and adjust the contribution of these crucial supervisory signals during training, we incorporate e k in CPCC by setting a weight w 1ki = 1-e ki . Second, we introduce the self-aware probability prediction pkk into the CPCC module. Specifically, we calculate the maximum value of pkk along class c, termed as the self-aware confidence weight w 2ki : w 2k can further enhance the reliability of CPCC. Therefore, the optimized function of CPCC is calculated between cross-sample prototypical prediction pko and pk : Loss Function of SCP-Net We use the combination of cross-entropy loss L ce and Dice loss L Dice to supervise the training process of labeled set  For both labeled data and unlabeled data, we leverage L spcc and L cpcc to provide unsupervised consistency constraints for network training and explore the valuable unlabeled knowledge. To sum it up, the overall loss function of SCPNet is the combination of the supervised loss and the unsupervised consistency loss, which is formulated as: λ (t) = 0.1 • e -5("
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,3.0,Experiments and Results,"Dataset and Evaluation Metric. We validate the effectiveness of our method on two public benchmarks, namely the Automated Cardiac Diagnosis Challenge 1 (ACDC) dataset "
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,4.0,Conclusion,"To summarize, our proposed SCP-Net, which leverages self-aware and crosssample prototypical consistency learning, has successfully tackled the challenges of prediction diversity and training effectiveness in semi-supervised consistency learning. The intra-class compactness of pseudo label is boosted by SPCC. The dual loss re-weighting method of CPCC enhances the model's reliability. The superior segmentation performance demonstrates that SCP-Net effectively exploits the useful unlabeled information to improve segmentation performance given limited annotated data. Moving forward, our focus will be on investigating the feasibility of learning an adaptable number of prototypes that can effectively handle varying levels of category complexity. By doing so, we expect to enhance the quality of prototypical predictions and improve the overall performance."
Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_18.
