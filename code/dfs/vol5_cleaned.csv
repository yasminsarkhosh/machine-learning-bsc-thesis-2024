Paper Title,Header Number,Header Title,Text
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,1.0,Introduction,"The periodic acquisition and analysis of volumetric CT and MRI scans of oncology patients is essential for the evaluation of the disease status, the selection of the treatment, and the response to treatment. Currently, scans are acquired every 2-12 months according to the patient's characteristics, disease stage, and treatment regime. The scan interpretation consists of identifying lesions (primary tumors, metastases) in the affected organs and characterizing their changes over time. Lesion changes include changes in the size of existing lesions, the appearance of new lesions, the disappearance of existing lesions, and complex lesion changes, e.g., the formation of conglomerate lesions. As treatments improve and patients live longer, the number of scans in longitudinal studies increases and their interpretation is more challenging and time-consuming. Radiological follow-up requires the quantitative analysis of lesions and patterns of lesion changes in subsequent scans. It differs from diagnostic reading since the goal is to find and quantify the differences between the scans, rather than to find abnormalities in a single scan. In current practice, quantification of lesion changes is partial and approximate. The RECIST 1.1 guidelines call for finding new lesions (if any), identifying up to the five largest lesions in each scan in the CT slice where they appear largest, manually measuring their diameters, and comparing their difference  In a previous paper, we presented an automatic pipeline for the detection and quantification of lesion changes in pairs of CT liver scans  The novelties of this paper are: 1) identification and formalization of longitudinal lesion matching and patterns of lesion changes in CT in a graph-theoretic framework; 2) new classification and detection of changes of individual lesions and lesion patterns based on the properties of the lesion changes graph and its connected components; 3) a simultaneous lesion matching method with more than two scans; 4) graph-based methods for the detection of changes in individual lesions and patterns of lesion changes. Experimental results on lung (83 CTs, 19 patients) and liver (77 CECTs, 18 patients) datasets show that our method yields high classification accuracy. To the best of our knowledge, ours is the first method to perform longitudinal lesion matching and lesion changes pattern detection. Only a few papers address lesion matching in pairs of CT/MRI scans "
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,2.0,Method,"We present a new generic model-based method for the automatic detection and classification of changes in individual lesions and patterns of lesion changes in consecutive CT scans. The tasks are formalized in a graph-theoretic framework in which nodes represent lesions, edges represent lesion matchings, and paths and connected components represent patterns of lesion changes. Lesion matchings are computed with an overlap-based lesion pairing method after establishing a common reference frame by deformable registration of the scans and organ segmentations. Changes in individual lesions and patterns of lesion changes are computed from the graph's properties and its connected components. We define seven individual lesion change classes and five lesion change patterns that fully describe the evolution of lesions over time. The method inputs the scans and the organ and lesion segmentations in each scan. Its outputs are the lesion matchings, the labels of the changes in individual lesions, and the patterns of the lesion changes. The method is a pipeline of four steps: 1) pairwise deformable registration of each prior scan, organ and lesion segmentations, with the most recent (current) scan as in "
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,2.1,Problem Formalization,"Let S = S 1 , . . . , S N be a series of N ≥ 2 consecutive patient scans acquired at times is a set of vertices v i j corresponding to the lesions associated with the lesion segmentation masks L i = l i 1 , l i 2 , . . . , l i n i , where n i ≥ 0 is the number of lesions in scan S i at time t i . By definition, any two lesions j,l indicates that the lesions corresponding to vertices v i j , v k l are the same lesion, i.e., that the lesion appears in scans S i , S k in the same location. Edges of consecutive scans S i , S i+1 are called consecutive edges; edges of non-consecutive scans, S i , S k , i < k -1, are called non-consecutive edges. The in-and out-degree of a vertex v i j , d in (v i j ) and d out (v i j ), are the number of incoming and outcoming edges, respectively. Let CC = {cc m } M m=1 be the set of connected components of the undirected graph version of G, where M is the number of connected components and By definition, for each 1 ≤ m ≤ M , the sets V m , E m are mutually disjoint and their unions are V , E, respectively. In a connected component cc m , there is an undirected path between any two vertices v i j , v k l consisting of a sequence of undirected edges in E m . . In this setup, connected components correspond to matched lesions and their pattern of evolution over time (Fig.  We define seven mutually exclusive individual lesion change labels for lesion v i j in scan S i based on the vertex in-and out-degrees (Fig. "
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,2.3,Classification of Changes in Lesions and in Patterns of Lesion Changes,"The changes in individual lesions are directly computed for each lesion from the resulting graph with the in-and out-degree of each vertex (Fig.  The changes in individual lesions, patterns of lesion changes, and lesion changes graph serve as the basis for individual lesion tracking, which consists of following the path from the lesion in the most recent scan backwards to its origins in earlier scans and recording the merged, split and complex lesion changes labels. Summarizing longitudinal studies and queries can also be performed with graph-based algorithms."
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,3.0,Experimental Results,"We evaluated our method with two studies on retrospectively collected patient datasets that were manually annotated by an expert radiologist. Dataset: Lung and liver CT studies were retrospectively obtained from two medical centers (Hadassah Univ Hosp Jerusalem Israel) during the routine clinical examination of patients with metastatic disease. Each patient study consists of at least 3 scans. DLUNG consists of 83 chest CT scans from 19 patients with a mean 4.4 ± 2.0 scans/patient, a mean time interval between consecutive scans of 125.9 ± 81.3 days, and voxel sizes of 0.6-1.0 × 0.6-1.0 × 1.0-3.0 mm 3 . DLIVER consists of 77 abdominal CECT scans from 18 patients with a mean 4.3 ± 2.0 scans/patient, a mean time interval between consecutive scans of 109.7 ± 93.5 days, and voxel sizes of 0.6-1.0 × 0.6-1.0 × 0.8-5.0 mm 3 . Lesions in both datasets were annotated by an expert radiologist, yielding a total of 1,178 lung and 800 liver lesions, with a mean of 14.2 ± 19.1 and 10.4 ± 7.9 lesions/scan (lesions with <20 voxels were excluded). Ground-truth lesion matching graphs and lesion changes labeling were produced by running the method on the datasets and then having the radiologist review and correct the resulting node labels and edges. Study 1: Lesion changes labeling, lesion matching, evaluation of patterns of lesion changes. We ran our method on the DLUNGS and DLIVER lesion segmentations. The settings of the parameters were: dilation distance d = 1 mm, overlap percentage p = 10%, number of iterations r = 5 and 7, and centroid maximum distance δ = 17 and 23 mm for the lungs and liver lesions, respectively. We compared the computed and ground-truth lesion changes graphs with two metrics: 1) lesion changes classification accuracy, which is the % of correct computed labels from the ground truth labels; 2) lesion matching precision and recall based on the presence/absence of computed vs. ground truth edges. The precision and recall definitions were adapted so that wrong or missed non-consecutive edges are counted as True Positive when there is a path between their vertices in either the ground-truth or the computed graph. Table  For the patterns of lesion changes, we compared the computed and ground truth patterns of lesion changes. The accuracy is the % of identical connected components in each category. Table  After reviewing the 42 and 37 lesions labeled as Lone in DLUNGS and DLIVER with > 5mm diameter, the radiologist determined that 1 and 8 of them had been wrongly identified as a cancerous lesion. Moreover, he found that 14 and 16 lesions initially labeled as Lone, had been wrongly classified: for these lesions he found 15 and 21 previously unmarked matching lesions in the next or previous scans. In total, 45 and 62 missing lesions were added to the ground truth DLUNGS and DLIVER datasets, respectively. These hard-to-find ground-truth False Negatives (3.7%, 7.2% of all lesions) may change the radiological interpretation and the disease status. See the Supplemental Material for examples of these scenarios."
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,4.0,Conclusion,"The use of graph-based methods for lesion tracking and detection of patterns of lesion changes was shown to achieve high accuracy in classifying changes in individual lesion and identifying patterns of lesion changes in liver and lung longitudinal CT studies of patients with metastatic disease. This approach has proven to be useful in detecting missed, faint, and surmised to be present lesions, otherwise hardly detectable by examining the scans separately or in pairs, leveraging the added information provided by evaluating all patient's scans simultaneously using the labels from the lesion changes graph and non-consecutive edges."
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,2.2,Lesion Matching Computation,"Lesion matchings are determined by the location and relative proximity of the lesions in two or more registered scans. The lesion matching rule is lesion voxel overlap: when the lesion segmentation voxels l i j , l k l of vertices v i j , v k l overlap, 1 ≤ i < k ≤ N , they are matched and the edge e i,k j,l = v i j , v k l is added to E. Lesion matchings are computed first on consecutive pairs and then on non-consecutive pairs of scans. Consecutive lesion matching on scans (S i , S i+1 ) is performed with an iterative greedy strategy whose aim is to compensate for registration errors: 1) the lesion segmentations in L i and L i+1 are isotropically dilated in 3D by d millimeters; 2) for all pairs of lesions v i j , v i+1 l , compute the intersection % of their corresponding lesion a segmentations l is added to E c ; ; 4) remove the lesion segmentations l i j , l i+1 l from L i , L i+1 , respectively. Steps 1-4 are repeated r times. This yields the consecutive edges graph ). The values of d , r are pre-defined empirically. Lesion matching on non-consecutive scans searches for lesion matchings that were not found previously due to missing lesions (unseen or undetected). It is performed by examining the pairs of connected components of G C and finding possible edges (lesion pairings) between them. Formally, let CC = {cc m } M m=1 be the set of undirected connected components of G C . Let τ m = t first m , t last m be the time interval between the first and last scans of cc m , and let centroid (cc m ) be the center of mass of all lesions in cc m at all times. Let G CC = (V cc , E cc ) be the graph of connected components of G C such that each vertex cc m of V cc corresponds to a connected component cc m and edges e CC i,j = cc i , cc j of E CC satisfy three conditions: 1) the time interval of cc i is disjoint and precedes by at least one time point that of cc j , i.e., t first i > t last i + 1; 2) the connected components are not too far from each other, i.e., the distance between the connected components centroids is smaller than a fixed distance δ, centroid (cc i )centroid cc j ≤ δ; 3) there is at least one pairwise matching between a lesion in t last i and a lesion in t first j computed with the consecutive lesion matching method described above. When the consecutive lesion matching between the lesions in t last i in cc i and the lesions in t first j in cc j yields a nonempty set of edges, these edges are added as non-consecutive edges to E c . . Iterating over all non-ordered pairs (cc i , cc j ) yields the set of consecutive and non-consecutive edges of E. We illustrate this process with the graph of Fig. "
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,1.0,Introduction,"Neuropsychiatric systemic lupus erythematosus (NPSLE) refers to a complex autoimmune disease that damages the brain nervous system of patients. The clinical symptoms of NPSLE include cognitive disorder, epilepsy, mental illness, etc., and patients with NPSLE have a nine-fold increased mortality compared to the general population  Although conventional magnetic resonance imaging (MRI) tools are widely used to detect brain injuries and neuronal lesions, around 50% of patients with NPSLE present no brain abnormalities in structural MRI  On the other hand, for the discovery task of potential biomarkers, sparse codingbased methods (e.g., 2,1 norm, 2,0 norm, etc.) force row elements to zero that remove some valuable features  In light of this, we propose a robust exclusive adaptive sparse feature selection (REASFS) algorithm to jointly address the aforementioned problems in biomarker discovery and early diagnosis of NPSLE. Specifically, we first extend our feature learning through generalized correntropic loss to handle data with complex non-Gaussian noise and outliers. We also present the mathematical analysis of the adaptive weighting mechanism of generalized correntropy. Then, we propose a novel regularization called generalized correntropy-induced exclusive 2,1 to adaptively accommodate various sparsity levels and preserve informative features. The experimental results on a benchmark NPSLE dataset demonstrate the proposed method outperforms comparing methods in terms of early noninvasive biomarker discovery and early diagnosis."
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,2.0,Method,"Dataset and Preprocessing: The T2-weighted MR images of 39 participants including 23 patients with NPSLE and 16 HCs were gathered from our affiliated hospital. All images were acquired at an average age of 30.6 years on a SIGNA 3.0T scanner with an eight-channel standard head coil. Then, the MR images were transformed into spectroscopy by multi-voxel 1 H-MRS based on a point-resolved spectral sequence (PRESS) with a two-dimensional multi-voxel technique. The collected spectroscopy data were preprocessed by a SAGE software package to correct the phase and frequency. An LCModel software was used to fit the spectra, correct the baseline, relaxation, and partial-volume effects, and quantify the concentration of metabolites. Finally, we used the absolute NAA concentration in single-voxel MRS as the standard to gain the absolute concentration of metabolites, and the NAA concentration of the corresponding voxel of multi-voxel 1 H-MRS was collected consistently. The spectra would be accepted if the SNR is greater than or equal to 10 and the metabolite concentration with standard deviations (SD) is less than or equal to 20%. The absolute metabolic concentrations, the corresponding ratio, and the linear combination of the spectra were extracted from different brain regions: RPCG, LPCG, RDT, LDT, RLN, LLN, RI, RPWM, and LPWM. A total of 117 metabolic features were extracted, and each brain region contained 13 metabolic features: Cr, phosphocreatine (PCr), Cr+PCr, NAA, NAAG, NAA+NAAG, NAA+NAAG/Cr+PCr, mI, mI/Cr+PCr, Cho+phosphocholine (PCh), Cho+PCh/Cr+PCr, Glu+Gln, and Glu+Gln/Cr+PCr."
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,2.1,Sparse Coding Framework,"Given a data matrix X = [x 1 ; • • • ; x n ] ∈ R n×d with n sample, the i-th row is represented by x i , and the corresponding label matrix is denoted as where y i is one-hot vector. The Frobenius norm of For sparse codingbased methods, the general problem can be formulated as where L(W), R(W), and λ are the loss function, the regularization term, and the hyperparameter, respectively. For least absolute shrinkage and selection operator (LASSO)  For multi-task feature learning  . Due to the row sparsity of 2,1 norm, the features selected for all different classes are enforced to be exactly the same. Thus, the inflexibility of 2,1 norm may lead to the deletion of meaningful features."
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,2.2,Generalized Correntropic Loss,"Originating from information theoretic learning (ITL), the correntropy  where E, k σ (•, •), and F AB (a, b) denote the mathematical expectation, the Gaussian kernel, and the joint probability density function of (A, B), respectively. When applying correntropy to the error criterion, the boundedness of the Gaussian kernel limits the disturbance of large errors caused by outliers on estimated parameters. However, the kernelized second-order statistic of correntropy is not suitable for all situations. Therefore, the generalized correntropy  where Γ (•) is the gamma function, α, β > 0 are the shape and bandwidth parameters, respectively, s = 1/β α is the kernel parameter, γ = α/(2βΓ (1/α)) is the normalization factor. Specifically, when α = 2 and α = 1, GGD degenerates to Gaussian distribution and Laplacian distribution, respectively. As an adaptive similarity measure, generalized correntropy can be applied to machine learning and adaptive systems  To analyze the adaptive weighting mechanism of generalized correntropy, we consider an alternative problem of (1), where The optimal projection matrix W should satisfy (∂J(W)/∂W) = 0, and we have where Λ is a diagonal matrix with error-based diagonal elements for adaptive sample weight."
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,2.3,"Generalized Correntropy-Induced Exclusive 2,1","To overcome the drawback of 2,1 norm and achieve adaptive sparsity regularization on metabolic features of different brain regions, we propose a novel GCIE 2,1 . A flexible feature learning algorithm exclusive 2,1  where Based on the exclusive 2,1 , we can not only removes the redundant features shared by all classes through row sparsity of 2,1 norm but also selects different discriminative features for each class through exclusive sparsity of 1,2 norm. Then we propose to introduce generalized correntropy to measure the sparsity penalty in the feature learning algorithm. We apply generalized correntropy to the row vector w i of W to achieve the adaptive weighted sparse constraint, and the problem (6) can be rewritten as where Since minimizing w i 2 is equivalent to maximizing exp(-s w i 2 ), we add a negative sign to this term. Through the GCIE 2,1 regularization term in  Optimization: Since the GCIE 2,1 is a non-smooth regularization term, the final problem "
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,3.0,Experimental Results and Conclusion,"Experimental Settings: The parameters α and λ 1 are are set to 1, while β and λ 2 are searched form {0.5, 1, 2, 5} and {0.1, 0.5, 1}, respectively. We use Adam optimizer and the learning rate is 0.001. To evaluate the performance of classification, we employ a support vector machine as the basic classifier, where the kernel is set as the radial basis function (RBF) and parameter C is set to 1. We average the 3-fold cross-validation results. Results and Discussion: We compare the classification accuracy of the proposed REASFS with several SOTA baselines, including two filter methods: maximal information coefficient (MIC) "
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,,Conclusion:,"In this paper, we develop REASFS, a robust flexible feature selection that can identify metabolic biomarkers and detect NPSLE at its early stage from noisy 1 H-MRS data. The main advantage of our approach is its robust generalized correntropic loss and a novel GCIE 2,1 regularization, which jointly utilizes the row sparsity and exclusive sparsity to adaptively accommodate various sparsity levels and preserve informative features. The experimental results show that compared with previous methods, REASFS plays a very important role in the biomarker discovery and early diagnosis of NPSLE. Finally, we analyze metabolic features and point out their clinical significance."
Multi-view Vertebra Localization and Identification from CT Images,1.0,Introduction,"Automatic Localization and identification of vertebra from CT images are crucial in clinical practice, particularly for surgical planning, pathological diagnosis, and post-operative evaluation  With the advance of deep learning, many methods are devoted to tackling these challenges. For example, Lessmann et al.  In this paper, to tackle the aforementioned challenges, we present a novel framework that converts the 3D vertebra labeling problem into a multi-view 2D vertebra localization and identification task. Without the 3D patch limitation, our network can learn 2D global information naturally from different view perspectives, as well as leverage the pre-trained models from ImageNet "
Multi-view Vertebra Localization and Identification from CT Images,2.0,Methodology,An overview of our proposed method for vertebra localization and identification using multi-view DRR from CT scans is shown in Fig. 
Multi-view Vertebra Localization and Identification from CT Images,2.1,DRR Multi-view Contrastive Learning,"DRR Generation. To accurately localize and identify each vertebra in CT images, we convert the 3D task into 2D, where global information can be naturally captured from different views, avoiding the large computation of 3D models. To achieve this, DRR (Digitally Reconstructed Radiograph) technique, a simulation procedure for generating a radiograph similar to conventional X-ray image, is performed by projecting a CT image onto a virtual detector plane with a virtual X-ray source. In this way, we can generate K DRR projection images of a CT image for every 360/K degree. The 3D labeling problem can then be formulated as a multi-view localization and identification task in a 2D manner. Specifically, the 2D ground-truth are generated by projecting the 3D centroids and labels onto the 2D image following the DRR projection settings. DRR Multi-view Contrastive Learning. After DRR generation, our goal is to localize and identify the vertebra on DRR images. However, as the dataset for the vertebra task is relatively small due to time-consuming manual annotation, we design a new multi-view contrastive learning strategy to better learn the vertebrae representation from various views. Unlike previous contrastive learning methods, where the pretext is learned from numerous augmented negative and positive samples "
Multi-view Vertebra Localization and Identification from CT Images,2.2,Single-view Vertebra Localization,"With multi-view DRR images, the 3D vertebra localization problem is converted into a 2D vertebra centroid detection task, followed by a multi-view fusion strategy (as introduced in Sect. 2.4) that transforms the 2D results to 3D. To achieve this, we utilize the commonly-used heatmap regression strategy for 2D vertebra centroid detection. Specifically, for each vertebra in a DRR image, our model is trained to learn the contextual heatmap defined on the ground-truth 2D centroid using a Gaussian kernel. During inference, we apply a fast peak search clustering method "
Multi-view Vertebra Localization and Identification from CT Images,2.3,Single-View Vertebra Identification,"After the vertebrae localization, we further predict the label of each vertebra using an identification network on multi-view DRR images. Unlike other 3D methods that require cropping vertebra patches for classification, our identification network performs on 2D, allowing us to feed the entire DRR image into the network, which can naturally capture the global information. Specifically, we use a segmentation model to predict the vertebra labels around the detected vertebra centroids, i.e., a 22 mm × 22 mm square centered at the centroid. During the inference of single-view, we analyze the pixel-wise labels in each square and identify the corresponding vertebra with the majority number of labels. Sequence Loss. In the identification task, we observe that the vertebra labels are always in a monotonically increasing order along the spine, which implies the presence of sequential information. To better exploit this property and enhance our model to capture such sequential information, we propose a Sequence Loss as an additional network supervision, ensuring the probability distribution along the spine follows a good sequential order. Specifically, as shown in Fig.  where i ∈ [1, n] and j ∈  The overall loss function L id for our identification network is: where L ce and L s refer to the Cross-Entropy loss and Sequence Loss, respectively. γ is a parameter to control the relative weights of the two losses."
Multi-view Vertebra Localization and Identification from CT Images,2.4,Multi-view Fusion,"Localization Multi-view Fusion. After locating all the vertebrae in each DRR image, we fuse and map the 2D centroids back to 3D space by a leastsquares algorithm, as illustrated in Fig.  where p denotes the 3D coordinate of the optimal intersection point, a k and n k represent the point on the k th target line and the corresponding direction vector. By taking derivatives with respect to p, we get a linear equation of p as shown in Eq. (  ( Identification Multi-view Voting. The Sequence Loss evaluates the quality of the predicted vertebra labels in terms of their sequential property. During inference, we further use this Sequence Loss of each view as weights to fuse the probability maps obtained from different views. We obtain the final voted identification map V of K views as: For each vertebra, the naive solution for obtaining vertebra labels is to extract the largest probability from each row in voted identification map V . Despite the promising performance of the identification network, we still find some erroneous predictions. To address this issue, we leverage the dynamic programming (described in Eq. ( "
Multi-view Vertebra Localization and Identification from CT Images,3.1,Dataset and Evaluation Metric,We extensively evaluate our method on the publicly available MICCAI VerSe19 Challenge dataset 
Multi-view Vertebra Localization and Identification from CT Images,3.2,Implementation Details,"All CT scans are resampled to an isotropic resolution of 1 mm. For DRR Multi-View Contrastive Learning, we use ResNet50 as encoder and apply the SGD optimizer with an initial learning rate of 0.0125, which follows the cosine decay schedule. The weight decay, SGD momentum, batch size and loss function are set to 0.0001, 0.9, 64, and cosine similarity respectively. We employ U-Net for both the localization and identification networks, using the pre-trained ResNet50 from our contrastive learning as backbone. Adam optimizer is set with an initial learning rate of 0.001, which is divided by 10 every 4000 iterations. Both networks are trained for 15k iterations. We empirically set α = 0.1, β = 0.8, γ = 1. All methods were implemented in Python using PyTorch framework and trained on an Nvidia Tesla A100 GPU with 40 GB memory."
Multi-view Vertebra Localization and Identification from CT Images,3.3,Comparison with SOTA Methods,"We train our method on 70 CT images and tune the hyperparameter on the rest 10 CT images from the training data. We then evaluate it on both testing and hidden datasets, following the same setting as the challenge. In the comparison, our method is compared with four methods which are the first four positions in the benchmark of this challenge  [17] and Chen M. "
Multi-view Vertebra Localization and Identification from CT Images,3.4,Ablation Study,Ablation Study of Key Components. We conduct an ablation study on the VerSe19 dataset to demonstrate the effectiveness of each component. As presented in Table  Ablation Study of Projection Number. We also conduct an ablation study on the same dataset to further evaluate the impact of the projection number K. The results are presented in Fig. 
Multi-view Vertebra Localization and Identification from CT Images,4.0,Conclusion,"In this paper, we propose a novel multi-view method for vertebra localization and identification in CT images. The 3D labeling problem is converted into a multi-view 2D localization and identification task, followed by a fusion strategy. In particular, we propose a multi-view contrastive learning strategy to better learn the invariant anatomical structure information from different views. And a Sequence Loss is further introduced to enhance the framework to better capture sequential structure embedded in vertebrae both in training and inference. Evaluation results on a public dataset demonstrate the advantage of our method."
Multi-view Vertebra Localization and Identification from CT Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_14.
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,1.0,Introduction,"Lung cancer is currently the foremost cause of cancer-related mortalities globally, with non-small cell lung cancer (NSCLC) being responsible for 85% of reported cases  Recently, several deep-learning methods have been put forward to differentiate between the NSCLC histological subtypes using CT images  Despite the promising results of previous multi-view methods, they still confront a severe challenge for accurate NSCLC histological subtype prediction. In fact, due to the limitation of scan time and hardware capacity in clinical practice, different views of CT volumes are anisotropic in terms of in-plane and inter-plane resolution  To overcome the challenge mentioned above, we propose a novel cross-aligned representation learning (CARL) method for the multi-view histologic subtype classification of NSCLC. CARL offers a holistic and disentangled perspective of multi-view CT images by generating both view-invariant and -specific representations. Specifically, CARL incorporates a cross-view representation alignment learning network which targets the reduction of multi-view discrepancies by obtaining discriminative view-invariant representations. A shared encoder with a novel discriminability-enforcing similarity constraint is utilized to map all representations learned from multi-view CT images to a common subspace, enabling cross-view representation alignment. Such aligned projections help to capture view-invariant features of cross-view CT images and meanwhile make full use of the discriminative information obtained from each view. Additionally, CARL learns view-specific representations as well which complement the view-invariant ones, providing a comprehensive picture of the CT volume data for histological subtype prediction. We validate our approach by using a publicly available NSCLC dataset from The Cancer Imaging Archive (TCIA). Detailed experimental results demonstrate the effectiveness of CARL in reducing multi-view discrepancies and improving NSCLC histological subtype classification performance. Our contributions can be summarized as follows: -A novel cross-aligned representation learning method called CARL is proposed for NSCLC histological subtype classification. To reduce the discrepancies of multiview CT images, CARL incorporates a cross-view representation alignment learning network for discriminative view-invariant representations. -We employ a view-specific representation learning network to learn view-specific representations as a complement to the view-invariant representations. -We conduct experiments on a publicly available dataset and achieve superior performance compared to the most advanced methods currently available. "
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,2.1,Architecture Overview,Figure 
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,2.2,Cross-View Representation Alignment Learning,"Since the discrepancies of different views may result in divergent statistical properties in feature space, e.g., huge distributional disparities, aligning representations of different views is essential for multi-view fusion. With the aim to reduce multi-view discrepancies, CARL introduces a cross-view representation alignment learning network for mapping the representations from distinct views into a common subspace, where view-invariant representations can be obtained by cross-view alignment. Specifically, inspired by  Technically speaking, given the axial view image I av , coronal view image I cv , and sagittal view image I sv , the cross-view representation alignment learning network tries to generate view-invariant representations h c v , v ∈ {av, cv, sv} via a shared encoder based on a residual neural network  where E c (•) indicates the shared encoder, and av, cv, sv represent axial, coronal, and sagittal views, respectively. In the common subspace, we hope that through optimizing the shared encoder E c (•), the view-invariant representations can be matched to some extent. However, the distributions of h c av , h c cv and h c sv are very complex due to the significant variations between different views, which puts a burden on obtaining well-aligned view-invariant representations with merely an encoder. To address this issue, we design a discriminability-enforcing similarity loss L dsim to further enhance the alignment of cross-view representations in the common subspace. Importantly, considering that the axial view has a higher resolution than other views and are commonly used in clinical diagnosis, we choose axial view as the main view and force the sub-views (e.g., the coronal and sagittal views) to seek distributional similarity with the main view. Mathematically, we introduce a cross-view similarity loss L sim which calculates the central moment discrepancy (CMD) metric  where CMD(•) denotes the distance metric which measures the distribution disparities between the representations of i-th sub-view h sub i and the main view h main . N is the number of sub-views. Despite the fact that minimizing the L sim can efficiently mitigate the issue of distributional disparities, it may not guarantee that the alignment network will learn informative and discriminative representations. Inspired by recent work on multimodal feature extraction  where y denotes the ground-truth subtype labels, λ controls the weight of L CE . We observed that L CE is hundred times smaller than L sim , so this study uses an empirical value of λ = 110 to balance the magnitude of two terms. By minimizing L dsim , the cross-view representation alignment learning network pushes the representations of each sub-view to align with those of the main view in a discriminability-enforcing manner. Notably, the benefits of such cross-alignment are twofold. Firstly, it greatly reduces the discrepancies between the sub-views and the main view, leading to consistent viewinvariant representations. Secondly, since the alignment between distinct views compels the representation distribution of the sub-views to match that of the discriminative main view, it can also enhance the discriminative power of the sub-view representations. In other words, the cross-alignment procedure spontaneously promotes the transfer of discriminative information learned by the representations of the main view to those of the sub-views. As a result, the introduced cross-view representation alignment learning network is able to generate consistent and discriminative view-invariant representations cross all views to effectively narrow the multi-view discrepancies."
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,2.3,View-Specific Representation Learning,"On the basis of learning view-invariant representations, CARL additionally learns view-specific representations in respective private subspaces, which provides supplementary information for the view-invariant representations and contribute to subtype classification as well. To be specific, a view-specific representation learning network containing three unique encoders is proposed to learn view-specific representations h p v , v ∈ {av, cv, sv}, enabling effective exploitation of the specific information from each view. We formulate the unique encoders as follows: where is the encoder function dedicated to capture single-view characteristics. To induce the view-invariant and -specific representations to learn unique characteristics of each view, we draw inspiration from  A reconstruction module is also employed to calculate a reconstruction loss L rec between original image I v and reconstructed image I r v using the L 1 -norm, which ensures the hidden representations to capture details of the respective view."
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,2.4,Histologic Subtype Classification,"After obtaining view-invariant and -specific representations from each view, we integrate them together to perform NSCLC subtype classification. Specifically, we apply a residual block "
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,2.5,Network Optimization,"The optimization of CARL is achieved through a linear combination of several loss terms, including discriminability-enforcing similarity loss L dsim , orthogonality loss L orth , reconstruction loss L rec and the classification loss L cls . Accordingly, the total loss function can be formulated as a weighted sum of these separate loss terms: where α, β and γ denote the weights of L dsim , L rec and L orth . To normalize the scale of L dsim which is much larger than the other terms, we introduce a scaling factor S = 0.001, and perform a grid search for α, β and γ in the range of 0.1S-S, 0.1-1, and 0.1-1, respectively. Throughout the experiments, we set the values of α, β and γ to 0.6S, 0.4 and 0.6, respectively."
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,3.1,Dataset,"Our dataset NSCLC-TCIA for lung cancer histological subtype classification is sourced from two online resources of The Cancer Imaging Archive (TCIA)  For preprocessing, given that the CT data from NSCLC-TCIA has an in-plane resolution of 1 mm × 1 mm and a slice thickness of 0.7-3.0 mm, we resample the CT images using trilinear interpolation to a common resolution of 1mm × 1mm × 1mm. Then one 128 × 128 pixel slice is cropped from each view as input based on the center of the tumor. Finally following "
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,3.2,Implementation Details,"The implementation of CARL is carried out using PyTorch and run on a workstation equipped with Nvidia GeForce RTX 2080Ti GPUs and Intel Xeon CPU 4110 @ 2.10GHz. Adam optimizer is used with an initial learning rate of 0.00002, and the batch size is set to 8."
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,3.3,Results,"Comparison with Existing Methods. Several subtype classification methods have been employed for comparison including: two conventional methods, four single-view and 3D deep learning methods, and four representative multi-view methods. We use publicly available codes of these comparison methods and implement models for methods without code. The experimental results are reported in Table "
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,4.0,Conclusion,"In summary, we propose a novel multi-view method called cross-aligned representation learning (CARL) for accurately distinguishing between ADC and SCC using multi-view CT images of NSCLC patients. It is designed with a cross-view representation alignment learning network which effectively generates discriminative view-invariant representations in the common subspace to reduce the discrepancies among multi-view images. In addition, we leverage a view-specific representation learning network to acquire viewspecific representations as a necessary complement. The generated view-invariant and -specific representations together offer a holistic and disentangled perspective of the multi-view CT images for histological subtype classification of NSCLC. The experimental result on NSCLC-TCIA demonstrates that CARL reaches 0.817 AUC, 76.8% Acc, 73.2% Sen, and 79.7% Spe and surpasses other relative approaches, confirming the effectiveness of the proposed CARL method."
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,1.0,Introduction,"Alzheimer's disease (AD) is an irreversible and progressive neurodegenerative disorder, and it is the most common cause of dementia. Early diagnosis and appropriate interventions play a vital role in managing AD  In recent years, brain network analyses via graph neural networks have been widely used in brain disease diagnosis  Although HGNN methods have achieved promising progress in many fields, such approaches have three major limitations for brain network analyses. (1)Lack of consideration of the unique topological properties in the brain network. The brain networks are cost-efficient small-world networks, which contain a series of organization patterns such as hub nodes and hierarchical modularity. However, current hypergraph constructed using K-nearest neighbors (KNN) exhibit a lack of power to extract specific high-order topological information from the brain network. (2)Lack of a robust ROI-aware encoding to overcome the anonymity of nodes in HGNN. Current GNN or HGNN methods are permutation invariant, indicating that the node order in a graph is insensitive to the performance of the graph neural network models. However, every brain region in the brain network has its specific brain function and location; such permutation invariance and anonymity are problematic for brain network analyses. (3)Lack of an appropriate mechanism to identify brain network dysfunction. Compared to identifying the disease-related brain regions, less attention has been paid to detecting the propagation patterns of neuropathological burdens using the HGNN methods, despite the brain network alterations with strong interpretation being more attractive to neuroimaging researchers. To overcome these limitations, we propose a hypergraph neural network explicitly tailored for brain networks to perform (1) identification of propagation patterns of neuropathology and (2) early diagnosis of AD. In this study, we adopt a second-order random walk on a brain network reference to generate two groups of hyperedges to depict the topology of the brain network. Additionally, an ROI identity encoding module is incorporated into our model to avoid the influence of anonymity of nodes during HGNN convolution on brain network analysis. Furthermore, we design a self-learned weighted hypergraph convolution to extract the discriminative hyperedges, which can be used to characterize the spreading pathways of neuropathological events in AD. The framework of our BrainHGNN is shown in Fig. "
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,2.1,Hypergraph Construction,"A brain network is often encapsulated into an adjacency matrix A s ∈ R N ×N (s = 1, • • • , S), where N stands for the number of brain regions in the brain network and S denotes the number of samples. To characterize the spreading pathway of neuropathological events in AD, we first calculate a group-mean adjacency matrix Ā = 1 S S s=1 A s as a brain network reference from a population of brain networks. Since complex brain functions are usually carried out by the interaction of multiple brain regions, a hypergraph neural network is introduced as the backbone of our proposed model to capture high-order relationships. A hyperedge in hypergraph connects more than two nodes, characterizing the high-order relationships. Let G = {V, E, W} denote a hypergraph with node set V and hyperedge set E, where each hyperedge is actually a subset of V randomly sampled based on Ā. Each diagonal element in the diagonal matrix W represents a weight of hyperedge. The hypergraph structure is represented by an incident matrix H ∈ R |V|×|E| , where H(v, e) = 1 means v node is in the e hyperedge. Moreover, the diagonal matrices of the hyperedge degrees D e and the vertex degrees D v can be calculated by δ(e) = v∈V H(v, e) and d(v) = e∈E w(e)H(v, e), respectively. Node Representation. Empirical biomarkers (such as deposition of amyloid plaques) calculated on PET neuroimaging data are the hallmarks of AD. We obtain the standard uptake value ratio for each brain region and represent them as a column vector f s ∈ R N , with each element corresponding to the feature of the node. However, the structural brain network has specific regions with distinct functions and messages, making the permutation invariance of hypergraph neural networks less suitable. We address this by generating a region-of-interest (ROI) identity encoding p ∈ R N ×m for each brain region and concatenating it with the empirical biomarkers, thereby reducing permutation invariance and anonymity of nodes in graph convolution. The parameter m denotes the dimension of identity encoding and is set to 2 in our experiments. The final node representation is formulated as, where f 0 s is a vector of the initial input biomarkers and p is a learnable matrix to encode the regional information. Hyperedge Generation. The brain, a complex network, exhibits information processing with maximum efficiency and minimum costs. Such brain organization requires the brain network to be equipped with both high local clustering (hierarchical modularity) and global efficiency (shortest path length)  where α p (v, x) = p dvx-1 , d vx ∈ {0, 1, 2} and d vx is the shortest path distance between node v and candidate target x. In Eq. ( "
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,2.2,Hypergraph Convolution,"As stated in  where X t ∈ R |V|×Mt is the node feature matrix in t th layer and Z t ∈ R |E|×Mt is the feature matrix of hyperedges in t th layer. Θ t+1 ∈ R Mt×Mt+1 is the learning parameter in (t + 1) th layer. The intuition behind Eq. (  In most cases, the hyperedge weight matrix W is pre-defined as either an identical matrix I or a diagonal matrix with specific elements determined by prior knowledge, for ease of use. Nevertheless, these two types of hyperedge weight matrices pose difficulties in identifying the discriminant hyperedges associated with AD-related brain network dysfunction. To address this issue, we propose using a learnable weight matrix mask to guide the learning of hyperedge weights, enabling us to identify propagation patterns of neuropathological burden in AD. The hyperedge convolution can be rewritten as, where W = sigmoid( M • W) is the learned hyperedge weight matrix, M is the learnable weight mask and • means Hadamard product."
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,2.3,Optimization,"In this study, we use the cross entropy loss to train our model and predict disease status. To avoid overfitting and discover AD-related propagation patterns, we first impose an l 1 -norm constraint to the learnable weight mask M to overshadow the irrelevant hyperedges and preserve the discriminative hyperedges that are highly related to the prediction of labels. Then, we require Θ to be smooth with an l 2 -norm constraint to void its divergence. Finally, the loss function can be formulated as follows: where z i is the predictive label of i th sample output from the fully connected layer by our BrainHGNN and y i is the ground truth of i th sample."
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,3.1,Dataset Description and Experimental Settings,"Data Processing. We collect 94 structural brain networks from the ADNI database (https://adni.loni.usc.edu) to calculate a group-mean adjacency matrix Ā to construct the incident matrix H of the common hypergraph. For each subject, we construct the structural brain network in the following steps. First, we apply Destrieux atlas  Parameter Settings. In our experiments, we set parameters num layer = 1, num epoch = 250, batchsize = 256, learning rate = 0.05. LeakyReLu activation function σ(•) is used for node convolution. The l 1 -norm is adopted on the learnable weight mask M with a hyper-parameter λ 1 = 1e -3 to extract the discriminative hyperedges. The l 2 -norm is applied on the learning parameter matrix Θ with a hyper-parameter λ 2 = 1e -5. Baselines. The comparison baselines incorporate two traditional methods, including support vector machines (SVM) and random forest (RF), as well as five graph-based methods, including graph convolution neural network (GCN) "
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,3.2,Evaluating Diagnostic Capability on Amyloid-PET and FDG-PET Data,"In this section, our objective is to evaluate the diagnostic performance of our BrainHGNN model. We first construct three group comparisons, including CN/ EMCI, EMCI/LMCI, and CN/LMCI, for amyloid-PET or FDG-PET data. For each comparison, we apply our BrainHGNN on empirical biomarkers to evaluate the classification metrics, including accuracy, F1-score, sensitivity, and specificity, according to 10-fold cross validation. From Table "
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,3.3,Evaluating the Statistical Power of Identifying Brain Network Dysfunction in AD,"In this section, we investigate the capability of identifying AD-related discriminative hyperedges by our BrainHGNN. Benefiting from the sparse weight mask, we can extract four discriminative hyperedges that most frequently occurred in the top-10 highest weight hyperedge list. Here, we take CN/LMCI group comparison as an example and map the discriminative hyperedges on the cortical surface for amyloid-PET data (first row in Fig. "
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,4.0,Conclusion,"In this paper, we propose a random-walk-based hypergraph neural network by integrating the topological nature of the brain network to predict the early stage of AD and discover the propagation patterns of neuropathological events in AD. Compared with other methods, our proposed BrainHGNN achieve enhanced classification performance and statistical power in group comparisons on ADNI neuroimaging dataset. In the future, we plan to apply our BrainHGNN to other neurodegenerative disorders that manifest brain network dysfunction."
Discovering Brain Network Dysfunction in Alzheimer’s Disease Using Brain Hypergraph Neural Network,,Acknowledgements,. This work was supported in part by the 
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,1.0,Introduction,"Gastroscopic Lesion Detection (GLD) plays a key role in computer-assisted diagnostic procedures. Although deep neural network-based object detectors achieve tremendous success within the domain of natural images, directly training generic object detectors on GLD datasets performs below expectations for two reasons: 1) The scale of labeled data in GLD datasets is limited in comparison to natural images due to the annotation costs. Though gastroscopic images are abundant, those containing lesions are rare, which necessitates extensive image review for lesion annotation. 2) The characteristic of gastroscopic images exhibits distinct differences from the natural images  Self-Supervised Backbone Pre-training methods enhance object detection performance by learning high-quality feature representations from massive unlabelled data for the backbone. The mainstream self-supervised backbone pretraining methods adopt self-supervised contrast learning  image modeling  Semi-Supervised object detection methods  The motivation of this paper is to explore how to enhance GLD performance using massive unlabeled gastroscopic images to overcome the labeled data shortage problem. The main challenge for this goal is the characteristic of gastroscopic lesions. Intuitively, such a challenge requires local feature representations to contain enough detailed information, meanwhile preserving discriminability. Enlightened by this, we propose the Self-and Semi-Supervised Learning (SSL) framework tailored to address challenges in daily clinical practice and use massive unlabeled data to enhance GLD performance. SSL overcomes the challenges of GLD by leveraging a large volume of unlabeled gastroscopic images using self-supervised learning for improved feature representations and semi-supervised learning to discover and utilize potential lesions to enhance performance. Specifically, it consists of a Hybrid Self-Supervised Learning (HSL) method for self-supervised backbone pre-training and a Prototype-based Pseudo-label Generation (PPG) method for semi-supervised detector training. The HSL combines the dense contrastive learning  -A Self-and Semi-supervise Learning (SSL) framework to leverage massive unlabeled data to enhance GLD performance. -A Large-scale Gastroscopic Lesion Detection datasets (LGLDD) -Experiments on LGLDD demonstrate that SSL can bring significant enhancement compared with baseline methods."
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,2.0,Methodology,"In this section, we introduce the main ideas of the proposed SSL for GLD. The proposed approach includes 2 main components and is illustrated in Fig. "
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,2.1,Hybrid Self-supervised Learning,"The motivation of Hybrid Self-Supervised Learning (HSL) is to learn the local feature representations of high discriminability meanwhile contain detailed information for the backbone from massive unlabeled gastroscopic images. Among existing backbone pre-training methods, dense contrastive learning can preserve local discriminability and masked image modeling can grasp local detailed information. Therefore, to leverage the advantages of both types of methods, we propose Hybrid Self-Supervised Learning (HSL), which combines patch reconstruction with dense contrastive learning to achieve the goal. Structure. HSL heritages the structure of the DenseCL  Enlightened by the SimMIM  Learning Pipeline. Like other self-supervised contrastive learning methods, HSL randomly generates 2 different ""views"" of the input image, uses the backbone to extract the dense feature maps F 1 , F 2 ∈ R H×W ×C , and then feeds them to the following projection heads. The global projection head of HSL uses F 1 , F 2 to obtain the global feature vector f g1 , f g2 like MoCo  The dense projection head use F 1 and F 2 to obtain local feature vector sets F l1 and F l2 (F l = {f l1 , f l2 , ..., f lS 2 }) like DenseCL  Training Objective. The HSL formulates the two contrastive learning as dictionary look-up tasks like DenseCL  The dense contrastive learning uses the local feature vector in F li as query r and keys T l = {t 1 , t 2 , ..., }. The negative keys t -here are the feature vectors of different images while the positive key t + is the correspondence feature vector of r in another view of the images. Specifically, we adopt the correspondence methods in DenseCL  The reconstruction task uses the feature vector in F to reconstruct each patch and obtain P i . The ground truth is the corresponding patches of the input view. We adopt the MSE loss function for it: The overall loss function is the weighted sum of these losses: where λ D and λ R are the weights of L D and L R and are set to 1 and 2."
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,2.2,Prototype-Based Pseudo-label Generation Method,"We propose the Prototype-based Pseudo-label Generation method (PPG) to discover potential lesions from unlabeled gastroscopic data meanwhile avoid introducing much noise to further enhance GLD performance. Specifically, PPG adopts a Memory Module to remember feature vectors of the representative lesions as memory and generates prototype feature vectors for each class based on the memories stored. To preserve the representativeness of the memory and further the prototype feature vectors, PPG designs a novel Memory Update Strategy. In semi-supervised learning, PPG generates pseudo-labels for unlabeled data relying on the similarity to the prototype feature vectors, which achieves a better balance between lesion discovery and noise avoidance. Memory Module. Memory Module stores a set of lesion feature vectors as memory. For a C-class GLD task, the Memory Module stores C × N feature vectors as memory. Specifically, for each lesion, we denote the feature vector used to classify the lesion in the detector as f c . PPG stores N feature vectors for each class c to formulate the class memory m c = {f c1 , f c2 , ..., f cN }, and the memory M of PPG can be expressed as M = {m 1 , m 2 , ..., m C }. Then, PPG obtains the prototype feature vector p c by calculating the center of each class memory m c , and the prototype feature vector set can be expressed as P t = {p 1 , p 2 , ..., p C }. Moreover, the prototype feature vectors further serve as supervision for detector training under a contrastive clustering formulation and adopt a contrastive loss: If the detector training loss is L Det , the overall loss L can be expressed as: where the λ cc is the weight of the contrastive learning loss and is set to 0.5. Memory Update Strategy. Memory Update Strategy directly influences the representativeness of the class memory m c and further the prototype feature vector p c . Therefore, PPG adopts a novel Memory Update Strategy, which follows the idea that ""The Memory Module should preserve the more representative feature vector among similar feature vectors"". The pipeline of the strategy is as follows: 1) Acquisition the lesion feature vector f c . 2) Identification of the most similar f s to f c from corresponding class memory m c based on similarity: 3) Updating the memory by selecting more unique features f s of F = {f s , f c } compared to the class prototype feature vector p c based upon similarity: The similarity function sim(u, v) can be expressed as sim(u, v) = u T v/ u v . To initialize the memories, we empirically select 50 lesions randomly for each class. To maintain stability, we start updating the memory and calculating its loss after fixed epochs, and only the positive sample feature vector can be selected to update the memory. Pseudo-label Generation. PPG proposes to generate pseudo-labels based on the similarity between the prototype feature vectors and the feature vector of potential lesions. To be specific, PPG first detects a large number of potential lesions with a low objectiveness score threshold τ u and then matches them with all the prototype feature vectors P to find the most similar one: PPG assigns the pseudo-label c for similarity value sim(p c , f u ) greater than the similarity threshold τ s otherwise omits it. We set τ u = 0.5 and τ s = 0.5"
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,3.0,Datasets,"We contribute the first Large-scale Gstroscopic Lesion Detection Datasets (LGLDD) in the literature. Collection : LGMDD collects about 1M+ gastroscopic images from 2 hospitals of about 500 patients and their diagnosis reports. After consulting some senior doctors and surveying gastroscopic diagnosis papers  Evaluation Metrics : We use standard object detection metrics to evaluate the GLD performance, which computes the average precision (AP) under multiple intersection-of-union (IoU) thresholds and then evaluate the performance using the mean of APs (mAP) and the AP of some specific IoU threshold. For mAP, we follow the popular object detection datasets COCO  We also report AP under some specific IoU threshold (AP 50 for .5, AP 75 for .75) and AP of different scale lesions (AP S , AP M , AP L ) like COCO "
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,4.0,Experiments,"Please kindly refer to the Supplemental Materials for implementation details and training setups. The Objectiveness score threshold controls the quality of pseudo-labels. a) A low threshold generates noisy pseudo-labels, leading to reduced performance (-0.6/-0.2 AP at thresholds 0.5/0.6). b) A high threshold produces high-quality pseudo-labels but may miss potential lesions, resulting in only slight performance improvement (+0.3 AP at threshold 0.7). c) PPG approach uses a low threshold (0.5) to identify potential lesions, which are then filtered using prototype feature vectors, resulting in the most significant performance enhancement (+0.9 AP). 3) Memory Update Strategy influences the representativeness of memory and the prototype feature vectors. We compare our Memory Update Strategy with a queue-like ('Q-like') memory update strategy (first in & first out). Experiment results (Table "
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,5.0,Conclusion,"In this work, we propose Self-and Semi-Supervised Learning (SSL) for GLD tailored for using massive unlabeled gastroscopic to enhance GLD performance. The key novelties of the proposed method include a Hybrid Contrastive Learning method for backbone pre-training and a Prototype-based Pseudo-Label Generation method for semi-supervised learning. Moreover, we contribute the first Large-scale GLD Datasets (LGLDD). Experiments on LGLDD prove that SSL can bring significant improvements to GLD performance. Since annotation cost always limits of datasets scale of such tasks, we hope SSL and LGLDD could fully realize its potential, as well as kindle further research in this direction."
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,1.0,Introduction,"Precision medicine permits more informed treatment decisions to be made based on individual patient characteristics (e.g. age, sex), with the goal of improving patient outcomes. Deep causal models based on medical images can significantly improve personalization by learning individual, data-driven features to predict the effect of treatments. 1 As a result, they could significantly improve patient outcomes, particularly in the context of chronic, heterogeneous diseases  However, despite significant advances, predictive deep learning models for medical image analysis are not immune to error, and severe consequences for the patient can occur if a clinician trusts erroneous predictions. A provided measure of uncertainty for each prediction is therefore essential to trust the model  Image-based precision-medicine is highly relevant in multiple sclerosis (MS), a chronic disease characterized by the appearance over time of new or enlarging T2 lesions (NE-T2) on MRI  To integrate uncertainty into clinical decision making, new validation measures must be defined. The usual strategy for validating uncertainty estimates, discarding uncertain predictions  In this work, we present the first uncertainty-aware causal model for precision medicine based on medical images. We validate our model on a large, multi-center dataset of MR images from four different randomized clinical trials (RCTs) for MS. Specifically, we develop a multi-headed, Bayesian deep learning probabilistic model "
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,2.1,Background on Individual Treatment Effect Estimation,"We frame precision medicine as a causal inference problem. Specifically, we wish to predict factual outcomes (on the treatment a patient received), counterfactual outcomes (on treatments a patient did not receive), as well as the individual treatment effect (ITE, the difference between the outcomes on two treatments). Let X ∈ R d be the input features, Y ∈ R be the outcome of interest, and T ∈ {0, 1, ..., m} be the treatment allocation with t = 0 as a control (e.g. placebo) and the remaining are m treatment options. Given a dataset containing triples D = {(x i , y i , t i )} n i=1 , the ITE for patient i and a drug T = t can be defined using the Neyman/Rubin Potential Outcome Framework  and is an observable quantity. Treatment effect estimation in machine learning therefore relies on a related causal estimand, τ t : τ t (x)"
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,2.2,Probabilistic Model of Individual Treatment Effects,"In this work, we seek to learn the probability distribution of individual potential outcome predictions ŷt (x) and the effect estimates ITE t (x). Let ŷt (x) ∼ N ( μt (x), σt 2 (x)) be a normal distribution for potential outcome predictions whose parameters are outputs of a neural network. This probabilistic framework conveniently allows for propagating the uncertainty estimates for each potential outcome to an uncertainty estimate for personalized treatment effects. Assuming independence between the two Gaussian distributions, ITE t (x) ∼ N ( μt (x) -μ0 (x), σt (x) 2 + σ0 (x) 2 ). For our specific context, the input x to our model consists of multi-sequence patient MRI, lesion maps, and clinical and demographic features at baseline. The model is based on a multi-headed network for treatment response estimation "
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,2.3,Evaluating Probabilistic Predictions,"Bounds for the ITE Error We can validate the quality of the estimated uncertainty for factual outcome predictions through the correlation between predictive uncertainty and Mean Squared Error (MSE) error. However, given that ground truth for the individual treatment effects are not available, we cannot compute MSE between ITE t and ITE t (x). In this work, we choose to compute the upper and lower bounds for this MSE. We validate our uncertainty estimates by showing that selecting patients with the highest confidence in their predictions reduces the bounds on the ITE error. The bounds serve as an approximation to the true ITE error, and can validate models even if the ground truth ITE is not available. We use the upper bound for the MSE as in  Evaluating Individual Treatment Recommendations. Predictive uncertainty can be used to improve treatment recommendations for the individual. Let π(x i , t i ) ∈ {0, 1} be a treatment recommendation policy taking as input a patient's features x i and their factual treatment assignment t i . The binary output of π(x i , t i ) denotes whether t i is recommended under π. In this work, we set π to be a function of the model's predictions, μt (x) and σt (x). For example, π can be defined such that a treatment is recommended if the number of predicted NE-T2 lesions on a particular treatment are less than 2  For the example of NE-T2 lesions, a lower value for ERUPT is better because there were fewer lesions on average for patients on the recommended treatment. Uncertainty for Clinical Trial Enrichment. Enriching a trial with predicted responders has been shown to increase statistical power in the context of MS  Where Var(y t ) is the variance of factual outcomes on treatment t. The approach taken by  3 Experiments and Results"
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,3.1,Dataset,"The dataset is composed of patients from four randomized clinical trials: BRAVO  Next, we examine the results for the ITE error. Figure "
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,3.3,Uncertainty for Individual Treatment Recommendations,"The effect of integrating uncertainty into treatment recommendations is evaluated by defining a policy using this uncertainty (Eq. 3). Here, we report outcomes on the lesion values (as opposed to log-lesions) for interpretability. In Fig.  In Fig. "
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,3.4,Uncertainty for Clinical Trial Enrichment,"Uncertainty estimation can also be useful when selecting a sub-population of to enroll in a clinical trial, in a technique called predictive enrichment  groups. As expected, groups with smaller average ITE uncertainty have greater statistical differences (lower z scores)."
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,4.0,Conclusion,"In this work, we present a novel, causal, probabilistic, deep learning framework for image-based precision medicine. Our multi-headed architecture produces distributions over potential outcomes on multiple treatment options and a distribution over personalized treatment effects. We evaluate our model on a real-world, multi-trial MS dataset, where we demonstrate quantitatively that integrating the uncertainties associated with each prediction can improve treatment-related outcomes in several real clinical scenarios compared to a simple mean prediction. The evaluation methods used in this work are agnostic to the method of uncertainty quantification which permits flexibility in the choice of measure. Overall, this work has the potential to greatly increase trust in the predictions of causal models for image-based precision medicine in the clinic."
Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_46.
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,1.0,Introduction,"Segmentation is a critical task in medical image analysis. Known approaches mainly utilize discrete data representations (e.g., rasterized label masks) with convolutional neural networks (CNNs)  Instead of segmenting structures with discrete grids, we explore the use of Implicit Neural Representations (INRs) which employ continuous representations to compactly capture coordinate-based signals (e.g., objects in images). INRs represent object shapes with a parameterized function f θ : (p, z) → [0, 1] that maps continuous spatial coordinates p = (x, y, z), x, y, z ∈ [-1, 1] and a shape embedding vector z to occupancy scores. This formulation enables direct modeling of object contours as the decision boundary of f θ , superior memory efficiency  The adoption of INRs for medical image segmentation, however, has been limited where most existing approaches directly apply pipelines designed for 3D reconstruction to images. These works emphasize either global embeddings z or point-wise ones. OSSNet  To address these limitations, we propose SwIPE (Segmentation with Implicit Patch Embeddings) which learns continuous representations of foreground shapes at the patch level. By decomposing objects into parts (i.e., patches), we aim to enable both accurate local boundary delineation and global shape coherence. This also improves model generalizability and training efficiency since local curvatures often reoccur across classes or images. SwIPE first encodes an image into descriptive patch embeddings and then decodes the point-wise occupancies using these embeddings. "
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,2.0,Methodology,"The core idea of SwIPE (overviewed in Fig.  In a typical discrete segmentation setting with C classes, an input image X is mapped to class probabilities with the same resolution f : X ∈ R H×W ×3 → Ŷ ∈ R H×W ×C . Segmentation with INRs, on the other hand, maps an image X and a continuous image coordinate p i = (x, y), x, y ∈ [-1, 1], to the coordinate's class-wise occupancy probability ôi ∈ R C , yielding f θ : (p i , X) → ôi , where f θ is parameterized by a neural network with weights θ. As a result, predictions of arbitrary resolutions can be obtained by modulating the spatial granularity of the input coordinates. This formulation also enables the direct use of discrete pixel-wise losses like Cross Entropy or Dice with the added benefit of boundary modeling. Object boundaries are represented as the zero-isosurface in f θ 's prediction space or, more elegantly, f θ 's decision boundary. SwIPE builds on the INR segmentation setting (e.g., in "
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,2.1,Image Encoding and Patch Embeddings,"The encoding process utilizes the backbone E b and neck E n to obtain a global image embedding z I and a matrix Z P of patch embeddings. We define an image patch as an isotropic grid cell (i.e., a square in 2D or a cube with identical spacing in 3D) of length S from non-overlapping grid cells over an image. Thus, an image X ∈ R H×W ×3 with a patch size S will produce H S • W S patches. For simplicity, we assume that the image dimensions are evenly divisible by S. A fully convolutional encoder backbone E b (e.g., Res2Net-50  n=2 to produce z I (the shape embedding for the entire image) and Z P (the grid of shape embeddings for patches). The feature maps are initially fed into a modified Receptive Field Block  The context-enriched feature maps are then fed through multiple cascaded aggregation and downsampling operations (see E n in Fig.  n=2 to patch embeddings Z P , we first resize them to Z P 's final shape via linear interpolation to produce {F n } 5 n=2 , which contain low-level (F 2 ) to high-level (F 5 ) information. Resizing enables flexibility in designing appropriate patch coverage, which may differ across tasks due to varying structure sizes and shape complexities. Note that this is different from the interpolative sampling in  , where w i is the ith weight of W. Compared to other spatial attention mechanisms like CBAM "
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,2.2,Implicit Patch Decoding,"Given an image coordinate p I i and its corresponding patch embedding z P i , the patch-wise occupancy can be decoded with decoder D P : (p P i , z P i ) → ôP i , where D P is a small MLP and p P i is the patch coordinate with respect to the patch center c i associated with z P i and is obtained by p P i = p I ic i . But, this design leads to poor global shape predictions and discontinuities around patch borders. To encourage better global shape coherence, we also incorporate a global image-level decoder D I . This image decoder, D I : (p I i , z I ) → ôI i , predicts occupancies for the entire input image. To distill higher-level shape information into patch-based predictions, we also condition D P 's predictions on p I i and z I . Furthermore, we find that providing the source coordinate gives additional spatial context for making location-coherent predictions. In a typical segmentation pipeline, the input image X is a resized crop from a source image and we find that giving the coordinate p S i (S for source) from the original uncropped image improves performance on 3D tasks since the additional positional information may be useful for predicting recurring structures. Our enhanced formulation for patch decoding can be described as D P : (p P i , z P i , p I i , z I , p S i ) → ôP i . To address discontinuities at patch boundaries, we propose a training technique called Stochastic Patch Overreach (SPO) which forces patch embeddings to make predictions for coordinates in neighboring patches. For each patch point and embedding pair (p P i , z P i ), we create a new pair (p P i , z P i ) by randomly selecting a neighboring patch embedding and updating the local point to be relative to the new patch center. This regularization is modulated by the set of valid choices to select a neighboring patch (connectivity, or con) and the number of perturbed points to sample per batch point (occurrence, or N SPO ). con = 4 means all adjoining patches are neighbors while con = 8 includes corner patches as well. Note that SPO differs from the regularization in "
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,2.3,Training SwIPE,"To optimize the parameters of f θ , we first sample a set of point and occupancy pairs {p S i , o i } i∈I for each source image, where I is the index set for the selected points. We obtain an equal number of points for each foreground class using Latin Hypercube sampling with 50% of each class's points sampled within 10 pixels of the class object boundaries. The point-wise occupancy loss, written as , where ôc i is the predicted probability for the target occupancy with class label c. Note that in practice, these losses are computed in their vectorized forms; for example, the Dice loss is applied with multiple points per image instead of an individual point (similar to computing the Dice loss between a flattened image and its flattened mask). The loss for patch and image decoder predictions is where α is a local-global balancing coefficient. Similarly, the loss for the SPO occupancy prediction Finally, the overall loss for a coordinate is formulated as , where β scales SPO and the last term (scaled by λ) regularizes the patch & image embeddings, respectively."
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,3.0,Experiments and Results,"This section presents quantitative results from four main studies, analyzing overall performance, robustness to data shifts, model & data efficiency, and ablation & component studies. For more implementation details, experimental settings, and qualitative results, we refer readers to the Supplementary Material. The losses for both tasks are optimized with AdamW  For fair comparisons, all the methods are trained using the same equallyweighted Dice and Cross Entropy loss for 30,000 and 50,000 iterations on 2D Sessile and 3D BCV, resp. The test score at the best validation epoch is reported. Image input sizes were 384 × 384 for Sessile and 96 × 96 × 96 for BCV. All the implicit methods utilize the same pre-sampled points for each image. For IOSNet "
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,3.2,Study 1: Performance Comparisons,"The results for 2D Polyp Sessile and 3D CT BCV organ segmentation are presented in Table  On the smaller polyp dataset, we observe notable improvements over the bestknown implicit approaches (+6.7% Dice) and discrete methods (+2.5% Dice) with much fewer parameters (9% of PraNet "
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,3.3,Study 2: Robustness to Data Shifts,"In this study, we explore the robustness of various methods to specified target resolutions and dataset shifts. The left-most table in Table  The results for the dataset shift study are given in the middle of Table "
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,3.4,Study 3: Model Efficiency and Data Efficiency,To analyze the model efficiency (the right-most column of charts in Table 
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,3.5,Study 4: Component Studies and Ablations,The left side of Table 
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,4.0,Conclusions,"SwIPE represents a notable departure from conventional discrete segmentation approaches and directly models object shapes instead of pixels and utilizes continuous rather than discrete representations. By adopting both patch and image embeddings, our approach enables accurate local geometric descriptions and improved shape coherence. Experimental results show the superiority of SwIPE over state-of-the-art approaches in terms of segmentation accuracy, efficiency, and robustness. The use of local INRs represents a new direction for medical image segmentation, and we hope to inspire further research in this direction."
SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_31.
Revisiting Feature Propagation and Aggregation in Polyp Segmentation,1.0,Introduction,"Colorectal cancer is a life-threatening disease that results in the loss of millions of lives each year. In order to improve survival rates, it is essential to identify colorectal polyps early. Hence, regular bowel screenings are recommended, where endoscopy is the gold standard. However, the accuracy of endoscopic screening can heavily rely on the individual skill and expertise of the domain experts involved, which are prone to incorrect diagnoses and missed cases. To reduce the workload on physicians and enhance diagnostic accuracy, computer vision technologies, such as deep neural networks, are involved to assist in the presegmentation of endoscopic images. The UNet-like model uses skip connections that transmit only single-stage features. In contrast, our approach utilizes FPE to propagate features from all stages, incorporating a gate mechanism to regulate the flow of valuable information. Deep learning-based image segmentation methods have gained popularity in recent years, dominated by UNet  To address these limitations, we propose a novel feature enhancement network for polyp segmentation that employs Feature Propagation Enhancement (FPE) modules to transmit multi-scale features from all stages to the decoder. Figure  Our major contributions to accurate polyp segmentation are summarized as follows. (1) The method addresses the limitations of the UNet-like encoder-decoder architecture by introducing three modules: Feature Propagation Enhancement (FPE), Feature Aggregation Enhancement (FAE), and Multi-Scale Aggregation (MSA). ( "
Revisiting Feature Propagation and Aggregation in Polyp Segmentation,2.0,Method,"Overview. Our proposed feature enhancement network illustrated in Fig.  where, {P 1 , P 2 , P 3 , P 4 } is the set of pyramidal features from four stages with the spatial size of 1/4, 1/8, 1/16, 1/32 of the input respectively. Features with lower spatial resolution usually contain richer high-level semantics. Then, these features are transmitted by the feature propagation enhancement module (FPE) to yield the feature set {C 1 , C 2 , C 3 , C 4 }, which provides multi-scale information from all the stages. This is different from the skip connection which only transmits the single-scale features at the present stage. Referring to  A noteworthy observation is that the gating mechanism has been widely utilized in both FPE and FAE to modulate the transmission and integration of features. By selectively controlling the flow of relevant information, this technique has shown promise in enhancing the overall quality of feature representations  Feature Propagation Enhancement Module. In contrast to the traditional encoder-decoder architecture with skip connections, the FPE aims to transmit multi-scale information from full stage at the encoder to the decoder, rather than single-scale features at the current stage. The FPE architecture is illustrated in Fig.  The features from the other three stages, denoted as P 1 , P 2 , and P 3 , are downsampled or upsampled to generate P 1 , P 2 , and P 3 . Instead of directly combining the four inputs, FPE applies gate mechanisms to emphasize informative features. The gate mechanism takes the form of Y = G(X ) * Y, where G (in this work, Sigmoid is used) measures the importance of each feature vector in the reference feature X ∈ R H×W . By selectively enhancing useful information and filtering out irrelevant information, the reference features X assist in identifying optimal features Y at the current level. The output of G is in [0, 1] H×W , which controls the transmission of informative features from Y or helps filter useless information. Notably, X can be Y itself, serving as a reference feature. FPE leverages such gate mechanism to obtain informative features in P and passes them through a CU respectively. After that, FPE concatenates features from the four branches to accomplish feature aggregation. A CU is followed to boost the feature fusion. Feature Aggregation Enhancement Module. The FAE is a novel approach that integrates the outputs of the last stages at the decoder with the FPE's outputs at both the current and deeper stages to compensate for the high-level semantics that may be lost in the process of progressive feature fusion. In contrast to the traditional encoder-decoder architecture with skip connections, the FAE assimilates the output of the present and higher-stage FPEs, delivering richer spatial and semantic information to the decoder. The FAE, depicted in Fig.  Multi-Scale Aggregation Module. The MSA module in our proposed framework, inspired by the Parallel Partial Decoder in PraNet "
Revisiting Feature Propagation and Aggregation in Polyp Segmentation,3.0,Experiments,"Datasets. We conduct extensive experiments on five polyp segmentation datasets, including Kvasir  Implementations. We utilize PyTorch 1.10 to run experiments on an NVIDIA RTX3090 GPU. We set an initial learning rate to 1e-4 and halve it after 80 epochs. We train the model for 120 epochs. The same multi-scale input and gradient clip strategies used in  Ablation Study. We carried out ablation experiments to verify the effectiveness of the proposed FPE, FAE, and MSA. For our baseline, we use the simple encoder-decoder structure with skip connections for feature fusion and perform element-wise summation at the decoder. Table "
Revisiting Feature Propagation and Aggregation in Polyp Segmentation,4.0,Conclusion,"We introduce a new approach to polyp segmentation that addresses inefficient feature propagation in existing UNet-like encoder-decoder networks. Specifically, a feature propagation enhancement module is introduced to propagate multiscale information over full stages in the encoder, while a feature aggregation enhancement module is attended at the decoder side to prevent the loss of high-level semantics during progressive feature fusion. Furthermore, a multiscale aggregation module is used to aggregate multi-scale features to provide rich information for the decoder. Experimental results on five popular polyp datasets demonstrate the effectiveness and superiority of our proposed method. Specifically, it outperforms the previous cutting-edge approach by a large margin (3%) on CVC-ColonDB and ETIS datasets. To extend our work, our future direction focuses on exploring more effective approaches to feature utilization, such that efficient feature integration and propagation can be achieved even on lightweight networks."
Identification of Disease-Sensitive Brain Imaging Phenotypes and Genetic Factors Using GWAS Summary Statistics,1.0,Introduction,"Nowadays, brain imaging genetics has gained increasing attention in the neuroscience area. This interdisciplinary field refers to integrates genetic variations (single nucleotide polymorphisms, SNPs) and structural or functional neuroimaging quantitative traits (QTs). Different imaging technologies can capture different knowledge of the brain and thus are a better choice in brain imaging genetics  To leverage the multi-modal brain imaging QTs and identify the joint effect of multiple SNPs, many learning methods were proposed for multi-modal brain imaging genetics  Since GWAS studies usually release their summary statistics results for academic use, we here developed a novel DMTSCCA method using GWAS summary statistics rather than individual data. The method, named S-DMTSCCA, has the same ability as DMTSCCA in modeling the association between multimodal imaging QTs and SNPs and does not require raw imaging and genetic data. We investigated the performance of S-DMTSCCA based on two kinds of experiments. Firstly, we applied S-DMTSCCA to GWAS summary statistics from Alzheimer's Disease Neuroimaging Initiative (ADNI) and compared it to DMTSCCA which directly ran on the original imaging genetic data of ADNI. Results suggested that S-DMTSCCA and DMTSCCA obtained equivalent results. Secondly, we applied S-DMTSCCA to a GWAS summary statistics from the UK Biobank. The experiment results showed that S-DMTSCCA can identify meaningful genetic markers for brain imaging QTs. More importantly, the structure information of SNPs was also captured which was usually missed by GWAS. It is worth noting that all these results were obtained without assessing the original neuroimaging genetic data. This demonstrates that our method is a powerful tool and provides a novel method for brain imaging genetics."
Identification of Disease-Sensitive Brain Imaging Phenotypes and Genetic Factors Using GWAS Summary Statistics,2.0,Method,"In this article, we represent scalars with italicized letters, column vectors with boldface lowercase letters, and matrices with boldface capitals. For X = (x ij ), the ith row is denoted as x i , jth column as x j , and the ith matrix as X i . X 2 denotes the Euclidean norm, X 2,1 denotes the sum of the Euclidean norms of the rows of X. Suppose X ∈ R n×p load the genetic data with n subjects and p biomarkers, and Y c ∈ R n×q (c = 1, ..., C) load the cth modality of phenotype data, where q and C is the number of imaging QTs and imaging modalities (tasks) respectively."
Identification of Disease-Sensitive Brain Imaging Phenotypes and Genetic Factors Using GWAS Summary Statistics,2.1,DMTSCCA,"DMTSCCA identifies the genotype-phenotype associations between SNPs and multi-modal imaging QTs using the following model  ( In this model, ) is a weight vector to balance among multiple sub-tasks. In this paper, κ ensures an equal optimization for each imaging modality. v c is the cth vector in V, where V = [v 1 , ..., v C ] ∈ R q×C denotes the canonical weight for phenotypic data. S ∈ R p×C and W ∈ R p×C are the canonical weights for genotypes, where S is the task-consistent component being shared by all tasks and W is the task-dependent component being associated with a single task. λ v , λ s and λ w are nonnegative tuning parameters."
Identification of Disease-Sensitive Brain Imaging Phenotypes and Genetic Factors Using GWAS Summary Statistics,2.2,Summary-DMTSCCA (S-DMTSCCA),"Now we propose S-DMTSCCA only using summary statistics from GWAS. It does not need individual-level imaging and genetic data. For ease of presentation, we derive our method by first introducing GWAS. GWAS uses linear regression to study the effect of a single SNP on a single imaging QT. Let x d denotes the genotype data with p SNPs and y l denotes the phenotype data with q imaging QTs, a typical GWAS model can be defined as where b dl is the effect size of the d-th SNP on the l-th imaging QT. α is the y-intercept, and is the error term which is independent of x d . When the SNPs and imaging QTs were normalized to have zero mean and unit variance, b dl will equal to the covariance between x d and y l , i.e., b dl = x T d y l n-1 . On this account, we can construct B ∈ R p×q by loading p×q summary statistics of GWAS. Obviously, B will be the covariance between multiple SNPs and multiple imaging QTs since its element is the covariance of a single SNP and a single imaging QT. Let B c denotes covariance of the c-th modality from GWAS, we have Further, we use ΣXX denote an estimated covariance of genetic data, i.e., ΣXX = X T X n-1 . X can be obtained from n subjects of the same or similar population since we do not have the original data. According the phenotype correlation  Let s * c , w * c and v * c denote the final results, we will present how to solve them only using GWAS results (B) and several subjects of a public reference database. Solving S and W: Since our method is bi-convex, we can solve one variable by fixing the remaining variables as constants. The model of S-DMTSCCA and DMTSCCA are the same  In both equations, D and D are diagonal matrices, and their i-th diagonal element are 1 2 s i 2 and .  (5-6), substituting Eq. (3) and Eq. (  Q is a diagonal matrix with the j-th element being 1 2(n-1)|vjc| (j = 1, ..., q). Now we have obtained all the solutions to DMTSCCA without using original imaging and genetic data. In contrast, we use the GWAS summary statistics to obtain the covariance between imaging QTs and SNPs. The in-set covariance Σ Y Y can also be calculated based on the results of GWAS. The in-set covariance Σ XX can be approximated using subjects of the same population. In this paper, we used the public 1000 genome project (1kGP) database to generate ΣXX . In practice, using ΣXX of the same population could yield acceptable results "
Identification of Disease-Sensitive Brain Imaging Phenotypes and Genetic Factors Using GWAS Summary Statistics,3.0,Experiment and Results,"We conducted two kinds of experiments to evaluate S-DMTSCCA. First, we used the ADNI data set where the original brain imaging phenotypes and genotypes are available. Specifically, our method cannot access individual-level imaging and genetic data. Instead, S-DMTSCCA can only work on the GWAS summary statistics obtained from this ADNI data set. At the same time, DMTSCCA directly ran on the original imaging and genetic data. By comparison, we can observe the performance difference between S-DMTSCCA and DMTSCCA. This can help evaluate the usefulness of our method. Second, we ran our method on a public GWAS result which studied the associations of imaging phenotypes and SNPs from the UK Biobank. To find the best parameters for λ s , λ w and λ v in DMTSCCA, we employed the grid search strategy with a moderate candidate parameter range 10 i (i = -5, -4, ..., 0, ..., 4, 5). Since S-DMTSCCA takes summary statistics as the input data without using the individual data, the conventional regularization parameters procedure is impracticable. Therefore, we used the grid search method with the same range based on the data set whose individual-level data was accessible to find the optimal parameters for S-DMTSCCA. Besides, to ensure equal optimization for each imaging modality, we used the same constants for the task weight parameters κ c in different sub-tasks. We used the 1kGP data sets as the reference samples. By conducting wholegenome sequencing on individuals from a range of ethnicity, the 1kGP institute obtains an extensive collection of human prevalent genetic variants "
Identification of Disease-Sensitive Brain Imaging Phenotypes and Genetic Factors Using GWAS Summary Statistics,3.1,Study on the ADNI Dataset,"Data Source. The individual-level brain genotype and imaging data we used were downloaded from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). One goal of ADNI is to investigate the feasibility of utilizing a multi-modal approach that combines serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment to measure the progression of Alzheimer's disease (AD). For the latest information, see https://www.adniinfo.org. We used three modalities, i.e. the 18-F florbetapir PET (AV45) scans, fluorodeoxyglucose positron emission tomography (FDG) scans, and structural MRI (sMRI) scans. These data had been aligned to the same visit of each subject. The sMRI data were analyzed with voxel-based morphometry (VBM) by SPM. All scans were aligned to a T1-weighted template image, segmented into gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) maps, normalized to the standard MNI space, and smoothed with an 8 mm FWHM kernel. Additionally, the FDG and AV45 scans were registered into the same MNI space. Then 116 regions of interest (ROIs) level measurements were extracted based on the MarsBaR automated anatomical labeling (AAL) atlas. These 116 imaging QTs were pre-adjusted to remove the effects of the baseline age, gender, education, and handedness by the regression weights generated from healthy controls. The genotype data were also from the ADNI database. Specifically, we studied 5000 SNPs of chromosome 19: 46670909 -46167652 including the well-known AD risk genes such as APOE  Biomarkers Identification. We presented the identified SNPs and imaging QTs for each imaging modality based on the estimated canonical weights in Fig.  Biomarkers Identification. Figure  Bi-multivariate Associations. In addition to feature selection, we calculated CCCs of S-DMTSCCA. The values for the three modalities were 0.1455, 0.1354, and 0.1474 respectively. This indicated that S-DMTSCCA could identify substantial bi-multivariate associations for each modality which might be attributed to its good modeling capability. These results again suggested that S-DMTSCCA can work well with summary statistics."
Identification of Disease-Sensitive Brain Imaging Phenotypes and Genetic Factors Using GWAS Summary Statistics,4.0,Conclusion,"In brain imaging genetics, DMTSCCA can identify the genetic basis of multimodal phenotypes. However, DMTSCCA depends on individual-level genetic and imaging data and thus was infeasible when the raw data cannot be obtained. In this paper, we developed a source-free S-DMTSCCA method using GWAS summary statistics rather than the original imaging and genetic data. Our method had the same modeling ability as the conventional methods. When applied to multi-modal phenotypes from ADNI, S-DMTSCCA showed an agreement in feature selection and canonical correlation coefficient results with DMTSCCA. When applied to multiple modalities of GWAS summary statistics, our method can identify important SNPs and their related imaging QTs simultaneously. In the future, it is essential to consider the pathway and brain network information in our method to identify higher-level biomarkers of biological significance."
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,1.0,Introduction,"Myopia, resulting in blurred distance vision, is one of the most common eye diseases, with a rising prevalence around the world, particularly among schoolchildren and young adults  As a crucial tool in the study of high myopia, fundus images demonstrate the retinal changes affected by myopia. Myopic maculopathy in color fundus photographs (CFP) can be important evidence in the evaluation of high myopia and myopic fundus diseases  There exist challenges to developing an automatic model that meets certain clinical conditions. For the inclusion criteria for high myopia, different studies will expect different outputs under different thresholds. High myopia is defined by a spherical equivalent (SE) ≤ -6.0 dioptres (D) or an axial length (AL) ≥ 26.0mm in most cases, but some researchers set the threshold of SE to -5.0D  The contributions of our work are summarized as: "
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,2.0,Method,"In this section, we propose a novel framework for high myopia screening in OCT called adjustable robust transformer (ARTran). This model can obtain the corresponding decisions based on the given threshold of inclusion criteria for high myopia and is trained end-to-end for all thresholds at once. During the testing phase, the screening results can be predicted interactively for a given condition."
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,2.1,Modified Vision Transformer for Screening,"Transformers have shown promising performance in visual tasks attributed to long-range dependencies. Inspired by this, we use ViT  Patients with high myopia are usually accompanied by directional structural changes, such as increased retinal curvature and choroidal thinning. On the other hand, due to the direction of light incidence perpendicular to the macula, OCT images have unbalanced information in the horizontal and vertical directions. Therefore, we propose a novel non-square strategy called anisotropic patch embedding (APE) to replace vanilla patch embedding for perceiving finer structural information, where an example is shown in Fig.  Considering that different researchers may have different requirements for inclusion criteria or risk control, we propose adjustable class embedding (ACE) to adapt to variable conditions. Take the -6.0D as the benchmark of inclusion criteria, we define the relationship of the biased label, SE, and the adjustment coefficient Δ: where 1 indicates the positive label and 0 indicates the negative label. Our ACE introduces two parameterized vectors v 1 and v 2 , and constructs a linear combination with the given δ to obtain v AP E (δ) to replace the fixed class token: where v AP E (0) is the equal combination and others are biased combinations. Several studies have demonstrated impressive performance in multi-label tasks using transformers with multiple class tokens "
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,2.2,Shifted Subspace Transition Matrix,"To enhance the robustness of the model, we follow the common assumptions of some impressive label noise learning methods  where the transition matrix guarantees statistical consistency. Li et al.  where T i,i (δ) > 0.5 is the i th diagonal element of SST, and the sum of each column of the matrix is 1. Thus any class-posterior of x is inside the simplex form columns of T (δ)  where the S(•) is the Sigmoid function, θ i is the parameter used only for column, and θ 0 is the parameter shared by both columns. Adjusting the δ is equivalent to shifting the geometric space of the simplex, where the spaces with a closer adjustment coefficient share a more common area. This ensures that the distribution of the noise class-posterior has a strong consistency with the adjustment coefficient. Furthermore, the range distribution of any T (δ) is one subspace of an extended transition matrix T Σ , whose edges is defined as the edges of T (-1) and T (1): To train the proposed ARTran, we organize a group of loss functions to jointly optimize the proposed modules. The benchmark noise posteriors and the adjusted noise posteriors are optimized with classification loss with benchmark and adjusted labels respectively. Following the instance-independent scheme, we optimize the SST by minimizing the volume of the extended SST  where L cls (•) indicates the cross entropy function, and L vol (•) indicates the volume minimization function."
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,3.1,Dataset,"We conduct experiments on an OCT dataset including 509 volumes of 290 subjects from the same OCT system (RTVue-XR, Optovue, CA) with high diversity in SE range and retinal shape. Each OCT volume has a size of 400 (frames) × 400 (width) × 640 (height) corresponding to a 6mm × 6mm × 2mm volume centered at the retinal macular region. The exclusion criteria were as follows: eyes with the opacity of refractive media that interfered with the retinal image quality, and eyes that have undergone myopia correction surgery. Our dataset contains 234 low (or non) myopia volumes, and 275 high myopia volumes, where labels are determined according to a threshold spherical equivalent -6.0D. We divide the dataset evenly into 5 folds for cross-validation according to the principle of subject independence for all experiments. For data selection, we select the center 100 frames of each volume for training and testing, so a total of 50,900 images were added to the experiment. And the final decision outcome of one model for each volume is determined by the classification results of the majority of frames. For data augmentation, due to the special appearance and location characteristics of high myopia in OCT, we only adopt random horizontal flipping and random vertical translation with a range of [0, 0.1]. "
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,3.2,Comparison Experiments and Ablations,"To evaluate the proposed ARTran in predicting under a benchmark inclusion criteria (-6.0D), we compare it with two convolution-based baselines: ResNet-50  We further perform ablations in order to better understand the effectiveness of the proposed modules. Table "
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,4.0,Conclusion,"In this work, we proposed ARTran to screen high myopia using OCT images. Experimental results demonstrated that our approach outperforms baseline classification methods and other screening methods. The ablation results also demonstrated that our modules helps the network to capture the features associated with high myopia and to mitigate the noise of labels. We organized the evaluation of the adjustable and interpretable ability. Experimental results showed that our method exhibits robustness under variable inclusion criteria of high myopia. We evaluated uncertainty and found that confusing samples had higher uncertainty scores, which could increase the interpretability of the screening task."
Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography,3.3,Adjustable Evaluation and Uncertainty Evaluation,"To evaluate the effectiveness of the adjustment module, we change the adjustment coefficient several times during the testing phase to obtain screening results at different thresholds. Figure "
Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains,1.0,Introduction,"Diabetic Retinopathy (DR) is a leading cause of blindness, affecting millions of people worldwide, and early severity grading is vital for disease management  Recently, several studies have explored the DG problem and reported significant performance drops in the retinal vessel segmentation  In contrast to previous works, we argue that three factors contribute to poor generalization in DGDR: visual and degradation style shifts, diagnostic pattern diversity, and data imbalance. Specifically, as shown in Fig.  In this paper, we propose a novel framework, Generalizable Diabetic Retinopathy Grading Network (GDRNet) to address the DGDR problem. Our framework consists of three critical components: fundus visual-artifact augmentation (FundusAug), dynamic hybrid-supervised loss (DahLoss), and domainclass-aware re-balancing (DCR). By simulating visual transformations and image degradations, FundusAug enables the model to learn robust features that are less sensitive to style shifts caused by factors such as lighting conditions or artifacts and noise. DahLoss employs a hybrid-supervised learning paradigm to handle diagnostic pattern diversity and dynamically balances the influence of supervised and unsupervised learning. Jointly functioning with FundusAug, it enables the model to preserve pixel-level diagnostic information and learn generalizable features with sufficient intra-class variations. Furthermore, DCR assigns soft-balanced weights to each domain-class pair to prevent underrepresentation caused by data imbalance while avoiding undesired over-emphasis introduced by hard weighting. Finally, to evaluate generalization ability, we design a publicly available benchmark named Generalizable Diabetic Retinopathy Grading Benchmark (GDRBench), comprising eight popular datasets and two evaluation settings."
Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains,2.0,Methodology,"An overview of GDRNet is shown in Fig.  Fundus Visual-Artifact Augmentation. The external machine and internal retinal illumination conditions can cause differences in visual attributes  where π n pn,mn denotes transformation n with probability p n and random intensity m n . To reduce the parameter space of FundusAug while still ensuring image diversity, we implemented FundusAug by applying each operation with a parameter-free procedure that uniformly selects a process with a probability of 0.5. FundusAug can generate realistic augmented views while preserving their diagnostic semantics, as well as providing a robust foundation for subsequent generalizable feature learning by increasing image diversity. A detailed description and visualization of operations can be found in the appendix. Dynamic Hybrid-Supervised Loss. While the supervised loss (SupLoss), e.g., the cross-entropy loss (CE), effectively guides the model to learn effective feature representations  where α decreasing within range [0, 1] is a hyper-parameter to dynamically control the task focus, and L sup and L scon could be any supervised and selfsupervised contrastive loss functions. In this paper, we adopt CE and instance discrimination loss  where p • t denotes the predicted probability of the true class under one-hot encoding label, f • denotes the l 2 normalized feature, the • symbol denotes the inner product, τ is the temperature parameter and A(i) ≡ (J ∪ I)/i. As illustrated in Fig.  Domain-Class-Aware Re-balancing. The domain-class data imbalance can result in certain categories and diagnostic patterns in specific datasets being underrepresented, leading to biased and inaccurate model predictions  where D denotes the set of domains, N is the amount of classes, and β with a range of [0, 1] is a hyperparameter that adjusts the balancing intensity. When β approaches to 0, DCR assigns weight more equally, and when β closes to 1, it acts more like the naive hard balancing method. By introducing β, DCR enables more nuanced weighting of samples based on their domain and class, reducing the risk of over-emphasizing underrepresented samples in the loss function. By considering the occurrence probabilities of all categories across all domains, DCR mitigates the data imbalance problem of underrepresented class-domain pairs. These two make DCR an effective solution for handling domain-class data imbalance and improving the generalizability of DR grading models."
Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains,3.0,Experiments,"Experimental Settings, Implementation Details and Evaluation Metrics. To comprehensively analyze and evaluate our framework, we designed the GDRBench involving two generalization ability evaluation settings and eight popular public datasets. First, GDRBench preserves the classic leave-onedomain-out protocol (DG test), which requires leaving one domain for evaluation and training models on the rest. It involves six datasets, including DeepDR  Comparison with Other Methods. We conducted a comprehensive experiment to evaluate our framework, comparing it with a vanilla baseline (ERM) and other state-of-the-art methods from various categories, including ophthalmic disease diagnosis (OSD)  Ablation Study of Proposed Components. To evaluate the effectiveness of our proposed components, we conducted an extensive ablation study under the DG test and presented the AUC score achieved by different models in Table  We first examined the individual effects of FundusAug with only visual transformation (VT), FundusAug with only image degradation (ID), DCR and DahLoss  "
Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains,4.0,Conclusion,"In this paper, we tackled the three-fold generalization issues that hinder the generalizability of DR grading, including style shifts, diagnostic pattern diversity, and data imbalance. To overcome these challenges, we proposed a novel and unified framework called GDRNet, incorporating three effective components: FundusAug, DahLoss, and DCR. FundusAug enables the generation of realistic augmented views, while DahLoss leverages supervised and unsupervised learning to preserve diagnostic patterns and increase intra-class variations of features. Finally, DCR softly handles the data imbalance across categories and domains to avoid potential performance decay. Together, these three components work synergistically to improve the generalization performance of the model. GDR-Net achieved superior performance on both DG and ESDG tests of the proposed publicly available GDRBench, demonstrating its effectiveness and robustness in addressing the three-fold generalization issues in DR grading. Overall, our work provides valuable insights and practical solutions for improving the generalization capability of deep learning in medical image analysis, and has the potential to benefit real-world clinical applications."
Towards Generalizable Diabetic Retinopathy Grading in Unseen Domains,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_42.
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,1.0,Introduction,"Chest radiography is currently the most common imaging examination, playing a crucial role in epidemiological studies  Generating chest radiographs based on radiology reports can be thought of as transforming textual input into visual output, while current methods typically rely on text-to-image generation in computer vision. The fidelity and diversity of synthesized images are two major qualities of generative models  The first work to explore generating chest X-rays conditioned on clinical text prompts is XrayGAN  In this paper, we propose a new report-to-X-ray generation method called DivXGAN to address the above issues. As illustrated in Fig. "
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,2.0,Method,"Let X and Z denote the image space and the low-dimensional latent space, respectively. Given a training set {x i , r i } N i=1 of N X-ray images, each of which x i is associated with a radiology report r i . The task of report-to-X-ray generation aims to synthesize multiple high-fidelity chest radiographs from the corresponding report r i and latent noises {z j ∈ Z} j=1,2,..., . The generative models are expected to produce X-rays with high fidelity and diversity, so as to be used for data augmentation of downstream applications."
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,2.1,Fidelity of Generated X-Rays,"One-Stage Generation. Existing generative method  Distill and Incorporate Report Knowledge. Semantic information and medical concepts in radiology reports should be fully interpreted and incorporated into visual features to reduce the distance between the generated data distribution and the real data distribution, thereby improving fidelity. We design a medical domain-specific text encoder with hierarchical structure to extract the embeddings of the free-text reports. At the word level, each sentence is represented as a sequence of T word tokens, plus a special token [SEN T ]. We embed each word token w t with an embedding matric W e , i.e., e t = W e w t . Unlike previous work  At the sentence level, a report consists of a sequence of S sentences, each of which is represented as e (i) [SENT ] using the word-level encoder described above. We also utilize a Transformer to learn the contextual importance of each sentence and encode them into a special token embedding e [REP O] , which serves as the holistic representation of the report: [SENT ] , e "
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,2.2,Diversity of Generated X-Rays,"A radiology report is a medical interpretation of the corresponding chest radiograph, describing the clinical information included and assessing the patient's physical condition. Reports that describe chest radiographs of different patients with similar physical conditions are often consistent. Ideally, multiple X-ray images with the same health conditions could be generated from a single report, only with some differences in irrelevant factors such as body size, etc. To this end, we omit the pixel-wise reconstruction loss and introduce noise vectors z in the latent space Z as one of the inputs to our one-stage generator, thereby providing the model with the necessary variability to ensure the diversity of synthesized X-rays. In this case, the generator G maps the low-dimensional latent space Z into a specific X-ray image space X r , conditioned on the report vector e i [REP O] : where x(j) i denotes the j-th synthesized X-ray from the i-th report r i . The noise vector z j ∈ Z follows a standard multivariate normal distribution N (0, I). In this way, given a radiology report, noise vectors can be sampled to generate various chest X-rays matching the medical description in the report."
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,2.3,Learning Objectives and Training Process,"Since DivXGAN uses a one-stage generator to directly generate high-fidelity chest radiographs, only one level of generator and discriminator needs to be alternately trained. The discriminator D outputs a scalar representing the probability that the input X-ray came from the real dataset and is faithful to the input report. There are three kinds of inputs that the discriminator can observe: real X-ray with matching report, synthesized X-ray with matching report, and real X-ray with mismatched report. The discriminator D x, e [REP O] ; θ d is trained to maximize the probability of assigning the report vector e [REP O] to the corresponding real X-ray x i , while minimizing the probability of the other two kinds of inputs. Due to multiple down-sampling blocks and residual connections, we employ the hinge loss  where p data , p g and p mis denote the data distribution, implicit generative distribution (represented by G) and mismatched data distribution, respectively. The generator G z, e [REP O] ; θ g builds a mapping from the latent noise distribution to the X-ray image distribution based on the correlated reports, fooling the discriminator to obtain high scores: It is worth noting that the parameters θ t of the text encoder in Eqs. ( "
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,3.1,Datasets and Experimental Settings,"We use two public datasets, namely Open-i  Our network is trained from scratch using the Adam "
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,3.2,Results and Analysis,"We compare our approach with several state-of-the-art methods based on generative adversarial networks, including text-to-image generation: StackGAN  Visualization of chest X-rays synthesized from a report using different methods is shown in Fig. "
Diversity-Preserving Chest Radiographs Generation from Reports in One Stage,4.0,Conclusion,"In this paper, we have devised a diversity-preserving method for high-fidelity chest radiographs generation from the radiology report. Different from state-ofthe-art alternatives, we propose to directly synthesize high-fidelity X-rays using a single pair of generator and discriminator. A domain-specific text encoder and latent noise vectors are introduced to distill medical concepts and incorporate necessary variability into the generation process, thus generating X-rays with high fidelity and diversity. We show the capability of our generative model in data augmentation for supervised downstream applications. Investigation of capturing high-frequency information of X-rays in generative models can be an interesting and challenging direction of future work."
Improved Flexibility and Interpretability of Large Vessel Stroke Prognostication Using Image Synthesis and Multi-task Learning,1.0,Introduction,"Ischemic stroke caused by large vessel occlusion (LVO) is one of the leading causes of death and disability worldwide  There have been a number of models proposed for the prognostication of LVO in recent years  CT Perfusion (CTP) maps are increasingly utilized for the prediction of LVO outcomes because they offer quantitative information on blood flow and volume in the brain, as well as the arrival time of blood bolus to brain tissues  In recent years, techniques of image synthesis have shown promising potential in medicine "
Improved Flexibility and Interpretability of Large Vessel Stroke Prognostication Using Image Synthesis and Multi-task Learning,2.1,Dataset and Pre-processing,"Data utilized in this research was collected from the Royal Adelaide Hospital, which provides the sole EVT service to all stroke patients in South Australia and Northern Territory. There were 460 LVO patients included in the study, admitted between 01 Dec 2016 and 01 Dec 2021, and treated with EVT with full image modalities (NCCT, CTA and CTP maps). Of these, 256 achieved functional independency (mRS ≤ 2) 3 months post-stroke. The non-imaging data (i.e., age, stroke severity, blood glucose, pre-admission functional status, use of intravenous thrombolysis, onset-to-groin puncture time, stroke hemisphere, and 3-month mRS score) was collected by experienced neurologists and nurses adhering to the standard admission procedure. The study was approved by The Central Adelaide Local Health Network Human Research Ethics Committee. The NCCT and CTA images were skull-stripped with the attenuation clipped between 0 and 100 Hounsfield Units (HU) for the NCCT images and 0 and 750 HU for the CTA images. Multimodal CT imaging data, including NCCT, CTA and CTP maps, were acquired using Canon Aquilion ONE scanners. The NCCT and CTA acquisitions have isotropic voxel sizes ranging from 0.4-0.6 mm and 0.4-0.7 mm, respectively. The acquisition voxel size of the CTP maps is 0.4×0.4×4.9 mm 3 . Four CTP maps, including cerebral blood volume (CBV), cerebral blood flow (CBF), mean transit time (MTT), and relative arrival time of contrast (Delay), were selected for their clinical utility, based on consultations with two senior neurologists. To rule out the impact of different brain sizes, affine registration to a CT template was performed for each modality "
Improved Flexibility and Interpretability of Large Vessel Stroke Prognostication Using Image Synthesis and Multi-task Learning,2.2,Models,"Problem Statement. We take i=1 to be a set of data for N patients, where X i NCCT , X i CTA , and X i Cli var are the NCCT, CTA and clinical non-imaging data (i.e., age, stroke severity, blood glucose, pre-admission functional status, use of intravenous thrombolysis, and onset-togroin puncture time) for the i th patient. Four CTP maps for the i th patient are denoted by the set X i CTP which is defined as Delay . The dichotomized prognostic outcome for the i th patient is denoted by We aim to: (i) evaluate the performance of CTP maps in predicting the dichotomized mRS score; and (ii) synthesize the CTP maps using two commonly used image modalities (NCCT and CTA) at admission for prognostic prediction. For the first aim, the model can be written as: where ŷ i acq is the predicted outcome and F acq is the mapping function from the acquired CTP maps and clinical information to the dichotomized mRS score. For the second aim, there are two tasks, including (i) learning a mapping function G for CTP map generation, and (ii) learning a function F syn to map synthetic CTP maps and clinical information to the dichotomized mRS score. That is: where Xi CBF , Xi CBV , Xi MTT , Xi Delay are the predicted CBF, CBV, MTT, and Delay maps and ŷ i syn is the predicted outcome from synthetic CTP maps for the i th patient. To fulfil the second aim, we propose a two-stage deep learning framework, including a clinical-guided synthesis and a multimodal prognostic prediction. The network architecture and loss function are detailed below. Stage 1: Clinical-Guided Synthesis. The method for synthesizing CTP maps utilizes a 3D generative adversarial network (GAN) model, which is illustrated in Fig.  where Loss mse and Loss bce calculate the mean square error and the binary cross entropy, respectively. D is a mapping function for discriminative and clinical tasks. y i hemi is the label of the clinical task {0 : occlusion in the left hemishpere; 1 : occlusion in the right hemishpere}. We used the total loss for the set of synthetic CTP maps for backpropagation. Step 1 : Step 2 : where F img is a mapping function of Xi CTP to a binary mRS score and F logistic is a mapping function of step 1 outputs and X i Cli var to a binary mRS score. Xi Delay , concatenated as channels."
Improved Flexibility and Interpretability of Large Vessel Stroke Prognostication Using Image Synthesis and Multi-task Learning,3.0,Experiments and Results,"We performed two sets of experiments in the current study. In the first set of experiments, we compared prognostic prediction performance between models using different modalities, including (i) NCCT and CTA, (ii) CTP maps, (iii) NCCT, CTA and CTP maps, (iv) non-imaging data, (v) NCCT, CTA and nonimaging data, (vi) CTP maps and non-imaging data, and (vii) NCCT, CTA, CTP maps and non-imaging data. Images were input into the models with the architecture described in Sect. 2.2 stage 2, where inputs were replaced with corresponding imaging modalities concatenated at the channel level. In the second set of experiments, we evaluated the quality of the synthetic images and the performance when using them for prognostic prediction. We initially compared our model to four synthesis models: UNET, WGAN, CycleGAN and L2GAN. The L2GAN has the same architecture as our model but is not assigned the additional clinical task in the discriminator. To evaluate the quality of the synthetic images, we compared the structural similarity index measure (SSIM) and peak signalto-noise ratio (PSNR) between synthesis models. Area under the ROC curve (AUC), accuracy (ACC), and F1-Score were used to assess the performance of prognostic prediction. We also compared our model to three state-of-the-art models that used raw images and clinical non-imaging data "
Improved Flexibility and Interpretability of Large Vessel Stroke Prognostication Using Image Synthesis and Multi-task Learning,3.1,Results of Image Synthesis and Prognostic Prediction,Data Modalities for Prognostic Prediction. The performance of models using different combinations of data modalities is shown in Figs. 
Improved Flexibility and Interpretability of Large Vessel Stroke Prognostication Using Image Synthesis and Multi-task Learning,4.0,Conclusion,"This study demonstrates that CTP maps, which are known to provide critical information for clinicians, also benefit prognostic prediction using deep learning techniques. When CTP maps are not available at hospital admission, their benefits can still be largely retained through image synthesis. Using multi-task learning with a simple clinical task, our model outperformed other synthesis methods in both image quality and the performance of prognostic prediction. Our synthetic CTP maps show key clinical features that are able to be readily discerned upon visual inspection. These findings verify the advantages of including additional CTP maps in LVO prognostication and establish the ability to effectively synthesize such maps to retain their benefits. While we acknowledge that our network architectures are not novel, we highlight the novelty of our architectures for stroke prognostication. The proposed framework can provide significant utility in the future to aid in the selection of patients for high-stakes time-critical EVT, particularly for those who have limited access to advanced imaging. Furthermore, by demonstrating the key clinical imaging features, our framework may improve confidence in building a clinically trusted model."
Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation,1.0,Introduction,"Inner ear malformations are found in 20-30% of children with congenital hearing loss  Currently, supervised deep metric learning garners significant interest due to its exceptional efficacy in data clustering and pathology classification. Most of these approaches are fully supervised and use supervisory signals that model the training by creating tuples of labeled training data. These tuples are then used to optimize the intra-class distance of the different samples in the latent space, as has been done mostly for 2D images  Our objective is to develop a fully automated pipeline for the classification of inner ear malformations, utilizing a relatively large and unique dataset of such anomalies. The pipeline's design necessitates a profound comprehension of this data type and the congenital malformations themselves. Given the CT scans in this region are complex, and the images originate from diverse sources, we employ an unsupervised approach, uniquely based on the 3D shape of the cochlear structure. We have observed that the cochlear structure can be roughly but consistently segmented by a 3D-UNET model trained exclusively on normal cochlear anatomies. We then use these segmentations and adopt an entirely unsupervised approach, meaning the deep learning model is trained from scratch on these segmentations, and the class labels are not used for training. To map these shapes to an optimal latent space representation, we utilize DeepDiffusion, which combines the diffusion distance on a feature manifold with the feature learning of the encoder. In this paper, we present the first automatic approach for the classification of congenital inner ear malformations. We use an unsupervised method to find the latent space representation of cochlear shapes, which allows for their further classification. We demonstrate that shapes from a segmentation model trained on normative cases, albeit imperfect, can be used to represent abnormalities. Moreover, our results indicate the potential for successfully applying this approach to other anatomies."
Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation,2.0,Data,"Our dataset comprises a total of 485 clinical CT scans, consisting of 364 normal scans and 121 scans with various types of inner ear malformations. The distribution of inner ear scans for each type of malformation is shown in Fig. "
Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation,3.1,Anatomical Representation,An overview of our pipeline is presented in Fig. 
Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation,3.2,Deep Diffusion Algorithm,"The DeepDiffusion (DD) algorithm  Where θ characterizes the encoder and M ∈ R N X P represents the latent feature manifold formed by the training samples, where N is the number of data samples and P is the output dimensions of the encoder. The extrinsic feature f is defined as the output of the encoder and has dimension P . M is initialized by stacking together the embeddings of the first forward pass through the encoder which has been randomly initialized as this has been shown to perform better than randomly initializing the weights of M itself as shown in  Every training sample has its unique identification number (ID b ) which is used to specify a diffusion source y b that is consistent throughout the training procedure. L fit constrains the ranking vector r b to being close to the diffusion source y b , which is defined as the vector containing one-hot encoding of ID b . The ranking vector is defined as r b = softmax(f b M T ) and represents the probabilistic similarities between the feature f b and all the intrinsic features contained in M . The fitting term is therefore defined as its minimization results in all the extrinsic features being embedded farther away from each other as they are being pulled toward their respective and unique diffusion source vectors. The smoothing term is defined as where the dissimilarity operator is the Jensen-Shannon divergence  Minimizing L smooth pulls extrinsic features and their neighboring intrinsic features together which implies that an extrinsic feature is more likely to be projected onto the surface of the latent feature manifold of the intrinsic features when L smooth is smaller."
Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation,3.3,Implementation,"For our encoder, we use the PointNet "
Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation,4.0,Results,"We evaluate the classification performance of our pipeline by analyzing the embeddings generated by the trained encoder. To visualize the projection of the features of the test in 2D we use the U-MAP  We have also included, in Fig. "
Unsupervised Classification of Congenital Inner Ear Malformations Using DeepDiffusion for Latent Space Representation,5.0,Conclusion,"We have presented the first approach for the automatic classification of congenital inner ear malformations. We show how using the 3D shape information of the cochlea obtained with a model only trained in normative anatomies is enough to classify the malformations and reduces the influence of the image's source, which is crucial in a clinical application setting. Our method shows a mean average precision of 0.77 with a mean ROC-AUC of 0.91, indicating its effectiveness in classifying inner ear malformations. Furthermore, the representation of the different cases in the latent space shows spatial relation between classes, which is correlated with the anatomical appearance of the different malformations. These promising results pave the way towards assisting clinicians in the challenging assessment of congenital inner ear malformations potentially leading to improved patient outcome of cochlear surgery."
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",1.0,Introduction,"In the UK, approximately 11,500 patients are diagnosed with rectal cancer each year  Histology-based digital biomarkers enable the possibility to predict a patient's response to therapy. The consensus molecular subtypes (CMS) classification system derived from gene expressions  As opposed to predicting response to radiotherapy alone, we aim to analyse this prediction in the context of the overall tissue architecture and the tumour biology as captured by CMS. Input to our model is a standard H&E Whole Slide Image (WSI) which is split into smaller patches to overcome the memory limitations of existing GPUs. To achieve our goal we need to capture the heterogeneity at the slide level, which is why applying full or semi-supervised approaches on individual tiles followed by a slide aggregation method is not suitable. Instead, we build on recent graph neural network (GNN) approaches that allow us to model the entire WSI as a graph. As local cell communities form the nodes of such a graph it can effectively model the micro-anatomy of the tissue. At the same time it is possible to make predictions at the node-, graph-, and slide-level. Related Work. To predict the grading of colorectal cancer (CRC), both cellbased and patch-based graphs have been used in separate works  Our methodology proposes a novel and disease relevant approach to a more interpretable model that effectively supports a diagnostic task. Pathologists and oncologists can use this information to inspect the validity of the prediction result and interrogate key aspects of the spatial biology that is critical for patient management. Ultimately, this type of information that is not available today will help to characterise interactions between the tumour and the host tissue and therefore help to support choice of therapy. The developed framework combines self-supervised training of a Vision Transformer (ViT) to extract morphological features, a superpixel algorithm for determining nodes of a graph, and a GNN for predictions. We achieve 0.82 AUC predicting complete response to radiotherapy using deep learning on WSIs for CRC patients, whilst providing novel interpretability of the results."
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",2.0,Methods,"In this section we present the patch-level feature extraction, provide the detail of the superpixel segmentation of the WSI, and illustrate the resulting graph presentation. A GNN with three branches for our output predictions is used to simultaneously make the three different predictions as shown in Fig.  Pipeline. For computational reasons, all images are split into patches of size 256 × 256 pixels. In order to have a common feature set all the way up to the last layer of the GNN, individual patches should be represented by morphological features that are label-agnostic. This last layer of the GNN then splits into three branches to predict response to radiotherapy, the CMS4 subtype classification for CRC, and epithelial tissue regions. This way we can guarantee the common latent features and derivation across branches, maintaining the contextual importance of each branch. The DINO framework  To find the nodes of the WSI graphs, we apply the SLIC superpixel algorithm  The superpixels centers are used as the nodes of the graph, and the node features are the weighted mean of the corresponding patch features which overlap with the superpixel region. The edges of the graph are determined by nearest neighbours from Delaunay triangulation, as in SlideGraph  Building on the ideas introduced by SlideGraph  where BCE is the binary cross entropy loss, ŷRT ∈ R is the slide-level prediction of response to radiotherapy, ŷCMS4 ∈ R is the slide-level prediction of CMS4, ŷepi ∈ R ni are the node-level predictions of epithelial tissue and n i is the number of nodes in the i th WSI graph. For each prediction branch, we can visualize the individual node predictions from the WSI graph, overlaid on the WSI itself, to get an idea of how the node predictions vary across the different tissue regions. Each graph-level prediction is derived from the corresponding branch node predictions, by applying pooling and dropout. Data. We train and validate our methods on two retrospective rectal cancer datasets, Grampian and Aristotle. Both cohorts received standard chemoradiotherapy of pelvic irradiation (45-50.4Gy in 25 fractions over 5 weeks) with capecitabine 900mg/m 2 . The pre-treatment biopsy slides were all sectioned and stained in the same laboratory, and scanned at 20x magnification (0.5 µm 2 /pixel) on an Aperio scanner. Pathological complete response, which we use as a target outcome here, was derived from histopathological assessment from posttreatment resections. The CMS labels for this data are derived from three different transcriptomic versions (single cohort, combined cohort correcting batch effects and combined cohort including 2036 cases run with the same platform), in order to generate robust classifications. In all cases the CMS call was calculated using the CMSclassifier random forest and single sample predictor  The epithelial labels for each graph node are calculated from epithelial masks for each WSI. These epithelial segmentation masks were generated at 10x magnification (1 µm 2 /pixel) with a U-Net  For consistency the tumour regions were marked up by an expert pathologist. We use these masks in our analysis to filter out background and irrelevant tissue from the images. Grampian and Aristotle are used in both training and validation, with a 70/30% training-validation split, keeping any WSIs from a single patient in the same dataset. We predict complete response to radiotherapy against all other responses, such as partial response and no response. The datasets are unbalanced, since in Grampian only 61/244 slides have complete response, and in Aristotle only 24/121 slides have complete response. They are even more unbalanced for CMS4, since only 28/244 slides in Grampian and 17/121 slides in Aristotle are labelled with CMS4. We address this imbalance in the Supplementary Materials. There are 365 slides total in our dataset, from 249 patients."
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",3.0,Experiments,"Implementation. We use the default DINO parameters, but train for 20 epochs with 5 warmup epochs. We apply the SLIC algorithm  Table "
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",,Response branch,"Response Results. Despite the noise in our reference data used for training, our model achieves good performance in terms of mean AUC scores on all three prediction branches of our model, predicting complete response to radiotherapy (RT) with 0.819 AUC, CMS4 with 0.819 AUC and epithelial tissue at the node level with 0.760 AUC across folds. Further metrics are provided in Table  The predicted response to radiotherapy can now be viewed in the context of disease biology as captured by CMS4. For example, the model demonstrates that CMS4 patients are less likely to respond to radiotherapy. In addition, it is now possible to view the spatial distribution of CMS4 active regions in the tissue architecture context as shown in Fig.  To explore the effects of the noisy CMS4 ground truth labels, we remove from our dataset any WSIs classified as 'Unmatched' for the CMS call, which for the main results of this paper we defined as 'Not CMS4'. Removing this data and rerunning our analysis improved our predictions for CMS4 by +0.06 AUC, and reduced our response to radiotherapy and epithelial predictions by -0.02 and -0.01 respectively. The results can be found in the Supplementary Materials. These small changes indicate that the noise in our data does not degrade the performance of our classifier, reinforcing it as a robust and accurate model."
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",4.0,Conclusion,"By setting the prediction of response to therapy in context with disease biology and spatial organisation of the tissue we are providing a novel approach for enhancing the interpretablity of complex prediction tasks. These results do not only enhance the interpretability, they also provide new ways to utilise large retrospective clinical trial cohorts for which no additional molecular data is available. Extending the amount of training data and improving model training will improve model performance, which is already impressive. We argue that this work also advances the state of the art in feature representation and analysis. Our prediction maps derive from the same graph model, and hence they share underlying graph features. The prediction branches only diverge at the final stage of translating these graph features into outcome predictions for our three clinically relevant outcomes. Importantly, this level of visualisation is not only accessible to pathologists, this joint prediction model also enhances the communication between pathologists and oncologists which is critical for patient management. By cross-referencing these prediction maps with our prior understanding of cancer biology, this approach can help to establish trust in the prediction model and also help to identify potential failure cases. This work relies on access to well annotated clinical trial samples which will limit our ability to include more data for training and testing. In future, we plan to use these methods to help better characterise tumour-stromal interactions of the tissue. We also plan to use a denser graph with less connectivity to be able to better predict the heterogeneous epithelial tissue. The Aristotle trial was funded by Cancer Research UK (CRUK/08/032). The funders played no role in the analyses performed or the results presented. Financial support: RW -EPSRC Center for Doctoral Training in Health Data Science (EP/S02428X/1), Oxford CRUK Cancer Centre; VHK -Promedica Foundation (F-87701-41-01) and Swiss National Science Foundation (P2SKP3_168322/1, P2SKP3_168322/2); TSM -S:CORT (see above); JR, KS -Oxford NIHR National Oxford Biomedical Research Centre and the PathLAKE consortium (InnovateUK). The computational aspects of this research were funded from the NIHR Oxford BRC with additional support from the Wellcome Trust Core Award Grant Number 203141/Z/16/Z. The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR or the Department of Health."
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_73.
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,1.0,Introduction,"Acute ischemic stroke (AIS) is a disease of ischemic necrosis or softening of localized brain tissue caused by cerebral blood circulation disturbance, ischemia, and hypoxia  There are some machine learning methods to determine the TSS of AIS by automatic discrimination  With the small samples and the high dimension of CTP, the convolution neural network (CNN) cannot effectively extract features, resulting in the problem of network non-convergence. Additionally, some existing TSS classification methods leverage multi-map mainly by simple linear connections, which do not thoroughly learn the supplementary information of CTP "
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,2.0,Methodology,The main framework of our proposed method is depicted in Fig. 
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,2.1,Dynamic Convolution Feature Extraction Network,"where π i (x) is the weight of the i-th convolution, which varies with each input x. . The attention block compresses the features of each channel through global average pooling and then uses two fully connected layers (with ReLU activation function between them) and a Softmax function to generate the attention weight of k convolution cores."
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,2.2,Multi-map Fusion Module,"For multi-map information fusion of low-order features, considering the small area of acute stroke focus, a multi-scale attention module is used to fuse multi-map features in the first three stages. Its structure is shown in Fig.  By setting different global average pooling (GAP) sizes, we can focus on the interactive feature information in channel dimensions at multiple scales, and aggregate local and global features. Through point-wise convolution (PWConv), point-wise channel interaction is used for each spatial location to realize the integration of local information. The local channel features are calculated as follows: The global channel features are calculated as follows: The final feature x i is calculated as follows: For the fusion of multi-map information of high-order features, considering the relevance of global information between different modes, the self-attention mechanism "
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,2.3,Multi-head Pooling Attention,"With the deepening of the network layers, the semantic information contained in the output features becomes higher and higher. After the post-fusion of the branch network, we use an MPA to learn the high-order semantic details further. Here, a smaller number of tokens is used to increase the dimension of each token to facilitate the storage of more information. Unlike the original multi-head attention (MHA) operator "
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,3.1,Experimental Configuration,"Dataset and Data Preprocessing. The dataset of 200 AIS patients in this paper is from a local hospital. The patients are divided into two categories: positive (TSS < 6 h) and negative (TSS ≥ 6 h). Finally, 133 in the positive subjects and 67 in the negative subjects are included. Each subject contains CBF, CBV, MTT, and Tmax. The size of all CTP images is set to 256 × 256 × 32. Experimental Setup. The network structure is based on PyTorch 1.9.0 framework and CUDA 11.2 Titan × 2. We use a five-fold cross-validation method to verify the effectiveness of our method. 80% of the data is used as a training set and 20% as a test set. During the training process, the Adam optimizer optimizes the parameters, and the learning rate is set to 0.00001. The learning strategy of fixed step attenuation is adopted, in which the step size is set to 15, γ is 0.8. The number of iterations of training is 50."
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,3.2,Experimental Results and Analysis,"Comparative Study. We evaluate the effectiveness of our method by comparing it with other approaches on the same dataset  Ablation Study. To assess the efficacy of each module in the proposed method, a series of ablation experiments are conducted by gradually incorporating the four main modules, namely Dconv, MFF, TransF, and MPA, into the backbone network. The results of these experiments are presented in Table  Map Combination Experiment. To investigate the impact of different modes on the time window of disease onset, a series of experiments are conducted on various mode combinations using the techniques proposed in this study. The results of different map combinations are presented in detail in Table  Comparison with SOTA Methods. In Table "
Acute Ischemic Stroke Onset Time Classification with Dynamic Convolution and Perfusion Maps Fusion,4.0,Conclusion,"In this study, we propose a TSS classification model that integrates dynamic convolution and multi-map fusion to enable rapid and accurate diagnosis of unknown stroke cases. Our approach leverages the dynamic convolution mechanism to enhance model representation without introducing additional network complexity. We also employ a multi-map fusion strategy, consisting of MFF and TransF, to incorporate local and global correlations across low-order and high-order features, respectively. Furthermore, we introduce an MPA module to extract and incorporate as much critical feature information as possible. Through a series of rigorous experiments, our proposed method outperforms several state-of-the-art models in accuracy and robustness. Our findings suggest that our approach holds immense promise in assisting medical practitioners in making effective diagnosis decisions for TSS classification."
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,,"London WC2R 2LS, UK","Abstract. Synthetic images generated from deep generative models have the potential to address data scarcity and data privacy issues. The selection of synthesis models is mostly based on image quality measurements, and most researchers favor synthetic images that produce realistic images, i.e., images with good fidelity scores, such as low Fréchet Inception Distance (FID) and high Peak Signal-To-Noise Ratio (PSNR). However, the quality of synthetic images is not limited to fidelity, and a wide spectrum of metrics should be evaluated to comprehensively measure the quality of synthetic images. In addition, quality metrics are not truthful predictors of the utility of synthetic images, and the relations between these evaluation metrics are not yet clear. In this work, we have established a comprehensive set of evaluators for synthetic images, including fidelity, variety, privacy, and utility. By analyzing more than 100k chest X-ray images and their synthetic copies, we have demonstrated that there is an inevitable trade-off between synthetic image fidelity, variety, and privacy. In addition, we have empirically demonstrated that the utility score does not require images with both high fidelity and high variety. For intra-and cross-task data augmentation, mode-collapsed images and low-fidelity images can still demonstrate high utility. Finally, our experiments have also showed that it is possible to produce images with both high utility and privacy, which can provide a strong rationale for the use of deep generative models in privacy-preserving applications. Our study can shore up comprehensive guidance for the evaluation of synthetic Supplementary Information The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_2."
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,1.0,Introduction,"Fig.  In 2002, SMOTE  Deep learning practitioners have been using various metrics to evaluate synthetic images, including Fréchet Inception Distance (FID)  Here, where μ 1 , μ 2 Σ 1 , and Σ 2 are the mean vectors and covariance matrices of the feature representations of two sets of images. A high difference between the image diversity (Σ 1 and Σ 2 ) also leads to a high FID score, which further complicates fidelity evaluation. Another entangled fidelity evaluation is the precision. As is shown in Fig.  When evaluating these entangled metrics, it is difficult to find the true weakness and strengths of synthetic models. In addition, large-scale experiments are currently the only way to measure the utility of synthetic data. The confusion of evaluation metrics and this time and resource-consuming evaluation of synthetic data utility increase the expenses of synthetic model selection and hinder the real-world application of synthetic data. In this study, we aim to provide a set of evaluation metrics that are mathematically disentangled and measure the potential correlation between different aspects of the synthetic image as in Fig.  Overall, our study contributes new insights into the use of synthetic data for medical image analysis and provides a more objective and reproducible approach to evaluating synthetic data quality. By addressing these fundamental questions, our work provides a valuable foundation for future research and practical applications of synthetic data in medical imaging."
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,2.0,Deep Generative Models and Evaluation Metrics,"In this study, we conducted an empirical evaluation using two state-of-theart deep generative models: StyleGAN2, which has brought new standards for generative modeling regarding image quality  With this motivation, we aimed to redefine the conventional evaluation metrics of synthetic images and disentangle them into four independent properties. In this study, we employed a metric-based membership inference attack in an unsupervised manner to evaluate the privacy of our model. We presume that synthetic records should have some similarity with the records used to generate them  We can compute the privacy preservation score of a single synthetic image s j |j ∈ [0, N)] with the definition of three sets. If the synthetic data p j is in the copy set of any real data, the privacy protection ability of this p j is 0, i.e., For the synthetic data, the overall privacy protection ability P ∈ [0, 1] was then defined by With this privacy definition, we have adjusted the original fidelity evaluation  The measurements of image distance can be tricky due to the high resolution of the original images (512×512). Thus, we first used VQ-VAE  Variety. To measure the variety, we introduced the JPEG file size of the mean image. The lossless JPEG file size of the group average image was used to measure the inner class variety in the ImageNet dataset  Utility. We divided our X-ray dataset into four groups: training datasets A1 and A2, validation set B, and testing set C. We also included an additional open-access pediatric X-ray dataset, D. For our simulation, we treated A1 as a local dataset and A2 as a remote dataset that cannot be accessed by A1. We evaluated the utility of synthetic data in two conditions: 1. A1 vs. adding synthetic data generated from A1. In this condition, no privacy issue is considered. 2. A1 vs. adding synthetic data generated from A2. In this condition, synthetic data will be evaluated using privacy protection skills. Under both conditions, we evaluated the intra-task augmentation utility and cross-task augmentation utility to simulate real-world use cases for synthetic data. Intra-task augmentation utility is measured by the percentage improvement in classification accuracy of C when adding synthetic data to the training dataset. We used a paired Wilcoxon signed-rank test to assess the significance of the accuracy improvement. If the improvement is significant, it indicates that the synthetic images are useful. We compared the augmentation utility with simple augmentations, such as random flipping, rotating, and contrasting. The cross-task augmentation utility is determined by the power of features extracted from the models trained with synthetic data. We used the models to extract features from D and trained a Support Vector Machine classifier on these features to measure accuracy. This allowed us to evaluate whether synthetic images can provide powerful features that facilitate downstream tasks. Similarly, the cross-task augmentation utility is also the percentage improvement in classification accuracy compared to the model trained only on A1."
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,3.0,Experimental Settings and Parameters,"We primarily evaluated the performance of synthetic data on the CheXpert dataset, with a focus on identifying the presence of pleural effusion (PE). To perform our evaluation, we split the large dataset into four subsets: A1 (15004 with PE and 5127 without PE), A2 (30692 with PE and 10118 without PE), B (3738 with PE and 1160 PE), and C (12456 with PE and 3863 without PE). To evaluate the cross-task utility of synthetic models, we used an X-ray dataset D For the StyleGAN2 method, we utilized six truncation parameters during sampling to generate six sets of synthetic images (φ ∈ [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]). In total, we trained 16 classification models on different combinations of datasets, including A1, A1+A2, A1+StyleGAN2-synthesized A1 (6 models), A1+LDMsynthesized A1, A1+StyleGAN2-synthesized A2 (6 models), and A1+LDM-synthesized A2. For further information on implementation details, hyperparameters and table values, please refer to our supplementary file and our publicized codes https://github.com/ayanglab/MedSynAnalyzer."
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,4.1,The Proposed Metrics Match with Human Perception,"In our work, we proposed to use VQ-VAE to extract discrete features from original high-resolution X-ray images. To prove the validity of our method, we selected twenty images from each synthetic dataset and dataset A1 and invited two clinicians to rate the fidelity and variety manually. The human perceptual fidelity is rated from 0 to 1; and the human perceptual variety is computed by the percentage of different scans identified from the selected twenty synthetic images, i.e., if they thought all twenty patients were derived from the same scan, the human perceptual variety score is 1/20 = 0.05. To assure a fair comparison, we allow discussion between them. The result is shown in Fig. "
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,4.2,The Trade-Off Between Fidelity and Variety,"All of our experiments showed a strong negative correlation between variety and fidelity, with a Pearson's correlation coefficient of -0.92 (p < 0.01). As it is widely known in GAN-based models, fidelity and variety are in conflict  For cross-task augmentation, synthetic data with a higher variety is favored for its utility as mode collapse can limit the focus of classification networks and lead to poor generalization performance on other tasks. For instance, a network trained to focus on lung opacity differences near the hemidiaphragm may not help in the accurate diagnosis of pediatric pneumonia, which is the motivation behind dataset D. As shown in Fig.  As mentioned, we invited two radiologists to visually assess synthetic images. The visual inspection showed that all twenty LDM-synthesized images were easily recognized as fake due to their inability to capture the texture of X-ray images, as shown in the supplementary file. However, the shape and boundaries of the lungs were accurately captured by LDM. Despite their low visual fidelity, we demonstrated that these synthetic images still contribute to powerful feature extraction, which is crucial for cross-task utility."
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,4.4,What Kind of Synthetic Data is Desired by Downstream Tasks,When Privacy is an Issue? It is also discussed in the literature about the dilemma between utility and privacy 
You Don’t Have to Be Perfect to Be Amazing: Unveil the Utility of Synthetic Images,5.0,Conclusion,"In this work, we proposed a four-dimensional evaluation metric for synthetic images, including a novel privacy evaluation score and utility evaluation score. Through intensive experiments in over 100k chest X-ray images, we drew three major conclusions which we can envision that have broad applicability in medical image synthesis and analysis. Firstly, there is an inevitable trade-off among different aspects of synthetic images, especially between fidelity and variety. Secondly, different downstream tasks require different properties of synthetic images, and synthetic images do not necessarily have to reach high metric scores across all aspects to be useful. Traditionally, low fidelity and mode collapses have been treated as disadvantages in data synthesis, and numerous algorithms have been proposed to fix these issues. However, our work demonstrates that these failures of synthetic data do not always sabotage their utility as expected. Lastly, we have showed that it is possible to achieve both privacy and utility in transfer learning problems. In conclusion, our work contributes to the development of synthetic data as a valuable solution to enrich real-world datasets, to evaluate thoroughly medical image synthesis as a pathway to overall enhance medical image analysis tasks."
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,1.0,Introduction,"The brain functional connectome (FC) is a graph with brain regions of interest (ROIs) represented as nodes and pairwise correlations of fMRI time series between the ROIs represented as edges. FC has been shown to be a promising biomarker for the early diagnosis and tracking of neurodegenerative disease progression (e.g., Alzheimer's Disease (AD)) because of its ability to capture disease-related alternations in brain functional organization  Node features are commonly utilized in FC to extract important information. It is also essential to recognize the significance of edge features in FC, which are highly informative in characterizing the interdependencies between ROIs. Furthermore, node embeddings obtained from GNN manipulation contain essential information that should be effectively leveraged. Current GNNs feasible to graphs with multiple time points  In this work, we proposed Brain Tokenized Graph Transformer (Brain TokenGT), the first framework to achieve FC trajectory embeddings with builtin interpretability, shown in Fig. "
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,2.1,Problem Definition,"The input of one subject to the proposed framework is a sequence of brain networks G = [G 1 , G 2 , ..., G t , ..., G T ] with T time points. Each network is a graph G = (V, E, A), with the node set V = {v i } M i=1 , the edge set E = V × V , and the weighted adjacency matrix A ∈ R M ×M describing the degrees of FC between ROIs. The output of the model is an individual-level categorical diagnosis ŷs for each subject s."
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,2.2,Graph Invariant and Variant Embedding (GIVE),"Regarding graph topology, one of the unique characteristics of FC across a trajectory is that it has invariant number and sequence of nodes (ROIs), with variant connections between different ROIs. Here, we designed GIVE, which consists of Invariant Node Embedding (INE) and Variant Edge Embedding (VEE)."
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,,Invariant Node Embedding (INE).,"To obtain node embeddings that capture the spatial and temporal information of the FC trajectory, we utilized evolving graph convolution  Formally, for each node v i in V , we define a dynamic neighbourhood graph as G i = [g i1 , g i2 , .., g it , ..., g iT ] (Fig.  Variant Edge Embedding (VEE). For tasks such as graph classification, an appropriate representation of edges also plays a key role in the successful graph representation learning. To achieve edge embeddings, we first integrated graphs from multiple time points by defining Spatial Edge and Temporal Edge, and then obtained spatial and temporal edge embeddings by transforming an FC trajectory to the dual hypergraph. For each FC trajectory, we should not only investigate the edges between different ROIs in one static FC (i.e., spatial domain) but also capture the longitudinal change across different time points (i.e., time domain). Instead of focusing only on intrinsic connections (i.e., spatial edges (e s )) between different ROIs in each FC, for each of the two consecutive graphs G t and G t+1 , we added M temporal edges (e t ) to connect corresponding nodes in G t and G t+1 , with weights initialized as 1. The attached features to spatial and temporal edges were both initialized by the concatenation of node features from both ends and their initial weights. Accordingly, one trajectory would be treated as a single graph for downstream edge embedding. We denote the giant graph with T time points contained as G T , with weighted adjacency matrix A T ∈ R T M×T M (Fig.  is the original node features matrix with a D dimensional feature vector for each node, M ∈ R |E|×M is the original incidence matrix, and E ∈ R |E|×(2D+1) is the initialized edge features matrix. We then performed hypergraph convolution  where W * is the diagonal hyperedge weight matrix, D and B are the degree matrices of the nodes and hyperedges respectively, and Θ is the parameters matrix. Interpretability is important in decision-critical areas (e.g., disorder analysis). Thanks to the design of spatio-temporal edges, we could achieve built-in binary level interpretability (i.e., both nodes and edges contributing most to the given task, from e t and e s , respectively) by leveraging HyperDrop  where 'score' function is hypergraph convolution layers used to compute scores for each hypergraph node (e s or e t in the original graph). 'TopE' selects the nodes with the highest E scores (note: ranking was performed for nodes from e s and e t separately, and HyperDrop was only applied to nodes from e s with hyperparameter E), and idx is the node-wise indexing vector. Finally, the salient nodes (from e t ) and edges (from e s ) were determined by ranking the scores averaged across the trajectory."
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,2.3,Brain Informed Graph Transformer Readout (BIGTR),"Proper readout for the embeddings from GNN manipulation is essential to produce meaningful prediction outcome for assisting diagnosis and prognosis. The vanilla ways are feeding the Node Embeddings, and Spatial and Temporal Edge Embeddings generated from the GIVE module into pooling and fully connected layers. However, this would result in a substantial loss of spatial and temporal information  for v, e s and e t respectively, where node u is a neighbour to node v in the spatial domain and node v is a neighbour to node v in the temporal domain, and x is the original token from GIVE. Thus, the augmented token features matrix is Z ∈ R (MT +|E|T +M (T -1))×(h+dp+2dq) , where h is the hidden dimension of embeddings from GIVE. Z would be further projected by a trainable matrix ω ∈ R (h+dp+2dq)×h . As we targeted individual-level (i.e., G T ) diagnosis/prognosis, a graph token X [graph] ∈ R h was appended as well. Thus, the input to transformer is formally defined as:"
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,3.0,Experiments,Datasets and Experimental Settings. We used brain FC metrics derived from ADNI  The experimental results (Table 
Beyond the Snapshot: Brain Tokenized Graph Transformer for Longitudinal Brain Functional Connectome Embedding,4.0,Conclusion,"This study proposes the first interpretable framework for the embedding of FC trajectories, which can be applied to the diagnosis and prognosis of neurodegenerative diseases with small scale datasets, namely Brain Tokenized Graph Transformer (Brain TokenGT). Based on longitudinal brain FC, experimental results showed superior performance of our framework with excellent built-in interpretability supporting the AD-specific brain network neurodegeneration. A potential avenue for future research stemming from this study involves enhancing the ""temporal resolution"" of the model. This may entail, for example, incorporating an estimation of uncertainty in both diagnosis and prognosis, accounting for disease progression, and offering time-specific node and edge level interpretation."
Automatic Bleeding Risk Rating System of Gastric Varices,1.0,Introduction,"Esophagogastric varices are one of the common manifestations in patients with liver cirrhosis and portal hypertension and occur in about 50 percent of patients with liver cirrhosis  Recent works have proven the effectiveness and superiority of deep learning (DL) technologies in handling esophagogastroduodenoscopy (EGD) tasks, such as the detection of gastric cancer and neoplasia  To learn from experienced endoscopists, GV datasets with bleeding risks annotation is needed. While most works and public datasets focus on colonoscopy  In sum, the contributions of this paper are: 1) a novel GV bleeding risk rating framework that constructively introduces segmentation to enhance the robustness of representation learning; 2) a region-constraint module for better feature localization and a cross-region attention module to learn the correlation of target GV with its context; 3) a GV bleeding risk rating dataset (GVbleed) with high-quality annotation from multiple experienced endoscopists. Baseline methods have been evaluated on the newly collected GVbleed dataset. Experimental results demonstrate the effectiveness of our proposed framework and modules, where we improve the accuracy by nearly 5% compared to the baseline model. "
Automatic Bleeding Risk Rating System of Gastric Varices,2.0,Methodology,The architecture of the proposed framework is depicted in Fig. 
Automatic Bleeding Risk Rating System of Gastric Varices,2.1,Segmentation Module,"Due to the large intra-class variation between GV with the same bleeding risk and small inter-class variation between GV and normal tissue or GV with different bleeding risks, existing classification models exhibit poor perform and tend to lose focus on the GV areas. To solve this issue, we first embed a segmentation network into the classification framework. The predict the varices mask is then used to assist the GV feature to obtain the final bleeding risk rate. Specifically, we use SwinUNet  where is a smooth constant equals to 10 -5 . A straightforward strategy to utilize the segmentation mask is directly using it as an input of the classification model, such as concatenating the image with the mask as the input. Although such strategy can improve the classification performance, it may still lose focus in some hard cases where the GV area can hardly be distinguished. To further regularize the attention and fully utilize the context information around the GV area, on top of the segmentation framework we proposed the cross-region attention module and the region-constraint module."
Automatic Bleeding Risk Rating System of Gastric Varices,2.2,Cross-Region Attention Module,"Inspired by the self-attention mechanism  Then, through similarity measuring, we can compute the attention with which composes of two correlations: self-attention over varices regions and crossregion attention between varices and background regions. Finally, the output feature is calculated as: where γ is a learnable parameter. Then the cross-region attentive feature V is fed into a classifier to predict the bleeding risk."
Automatic Bleeding Risk Rating System of Gastric Varices,2.3,Region Constraint Module,"To improve the focus ability of the model, we propose the region constraint module (RCM) to add a constraint on the class activation map (CAM) of the classification model. Specifically, we use the feature map after the last convolutional layer to calculate the CAM  After getting the CAM, we regularize CAM by calculating the dice loss between the CAM and ground truth mask of varices region l co ."
Automatic Bleeding Risk Rating System of Gastric Varices,2.4,Network Training,"In our framework, we use the cross entropy loss as the classification loss: where p is the prediction of the classifier and y is the ground-truth label. And the total loss of our framework can be summarized as: where N is the total number of samples, ω s , ω co and ω cl are weights of the three losses, respectively. The training process of the proposed network consists of three steps: 1) The segmentation network is trained first; 2) The ground-truth segmentation masks and images are used as the inputs of the CRAM, the classification network, including CRAM and RCM, are jointly trained; 3) The whole framework is jointly fine-tuned."
Automatic Bleeding Risk Rating System of Gastric Varices,3.0,GVBleed Dataset,"Data Collection and Annotation. The GVBleed dataset contains 1678 endoscopic images with gastric varices from 527 cases. All of these cases are collected from 411 patients in a Grade-III Class-A hospital during the period from 2017 to 2022. In the current version, images from patients with ages elder than 18 are retained 1 . The images are selected from the raw endoscopic videos and frames. To maximize the variations, non-consecutive frames with larger angle differences are selected. To ensure the quality of our dataset, senior endoscopists are invited to remove duplicates, blurs, active bleeding, chromoendoscopy, and NBI pictures. Criterion of GV Bleeding Risk Level Rating. Based on the clinical experience in practice, the GV bleeding risks in our dataset are rated into three levels, i.e., mild, moderate, and severe. The detailed rating standard is as follows: 1) Mild: low risk of bleeding, and regular follow-up is sufficient (usually with a diameter less than or equal to 5 mm). 2) Moderate: moderate risk of bleeding, and endoscopic treatment is necessary, with relatively low endoscopic treatment difficulty (usually with a diameter between 5 mm and 10 mm). 3) Severe: high risk of bleeding and endoscopic treatment is necessary, with high endoscopic treatment difficulty. The varices are thicker (usually with a diameter greater than 10 mm) or less than 10mm but with positive red signs. Note that the diameter is only one reference for the final risk rating since the GV is with 1 Please refer to the supplementary material for more detailed information about our dataset. various 3D shapes and locations. The other facts are more subjectively evaluated based on the experience of endoscopists. To ensure the accuracy of our annotation, three senior endoscopists with more than 10 years of clinical experience are invited to jointly label each sample in our dataset. If three endoscopists have inconsistent ratings for a sample, the final decision is judged by voting. A sample is selected and labeled with a specific bleeding risk level only when two or more endoscopists reach a consensus on it. The GVBleed dataset is partitioned into training and testing sets for evaluation, where the training set contains 1337 images and the testing set has 341 images. The detailed statistics of the three levels of GV bleeding risk in each set are shown in Table "
Automatic Bleeding Risk Rating System of Gastric Varices,4.1,Implementation Details,"In experiments, the weights ω s , ω co , and ω cl of the segmentation loss, region constraint loss, and classification loss are set to 0.2, 1, and 1, respectively. The details of the three-step training are as follows: 1) Segmentation module: We trained the segmentation network for 600 epochs, using Adam as the optimizer, and the learning rate is initialized as 1e-3 and drops to 1e-4 after 300 epochs. 2) Cross-region attention module and region constraint module: We used the ground-truth varices masks and images as the inputs of the CRAM, and jointly trained the CRAM and RCM for 100 epochs. Adam is used as the optimizer, the learning rate is set to 1e-3; 3) Jointly fine-tuning: The whole framework is jointly fine-tuned for 100 epochs with Adam as optimizer and the learning rate set to 1e-3. In addition, common data augmentation techniques such as rotation and flipping were adopted here."
Automatic Bleeding Risk Rating System of Gastric Varices,4.2,Results Analysis,Table 
Automatic Bleeding Risk Rating System of Gastric Varices,5.0,Conclusions,"In this paper, we propose a novel bleeding risk rating framework for gastric varices. Due to the large intra-class variation between GV with the same bleeding risk and small inter-class variation between GV and normal tissue or GV with different bleeding risks, existing classification models cannot correctly focus on the varices regions and always raise poor performance. To solve this issue, we constructively introduce segmentation to enhance the robustness of representation learning. Besides, we further design a region-constraint module for better feature localization and a cross-region attention module to learn the correlation of target GV with its context. In addition, we collected the GVBleed dataset with high-quality annotation of three-level of GV bleeding risks. The experiments on our dataset demonstrated the effectiveness and superiority of our framework."
Automatic Bleeding Risk Rating System of Gastric Varices,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_1.
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting,1.0,Introduction,"Radiology is a critical medical field that relies on accurate and efficient communication between radiologists and other healthcare professionals enabled through radiology reports. However, generating these reports takes a lot of time and is prone to errors, as it often relies on ambiguous natural language. One alternative to free-text reports is to use structured reporting, which is endorsed by radiology societies, saves time, and offers standardized content and terminology  Automated report generation can reduce radiologists' workload and support quick diagnostic decisions. Most current research focuses on generating freetext reports, which lack standardization, and still face challenges of ambiguity and difficulties in clinical correctness evaluation  Structured reports with high standardization have a structured layout and content, e.g., organized in expendable trees with drop-down menus to select answers  We demonstrate the effectiveness of our streamlined design and hierarchical framework in our experimental results, reaching competitive results to the SOTA on the medical VQA benchmark VQARad and setting a baseline for Rad-ReStruct. Overall, our work is a significant step towards automating the population of structured radiology reports and provides a valuable benchmark for future research in this area."
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting,2.1,Rad-ReStruct Dataset,"We propose the first benchmark dataset to enable the development and comparison of methods for the population of structured reports entailing hierarchical and fine-grained classifications for radiological images. Rad-ReStruct is based upon the IU-Xray dataset  Creation of Structured Report Template. We build upon the semistructured encoded findings provided for the IU-XRay data collection  We utilize this semi-structured finding representation to construct a highly detailed report template as shown in Fig.  Overall, our structured report template provides a rigorous and comprehensive framework for classifying radiological images and mimics the style of a structured report in a clinical setting. This enables the development and comparison of methods for the population of structured reports and the prediction of finegrained radiological findings. Dataset and Evaluation Metrics. Our dataset consists of structured reports for each patient in the IU-XRay data collection, for which finding codes and a frontal X-Ray were available. The new dataset includes 3720 images matched to 3597 structured patient reports entailing more than 180k questions. If multiple images belong to one patient, each image is considered an independent sample. We use a 80-10-10 split to create train, validation and test set. To avoid data leakage, we ensure that different images of the same patient are in the same split. The goal of our task is to produce fine-grained finding classifications for populating a structured report given an X-Ray image of a patient. This task involves answering a series of questions about the image, gradually adding more detail. We define several evaluation metrics for the proposed benchmark. As the distribution of questions and answers is very imbalanced, we evaluate with the macro precision, recall, and F1 score over all possible paths in the question tree to encourage methods that also perform well in under-represented question-answer combinations. One path is a unique position in the report combined with a specific answer option. Further, we employ report-level accuracy to measure how many predicted reports are entirely correct. During the evaluation, we enforce consistency within the question hierarchy. For example, if the answer to a higherlevel question is ""no"", we prohibit to answer a lower-level question positively. This ensures the generated reports are consistent and coherent, as in a real medical report. Lastly, as multiple instances of an object, sign or pathology can occur for one patient, we iteratively ask for further occurrences, when the model predicts a positive answer. (e.g., ""Are there other opacities in the lung?""). We restrict the number of follow-up questions by the maximum of per-patient occurrences in the data. As the order of occurrences is ambiguous, we apply instance matching during the metric computation. We order all predicted instances such that the highest F1 score for this finding is achieved."
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting,2.2,Hierarchical Visual Question Answering,"With Rad-ReStruct, we propose a hierarchical VQA task, where lower level questions are dependent on context information. For instance, to answer the questions ""What is the degree?"" it is essential to know what the question is referring to. This information is given through the previous question, which could be ""Is there Pneumonia in the lung? Yes"". To integrate this context, we propose a hierarchical VQA framework that can effectively answer questions about medical images by considering previously asked questions. We extend the input to the model by pre-pending the current question with the history of previously asked questions and the model's answers. This extension enables interpretable and consistent results. We leverage a pretrained image encoder, EfficientNet-b5  Feature Encoding. For fusing the image and text features, we construct a token sequence (Nx458x768) of the form <image_tokens> <history_tokens> <question_tokens>. The image tokens (Nx196x768) consist of the flattened embedding representation of the image encoder, while the history and question text (Nx259x768) is encoded jointly using RadBERT. The different parts are separated by a <SEP> token and fused by a single transformer layer. We encode the type of input in the token-type IDs with different IDs for the image tokens, history questions, history answers, and the current question. Further, we use modified positional encodings to preserve the 2D spatial information of the image as well as the sequential order of the text. We create a joint positional encoding (Nx768x458) by concatenating a 1D positional encoding "
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting,3.0,Experiments and Results,"We test our model on Rad-ReStruct, setting a baseline for this new dataset. To further validate our model design, we compare the performance of our model with previous work on the standard VQA benchmark VQARad. We train all models on a NVIDIA A40 GPU. We use pytorch-lightning 1.8.3. and the AdamW optimizer with a learning rate of 5e-5 for VQARad and 1e-5 for Rad-ReStruct. For all models, we set the number of epochs by maximizing validation set performance. Rad-ReStruct. For Rad-ReStruct, the history includes all higher-level questions on the same question path. Additionally, attribute questions asked pre- viously about an element, are included in the history, enabling the model to provide consistent predictions. Lastly, the history includes previously predicted instances of the same element. Table  Our labels' hierarchical, structured formulation enables a performance analysis on different topics and levels. Hi-VQA performs well in detecting the existence of sub-topics like objects, diseases, signs, and abnormalities. However, attribute prediction performance is much lower, likely due to the rarity and complexity of these questions and error propagation from higher levels. Such an analysis is precious to understand what a model learned and when it should be trusted. VQARad is a medical VQA benchmark with 315 radiological images and 3515 questions. The task is to make a classification over 2248 possible answers. In VQARad multiple questions are asked about one image, but in previous work they are always answered separately. To make use of possible inter-dependencies between questions, we define five question levels based on question topics in VQARad, ranging from general to specific: Modality → Plane → Organ → Presence, Count, Abnormality → Color, Position, Size, Attributes, Other. For a certain question, previously asked questions from lower or the same level are included in the history. During training, we augment the history by randomly dropping and reordering questions within a level to prevent overfitting on this small dataset. Table "
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting,4.0,Discussion and Conclusion,"By introducing Rad-ReStruct, the first structured radiology reporting benchmark, we create a framework to develop, evaluate, and compare structured reporting methods. The structured formulation enables an accurate evaluation of clinical correctness at different levels of granularity, focusing on levels with greater clinical importance. Moreover, such a structured finding representation could then again, rule-based, be converted to a text report while maintaining clinical accuracy. To model structured reporting, we present hi-VQA, a novel, hierarchical VQA framework with a streamlined architecture that leverages history context for multi-question and multi-level tasks. The autoregressive formulation and consistent evaluation increase interpretability and mimic the workflow of structured reporting. Moreover, as each prediction takes previous answers into account, it would allow for an interactive workflow, where the model can make predictions and react to corrections while a radiologist fills out a report. We set a first baseline for Rad-ReStruct, with particularly good performance on higher-level questions. Although our model has limited performance on the low-level attribute questions, it performed competitive to state-of-the-art on VQARad, indicating the difficulty of our new task. We see this as an opportunity to develop methods for fine-grained understanding of radiology images, rather than solely focusing on higher-level diagnoses. Further, we show the positive effect of history integration, which is crucial for hierarchical and contextdependent tasks such as structured report population. Our work represents a significant step forward in the development of automated structured radiology report population methods, while allowing an accurate and multi-level evaluation of clinical correctness and fostering fine-grained, in-depth radiological image understanding."
Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_40.
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,1.0,Introduction,"Bias in medicine has demonstrated a notable challenge for providing comprehensive and equitable care. Implicit biases can negatively affect patient care, particularly for marginalized populations with lower socioeconomic status  Within the radiology arm of AI research, there have been significant advances in diagnostics and decision making  Pulmonary embolism (PE) is an example of health disparities related to race. Black patients exhibit a 50% higher age-standardized PE fatality rate and a twofold risk for PE hospitalization than White patients  However, one issue with traditional survival analysis is bias from single modal data that gets compounded when curating multimodal datasets, as different combinations of modes and datasets create with a unified structure. Multimodal data sets are useful for fair AI model development as the bias complementary from different sources can make de-biased decisions and assessments. In that process, the biases of each individual data set will get pooled together, creating a multimodal data set that inherits multiple biases, such as racial bias  We developed a PE outcome model that predicted mortality and detected bias in the output. We then implemented methods to remove racial bias in our dataset and model and output unbiased PE outcomes as a result. Our contributions are as follows: "
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,2.0,Bias in Survival Prediction,"This section describes the detail of how we identify the varying degrees of bias in multimodal information and illustrates bias using the relative difference in survival outcomes. We will first introduce our pulmonary embolism multimodal datasets, including survival and race labels. Then, we evaluate the baseline survival learning framework without de-biasing in the various racial groups. Dataset. The Pulmonary Embolism dataset used in this study from 918 patients (163 deceased, median age 64 years, range 13-99 years, 52% female), including 3978 CTPA images and 918 clinical reports, which were identified via retrospective review across three institutions. The clinical reports from physicians that provided crucial information are anonymized and divided into four parts: medical history, clinical diagnosis, observations and radiologist's opinion. For each patient, the race labels, survival time-to-event labels and PESI variables are collected from clinical data, and the 11 PESI variables are used to calculate the PESI scores, which include age, sex, comorbid illnesses (cancer, heart failure, chronic lung disease), pulse, systolic blood pressure, respiratory rate, temperature, altered mental status, and arterial oxygen saturation at the time of diagnosis  Diverse Bias of Multimodal Survival Prediction Model. We designed a deep survival prediction (SP) baseline framework for multimodal data as shown in Fig.  In Table "
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,3.0,De-biased Survival Prediction Model,"Based on our SP baseline framework and multimodal findings from Sect. 2, we present a feature-level de-biased SP module that enhances fairness in survival outcomes by decoupling race attributes, as shown in the lower right of Fig.  To promote race-intrinsic learning in E m i and C m i , we apply diversify with latent vectors swapping. The randomly permuted zsur in each mini-batch concatenate with z ID to obtain z sw = [z ID ; zsur ]. The two neural networks are trained to predict y ID or ỹID with CE loss or GCE loss. As the random combination are generated from different samples, the swapping decreases the correlation of these feature vectors, thereby enhancing the race-intrinsic attributes. The loss functions of swapping augmentation added to train two neural networks is defined as: The survival prediction head C m sur predicts the risk on the survival feature z sur . CoxPH loss function  where Y t and Y e are survival labels including the survival time and the event, respectively. The weights λ sw and λ sur are assigned as 0.5 and 0.8, respectively, to balance the feature disentanglement and survival prediction."
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,4.0,Experiment,"We validate the proposed de-biased survival prediction frameworks on the collected multi-modality PE data. The data from 3 institutions are randomly split The lung region of CPTA images is extracted with a slice thickness of 1.25 mm and scaled to N × 512 × 512 pixels  The GatorTron  We build the encoders of the baseline SP modules and de-biased SP modules with multi-layer perceptron (MLP) neural networks and ReLu activation. The MLPs with 3 hidden layers are used to encode image and text features, and another MLPs with 2 layers encodes the features of PESI variables. A fully connected layer with sigmoid activation acts as a risk classifier C m sur (z m sur ) for survival prediction, where z m sur is the feature encoded from single modal data. For training the biased and de-biased SP modules, we collect data from one modality as a batch with synchronized batch normalization. The SP modules are optimized using the AdamW "
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,4.1,Results,"Table  We conducted ablation studies to examine the effect of the two key components, including swapping feature augmentation and race-balance resampling. As shown in Table "
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,5.0,Discussions and Conclusions,"In this work, we developed a de-biased survival prediction framework based on the race-disentangled representation. The proposed de-biased SP framework, based on the SOTA PE detection backbone and large-scale clinical language model, can predict the PE outcome with a higher survival correlation ahead of the clinical evaluation index. We detected indications of racial bias in our dataset and conducted an analysis of the multimodal diversity. Experimental results illustrate that our approach is effective for eliminating racial bias while resulting in an overall improved model performance. The proposed technique is clinically relevant as it can address the pervasive presence of racial bias in healthcare systems and offer a solution for minimizing or eliminating bias without pausing to evaluate their affection for the models and tools. Our study is significant as it highlights and evaluates the negative impact of racial bias on deep learning models. The proposed de-biased method has already shown the capacity to relieve them, which is vital when serving patients with an accurate analysis. The research in our paper demonstrates and proves that eliminating racial biases from data improves performance, and yields a more precise and robust survival prediction tool. In the future, these de-biased SP modules can be plugged into other models, offering a fairer method to predict survival outcomes."
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_50.
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,1.0,Introduction,"Alzheimer's disease (AD) is one of the most pervasive neurodegenerative disorders, causing an increasing morbidity burden that may outstrip diagnosis and management capacity with the population ages. The assessment of AD usually involves the acquisition of structural magnetic resonance imaging (sMRI) images, since it offers accurate visualization of the anatomy and pathology of the brain. Brain abnormalities (e.g., atrophy, enlargement, malformation) are known to be the most discriminative and reliable biomarkers  Recently, convolutional neural networks (CNN) have been used for automatic classification of AD from sMRI. Many methods  In this paper, we propose a novel domain-knowledge-constrained neural network for the diagnosis of AD using sMRI from multiple source domains. We designed a new domain-knowledge encoding module into a ResNet-like architecture for feature learning that yields a latent feature space with domain specific and domain shared information. In addition, we propose to use segmentationfree, resampling-free, patch-free 3D sub-images, which offers global context information and subject-level abnormalities to further refines generalizable and reproducible predictions."
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,2.0,Methods,We propose to design and implement an end-to-end neural network (Fig. 
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,2.1,Patch-Free 3D Feature Extractor,"We first estimate a bounding box around relevant anatomical objects in the input sMRI. The objects are automatically identified by affine registration, which transforms the reference template to each image in the dataset to estimate label for the image. We note that, the estimated labels are only used to locate the bounding box, it has no effect on the individual's atrophy since we pad extra space to ensure the cropped image contain all interested objects with respect to registration errors. Then, we crop the input image using the located bounding box to obtain the sub-image as input to our network. It need to be clarified that the cropping size is a fixed tuple determined by the maximum bounding box containing informative anatomical objects associated with AD. To encode global context information, we propose a patch-free 3D feature extractor for different source domains, which is expected to learn domaininvariant features while not eliminating domain-specific features. Each domain has a unique label classifier, allowing adjustments for domain differences. Based on ResNet, we design our feature extractor as shown in Fig.  where X l and X l+1 are the input and output of the basic block and F (W i , X l ) denotes the nonlinear mapping in the basic block. Since the dimensions of X and F (W i , X) must be the same for summation, we use the linear mapping W s to adjust the dimensions of X in the shortcut connection. In the proposed method, we use global average pooling function which is more suitable for disease classification, because the global average pooling operation reflects the information of gray matter volume in brain regions and preserves the relative position relationship between different channels of the feature map. In the output layer, we use a softmax classifier based on cross-entropy loss to calculate the loss between the predicted and true labels."
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,2.2,Global Average Pooling,"Global average pooling solves the problem of excessive image feature dimensions. If the feature maps of 3D images are directly expanded for classification, it will significantly increase the number of classifier parameters and increase the time and space complexity of training. Global average pooling averages the 3D feature maps in the channel dimension, preserving the relative position relationship between channels and reducing the resources required for model training. The dimension change in the global average pooling is , where B denotes the batch-size and C denotes the channel number. where δ denotes the image feature extracted by ResNet, and D, H, W denote the three dimensions of the feature. Since global average pooling has fewer parameters, it can prevent over-fitting to some extent, further more, global average pooling sums out the spatial information, thus it is more robust to spatial translation of the input."
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,2.3,Domain-Knowledge Encoding,"The domain-knowledge encoding module is designed to give relative similarity weights to source domains from a new sample. The weights reflect the similarity between the testing sample and source domains, allowing the module to share strength only between similar domains. Our model uses multiple classifiers for prediction from the features extracted by the feature extractor. The classifiers are independent from each other. We feed the image features to different classifiers and generate weights to each classifier, summing the predictions of each classifier according to the weights as the final output. where Y denotes the prediction result of X, c_num denotes the number of classifiers, D i denotes the center which X belongs, δ denotes the extracted feature from X, classif ier j denotes one classifier and θ j are the parameters in classif ier j . Multiple classifiers can capture the invariant and specific feature distributions between different domains, comparing the similarity of feature distributions between training source and unseen target domains by a joint training of the admixture classifiers, generating weights to integrate the feature distributions of known domains to fit the unknown domain feature distributions."
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,3.1,Data Description,"Structural T1-weighted brain MRI data of 809 subjects (468 male, 341 female, age 68.16 ± 8.12 years, range 42-89 year) were acquired from 7 in-house independent multiple centers as detailed in "
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,3.2,Implementation Details,"We first evaluated the model using leave-center-out cross-validation, where one center was selected for testing at a time and all remaining centers were used for training. Then, we applied the trained model on an independent validation set of unseen images for subjects with MCI. All images were cropped to have the same size of  During training, we sorted all training centers and feed the image features from site i to all classifiers, and set the weight of classif ier j(j=i) to 1 and the weight of the rest classifiers to 0. We used cross-entropy to calculate the prediction error and update the parameters of the feature extractor and classif ier j by backpropagation. In testing stage, we feed the image features from the test center to all classifiers, and the final prediction was used the weighted average of predicted probability over all classifiers as the final prediction. We used SGD algorithm to optimize the model coefficients, and set the initial learning rate to 0.001 and reduce the learning rate to one-tenth of the previous value every 50 epochs. The method was implemented using PyTorch 1.1 with Python 3.7. The experiments were run on an Intel Xeon CPU with 16 cores, 43 GB. RAM and a NVIDIA A5000 GPU with 24 GB memory. The code and model are available at https://github.com/Yanjie-Z/DomainKnowledge4AD. Fig. "
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,3.3,Performance Evaluation,"To evaluate the proposed approach, we feed 2 different types of input to the conventional 3D-ResNet  We used AUC-ROC curves to evaluate the classification effectiveness  To evaluate the interpretability of the model, we used Grad-CAM "
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,4.0,Discussion,"We proposed a novel reproducible and generalizable neural network to assist the automatically diagnosis of AD that benefits from domain knowledge and global contextual information with the help of segmentation-free, resamplingfree, patch-free sub-image. The model was evaluated with leave-center-out crossvalidation and with an independent set of unseen images for subjects with MCI (Fig.  We did ablation studies to evaluate the proposed method (Table  Our results and all comparative frameworks tend to predict worse for center 3, probably because it has some subjects with AD who have higher MMSE (Fig. "
Learning with Domain-Knowledge for Generalizable Prediction of Alzheimer’s Disease from Multi-site Structural MRI,5.0,Conclusion,"We proposed a novel end-to-end domain-knowledge constrained neural network for automatic and reproducible diagnosis of AD using sMRI images. We proposed a new domain-knowledge encoding module that learn simultaneously with a ResNet-like feature extractor for domain specific and domain shared representations. The network directly takes the segmentation-free, patch-free images in original resolution as input, which is able to learn with global contextual information for subject-level pathological brain dysmorphologies features to further refines reproducible predictions. Our experiments demonstrate superior performance and generalize well to completely unseen domain."
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,1.0,Introduction,"Breast cancer impacts women globally  In clinical practice, breast density and traditional statistical methods for predicting breast cancer risks such as the Gail [14] and the Tyrer-Cuzick models  When prior mammograms are available, radiologists compare prior exams to the current mammogram to aid in the detection of breast cancer. Several studies have shown that utilizing past mammograms can improve the classification performance of radiologists in the classification of benign and malignant masses  In this paper, we introduce a deep neural network that makes use of prior mammograms, to assess a patient's risk of developing breast cancer, dubbed PRIME+ (PRIor Mammogram Enabled risk prediction). We hypothesize that mammographic parenchymal pattern changes between current and prior allow the model to better assess a patient's risk. Our method is based on a transformer model that uses attention  The method is trained and evaluated on a large and diverse dataset of over 9,000 patients and shown to outperform a model based on state-of-the art risk prediction techniques for mammography "
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,2.1,Risk Prediction,"Survival analysis is done to predict whether events will occur sometime in the future. The data comprises three main elements: features x, time of the event t, and the occurrence of the event e  We typically want to estimate the hazard function h(t), which measures the rate at which patients experience the event of interest at time t, given that they have survived up to that point. The hazard function can be expressed as the limit of the conditional probability of an event T occurring within a small time interval [t, t + Δt), given that the event has not yet occurred by time t: The cumulative hazard function H(t) is another commonly used function in survival analysis, which gives the accumulated probability of experiencing the event of interest up to time t. This function is obtained by integrating the hazard function over time from 0 to t: H(t) = t 0 h(s)ds."
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,2.2,Architecture Overview,"We build on the current state-of-the art MIRAI  When dealing with right-censored data, we use an indicator function δ i (t) to determine whether the information for sample i at time t should be included in the loss calculation or not. This helps us exclude unknown periods and only use the available information. It is defined as follows: Here, e i is a binary variable indicating whether the event of interest occurs for sample i (i.e., e i = 1) or not (i.e., e i = 0), and C i is the censoring time for sample i, which is the last known time when the sample was cancer-free. We define the ground-truth H is a binary vector of length T max , where T max is the maximum observation period. Specifically, H(t) is 1 if the patient is diagnosed with cancer within t years and 0 otherwise. We use binary cross entropy to calculate the loss at time t for sample i: . The total loss is defined as: Here, N is the number of exams in the training set. The goal of training the model is to minimize this loss function, which encourages the model to make accurate predictions of the risk of developing breast cancer over time."
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,2.3,Incorporating Prior Mammograms,"To improve the performance of the breast cancer risk prediction model, we incorporate information from prior mammograms taken with the same view, using a transformer decoder structure  During training, we randomly select one prior mammogram, regardless of when they were taken. This allows the model to generalize to varying time intervals. To pair each current mammogram during inference with the most relevant prior mammogram, we first select the prior mammogram taken at the time closest to the current time. This approach is based on research showing that radiologists often use the closest prior mammogram to aid in the detection of breast cancer  Next, a shared backbone network is used to output the current feature x curr and the prior feature x prior . These features are then flattened and fed as input to the transformer decoder, where multi-head attention is used to find information related to the current feature in the prior feature. The resulting output is concatenated and passed through a linear layer to produce the current-prior comparison feature x CP C . The current-prior comparison feature and current feature are concatenated to produce the final feature x * = x CP C ⊕ x curr , which is then used by the base hazard network and time-dependent hazard network to predict the cumulative hazard function Ĥ."
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,3.1,Dataset,"We compiled an in-house mammography dataset comprising 16,113 exams (64,452 images) from 9,113 patients across institutions from the United States, gathered between 2010 and 2021. Each mammogram includes at least one prior mammogram. The dataset has 3,625 biopsy-proven cancer exams, 5,394 biopsyproven benign exams, and 7,094 normal exams. Mammograms were captured using Hologic (72.3%) and Siemens (27.7%) devices. We partitioned the dataset by patient to create training, validation, and test sets. The validation set contains 800 exams (198 cancer, 210 benign, 392 normal) from 400 patients, and the test set contains 1,200 exams (302 cancer, 290 benign, 608 normal) from 600 patients. All data was de-identified according to the U.S HHS Safe Harbor Method. Therefore, the data has no PHI (Protected Health Information) and IRB (Institutional Review Board) approval is not required."
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,3.2,Evaluation,"We make use of Uno's C-index  We evaluate the effectiveness of PRIME+ by comparing it with two other models: (1) baseline based on MIRAI, a state-of-the art risk prediction method from "
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,3.3,Implementation Details,"Our model is implemented in Pytorch and trained on four V100 GPUs. We trained the model using stochastic gradient descent (SGD) for 20K iterations with a learning rate of 0.005, weight decay of 0.0001, and momentum of 0.9. We use a cosine annealing learning rate scheduling strategy  We resize the images to 960 × 640 pixels and use a batch size of 96. To augment the training data, we apply geometric transformations such as vertical flipping, rotation and photometric transformations such as brightness/contrast adjustment, Gaussian noise, sharpen, CLAHE, and solarize. Empirically, we find that strong photometric augmentations improved the risk prediction model's performance, while strong geometric transformations had a negative impact. This is consistent with prior work "
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,3.4,Results,"Ablation Study. To better understand the merit of the transformer decoder, we first performed an ablation study on the architecture. Our findings, summarized in Table  As shown in the top rows in Table  We observe similar performance improvements when evaluating cases with at least 180 days to cancer diagnosis. Interestingly, the C-index as well as timedependent AUCs of all three methods decreased compared to when evaluating using all cases. The intuition behind this result is that mammograms taken near the cancer diagnosis (<180 days) likely contain visible signs of cancer and thus the task of risk prediction is easier. The model must learn patterns of risk, not Lastly, we empirically confirm that a transformer decoder effectively models spatial relations between prior and current mammograms by demonstrating consistent performance improvements of PRIME+ across both short-term and longterm risk prediction settings. Our results suggest that incorporating changes in patients using prior mammograms and a transformer decoder improves the performance of breast cancer risk prediction models. Analysis Based on Density. To better understand why adding prior images improves performance, we divided our test set into subgroups to examine the performance of the baseline model and the PRIME+ model on each of these groups. Mammographic breast density is one of the most important risk factor to predict breast cancer  Mammographic breast density was determined using the Breast Imaging Reporting and Data System (BI-RADS) composition classification. BI-RADS category A, B are defined as fatty breasts and BI-RADS category C, D are classified as dense breasts. To determine the density category, we employed an internally developed density prediction model, as most exams lack BI-RADS ground truth. This model achieved an accuracy of 0.81 on the internal density validation set. We categorized the exams into two groups based on changes in density: ""change"" and ""no change"". Density change was defined according to whether the BI-RADS category changed in the current image as compared to the prior image. As shown in Table "
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,4.0,Conclusion,"In this paper, we introduce a novel breast cancer risk prediction method, PRIME+, which incorporates prior mammograms with a transformer decoder to capture changes in breast tissue over time. By doing so, we achieve high performance for both short-term and long-term risk prediction. Our extensive experiments on a dataset of 16,113 exams show that PRIME+ outperformed a model based on the state-of-the-art for breast cancer risk prediction "
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,1.0,Introduction,"Alzheimer's disease (AD) is an irreversible progressive neurodegenerative brain disorder that ranks as the sixth leading cause of death in the United States  In the most of existing multimodal fusion methods, Euclidean space is typically assumed as the natural geometry of the brain. As such, both feature embedding and model establishment are conducted in the Euclidean space. However, recent studies have suggested that non-Euclidean hyperbolic space may offer a more accurate interpretation of brain connectomes than Euclidean space  To answer this question, we propose a novel graph-based hyperbolic deep model to conduct multimodal fusion in hyperbolic space for MCI study. Specifically, we embedded brain functional activities into hyperbolic space, where a hyperbolic graph convolution neural network (HGCN) "
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,2.0,Related Work,Gromov has demonstrated that hyperbolic spaces are particularly well-suited for representing tree-like structures 
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,3.1,Data Description and Preprocessing,"We used 209 subjects, comprising 116 individuals from the NC group (60 females, 56 males; 74.26 ± 8.42 years) and 93 subjects from the MCI group (53 females, 40 males; 74.24 ± 8.63 years) from ADNI dataset. Each subject has structural MRI (T1-weighted), resting-state fMRI (rs-fMRI), and DTI. We performed standard preprocessing procedures as described in "
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,3.2,Preliminary,"The hyperbolic space H n K is a specific type of n-dimension Riemannian manifold (M, g) with a constant negative sectional curvature K  A manifold M of n-dimension is a topological space that is locally Euclidean. For all x ∈ M, the tangent space T x M at a point x is the vector space of the same n-dimension as M, containing all tangent vector tangentially pass through x. The metric tensor g x  x : M → T x M that maps an arbitrary point y ∈ M to T x M: where λ K x = 2 1+K||x|| 2 is the conformal factor at point x."
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,3.3,Functional Profile Learning in Hyperbolic Space,"In this work, we aim to learn a disease-related functional profile in the hyperbolic space. To this end, we firstly mapped the averaged functional signal of region i in Euclidean -f E i , to hyperbolic space via (4): ), here we choose point 0, the origin in hyperbolic space to conduct transformation. Then upon f H i , we defined the parameterized functional-pairwise distance between region i and region j in hyperbolic space by a learnable mapping matrix M : It is worth noting that (  )) Here, A f H i,j ∈ B n K represents the pairwise functional profile between brain regions i and j in hyperbolic space. σ is the bandwidth parameter of Gaussian kernel and is treated as a hyper-parameter. In the proposed model, M is initialized as identity matrix to avoid introducing any bias and iteratively updated during the training process based on classification results."
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,3.4,Multimodal Fusion by HGCN,"A major goal of this work is to conduct effective multimodal fusion of brain structural and functional data in hyperbolic space for MCI study. To achieve this aim, we combined the learned functional profile with the brain structural network in the hyperbolic space by: where I is the identity matrix, A S E is the original individual structural network calculated by fiber count, and A S H = Exp K 0 (A S E ) is the hyperbolic counterpart of A S E . θ ∈ (0, 1) is a learnable parameter to control the contributions of structural and functional components in the combined new brain connectome ÂH . In the training process, A f H and A S H will be iteratively updated, and disease-related knowledge (from classification) is extracted and passed to functional profile (A f H ) and structural network (A S H ) and then transferred to the new brain connectome ÂH . Next, ÂH will be used as the new topology along with node features in graph convolution conducted by HGCN. HGCN performs graph convolution within the hyperbolic space in two steps: where h (l) i is the input of i-th node in the l-th layer, W ∈ R d×d and b ∈ R d are the weight and bias of the l-th layer. (  i ) and ÂH is used as the adjacency matrix A in Eq. 10."
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,4.1,Experimental Setting,"Data Settings. In this study, the entire brain was partitioned into 148 distinct regions using the well-established Destrieux Atlas. Averaged fMRI signals were then calculated for each brain region, and the brain structural network (A S ) was generated for each individual. 5-fold cross-validation was performed using a cohort of 209 individuals, consisting of 116 elder normal control (NC) and 93 MCI patients. Model Settings. The HGCN model in this work has two layers, wherein the output of each HGCN layer was set to 148 and 296, respectively. These outputs were subsequently combined using a hyperbolic fully connected layer "
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,4.2,Classification Performance Comparison,We compared the classification performance of our proposed model with other state-of-the-art methods on MCI/NC classification task using multi-modal data and presented the results in Table 
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,4.3,Ablation Study,"The proposed model implements both feature embedding and graph neural network establishment in hyperbolic space. We conducted ablation studies to evaluate the impact of the two factors on the classification performance of MCI/NC classification. In contrast to Euclidean space, where the curvature is a constant 0, hyperbolic space has negative curvature, and we evaluated a range of curvature values from -0.4 to -1.4. The results were summarized in Table "
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,4.4,Feature Representation,"To gain further insight into the impact of space choice on feature representation, we visualized the distribution of two distinct feature representations in Euclidean space and hyperbolic space. Firstly, we employed principal component analysis (PCA) to project the high-dimensional input feature vectors from both the Euclidean space and the hyperbolic space onto a two-dimensional space. The resulting visualizations are presented in Fig. "
Multimodal Deep Fusion in Hyperbolic Space for Mild Cognitive Impairment Study,5.0,Conclusion,"It is widely believed that the AD/MCI related brain alterations involve both brain structure and function. However, effectively modeling the complex relationships between structural and functional data and integrating them at the network level remains a challenge. Recent advances in graph modeling in hyperbolic space have inspired us to integrate multimodal brain networks via graph convolution conducted in the hyperbolic space. To this end, we mapped brain structural and functional features into hyperbolic space and conducted graph convolution by a hyperbolic graph convolution neural network, which enabled us to obtain a new brain connectome that incorporates multimodal information. Our developed model has demonstrated exceptional performance when compared to state-of-the-art methods. Furthermore, the results suggest that feature embedding and graph convolution neural network establishment in hyperbolic space are both crucial for enhancing performance."
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,1.0,Introduction,"Deep learning models have demonstrated undoubted potential in achieving expert-level medical image interpretation when powered by large-scale labeled datasets  The current mainstream approaches for medical image-text pre-training are based on the popular self-supervised learning (SSL) technique known as contrastive learning  In fact, the learning principles of contrastive learning and masked autoencoding suggest that they could be complementary to each other. Contrastive imagetext learning explicitly discriminates the positive and negative pairs of images and text reports, making it good at promoting strong discriminative capabilities of image representations. Instead, masked autoencoding aims to reconstruct masked image/text tokens, which emphasizes learning local image structures, but may be less effective in capturing discriminative representations. This motivates us to propose a novel contrastive masked image-text modeling (CMITM) method for medical visual representation learning. Our framework is designed to accomplish three self-supervised learning tasks: First, aligning the representations of masked images with text reports. Second, reconstructing the masked images themselves. Third, reconstructing the masked text reports using the learned image representations. To reduce the information misalignment between the masked images and text reports, we incorporate a representation decoder to recover the missed information in images, which benefits the cross-modal learning. Moreover, the synergy of contrastive learning and masked autoencoding is unleashed via a cascaded training strategy. Our framework is pre-trained on a large-scale medical dataset MIMIC-CXR with paired chest X-ray images and reports, and is extensively validated on four downstream classification datasets with improved fine-tuning performance. Combining the two techniques yields consistent performance increase and the improvement of our method even surpasses the benefits of adding data from 1% to 100% labels on CheXpert dataset."
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,2.0,Method,Figure 
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,2.1,Cross-Modal Contrastive Learning with Masked Images,"Cross-modal contrastive learning has demonstrated to be an effective tool to align the representations of a medical image with that of its paired text report. In this way, the network is guided to interpret the image contents with the knowledge provided by medical reports. Different from previous methods, the cross-modal contrastive learning in our framework is between the representations of masked images and unmasked reports, aiming to integrate the benefits of both contrastive learning and masked image modeling. Specifically, denote D = {x v,i , x t,i } N i=1 as the multi-modal dataset consisting of N pairs of medical images x v,i and medical reports x t,i . Each input image is split into 16 × 16 non-overlap patches and tokenized as image tokens a v,i , and each text report is also tokenized as text tokens a t,i . A random subset of image patches is masked out following masked autoencoder (MAE)  where s vt i,j = z T v,i z t,j , s tv i,j = z T t,i z v,j , τ denotes the temperature which is set to be 0.07 following common practice, and B is the number of image-report pairs in a batch. The cross-modal contrastive loss is used to supervise the network training associated with the data flow in orange line in Fig. "
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,2.2,Masked Image-Text Modeling,"The masked image-text modeling component in our framework consists of two parallel tasks, i.e., masked image reconstruction with image only information and masked text reconstruction with cross-modal information. We follow the design in "
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,,Masked Image Reconstruction.,"As aforementioned, the input images are masked and processed by the image encoder E v to obtain q um v,i . As shown in Fig.  where xm v,i , xm v,i denote the predicted and the original high-resolution masked patches, | • | calculates the number of masked patches, and norm denotes the pixel normalization with the mean and standard deviation of all pixels in a patch suggested in MAE "
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,,Cross-Modal Masked Text Reconstruction.,"To make the most of the text reports paired with imaging data for learning visual representations, the task of cross-modal masked text modeling aims to encourage the encoded visible image tokens q um v,i to participate in completing the masked text reports. Specifically, besides the full texts, we also forward a masked text report with a masking ratio of 50% to the text encoder E t . Following  where a m t,i , âm t,i denote the original and recovered masked text tokens respectively, H denotes the cross entropy loss. Similar to masked image reconstruction, the loss is also only computed on the masked text tokens. The image and text reconstruction losses are used to supervise the network training associated with the data flow in gray line in Fig. "
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,2.3,Training Procedures and Implementation Details,Training a framework that combines cross-modal contrastive learning and masked autoencoding is non-trivial. As observed in prior work 
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,3.0,Experiments,"To validate our framework, the image encoder of the pre-trained model is used to initialize a classification network with a ViT-B/16 backbone and a linear classification head. We adopt the fine-tuning strategy as used in  Pre-training Dataset. To pre-train our framework, we utilize MIMIC-CXR dataset  Fine-Tuning Datasets. We transfer the learned image representations to four datasets for chest X-ray classification. NIH ChestX-ray  Ablation Study. We perform ablation analysis on RSNA and COVIDx datasets with 1% labeled data to investigate the effect of each component in the proposed method. In Fig. "
Contrastive Masked Image-Text Modeling for Medical Visual Representation Learning,4.0,Conclusion,"We present a novel framework for medical visual representation learning by integrating the strengths of both cross-modal contrastive learning and masked image-text modeling. With careful designs, the effectiveness of our method is demonstrated on four downstream classification datasets, consistently improving data efficiency under data-scarce scenarios. This shows the complementary benefits of the two SSL techniques in medical visual representation learning. One limitation of the work is that the pre-training model is evaluated solely on classification tasks. A compelling extension of this work would be to conduct further evaluation on a broader spectrum of downstream tasks, including organ segmentation, lesion detection, and image retrieval, thereby providing a more comprehensive evaluation of our model's capabilities."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,1.0,Introduction,"Lung cancer screening has a significant impact on the rate of mortality associated with lung cancer. Studies have proven that regular lung cancer screening with low-dose computed tomography (LDCT) can lessen the rate of lung cancer mortality by up to 20%  Fig.  One of the major challenges of lung nodule malignancy prediction is the quality of datasets  Technically, current studies on lung nodule malignancy prediction mainly focus on deep learning-based techniques  In this paper, we suggest mimicking radiologists' diagnostic procedures from intra-context parsing and inter-nodule recalling (see illustrations in Fig. "
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,,abbreviated as PARE.,"At the intra-level, the contextual information of the nodules provides clues about their shape, size, and surroundings, and the integration of this information can facilitate a more reliable diagnosis of whether they are benign or malignant. Motivated by this, we first segment the context structure, i.e., nodule and its surroundings, and then aggregate the context information to the nodule representation via the attention-based dependency modeling, allowing for a more comprehensive understanding of the nodule itself. At the inter-level, we hypothesize that the diagnosis process does not have to rely solely on the current nodule itself, but can also find clues from past learned cases. This is similar to how radiologists rely on their accumulated experience in clinical practice. Thus, the model is expected to have the ability to store and recall knowledge, i.e., the knowledge learned can be recorded in time and then recalled as a reference for comparative analysis. To achieve this, we condense the learned nodule knowledge in the form of prototypes, and recall them to explore potential inter-level clues as an additional discriminant criterion for the new case. To fulfill both LDCT and NCCT screening needs, we curate a large-scale lung nodule dataset with pathology-or follow-up-confirmed benign/malignant labels. For the LDCT, we annotate more than 12,852 nodules from 8,271 patients from the NLST dataset  Our contributions are summarized as follows: "
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,2.0,Method,Figure 
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,2.1,Context Segmentation,"The nodule context information has an important effect on the benign and malignant diagnosis. For example, a nodule associated with vessel feeding is more likely to be malignant than a solitary one "
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,2.2,Intra Context Parse,"In this stage, we attempt to enhance the discriminative representations of nodules by aggregating contextual information produced by the segmentation model. Specifically, the context mask is tokenized into a set of sequences via the overlapped patch embedding. The input image is also split into patches and then embedded into the context tokens to keep the original image information. Besides, positional encoding is added in a learnable manner to retain location information. Similar to the class token in ViT "
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,2.3,Inter Prototype Recall,"Definition of the Prototype: To retain previously acquired knowledge, a more efficient approach is needed instead of storing all learned nodules in memory, which leads to a waste of storage and computing resources. To simplify this process, we suggest condensing these pertinent nodules into a form of prototypes. As for a group of nodules, we cluster them into N groups {C 1 , ..., C N } by minimizing the objective function N i=1 p∈Ci d(p, P i ) where d is the Euclidean distance function and p represents the nodule embedding, and refer the center of each cluster, P i = 1 |Ci| p∈Ci p, as its prototype. Considering the differences between benign and malignant nodules, we deliberately divide the prototypes into benign and malignant groups, denoted by P B ∈ R N/2×D and P M ∈ R N/2×D . Cross Prototype Attention: In addition to parsing intra context, we also encourage the model to capture inter-level dependencies between nodules and external prototypes. This enables PARE to explore relevant identification basis beyond individual nodules. To accomplish this, we develop a Cross-Prototype Attention (CPA) module that utilizes nodule embedding as the query and the prototypes as the key and value. It allows the nodule embedding to selectively attend to the most relevant parts of prototype sequences. The state of query at the output of the last CPA module servers as the final nodule representation to predict its malignancy label, either ""benign"" (y = 0) or ""malignant"" (y = 1)."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,,Updating Prototype Online:,"The prototypes are updated in an online manner, thereby allowing them to adjust quickly to changes in the nodule representations. As for the nodule embedding q of the data (x, y), its nearest prototype is singled out and then updated by the following momentum rules, where λ is the momentum factor, set to 0.95 by default. The momentum updating can help accelerate the convergence and improve the generalization ability."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,2.4,Training Process of PARE,"The Algorithm 1 outlines the training process of our PARE model which is based on two objectives: segmentation and classification. The Dice and crossentropy loss are combined for segmentation, while cross-entropy loss is used for classification. Additionally, deep classification supervision is utilized to enhance the representation of nodule embedding in shallow layers like the output of the UNet and SCA modules."
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,3.1,Datasets and Implementation Details,Data Collection and Curation: NLST is the first large-scale LDCT dataset for low-dose CT lung cancer screening purpose  Cross prototype attention 13: end for 
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,,16:,Update prototype according to Eq. 1
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,,17:,"J ← seg loss(m, s) + 3 i=1 cls loss(y, p i ) Update loss 18: end for patient, and localized and labeled the nodules in the scan as benign or malignant based on the rough candidate nodule location and whether the patient develops lung cancer provided by NLST metadata. The nodules with a diameter smaller than 4mm were excluded. The in-house cohort was retrospectively collected from 2,565 patients at our collaborating hospital between 2019 and 2022. Unlike NLST, this dataset is noncontrast chest CT, which is used for routine clinical care. Segmentation annotation: We provide the segmentation mask for our in-house data, but not for the NLST data considering its high cost of pixel-level labeling. The nodule mask of each in-house data was manually annotated with the assistance of CT labeler "
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,,Train-Val-Test:,"The training set contains 9,910 (9,413 benign and 497 malignant) nodules from 6,366 patients at NLST, and 2,592 (843 benign and 1,749 malignant) nodules from 2,113 patients at the in-house cohort. The validation set contains 1,499 (1,426 benign and 73 malignant) nodules from 964 patients at NLST. The NLST test set has 1,443 (1,370 benign and 73 malignant) nodules from 941 patients. The in-house test set has 1,437 (1,298 benign and 139 malignant) nodules from 452 patients. We additionally evaluate our method on the LUNGx  Implementation: All experiments in this work were implemented based on the nnUnet framework "
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,3.2,Experiment Results,Ablation Study: In Table 
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,,Generalization on LDCT and NCCT:,"Our model is trained on a mix of LDCT and NCCT datasets, which can perform robustly across low-dose and regular-dose applications. We compare the generalization performance of the models obtained under three training data configurations (LDCT, NCCT, and a combination of them). We find that the models trained on either LDCT or NCCT dataset alone cannot generalize well to other modalities, with at least a 6% AUC drop. However, our mixed training approach performs best on both LDCT and NCCT with almost no performance degradation. Method AUC NLNL "
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,4.0,Conclusion,"In this work, we propose the PARE model to mimic the radiologists' diagnostic procedures for accurate lung nodule malignancy prediction. Concretely, we achieve this purpose by parsing the contextual information from the nodule itself and recalling the previous diagnostic knowledge to explore related benign or malignancy clues. Besides, we curate a large-scale pathological-confirmed dataset with up to 13,000 nodules to fulfill the needs of both LDCT and NCCT screening scenarios. With the support of a high-quality dataset, our PARE achieves outstanding malignancy prediction performance in both scenarios and demonstrates a strong generalization ability on the external validation."
Uncertainty Inspired Autism Spectrum Disorder Screening,1.0,Introduction,"Autism Spectrum Disorder (ASD) has been a prevalent neurodevelopmental disorder worldwide that one in 44 kids aged 8 years in the United States suffers from it as reported in 2021  However, diagnosing ASD relies on subjective evaluations that are expensive and clinically demanding. Seminal works  Early efforts  However, existing deep-learning-based methods usually define an imageranking strategy as a pre-processing step to select a certain number of images. During training, each visual stimulus is treated equally, ignoring the distinct contributions of different stimuli. Besides, during the diagnosis procedure, a fixed number of images are shown to a subject which takes a relatively long time, thereby leading to poor cooperation of subjects, especially little kids. To tackle the above issues, in this paper, we propose a novel Uncertaintyinspired ASD Screening Network, named UASN, to distinguish the importance of each visual stimulus for different individuals. Despite the success of uncertainty in computer vision  Specifically, the uncertainty in UASN works in two ways to ensure both higher accuracy and lower time consumption. On the one hand, given an input gaze pattern, we estimate the uncertainty by comparing the difference between the fixation map and the ones of ASD and TD groups. The uncertainty will be assigned a lower value for a larger disparity, suggesting the importance of the given gaze pattern for identifying a certain individual. Subsequently, guided by the estimated uncertainty, we design a truncated weighting loss to select the most distinctive gaze patterns and further dynamically adjust the contributions made by different stimuli, resulting in a more efficient classification. On the other hand, how to reduce the diagnosis time is also a key factor in real clinical applications, especially for preschool children. To achieve this goal, we propose a personalized diagnosis method that ranks the stimuli according to the estimated uncertainty. Instead of the random shuffle mode for image viewing, we recommend a top similar or dissimilar stimulus for the next viewing according to the decision of the previous gaze patterns. Following the proposed protocol, our method achieves state-of-the-art performance while spending much less diagnosis time. In general, our contributions can be summarized as follows: 1) we propose the first usage of the Uncertainty-inspired ASD Screening Network, named UASN, for identifying ASD people; 2) we estimate the uncertainty of each gaze pattern and further design a truncated weighting loss, which can enforce the model to dynamically adjust the contributions of different gaze patterns during training; 3) we design a personalized online diagnosis protocol that can dramatically reduce the diagnosis time without losing accuracy; 4) we conduct comprehensive experiments on the Saliency4ASD benchmark and achieve state-of-the-art performance only using 1/5 visual stimuli compared with other leading approaches."
Uncertainty Inspired Autism Spectrum Disorder Screening,2.0,Uncertainty Inspired ASD Screening,"Our UASN is built upon traditional DNNs and consists of two novel stages: 1) uncertainty-guided training, and 2) uncertainty-guided personalized diagnosis. During training, we estimate an uncertainty value for each gaze pattern and further apply it for weighting the training loss. Besides, for a more simplified diagnosis procedure, we design an uncertainty-based strategy that adaptively selects the most discriminative images based on the subject's gaze behaviors."
Uncertainty Inspired Autism Spectrum Disorder Screening,2.1,Uncertainty Guided Training,"Figure  Gaze Pattern Feature Extraction. Formally, by collecting a group of ASD and TD subjects S = {s i } M i=1 's eye movement data on a set of images X = {x j } N j=1 , we get the corresponding scanpaths which comprise each fixation point's position and duration in the temporal order. The labels of the two clinical groups are denoted as Y = {y i } M i=1 ∈ {0, 1}. To generate the discriminative features of the given gaze pattern (i-th subject watching j-th image), we first feed the image x j into a ResNet-50  where ŷj i and y j i denote the predicted and ground truth labels of i-th subject respectively. If belonging to ASD, y j i = 1 for all images {x j } N j=1 , otherwise 0. Uncertainty Estimation. We believe that different images contribute unequally to a subject's final classification due to the subject's unique preferences for viewing images. As a result, we estimate an uncertainty value for each gaze pattern based on the variance between its fixation map and the ones of the two clinical groups (i.e., ASD and TD). For instance, an ASD's fixation map on a discriminative image should appear more similar to the ASD group's averaged one than the TD group's so the variance is supposed to be large. We first generate the fixation map for each gaze pattern according to the fixation data. Then, given two groups' fixation maps on each image in the dataset, for each subject, we apply cosine similarity to compute an uncertainty measurement on each image. Let F j i denote the fixation map of the i-th subject's fixation map on the j-th image, and F j + , F j -denote the fixation maps of ASD and TD group for j-th image respectively. The uncertainty can be written as: where C is the cosine similarity function, D j i is the distinguishability of the j-th image when viewed by the i-th subject and µ j i denotes the uncertainty. Specifically, for images that do not contain certain subjects' eye-tracking data, we reasonably set the µ j i to be large because we assume that the absence of eye-tracking data is due to a lack of interest, and further signals ineffectiveness. Truncated Weighting Loss. Upon obtaining the uncertainty value µ j i , we can utilize the uncertainty to re-weight the training loss by teaching the model which images to trust and which to discredit. We hope that the larger the µ is, the less the image contributes to the final classification, so the corresponding loss needs to be reduced correspondingly. However, for some gaze patterns that are confusing and much more difficult to distinguish between ASD and healthy people, it is more suitable to discard those unreliable patterns. Considering this, we finally propose a truncated weighting BCE loss for training. Specifically, when the estimated uncertainty value is larger than a pre-defined threshold t, we set the corresponding loss to zero. In summary, The final loss function is denoted as: where I is the indicator function and t is the threshold. Only when the condition of [µ ≤ t] is met, is the value of the indicator function set to 1, and the L tr _bce remains. Specifically, for a more reasonable computation, we do not simply set the t to be a fixed value. Instead, we determine t with an adaptive technique. For each subject, we sort the uncertainty values from low to high and choose the 1/3 of the images with the lowest uncertainty and retain the contributions they make to the prediction while discarding the remainder."
Uncertainty Inspired Autism Spectrum Disorder Screening,2.2,Uncertainty Guided Personalized Diagnosis,"On the basis of training our model in an uncertainty-guided way, we are encouraged to go deeper to simplify the diagnostic procedure. To this end, we incorporate personalized diagnosis into our work, taking into account the gaze behavior features of each subject. Figure  Similarity-Based Image Ranking. Then, we build a viewing list simulating the diagnosis procedure where images are shown to a subject one by one and the list is updated in real-time. In the beginning, we select an image from the image set randomly to initialize the viewing list. When a trial is completed, the viewing list is subsequently updated. In each trial, we generate an average feature of all images in the viewing list. We then compute the cosine similarity between the feature of the current image and images in the feature bank to obtain a similarity list. We sort the list in similarity-ascending order. Uncertainty-Based Viewing List Updating. To determine which images should be included in the viewing list, a method based on uncertainty is developed. First, we feed the image in the list and the corresponding eye-tracking data into the ASD screening network which is composed of the Uncertainty Estimation module and the Gaze Pattern Feature Extraction module in Fig. "
Uncertainty Inspired Autism Spectrum Disorder Screening,3.1,Dataset and Experimental Settings,"Dataset. So far, only Saliency4ASD  Evaluation Protocol. We employ the leave-one-subject-out cross-validation method for evaluating the model's performance. Specifically, for the Saliency4ASD dataset, we perform a 28-round validation with each round selecting only one subject for testing while the remaining 27 subjects work as the training data. Metrics. We follow the previous works  Implementation Details. The experimental setting mostly follows the "
Uncertainty Inspired Autism Spectrum Disorder Screening,3.2,Comparison with State-of-the-Art,We conduct extensive experiments to explore whether our proposed model outperforms the state-of-the-art. We compare our UASN with 
Uncertainty Inspired Autism Spectrum Disorder Screening,3.3,Ablation Study,"Effect of the Uncertainty Estimation. To verify the effect of uncertainty estimation, we divide the 300 images into three non-overlapping subsets based on the ascending order of uncertainty level, denoted as top-100, middle-100, and bottom-100 subsets. Table "
Uncertainty Inspired Autism Spectrum Disorder Screening,4.0,Conclusion,"In this paper, we present UASN, a novel ASD screening approach, inspired by uncertainty. The uncertainty benefits the ASD diagnosis in two ways: a weighted truncated training loss that enables the model to learn the most discriminative and effective features of gaze patterns and a personalized procedure that dynamically ranks the stimuli according to the subject's gaze behaviors. Comprehensive results show superior performance in classifying ASD people."
Uncertainty Inspired Autism Spectrum Disorder Screening,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_39.
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,1.0,Introduction,"Screening mammography helps detect breast cancer earlier and has reduced the breast cancer mortality rate significantly  rate of false positive (FP) predictions of CAD can cause a significant reduction in radiologists' specificity  In this work, we tackle these challenges and propose a Multi-view and Multiinstance learning system, M&M. M&M is an end-to-end system that detects malignant findings and provides breast-level classification. To achieve these goals, M&M leverages three components: (1) Sparse R-CNN to replace dense anchors with a set of sparse proposals; (2) Multi-view cross-attention to synthesize information from two views and iteratively refine the predictions, and (3) Multiinstance learning (MIL) to include negative images during training. Ultimately, each component contributes to our goal of reducing false positives. We validate M&M through evaluation on five datasets: two in-house datasets, two public datasets -DDSM  1. We show that sparsity of proposals is beneficial to the analysis of mammograms, which have low disease prevalence (Sec. With MIL, M&M improves the recall at 0.1 FP/image by 12.6% (Fig. "
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,2.1,Sparse R-CNN with Dual Classification Heads,"The sparsity of malignant findings calls into question the use of dense detectors. As shown in Fig.  We modify Sparse R-CNN to include dual classification modules (Fig.  (1) The strictly positive function SoftPlus(x) = log(1 + e x ) is chosen to enforce consistency: a high objectness logit o i is required to generate a high malignancy logit m i . Thus, at the finding level, we obtain the following loss where L giou and L L1 are regression losses as in Sparse R-CNN. L objectness and L malignancy are focal losses applied to the predicted objectness o i and the predicted malignancy m i across all cascading heads 1 ≤ i ≤ 6, respectively."
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,2.2,Multi-view Reasoning,"A standard screening exam includes two standard views of each breast. The craniocaudal (CC) view is taken from the top down, while the mediolateral oblique (MLO) view is captured from the side at an oblique angle. Radiologists examine both views when making a clinical decision as a finding may look innocuous in one view but suspicious in the other. To enable multi-view reasoning, M&M incorporates a cross-attention module  The enhanced embeddings hCC i-1 , hMLO i-1 then generate DynamicConv to interact with RoI features and produce new features h CC i , h MLO i for the (i + 1) th head. Thus, with the proposed cross-attention module, the CC view's proposal features are refined iteratively using the MLO view's proposal features and vice versa."
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,2.3,Multi-instance Learning,"Mammogram annotation is costly to obtain due to a dependency on radiologists. This high cost means that bounding boxes are often unavailable. Further, most mammograms are negative: they do not contain any findings. Yet, a model generalizes poorly if these negative images are dropped during training (Fig.  Since image-and breast-level labels are available, we adopt an MIL module to include images without bounding boxes during training. To compute imageand breast-level scores, we leverage the proposal malignancy logits m i (Eq. (  Next, as CC and MLO views offer complimentary information on a breast, we obtain breast-level malignancy score by averaging the image-level scores across these views. We apply cross-entropy losses L image and L breast at the image and breast level for all training samples. The lesion loss L lesion (Eq. (  (5)"
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,3.0,Experiments,"Implementation Details. We use PyTorch 1.10. The training settings follow Sparse R-CNN  Datasets. We utilize three 2D digital mammography datasets: (1) OPTIMAM : a development dataset derived from the OPTIMAM database  Metrics. We report average precision with Intersection over Union from 0.25 to 0.75. AP mb denotes average precision on the set of annotated malignant and benign images. AP denotes average precision when all data is included. We report free response operating characteristic (FROC) curves and recalls at various FP/image (R@t). Following  Detection Results.  GMIC  ResNet50  23 points (pt) between excluding and including negative images. Large Δ means the models are producing too many FPs on negative images. Sparse R-CNN  Classification Results. Table 3a reports M&M's breast-level and exam-level classification results on OPTIMAM and the two inhouse datasets. We use GMIC  Further studies. In the appendix, we present more qualitative evaluation as well as further ablation studies on (1) number of learnable proposals, (2) different MIL schemes, (3) backbone choices and (4) positional encoding."
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,4.0,Discussion and Conclusion,"We present M&M, an end-to-end model leveraging multi-view reasoning and multi-instance learning for mammography detection and classification. As a detector, M&M offers significant improvement in recall at low FP/image (Fig.  Second, M&M incorporates multi-view reasoning through iterative application of cross-attention and proposal refinement in the cascading heads. M&M's multiview module is effective (Fig.  As a classifier, M&M establishes strong performance on several datasets (Table "
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_75.
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,1.0,Introduction,"The early detection of lesions in medical images is critical for the diagnosis and treatment of various conditions, including neurological disorders. Stroke is a leading cause of death and disability, where early detection and treatment can significantly improve patient outcomes. However, the quantification of lesion burden is challenging and can be time-consuming and subjective when performed manually by medical professionals  One widely used category of unsupervised methods is latent restoration methods. They involve autoencoders (AEs) that learn low-dimensional representations of data and detect anomalies through inaccurate reconstructions of abnormal samples  In contrast, generative adversarial networks (GANs)  Several techniques have been proposed that make use of the inherent spatial information in the data rather than relying on constrained latent representations  In related computer vision areas, such as industrial inspection  This work aims to combine the advantages of constrained latent restoration for understanding healthy data distribution with generative in-painting networks. Unlike previous methods, our approach does not rely on a learned noise model, but instead creates masks of probable anomalies using latent restoration. These guide generative in-painting networks to reverse anomalies, i.e., preserve healthy tissues and produce pseudo-healthy in-painting in anomalous regions. We believe that our proposed method will open new avenues for interpretable, fast, and accurate anomaly segmentation and support various clinical-oriented downstream tasks, such as investigating progression of disease, patient stratification and treatment planning. In summary our main contributions are: • We investigate and measure the ability of SOTA methods to reverse synthetic anomalies on real brain T1w MRI data. • We propose a novel unsupervised segmentation framework, that we call PHANES, that is able to preserve healthy regions and utilize them to generate pseudo-healthy reconstructions on anomalous regions. • We demonstrate a significant advancement in the challenging task of unsupervised ischemic stroke lesion segmentation."
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,2.0,Background,"Latent restoration methods use neural networks to estimate the parameters θ, φ of an encoder E θ and a decoder D φ . The aim is to restore the input from its lower-dimensional latent representation with minimal loss. The standard objective is to minimize the residual, e.g., using mean squared error (MSE) loss: In the context of variational inference  ( KL is the Kullback-Leibler divergence; q φ (z|x) and p θ (x|z) are usually known as the encoder E φ and decoder D θ ; the prior p(z) is usually the normal distribution N (μ 0 , σ 0 ); and the ELBO denotes the Evidence Lower Bound. In unsupervised anomaly detection, the networks are trained only on normal samples x ∈ X ⊂ R N . Given an anomalous input x / ∈ X , it is assumed that the reconstruction x ph = (D φ (E θ (x))) ∈ X represents its pseudo-healthy version. The aim of the pseudo-healthy reconstructions is to accurately reverse abnormalities present in the input images. This is achieved by preserving the healthy regions while generating healthy-like tissues in anomalous regions. Thus, anomalies can ideally be directly localized by computing the difference between the anomalous input and the pseudo-healthy reconstructions: s(x) = |x -x ph |."
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,3.0,Method,"Figure  where E l φ is the l-th embedding of the L encoder layers, , and L Sim is the cosine similarity. Mask Generation. Simple residual errors have a strong dependence on the underlying intensities  with S lpips being the learned perceptual image patch similarity  Inpainting Generative Network. The objective of the refined PH generative network is to complete the masked image by utilizing the remaining healthy tissues to generate a full PH version of the input. Considering computational efficiency, we have employed the recent in-painting AOT-GAN  ) m, with being the pixel-wise multiplication. We compute the final anomaly maps based on residual and perceptual differences:"
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,4.0,Experiments,"Datasets. We trained our model using two publicly available brain T1w MRI datasets, including FastMRI+ (131 train, 15 val, 30 test) and IXI (581 train samples), to capture the healthy distribution. Performance evaluation was done on a large stroke T1-weighted MRI dataset, ATLAS v2.0 "
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,4.1,Reversing Synthetic Anomalies,"In this section, we test whether reconstruction-based methods can generate pseudo-healthy images and reverse synthetic anomalies. Results are evaluated in Table  VAEs produce blurry results that lead to poor reconstruction of both healthy and anomalous regions (LPIPS) and thus poor segmentation performance. While DAEs preserve the healthy tissues well with an LPIPS of 3.94, they do not gen- erate pseudo-healthy reconstructions in anomalous regions (LPIPS ≈ 20). However, they change the intensity of some structures, e.g., hypo-intensities, allowing for improved detection accuracy (see AUPRC and Dice). Simplex noise in "
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,4.2,Ischemic Stroke Lesion Segmentation on T1w Brain MRI,"In this section, we evaluate the performance of our approach in segmenting stroke lesions and show the results in Table "
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,5.0,Discussion,"This paper presents a novel unsupervised anomaly segmentation framework, called PHANES. It possesses the ability to reverse anomalies in medical images by preserving healthy tissues and substituting anomalous regions with pseudohealthy reconstructions. By generating pseudo-healthy versions of images containing anomalies, PHANES can be a useful tool in supporting clinical studies. While we are encouraged by these achievements, we also recognize certain limitations and areas for improvement. For example, the current binarization of anomaly maps does not account for the inherent uncertainty in the maps, which we aim to explore in future research. Additionally, our method relies on accurate initial estimates of the latent restoration and anomaly maps. Nevertheless, our proposed concept is independent of specific approaches and can leverage advancements in both domains. Our method is not optimized for detecting a certain anomaly distribution but rather demonstrates robustness in handling various small synthetic anomalies and diverse stroke lesions. We look forward to generalizing our method to other anatomies and imaging modalities, paving the way for exciting future research in the field of anomaly detection. In conclusion, we demonstrated exceptional performance in reversing synthetic anomalies and segmenting stroke lesions on brain T1w MRIs. We believe that deliberate masking of (possible) abnormal regions will pave new ways for novel anomaly segmentation methods and empower further clinical applications."
DiffULD: Diffusive Universal Lesion Detection,1.0,Introduction,"Universal Lesion Detection (ULD) in computed tomography (CT)  Previous arts in ULD are mainly motivated by the anchor-based detection framework, e.g., Faster-RCNN  To get rid of the drawbacks caused by the anchor mechanism, researchers resort to anchor-free detection, e.g., FCOS  Recently, the diffusion probabilistic model (DPM)  To address this issue, we hereby introduce a novel center-aligned BBox padding strategy in DPM detection to form a diffusion-based detector for Universal Lesion Detection, termed DiffULD. As shown in Fig.  The following DPM training procedure contains two diffusion processes. i) In the forward training process, DiffULD corrupts the perturbated GT with Gaussian noise gradually to generate noisy boxes step by step. Then the model is trained to remove the noise and reconstruct the original perturbated GT boxes. ii) In the reverse inference process, the trained DiffULD can refine a set of randomly generated boxes iteratively to obtain the final detect predictions. Our method gets rid of the drawbacks of pre-defined anchors and the deterioration of training DPM with insufficient GT targets. Besides, DiffULD is inherently advanced in locating targets with diverse sizes since it can predict with arbitrary boxes, which is an advantageous feature for detecting lesions of irregular shapes and various sizes. To validate the effectiveness of our method, we conduct experiments against seven state-of-the-art ULD methods on the public dataset DeepLesion "
DiffULD: Diffusive Universal Lesion Detection,2.0,Method,"In this section, we first formulate our overall detection process for DiffULD and then specify the training manner, inference process and backbone design."
DiffULD: Diffusive Universal Lesion Detection,2.1,Diffusion-Based Detector for Lesion Detection,"Universal Lesion Detection can be formulated as locating lesions in input CT scan I ct with a set of boxes predictions z 0 . For a particular box z , it can be denoted as z = [x 1 , y 1 , x 2 , y 2 ], where x 1 , y 1 and x 2 , y 2 are the coordinates of the top-left and bottom-right corners, respectively. We design our model based on a diffusion model mentioned in  where ᾱt represents the noise variance schedule and t ∈ {0, 1, ..., T }. Subsequently, a neural network f θ (z t , t, I ct ) conditioned on the corresponding CT scan I ct is trained to predict z 0 from a noisy box z T by reversing the noising process step by step. During inference, for an input CT scan I ct with a set of random boxes, the model is able to refine the random boxes to get a lesion detection prediction box z 0 , iteratively."
DiffULD: Diffusive Universal Lesion Detection,2.2,Training,"In this section, we specify the training process with our novelty introduced 'Center-aligned BBox padding'. Center-Aligned BBox Padding. As shown in Fig. We consider the generation in two parts: box scaling and center sampling. (i) Box scaling: We set a hyper-parameter λ scale ∈ (0, 1) for scaling. For z i , ] of perturbated boxes from a 2D Gaussian distribution N whose probability density function can be denoted as: where σ is a size-adaptive parameter, which can be calculated according to the z i 's width and height: Besides, for each input CT scan I ct , we collect all GT BBoxes in z and add random perturbations to them and generate multiple perturbated boxes for each of them. Thus the number of perturbated boxes in an image varies with its number of GT BBoxes. For better training, we fix the number of perturbated boxes as N for all training images. As shown in Fig. "
DiffULD: Diffusive Universal Lesion Detection,,Box Corruption.,As shown in Fig. 
DiffULD: Diffusive Universal Lesion Detection,,Loss function.,"As the model generates the same number of (N ) predictions for the input image, termed as a prediction set, the loss function should be set-wised  where L L1box and L giou are the pairwise box loss. We adopt λ L1box = 2.0 and λ giou = 5.0."
DiffULD: Diffusive Universal Lesion Detection,2.3,Inference,"At the inference stage, with a set of random boxes sampled from Gaussian distribution, the model does refinement step by step to obtain the final predictions z 0 . For better performance, two key components are used: Box Filtering. In each refinement step, the model receives a set of box proposals from the last step. As the prediction starts from arbitrary boxes and the lack of GT (lesion), most of these proposals are very far from lesions. Keeping refining them in the following steps will hinder network training. Toward efficient detection, we send the proposals to the detection head and remove the boxes whose confidential scores are lower than a particular threshold λ conf . The remaining high-quality proposals are sent for followed DDIM sampling. Box Update with DDIM Sampling. DDIM "
DiffULD: Diffusive Universal Lesion Detection,2.4,Backbone Design,"Our overall backbone design is identical to  Multi-window Input. Most prior arts in ULD use a single and fixed window (e.g., a wide window of  3D Context Feature Fusion. We modify the original A3D "
DiffULD: Diffusive Universal Lesion Detection,3.1,Settings,"Our experiments are conducted on the standard ULD dataset DeepLesion  Training Details. DiffULD is trained on CT scans of size 512 × 512 with a batch size of 4 on 4 NVIDIA RTX Titan GPUs with 24GB memory. For hyperparameters, the threshold N for box padding is set to 300. λ scale for box scaling is set to 0.4. λ conf for box filtering is set to 0.5. We use the Adam optimizer with an initial learning rate of 2e -4 and the weight decay as 1e -4. The default training schedule is 120K iterations, with the learning rate divided by 10 at 60K and 100K iterations. Data augmentation strategies contain random horizontal flipping, rotation, and random brightness adjustment. Evaluation Metrics. The lesion detection is classified as true positive (TP) when the IoU between the predicted and the GT BBox is larger than 0.5. Average sensitivities computed at 0.5, 1, 2, and 4 false-positives (FP) per image are reported as the evaluation metrics on the test set for a fair comparison (Table  Table "
DiffULD: Diffusive Universal Lesion Detection,,Methods,Slices @0.5 @1 @2 @4 Avg.[0.  
DiffULD: Diffusive Universal Lesion Detection,3.2,Lesion Detection Performance,"We evaluate the effectiveness of DiffULD against anchor-based ULD approaches such as 3DCE  we conduct an extensive experiment to explore DiffULD's potential on an revised test set of completely annotated DeepLesion volumes, introduced by Lesion-Harvester "
DiffULD: Diffusive Universal Lesion Detection,3.3,Ablation Study,We provide an ablation study about our proposed approach: center-aligned BBox padding. As shown in Table  Our baseline method is training the diffusion model 
DiffULD: Diffusive Universal Lesion Detection,4.0,Conclusion,"In this paper, we propose a novel ULD method termed DiffULD by introducing the diffusion probability model (DPM) to Universal Lesion Detection. We present a novel center-aligned BBox padding strategy to tackle the performance deterioration caused by directly utilizing DPM on CT scans with sparse lesion BBoxes. Compared with other training target padding methods (e.g., padding with random boxes), our strategy can provide additional training targets of higher quality and boost detection performance while avoiding significant deterioration. DiffULD is inherently advanced in locating lesions with diverse sizes and shapes since it can predict with arbitrary boxes, making it a promising method for ULD. Experiments on both standard and revised DeepLesion datasets show that our proposed method can achieve competitive performance compared to state-of-the-art ULD approaches."
DiffULD: Diffusive Universal Lesion Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 10.
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,1.0,Introduction,"Breast cancer is one of the high-mortality cancers among women in the 21st century. Every year, 1.2 million women around the world suffer from breast cancer and about 0.5 million die of it  Due to being collected by various devices, the resolution of histopathological images extracted may not always be high. Low-resolution (LR) images lack of lots of details, which will have an important impact on doctors' diagnosis. Considering the improvement of histopathological images' acquisition equipment will cost lots of money while significantly increasing patients' expense of detection. The super-resolution (SR) algorithms that improve the resolution of LR images at a small cost can be a practical solution to assist doctors in diagnosis. At present, most single super-resolution methods only have fixed receptive fields  In recent years, a series of deep learning methods have been proposed to solve the breast cancer histopathological image classification issue by the highresolution (HR) histopathological images.  To tackle the problem of LR breast cancer histopathological images reconstruction and diagnosis, we propose the Single Histopathological Image Super-Resolution Classification network (SHISRCNet) integrating Super-Resolution (SR) and Classification (CF) modules. The main contributions of this paper can be described as follows: (1) In the SR module, we design a new block called Multi-Features Extraction block (MFEblock) as the backbone. MFEblock adopts multi-scale receptive fields to obtain multi-scale features. In order to better fuse multi-scale features, a new fusion method named multi-scale selective fusion (MSF) is used for multi-scale features. These make MFEblock reconstruct LR images into SR images well. (2) The CF module completes the task of image classification by utilizing the SR images. Like SR module, it also needs to extract multi-scale features. The difference is that the CF module can use the method of downsampling to capture multi-scale features. So we combine the multi-scale receptive fields (SKNet)  (3) Through the joint training of these two designed modules, the superresolution and classification of low-resolution histopathological images are integrated into our model. For improving the performance of CF module and reducing the error caused by the reconstructed SR images, we introduce HR images to CF module in the training stage. The experimental results demonstrate that the effects of our method are close to those of SOTA methods that take HR breast cancer histopathological images as inputs."
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,2.0,Methods,This section describes the proposed SHISRCNet. The overall pipeline of the proposed network is shown in Fig. 
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,2.1,Super-Resolution Module,"To better extract and fuse multi-scale features for super-resolution, we propose a new SR network, called SRMFENet. Like SRResNet  where n is the number of atrous convolutions and is set to 4 by the experiments. This design not only preserves the depth of the network, but also increases the width of the network. It is beneficial for the network to extract shallow local texture information and global semantic information. After the feature extraction phase, a new fusion method named MSF fuses all of different scale features Y i . In the end, the input features X are added with the fused features. The details of MSF show in the Fig. "
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,2.2,Classification Module,"The task of the CF module is to classify the reconstructed SR images. It can use the method of downsampling to capture multi-scale features. So we combine the multi-scale receptive fields (SKNet as backbone network) with the FPN (a downsampling method) to achieve the feature extraction of this module. In Fig.  Then, using GAP along the channel dimension to get the global information S. A FC layer generates a compact feature vector Z which guides the feature selection procedure. And Z is reconstructed into two weight vectors a, b of the same dimension as S through two FC layers, which can be defined as: where δ denotes ReLU and W a , W b , W c , means the weight of the FC layers. Specifically, a softmax operator is applied on a and b 's channel-wise digits: The fused feature map F is obtained through the attention weights on multi-scale features:"
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,3.0,Loss Function,"The SR module and the CF module exploit different loss functions for training. In the SR module, L1 Loss is used for super-resolution. In the CF module, we introduce HR images to CF module in the training stage for improving the performance of CF module and reducing the error caused by the reconstructed SR images. We use F ocal Loss  where λ 1 , λ 2 and λ 3 are the weights of L1 Loss, F ocal Loss and NT -Xent Loss, respectively. In the inference stage, only SR images are taken as inputs by CF module. In our experiment, λ 1 , λ 2 and λ 3 are set to 0.6, 0.3 and 0.1, respectively. And the temperature parameter in NT -Xent Loss is set to 0.5."
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,3.0,Experiment,Dataset: This work uses the breast cancer histopathological image database (BreaKHis)
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,4.1,The Results of Super-Resolution and Classification,Table 
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,1.0,Introduction,"Alzheimer's disease (AD) is one of the most common neurological diseases in elderly people, accounting for 50-70% of dementia cases  In recent years, deep learning (DL) based methods  Inspired by the advance in transformers  Our contributions have three folds:  (2) We design a global prompt to adapt to high-dimension MRI data and build a prompt learning framework for transferring knowledge from AD diagnosis to pMCI detection. "
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,2.0,Methodology,"We aim to predict if an MCI patient will remain stable or develop Alzheimer's Disease, and formulate the problem as a binary classification task based on the brain MR image and the corresponding tabular attribute information of an MCI patient from baseline examination. As Fig. "
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,2.1,Network Architecture,"We propose a transformer-based framework for pMCI prediction (VAP-Former) based on visual and attribute data, which is shown in Fig. "
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,2.2,Knowledge Transfer with Multi-modal Prompt Learning,"To effectively transfer the knowledge learned from AD prediction task to the pMCI prediction task, we propose a multi-modal prompt fine-tuning strategy that adds a small number of learnable parameters (i.e., prompt tokens) to the input of the transformer layer and keeps the backbone frozen. The overall pipeline is shown in Fig.  Tabular Context Prompt Learning. In the attribute encoder, we insert prompts into every transformer layer's input  where k is the number of the tabular prompt. The prompt fine-tuning Tabtransformer can be formulated as: where X i ∈ R M ×C denotes the attribute embedding at L i 's output space and P i denotes the attribute prompt at L i+1 's input space concatenated with X i . Visual Context Prompt Learning with Paired Attention. For the visual prompt learning part, we concatenate a small number of prompts with visual embedding to take part in the spatial-wise and channel-wise attention block  •] be the concatenation operation, the SWA and CWA can be formulated as: where I S ∈ R N ×C and P spatial ∈ R P 2 ×C are spatial-wise visual embedding and prompts, and I C ∈ R N ×C and P channel ∈ R P 2 ×C are channel-wise visual embedding and prompts, and P is the number of visual prompt. After that, the initial feature map I is added to the attention feature map using a skip connection. Let + be the element-wise addition, this process is formulated as I = I + (I S + I C )."
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,2.3,Global Prompt for Better Visual Prompt Learning,"Compared to natural images with relatively low dimensionality (e.g., shaped 224 × 224 × 3) and the salient region usually locates in a small part of the image, brain MRIs for diagnosis of Alzheimer's are usually high dimensional (e.g., shaped 144 × 144 × 144) and the cues to diagnosis disease (e.g., cortical atrophy and beta protein deposition) can occupy a large area of the image. Therefore, vanilla prompt learning  (1) the prompt token can influence the global feature; (2) the prompt token can effectively interact with the visual input features. A simple approach to achieve the second goal is to increase the number of prompt tokens so that they can better interact with other input features. However, the experiment proves that this is not feasible. We think it is because too many randomly initialized prompt tokens (i.e., unknown information) will make the model hard to train. Thus, we tailor-design a global prompt token g to achieve the above two goals. Specifically, we apply a linear transformation T to the input prompt tokens P to obtain the global prompt token g (i.e., a vector), which further multiplies the global feature map. Since the vector is directly multiplied with the global feature, we can better find the global feature responses in each layer of the visual network, which enables the model to better focus on some important global features for pMCI diagnosis, such as cortical atrophy. Since this linear transformation, T operation is learnable in the prompt training stage, the original prompt token can better interact with other features through the global token. To embed the global prompt token into our framework, we rewrite Eq. 3 as: where T denotes the linear transformation layer with P × C elements as input and 1×C element as output. Experiments not only demonstrate the effectiveness of the above method but also make the training stage more stable."
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,3.1,Datasets and Implementation Details,The datasets are from Alzheimer's Disease Neuroimaging Initiative (ADNI) 
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,3.2,Comparison with the State-of-the-Art Methods,"To validate the proposed VAP-Former and prompt fine-tuning (PT) strategy, we compare our model with three unimodal baselines: 1) UNETR++, which denotes the encoder of UNETR++  In Table "
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,3.3,Ablation Study and Investigation of Hyper-parameters,"In this section, we study the internal settings and mechanism of the proposed PT strategy. First, we investigate how the prompts affect the VAP-Former performance. So we remove the visual prompts from the VAP-Former (denoted as TabPrompt in Table "
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,4.0,Conclusion,"To detect pMCI with visual and attribute data, we propose a simple but effective transformer-based model, VA-Former, to learn multi-modal representations. Besides, we propose a global prompt-based tuning strategy, which is integrated with the VA-Former to obtain our overall framework VAP-Former. The proposed framework can efficiently transfer the learned knowledge from AD classification to the pMCI prediction task. The experimental results not only show that the VAP-Former performs better than uni-modal models, but also suggest that the VAP-Former with the proposed prompt tuning strategy even surpasses the full fine-tuning while dramatically reducing the tuned parameters."
Visual-Attribute Prompt Learning for Progressive Mild Cognitive Impairment Prediction,,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 53.  removing it from the model (denoted as Vis-TabPrompt). As shown in Table  To investigate the impact of the number of prompts on performance, we evaluate VAP-Former with varying numbers of prompts. Given that the number of visual tokens exceeds that of tabular tokens. We fix the number of tabular prompts at 5 (left plot in Fig. "
Text-Guided Foundation Model Adaptation for Pathological Image Classification,1.0,Introduction,"Deep learning for medical imaging has achieved remarkable progress, leading to a growing body of parameter-tuning strategies  In this study, we propose CITE, a data-efficient adaptation framework that Connects Image and Text Embeddings from foundation models to perform pathological image classification with limited training samples (see Fig.  1. We demonstrate the usefulness of injecting biomedical text knowledge into foundation model adaptation for improved pathological image classification. "
Text-Guided Foundation Model Adaptation for Pathological Image Classification,2.0,Related Work,Medical Image Classification. Deep learning for medical image classification has long relied on training large models from scratch  Vision-Language Pre-training. Recent work has made efforts in pre-training vision-language models. CLIP  Model Adaptation via Prompt Tuning. Prompt tuning proves to be an efficient adaptation method for both vision and language models  Biomedical Language Model Utilization. Biomedical text mining promises to offer the necessary knowledge base in medicine 
Text-Guided Foundation Model Adaptation for Pathological Image Classification,3.0,Methodology,Figure 
Text-Guided Foundation Model Adaptation for Pathological Image Classification,3.1,Connecting Text and Imaging,"An image I to be classified is processed through a pre-trained vision encoder to generate the image embedding x v with dimension d v , where v stands for ""vision"": For the label information, we encode the class names T c (c ∈ [1, C]) with a pre-trained biomedical language model instead of training a classification head (see Fig.  Vision-language models like CLIP  where sim(•, •) refers to cosine similarity and τ is the temperature parameter. For irrelevant vision and language encoders, we introduce an extra projection layer to the end of the vision encoder to map the image embeddings to the same latent space as the text embeddings. We replace x v in Eq. (  During adaptation, the extra parameters are updated by minimizing the cross-entropy of the predictions from Eq. ( "
Text-Guided Foundation Model Adaptation for Pathological Image Classification,3.2,Learning Visual Prompt,"Medical concepts exhibit a great visual distribution shift from natural images, which becomes impractical for a fixed vision encoder to capture task-specific information in few-shot scenarios. Visual prompt tuning (VPT  A vision transformer first cuts the image into a sequence of n patches and projects them to patch embeddings E 0 ∈ R n×dv , where d v represents the visual embedding dimension. A CLS token c 0 ∈ R dv is prepended to the embeddings, together passing through K transformer layers {L k v } k=1,2,...,K . CLS embedding of the last layer output is the image feature x v . Following the setting of shallow VPT, we concatenate the learnable prompt tokens P = [p where [•, •] refers to concatenation along the sequence length dimension, and Z k ∈ R p×dv represents the output embeddings of the k-th transformer layer at the position of the prompts (see Fig. "
Text-Guided Foundation Model Adaptation for Pathological Image Classification,4.0,Experimental Settings,"Dataset. We adopt the PatchGastric  The evaluation metric is patient-wise accuracy, where the prediction of a WSI is obtained by a soft vote over the patches, and accuracy is averaged class-wise. Implementation. We use CLIP ViT-B/16  Our implementation is based on CLIP 1 , HuggingFace Training Details. Prompt length p is set to 1. We resize the images to 224×224 to fit the model and follow the original data pipeline in PatchGastric "
Text-Guided Foundation Model Adaptation for Pathological Image Classification,5.0,Results,"CITE Consistently Outperforms all Baselines Under all Data Scales. Figure  (2) Linear probe: train a classification head while freezing the backbone encoder. (3) Fine-tune: train a classification head together with the backbone encoder. (4) CLAM  For each combination, CITE consistently outperforms linear and fine-tune baselines. CITE Shows Model Extensibility. We evaluate our approach with additional backbones and biomedical language models to assess its potential extensibility. Table "
Text-Guided Foundation Model Adaptation for Pathological Image Classification,6.0,Conclusion,"Adapting powerful foundation models into medical imaging constantly faces data-limited challenges. In this study, we propose CITE, a data-efficient and model-agnostic approach to adapt foundation models for pathological image classification. Our key contribution is to inject meaningful medical domain knowledge to advance pathological image embedding and classification. By tuning only a small number of parameters guided by biomedical text information, our approach effectively learns task-specific information with only limited training samples, while showing strong compatibility with various foundation models. To augment the current pipeline, the use of synthetic pathological images is promising "
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,1.0,Introduction,"Magnetic Resonance Imaging (MRI) has been extensively applied to clinical diagnosis  Artificial Intelligence (AI) plays an increasingly important role in MRI-based diagnosis, for both MR image reconstruction and clinical decision making. Deep Neural Networks (DNN) were trained to reconstruct the MR images from the sub-sampled k-space  In this paper, we introduce a Distributionally Robust Learning (DRL)-based approach "
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,2.1,Distributionally Robust Learning,"We will use the DRL framework under a multi-class classification setting developed by  where is the coefficient matrix, P * is the true distribution of the data (x, y), h B (x, y) log 1 e B x -y B x is the loss function to be minimized, and E P * denotes the expectation under the distribution P * . Since P * is usually unknown, Problem (1) cannot be solved directly. The ERM approach tackles this by replacing the expected true risk by a sample averaged risk. Given N realizations of (x, y), ERM minimizes the following empirical risk ERM can produce unreliable solutions when the samples are contaminated by noise or drawn from an outlying distribution. To obtain robust estimators that can hedge against noise in the training data and generalize well out-of-sample,  where Ω contains a set of probability distributions that are close to the empirical distribution PN measured by the Wasserstein metric, Ω {Q ∈ P(Z) : W 1 (Q, PN ) ≤ }, where Z is the set of possible values for (x, y), P(Z) is the space of all probability distributions supported on Z, is a pre-specified radius of the ambiguity set Ω, PN is the empirical distribution that assigns an equal probability 1/N to each observed sample (x i , y i ), and W 1 (Q, PN ) is the order-1 Wasserstein distance between Q and PN defined as where Π is the joint distribution of z 1 (x 1 , y 1 ) and z 2 (x 2 , y 2 ) with marginals Q and PN , respectively, and l is a distance metric on the data space. An equivalent reformulation (4) of (3) was developed by  , where W is a positive semidefinite weight matrix to account for any transformation on the input feature x and can be estimated from data using metric learning (see Sec. 2.2) and with M → ∞: where"
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,2.2,DRL for Deep Stroke Diagnosis Networks,"We apply DRL to ViT and CNN-based MR image stroke classification models, in order to enhance their robustness against image perturbations in accelerated MRI. We apply the DRL reformulation (4) to the last layer B, and certain intermediate linear layers in a deep MR image classifier. For a ViT model (cf. Fig.  where xi is the perturbed version of an MR image slice x i , D the training set, S {(i, j)|x i , x j ∈ D, y i = y j }, |S| denotes the cardinality of the set S, φ L is the input to L and φ (t) L is the t-th hidden state (i.e., the vector representation for each instance in the sequence, output by and fed into different layers in ViT) in the sequence φ L of length T , and c is a fixed parameter. T = 1 if L refers to the B layer. The intuition of (  We chose two approaches to generate the perturbation in accelerated MRI. Cartesian Undersampling (CU) perturbation "
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,3.1,Experimental Materials and Settings,"Our dataset included MRI brain scans from 226 patients performed at an urban tertiary referral academic medical center that is a comprehensive stroke center. Clinical scans of adult patients aged 18-89 years with recent (acute or subacute) strokes were identified between 1/1/2013 and 1/1/2021 for inclusion in this study via a search of the Philips Performance Bridge. Scans meeting this criteria were downloaded and simultaneously anonymized to preserve patient anonymity and prevent disclosure of protected health information as part of this IRB exempt study. No patient demographic information was retained for the scans, as it was considered to represent an unnecessary risk for accidental release of protected health information. The diffusion weighted images with a gradient of B=1000 were utilized for the analysis (see the Supplement To evaluate the binary classification performance of different models, we use the Area Under the Receiver Operating Characteristic (AUROC) curve as our main metric. As our dataset is unbalanced, we also considered the Area Under Precision-Recall Curve (AUPRC). We ran the experiments 3 times using different random seeds. The training of our DNNs were implemented on 3 NVIDIA RTX A6000 (48GB VRAM) GPUs, and each DRL training epoch can be completed within 0.03 GPU hours. We used a learning rate of 1 × 10 -5 and batchsize of 128 for DRL training, while no weight decay was applied. To solve the LMI problem in (5), we used SDPT3 v4.0 "
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,3.2,Results,We show the stroke classification AUROC in Fig. 
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,4.0,Conclusions,"In this study, we implemented a DRL-based robust learning approach to improve the robustness of deep image classifiers, in order to achieve more accurate stroke classification from brain MR images reconstructed from a sub-sampled k-space. Our work can potentially be applied to accelerate and improve time-critical stroke diagnosis. Future work can apply DRL to more MRI diagnosis tasks (e.g., lesion area segmentation), justifying its effectiveness on more types of subsampling methods in MRI acceleration."
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_74.
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,1.0,Introduction,"Computer-aided diagnosis utilizes machine learning techniques to conduct a pathological diagnosis concerning biomedical imaging data collected from various pathological modalities, such as computed tomography  Due to the difficulty of acquiring the abundant annotated training data, the current SOTA method, i.e., CSM  Similar to the previous MIL-based WVAD methods  To assess the proposed contrastive feature decoupling network, we conduct experiments on two datasets, i.e., Polyp and PANDA-MIL. The main contributions are summarized as follows. -Our contrastive feature decoupling network learns a memory bank to learn normal atoms for decoupling each snippet as normal and diseased feature ingredients as opposite contrastive learning samples. Such a feature decoupling intrinsically fits the contrastive learning paradigm for optimizing MIL objectives on bags and instances. 2 Related Work"
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,2.1,Disease Detection,"With the evolution of artificial intelligence techniques in the past decades, deep learning has shown its potential for computer-aided diagnosis of various symptoms "
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,2.2,Contrastive Learning,"The characteristics of self-supervised learning are defining the proxy objective or addressing pretext tasks using pseudo labels for the unlabeled instances. One popular branch is contrastive learning which shows a remarkable ability to obtain the desired semantic representation from various perspectives. For example, CoLA "
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,3.0,Method,"We aim to design a MIL-based WVAD model for tackling disease detection across different pathological modalities. Our model contains an offline trained memory bank to store feature atoms before the CFD training procedure, which associates a contrastive loss to boost the model performance using decoupled features per instance. Winin the MIL scenario, our model employs two classifiers to enable reasoning of the disease scores at instance and bag levels. Figure "
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,3.1,Memory Bank Construction,"Given dataset D comprising normal sub-dataset D 0 and abnormal sub-dataset D 1 , we first encode all instances per bag B ∈ D into instance-level feature set F = {f t } T t=1 ∈ R T ×C via a pre-trained feature extractor E. That is, F = E(B), T is the number of instances, and C represents the instance-level feature dimension. We then collect all normal instance-level features f ∈ R 1×C from D 0 to learn the memory bank M by using the dictionary learning technique  where D 0 is the normal sub-dataset collected from the training split, w t is the learned weights within the memory bank learning process, and λ is a hyperparameter to constrain the memory bank sparsity."
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,3.2,Contrastive Feature Decoupling,"With the learned normal instance features stored in the memory bank M, we are able to reconstruct a normal version for any given bag-level feature F. Such a normal version is denoted as a normal-like feature F H . To this end, we reconstruct a given F concerning M via the following equation where σ stands for the softmax function; φ q , φ k , and φ v respectively represent the query, key, and value linear projections, as introduced in the self-attention framework  where ω ∈ R 1×C is the weight for reweighting F by channel-wise multiplication. For depicting the degree of disease/abnormal to estimate the channel-wise weight ω, we consider attending to the distant features with respect to the normal ones, i.e., F H . In practice, we estimate the degree of disease/abnormal based on the difference between F and F H by where φ d , G, and Ψ are linear projections, global average pooling, and the multiscale temporal network  where δ is the hyperparameter for constrained margin and {•} K is the operator that selects top-K instances. The other common loss used in the recent MILbased works is the classification loss building upon the binary cross entropy where S = φ I (F D ) represents the instance-level prediction inferred by an instance-level classifier φ I and s = φ B (G((1ω) F H )) means the bag-level prediction resulting from a bag-level classifier φ B ."
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,3.3,Regularization,"Motivated by the recent WVAD methods  By using the decoupled features F H and F D , we are ready to regularize the opposite decoupled features across bags with the aid of a contrastive loss. An expected contrastive loss aims to make our model attract features from the same category while distracting the features from distinct classes. In practice, we formulate such a contrastive loss by where τ denotes the temperature parameter in the normalized temperaturescaled loss. Notice that ( "
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,4.1,Dataset and Metric,We evaluate our model against SOTAs on the existing Polyp  Polyp. This dataset collects colonoscopy videos from Hyper-Kvasir 
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,,PANDA-MIL.,The Prostate cANcer graDe Assessment (PANDA) challenge  Metric. We follow the previous methods 
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,4.2,Implementation Details,"All the evaluated methods in the experiment used the same feature encoder, i.e., I3D "
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,4.3,Comparison Results,Table 
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,4.4,Ablation Study,We analysis on why the CFD network performs better than other methods listed in Table 
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,5.0,Conclusion,"This paper casts disease detection as a MIL-based WVAD task and introduces Contrastive Feature Decoupling (CFD) network to learn a memory bank boosted with contrastive learning. With the learned feature atoms stored in the memory bank, our contrastive feature decoupling is able to decouple each snippet as normal and abnormal proxies. Further, the decoupled abnormal proxies highlight the abnormal feature ingredients for better reasoning the disease score. Our feature decoupling intrinsically fits the contrastive learning paradigm to define opposite training samples for model optimization. Besides, we introduce a new dataset of prostate cancer detection, i.e., PANDA-MIL, to provide a biomedical imaging dataset concerning a different pathological modality. Experiments demonstrate that our CFD network achieves new SOTA performance on the Polyp and PANDA-MIL datasets, indicating that our method effectively addresses the disease detection task across different pathological modalities."
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,1.0,Introduction,"Curriculum learning methods in deep learning are inspired by human education and involve structuring the training data from easy to hard to teach networks progressively. However, developing a good difficulty metric or measurer for curriculum learning is a challenge. Recently, there are two main categories of approaches to designing the difficulty metrics. The first is that human experts quantify data difficulty based on data characteristics such as complexity  Another popular approach is difficulty measurers based on the network, including transfer learning  We propose a new approach to address the challenges of curriculum learning, which we call Dynamic Curriculum Learning via In-Domain Uncertainty (DCLU). Our approach is motivated by two key observations: 1) sample difficulty is influenced by both the complexity of the data and the model's inability to explain data, related to in-domain uncertainty, and 2) reducing in-domain uncertainty by improving the learning process can boost model performance. To estimate in-domain uncertainty, we use a Dirichlet distribution classifier, which provides uncertainty estimates and predictions simultaneously. DCLU then sorts the training data from easy to hard based on in-domain uncertainty estimation, allowing the model to focus on easier samples first. Our approach does not require additional networks to capture uncertainty and is end-to-end. In particular, Our dynamic difficulty measurer (DDM) generates uncertainties and predictions for each image simultaneously. Uncertainties reflect the difficulty and we use these as the criteria for data rearrangement. Uncertainty estimation runs at each iteration to ingest the feedback of the current network. We also propose an effective uncertainty-aware sampling pacing function (UAS) to sort all training data according to the latest results of DDM and gradually introduce progressively harder samples to learn new parameter vectors and update the network until the entire dataset is covered. The full process of the proposed method is shown in Fig. "
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,2.0,Related Work,In-Domain Uncertainty. In-domain uncertainty represents the uncertainty associated with inputs extracted from a data distribution equivalent to the training data distribution  In-Domain Uncertainty Estimate. MC dropout  Curriculum Learning. Bengio et al. 
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,3.1,Overview,"In DCLU, we randomly input data into the dynamic difficulty measurer (DDM) at the first epoch to obtain the uncertainty for each data point. Next, we apply the uncertainty-aware sampling pacing function (UAS) to present the training data to the DDM in order of the ascending uncertainties to synchronously produce predictions and new uncertainty estimates. New uncertainties can be used as a criterion for sorting data in the next iteration. Additionally, our pacing function selects a fraction of easier samples and learns a parameter vector to update the network. With the training process progressing, the proportion of selected samples increases until it eventually comprises the entire dataset. The pseudo-code for our method is given in the Appendix."
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,3.2,Dynamic Difficulty Measurer,"The difficulty measurer is used to measure the difficulty of the data to decide on the order of the training data, which is a crucial component of curriculum learning. Our dynamic difficulty measurer (DDM) is a multi-class classifier with Dirichlet distribution, based on evidential deep learning  where K is the total number of classes. We allocate b k in correspondence to Dirichlet distribution with parameters α k = e k + 1. So, the belief mass and uncertainty can be formulated as Additionally, the Dirichlet distribution of DDM can be defined with parameters α = [α 1 , ..., α K ] as (2): where K k=1 p k = 1 and 0 ≤ p 1 , ..., p K ≤ 1. B(α) is a K-dimensional multinomial beta function  Thus, DDM can generate both predictions and in-domain uncertainty estimates for each sample simultaneously. To be specific, in-domain uncertainty estimates include data and model uncertainty. Data uncertainty cannot be eliminated with training and model uncertainty can be reduced by improving the learning process "
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,3.3,Uncertainty-Aware Sampling Pacing Function,"The pacing function is based on using the difficulty measurer to determine how training examples (data) are fed into the network during the training process. Our dynamic difficulty measurer (DDM) can provide difficulty scores for all data at each iteration. However, some existing pacing functions attempt to partition the dataset into multiple subsets and gradually feed them into the network during training -this fails to satisfy the requirement of our difficulty measurer. To address this problem, we propose the uncertainty-aware sampling pacing function (UAS) consisting of two modules: the reorder and sampling modules. Within the reorder module, UAS sorts all training data from easy to hard according to ascending uncertainties from DDM at the last epoch and sends them into the network to yield predictions and new uncertainty estimates. The sampling module specifies a fraction of easier samples to update to the network. The weight assigned to the selected examples is set to 1, while the weight for other data is set to 0. These weights α will be applied to the objective function, which allows the parameters learned from the specified examples to update the network. We increase the fraction exponentially in each epoch until it eventually comprises the entire dataset. In our work, we implement UAS through two approaches. Firstly, UAS (exponential) incorporates both the reorder and sampling modules, prompting the network to prioritize learning easier examples during the initial stages of training and then gradually learn more difficult examples. In each epoch, UAS (exponential) first utilizes the reorder module to sort all training data from easy to hard. Then, it applies the sampling module to select a fraction of easier examples to update the network. Additionally, UAS (full) only contains the reorder module to enable the network to learn all sorted data, ranging from easy to hard examples."
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,3.4,Loss Function,"Assume D(p k |α k ) is the prior on the cross-entropy loss. The classification loss function for each sample can be formulated by the Bayes risk by (4): where ψ is the digamma function, p ik is the estimated probability of the k-th class of the i-th sample. Additionally, Kullback-Leibler (KL) divergence is used to reduce the total evidence to zero under the condition of incorrect classification, which can be denoted by (  (5) where Γ denotes the gamma function. To achieve pace control and reduce the effects of overfitting due to easier data that may always appear at the beginning of training, the final objective function is: where α i is the weight from the UAS pacing function to control the pace, λ t = min(1, t/50) is the annealing parameter, t is the current training epoch and w is weights of the network."
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,4.1,Dataset and Experimental Setup,Dataset. We evaluated our method on two public medical image datasets including ISIC 2018 Task 3  Implementation Details. We employ ResNet-18 
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,4.2,Experimental Results,Comparison with State-of-the-Art. Our method is compared with various curriculum learning methods: 1) Vanilla samples batches randomly on the entire dataset without any curriculum learning techniques; 2) Fixed curriculum learning (FCL) 
Dynamic Curriculum Learning via In-Domain Uncertainty for Medical Image Classification,5.0,Conclusion,"Our approach, called Dynamic Curriculum Learning via In-Domain Uncertainty (DCLU), introduces a new perspective on curriculum design by utilizing the current stage of the network to estimate the difficulty of data based on its in-domain uncertainty. To support this, we also introduce an uncertainty-aware sampling pacing function that is compatible with our dynamic difficulty measurer. Our experimental results confirm that DCLU is successful in reducing uncertainty, and we demonstrate the effectiveness of our approach on two medical image datasets through extensive experimentation."
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,1.0,Introduction,"Brain functional networks characterize functional interactions of human brain, where brain regions correspond to nodes and functional interactions between brain regions are considered as edges. Brain functional networks are widely utilized to classify brain diseases, including Alzheimer's disease  Recently, Wasserstein distance has attracted broad attention in image processing  -We provide a new approach for investigating the transfer of information between brain regions with sliced Wasserstein distance. -The proposed sliced Wasserstein graph kernel is positive definite and a faster method for comparing brain functional networks. -The proposed sliced Wasserstein graph kernel can improve classification accuracy compared to state-of-the-art graph methods for classifying brain diseases."
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,2.1,Data and Preprocessing,The functional network data used in the experiments are based on three datasets of brain diseases: ADHD
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,2.2,Sliced Wasserstein Graph Kernel,"Sliced Wasserstein graph kernel is used to measure the similarity of paired brain functional networks. In this subsection, we firstly introduce graph sliced Wasserstein distance and then use it to calculate sliced Wasserstein graph kernel. Throughout the paper, we will refer to brain functional network when mentioning the graph, unless noted otherwise. In image processing, computer vision, and graph comparison, many efficient algorithms of machine learning are available in Euclidean space  Let r and c be two probability measures on R d . The Wasserstein distance between r and c is defined as where p ∈ [1, ∞] and Γ (r, c) denotes the set of all transportation plans of r and c. The sliced Wasserstein kernel on Wasserstein distance has been studied in "
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,,Graph Sliced Wasserstein Distance: Given two graphs,"and c Φ(G2) denote probability measures on d-dimensional feature representation of graph G 1 and G 2 . The graph sliced Wasserstein distance between G 1 and G 2 is defined as where Φ denotes the d-dimensional feature projection. r Φ(G1) and c Φ(G2) are the probability measures of graph G 1 and G 2 , respectively, in d-dimensional Euclidean space. The 2-sliced Wasserstein distance is defined as where g θ# r Φ(G1) and g θ# c Φ(G2) are the one-dimensional projections of the measure r Φ(G1) and c Φ(G2) . θ is a one dimensional absolutely continuous positive probability density function. Proof. According to "
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,,Algorithm 1. Compute sliced Wasserstein graph kernel,"Input: Two graphs G1, G2, parameter λ Output: kernel value KSW G(G1, G2) XG 1 ← Φ(G1); %Compute feature representations of graph G1 and G2% XG 2 ← Φ(G2); r ← r(XG 1 ); %Compute probability measure on feature representations% According to the definition of graph sliced Wasserstein distance in Eq.(  Feature Projection: We use Laplacian embedding as a feature projection function to project a graph into a set of points in Euclidean space. Then, we calculate the eigen-decomposition on these embeddings and acquire the Laplacian eigenvalues and eigenvectors. On Laplacian eigenvectors, we construct the points in Euclidean space where each row of Laplacian eigenvectors is a node representation. In Eq.(3), r Φ(G1) and c Φ(G2) are the feature projection of graph G 1 and G 2 by using Laplacian embedding. Assume that is the Laplacian eigenvectors of graph G 1 and G 2 . We reformulate the graph sliced Wasserstein distance as a sum rather than an integral. Following the work of  where i[n] and j[n] are the indices of sorted >. By combining graph sliced Wasserstein distance and feature projection on graphs, we can construct a new graph kernel called sliced Wasserstein graph kernel which can be used to measure the similarity between the paired graphs."
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,,Sliced Wasserstein Graph Kernel: Given two graphs,"and graph sliced Wasserstein distance on them (i.e., D Φ GSW (G 1 , G 2 )). We define the sliced Wasserstein graph (SWG) kernel as Obviously, sliced Wasserstein graph kernel is a case of Laplacian kernel. The procedure of calculating SWG kernel is described in Algorithm 1. According the theorems in  Proof. The sliced Wasserstein graph kernel is the extension of sliced Wasserstein kernel on graphs. According to "
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,2.3,Sliced Wasserstein Graph Kernel Based Learning,"We use the image processing method described in the data preprocessing to analyze the rs-fMRI data for all subjects and create a brain functional network for each subject. In these brain networks, brain regions are represented as nodes, while the functional connections between paired brain regions are represented as edges. After constructing the brain functional networks for all subjects, we compute the sliced Wasserstein graph kernel using Eq.( "
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,3.1,Experimental Setup,"In the experiments, we compare our proposed method with the state-of-the-art graph methods including graph kernels and graph neural networks. Graph kernels include Weisfeiler-Lehman subtree (WL-ST) kernel "
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,3.2,Classification Results,"We compare the proposed SWG kernel with the state-of-the-art graph kernels on three classification tasks, i.e., ADHD vs. NCs, ASD vs. NCs and EMCI vs. NCs classification. Classification performance is evaluated by accuracy (ACC) and area under receiver operating characteristic curve (AUC). The classification results are shown in Table "
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,3.3,Analysis on Wasserstein Distance,The Wasserstein distance was initially proposed to examine mass translocation 
Positive Definite Wasserstein Graph Kernel for Brain Disease Diagnosis,4.0,Conclusion,"In this paper, we propose a sliced Wasserstein graph kernel to measure the similarities between a pair of brain functional networks. We use this graph kernel to develop a classification framework of brain functional network. We perform the classification experiments in the brain functional network data including ADHD, ASD, and EMCI constructed from fMRI data. The results indicate that our proposed method outperforms the existing state-of-the-art graph kernels and graph neural networks in classification tasks. In computational speed, the proposed method is faster than latest graph kernels and graph neural networks."
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,1.0,Introduction,"Medical lesion detection plays an important role in assisting doctors with the interpretation of medical images for disease diagnosing, cancer staging, etc., which can improve efficiency and reduce human errors  Recently, large-scale pre-trained vision-language models (VLMs), by learning the visual concepts in the images through the weak labels from text, have prevailed in natural object detection or visual grounding and shown extraordinary performance. These models, such as GLIP  However, current existing VLMs are mostly based on a single prompt to establish textual and visual alignment. This prompt needs refining to cover all the features of the target as much as possible. Apparently, even a well-designed prompt is not always able to combine all expressive attributes into one sentence without semantic and syntactic ambiguity, e.g., the prompt design for melanoma detection should include numerous kinds of information describing attributes complementing each other, such as shape, color, size, etc  In this work, instead of striving to design a single satisfying prompt, we aim to take advantage of pre-trained VLMs in a more flexible way with the form of multiple prompts, where each prompt can elicit respective knowledge from the model which can then be fused for better lesion detection performance. To achieve this, we propose an ensemble guided fusion approach derived from clustering ensemble learning  We evaluate the proposed approach on a broad range of public medical datasets across different modalities including photography images for skin lesion detection ISIC 2016 "
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,2.0,Related Work,"Object Detection and Vision-Language Models. In the vision-language field, phrase grounding can be regarded as another solution for object detection apart from conventional R-CNNs  Ensemble Learning. As pointed out by a review "
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,3.0,Method,"In this section, we first briefly introduce the vision-language model for unifying object detection as phrase grounding, e.g., GLIP "
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,3.1,Preliminaries,"Phrase grounding is the task of identifying the fine-grained correspondence between phrases in a sentence and objects in an image. The GLIP model takes as input an image I and a text prompt p that describes all the M candidate categories for the target objects. Both inputs will go through specific encoders Enc I and Enc T to obtain unaligned representations. Then, GLIP uses a grounding module to align image boxes with corresponding phrases in the text prompt. The whole process can be formulated as follows: where O ∈ R N ×d , P ∈ R M ×d denote the image and text features respectively for N candidate region proposals and M target objects, S ground ∈ R N ×M represents the cross-modal alignment scores, and T ∈ {0, 1} N ×M is the target matrix."
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,3.2,Language Syntax Based Prompt Fusion,"As mentioned above, it is difficult for a single prompt input structure such as GLIP to cover all necessary descriptions even through careful designation of the prompt. Therefore, we propose to use multiple prompts instead of a single prompt for thorough and improved grounding. However, it is challenging to combine the grounding results from multiple prompts since manual integration is subjective, ineffective, and lacks uniform standards. Here, we take the first step to fuse the multiple prompts at the prompt level. We achieve this by extracting and fusing the prefixes and suffixes of each prompt based on language conventions and grammar rules. As shown in Fig.  (2)"
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,3.3,Ensemble Learning Based Fusion,"Although the syntax based fusion approach is simple and sufficient, it is restricted by the form of text descriptions which may cause ambiguity in the fused prompt during processing. Moreover, the fused prompts are normally too long that the model could lose proper attention to the key information, resulting in extremely unstable performance (results shown in Sect. 4.2). Therefore, in this subsection, we further explore fusion approaches based on ensemble learning. More specifically, the VLM outputs a set of candidate region proposals C i for each prompt p i , and these candidates carry more multidimensional information than prompts. We find in our preliminary experiments that direct concatenation of the candidates is not satisfactory and effective, since simply integration hardly screens out the bad predictions. In addition, the candidate, e.g., c ij ∈ C i , carries richer information that can be further utilized, such as central coordinate x j and y j , region size w j and h j , category label, and prediction confidence score. Therefore, we consider step-wise clustering mechanisms using the above information to screen out the implausible candidates based on clustering ensemble learning  Another observation in our preliminary experiments is that most of the candidates distribute near the target if the prompt description matches better with the object. Moreover, the candidate regions of inappropriate size containing too much background or only part of the object should be abandoned directly. As such, we consider clustering the center coordinate (x j , y j ) and region size (w j , h j ) respectively to filter out those candidates with the wrong location and size. This step-wise clustering with the aid of different features embodies a typical ensemble learning idea. Therefore, we propose a method called Ensemble Guided Fusion based on semi-clustering ensemble, as detailed in Fig.  The remaining candidates are then transferred to the integration module for being integrated into the final fused result C fuse that is mutually independent: Besides, we also propose three fusion strategies to recluster candidates in different ways before executing ensemble guided fusion, i.e., fusing the multiple prompts equally, by category, and by attribute. Compared to the first strategy, fusing by category and by attribute both have an additional step of reorgnization. Candidates whose prompts belong to the same category or have identical attributes will share the similar distribution. Accordingly, we rearrange these candidates C i into a new set C for the subsequent fusion process."
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,4.1,Experimental Settings,We collect three public medical image datasets across various modalities including skin lesion detection dataset ISIC 2016 
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,4.2,Results,"This section demonstrates that our proposed ensemble guided fusion approach can effectively benefit the model's performance. The Proposed Approach Achieves the Best Performance in Zero-Shot Lesion Detection Compared to Baselines. To confirm the validity of our method, we conduct extensive experiments under the zero-shot setting and include a series of fusion baselines: Concatenation, Non-Maximum Suppression (NMS)  Fine-Tuned Models Can Further Improve the Detection Performance. We conduct 10-shot fine-tuning experiments as a complement, and find the performance greatly improved. As shown in Table  Visualizations. Figure  Comparison Among Fusion Strategies. In this work, we not only provide various solutions to fuse multiple prompts but also propose three fusion strategies to validate the generalization of ensemble-guided fusion. As shown in Table  Ablation Study. As shown in Table "
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,5.0,Conclusion,"In this paper, we propose an ensemble guided fusion approach to leverage multiple text descriptions when tackling the zero-shot medical lesion detection based on vision-language models and conduct extensive experiments to demonstrate the effectiveness of our approach. Compared to a single prompt that typically requires exhaustive engineering and designation, the multiple medical prompts provide a flexible way of covering all key information that help with lesion detection. We also present several fusion strategies for better exploiting the relationship among multiple prompts. One limitation of our method is that it requires diverse prompts for effective clustering of the candidates. However, with the help of other prompt engineering methods, the limitation can be relatively alleviated."
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_28.
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,1.0,Introduction,Weakly supervised anomaly detection holds significant potential in real-world clinical applications 
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,,Fig. 1. Non-Markovian Diffusion Framework:,"To enhance information transfer, we make an assumption that the forward states and reverse states at the same time step t follow different distributions. Based on this assumption, we introduce conditions in the forward process, where all previous states are used as conditions for the current state. This enables efficient information transfer between states. And the final state xT incorporates comprehensive information from the forward states. Similarly, the reverse process incorporates information from previous reverse states. Moreover, traditional generative models, such as GANs "
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,2.0,Method,"In this section, we present a fast non-Markovian diffusion model that utilizes pixel-wise strong conditions and encoding/sampling accelerator for anomaly segmentation to enhance generation fidelity and sampling speed. Section 2.1 introduces the non-Markonvian model and hybrid conditions for guided sampling. Section 2.2 proposes the acceleration approach for encoding and sampling."
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,2.1,Non-Markovian Diffusion Model with Hybrid Condition,"Learning deterministic mappings between diseased and healthy samples sharing the same anatomical structures is essential to enhance inaccurate and timeconsuming diffusion-based approaches, which require strong guidance during sampling. However, current diffusion-based models only provide insufficient conditions (such as binary classification results), leading to vague anomaly distributions. To achieve consistent and stable generation, we propose a hybrid conditional diffusion model dependent on the non-Markovian assumption. It injects noise into the original distribution sequentially using the Gaussian distribution and then reconstructs the original distribution by reverse sampling. Following the expression of  where the discrete states {x t } T t=0 are from step 0 to T , forward step q and trained reverse step p θ have one-to-one mapping. Denoting {α t } T t=0 and {σ t } T t=0 as variance scales for noise perturbation, the Gaussian transition kernels are: To keep the anatomical information across states, the proposed non-Markovian anatomy structure mappings are built by adding previous-state information into forward and reverse states, which preserves distribution prior for high-quality reconstruction. Denoting all accumulated states from the forward process as c and the state in the backward step t as xt , we formulate the Generalized Non-Markovian Diffusion Framework (GNDF) as: Similar to vanilla DDPM  To enable the diffusion model to effectively differentiate between anatomical and anomaly information from previous states, we introduce a hybrid condition that includes the input state x 0 , coarse segmentation maps, and classifier gradients derived from healthy labels. In order to simplify the computational complexity and leverage the rich information contained in x 0 , we replace the forward state conditions c with the original state x 0 . Regarding hybrid condition implementation, we train a binary classifier with healthy and diseased images to provide further guidance on anomaly regions independently, following class-conditional methods  where x t is the concatenation of xt , x n 0 , and x seg . y denotes the corresponding binary label for each data. ω(t) is a weighted function for providing dynamic weights to explore density regions. In this work, we simply set ω(t) as 1. The full framework of our model is shown in Fig. "
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,2.2,Accelerated Encoding and Sampling,"Diffusion acceleration, which is equivalent to reconstruction error minimization  Following the setting of continuous-time Ito stochastic differential equation, where the drift coefficient, the diffusion coefficient, and the Wiener process are denoted as f (x, t), g(t), w, we have our forward process and probability flow ODE sampling  By decomposing the conditional sampling scheme according to Bayes Theorem, the guided diffusion process can be achieved by mixture guidance composed of conditional noise prediction model and classifier gradient: Denote the classifier as C, the binary label as y, the signal-to-noise coefficient as λ, and the conditional noise prediction network as θ x , λ, y , we further have the hybrid conditional diffusion network ˆ θ as: ˆ θ (x , λ, y) := θ (x , λ, y) + s • C(x t , t, y). (8) The semi-linear probability flow ODE is solved reversely with second-order multi-step numerical methods  Finally, we post-process the reconstructed samples by subtracting original inputs and performing Otsu's threshold to obtain the anomaly segmentation map. "
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,3.1,Dataset and Evaluation Metric,BRATS 2020 
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,3.2,Implementation Details,"To fairly compare with previous state-of-the-art diffusion models, we use the same network architectures as DiffANO "
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,3.3,Comparison with State-of-the-Art Methods,"To compare the performance, we choose the anomaly detection methods including memory-based methods (such as PatchCore "
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,3.4,Ablation Study,We conduct ablation studies for hybrid conditions and the steps of encoding and sampling procedures. From Table 
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,4.0,Conclusion and Discussion,"We propose a Fast Non-Markovian Diffusion Model (FNDM) for weakly supervised anomaly detection. FNDM first encodes the images into noisy ones, then applies hybrid conditional generation to reconstructed original images without anomalies. FNDM achieves high-fidelity generation on weak labels by leveraging non-Markovian modeling and pixel-wise hybrid conditions. Besides, FNDM conducts ODE fast solver for encoding and sampling to reach 6-time acceleration. Extensive experiments on two brain datasets reveal the effectiveness and superiority of our approach for anomaly detection. The limitation of our method is that, as a diffusion-based method, it still needs more evaluation steps than GANs. In the future, we could investigate the knowledge distillation techniques to further reduce the sampling steps and apply FNDM in other modalities."
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis,1.0,Introduction,"Computer-aided diagnosis systems have become a prominent tool in medical diagnosis. Yet, their adoption is limited by the need for large amounts of annotated data for training, which hinders their scalability and adaptability to new clinical findings  Inspired by the success of using large language models to predict image descriptors in natural images  We evaluate Xplainer on two chest X-ray datasets, CheXpert "
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis,2.1,Model Overview,"We propose Xplainer, an explainable zero-shot classification-by-description approach for diagnosing pathologies from X-Ray scans. Given an image i and a list of clinical observations o p1-n per pathology p, the goal is to make a multi-label prediction indicating the diagnosis for the patient. Our zero-shot approach leverages the alignment of image and text embeddings provided by contrastive language-image pretraining (CLIP)  We repeat this process for all pathologies we want to diagnose in the image. As the prediction of a pathology diagnosis is directly extracted from the observation probabilities, our method is explainable by design, producing a diagnosis prediction and the detected X-ray observations leading to that prediction. Moreover, the observation probabilities show which observations the model mainly considers for its diagnosis. Figure  To integrate multiple images of one patient, we calculate positive and negative observation probabilities for each image and average them before calculating the pathology probability."
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis,2.2,Prompt Engineering,"Successful zero-shot inference relies on a good alignment between the contrastive pretraining and the downstream task  Radiology reports often include both presence and absence of particular observations. When comparing a prompt with an image embedding, it is hard for the model to differentiate between an observation's positive and negative occurrence, as their formulation can be very similar. Previous work "
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis,3.0,Experiments and Results,"We evaluate Xplainer in a zero-shot setting on the commonly used chest X-ray datasets, CheXpert  Additionally, we compare the initial ChatGPT output to our refined prompts (Table  For the ""No Finding"" class, we compare to either define specific prompts such as ""Clear lung fields"" or ""Normal heart size and shape"" to classify ""No Finding"" or model it as the absence of all of the other 13 labels (Rule-based).  shows that a rule-based modeling of this class leads to better results. A reason for this could be that there is no clearly defined list of observations that indicate a healthy X-ray scan, which a radiologist would mention in his report. Lastly, we investigate different image aggregation methods for pathology prediction. We compare only using a single frontal view X-ray to using all images available for a patient. For aggregation, we compute positive and negative observation probabilities for every image. In Max aggregation, we then use the highest observation probability. The intuition behind this approach is that an observation might be seen much better from one perspective than another, and then only the perspective where the model is most confident should be used. On the other hand, different views give different insights about which kind of observation a visual cue on the image indicates. To leverage this multi-view information, we test Mean aggregation, where all observation probabilities are averaged over multiple images. The results shown in Table  Qualitative Results. Figure  Discussion. One downside of modeling a joint probability is that it assumes that all descriptors appear simultaneously and gives all descriptors the same Fig.  The use of descriptors in Xplainer provides a flexible and adaptive approach to automated diagnosis prediction. By identifying and classifying the presence of descriptive observations, our model can capture the underlying characteristics of a disease without relying on labeled data. This means that our system can easily adapt to new settings with different clinical findings, including new conditions where the symptoms are known, but there is no training data available yet. Additionally, using descriptors allows for adapting the system to specific populations, where the essential descriptors can differ. This is because the model is not constrained by pre-defined labels but rather by the meaningful underlying features of a given diagnosis."
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis,4.0,Conclusion,"In this work, we present a novel and effective zero-shot approach for chest X-ray diagnosis prediction, which provides an explanation for the model's decision. We leverage BioVil, a pretrained, domain-specific CLIP model, and use contrastive observation-based prompting to make predictions without label supervision. Our approach significantly outperforms previous zero-shot methods on CheXpert and Chest-Xray14, showcasing the effectiveness of our approach. Furthermore, we show that designing informative prompts is crucial to improve model performance. Our ablation studies demonstrate that adding disease indication and report style formulation to observation-based prompts notably enhances performance, underscoring the importance of aligning prompts with the domainspecific language used in medical reports. Additionally, contrastive prompts significantly boost performance, suggesting that the model can benefit from explicitly contrasting positive and negative examples. Our work highlights the potential of contrastive pretraining combined with observation-based prompting as a promising avenue for zero-shot medical image classification, where labeled data is scarce or expensive to obtain, and explainability is vital. We envision that our approach can be extended to other medical imaging domains and have practical applications in real-world scenarios. Our findings contribute to the growing body of research to improve the accuracy and interpretability of medical image diagnosis."
Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_41.
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,1.0,Introduction,"With emerging evidence from various post-mortem and in vivo tau PET imaging studies, the existence of several well-recognized atypical patterns of neurofibrillary tangle distribution in subsets of patients and abnormal clinical presentations challenge  In contrast to many previous studies  To address these limitations, this paper presents a novel Reeb graph representation that encodes the topography of tau pathology from PET imaging, and a directed-graph-based framework for uncovering spatiotemporal heterogeneity from cross-sectional tau PET data. More specifically, we first generate a pattern representation from Reeb graph analysis on cortical surfaces to encode the tau pathology pattern. Secondly, the spatial coherence and temporal consistency of tau spreading patterns across subjects are combined within a directed graph for clustering, which is robust to sample sizes and intensity deviations. With largescale imaging data from the ADNI and A4 studies, we obtained three subtypes with systematic spatiotemporal variations in tau spreading patterns by utilizing an efficient community detection method on graphs. We also demonstrate that our method exhibits more robust generalization performance than event-based model on both synthetic and real data."
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,2.1,Reeb Graph Analysis for Pathology Detection,"The reeb graph encodes the topology of pathology on the cortical surface and is used to extract the salient tau pathological patterns. Given a Morse function f on the mesh M, its Reeb graph is defined as follows  In our work, M is a common template surface fsaverage from FreeSurfer  The surface can be partitioned by using the level contours in the neighborhood of these critical points, and the neighboring nodes are connected in the Reeb graph through arcs by applying We further develop a Reeb graph simplification scheme to remove the noisy peaks of the original SUVR function (Fig.  its weight is defined according to its persistence: where A k is area of the patch enclosed by triangles belonging to this edge, and f (C j ) is the peak SUVR value of this patch. To simplify the Reeb graph, we iteratively remove saddle points and spurious edges based on the persistence threshold δ. At each iteration, we scan these saddle points sequentially and for an edge we collapse this edge by removing C i and adding all its connections to C j . The weights of these new edges are updated according to Eq. 1. These steps can be repeated until the persistent threshold is reached. The pruned Reeb graph is illustrated in Fig.  The number of critical points in the pruned Reeb graph is determined by the complexity of SUVR function and the persistence threshold. We set δ = 300 so the pruned Reeb graph typically has less than 10 patches. The edges of the simplified Reeb graph are sets of triangles with topological changes, and the patches enclosed by these triangle sets are the regions with salient tau pathology."
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,2.2,Directed Graph Construction for Spatiotemporal Subtyping,"The Reeb graph patches of different subjects typically have distinct shapes, which makes patch matching an essential step for pattern comparison. The solution we develop here is to establish patch correspondence between subjects based on their spatial proximity by an assignment algorithm. We define the cost function for matching the patch x i of subject x and the patch y j of subject y as: where dP eak is the distance between the peaks, and dHausdorf f is the Hausdorff distance between two patch sets. Both are calculated based on geodesic distances on the surface. Patch matching is achieved through iterative assignments to accommodate both one-to-one and many-to-one matching. The latter one is illustrated in Fig.  The spatiotemporal similarity of tau pathology implies spatial coherence and temporal consistency, and the directed graph can be used to encode both the similarity as the edge weight and the disease staging as edge direction. The temporal similarity is derived from the consistency of the pathology occurring orders based on the correspondences. Based on the assumption that tau accumulation is a monotonic process, we sort the Reeb graph patches y i in a peak SUVR descending order, and this sequence implies the patch occurring order. When comparing with subject x, patches x i are arranged according to their correspondences y i . For patches of x having the same correspondence in y, they are sorted based on their own peak SUVR values decreasingly. The temporal crossings between the two sequences as indicated in Fig.  where SU V R xi is the peak SUVR of x i , and P atchS xi is the patch size. The spatial coherence can be estimated from the spatial deviations caused by the unpaired patches. An unpaired penalty is defined as: The crossing and unpaired penalties are combined to define a distance matrix: where D is weighted combination of two penaty terms by α and β. For the construction of a dense graph, we transform D into a similarity matrix S = 1/(1 + D) as the weights between each pair of subjects. To maintain sparsity and form clusters, we primarily focus on subjects exhibiting similar patterns and adopt a K-nearest-neighbor(KNN) approach to keep only the top K-relevant connections for each subject. Disease staging is determined based on the assumption that later-stage subjects typically have more widespread distribution of tau pathology and/or higher peak intensities. To generate directionality between subjects in our graph representation, we calculate an unnormalized unpaired penalty as Dunpair x,y = Dunpair x,y × max xi SU V R xi because it is composed of both the spreading size and SUVR values. Specifically, if Dunpair x,y > Dunpair y,x , then subject x is considered to be in a later stage than y because of having more tau pathology and higher SUVR values, so the edge direction is y → x. By applying the directions to the KNN graph, we get a directed graph for representing the spatiotemporal relationships between subjects as illustrated in Fig. "
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,3.0,Experiments and Results,"In the current work, we choose the unpaired costs c 1 = 300 and c i = 500(i > 1) in linear assignment of patches, the weights for distance matrix definition as α = 2 and β = 1, and K = 10 for KNN-based graph construction. For Louvain community detection on directed graphs, we set its resolution parameter γ = 0.3. Beside clustering based on the directed graph from training data, we can apply the trained model from our method to estimate the subtype of a validation/test set via major voting by the K most similar training samples. For the event-based SuStaIn method "
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,3.1,Synthetic Experiments,"Synthetic Data. The synthetic tau SUVR data is generated by adding simulated values derived from a Gaussian mixture model to an SUVR template (SU V R tem ), which is calculated as the mean of normal controls, as follows: where f is a Gaussian distribution, x k is the distance vector to seed k, and the standard deviation σ measures the spreading size. The magnitudes of the seeds a k are sampled from the ranges shown in Fig. "
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,,(b).,"A training set and four test sets with different magnitude ranges and seeds randomly sampled from temporal, parietal and frontal lobe, respectively, were generated. Each set has three subtypes with distinct tau propagation orders as shown in Fig.  Synthetic Results. The performances are measured by accuracy and shown in Fig. "
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,3.2,Real Data Experiments,"Real Dataset. 706 tau PET scans from Alzheimer's Disease Neuroimaging Initiative (ADNI) (adni.loni.usc.edu) and Anti-Amyloid Treatment in Asymptomatic Alzheimer's (A4)  Subtyping A+T+ Subjects. Using A+T+ data, we obtained three pathologically different subtypes for both methods (Fig. "
Uncovering Heterogeneity in Alzheimer’s Disease from Graphical Modeling of the Tau Spatiotemporal Topography,4.0,Conclusions,"In the current study, we proposed a novel directed-graph-based framework with a new spatiotemporal pattern representation for parsing tau pathology heterogeneity and demonstrated its improved performance over the state-of-art SuStaIn method. Application of the proposed method on large-scale tau PET imaging datasets successfully demonstrated three subtypes with clear relevance to previously well-described clinical subtypes with distinct spatiotemporal patterns."
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,1.0,Introduction,"Renal cancer is the most lethal malignant tumor of the urinary system, and the incidence is steadily rising  To improve diagnostic efficiency and accuracy, many computational methods were proposed to analyze renal US images and could assist radiologists in making clinical decisions  In practice, experienced radiologists usually utilize dynamic information on tumors' blood supply in CEUS videos to make diagnoses  In this work, we propose a novel multi-modal US video fusion network (MUVF-YOLOX) based on CEUS videos for renal tumor diagnosis. Our main contributions are fourfold. "
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,2.1,Overview of Framework,The proposed MUVF-YOLOX framework is shown in Fig. 
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,2.2,Dual-Attention Strategy for Multimodal Fusion,"Using complementary information between multi-modal data can greatly improve the precision of detection. Therefore, we propose a novel AMF module to fuse the features of different modalities. As shown in Fig.  Taking the B-mode as an example, we first map the B-mode features F B and the CEUS-mode features ) using linear projection. Then cross-attention uses scaled dot-product to calculate the similarity between Q B and K C . The similarity is used to weight V C . Crossattention extracts modality-invariant features through correlation calculation but ignores modality-specific features in individual modalities. Therefore, we apply self-attention in parallel to highlight these features. The self-attention calculates the similarity between Q B and K B and then uses the similarity to weight V B . Similarly, the features of the CEUS modality go through the same process in parallel. Finally, we merge the two cross-attention outputs by addition since they are both invariant features of two modalities and concatenate the obtained sum and the outputs of the two self-attention blocks. The process mentioned above can be formulated as follows: where, F invar represents the modality-invariant features. F B-spec and F C-spec represent the modal-specific features of B-mode and CEUS-mode respectively. F AM F is the output of the AMF module."
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,2.3,Video-Level Decision Generation,"In clinical practice, the dynamic changes in US videos provide useful information for radiologists to make diagnoses. Therefore, we design an OTA module that aggregates single-frame renal tumor detection results in temporal dimension for diagnosing tumors as benign and malignant. First, we utilize a feature selection module  After temporal feature aggregation, F temp is fed into a multilayer perceptron head to predict the class of tumor. 3 Experimental Results"
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,3.1,Materials and Implementations,"We collect a renal tumor US dataset of 179 cases from two medical centers, which is split into the training and validation sets. We further collect 36 cases from the two medical centers mentioned above (14 benign cases) and another center (Fujian Provincial Hospital, 22 malignant cases) to form the test set. Each case has a video with simultaneous imaging of B-mode and CEUS-mode. Some examples of the images are shown in Fig.  Weights pre-trained from ImageNet are used to initialize the Swin-Transformer backbone. Data augmentation strategies are applied synchronously to B-mode and CEUS-mode images for all experiments, including random rotation, mosaic, mixup, and so on. All models are trained for 150 epochs. The batch size is set to 2. We use the SGD optimizer with a learning rate of 0.0025. The weight decay is set to 0.0005 and the momentum is set to 0.9. In the test phase, we use the weights of the best model in validation to make predictions. All Experiments are implemented in PyTorch with an NVIDIA RTX A6000 GPU. AP 50 and AP 75 are used to assess the performance of single-frame detection. Accuracy and F1-score are used to evaluate the video-based tumor diagnosis.  "
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,3.2,Ablation Study,Single-Frame Detection. We explore the impact of different backbones in YOLOX and different ways of multi-modal fusion. As shown in Table  Table 
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,3.3,Comparison with Other Methods,The comparison results are shown in Table 
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,4.0,Conclusions,"In this paper, we create the first multi-modal CEUS video dataset and propose a novel attention-based multi-modal video fusion framework for renal tumor diagnosis using B-mode and CEUS-mode US videos. It encourages interactions between different modalities via a weight-sharing dual-branch backbone and automatically captures the modality-invariant and modality-specific information by the AMF module. It also utilizes a portable OTA module to aggregate information in the temporal dimension of videos, making video-level decisions. The design of the AMF module and OTA module is plug-and-play and could be applied to other multi-modal video tasks. The experimental results show that the proposed method outperforms single-modal, single-frame, and other stateof-the-art multi-modal approaches."
Visual Grounding of Whole Radiology Reports for 3D CT Images,1.0,Introduction,"In recent years, a number of medical image recognition systems have been developed  2) Long and complex sentences: Radiology reports on X-ray images are often simple, noting only the presence or absence of anomalies. On the other hand, in CT examinations, the qualitative diagnosis of each anomaly is often performed. In cases, multiple anomalies are simultaneously described in a sentence. Therefore, the description tend to be long and complicated with multiple sentences (Fig.  In this work, we propose a novel visual grounding framework for 3D CT images and radiology reports. The main idea is to separate the task into three parts: 1) anatomical segmentation on images, 2) report structuring, and 3) localization of described anomalies. In the anatomical segmentation, multiple organs and tissues are extracted using the deep learning based segmentation model and provided as landmarks. The report structuring model, which is based on BERT  Our contributions are as follows: -We show the first visual grounding results for 3D CT images that covers various body parts and anomalies. -We introduce a novel grounding architecture that can leverage report structuring results of presence/type/location of described anomalies. -We validate the efficacy of the proposed framework using a large-scale dataset with region-description correspondence annotations."
Visual Grounding of Whole Radiology Reports for 3D CT Images,2.0,Related Work,Visual Grounding. Visual grounding task involves learning the correspondences between descriptions in the text and image regions from a given training set of region-description pairs  Vision-Language Tasks on Medical Image. The existence of public datasets with paired images and reports 
Visual Grounding of Whole Radiology Reports for 3D CT Images,3.0,Methods,"We first formulate the problem. Next, we explain three key components of anatomical segmentation, report structuring, and anomaly localization in our framework. In our framework, multiple organ labels obtained as the output of anatomical segmentation encourage the grounding model to learn detailed anatomy, and report structuring allows the grounding model to accurately extract the features of the target anomaly from complex sentences."
Visual Grounding of Whole Radiology Reports for 3D CT Images,3.1,Problem Formulation,Our research assumes that a dataset of image-report pairs with regiondescription correspondence annotations is provided for training. We show the overall framework in Fig.  Fig. 
Visual Grounding of Whole Radiology Reports for 3D CT Images,3.2,Anatomical Segmentation,"The task of the anatomical segmentation is to extract relevant anatomies that can be clues for visual grounding. We use the commercial version of the 3D image analysis software (Synapse 3D V6.8, FUJIFILM corporation, Japan) to extract 32 organs and tissues (See Appendix Table. A1). In this software, anatomies are extracted using U-Net based architectures "
Visual Grounding of Whole Radiology Reports for 3D CT Images,3.3,Report Structuring,"The tasks of the report structuring are as follows: 1) anatomical prediction, 2) phrase recognition, and 3) relationship estimation between phrases (See Appendix Fig. "
Visual Grounding of Whole Radiology Reports for 3D CT Images,3.4,Anomaly Localization,"The task of the anomaly localization is to output a localization map of the anomaly mentioned in the input report T . The CT image I and the organ label image I a are concatenated along the channel dimension and encoded by a convolutional backbone to generate a visual embedding V . The sentences in the report T are encoded by BERT  where The overall architecture of this module is illustrated in Appendix Fig.  Anomaly-Wise Feature Aggregator. The results of the report structuring m ti ∈ R NC are defined as follows: where c j is the class index labeled by the phrase recognition module (Let C be the number of classes). In this module, aggregate character-wise embeddings based on the following formula. e k = {r j |m tij = k} (4) L ti = LSTM([v organ ; p 1 ; e 1 ; p 2 ; e 2 ; ..., p C ; e C ]) where v organ and p k are trainable embeddings for each organ and each class label respectively. [•; •] stands for concatenation operation. In this way, embeddings of characters related to the anomaly t i are aggregated and concatenated. Subsequently, representative embeddings of the anomaly are generated by an LSTM layer. In the task of visual grounding focused on 3D CT images, the size of the dataset that can be created is relatively small. Considering this limitation, we use an LSTM layer with strong inductive bias to achieve high generalization performance."
Visual Grounding of Whole Radiology Reports for 3D CT Images,4.1,Clinical Data,"We retrospectively collected 10,410 CT studies (11,163 volumes/7,321 unique patients) and 671,691 radiology reports from one university hospital in Japan. We assigned a bounding box to each anomaly described in the reports as shown in Appendix Fig. "
Visual Grounding of Whole Radiology Reports for 3D CT Images,4.2,Implementation Details,"We use a VGG-like network as Image Encoder, with 15 3D-convolutional layers and 3 max pooling layers. For training, the voxel spacings in all three dimensions are normalized to 1.0 mm. CT values are linearly normalized to obtain a value of [0-1]. The anatomy label image, in which only one label is assigned to each voxel, is also normalized to the value [0-1], and the CT image and the label image are concatenated along the channel dimension. As our Text Encoder, we use a BERT with 12 transformer encoder layers, each with hidden dimension of 768 and 12 heads in the multi-head attention. At first, we pre-train the BERT using 6.7M sentences extracted from the reports in a Masked Language Model task. Then we train the whole architecture jointly using dice loss "
Visual Grounding of Whole Radiology Reports for 3D CT Images,5.0,Experiments,We did two kinds of experiments for comparison and ablation studies. The comparison study was made against TransVG 
Visual Grounding of Whole Radiology Reports for 3D CT Images,5.1,Evaluation Metrics,"We report segmentation performance using Dice score, mean intersection over union (mIoU), and the grounding accuracy. The output masks are thresholded to compute mIoU and grounding accuracy score. The mIoU is defined as an average IoU over the thresholds [0.1, 0.2, 0.3, 0.4, 0.5]. The grounding accuracy is defined as the percentage of anomalies for which the IoU exceeds 0.1 under the threshold 0.1."
Visual Grounding of Whole Radiology Reports for 3D CT Images,5.2,Results,"The experimental results of the two studies are shown in Table . 1. Both of MDETR and TransVG failed to achieve stable grounding in this task. A main difference between these models and our baseline model is using a source-target attention layer instead of the transformer. It is known that a transformer-based algorithm with many parameters and no strong inductive bias is difficult to generalize with such a relatively limited number of training data. For this reason, the baseline model achieved a much higher accuracy than the comparison methods. The ablation study showed that the anatomical segmentation and the report structuring can improve the performance. In Fig.  The grounding performance for each combination of organ and anomaly type is shown in Fig. "
Visual Grounding of Whole Radiology Reports for 3D CT Images,6.0,Conclusion,"In this paper, we proposed the first visual grounding framework for 3D CT images and reports. To deal with various type of anomalies throughout the body and complex reports, we introduced a new approach using anatomical recognition results and report structuring results. The experiments showed the effectiveness of our approach and achieved higher performance compared to prior techniques. However, in clinical practice, radiologists write reports from comparing multiple images such as time-series images, or multi-phase scans. Realizing such sophisticated diagnose process by a visual grounding model will be a future research."
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,1.0,Introduction,"Pathological image analysis is a vital area of research within medical image analysis, focused on utilizing computer technology to aid doctors in diagnosing and treating diseases by analyzing pathological tissue slide images  Specifically, in clinical problems of pathological image analysis, doctors usually summarize patient-level labels based on slide labels as the diagnostic results  There are generally two methods to solve the ML-MIL problem. The first method is to directly average the prediction values of slides or take the maximum prediction value  To address the multi-level multi-instance learning (ML-MIL) problem in medical field, we propose a novel framework called Patients and Slides are Equal (P&SrE). Inspired by the iterative labeling process in medical diagnosis, this framework treats patients and slides as instances at the same level and uses transformers and attention mechanisms to build connections between them. This simple yet effective method allows for interaction between patient-level and slidelevel information to correct their respective features and improve classification performance. Our framework consists of two steps: first, at the patch-slide level, a common MIL framework is used to train a MIL neural network and obtain slide-level feature vectors; then, at the slide-patient level, we use self-attention mechanisms to combine the slides of the same patient into patient-level feature vectors, and treat these patient-level feature vectors together with all slide-level feature vectors of the same patient as instances at the same level, which are inputted into transformers for feature interaction and prediction of patient-and slide-level labels. Our method can effectively solve the problem of difficult training due to the scarcity of samples at the highest level in ML-MIL, and can be integrated into two state-of-the-art methods to further improve performance. We conducted rigorous experiments on two datasets and demonstrated the effectiveness of our method. Our contributions include: 1) Proposing a novel general framework to address the unique ""patch-slidepatient"" ML-MIL problem in the medical field. Before this, no other framework had directly tackled this specific problem, making our proposal a ground-breaking step in the application of ML-MIL in healthcare; 2) Proposing a simple yet highly effective method that leverages self-attention mechanisms and transformer models to enhance the interaction between slide and patient information. This innovative approach not only improves the classification performance at the patient level but also at the slide level, showcasing its effectiveness and versatility; 3) Conducting extensive experiments on two separate datasets. Our method was seamlessly integrated with two prior state-of-the-art methods, demonstrating its compatibility and adaptability. The experiments resulted in improved performance, indicating that our method enhances the efficacy of these existing approaches."
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,2.1,Overview,"Our proposed method P&SrE is illustrated in Fig.  For ABMIL, the attention of each patch is computed by an MLP. Specifically, for M j patches p k , an encoder is applied to obtain the patch feature matrix F i , where,F i ∈ R Mj ×1024 . Then, F i is passed through an fc layer followed by a Tanh activation and another fc layer followed by a sigmoid activation to obtain two feature matrices, F i and F i , both ∈ R Mj ×128 . These matrices are elementwise multiplied and then passed through an fc layer to obtain the weight of each patch, ω k . For DSMIL, the attention of each patch is based on the cosine distance between instances and key instances. First, an fc layer is applied to the patch feature matrix F i to obtain the importance score θ k for each patch. The patch with the highest score is selected as the key instance. Then, the feature matrix F i is mapped to a matrix Q i ∈ R Mj ×128 and the cosine similarity between all instances and the key instance is computed as the weight of each patch, ω k . Although ABMIL and DSMIL compute attention differently, both methods compute the attention-weighted sum of patch instances features as the bag representation of the slide. Therefore, the slide feature output by both methods can be generalized as: Finally, we obtain the feature vector set H i ={h j |j=1 to N i } for all slides {s j } of patientX i through patch-slide MIL."
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,2.3,Patient-Slide Level MIL,"After performing patch-slide level MIL, we move on to patient-slide level MIL. In general MIL algorithms, the patient is regarded as the bag and the slide as the instance. However, considering the diagnostic process in clinical practice, we propose to treat both patients and slides as instances at the same level. Specifically, our P&SrE framework for patient-slide level consists of two parts: patient-level feature generation based on self-attention and patient-slide feature interaction based on Transformer  Patient-Level Feature Generation Based on Self-attention. Doctors usually select certain key slides for careful observation and information aggregation during diagnosis, similar to the self-attention mechanism. Therefore, we directly use a fully connected (FC) layer to integrate the feature-level features into patient-level features v i through attention mechanism, serving as patient instances. Specifically, given the feature vector collection {h j } from multiple slides in the previous step, we input it to the FC layer and apply the sigmoid activation function to output the weight α j for each h j . Then, we perform a weighted average of the vectors based on this weight to obtain the patient feature v i : Patient-Slide Feature Interaction Based on Transformer. This process is where our framework shines. After doctors summarize the patient-level results, they typically review the slides to double-check the diagnosis results. This patient-slide feature interaction (PSFI) naturally lends itself to the construction of a Transformer, and information exchange and integration between slides and patient level are bidirectional. Thus, self-attention is more ideal for this purpose than other kinds of attention (such as cross-attention or doctors' attention). By using the self-attention-based transformer structure, each input token is treated equally (i.e., viewed as the same instance level), and tokens can interact extensively with each other, enabling mutual correction between patients and slides and even between slides. Specifically, we merge the slide feature set {h j } and the patient feature v i into the input tokens and then input them into a multi-layer transformer through self-attention and feed-forward neural network layers to obtain the interaction information between slides and output tokens T out i : where d is the dimension of the token, and t k and t l come from T in i . β k,l is multi-head attention matrix, and W Q , W K , and W V are weight matrices of query, key, and value, respectively. W R and W O are transformation matrices. b 1 and b 2 are bias vectors. This update procedure is repeated for L layers, where the t k are fed to the successive transformer layer. Finally, we obtain the output tokens Then, all output tokens are input into a shared FC layer, and the patient's predicted logits Y i and the predicted classification logits {z j |j = 1 to N i } for each slide are output. Training Progress and Loss Function. During training, we sampled one patient at a time and pre-extracted their batch-level features for all slides, in order to save GPU memory. Due to the issue of class imbalance in both slide level and patient level, we use the LADE "
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,3.1,Dataset and Evaluation,"CD-ITB Dataset. CD-ITB is a private dataset consisting of 853 slides from 163 patients, with binary patient-level labels of CD or ITB in a ratio of 103:60 and tri-class slide-level labels of CD, ITB, and normal slides in a ratio of 436:121:296, respectively. On average, there were 5 slides per patient. The slides were scanned at a magnification of 40× (0.25 µm/px), and annotations were curated by experienced pathologists. We adopted a patient-level stratification approach for 5-fold cross-validation, with 20% of the training set randomly assigned as the validation set for each fold. The dataset comprises an average of 2.3k instances per bag, with the largest bag containing over 16k instances. Camelyon17 Dataset. Camelyon17  Metrics. We report class-wise weighted accuracy (Acc), precision(Pre), Recall, and F1-score (F1). To avoid randomness, we run all experiments five times and report the averaged metrics."
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,3.2,Implementation Details,"We utilized ResNet50, which was pre-trained on ImageNet1K, to extract features from patches. Each patch was of size 512 × 512 pixels. For both ABMIL and DSMIL networks, we kept the original parameters for the number of channels at each layer. Following the reference "
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,3.3,Comparisons and Results,"We compared our strategy with two state-of-the-art MIL methods to evaluate its performance. To investigate the impact of self-attention and transformers on slide-level and case-level results, we conducted ablation experiments: ""ABMIL + P&SrE (with/without PSFI)"" and ""DSMIL + P&SrE (with/without PSFI)"", respectively. For slide-level classification, we used mean pooling and max pooling to pool feature vectors of patches into a representative vector for the slide, which was then fed into a fully connected layer for classification. At the patient level, we used two approaches for prediction: MaxS, where the feature of the instance that achieves the maximum positive probability from the slide-level MIL model is selected to patient-level model, and MaxMinS, where the mean value of features of the maximum and minimum positive probability from the slide-level MIL model is selected to patient-level model. The results of 5-fold CV at the slide and patient levels are reported in Table "
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,4.0,Limitations,"Our study has some limitations that should be addressed. For instance, we did not explore the possibility of treating patches as an equivalent level to slides and patients. The primary reason is that the vast number of patches required for analysis is significantly larger than that of slides and patients, which presents a computational challenge for training. As a result, we have not yet explored this avenue. In the future, we plan to leverage clustering and active learning methods to reduce the number of patches and enable the interaction of all three levels with the Transformer, which would further enhance the accuracy and efficiency of our proposed method."
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,5.0,Conclusion,"This study proposes a highly scalable and versatile framework to address M-MIL problems. We first classify the process from patch to slide to the patient in medical pathology diagnosis as a multi-level MIL problem. Based on existing state-of-the-art MIL methods, we then extend the framework to P&SrE, which conducts feature extraction and interaction at the slide-patient level. By introducing a transformer, the framework enables iterative interaction and correction of information between patients and slides, resulting in better performance at both the patient level and slide level compared to existing state-of-the-art algorithms on two validation datasets."
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,1.0,Introduction,"Deep neural networks have recently shown impressive performance on lesion quantification in positron emission tomography (PET) images  Synthesized PET images may exhibit a different data distribution from real clinical images (see Fig.  In this paper, we propose a novel single-stage SDG framework, which learns with human annotation-free, list mode-synthesized PET images for generalizable lesion detection in real clinical data. Compared with domain adaptation and MDG, the proposed method, while more challenging, is quite practical for real applications due to the relatively cheaper NET data collection and annotation. Specifically, we design a new data augmentation module, which generates out-of-domain samples from single-source data with multi-scale random convolutions. We integrate this module into a deep lesion detection neural network and introduce a cross-domain consistency constraint for feature encoding between original synthesized and augmented images. Furthermore, we incorporate a novel patch-based gradient reversal mechanism into the network and accomplish a pretext task of domain classification, which explicitly promotes domain-invariant, generalizable representation learning. Trained with a single-source synthesized dataset, the proposed method provides superior performance of hepatic lesion detection in multiple cross-scanner real clinical PET image datasets, compared with the reference baseline and recent state-of-the-art SDG methods."
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,2.0,Methodology,Figure 
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,2.1,Synthesized Data Augmentation,"In the synthesized PET image dataset, each subject have multiple simulated lesions of varying size with known boundaries  Given a synthesized input image x S ∈ X S , our data augmentation module A first performs a random convolution operation R(x S ) with a k × k kernel R, where the kernel size k and the convolutional weights are randomly sampled from a multi-scale set K = {1, 3, 5, 7} and a normal distribution N (0, 1/k 2 ), respectively. Then, inspired by  where α ∈ [0, 1] is randomly sampled from a uniform distribution U(0, 1). This data mixing strategy allows continuous interpolation between the source domain and a randomly generated out-of-distribution domain to improve model generalizability. Finally, if the foreground (i.e., lesion region) of x M has a higher mean intensity value than the background (non-lesion region), we use x M as the final augmented image, x A = x M . Otherwise, we invert the image intensity of x M to obtain is the maximum/minimum intensity of x M and 1 is a matrix with all elements being one and the same dimension as x M . This intensity inversion operation is to ensure the lesion region has higher intensity values than other regions, mimicking the image characteristics of real-world PET data in our experiments. Here we calculate the mean intensity value of the background from the regions that have a distance greater than half of the image width from the closest lesion center. In our modeling, for each synthesized training image x S , we generate multiple augmented images (i.e., 3), {x i A } 3 i=1 , and feed them into the encoder E for feature learning. Due to the distance preservation property of random convolutions  where E is an expectation operator, |E (x S )| is the number of elements in E (x S ), and || • || F denotes the Frobenius norm. Unlike the previously reported work "
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,2.2,Patch Gradient Reversal,"Because of random convolution weights, the original synthesized X S and augmented X A data can have substantially different image appearances. Consequently, the use of the loss L con in Eq. (1) may not be sufficient to enforce consistent feature encoding. To address this issue, we propose to use a pretext task as an additional information resource for the encoder E and to further promote domain-agnostic representation learning. Specifically, we incorporate a domain classifier C on top of the encoder E to perform a pretext task of domain discrimination, i.e., predict whether each input image is from the original synthesized data X S or augmented data X A . This domain classification accompanies the main task of lesion detection (see Fig.  In general, the classifier C will encourage the encoder E to learn discriminative features for accurate domain classification. In order to make features invariant to different domains, we reverse the gradient propagated from the domain classifier C with a multiplication of -1  Formally, let X = {X S , X A } denote the input data for the encoder E and Z = {Z S , Z A } represent the corresponding domain category labels, with Z S and Z A for the original source images X S and corresponding random convolutionaugmented image X A , respectively. Each label z ∈ Z is a 3D image with all voxel intensity being 0's for z ∈ Z S or 1's for z ∈ Z A . We define the domain classification objective L cls as follows where ẑ = C (E (x )) is the prediction of x . For source-domain data (X S , Y S ), the augmented images X A have the same gold-standard lesion labels Y A = Y S , each of which is a 3D binary image with 1 s for lesion voxels and 0 s for non-lesion regions. Let Y = {Y S , Y A }. We formulate the lesion detection objective L det as where the first and second terms in Eq. (  where λ con and λ cls are weighting parameters. Note that while we minimize L for model training, we reverse the gradient propagated from the domain classifier C before sending it to the encoder E during the backpropagation."
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,3.0,Experiments,"Datasets. We evaluate the proposed method with multiple 68 Ga-DOTATATE PET liver NET image datasets that are acquired using different PET/CT scanners and/or imaging protocols. The synthesized source-domain dataset contains 103 simulated subjects, with an average of 5 lesions and 153 transverse slices per subject. This dataset is synthesized using list mode data from a single real, healthy subject acquired on a GE Discovery MI PET/CT scanner with list mode reconstruction  Comparison with State of the Art. We compare our method with several recent state-of-the-art SDG approaches, including causality-inspired SDG (CISDG)  The qualitative results are provided in the Supplementary Material. Ablation Study. In Table "
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,4.0,Conclusion,"We propose a novel SDG framework that uses only a single dataset for hepatic lesion detection in real clinical PET images, without any human data annotations. With a specific data augmentation module and a new patch-based gradient reversal, the framework can learn domain-invariant representations and generalize to unseen domains. The experiments show that our method outperforms the reference baseline and recent state-of-the-art SDG approaches on cross-scanner or -protocol real PET image datasets. A potential limitation may be the need of a proper selection of weights for different tasks during model training."
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 12.
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,1.0,Introduction,"Identifying unusual patterns in data is of great interest in many applications such as medical diagnosis, industrial defect inspection, or financial fraud detection. Finding anomalies in medical images is especially hard due to large inter-patient variance of normality, the irregular appearance-, and often rare occurrence of diseases. Therefore, it is difficult and expensive to collect large amounts of annotated samples that cover the full abnormality spectrum, with supervised  Reconstruction-based AEs have emerged as a very popular framework for unsupervised anomaly detection and are widely adopted in medical imaging  Much effort has been made in the medical imaging community to improve the limitations of traditional anomaly detection methods, particularly in the context of brain MRI. Apart from the reduced dimensionality of the bottleneck, several other techniques have been introduced to regularize AEs  In this work, we first investigate whether SOTA AEs can learn meaningful representations for anomaly detection. Specifically, we investigate whether AEs can learn the healthy anatomy, i.e., absence of pathology, and generate pseudohealthy reconstructions of abnormal samples on challenging medical anomaly detection tasks. Our findings are that SOTA AEs either do not efficiently constrain the latent space and allow the reconstruction of anomalous patterns, or that the decoder cannot accurately restore images from their latent representation. The imperfect reconstructions yield high residual errors on normal regions (false positives) that can easily overshadow residuals of interest, i.e., pathology  Our manuscript advances the understanding of anomaly detection by providing insights into what AEs learn. In summary, our contributions are: • We broaden the understanding of AEs and highlight their limitations. • We test whether SOTA AEs can learn the training distribution of the healthy population, accurately reconstruct inputs from their latent representation and reliably detect anomalies. • As a solution, we propose MorphAEus, novel deformable AEs that provide pseudo-healthy reconstructions of abnormal samples and drastically reduce false positives, achieving SOTA unsupervised pathology detection."
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,2.0,Background,The widely held popular belief is that AEs can learn the distribution of the training data and identify outliers from inaccurate reconstructions of abnormal samples 
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,2.1,Unsupervised Anomaly Detection: Assumptions,"Let X ⊂ R N be the data space that describes normal instances for a given task. The manifold hypothesis implies that there exists a low-dimensional manifold M ⊂ R D ⊂ X where all the points x ∈ X lie, with D N  Given a set of unlabeled data x 1 , .., x n ∈ X the objective of unsupervised representation learning is to find a function f : R N → R D and its inverse g : R D → R N , such that x ≈ g(f (x)), with the mapping f defining the lowdimensional manifold M. The core assumption of unsupervised anomaly detection is that once such functions f and g are found, the learned manifold M would best describe the normal data samples in X and results in high reconstruction errors for data-points x / ∈ X , that we call anomalous. An anomaly score is therefore usually derived directly from the pixel-wise difference: The nominal and abnormal distributions are considerably separated from each other when x is from a different domain. However, anomalies are often defects in otherwise normal images. In medical imaging, the set X describes the healthy anatomy and the data set X usually contains images with both healthy and pathological regions. The two distributions usually come from the same domain and might overlap considerably. The core assumption is that only the normal structures can be reconstructed from their latent representation very well, with the pathological regions ideally replaced by healthy structures. Therefore x ≈ g(f (x)) ∈ X would represent the healthy synthesis of the abnormal sample x and the residual |x -g(f (x))| would highlight only the abnormal regions."
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,2.2,Auto-Encoders: Challenges,"AEs aim to extract meaningful representations from data, by learning to compress inputs to a lower-dimensional manifold and reconstruct them with minimal error. They use neural networks to learn the functions f and g, often denoted as encoder E θ with parameters θ and decoder D φ parameterized by a set of parameters φ. The embedding z = E(x|θ) is a projection of the input to a lower-dimensional manifold Z, also referred to as the bottleneck or latent representation of x. The standard objective of AEs is finding the set of parameters θ and φ that minimize the residual, with the mean squared error (MSE) being a popular choice for the reconstruction error: In the introduction, we presented the desired properties of AEs for outlier detection, namely i) reconstructions should match the training distribution and ii) decoders have sufficient capacity to accurately restore inputs. Figure "
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,3.0,MorphAEus: Deformable Auto-encoders,"We propose MorphAEus, deformable AEs that learn minimal and sufficient features for anomaly detection, see Fig.  Pseudo-healthy Reconstructions. Given a dataset X = {x 1 , .., x n } we optimize the encoder and decoder with parameters θ, φ to minimize the MSE loss between the input and its reconstruction. For minimality, we propose to use deep AEs constrained to reconstruct only ID samples (see Fig.  with x rec = D φ (E θ (x)), P L(x, x rec ) = l (V GG l (x)-V GG l (x rec )) 2 with V GG l being the output of the l ∈ {1, 6, 11, 20}-th layer of a pre-trained VGG-19 encoder. We have empirically found α = 0.05 to be a good weight to predict perceptually similar reconstructions without compromising pixel-wise accuracy. Local Deformation. Imperfect reconstructions yield high residuals on normal regions which might overshadow the residuals errors associated with anomalous regions. Skip connections  where x morph = x rec • Φ, ψ are the deformation parameters, LNCC is the local normalized cross correlation, • is a spatial transformer and β weights the smoothness constraint on the deformation fields. We opted for the LNCC instead of the MSE to emphasize shape registration and enhance robustness to intensity variations in the inputs and reconstructions. By sharing encoder/decoder parameters, the deformation maps are not only beneficial at inference time, but also guide the training process to learn more accurate features. The full objective is given by the two losses: To ensure a good initialization for the deformation estimation, we introduce the deformation loss after 10 epochs. Deformable registration between normal and pathological samples is in itself an active area of research "
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,4.0,Pathology Detection on Chest X-rays,"In this section, we investigate whether AEs can learn the healthy anatomy, i.e., absence of pathology, and generate pseudo-healthy reconstructions of abnormal chest X-ray images. Pathology detection algorithms are often applied to finding hyper-intense lesions, such as tumors or multiple sclerosis on brain scans. However, it has been shown that thresholding techniques can outperform learningbased methods  Datasets. We use the Covid-19 radiography database on Kaggle  Results. Of the baselines, only adversarially-trained AEs can reconstruct pseudo-healthy images from abnormal samples, as shown in Fig.  Ablation Study: Importance of Morphological Adaptations. We evaluate the effectiveness of individual components of MorphAEus in Fig. "
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,5.0,Discussion,"In this work, we have investigated whether AEs learn meaningful representations to solve pathology detection tasks. We stipulate that AEs should have the desired property of learning the normative distribution (minimality) and producing highly accurate reconstructions of ID samples (sufficiency). We have shown that standard, variational, and recent adversarial AEs generally do not satisfy both conditions, nor are they very suitable for pathology detection tasks where the distribution of normal and abnormal instances highly overlap. In this paper, we introduced MorphAEus, a novel deformable AE that demonstrated notable performance improvement in detecting pathology. We believe our method is adaptable to various anomaly types, and we are eager to extend our research to different anatomies and imaging modalities, building upon promising early experiments. However, it is important to address false positive detection, which could be influenced by unlabelled artifacts like medical devices. Our future work aims to conduct a thorough analysis of false positives and explore strategies to mitigate their impact, ultimately enhancing the accuracy. Although there are obstacles to overcome, AEs remain a viable option for producing easily understandable and interpretable outcomes. Nevertheless, it is crucial to continue improving the quality of the representations to advance unsupervised anomaly detection. Our findings demonstrate that MorphAEus is capable of learning superior representations, and can leverage the predicted dense displacement fields to refine its predictions and minimize the occurrence of false positives. This allows for accurate identification and localization of diseases, resulting in SOTA unsupervised pathology detection on chest X-rays."
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,1.0,Introduction,"In current radiology practice, a signed report is often the primary form of communication, to communicate results of a radiological imaging exam between radiologist. Speech recognition software (SRS), which converts dictated words or sentences into text in a report, is widely used by radiologists. Despite SRS reducing the turn-around times for radiology reports, correcting any transcription errors in the report has been assumed by the radiologists themselves. But, persistent report communication errors due to SRS can significantly impact report interpretation, and also have dire consequences for radiologists in terms of medical malpractice  In practice, CXR images from multiple patient visits are usually examined simultaneously to find interval changes; e.g., a radiologist may compare a patient's current CXR to a previous CXR, and identify deterioration or improvement in the lungs for pneumonia. Reports from longitudinal visits contain valuable information regarding the patient's history, and harnessing the longitudinal multimodal data is vital for the automated pre-filling of a comprehensive ""findings"" section in the report. In this work, we propose to use longitudinal multi-modal data, i.e., previous visit CXR, current visit CXR, and previous visit report, to pre-fill the ""findings"" section of the patient's current visit report. To do so, we first gathered the longitudinal visit information for 26,625 patients from the MIMIC-CXR dataset"
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,2.0,Methods,"Dataset. The construction of the Longitudinal-MIMIC dataset involved several steps, starting with the MIMIC-CXR dataset, which is a large publicly available dataset of 377,110 chest X-ray images corresponding to 227,835 radiographic reports from 65,379 patients  Following this, patients with ≥2 visit records were selected, resulting in 26,625 patients in the final Longitudinal-MIMIC dataset with a total of 94,169 samples. Each sample used during model training consisted of the current visit CXR, current visit report, previous visit CXR, and the previous visit report. The final dataset was divided into training (26,156 patients and 92,374 samples), validation (203 patients and 737 samples), and test (266 patients and 2,058 samples) splits. We aimed to create the Longitudinal-MIMIC dataset to enable the development and evaluation of models leveraging multi-modal data (CXR + reports) from longitudinal patient visits. Model Architecture. Figure  , where w i is the i-th word in the current report. The Text Encoder encoded text information for language feature embedding using a previously published method "
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,,Encoder. Our model uses an Image Encoder,"where θ E R refers to the parameters of the report text encoder. Cross-Attention Fusion Module. A multi-modal fusion module integrated longitudinal representations of images and texts using a cross-attention mechanism  The input to sub-block-2 D L is H dec,b . This structure is similar to sub-block-1, but interacts with H L instead of H IC . The output of this block is H dec,b,L and combined with H dec,b,I by adding them together. After fusing these embeddings and doing traditional layer normalization for them, we use these embeddings as the output of a block. The output of the previous block is used as the input of the next block. After N blocks, the final hidden states are obtained and used with a Linear and Softmax layer to get the target report probability distributions."
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,3.0,Experiments and Results,"Baseline Comparisons. We compared our proposed method against prior image captioning and medical report generation works respectively. The same Longitudinal-MIMIC dataset was used to train all baseline models, such as AoANet  Evaluation Metrics. Conventional natural language generation (NLG) metrics, such as BLEU "
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,,Results. Table,"Generic image captioning approaches like AoANet resulted in unsatisfactory performance on the Longitudinal-MIMIC dataset as they failed to capture specific disease observations. Moreover, our approach outperforms previous report generation methods, R2Gen and R2CMN that also use memory-based models, due to the added longitudinal context arising from the use of longitudinal multimodal study data (CXR images + reports). In our results, the BLEU scores show a substantial improvement, particularly in BLEU-4, where we achieve a 1.4% increase compared to the previous method R2CMN. BLEU scores measure how many continuous sequences of words appear in predicted reports, while Rouge L evaluates the fluency and sufficiency of predicted reports. The highest Rouge L score demonstrates the ability of our approach to generate accurate reports, rather than meaningless word combinations. We also use METEOR for evaluation, taking into account the precision, recall, and alignment of words and phrases in generated reports and the ground truth. Our METEOR score shows a 1.1% improvement over the previous outstanding method, which further solidifies the effectiveness of our approach. Meanwhile, our model exhibits a significant Fig.  Effect of Model Components. We also studied the contribution of different model components and detail results in Table  In our simple fusion experiment, we removed the cross-attention module and concatenated the encoded embeddings of the previous CXR image and previous visit report as one longitudinal embedding, while retaining the rest of the model. We saw a performance drop compared to our approach on our dataset, and also noticed that the results were worse than using the images or reports alone. These experiments demonstrate the utility of the cross-attention module in our proposed work."
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,4.0,Discussion and Conclusion,"Case Study. We also ran a qualitative evaluation of our proposed approach on two cases as seen in Fig.  Error Analysis. To analyze errors from our model, we examine generated reports alongside ground truths and longitudinal information. It is found that the label accuracy of the observations in the generated reports is greatly affected by the previous information. For example, as time changes, for the same observation ""pneumothorax"", the label can change from ""positive"" to ""negative"". And such changing examples are more difficult to generate accurately. According to our statistics, on the one hand, when the label results of current and previous report are the same, 88.96% percent of the generated results match them. On the other hand, despite mentioning the same observations, when the labels of current and previous report are different, there is an 84.42% probability of generated results being incorrect. Thus how to track and generate the label accurately of these examples is a possible future work to improve the generated radiology reports. One possible way to address this issue is to use active learning "
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,,Conclusion.,"In this paper, we propose to pre-fill the ""findings"" section of chest X-Ray radiology reports by considering the longitudinal multi-modal (CXR images + reports) information available in the MIMIC-CXR dataset. We gathered 26,625 patients with multiple visits to constitute the new Longitudinal-MIMIC dataset, and proposed a model to fuse encoded embeddings of multi-modal data along with a hierarchical memory-driven decoder. The model generated a pre-filled ""findings"" section of the report, and we evaluated the generated results against prior image captioning and medical report generation works. Our model yielded a ≥ 3% improvement in terms of the clinical efficacy F-1 score on the Longitudinal-MIMIC dataset. Moreover, experiments that evaluated the utility of different components of our model proved its effectiveness for the task of pre-filling the ""findings"" section of the report."
Utilizing Longitudinal Chest X-Rays and Reports to Pre-fill Radiology Reports,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 19.
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",1.0,Introduction,"Pulp calcification is a type of pulp degeneration, characterized by the deposition of calcified tissue in the root canal. Clinically, pulp calcification usually occurs with pulp periapical diseases, which brings great challenges to root canal therapy. Finding pulp calcification before root canal treatment is very important for dentists to decide treatment strategies  To this end, we propose a new method of tooth segmentation, identification and pulp calcification recognition based on Transformer to achieve accurate recognition of pulp calcification in high-resolution CBCT images. Specifically, we propose a coarse-to-fine method to segment the tooth instance in the low-resolution CBCT image, and back to the high-resolution CBCT image to intercept the region of the tooth as the input for the fine segmentation, identification and calcification recognition of the tooth instance. In order to enhance the weak distinction between normal teeth and calcified teeth, we put forward tooth instance correlation and triple loss to further improve the recognition performance of calcification. Finally, we introduce transformer to realize the above three tasks in an integrated way, and achieve mutual promotion of task performance. The clinical oral CBCT image data is used to verify the effectiveness of the proposed method."
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",2.1,The Proposed Framework,"The network structure designed in this work is shown in Fig.  In the pulp calcification recognition module, we extract the features of each tooth from the deep feature through the results of tooth segmentation, and input them into the pulp calcification recognition module. Specifically, we design an instance correlation transformer (ICT) block. This block allows teeth to learn information from other teeth, so that different teeth can interact, which enables the network itself to explore the relationship between instances, thus improving the recognition performance of calcified teeth. In addition, we introduce a discriminator in the ICT block, which uses triple loss to learn the spatial distribution between categories, so as to learn better classification embedding."
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",2.2,Tooth Segmentation and Identification Module,"In order to obtain the features of each tooth from the high-resolution CBCT image, we first recognize the segmented and identificated teeth,and combine the result of them for tooth instance segmentation. We use swin-transformer as the network backbone. Swin-transformer  The segmentation loss L seg of the model can be defined as L seg = L cs + γ 1 L dice + γ 2 L bs , where γ 1 ,γ 2 are the balance parameters, where L cs is the 33 categories of pixels (32 teeth classes + background) cross entropy loss. L dice is the dice loss of each tooth instance segmentation. L bs is binary segmentation. We calculate the cross entropy loss between teeth and background. The purpose of this step is to assist the model to distinguish foreground and background Calcified root canals, especially small root canal calcification, are shown on CBCT images as the shadow in the center of the cross section faded and disappeared, and the density of the shadow is close to or the same as that of the surrounding dentin, which is significantly different from the root canal images of other root canals of the same affected tooth and normal adjacent teeth. Based on the above clinical observation, we design the tooth instance correlation block for better calcification recognition. The multi-head attention layer is widely used to build image relationship models in channel and spatial dimensions  where MSA is multi-head self attention, L is the layer of MSA, and LN is the standardization layer."
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",2.4,Triple Loss and Total Loss Function,"In order to make the model more discriminative for the recognition of calcified teeth, a discriminator is designed in this work, which uses triplet loss to make the embedding of the input classifier more discriminative. This process is to make the features of the same category as close as possible, and the features of different categories as far away as possible. Meanwhile, in order to prevent the features of the instance from converging into a very small space, it is required that for two positive cases and one negative case, the negative case should be at least margin away from the positive case  where D is the European distance, α is a margin between positive and negative pairs. The classification loss is defined as L cls = L pc + γ 3 L t , where γ 3 is the balance parameter, L pc is the cross entropy loss as the loss of calcified tooth classification. Finally, the total loss function of the network is defined as L total = L seg + L cls ."
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",2.5,Implementation,"The initialization setting of the learning rate is 1e-3, with 60000 iterations. The Adam algorithm is used to minimize the objective function. Two RTX3090 GPUs are used, each with 24G memory. The attenuation setting of the learning rate is 0.99 for every 500 iterations. All parameters, including weights and deviations, are initialized using a truncated normal distribution with a standard deviation of 0.1. In the tooth instance segmentation task, we use connected component analysis to extract the maximum area of predicted voxels and remove some small false-positive voxels. Code is available at: https://github.com/Lsx0802/ ToothICT."
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",3.1,"Clinical Data, Experimental Setup and Evaluation Metric","This study was performed in line with the principles of the Declaration of Helsinki. In this work, 151 CBCT imaging data from the Imaging department of the local institute were acquired. The image resolution of the CBCT equipment used was 0.2 ∼ 1.0 mm, and the size of the CBCT volume is 672 × 688 × 688. The bulb voltage was 60 ∼ 90 kV, and the bulb current was 1 ∼ 10 mA. 151 cases of dental symptoms were identified as CBCT oral indications by two dentists with 10 years of clinical experience. Among them, 60 patients had dental pulp calcification. In addition, each tooth was also marked for calcification. One dentist is responsible for the data label, and two doctors review it. When they disagree, they will reach an agreement through negotiation. The CBCT data is preprocessed as follows. First, considering the balance between computational efficiency and instance segmentation accuracy, all CBCT images are normalized to 0.4 × 0.4 × 0.4 mm 3 . Then, in order to reduce the impact of extreme values, especially in the metal artifact area, we cut the voxellevel intensity value of each CBCT scan to [0, 2500], and finally normalized the pixel value to the interval [0,1]. For Pulp calcification recognitionin, the adopted evaluation metrics include: Accuracy, Precision, Recall, F1, Dice. The measurement results were conducted with 10 times of four-fold cross validation. In addition, we have compared the performance of the proposed method with the relevant tooth segmentation methods, we used typical segmentation metrics for performance evaluation: Dice, Jaccard similarity coefficient (Jaccard), 95% Hausdorff distance (HD95), Average surface distance (ASD) and have conducted the ablation study of the proposed method."
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",3.2,Performance Evaluation,"As shown in Table  Our model adopts a network based on swin transformer. Through its powerful global and local modeling ability, while retaining the jumping connection in UNet to retain the shallow features, the performance of Backbone1 is better than the previous segmentation model. In particular, we use reverse skip connection and use deep features to guide shallow feature learning, which has achieved obvious improvement in segmentation performance. After combining the task of calcification classification, the segmentation network has been improved a little, which benefits from the ICT module we adopted, because it not only learns the correlation characteristics between calcified teeth and normal teeth, but also learns the morphological correlation between teeth, which is beneficial to tooth segmentation. Table  The accuracy of the model is only 74.62% when only swin transformer is used to classify tooth samples, while our proposed model can improve the performance of pulp calcification recognition by 3.85%. Especially, in the ablation experiment, when the ICT module is removed, the model performance drops obviously, which proves that our proposed ICT module can effectively learn the relationship between dental examples. In addition, after the loss Lt of the discriminant module is removed, the accuracy of the model decreases by about 1.16%, which proves that this method can effectively reduce the distance between similar samples and increase the distance between different samples. (See the supplementary materials for more visualization results) "
"Transformer-Based Tooth Segmentation, Identification and Pulp Calcification Recognition in CBCT",4.0,Conclusion,"In this study, we proposed a calcified tooth recognition method based on transformer, which can detect calcified teeth in high-resolution CBCT images while achieving tooth instance segmentation and identification. Specifically, we proposed a coarse-to-fine processing method to make it possible to process highresolution CBCT with deep network for calcification recognition. In addition, the design of instance correlation and triple loss further improved the accuracy of calcification detection. The validation of clinical data showed the effectiveness and advantages of the proposed method. We believe that this research will bring help to the intellectualization of oral imaging diagnosis and the navigation of oral surgery."
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,1.0,Introduction,"Deep learning has enabled significant progress in image-based computer-aided diagnosis  To bridge this gap, this study aims to answer the following guiding questions by conducting experiments to dissect the differential impact of pruning: Q1. What is the impact of pruning on overall performance in longtailed multi-label medical image classification? Q2. Which disease classes are most affected by pruning and why? Q3. How does disease co-occurrence influence the impact of pruning? Q4. Which individual images are most vulnerable to pruning? We focus our experiments on thorax disease classification on chest X-rays (CXRs), a challenging long-tailed and multi-label computer-aided diagnosis problem, where patients may present with multiple abnormal findings in one exam and most findings are rare relative to the few most common diseases  This study draws inspiration from Hooker et al.  Unlike existing work, we explicitly connect class ""forgettability"" to the unique aspects of our problem setting: disease frequency (long-tailedness) and disease cooccurrence (multi-label behavior). Since many diagnostic exams, like CXR, are long-tailed and multi-label, this work fills a critical knowledge gap enabling more informed deployment of pruned disease classifiers. We hope that our findings can provide a foundation for future research on pruning in clinically realistic settings."
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,2.1,Preliminaries,"Datasets. For this study, we use expanded versions of NIH ChestXRay14  Model Pruning & Evaluation. Following Hooker et al. "
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,2.2,Assessing the Impact of Pruning,"Experimental Setup. We first train a baseline model to classify thorax diseases on both NIH-CXR-LT and MIMIC-CXR-LT. The architecture used was a ResNet50  Overall and Class-Level Analysis. To evaluate the overall impact of pruning, we compute the mean AP across classes for each sparsity ratio and dataset. We use Welch's t-test to assess performance differences between the 30 uncompressed models and 30 k-sparse models. We then characterize the class-level impact of pruning by considering the relative change in AP from an uncompressed model to its k-sparse counterpart for all k. Using relative change in AP allows for comparison of the impact of pruning regardless of class difficulty. We then define the forgettability curve of a class c as follows: med AP i,k,c -AP i,0,c AP i,0,c i∈{1,...,30} k∈{0,0.05,...,0.9,0.95} "
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,2.3,Pruning-Identified Exemplars (PIEs),"Definition. After evaluating the overall and class-level impact of pruning on CXR classification, we investigate which individual images are most vulnerable to pruning. Like Hooker et al. "
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,,Analysis and Human Study.,"To understand the common characteristics of PIEs, we compare how frequently (i) each class appears and (ii) images with d = 0, . . . , 3, 4+ simultaneous diseases appear in PIEs relative to non-PIEs. To further analyze qualities of CXRs that require domain expertise, we conducted a human study to assess radiologist perceptions of PIEs. Six board-certified attending radiologists were each presented with a unique set of 40 CXRs (half PIE, half non-PIE). Each image was presented along with its ground-truth labels and the following three questions:  "
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,3.1,What is the Overall Effect of Pruning?,"We find that under L1 pruning, the first sparsity ratio causing a significant drop in mean AP is 65% for NIH-CXR-LT (P < 0.001) and 60% for MIMIC-CXR-LT (P < 0.001) (Fig. "
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,3.2,Which Diseases are Most Vulnerable to Pruning and Why?,Class forgettability curves in Fig. 
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,3.3,How Does Disease Co-occurrence Influence Class Forgettability?,"Our analysis reveals that for NIH-CXR-LT, the absolute difference in log test frequency between two diseases is a strong predictor of the pair's FCD (ρ = 0.64, P 0.001). This finding suggests that diseases with larger differences in prevalence exhibit more distinct forgettability behavior upon L1 pruning (Fig.  We also find, however, that there is a push and pull between differences in individual class frequency and class co-occurrence with respect to FCD. To illustrate, consider the disease pair (Emphysema, Pneumomediastinum) marked in black in Fig.  We observe a statistically significant interaction effect between the difference in individual class frequency and class co-occurrence on FCD (β 3 = -0.31, P = 0.005). Thus, for disease pairs with a very large difference in prevalence, the effect of co-occurrence on FCD is even more pronounced (Supplement)."
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,3.4,What Do Pruning-Identified CXRs have in Common?,"For NIH-CXR-LT, we find that PIEs are more likely to contain rare diseases and more likely to contain 3+ simultaneous diseases when compared to non-PIEs (Fig.  In a human reader study involving 240 CXRs from the NIH-CXR-LT test set (120 PIEs and 120 non-PIEs), radiologists perceived that PIEs had more label noise, lower image quality, and higher diagnosis difficulty (Fig.  Overall, these findings suggest that pruning identifies CXRs with many potential sources of difficulty, such as containing underrepresented diseases, (partially) incorrect labels, low image quality, and complex disease presentation."
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,4.0,Discussion and Conclusion,"In conclusion, we conducted the first study of the effect of pruning on multi-label, long-tailed medical image classification, focusing on thorax disease diagnosis in CXRs. Our findings are summarized as follows: 1. As observed in standard image classification, CXR classifiers can be heavily pruned (up to 60% sparsity) before dropping in overall performance. 2. Class frequency is a strong predictor of both when and how severely a class is impacted by pruning. Rare classes suffer the most. 3. Large differences in class frequency lead to dissimilar ""forgettability"" behavior and stronger co-occurrence leads to more similar forgettability behavior. -Further, we discover a significant interaction effect between these two factors with respect to how similarly pruning impacts two classes. 4. We adapt PIEs to the multi-label setting, observing that PIEs are far more likely to contain rare diseases and multiple concurrent diseases. -A radiologist study further suggests that PIEs have more label noise, lower image quality, and higher diagnosis difficulty. It should be noted that this study is limited to the analysis of global unstructured L1 (magnitude-based) pruning, a simple heuristic for post-training network pruning. Meanwhile, other state-of-the-art pruning approaches "
How Does Pruning Impact Long-Tailed Multi-label Medical Image Classifiers?,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 64.
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,1.0,Introduction,"Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest forms of human cancer, with a 5-year survival rate of only 9%  Previous studies have utilized image texture analysis with hand-crafted features to predict the survival of patients with PDACs  Here ♦ and are points sampled from subset Vc and Pc defined in Eq. power of these features may be limited. In recent years, deep learning-based methods have shown promising results in prognosis models  We propose a novel approach for measuring the relative position relationship between the tumor and the vessel by explicitly using the distance between them. Typically, Chamfer distance  In this study, we make the following contributions: "
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,2.0,Methods,As shown in Fig. 
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,2.1,Texture-Aware Vision Transformer: Combination of CNN and Transformer,"Recently, self-attention models, specifically vision transformers (ViTs  1 × 1 × 1 convolutional layer. The 3 × 3 × 3 convolution captures local spatial information, while the 1 × 1 × 1 convolution maps the input tensor to a higherdimensional space (i.e., C l > C). The texture-aware CNN block downsamples the input, and the texture-aware self-attention block captures long-range nonlocal dependencies through a patch-wise self-attention mechanism. In the texture-aware self-attention block, the input feature F c is divided into N non-overlapping 3D patches F u ∈ R V ×N ×Cu , where V = hwd and N = HW D/V is the number of patches, and h, w, d are the height, width, and depth of a patch, respectively. For each voxel position within a patch, we apply a multi-head self-attention block and a feed-forward block following  Instead of directly fusing the outputs as in previous work, we employ a 3-way cross-attention block to extract cross-modality information from these phases. The cross-attention is performed on the concatenated self-attention matrix with an extra mask M ∈ {0, -∞} 3C×3C , defined as: Here, Q, K, V are the query, key, and value matrices, respectively, obtained by linearly projecting the input F T o ∈ R 3C×D . The cross-modality output F cross and in-modality output F T o are then concatenated and passed through an average pooling layer to obtain the final output feature of the texture branch, denoted as F t ∈ R Ct ."
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,2.2,Neural Distance: Positional and Structural Information,"Between PDAC and Vessels The vascular involvement in patients with PDAC affects the resectability and treatment planning  where v ∈ V and p ∈ P are points on the surfaces of blood vessels and PDAC, respectively. The point-to-surface distance d ps (v, P) is the distance from a point v on V to P, defined as d ps (v, P) = min p∈P v -p 2 2 , and vice versa. To numerically calculate the integrals in the previous equation, we uniformly sample from the surfaces V and P to obtain the sets V and P consisting of N v points and N p points, respectively. The distance is then calculated between the two sets using the following equation: However, the above distance treats all points equally and may not be flexible enough to adapt to individualized prognostic predictions. Therefore, we improve the above equation in two ways. Firstly, we focus on the sub-sets Vc and Pc of V and P, respectively, which only contain the K closest points to the opposite surfaces P and V, respectively. The sub-sets are defined as: Secondly, we regard the entire sets Vc and Pc as sequences and calculate the distance using a 2-way cross-attention block (similar to Eq. 1) to build a neural distance based on the 3D spatial coordinates of each point: Neural distance allows for the flexible assignment of weights to different points and is able to find positional information that is more suitable for PDAC prognosis prediction. In addition to neural distance, we use the 3D-CNN model introduced in  Finally, we concatenate the features extracted from the two components and apply a fully-connected layer to predict the survival outcome, denoted as O OS , which is a value between 0 and 1. To optimize the proposed model, we use the negative log partial likelihood as the survival loss "
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,3.0,Experiments,"Dataset. In this study, we used data from Shengjing Hospital to train our method with 892 patients, and data from three other centers, including Guangdong Provincial People's Hospital, Tianjin Medical University and Sun Yatsen University Cancer Center for independent testing with 178 patients. The contrast-enhanced CT protocol included non-contrast, pancreatic, and portal venous phases. PDAC masks for 340 patients were manually labeled by a radiologist from Shengjing Hospital with 18 years of experience in pancreatic cancer, while the rest were predicted using self-learning models  Implementation Details: We used nested 5-fold cross-validation and augmented the training data by rotating volumetric tumors in the axial direction and randomly selecting cropped regions with random shifts. We also set the output feature dimensions to C t = 64 for the texture-aware transformer, C s = 64 for the structure extraction and K = 32 for the neural distance. The batch size was 16 and the maximum iteration was set to 1000 epochs, and we selected the model with the best performance on the validation set during training for testing. We implemented our experiments using PyTorch 1.11 and trained the models on a single NVIDIA 32G-V100 GPU. Ablation Study. We first evaluated the performance of our proposed textureaware transformer (TAT) by comparing it with the ResNet18 CNN backbone and ViT transformer backbone, as shown in Table  Secondly, we evaluated each component in our proposed method, as shown in Fig. "
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,,Comparisons.,"To further evaluate the performance of our proposed model, we compared it with recent deep prediction methods  In Table  Neoadjuvant Therapy Selection. To demonstrate the added value of our signature as a tool to select patients for neoadjuvant treatment before surgery, we plotted Kaplan-Meier survival curves in Fig. "
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,4.0,Conclusion,"In our paper, we propose a multi-branch transformer-based framework for predicting cancer survival. Our framework includes a texture-aware transformer that captures both local and global information about the PDAC and pancreas. We also introduce a neural distance to calculate a more reasonable distance between PDAC and vessels, which is highly correlated with PDAC survival. We have extensively evaluated and statistically analyzed our proposed method, demonstrating its effectiveness. Furthermore, our model can be combined with established high-risk features to aid in the patient selections who might benefit from neoadjuvant therapy before surgery."
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 24.
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,1.0,Introduction,"Existing tumor augmentation methods, including ""Copy-Paste"" strategy based methods  To overcome the above challenges, a Domain-aware and Content-consistent tumor Augmentation method, named DCAug, is developed (Fig.  -A content-aware and domain-aware tumor augmentation method is proposed, which eliminates the distortion in content and domain space between the true tumor image and synthetic tumor image. -Our novel DaCL and CdCL disentangle the image information into two completely independent parts: 1) domain-invariant content information; 2) individual-specific domain information. It has the advantage of alleviating the challenge of distortion in synthetic tumor images. -Experimental results on two public tumor segmentation datasets demonstrate that DCAug improves the diversity and quality of synthetic tumor images."
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,2.1,Problem Definition,"Formulation: Given two images and the corresponding tumor labels {X A , Y A }, {X B , Y B }, tumor composition process can be formulated as: where , respectively. There are two challenges need to be solved: 1) X b→A A , X a→B B , by adjusting the domain information of the copied tumor, making the copied tumor have the same domain space as the target image to avoid domain distortion; 2) B , maintaining the domain-invariant content information consistency during tumor copy to avoid content distortion. To achieve the above goals, a novel Cross-cycle Framework (Fig. "
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,2.2,Domain-Aware Contrastive Learning for Domain Adaptation,"Our domain-aware contrastive learning (DaCL) strategy can adaptively adjust the domain space of the transferred tumor and makes the domain space consistent for domain adaptation. Specifically, the input of DCAug is two combined images X b A , X a B that consist of source images and tumor regions copied from another image. The synthetic tumors X b→A A generated by the generator, the  A as the anchor, the positive and the negative sample, respectively. To find the domain space of these samples for contrast, a fixed pre-trained style representation extractor f is used to obtain domain representations for different images. Thus, DaCL between the anchor, the positive, and the negative sample can be formulated as: where D(x, y) is the L 2 distance between x and y, w i is weighting factor. Additionally, to further disentangle the individual-specific domain information, a reversed process is designed. By utilizing the synthetic tumors X a→B B , X b→A A , the reversed images X a→B A ,X b→A B can be construed as: The whole reversed process receives the reversed images X a→B A , X b→A B as inputs and tries to restore the original domain information of the synthetic tumor XA , XB . where φ denotes the ith layer of the VGG-19 network, μ and σ represent the mean and standard deviation of feature maps extracted by φ, respectively. In summary, the total loss for the cross-cycle framework is ) where α, β, and γ represent the weight coefficients."
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,3.1,Datasets and Implementation Details,ATLAS Dataset 
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,3.2,Comparison with State-of-the-Art Methods,Experimental results in Table  The representative segmentation scans are shown in Fig. 
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,3.3,Significant in Improving Existing Tumor Augmentation Methods,"The necessity of considering both content and domain information in the tumor generation is also demonstrated, three representative methods, Mixup (""Copy-Paste""), CutMix (""Copy-Paste""), and StyleMix (style-transfer), are selected. The DCAug optimizes generated samples from above methods from content and domain aspects to further improve the quality of generated samples. And the nnUet are trained by optimized samples. From the segmentation performances (Table "
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,4.0,Conclusion,"In this paper, our domain-aware and content-consistent tumor augmentation method eliminated the content distortion and domain gap between the true tumor and synthetic tumor by simultaneously focusing the content information and domain information. Specifically, DCAug can maintain the domain-invariant content information consistency and adaptive adjust individual-specific domain information by a new cross-cycle framework and two novel contrastive learning strategies when generating synthetic tumor. Experimental results on two tumor segmentation tasks show that our DCAug can significantly improve the quality of the synthetic tumors, eliminate the gaps, and has practical value in medical imaging applications."
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,2.3,Cross-Domain Consistency Learning for Content Preservation,"Cross-domain consistency learning (CdCL) strategy can preserve the domaininvariant content information of tumor in the synthesized images X b→A A , X a→B B for avoiding content distortion. Specifically, given the original images produced by generator, and the reconstructed images XA , XB generated by the reversed process. The tumor can be first extracted from those images Although the domain space is various, the tumor content inside To evaluate the tumor content inside cross-domain images, the content consistency losses, including L A pixel (X A , XA ), L B pixel (X B , XB ), L a→B content , L b→A content , are computed between those images for supervising the content change. The details of content consistency loss are described in the next section."
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,2.4,Loss Function,"In summary, three types of losses are used to supervise the cross-cycle framework. Specifically, given the original images X A , X B and the combined images X b A , X B a , the synthesized images X a→B B , X b→A A are produced by the generator, and the reconstructed images XA , XB are generated by the reversed process. The pixel-wise loss (L pixel ) computes the difference between original images and reconstructed images at the pixel level. To disentangle the individual-specific domain information, the higher feature representations extracted from pre-trained networks combined with CL are used: And two content loss L b→A content , L a→B content are employed to maintain tumor content information during the domain adaptation:"
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,1.0,Introduction,"Chest X-rays (CXRs) are frequently used for disease detection and disease progression monitoring. However, interpreting CXRs can be challenging and timeconsuming, particularly in regions with a shortage of radiologists. This can lead to delayed or inaccurate diagnoses and management, potentially harming patients. Automating the CXR interpretation process can lead to faster and more accurate diagnoses. Advances in Artificial Intelligence (AI), particularly in the field of computer vision for medical imaging, have significantly alleviated the challenges faced in radiology. The availability of large labeled collections of CXRs has been instrumental in driving progress in this area  Although significant strides have been made in AI-assisted medical image segmentation and disease detection, tasks requiring intricate reasoning have received less attention. One such complex task is monitoring disease progression in a sequence of images, which is particularly critical in assessing patients with pneumonia and other CXR findings. For example, temporal lung changes serve as vital indicators of patient outcomes and are routinely mentioned in radiology reports for determining the course of treatment  Inspired by the success of Transformer models in remote sensing change detection tasks  (1) We propose CheXRelFormer, an end-to-end siamese disease progression model that can accurately detect changes in CXR image pairs by attend- ing to informative regions and identifying fine-grained relevant visual differences. (2) CheXRelFormer leverages hierarchical vision Transformers and a difference module to compute multi-level feature differences across CXR images, allowing the model to capture long-range spatial and temporal information. (3) We experimentally demonstrate that CheXRelFormer outperforms existing state-of-the-art baselines in detecting disease progression in CXR images."
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,2.0,Methodology,"Let C = {(X, X ) i } N i=1 be a set of CXR image pairs, where X, X ∈ R H×W ×C , and H, W , and C are the height, width, and number of channels, respectively. Each image pair (X, X ) i is associated with a set of labels Y i = {y i,m } M m=1 , where y i,m ∈ {0, 1, 2} indicates whether the pathology m appearing in the image pair has improved, worsened, or remained the same. The goal is to design a model that accurately predicts the disease progression labels for an unseen image pair (X, X ) and a wide range of pathologies. To this end, we use a hierarchical Transformer  where Q, K, V ∈ R N ×C are the queries, keys, and values, respectively; W O is a learned weight matrix, J is the number of heads, and head j is the j-th attention head, computed as Here, d k is the dimensionality of the key and query vectors in each head, and the softmax function is applied along the rows of the matrix. The queries, keys, and values Q j , K j , and V j are obtained via a set of linear projection matrices as where W Q , W K , W V are learned weight tensors that project the input embeddings onto a lower-dimensional space. Similarly, the query, key, and value matrices for the second image in the pair are computed as where the weight tensors W Q , W K , W V are shared across the two images in the pair. The output of each multi-head self-attention block for each image pair, denoted by (F, F ), is then fed into a position-wise feedforward network which consists of two linear transformations and a depth-wise convolution  Here, W d is the shared depth-wise convolution weight matrix and each feedforward layer f 1 , f 2 consists of a linear transformation followed by a non-linear activation. The difference module then processes the visual features from each Transformer layer to compute multi-level feature differences as follows: Here, l = 1, . . . , L denotes the l-th Transformer layer, with initial inputs (F 1 , F 1 ) = (F c , F c ). Furthermore, φ is a non-linear activation, W l is a learned weight parameter that essentially represents a multi-scale trainable distance metric, and [•, •] is the concatenation operation. By computing differences between features at different scales, the proposed model can capture local and global structures that are relevant to the disease progression task. Each multi-scale feature difference map is then passed through a feed-forward layer that maps the input features to a common feature space where θ l represents the set of learnable parameters for the l-th feed-forward network. The concatenated feature tensor combines information from multiple scales and is denoted as where [•] denotes concatenation along the channel dimension. A feed-forward network with a global average pooling step, denoted by g, creates a fused feature representation with fixed dimensionality, which is finally passed through the final classification layer, denoted by h, to obtain the label predictions The network is trained end-to-end with a multi-label cross-entropy classification loss where σ represents the sigmoid function and ŷi,m , y i,m are the model prediction and the ground truth for example (X, X ) i . An overview of the model architecture is represented in Fig. "
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,3.1,Implementation Details,CheXRelFormer is implemented in Pytorch 
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,3.2,Dataset,We make use of the Chest ImaGenome dataset 
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,3.3,Baselines,"To assess the performance of the proposed CheXRelFormer model, we conduct a comparative analysis with several baselines. Local: This model employs a previously proposed siamese network "
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,,Global:,The Global model is a siamese network similar to the Local model but encodes global image-level information. CheXRelNet: CheXRelNet combines global image-level information with local intra-image and inter-image information 
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,3.4,Experimental Results,"Table  Most importantly, we observe up to 12% performance gains for pathology labels with limited amounts of data, such as atelectasis and consolidation. These findings suggest that CheXRelFormer has the potential to be a valuable tool for detecting changes in CXR images associated with various common diseases."
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,3.5,Ablations on CheXRelFormer Architecture Components,"We perform an ablation study to understand the impact of four factors, the difference module, the use of global vs. localized visual information, and the impact of multi-level features. Specifically, in  An interesting observation is that CheXRelFormer Local underperforms as the focus on specific anatomies limits the visual information available to the model. Given the highly fine-grained nature of this task, this result suggests that the relationship between an area and its surroundings is critical to a radiologist's perception of change, and the local Transformer cannot provide the necessary second-order information to the model. Therefore, our results show that global image-level information is crucial for accurately predicting disease change. In addition, the absolute difference model, CheXRelFormer AbsDiff, failed to perform the disease change classification task, indicating the importance of the proposed difference module. By incorporating the difference module and computing multi-level feature differences at multiple resolutions, CheXRelFormer learns to focus on the changes between two CXRs and to ignore irrelevant information. Our results demonstrate that multi-level feature differences are critical for improving performance in predicting disease change."
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,3.6,Qualitative Analysis,In Fig. 
Hierarchical Vision Transformers for Disease Progression Detection in Chest X-Ray Images,4.0,Conclusion,"Monitoring disease progression is a critical aspect of patient management. This task requires skilled clinicians to carefully reason and evaluate changes in a patient's condition. In this paper, we propose CheXRelFormer, a hierarchical Transformer with a multi-scale difference module, trained on global image pair information to detect disease changes. Our model is inspired by the way clinicians monitor changes between CXRs, and improves the state of the art in this challenging medical imaging task. Our ablation studies show that global attention and the proposed difference module are critical components, and both help detect fine-grained changes between images. While our work shows significant progress, given the fine-grained nature of visual features that characterize findings in CXRs, disease progression remains a challenging task. In future work, we intend to include multimodal contextual information beyond the images, such as patient history and reports, to enhance the results. CheXRelFormer offers a promising solution for monitoring disease progression, and future work can extend the proposed methodology to various medical imaging modalities."
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,1.0,Introduction,"Intracerebral Hemorrhage (ICH) is a bleeding into the brain parenchyma, which has the second-highest incidence of stroke (accounts for more than 10% of strokes) and remains the deadliest type of stroke with mortality more than 40%  One of the major challenges in treatment effect estimation is missing counterfactual outcome  To handle these challenges, some related works were based on the concept of balanced representation learning, which proposes to use additional loss to mitigate the aforementioned selection bias in the representation space  In this paper, we propose a novel prognostic model that leverages both imaging and tabular data to achieve accurate treatment outcome prediction. This model is intended to be trained on observational data obtained from the nonrandomized controlled trials. Specifically, to increase the reliability of the model, we employ a variational autoencoder model to generate a low-dimensional prognostic score that alleviates the problem of selection bias. Moreover, we introduce a variational distributions combination module that integrate information from imaging data and non-imaging clinical data to generate the aforementioned prognostic score. We evaluate our proposed model on a clinical dataset of intracerebral hemorrhage and demonstrate a significant improvement in treatment outcome prediction compared to existing treatment effect estimation techniques. "
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,2.1,Formulation and Motivation,"We aim to predict the individualized treatment outcome based on a set of observations that include the actual treatment T , observed covariates X, and factual outcome Y . In this paper, we study the one-year functional outcome of patient who underwent either conservative treatment (T = 0) or surgery (T = 1). For each individual, let t ∈ {0, 1} denote the treatment assignment, x = (x img , x tab ) represent the observed covariates comprising imaging data x img and non-imaging tabular data x tab , and y indicate the factual outcome. In this study, the treatment outcome was assessed using 1-year modified Rankin Scale (mRS)  The non-randomized controlled trials impacted by treatment preference can lead to selection bias, rendering the model unreliable due to potential encounters with unobserved scenarios during training. To address this issue, our model is inspired by the approach commonly used by doctors in clinical practice: using a combination of imaging data and non-imaging biomarkers to generate a prognostic score (e.g., GCS score) that predicts the likelihood of good or poor condition after treatment. In this study, a prognostic score is defined as any function f T (X) of X and T that Markov separates Y and X, such that Y ⫫ X|f T (X). The insight is that a patient's health status can be effectively captured by a lowdimensional score Z = f T (X), which is a form of dimension reduction that is sufficient for causal inference and can naturally mitigate the problem brought by non-randomized controlled trials. This is because, as illustrated in Fig. "
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,2.2,Generative Prognostic Model,"Architecture. As can be seen in Fig.  where D KL (q(z | x, y, t)∥p(z | x, t)) is the Kullback-Leibler (KL) divergence between distributions q(z | x, y, t) and p(z | x, t), and β is the weight balancing the terms in the ELBO. Note that the first term of Eq. 1 corresponds the classification error, which is minimized by the cross entropy loss. The second term of Eq. 1 uses KL divergence to encourage convergence of the prior distribution towards the posterior distribution. The training objective is to maximize the ELBO given the observational data, so that the model can be optimized."
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,2.3,Variational Distributions Combination,"Once the features of imaging and non-imaging tabular data have been extracted, the primary challenge is to effectively integrate the multi-modal information. One approach that is immediately apparent is to train a single encoder network that takes all modalities as input, which can explicitly parameterize the joint distribution. Another commonly used method called Mixture-of-Experts (MoE) proposes to fuse the distributions from different modalities by weighting  where and Σ pri t are mean and covariance of universal prior expert, which is typically a spherical Gaussian (N (0, 1)). For posterior distribution q(z t | x, y, t), we first additionally concatenate the features and y together, and then generate the joint distribution by the same way. The PoE for generating joint distributions offers several advantages over the aforementioned approaches. Compared with the approaches that simply combing the features and then generating the joint distributions, PoE not only can effectively address the potential issue of prediction outcomes being overly influenced by the modality with a more abundant feature "
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,3.1,Dataset and Experimental Setup,"Datasets. We utilized an in-house dataset of intracerebral hemorrhage cases obtained from the Hong Kong Hospital Authority. The dataset comprises 504 cases who underwent head CT scans and were diagnosed with ICH. Among them, 364 cases received conservative treatment, and 140 cases underwent surgery treatment. For each case, we collected both CT imaging and non-imaging clinical data. The non-imaging data have 17 clinical characteristics which have been proved to be potentially associated with the treatment outcome in clinical practice  Evaluation Metrics. We employed three evaluation metrics that are commonly used in treatment effect estimation and outcome prediction in our experiments, including the policy risk (P ROL ), the accuracy (Acc) and the area under the ROC curve (AU C). P ROL measures the average loss incurred when utilizing the treatment predicted by the treatment outcome estimator "
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,,Implementation Details.,"In preprocessing the imaging data, raw image intensity values were truncated to [-20, 100], normalized to have zero mean and unit variance, and slices were uniformly resized to 224×224 in the axial plane. We implemented our model using PyTorch and executed it on an NVIDIA A100 SXM4 card. For training, we used the Adam optimizer, a weight decay of 5×10 -3 , and an initial learning rate of 5 × 10 -3 . The training process lasted for a total of 2 h, consisting of 1000 epochs with a batch size of 128. Our reported results are the average and standard deviation obtained from three independent runs. "
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,3.2,Experiment Results,"Comparison with State-of-the-Art Methods. We benchmarked our method against state-of-the-art approaches for treatment effect estimation, which are recognized as strong competitors in this field. These approaches include BNN  Table  Next, we studied the contributions of multi-modality distribution combination. As can be seen in Fig. "
Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data,4.0,Conclusion,"This paper introduces a novel generative prognostic model for predicting ICH treatment outcomes using imaging and non-imaging data. The model is designed to be trained on data collected from non-randomized controlled trials, addressing the imbalance problem with a VAE model and integrating multi-modality information using a variational distribution combination module. The model was evaluated on a large-scale dataset, confirming its effectiveness."
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,1.0,Introduction,"Multiple Instance Learning (MIL)  Among the proposed approaches for learning in the MIL scenario  In this work, we are particularly interested in the detection of intracranial hemorrhage (ICH), a serious life-threatening emergency caused by blood leakage inside the brain  State-of-the-art ICH detection methods rely on DL models, specifically convolutional neural networks (CNNs), to extract meaningful ICH features  In this work, we improve upon the state-of-the-art deep MIL methods by introducing dependencies between instances in a sound probabilistic manner. These dependencies are formulated over a neighborhood graph to impose smoothness on the latent function that encodes the attention given to each instance. Smoothness is achieved by introducing specific first-and second-order constraints on the latent function. Our model, called SA-DMIL, is applied to the ICH detection problem, obtaining (a) significant improvements upon the performance of non-smooth models at both scan and slice levels, (b) smoother attention weights across slices by benefiting from the inter-slice dependencies, and (c) a superior performance against other popular MIL methods on the same test set."
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,2.1,Problem Formulation,"We start by formulating ICH detection as a Multiple Instance Learning (MIL) problem. To do so, we map slices to instances and CT scans to bags. The slices (instances) will be denoted by , where H and W are the height and width of the image, 3 is the number of color channels, b is the index of the scan to which the slice belongs to and i is the index of the slice inside the bag. We will denote the label of a slice by y b i ∈ {0, 1}. If the slice contains hemorrhage, then y b i = 1, otherwise y b i = 0. Note that the slice labels remain unknown since only scan labels are given. As we know, slices are grouped to form the CT scans. Each scan (bag) will be denoted by Here, N b is the number of slices in bag b. We will assume that B CT scans are given, so b ∈ {1, . . . , B}. Given a CT scan b, we will denote its label by T b ∈ {0, 1}. Notice that T b = 1 if and only if some of y i b = 1, i.e., the following relationship between scan and slice labels holds,"
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,2.2,Attention-Based Multiple Instance Learning Pooling,"The attention-based MIL pooling was proposed in  , where z b i ∈ R D , the attention-based MIL pooling computes where Notice that w ∈ R L and V ∈ R L×D are trainable parameters, where D denotes the size of feature vectors. We refer to s z b i as attention weights and to f z b i as attention values. This operator was proposed under the assumption that the instances in a bag show neither dependency nor order among each other. Although this may be the case in simple problems, it does not occur in problems such as ICH detection. Note that the attention weights of slices in a bag are correlated: given a slice containing ICH, we expect that the adjacent slices will also contain ICH with high probabilities. This is essential in finding slices with ICH. In the next subsection, we show how to introduce this correlation between attention weights."
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,2.3,Modeling Correlation Through the Attention Mechanism,"Ideally, in the case of a positive scan (T b = 1), high attention weights should be assigned to slices that are likely to have a positive label (y b i = 1). Given the dependency between slices, contiguous slices should have similar attention values. In other words, the differences between the attention values of contiguous slices should be small. Thus, for each bag b, these quantities should be small where A b ij = 1 if the slices i, j are related in bag b, and 0 otherwise. We smooth f z b i instead of s z b i because a non-constrained parameter f ensures consistent smoothing while s requires a normalization across instances in a bag. Equations (  To compute L b S1 and L b S2 efficiently we consider the simple graph defined by the dependency between slices. For a bag b, its adjacency matrix is ij is a diagonal matrix that contains the degree of each slice (the degree of the slice i is the number of slices j such that A b ij = 1). This is, D b ii = degree(i) and D b ij = 0 if i = j. Using these, one can compute the graph Laplacian matrix of a bag as where where k ∈ {1, 2}, can be added to the loss function of a network to be minimized along the taskspecific loss. Note that these two terms provide two different approaches to exploiting the correlations between instances through the loss function. We will refer to this approach as smooth attention (SA) loss. In the following subsection, we propose a model that can use either L S1 or L S2 . The effect of each term will be discussed in Sect. 4."
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,2.4,SA-DMIL Model Description,"We propose to couple the attention-based MIL pooling with the SA loss terms introduced in Subsect. 2.3. The proposed model, named Smooth Attention Deep Multiple Instance Learning (SA-DMIL), is depicted in Fig.  The CNN module in Fig.  where we have written where α ∈ [0, 1] is an hyperparameter and L CE the common cross-entropy loss, where k ∈ {1, 2}, and L Sk = b L b Sk (see Eqs. ( "
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,3.1,Data and Data Preprocessing,The dataset used in this work was obtained from the 2019 Radiological Society of North America (RSNA) challenge 
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,3.2,Experimental Settings,We fix D = 128 and L = 50 in Eq. ( 
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,4.1,Hyperparameters Tuning,"In this subsection, we study the effect of SA loss in terms of performance. Table "
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,4.2,Smooth Attention MIL vs. Other MIL Methods,The performance of other popular MIL methods is also included in Table 
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,4.3,Visualizing Smooth Regularizing Effects at Slice Level,"So far we have observed enhanced performance through the SA term. In this subsection, we visually illustrate how this novel term imposes smoothness between attention scores of consecutive slices, leading to more accurate predictions. Figure "
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,5.0,Conclusion,"In this study we have proposed SA-DMIL, a new model that obtains significant improvements in ICH classification compared to state-of-the-art MIL methods. This is done by adding a smoothing regularizing term to the loss function. This term imposes a smoothness constraint on the latent function that encodes the attention weights, which forces our model to learn dependencies between instances rather than training each instance independently in a bag. This flexible approach does not introduce any additional complexity, so similar ideas can be applied to other methods to model dependencies between neighboring instances."
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,,Data Use Declaration,The dataset used in this study is from the 2019 RSNA Intracranial Hemorrhage Detection Challenge and is publicly available in this link.
Smooth Attention for Deep Multiple Instance Learning: Application to CT Intracranial Hemorrhage Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 32.
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,1.0,Introduction,"Images and text are inherently intertwined in clinical diagnosis and treatment. Having an automated approach that is able to answer questions based on images, giving insight to clinicians and patients, can be a valuable asset. In such a medical Visual Question Answering (VQA) setting the common approach is to treat VQA as a multi-class classification problem solved by neural networks. Given a joint encoded representation of the image and question, the model classifies it into a predefined set of answers. Although these approaches yield good performance  We believe that a possible solution can be found in the generative capability of language models, since they are able to produce free text, instead of being limited to closed-set predictions. However, leveraging language models for solving open-ended medical VQA is limited due to several challenges, such as finding ways to properly communicate the visual features and letting such large-scale models be employed on small-sized medical VQA datasets. Inspired by recent image captioning models  Furthermore, large-scale language models can generalize across domains while keeping their weights frozen  In summary, we contribute in three major aspects: (i) We propose the first large-scale language model-based method for open-ended medical VQA. (ii) We adopt parameter-efficient tuning strategies for the language backbone, which gives us the ability to fine-tune a large model with a small dataset without the danger of overfitting. (iii) We demonstrate through extensive experiments on relevant benchmarks that our model yields strong open-ended VQA performance without the need for extensive computational resources."
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,2.0,Related Works,"To describe existing medical VQA methods, we make a distinction between classification methods and generative methods. The majority of methods are classification-based and make use of different types of encoders, such as CNNs or Transformers  Classification-Based VQA. We highlight a number of methods that showed good performance on current competitive medical VQA datasets. The Mixture Enhanced Visual Features (MEVF)  Open-Ended VQA. MedFuseNet "
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,3.1,Problem Statement,"Given an input image I and an input question in natural language Q, our method aims to sequentially generate an answer A = {A 0 , A 1 , ..., A N } composed of N tokens, by conditioning on both inputs. From a model definition perspective, we aim to find the optimal parameters θ * for a model by maximizing the conditional log-likelihood as follows: (1)"
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,3.2,Model Architecture,"Our VQA model is designed as an encoder-decoder architecture, with a twostream encoder and a language model (LM) as a decoder, as illustrated in Fig. "
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,,Vision Encoding Stream.,"For encoding the image, we employ a pre-trained vision encoder to extract visual features {x 1 , x 2 ...x x }. To use these features as input to the decoder, they should be mapped into the latent space of the language decoder. Following "
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,,Language Encoding Stream.,"Regarding the encoding of the textual part, firstly we utilize a standard tokenization process to obtain a sequence of tokens, both for the question Q = {q 1 , q 2 ...q q } ∈ R q ×e and answer A = {a 1 , a 2 ...a a } ∈ R a ×e . This is followed by embedding the tokens using the embedding function of a pre-trained language model. Prompt Structure. To create a structured prompt, following existing QA methods using language models  The parameters of the language model are initialized from a pre-trained model, which has been previously pre-trained on huge web-collected datasets. "
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,3.3,Parameter-Efficient Strategies for Fine-Tuning the Language Model,"Standard fine-tuning of language models can hurt the generalization capabilities of the model, especially if small, domain-specific datasets are used as in our case. Therefore, we consider four different parameter-efficient strategies that adapt the attention blocks of language models, as illustrated in Fig.  Frozen Method: the parameters of the language model are kept entirely frozen during training, following "
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,4.0,Experimental Setup,Datasets. The three datasets used for the evaluation of our method are Slake 
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,5.0,Results,"Benefits of Parameter-Efficient Fine-Tuning. The evaluation of our method across various language models and fine-tuning settings in Table  Comparison Between Standard and Medical LMs. Using a language model pre-trained on a general text corpus, such as GPT2  Benefit of Open-Ended Answer Generation. Our method is performing significantly better on the open-set answering, in comparison to classificationbased methods, as shown in Table  In Fig.  Effect of Using Different Prompt Structures. We also investigate the influence of the prompt structure on the overall performance, demonstrated in Table  Interestingly, the model is sensitive the order of the elements in the prompt, as the swapping of the question embeddings and the visual prefix yields decreases the performance. The reason for this is that the language model conveys lower to no importance the visual information if it is located in front of the question. In this situation the language model basically generates blind answers. This highlights the importance of prompt structure."
Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models,6.0,Conclusion,"In this paper, we propose a new perspective on medical VQA. We are using generative language models to generate answers in an open-ended manner, instead of performing a closed-set classification. Additionally, by using various parameterefficient fine-tuning strategies we are able to use language models with billions of parameters, even though dataset sizes in this domain are small. This leads to excellent performance compared to classification-based methods. In conclusion, our approach offers a more accurate and efficient solution for medical VQA."
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,1.0,Introduction,Neurodegenerative diseases like Alzheimer's disease (AD) are the main causes of cognitive impairment and earlier neuropathological alterations  thickness change is an essential feature that can quantify the potential brain atrophy in these diseases 
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,2.0,Methods,"In this section, we will present the technical details of our method, which involve three main parts: surface segmentation, similarity metric and personalized templates. By integrating these 3 parts, we demonstrate the effectiveness of our method through normality assessment experiments on patients with mild cognitive impairment (MCI) and Alzheimer's disease (AD) on data from Alzheimer Disease Neuroimaging Initiative(ADNI) "
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,2.1,Brain Surface Segmentation,"We employ FreeSurfer to extract 3D surface mesh from T1-weighted Volumetric MRI data.  The segmented patches are generated for gyral and sulcal sub meshes separately, which is shown in Fig.  The Geo denotes the geodesic distance between v and b. The distance transform computation is implemented using fast marching algorithm.  Each Voronoi cell defines a segmented patch. Since the surface is not perfectly smooth, the mesh is usually over segmented. To address this issue, we employ a region grow strategy to merge patches. Starting from one patch, we compute the geodesic distances between its center vertex and those of all neighboring patches. If the computed distance falls below a specified threshold, the two patches are merged into a single patch. In the event that no neighboring patch meets this criterion, we proceed to an unvisited patch. The iteration repeats until all patches are visited. The threshold is empirically set as twice the mean edge length on the mesh. Patches are labeled as gyral or sulcal based on the sub-mesh they're generated from."
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,2.2,Patch Similarity Metric,"We employ shape similarity metrics to identify comparable patches. Histogrambased shape descriptors are used for topological similarity. Specifically, a histogram H is constructed for a function F defined on all vertices in a patch. The input values for histogram generation are normalized by the root mean square (RMS) of the function. The maximum bin value of the histogram is set as the maximum feature value, and the number of bins is fixed at 20 to ensure comparability between histograms. The values in each bin are normalized with respect to the total number of vertices in the patch, ensuring that the histogram values add up to 1. This normalization is done to avoid the scaling problem, as patches with similar topography can have different sizes. The distance transform, described in previous section, and shape index are used for constructing histogram, H dt and H si . Distance transform is used since the distance to boundary encodes information about the gyral/sulcal patches,such as width of the gyrus. For example, a crest gyral structure and a plateau one have different distance to boundary distribution. For topological comparison, chi square distance is computed as similarity. Given histograms H i and H j , the chi square distance is defined below: H i [k] denotes the value in the kth bin of H i . For any two patches,i and j, we can define a distance vector The final similarity score S(i, j) is defined as , where w is a weighting vector."
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,2.3,Personalized Template Set,"To mitigate the impact of inter-subject variability in brain structure folding patterns, a personalized set of templates is chosen for each patch. This involves selecting a cohort of N cognitively normal (CN) subjects as the templates for comparison, while a separate group of M mild cognitive impairment (MCI) and M Alzheimer's disease (AD) subjects is selected for testing purposes. For all subjects in both the template and test groups, we first obtain the sets T temp and T test of segmented patches as described in the previous section. From the output of Freesurfer, we compute the vertex-wise spherical registered coordinates (stored in the sphere.reg file) and cortical thickness (stored in the .thickness file) as SR(v) and CT (v), respectively, where v denotes a vertex  where |P i | is the cardinality of P i . Then, for each patch P i ∈ T test , we compute the nearest 200 patches P 1...200 ⊂ T temp that has the same gyral/sulcal label as query patch in terms of ||P SR i -P SR j ||, where P j ∈ T test . This is to leverage the location information of the patch with respect to the whole hemisphere, as matched patches should be from the neighborhood region of the query patch and label restriction is to reduce mismatch of gyral/sulcal regions. From P 1...200 , we select the 50 most similar patches based on their similarity scores. Specifically, we compute the similarity score S(i, j) for patch P i and patch P j ∈ P 1...200 , and choose the top 50 patches in P 1...200 with the largest similarity scores to form a personalized template set T Pi for patch P i . We repeat this process for each patch in the test set. In the next step, we define the normality metric used to measure the normality of a patch. For each patch P i and its personalized template set T Pi , we first compute the mean (μ) and standard deviation (std) of the patch cortical thickness for all patches in T Pi . We then define the z-score of P i as Z(P |. This z-score is used as a normality measure because if P i is abnormal, it will have a larger z-score and vice versa. Finally, we use this z-score for each patch in the test subject set to conduct normality assessment experiments. To investigate the discriminative power of our patch-based personalized matching approach, we randomly selected 200 cognitive normal (CN) subjects as templates, 100 mild cognitive impairment (MCI) and 100 Alzheimer's disease (AD) subjects from the ADNI dataset to compare our method to Freesurfer's Spherical Registration. The experiments are limited to the left hemisphere for computing cost. Specifically, we used the sphere.reg file output from Freesurfer, which was decimated to 50000 vertices. For each query subject's surface, we computed the spherical registration of each vertex to a template surface by finding the closest vertex in the template's vertices based on Euclidean distance on the unit sphere space, as described by the sphere.reg file. We then computed a patch-based zscore for each patch P i using the thickness of P i and the spherically matched vertices from each template subject, which we denote as the sphere-based zscore. Alternatively, we matched patches as previously described in the Method section, and computed a patch-based z-score for each patch in the MCI and AD subject sets."
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,3.1,Normality Assessment Experiments,"For each MCI and AD subject, the mean sphere-based z-score and mean patch-based z-score are computed by averaging the respective scores of patches for 1 subject. These mean z-scores represent how abnormal a subject is. The resulting distributions are shown in Fig. "
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,3.2,"CN vs MCI, AD Prediction Experiment","In this experiment, we conduct CN vs MCI and CN vs AD prediction accuracy test. Similar to previous experiment, we choose 200 CN subjects as templates and another 100 CN subjects, 100 MCI subjects and 100 AD subjects for testing. Afterwards, sphere-based z-scores and patch-based z-scores are computed for all patches in CN, MCI and AD subjects from 200 template subjects. For training feature, we use the mean z-score of patches in each ROI in one subject, which is a length-34 vector for 34 ROIs from Freesurfer(.aparc.annot) "
Personalized Patch-Based Normality Assessment of Brain Atrophy in Alzheimer’s Disease,4.0,Conclusion,"In this paper, we proposed a novel personalized patch-based method for brain atrophy detection by matching segmented patches based on gyral/sulcal label, location, shape similarity and constructing personalized template set for abnormal detection. Through normality assessment and MCI AD prediction experiments, the method is shown to be more effective at detecting brain atrophy."
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,1.0,Introduction,"Coronavirus disease 2019 (COVID-19) has been a highly infectious viral disease, that can affect people of all ages, with a persistently high incidence after the outbreak in 2019  During the inference stage, modalities can be incomplete amongst some test samples, which is known as incomplete multimodal learning "
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,2.1,Architecture Overview,"We propose a model that can detect the COVID-19 pneumonia status from incomplete multi-modalities of CT scans and X-ray images. Specifically, CT scans and X-ray images are first embedded using convolutional layers and then processed by Transformer layers to obtain the global feature correlations. Then, a novel feature fusion layer can simulate incomplete modalities in the latent space while learning to fuse the features. Finally, the predictions are made using a ResNet based classification model followed by a learnable MLP layer."
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,2.2,Feature Fusion Layer,"The objective of this layer is to combine features from different modalities, which have varying dimensions. To achieve this, we need to reduce the threedimensional CT data to two-dimensional by convolutional layers before fusing them with two-dimensional X-ray data. We then reshape the data into smaller patches and apply a Transformer encoder to it, resulting in a two-dimensional matrix of the desired size. In this study, we investigate the data fusion at two stages, namely the early fusion and late fusion "
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,2.3,Feature Matrix Dropout Layer,"This layer helps regularize the model towards learning a modality agnostic representation. Specifically, during the feature fusion layer, random features are dropped from the feature matrix ∈ R X,Y  where R T (•) is the round function that converts all values of M δ into 0 if they are under the threshold T and 1 otherwise. Finally the obtained matrix is interpolated or upsampled by a given scale N to match the size of the feature matrices where F D represents the final feature map after the patch dropout, F M is the initial feature map, I M (•) represents a nearest interpolation and (•) (•) represents an element-wise matrix multiplication. This gives us the final feature map F D , which has a similar structure to F M but with some parts of size N × N converted to 0."
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,2.4,Transformer Layer,"Transformers helps finding the different dependencies between the different modalities making use of their attention-based nature. Therefore, in this paper we will make use of the benefits that the transformers offers by implementing a ViT based transformer layer  where σ(•) represents the softmax function, d represents the size of each one of the heads, and W 0 denotes a value embedding. Meanwhile, Q, K, V ∈ R X,d represent the Query, key and embedding respectively. To constitute the ViT the values of each of the SA are concatenated forming the Multi-Headed Self Attention Layer (MSA) to afterward be normalized by a layer normalization. Finally, a Multi-Layer Perceptron (MLP) is applied to give non-linearity to the data, obtaining where A k-1 is the previous transformer layer, LN(•) is a Layer Normalization. In the vanilla ViT a final MLP is introduced to obtain the probabilities of each class however, in this paper the transformer layer is only used for feature extraction, therefore, the final MLP is deleted."
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,2.5,Dual X-Ray Attention,The Dual X-Ray attention block main idea is to find the different dependencies between the two input X-Ray images. Transformers have showed great results when looking for dependencies 
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,3.1,Dataset,We leverage images from the BIMCV-COVID19 dataset 
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,3.2,Implementation Details,"This framework is implemented with Pytorch 1.13.0 using an NVIDIA A10 GPU with 24 GB of VRAM. The input size is 250 × 250 × 200 for the CT scan and 2048 × 2048 with a batch size of 4 for the X-Ray. For the proposed model, we applied Adam optimizer with an cosine annealing scheduler of an initial learning rate 1e-5, and 100 epochs for training. The data partition for training, validation, and testing is 80%, 10%, and 10% on patient level."
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,3.3,Results,"We use the following metrics to evaluate the performance of the binary classification model: the AUC score, Recall and Precision. To obtain the values on the Table "
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,3.4,Ablation Study,"We show the classification performance on Table  The saliency maps are visualized to pinpoint the diagnostic areas in a CT or X-ray image. Clinically, the saliency maps are helpful to assist radiologists. In the Fig. "
COVID-19 Pneumonia Classification with Transformer from Incomplete Modalities,4.0,Conclusion,"In this paper, we propose a novel multi-modality framework for COVID-19 pneumonia image diagnosis. A Dual fusion layer is introduced to help establish the dependencies between the different input modalities, as well as to take in different dimensionality inputs. The proposed Dual X-Ray attention layer makes it possible to effectively extract dependencies from the two X-Ray images focusing on the salient features between multi-modality images, whereas irrelevant features are ignored. The feature deletion layer helps to regularize the model dropping random features and improving the generalization of the model. Consequently, we provide the possibility to use one modality of CT or X-Ray for COVID-19 pneumonia diagnosis. Moreover, this model has the potential to be applied to other chest abnormalities in clinical practice."
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,1.0,Introduction,"Assessing vascular lumen and topology is among the most critical tasks for timely diagnosis of acute cerebrovascular disease, including the detection of vessel occlusions (e.g., by left-right hemisphere comparisons) and the assessment of redundant blood flow paths via the communicating arteries  However, this circular structure cannot be properly unfolded with the common Curved Planar Reformation (CPR) technique  In computer graphics, disk-like mesh parameterization  This work aims to generate a complete textbook-style vessel map of the cerebral vasculature along the CoW to generate a standardized overview image. Building on the ARAP algorithm, we propose CeVasMap (Cerebral Vasculature Mapping): a method to unfold circular structures which can be flexibly extended with peripheral vessels, either by including them in the unfolding directly or attaching them individually. This results in locally restricted distortions, retain-ing curvature information and keeping most of the image distortion-free. Given labeled centerlines, we create a smooth initial mesh with optimal viewing direction dependent on the vessels' principal components and deform it as rigidly as possible to jointly display all vessels of interest. We provide a comprehensive vessel-level evaluation by calculating and visualizing the distortions resulting from the underlying 2D-3D vector field. "
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,2.1,Data,"We use a data set consisting of 30 CTA scans (Siemens Somatom Definition AS+) from stroke patients (63.3% males, 74, 5 ± 12, 5 years, 33.3% MCA stroke) with an average voxel spacing of 0.634 mm (in-plane) and 0.57 mm (axial). The brain vasculature is segmented and labeled "
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,2.2,Rationale,"The goal of this work is to generate a single 2D image which contains all vessels of interest jointly with the surrounding parenchyma. Hence, the task at hand is to find an unfolding transformation Φ( u) = x : R 2 → R 3 mapping from a 2D position u in the unfolded target image to its 3D CTA volume location x. The corresponding coordinate systems are illustrated in Fig.  Clinically useful images should be merged at the bifurcations to allow consistent path tracing. Additionally, anatomical properties such as vessel curvature should be preserved whereas strong distortions are to be avoided. The main concept of the proposed method is the generation of a joint mapping for the CoW. Further attached vessels can then be merged to it to form a complete overview at the cost of a higher distortion, see Sect. 2.3. For configurations that lead to strong distortions, outer vessels are unfolded individually and attached to the main component. Their arrangement is inspired by the common textbook CoW schematic for better orientation, see Fig. "
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,2.3,Measuring Distortions,"An image resulting from a transformation Φ lacks a consistent pixel spacing, the specified one is simply an average from the covered 3D distances, hence pixel distances deviating from that average are distorted. We compute distortion metrics on the vector field Φ to guide the decision process during the merging and for the evaluation. Since conventional metrics to find sources and sinks such as the Jacobian determinant can only be calculated for equidimensional Φs, we compute a scalar metric d per pixel u = (u, v) by deriving the transformation w.r.t. both image directions ∂Φ( u)  ∂ u ∈ R 3×2 and applying the Frobenius norm, When sampling a 2D image of size N × M with isotropic sampling μ, a pixel gradient of 1 in one image direction implies a sampling in 3D with μ, meaning no distortion. To normalize distortion-free values of"
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,2.4,ARAP Vessel Unfolding,"CPR unfolding can lead to displeasing results, e.g., at highly curved segments perpendicular to image read-out direction, impairing the quality of the displayed lumen and parenchyma in the whole image row, see bottom right in Fig.  with N (i) being the 1-step neighborhood of i. By minimizing the norm of the rigid error on the right-hand side, one can approximate a rigid transformation, since actual rigidness is infeasible. For the derivation of the rotation R i and the weights w ij and w i , we refer to Sorkine et al.  On one hand, we lack a fully defined object surface due to our sparse vessel structure. On the other hand, staying close to a meshed plane would mimic multiplanar reformation images and help with the orientation. Since this structure would lead to strong distortion due to its simplicity compared to the centerlines, we define our read-out mesh in a two-step approach. First, we use the principal component vectors stored column-wise in A ∈ R 3×3 , using column a 3 as the optimal viewing direction by projecting the points c j from the set of centerline points j ∈ C along the two maximum principal components to acquire two 1D distributions k 1,2 , see Fig.  as vertex positions where u and v are sampled uniformly from and c is the centerline mean position. To avoid escalating border regions, the last value of q is set to 0 outside of the centerline bounding box. Next, S init is deformed to contain all vessel points following the ARAP minimization in Eq. 2 by restraining a subset of p i to c j . Volume intensities can be read out at the vertex positions of the resulting mesh S, forming the target vector field Φ CeVasMap ( u) = p beginning from the unfolded image."
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,2.5,Merging and Image Assembly,"In principle, the described approach can be applied to all vessels of interest at once, introducing high distortion at nearly perpendicular bifurcations. Due to a varying amount of contrast agent; scan, segmentation or labeling quality; or occlusions in stroke, the vessels can differ greatly in length. Long, curved structures influence the unfolding more severely since S init has to adapt to more points c j , increasing the initial fitting error. In such cases, individual vessels are unfolded separately to maintain sufficient image quality and included in the cerebral vessel map at the position and rotation angle according to the textbook schematic, see Fig. "
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,2.6,Evaluation,"The distortion by the proposed transformation and the baseline is evaluated quantitatively and qualitatively using the metrics from 2.3. The baseline method is the CPR transformation Φ CPR ( u) following Kanitsar et al.  The pixel-wise metric d uv is used to visually inspect the results. Quantitatively, distortion is assessed showing the mean-like metric D as a distribution over all 30 patients. Additionally, the median of |d uv | per vessel unfolding averaged over all patients is shown to also investigate the maximal distortion of the lesser affected half of the pixels. To assess the expected distortion increase caused by merging, we split the cerebral vessels into two groups: For the CoW vessel segments (merged by default), D and median metric are calculated over the image patches of each vessel, both for individual unfoldings and extracted from the merging. Similarly, the D-distribution and frequency of successfully merging outer vessel pairs to the unfolded circle structure is reported when setting an upper distortion threshold of D < 25% for the vessel of interest. To avoid that simultaneously merged vessel pairs influence each other or the circle, we compute S init only from the circle and then merge vessel pairs using ARAP. This also eliminates the need to evaluate distortions for all possible vessel combinations.  "
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,3.0,Results,Quantitative metrics per vessel (pair) are presented in Fig.  When merging the CoW (Fig. 
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,4.0,Discussion and Conclusion,"We present CeVasMap, a flexible method to generate textbook-style vessel maps of the main cerebral arteries, including an unfolding technique capable of displaying circular structures infeasible for the widely used CPR approach. To unfold individual vessels or vessel groups, a read-out mesh is generated by fitting splines to centerline projections along the principal components. This initial mesh is deformed using the ARAP algorithm to display the full vessel lumen. In our approach, we suggest jointly mapping the inner circular structure of the CoW by default and extending it with outer vessels depending on the task at hand and the introduced distortion. For this purpose, a gradient-based distortion metric is calculated on the 2D-3D unfolding vector field, which also facilitates quantitative and qualitative assessment of the mapping quality. The method results in higher standard deviations and slightly higher mean values of the proposed gradient-based distortion metric D compared to the best possible CPR angle. However, CeVasMap generally achieves lower median values (≤10 µm/mm for the CoW vessels, and mostly ≤50 µm/mm for the longer outer segments), meaning larger parts of the image are distortion-free. Even when merging the complete CoW (average distortion median of 65 µm/mm), our method can still compete with the individual CPR unfolding using less favorable angles. When merged with the CoW, distortion for outer vessels, especially those nearly perpendicular to the CoW plane, is quite high, and only the MCAs, PCAs and A2 arteries achieve satisfying results due to their orientation. For instance, the ICA could only be merged in 10.2% while the MCA has a 88.3% success rate with our proposed threshold. To assess only distortions close to the structures of interest, i.e., the vessels, we evaluated our metrics within a narrow corridor around them. When generating full cerebrovascular unfoldings, one would rather unfold more parenchyma for a hole-free image (cf. Fig.  Rotating the unfolded views as often done in CPR is in theory feasible with our approach-by rotating the principal components A-but is not expected to give pleasing results as ARAP target points are rotated out-of-plane. Insufficient unfolding length or asymmetric manifestations of vessel pairs can occur due to segmentation or labeling inconsistencies. Nevertheless, our method is more robust against segmentation or labeling errors causing strongly curved, incorrect or fistula-like pathways. In contrast to CPR, our method is robust against varying centerline point distances and inconsistent centerline point ordering. The flexible nature of our approach can also be used to focus on the arteries of interest by merging them selectively and keeping the remaining vessels separate but arranged intuitively inspired by textbook presentations, effectively reducing overall distortion. Such a selective merging could also be done interactively as, compared to higher computation times for segmentation and labeling, most unfolding steps are parallelizable and can be computed in few seconds. The clinical applications of a cerebrovascular overview map are manifold. The configuration of the CoW and surrounding major vessels is visualized, giving insight into the collateral blood supply or supporting contralateral comparisons for time-critical stroke detection while being able to view the vessel lumen and its pathologies at a glance. To evaluate challenging real-world scenarios, this work is specifically tested using stroke data as it one of the main reasons for topological impairments of the CoW. This unique representation (compared to volume renderings or slice images) of the cerebrovascular system can be useful for diagnostic reports, comparing patients, disease tracking or to simply to mark locations of possible findings. The maps are also helpful for navigation within the volume as the transformation allows us to readily display multiplanar reformations of the original volume centered at 3D positions corresponding to (manually selected) positions in the unfolded overview image (""picking""). Finally, this condensed lower-dimensional representation could also be beneficial for training downstream deep learning models more efficiently."
Flexible Unfolding of Circular Structures for Rendering Textbook-Style Cerebrovascular Maps,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_71.
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,1.0,Introduction,"Gastric cancer (GC) is the third leading cause of cancer-related deaths worldwide  Non-contrast CT is a commonly used imaging protocol for various clinical purposes. It is a non-invasive, relatively low-cost, and safe procedure that exposes patients to less radiation dose and does not require the use of contrast injection that may cause serious side effects (compared to multi-phase contrastenhanced CT). With recent advances in AI, opportunistic screening of diseases using non-contrast CT during routine clinical care performed for other clinical indications, such as lung and colorectal cancer screening, presents an attractive approach to early detect treatable and preventable diseases  In this paper, we propose a novel approach for detecting gastric cancer on non-contrast CT scans. Unlike the conventional ""segmentation for classification"" methods that directly employ segmentation networks, we developed a clusterinduced Mask Transformer that performs segmentation and global classification simultaneously. Given the high variability in shape and texture of gastric cancer, we encode these features into learnable clusters and utilize cluster analysis during inference. By incorporating self-attention layers for global context modeling, our model can leverage both local and global cues for accurate detection. In our experiments, the proposed approach outperforms nnUNet "
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,2.0,Related Work,Automated Cancer Detection. Researchers have explored automated tumor detection techniques on endoscopic 
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,,Mask Transformers.,Recent studies have used Transformers for natural and medical image segmentation 
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,3.0,Methods,"Problem Formulation. Given a non-contrast CT scan, cancer screening is a binary classification with two classes as L = {0, 1}, where 0 stands for""normal"" and 1 for""GC"" (gastric cancer). The entire dataset is denoted by , where X i is the i-th non-contrast CT volume, with Y i being the voxel-wise label map of the same size as X i and K channels. Here, K = 3 represents the background, stomach, and GC tumor. P i ∈ L is the class label of the image, confirmed by pathology, radiology, or clinical records. In the testing phase, only X i is given, and our goal is to predict a class label for X i ."
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,,Knowledge Transfer from Contrast-Enhanced to Non-contrast CT.,"To address difficulties with tumor annotation on non-contrast CTs, the radiologists start by annotating a voxel-wise tumor mask on the contrast-enhanced CT, referring to clinical and endoscopy reports as needed. DEEDs  Specifically, given image X ∈ R H×W ×D , annotation Y ∈ R K×HW D , and patient class P ∈ L, our model consists of three components: 1) a CNN backbone to extract its pixel-wise features F ∈ R C×HW D (Fig.  where c and p stand for query and pixel features, Q c , K p , V p represent linearly projected query, key, and value. We adopt cluster-wise argmax from KMax-DeepLab  We further interpret the object queries as cluster centers from a cluster analysis perspective. All the pixels in the convolutional feature map are assigned to different clusters based on these centers. The assignment of clusters (a.k.a. mask prediction) M ∈ R N ×HW D is computed as the cluster-wise softmax function over the matrix product between the cluster centers C and pixel-wise feature matrix F, i.e., The final segmentation logits Z ∈ R K×HW D are obtained by aggregating the pixels within each cluster according to cluster-wise classification, which treats pixels within a cluster as a whole. The aggregation of pixels is achieved by Z = C K M, where the cluster-wise classification C K is represented by an MLP that projects the cluster centers C to K channels (the number of segmentation classes). The learned cluster centers possess high-level semantics with both intercluster discrepancy and intra-cluster similarity for effective classification. Rather than directly classifying the final feature map, we first generate the clusterpath feature vector by taking the channel-wise average of cluster centers C = Additionally, to enhance the consistency between the segmentation and classification outputs, we apply global max pooling to cluster assignments R to obtain the pixel-path feature vector R ∈ R N . This establishes a direct connection between classification features and segmentation predictions. Finally, we concatenate these two feature vectors to obtain the final feature and project it onto the classification prediction P ∈ R 2 via a two-layer MLP. The overall training objective is formulated as, where the segmentation loss L seg (•, •) is a combination of Dice and cross entropy losses, and the classification loss L cls (•, •) is cross entropy loss."
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,4.1,Experimental Setup,"Dataset and Ground Truth. Our study analyzed a dataset of CT scans collected from Guangdong Province People's Hospital between years 2018 and 2020, with 2,139 patients consisting of 787 gastric cancer and 1,352 normal cases. We used the latest patients in the second half of 2020 as a hold-out test set, resulting in a training set of 687 gastric cancer and 1,204 normal cases, and a test set of 100 gastric cancer and 148 normal cases. We randomly selected 20% of the training data as an internal validation set. To further evaluate specificity in a larger population, we collected an external test set of 903 normal cases from Shengjing Hospital. Cancer cases were confirmed through endoscopy (and pathology) reports, while normal cases were confirmed by radiology reports and a two-year follow-up. All patients underwent multi-phase CTs with a median spacing of 0.75 × 0.75 × 5.0 mm and an average size of (512, 512, 108) voxel. Tumors were annotated on the venous phase by an experienced radiologist specializing in gastric imaging using CTLabeler  Implementation Details. We resampled each CT volume to the median spacing while normalizing it to have zero mean and unit variance. During training, we cropped the 3D bounding box of the stomach and added a small margin of "
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,,Evaluation Metrics and Reader Study.,"For the binary classification, model performance is evaluated using area under ROC curve (AUC), sensitivity (Sens.), and specificity (Spec.). And successful localization of the tumors is considered when the overlap between the segmentation mask generated by the model and the ground truth is greater than 0.01, measured by the Dice score. A reader study was conducted with two experienced radiologists, one from Guangdong Province People's Hospital with 20 years of experience and the other from The First Affiliated Hospital of Zhejiang University with 9 years of experience in gastric imaging. The readers were given 248 non-contrast CT scans from the test set and asked to provide a binary decision for each scan, indicating whether the scan showed gastric cancer. No patient information or records were provided to the readers. Readers were informed that the dataset might contain more tumor cases than the standard prevalence observed in screening, but the proportion of case types was not disclosed. Readers used ITK-SNAP "
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,4.2,Results,Our method Outperforms Baselines. Our method outperforms three baselines (Table 
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,,AI Models Surpass Experienced Radiologists on Non-contrast CT Scans.,As shown in Fig. 
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,,Subgroup Analysis.,In Table  Comparison with Established Screening Tools. Our method surpasses or performs on par with established screening tools 
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,5.0,Conclusion,"We propose a novel Cluster-induced Mask Transformer for gastric cancer detection on non-contrast CT scans. Our approach outperforms strong baselines and experienced radiologists. Compared to other screening methods, such as blood tests, endoscopy, upper-gastrointestinal series, and ME-NBI, our approach is non-invasive, cost-effective, safe, and more accurate for detecting early-stage tumors. The robust performance of our approach demonstrates its potential for opportunistic screening of gastric cancer in the general population."
Self-supervised Polyp Re-identification in Colonoscopy,1.0,Introduction,"Optical colonoscopy is the standard of care screening procedure for the prevention and early detection of colorectal cancer (CRC). The primary goal of a screening colonoscopy is polyp detection and preventive removal. It is well known that many polyps go unnoticed during colonoscopy  One may notice that the described task generally falls into the category of the well known multiple object tracking (MOT) problem  A recently published method  In this work we propose an alternative approach that allows polyp detections grouping over an extended period of time (up to 10 min), relaxing the spatiotemporal proximity limitation. It involves two steps: (I) a short-term multi-object tracking, which forms initial, relatively short tracklets, followed by (II) a longerterm tracklets grouping by appearance-based polyp re-identification (ReID). As the first step can be done by any generic multiple object tracking algorithm (e.g. we use a tracking by detection method  To avoid manual data annotation, which is extremely ineffective in our case, we turn to self-supervision and adapt the widely used contrastive learning approach  As tracklet re-identification is a sequence-to-sequence matching problem, the standard solution is comparing sequences element-wise and then aggregating the per-element comparisons, e.g. by averaging or max/min pooling  We extensively test the proposed method on hundreds of colonoscopy videos and evaluate the contribution of method components using an ablation study. Finally, we demonstrate the effectiveness of the proposed ReID method for improving the accuracy of polyp characterization (CADx). To summarize, the three main contributions of the paper are: -An adaptation of contrastive learning to video input for the purpose of appearance based object tracking. -An early fusion, joint multi-view object representation for ReID, based on transformer networks. -The application of polyp ReID to boost the polyp CADx performance."
Self-supervised Polyp Re-identification in Colonoscopy,2.0,Methods,"This work assumes the availability of an automatic polyp detector. Quite a few highly accurate polyp detectors were recently reported  As briefly mentioned above, the proposed approach starts with an initial grouping of polyp detections using an off-the-shelf multiple object tracking algorithm. Such a tracker is expected to track polyps through consecutive frames as long as they do not leave the camera field of view, forming disjoint, time separated polyp tracklets. In this work we use the ByteTrack  The resulting tracklets are typically relatively short, and there are quite a few tracklets corresponding to the same polyp. To improve the result, we propose an Appearance-based Polyp Re-Identification (ReID), which groups multiple disjoint tracklets by their visual appearance into a joint tracklet, associated with a single polyp. In what follows we describe in detail the proposed ReID component. As stated above, the objective of ReID is to ascertain whether two timeseparated, disjoint tracklets belong to the same polyp. To this end we seek a tracklet representation that allows measuring visual similarity between tracklets. The two basic alternatives are either a single representation for the whole tracklet, or a sequence of single-frame representations for each tracklet frame. We will consider both options below."
Self-supervised Polyp Re-identification in Colonoscopy,2.1,Single-Frame Representation for ReID,"To generate a single frame representation we train an embedding model that maps a polyp image into a latent space, s.t. the vectors of different views of the same polyp are placed closer, and of different polyps away from each other  A straightforward approach to train such model is supervised learning, which requires forming a large collection of polyp image pairs, manually labeled as same/not same polyp  Hence, we turn to SimCLR  One caveat of SimCLR is the difficulty to generate augmentations beneficial for the learning process  Instead of customizing the augmentations to fit the colonoscopy setup, we leverage the temporal nature of videos, and take different polyp views from the same tracklet as positive samples (see Fig.  where sim is the dot product and τ is the temperature parameter  Tracklets represented as sequences of per-frame embeddings can be matched by computing pair-wise distances between frames, followed by an aggregatione.g. min/max/mean distance "
Self-supervised Polyp Re-identification in Colonoscopy,2.2,Multi-view Tracklet Representation for ReID,"As discussed earlier, an alternative to the single frame approach, is a unified representation for the whole tracklet. A commonly used practice is to compute single frame embedding (for each view) and fuse them  To achieve this, we employ a transformer network "
Self-supervised Polyp Re-identification in Colonoscopy,3.0,Experiments,This section includes two parts. The first provides a stand-alone evaluation of the proposed ReID method. The second assesses the impact of ReID on polyp classification accuracy.
Self-supervised Polyp Re-identification in Colonoscopy,3.1,ReID Standalone Evaluation,"Dataset. We use 22,283 colonoscopy videos, split into training (21,737) and test (546) sets. These recordings were captured from standard colonoscopy procedures conducted at six medical centers during the period of 2019 to 2022. The average length of the recorded procedures is 15 min, with a median duration of 13 min. For training, we automatically generated polyp tracklets using automatic polyp detection and tracking as described in Sect. 2. The tracking algorithm might produce short and uninformative tracklets as well as outliers. The following clean up steps were performed on the training set: we filtered out tracklets shorter than 1 s or having less than 15 high confidence detections, as defined in  Training. We utilize ResNet50V2  We first train the single frame encoder and use its weights to initialize the single frame module of the multi-view encoder. Due to memory limitations, we use 8 views per tracklet during training, resulting in 1024 * 8 = 8192 images per training step. The model was trained for 5,000 steps using cloud v3 TPUs with 16 cores. The single frame encoder has 24M parameters, and the multi-view encoder adds an additional 1M parameters. Evaluation. We start by comparing various ReID techniques described in Sect. 2. Namely, we evaluate the accuracy of tracklet re-identification using: (a) single-frame representation with pairwise distances aggregation by Min / Max / Mean functions  In addition, we evaluate the effectiveness of ReID by measuring the average polyp fragmentation rate (FR), defined as the average number of tracklets polyps are split into. Obviously, lower fragmentation rate means better result (with the best fragmentation of 1), but it may come at the expense of wrong tracklet matching (false positive). We measure the fragmentation rate at the operating point of 5% false positive rate. The number of polyp fragments is determined by matching tracklets to manually annotated polyps and counting  "
Self-supervised Polyp Re-identification in Colonoscopy,3.2,ReID for CADx,"In this section, we investigate the potential benefits of using polyp ReID as part of a CADx system. Polyp CADx aims to assist physicians to figure out, in real time, during the procedure, whether the detected polyp is an adenoma. Most reported CADx systems compute a classification score for each frame, and aggregate scores from multiple frames to determine the final polyp classification. Grouping polyp frames into a tracklet, to be fed into the CADx, is usually done by a spatio-temporal tracker  Here, we investigate if the proposed ReID model, used to group disjoint tracklets of the same polyp, can increase the accuracy of CADx. Data. We use 3290 colonoscopy videos split into train, validation, and test sets (2666, 296, and 328 videos respectively). The videos are processed by a polyp detector and tracker to form polyp tracklets. The tracklets are then manually grouped together to build a single sequence for every polyp. Each polyp is annotated by a certified gastroenterologist as either adenoma or non-adenoma. CADx. We trained a simple image classification CNN, composed of a MobileNet  For evaluation, we used the model to predict the classification score for each frame and aggregated the scores using soft voting to achieve the final prediction for each tracklet. Evaluation. To assess the contribution of the ReID to polyp classification, we compare the CADx results on the test set, while using different grouping methods to merge multiple polyp detections into tracklets. The 3 evaluated methods are: (1) manual annotation (2) grouping by tracking, and (3) grouping by ReID. The manually annotated tracklets -the ground truth (GT) -are the longest sequences, containing all frames of each polyp in the test set. In grouping by tracking, we use tracklets generated by the spatio-temporal tracking algorithm "
Self-supervised Polyp Re-identification in Colonoscopy,4.0,Conclusions,"In this study we present a novel multi-view self-supervised learning method for learning informative representations of a sequence of video frames. By jointly encoding multiple views of the same object, we get more discriminative features in comparison to traditional embedding fusion techniques. This approach can be used to group disjoint tracklets generated by a spatio-temporal tracking algorithm based on their appearance, by measuring the similarity between tracklets representations. Its applicability to medical contexts is of particular relevance, as medical data annotation often requires specific expertise and may be costly and time consuming. We use this method to train a polyp re-identification model (ReID) from large unlabeled data, and show that using the ReID model as part of a CADx system enhances the performance of polyp classification. There are some limitations however in identifying polyps based on their appearance, as it may be changed drastically during the procedure (for example, during resection). In future work we may examine the use of ReID for additional medical applications, such as listing detected polyps in an automatic report, bookmarking of specific areas of the colon during the procedure, and calculation of clinical metrics such as Polyp Detection Rate and Polyps Per Colonoscopy."
Self-supervised Polyp Re-identification in Colonoscopy,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_57.
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,1.0,Introduction,"Punctate white matter lesion (PWML) is a typical type of cerebral white matter injury in preterm infants, potentially leading to psychomotor developmental delay, motor delay, and cerebral palsy without timely treatment  Automated localization and delineation of PWML are practically challenging. This is mainly because that PWMLs are isolated small objects, with typically only dozens of voxels for a lesion and varying numbers of lesions across different subjects. Also, due to underlying immature myelination of infant brains  Counterfactual reasoning, explained by our task, studies how a real clinical brain image appearance (factual) changes in a hypothetical scenario (whether lesion exist or not). This idea has been applied as structural causal models (SCMs) in a deep learning way in recent years. At the theoretical level, Monteiro et al.  Overall, our DeepPWML is practically easy to implement, as the counterfactual part learns simple but effective linear manipulations, the tissue segmentation part can adopt any off-the-shelf networks, and the PWML segmentation part only needs a lightweight design. On a real-clinical dataset, our method led to a state-of-the-art performance in the infant PWML segmentation task."
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,2.0,Method,"As shown in Fig.  The high-resolution counterfactual maps (CF maps) and segmentation probability maps (SP maps) are further combined with the input patch to train a lightweight P-SEG for PWML segmentation. In the test stage, an input patch is first determined by the CLS module whether it is positive. Positive inputs will pass through the T-SEG, CMG, and P-SEG modules to get the PWML segmentation results. It is worth noting that the test patches are generated by sliding windows, and the overlapping results are averaged to get the final segmentation results for the image, which reduces the impact of incorrect classification of the CLS module. In our experiments, T-SEG used the voxel-wise Cross-Entropy Loss. CLS used the Categorical Cross-Entropy Loss, CMG combined the sparsity loss (L1 and L2 norms) with the classification loss. Finally, P-SEG used the Dice Loss. In the following subsections, we will introduce every module in our design."
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,2.1,Tissue Segmentation Module,"The task is to mark every pixel of the brain as cerebrospinal fluid (CSF), gray matter (GM), or white matter (WM). The choice of this module can be flexible, and there are many off-the-shelf architecture designs available. We adopt a simple Dense-Unet architecture "
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,2.2,Classification Module and Counterfactual Map Generator,"The CLS and the CMG are trained sequentially. The CLS is trained to determine whether the current patches have lesions. The CMG is a counterfactual reasoning step for the CLS. Based on the characteristic of PWML, CMG learns a simple linear sparse transform shown as the CF map. This map aims to offset the bright PWML pixels of the image patches, which are classified as positive, or seed PWML on the patches judged as negative. In other words, CMG is learning a residual activation map for conversion between control and PWML. We adopt the T-SEG module's encoder with two fully connected layers as the CLS module. Furthermore, the architecture of CMG is a simple U-net adding a ""switch"" state in its skip-connection parts according to the method of Oh et al.  The state of the ""switch"" is determined by the classifier's result on the current patch. If the judgement is positive, correspondingly, the ""switch"" status is 0. In this condition, the activated areas in the CF map should be where PWMLs exist. Then the pseudo patches in Fig.  The CMG module is summarised as follows: Firstly, PWML patches C P and control patches C N are fed to the encoder to obtain encoded representations F P and F N : Secondly, ""switch"" filled with zeros/ones with the same size as PWML/normal representations F P /F N are added to these representations and then pass through the decoder to obtain the CF maps M P /M N : Finally, the original patches C P /C N are added/subtracted to the CF maps M P /M N to yield the transformed patches C P / C P , which are classified by the CLS module as the opposite classes:"
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,2.3,PWML Segmentation Module,"The SP map includes the potential PWML existence, but also a lot of tissue segmentation uncertainty. The CF map directly shows the PWML location, but due to the accuracy of the CLS module, the CF map itself will also carry some false positives fault. If we synthesize the CF map, the SP map and the original input patches for appearance information, the best segmentation result can be achieved by allowing the network to verify and filter out each information in a learnable way. The P-SEG module is implemented as a lightweight variant of the Dense-Unet. Different simplified versions have been tested, with the results summarized in Sect. 3.2. After getting the PWML segmentation result, we use the tissue segmentation result to filter out PWMLs mis-segmented at the background and CSF. "
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,3.1,Dataset and Experimental Setting,"Dataset: Experiments were performed on a dataset with two groups (control and PWML), where control included 52 subjects without PWML observed, and PWML included 47 subjects with PWMLs. All infants in this study were born with gestational age (GA) between 28 to 40 weeks and scanned at postmenstrual age (PMA) between 37 to 42 weeks. Two neuroscientists manually labeled PWML areas and corrected tissue labels generated by iBeat  Experimental Setting: Our method was implemented using Tensorflow. All modules were trained and tested on an NVIDIA GeForce RTX 3060 GPU. We adopted Adam as the optimizer, with the learning rate varying from 0.001 to 0.00001 according to modules. The inputs were fixed-size patches (32 × 32 × 32) cut from the T1w images. The train/validation/test ratio was 0.7/0.15/0.15 and divided on subject-level. We didn't use any data augmentation during training. We used Dice, True Positive Rate (TPR), and Positive Predictive Value (PPV) to quantitatively evaluate the segmentation performance. "
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,3.2,Results,"First, the T-SEG module is trained using a fully supervised way. Its tissue segmentation accuracy on the test set is about 93% in terms of Dice. Second, the CLS and other modules are trained with PWML group data. We defined the input training patches' class labels by whether they contain PWMLs or not. In other words, if any patch has at least one lesion voxel, it is positive. The accuracy of the test set can reach around 90%. Third, we train the CMG module based on the well-trained and fixed CLS module. Finally, based on T-SEG and CMG, we train P-SEG. We combine the SP map, CF map, and T1w image in a channel-wise way as the input of the module without any additional processing of these features. Comparison Results: We compared our method with the state-of-the-art method  Ablation Studies: We further evaluated the effectiveness of our design by comparing the results of the pipelines with and without SP maps and CF maps. The ablation results are shown in the last six rows of Table  By comparing ""baseline"", ""SP map"", and ""CF map"", we can find that the two kinds of information individually are not good for segmenting PWMLs. The reason is as follows. The SP map mainly focuses on tissue segmentation task. The CF map has some false activation due to the offset of the highlighted areas for PWML. Fusing these two kinds of information has reduced their respective defects (""SP map + CF map""). The icing on the cake is that when the appearance features of T1w are used again, the accuracy will be significantly improved (""SP map + T1"" and ""CF map + T1""). This means ""SP map"" and ""CF map"" each can be an auxiliary information but not sufficient resource for this task. Finally, after combining the three together, all indicators have been significantly improved (""SP map + CF map + T1""). Visual Analysis: Figure  Comparison of Different Backbones of the P-SEG Module: We test from simple several layers to the whole dense-Unet to determine the required complexity in Table "
Punctate White Matter Lesion Segmentation in Preterm Infants Powered by Counterfactually Generative Learning,4.0,Conclusion,"In this study, we designed a simple and easy-to-implement deep learning framework (i.e. DeepPWML) to segment PWMLs. Leveraging the idea of generative counterfactual inference combined with an auxiliary task of brain tissue segmentation, we learn fine-grained positional and morphological representations of PWMLs to achieve accurate localization and segmentation. Our lightweight PWML segmentation network combines lesion counterfactual maps with tissue segmentation probability maps, achieving state-of-the-art performance on a real clinical dataset of infant T1w MR images. Moreover, our method provides a new perspective for the small-size segmentation task."
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,1.0,Introduction,"Epilepsy is a chronic neurological condition that affects more than 60 million people worldwide in which patients experience epileptic seizures due to abnormal brain activity  The current gold standard practice for detection and classification of epileptic seizures is the hospital-based Video EEG Monitoring (VEM) units  Initial works primarily employed hand-crafted features based on patient motion trajectory by attaching infrared reflective markers to specific body key points  In this work, we address the above two challenges by building an in-house dataset of privacy-preserved extracted features from a video and propose a framework for early detection of seizures. Specifically, we investigate two aspects - "
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,2.0,Proposed Method,"In this section, we first outline the process of extracting privacy-preserving information from RGB video samples to build our in-house dataset. Later, we explain our proposed approach for early detection of seizures in a sample."
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,2.1,Privacy Preserving Optical Flow Acquisition,Our in-house dataset of RGB videos of patients experiencing seizures resides on hospital premises and is not exportable due to the hospital's ethics agreement
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,2.2,Early Detection of Seizures in a Sample,"Consider an input optical flow video sample V i as shown in Fig.  Processing a Single Sample. Since seizure patterns comprise of body movements, we implement transfer learning from a feature extractor pre-trained on action recognition task to extract the spatial features from the optical flow frames. Prior work  We leverage transformers to effectively learn temporal relations between the extracted spatial features of the seizure patterns. Following the strategy of ViT  To enable the interaction between tokens and learn temporal relationships for input sample classification, we employ the Vanilla Multi-Head Self Attention (MHSA) mechanism  Subsequently, the output of MHSA is passed to a two-layered MLP with GELU non-linearity while applying layer normalization and residual connections concurrently. Eq. 3, 4 represent this overall process. where m L ∈ R (N +1)×D are the final output feature representations and L is the total number of encoding layers in the Transformer Encoder. Note that the first R N ×D features correspond to the patch tokens , while the final R D correspond to the class token of the m L as shown in Fig.  Progressive Knowledge Distillation. To achieve early detection, we use Knowledge Distillation in a Progressive manner (PKD), starting from a SETR block trained on a full video sample and gradually moving to a SETR block trained on a partial video sample, as shown in Fig.  ; Later, the Subteacher k-2 SETR passes its distilled knowledge to its subsequent student (Sub-teacher k-3 ) SETR, and this continues until the final Sub-teacher 1 SETR passes its knowledge to the bottom most Student 0 SETR. Since the consecutive segments of the videos do not differ significantly, PKD is more effective than direct distillation, which is proven by results in Sect. 3.4. For distilling knowledge we consider both class token and patch tokens of the teacher and student networks. A standard Kullback-Leibler divergence (L KL ) loss is applied between the probabilities generated from class token of the teacher and student SETR, whereas a mean squared error (L MSE ) loss is computed between the patch tokens of teacher and student SETR. Overall, a student SETR is trained with three losses -L KL and L MSE loss for knowledge distillation, and a cross-entropy (L CE ) loss for classification, given by the equations below. where q S j and q T j are the soft probabilities (moderated by temperature τ ) of the student and teacher SETRs for the j th class, respectively. where N is the number of patches and p T i and p S i are the patches of teacher and student SETRs respectively. where α and β are the weights for L KL and L MSE loss respectively. 3 Datasets and Experimental Results"
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,3.1,In-House and Public Dataset,"Our in-house dataset 2 contains optical flow information extracted from highdefinition (1920 × 1080 pixels at 30 frames per second) video recordings of TCS seizures (infrared cameras are used for nighttime seizures) in a VEM unit in hospital. To annotate the dataset, two neurologists examined both the video and corresponding EEG to identify the clinical seizure onset (t ON ) and clinical seizure offset (t OF F ) times for each seizure sample. We curated a dataset comprising of 40 TCSs from 40 epileptic patients, with one sample per patient. The duration (in seconds) of the 40 TCSs in our dataset ranges from 52 to 367 s, with a median duration of 114 s. We also prepared normal samples (no seizure) for each patient by considering the pre-ictal duration from (t ON -300) to (t ON -60) seconds, resulting in dataset of 80 samples (40 normal and 40 TCSs). We refrain from using the 60 s prior to clinical onset as it corresponds to the transition period to the seizure containing preictal activity "
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,3.2,Training Implementation and Evaluation Metrics,"We implement all experiments in PyTorch 1.8.1 on a single A100 GPU. The SETR block takes in a total of 64 frames (N ) with 512 1-D spatial feature per frame, has 8 MHSA heads (N h ) with a dropout rate of 0.1, 3 encoder layers (L), and 256 hidden dimensions (D). For early detection, we experiment by progressively segmenting a sample into -{4,8,16} parts (k). We employ a grid search to select the weight of 0.2 and 0.5 for KL divergence (τ = 10) and MSE loss respectively. We train all methods with a batch size of 16, a learning rate of 1e-3 and use the AdamW optimizer with a weight decay of 1e-4 for a total 50 epochs. For GESTURES dataset, we implement a weighted BCE loss to deal with the dataset imbalance, whereas for our in-house dataset we implement the standard BCE loss. We use precision, recall and f1-score for benchmarking."
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,3.3,Performance for Early Detection,Table 
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,4.0,Conclusion,"In this work, we show that it is possible to detect epileptic seizures from optical flow modality in a privacy-preserving manner. Moreover, to achieve real-time seizure detection, we specifically develop a novel approach using progressive knowledge distillation which proves to detect seizures more accurately during their progression itself. We believe that our proposed privacy-preserving early detection of seizures will inspire the research community to pursue real-time seizure detection in videos as well as facilitate inter-cohort studies."
Privacy-Preserving Early Detection of Epileptic Seizures in Videos,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_21.
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,1.0,Introduction,"Colonoscopy plays a crucial role in identifying and removing early polyps and reducing mortality rates associated with rectal cancer. Over the past few years, the research community has devoted great effort to understanding colonoscopy videos using either optical flow  However, those works are mainly designed based on the experience of previous natural video object detection studies, ignoring the inherent uniqueness of the colonoscopy motion patterns. Thus, we rethink the video polyp detection task and conclude three core challenges in colonoscopy videos. 1) Fast motion speed. In Fig.  To address the above issues, we propose the YONA framework, which fully exploits the reference frame information and only needs one adjacent reference frame for accurate video polyp detection. Specifically, we propose the Foreground Temporal Alignment (FTA) module to explicitly align the foreground channel activation patterns between adjacent features according to their foreground similarity. In addition, we design the Background Dynamic Alignment (BDA) module after FTA that further learns the inter-frame background spatial dynamics to better eliminate the influence of motion speed and increase the training robustness. Finally, parallel to FTA and BDA, we introduce the Cross-frame Boxassisted Contrastive Learning (CBCL) that fully utilizes the box annotations to enlarge polyp and background discrimination in embedding space. In summary, our contributions are in three-folds: (1) To the best of our knowledge, we are the first to investigate the obstacles to the development of existing video polyp detectors and conclude that two-frame collaboration is enough for video polyp detection. "
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,2.0,Method,"The whole pipeline is shown in Fig.  Overall, the whole network is optimized with the combination loss function in an end-to-end manner. The final loss is composed of the same detection loss with CenterNet and our proposed contrastive loss, formulated as L = L detection + λ contrast L contrast . "
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,2.1,Foreground Temporal Alignment,"Since the camera moves at a high speed, the changes in the frame are very drastic for both foreground and background targets. As a result, multi-frame (reference>3) fusion may easily incorporate more noise features into the aggregation features. On the other hand, the occluded or distorted foreground context may also influence the quality of aggregation. Thus we propose to conduct temporal alignment between adjacent features by leveraging the foreground context of only one adjacent reference frame. It is designed to align the certain channel's activation pattern of anchor feature to its preceding reference feature. Specifically, given the intermediate features F a , F r and reference binary map M r , we first pooling F r to 1D channel pattern f r by the binary map on the spatial dimension (R N ×C×H×W → R N ×C×1 ) and normalize it to [0, 1]: Then, the foreground temporal alignment is implemented by channel attention mechanism, where the attention maps are computed by weighted dot-product. We obtain the enhanced anchor feature by adding the attention maps with the original anchor feature through skip connection to keep the gradient flow. where α is the adaptive weight by similarity measuring. At the training stage, the ground truth boxes of the reference frame are used to generate the binary map M r . During the inference stage, we conduct FTA only if the validated bounding box of the reference frame exists, where ""validated"" denotes the confidence scores of detected boxes are greater than 0.6. Otherwise, we will skip this process and feed the original inputs to the next module. Adaptive Re-weighting by Similarity Measuring. As discussed above, due to video jitters, adjacent frames may change rapidly at the temporal level, and directly fusing the reference feature will introduce noisy information and misguide the training. Thus we designed an adaptive re-weighting method by measuring the feature similarity, where the weight indicates the importance of the reference feature to the anchor feature. Specifically, if the foreground feature of the reference is close to the anchor, it is assigned a larger weight at all channels. Otherwise, a smaller weight is assigned. For efficiency, we use the cosine similarity metric "
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,2.2,Background Dynamic Alignment,"The traditional convolutional-based object detector can detect objects well when the background is stable. However, once it receives obvious interference, such as light or shadow, the background changes may cause the degradation of spatial correlation and lead to many false-positive predictions. Motivated by the inter-frame difference method  Finally, a 3 × 3 deformable convolution embeds the spatial dynamic changes of D on the enhanced anchor feature F . where D works as the deformable offset and F * is the final aligned anchor feature. Then the enhanced anchor feature is fed into three detection heads composed of a 3 × 3 Conv and a 1 × 1 Conv to produce center, size, and offset features for detection loss: where L focal is focal loss and L L1 is L1 loss."
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,2.3,Cross-Frame Box-Assisted Contrastive Learning,"Typically, in colonoscopy videos, some concealed polyps appear very similar to the intestine wall in color and texture. Thus, an advanced training strategy is required to distinguish such homogeneity. Inspired by recent studies on supervised contrastive learning  where q j ∈ R C , j = 0, ..., N T is the query feature, i + ∈ R C and i -∈ R NT ×C are positives and negatives. N j denote embedding collections of the negatives. We repeat this process until every foreground channel pattern is selected and sum all steps as the final contrastive loss: 3 Experiments We evaluate the proposed method on three public video polyp detection benchmarks: SUN Colonoscopy Video Database "
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,3.1,Quantitative and Qualitative Comparison,Quantitative Comparison. The comparison results are shown in Table  Qualitative Comparison. Figure 
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,3.2,Ablation Study,"We investigated the effectiveness of each component in YONA on the SUN database, as shown in Table  FTA CW "
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,4.0,Conclusion,"Video polyp detection is a currently challenging task due to the fast-moving property of colonoscopy video. In this paper, We proposed the YONA framework that requires only one adjacent reference frame for accurate and fast video polyp detection. To address the problem of fast-moving polyps, we introduced the foreground temporal alignment module, which explicitly aligns the channel patterns of two frames according to their foreground similarity. For the complex background content, we designed the background dynamic alignment module to mitigate the large variances by exploiting the inter-frame difference. Meanwhile, we employed a cross-frame box-assisted contrastive learning module to enhance the polyp and background discrimination based on box annotations. Extensive experiment results confirmed the effectiveness of our method, demonstrating the potential for practical use in real clinical applications."
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_5.
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,1.0,Introduction,"Breast cancer is a life-threatening disease that has surpassed lung cancer as leading cancer in some countries and regions  While ultrasound videos offer more information, prior studies have primarily focused on static image classification  Given the difficulties in collecting ultrasound video data, we investigate the feasibility of enhancing the performance of ultrasound video classification using a static image dataset. To achieve this, we first analyze the relationship between ultrasound videos and images. The images in the ultrasound dataset are keyframes of a lesion that exhibit the clearest appearance and most typical symptoms, making them more discriminative for diagnosis. Although ultrasound videos provide more information, the abundance of frames may introduce redundancy or vagueness that could disrupt classification. From the aspect of feature distribution, as shown in Fig.  In this paper, we propose a novel Keyframe Guided Attention Network (KGA-Net) to boost ultrasound video classification. Our approach leverages both image (keyframes) and video datasets to train the network. To classify videos, we use frame attention to predict feature weights for all frames and aggregate them to make the final classification. The feature weights determine the contribution of each frame for the final diagnosis. During training, we construct category feature centers for malignant and benign examples respectively using center loss  Our experimental results on the public BUSV dataset  In conclusion, our contributions are as follows: 1. We analyze the relationship between ultrasound video data and image data, and propose the coherence loss to use image feature centers to guide the training of frame attention. 2. We propose KGA-Net, which adopts a static image dataset to boost the performance of ultrasound video classification. KGA-Net significantly outperforms other video baselines on the BUSV dataset. 3. The qualitative analysis of the frame attention verifies the explainability of our method and provides a new perspective for selecting keyframes."
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,2.0,Related Works,"Breast Ultrasound Classification. Breast ultrasound (BUS) plays an important supporting role in the diagnosis of breast-related diseases. Recent research demonstrated the potential of deep learning for breast lesion classification tasks  Video Recognition Based on Neural Networks. Traditional methods are based on Two-stream networks  To address computational complexity, MViT "
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,3.0,Methodology,As shown in Fig. 
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,3.1,Video and Image Classification Network,"The video classification network is illustrated in Fig.  where w i denotes the weight for the i th frame and FC is the fully-connected layer. Then, the features are aggregated by Finally, the classification head is applied to the final result of lesion classification. To train the model, the cross-entropy loss (CE Loss) is applied to the classification prediction of the video. The image classification network is used to assist in training the video model. We use the same 2D CNN as the backbone network in the video classification network. The model weights are shared for the two backbones for better generalization. To promote the formation of feature centers, we apply the center loss "
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,3.2,Training with Coherence Loss,"In this section, we introduce the coherence loss to guide the frame attention with the assistance of the category feature centers. We use the same method as center loss  The distances of frame features and the feature centers can measure the quality of the frames. The frame features close to the centers are more discriminative for the classification task. Therefore, we use these distances to guide the generation of frame attention. Specifically, we push the frames close to the centers to have higher attention weights and decrease the weights far from the centers. To do this, for each video frame with feature F i , we first calculate the feature distance from its corresponding class center. Formally, where Y ∈ {mal, benign} is the label of the video V and d i is the computed distance of frame i. Afterward, we apply coherence loss to the attention weights w = [w 1 , w 2 , ..., w N ] to make them have a similar distribution with the feature distances d = [d 1 , d 2 , ..., d N ] . To supervise the distribution, the coherence loss is defined as the L2 loss of the gram matrix of these two vectors where is the gram matrix of normalized attention weights, and is the gram matrix of normalized feature distances. Note that lower distances correspond to stronger attention, hence we use the opposite of w to get Gram w ."
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,3.3,Total Training Loss,"To summarize, the total training loss of our KGA-Net L V CE and L I CE denote the cross-entropy for video classification and image and frame classification. L Center means the center loss. λ is the weight for coherence loss. Empirically, we set λ = 1 in our experiments. During inference, to perform classification on video data, the video classification network can be utilized individually for prediction."
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,4.1,Implementation Details,"Datasets. We use the public BUSV dataset  During inference, we use the video classification network individually. In order to satisfy the fixed video length requirement of MViT "
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,4.2,Comparison with Video Models,"In this section, we compare our KGA-Net with other competitive video classification models. However, comparing with ultrasound-video-based work is challenging due to limited code accessibility and lack of keyframe detection model in existing methods  As shown in Table "
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,4.3,Ablation Study,"In this section, we ablate the contribution of each key design in our KGA-Net. We observe their importance by removing these key components from the whole network. The results are shown in Table  Image guidance is the main purpose of our method. To portray the effect of using the image dataset, we train the KGA-Net using BUSV dataset alone in the first row of Table  Frame attention and coherence loss are two essential modules of our KGA-Net. We train a KGA-Net without the coherence loss in the third row of Table  In the second row, we further replace the feature attention module with feature averaging of video frames. It can be seen that both of these two modules contribute to the overall performance according to AUC and ACC. It is worth noting that these two models without coherence loss obtain very low sensitivity and high specificity, which means the model predictions are imbalanced and intend to make benign predictions. It is because that clear malignant appearances usually only exist in limited frames in a malignant video. Without our coherence loss or frame attention, it is difficult for the model to focus on typical frames that possess malignant features. This phenomenon certifies the effectiveness of our KGA-Net to prevent false negatives in diagnosis.  In Fig.  The qualitative analysis proves the interpretability of our method, which will benefit clinical usage. Moreover, the attention weights reveal the importance of each frame for lesion diagnosis. Therefore, it can provide a new perspective for the keyframe extraction task of ultrasound videos."
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,5.0,Conclusion,"We propose KGA-Net, a novel video classification model for breast ultrasound diagnosis. Our KGA-Net takes as input both the video data and image data to train the network. We propose the coherence loss to guide the training of the video model by the guidance of feature centers of the images. Our method significantly exceeds the performance of other competitive video baselines. The visualization of the attention weights validates the effectiveness and interpretability of our KGA-Net."
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,1.0,Introduction,"Advances in deep learning have been witnessed in many research areas over the past decade. In medical field, automatic analysis of medical image data has actively been studied. In particular, segmentation which identify region of interest (RoI) in an automatic way is an essential medical imaging process. Thus, deep learning-based segmentation has been utilized in various medical domains such as brain, breast cancers, and colon polyps. Among the popular architectures, variants of U-Net have been widely adopted due to their effective encoderdecoder structure, proficient at capturing the characteristics of cells in images. Recently, it has been demonstrated that the attention modules  However, as image-only training trains a model with pixels that constitute an image, there is a limit in extracting fine-grained information about a target object even if transfer learning is applied through a pre-trained model. Recently, to overcome this limitation, multi-modality studies have been conducted, aiming to enhance the expressive power of both text and image features. For instance, CLIP  The trend of text-image multi-modality-based research on image processing has extended to the medical field.  In this paper, we propose a new text-guided cross-position attention module (CP AM T G ) that combines text and image. In a medical image, a position attention module (PAM) effectively learns subtle differences among pixels. We utilized PAM which calculates the influence among pixels of an image to capture the association between text and image. To this end, we converted the global text representation generated from the text encoder into a form, such as an image feature map, to create keys and values. The image feature map generated from an image encoder was used as a query. Learning the association between text and image enables us to learn positional information of targets in an image more effectively than existing models that learned multi-modality from medical images. CP AM T G showed an excellent segmentation performance in our comprehensive experiments on various medical images, such as cell, chest X-ray, and magnetic resonance image (MRI). In addition, by applying the proposed technique to the automatic RoI setting module for the deep learning-based diagnosis of sacroiliac arthritis, we confirmed that the proposed method could be effective when it is used in a practical application of computer-aided diagnosis. Our main contributions are as follows: -We devised a text-guided cross-position attention module (CP AM T G ) that efficiently combines text information with image feature maps. -We demonstrated the effect of CP AM T G on segmentation for various types of medical images. -For a practical computer-aided diagnosis system, we confirm the effectiveness of the proposed method in a deep learning-based sacroiliac arthritis diagnosis system. "
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,2.0,Methods,"In this section, we propose text-guided segmentation model that can effectively learn the multi-modality of text and images. Figure "
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,2.1,Configuration of Text-Image Encoder and Decoder,"As Transformer has demonstrated its effectiveness in handling the long-range dependency in sequential data through self-attention  To create a segmentation mask from medical images (I), we used U-Net  The weights of text and image encoders were initialized by the weights of CLIP's pre-trained transformer and VGG16 pre-trained on ImageNet, respectively, and fine-tuned by a loss function for segmentation which will be described in Sect. 3."
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,2.2,Text-Guided Cross Position Attention Module,"We introduce a text-guided cross-position attention module (CP AM T G ) that integrates cross-attention  In Fig.  where H Q , H K , and H V are convolution layers with a kernel size of 1, and Q, K, and V are queries, keys, and values for self-attention. Finally, by upsampling the low-dimensional CP AM T G obtained through crossattention of text and image together with skip-connection, more accurate segmentation prediction can express the detailed information of an object. 3 Experiments"
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,3.1,Setup,"Medical Datasets. We evaluated CP AM T G using three datasets: MoNuSeg  Training and Metrics. For a better training, data augmentation was used. We randomly rotated images by -20 • ∼ +20 • and conducted a horizontal flip with 0.5 probability for only the MoNuSeg and QaTa-COV19 datasets. The batch size and learning rate were set to 2 and 0.001, respectively. The loss function (L T ) for training is the sum of the binary cross-entropy loss (L BCE ) and the dice loss (L DICE ): The mDice and mIoU metrics, widely used to measure the performance of segmentation models, were used to evaluate the performance of object segmentation. For experiments, PyTorch (v1.7.0) were used on a computer with NVIDIA-V100 32 GB GPU."
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,3.2,Segmentation Performance,Table  MedT  Figure 
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,3.3,Ablation Study,"To validate the design of our proposed model, we perform an ablation study on position attention and CP AM T G . Specifically, for the SIJ dataset, we examined the effect of attention in extracting feature maps through comparison with backbone networks (U-Net) and PAM. In addition, we investigated whether text information about images serves as a guide in the position attention process for image segmentation by comparing it with CP AM T G . Table "
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,3.4,Application: Deep-Learning Based Disease Diagnosis,"In this section, we confirm the effectiveness of the proposed segmentation method through a practical bio-medical application as a deep learning-based active sacroiliitis diagnosis system. MRI is a representative means for early diagnosis of ""active sacroiliitis in axSpA"". As active sacroiliitis is a disease that occurs between the pelvic bone and sacral bone, when a MR slice is input, the diagnostic system first separates the area around the pelvic bone into an RoI patch and uses it as an input for the active sacroiliitis classification network  We segmented the pelvic bones in MRI slices using the proposed method to construct a fully automatic deep learning-based active sacroiliitis diagnosis system, including RoI settings from MRI input images. Figure "
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,4.0,Conclusion,"In this study, we developed a new text-guided cross-attention module (CP AM T G ) that learns text and image information together. The proposed model has a composite structure of position attention and cross-attention in that the key and value are from text data, and the query is created from the image. We use a learnable parameter to convert text features into a tensor of the same dimension as the image feature map to combine text and image information effectively. By calculating the association between the reshaped global text representation and each component of the image feature map, the proposed method outperformed image segmentation performance compared to previous studies using both text and image or image-only training method. We also confirmed that it could be utilized for a deep-learning-based sacroiliac arthritis diagnosis system, one of the use cases for practical medical applications. The proposed method can be further used in various medical applications."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,1.0,Introduction,"The spreading digitalisation of pathology labs has enabled the development of deep learning (DL) tools that can assist pathologists in their daily tasks. However, supervised DL methods require detailed annotations in whole-slide images (WSIs) which is time-consuming, expensive and prone to inter-observer disagreements  Domain shift in DL occurs when the data distributions of testing and training differs  In this work, we evaluate an attention-based MIL model on unseen data from a new hospital and propose a way to quantify the domain shift severity. The model is trained to perform binary classification of WSIs from lymph nodes of breast cancer patients. We split the data from the new hospital into several subsets to investigate clinically realistic scenarios triggering different levels of domain shift. We show that our proposed unsupervised metric for quantifying domain shift correlates best with the changes in performance, in comparison to multiple baselines. The approach of validating a MIL algorithm in a new site without collecting new labels can greatly reduce the cost and time of quality assurance efforts and ensure that the models perform as expected in a variety of settings. The novel contributions of our work can be summarised as: 1. Proposing an unsupervised metric named Fréchet Domain Distance "
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,2.0,Methods,"Our experiments center on an MIL algorithm with attention developed for classification in digital pathology. The two main components of our domain shift quantification approach are the selection of MIL model features to include and the similarity metric to use, described below."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,2.1,MIL Method,"As the MIL method for our investigation, we chose the clustering-constrainedattention MIL (CLAM) "
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,2.2,MIL Features,"We explored several different feature sets that can be extracted from the attention-based MIL framework: learnt embedding of the instances (referred to as patch features), and penultimate layer features (penultimate features). A study is conducted to determine the best choices for type and amount of patch features. As a baseline, we take a mean over all patches ignoring their attention scores (mean patch features). Alternatively, the patch features can be selected based on the attention score assigned to them. Positive evidence or Negative evidence are defined as the K patch features that have the K highest or lowest attention scores, respectively. Combined evidence is a combination of an equal number of patch features with the highest and lowest attention scores. To test if the reduction of the number of features in itself has a positive effect on domain shift quantification, we also compare with K randomly selected patch features."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,2.3,Fréchet Domain Distance,"Fréchet Inception Distance (FID)  We are interested in using the FD for measuring the domain shift between different WSI datasets X d . To this end, we extract features from the MIL model applied to all the WSIs in X d , and arrange these in a feature matrix  F DD K uses the K aggregated positive evidence features, but in the results we also compare to M d described from penultimate features, mean patch features, and the other evidence feature selection strategies."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,3.0,Datasets,"Grand Challenge Camelyon data  The datasets of lobular and ductal carcinomas each contain 50 % of WSIs from sentinel and axillary lymph node procedures. The sentinel/axillary division is motivated by the differing DL prediction performance on such subsets, as observed by Jarkman et al. "
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,4.0,Experiments,"The goal of the study is to evaluate how well F DD K and the baseline methods correlate with the drop in classification performance of attention-based MIL caused by several potential sources of domain shifts. In this section, we describe the experiments we conducted."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,4.1,MIL Training,"We trained, with default settings, 10 CLAM models to classify WSIs of breast cancer metastases using a 10-fold cross-validation (CV) on the training data. The test data was kept the same for all 10 models. The classification performance is evaluated using the area under receiver operating characteristic curve (ROC-AUC) and Matthews correlation coefficient (MCC) "
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,4.2,Domain Shift Quantification,"As there is no related work on domain shift detection in the MIL setting, we selected methods developed for supervised algorithms as baselines: -The model's accumulated uncertainty between two datasets. Deep ensemble  For all possible pairs of Camelyon and the other test datasets, and for the 10 CV models, we compute the domain shift measures and compare them to the observed drop in performance. The effectiveness is evaluated by Pearson correlation and visual investigation of corresponding scatter plots. All results are reported as mean and standard deviation over the 10-fold CV."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,5.0,Results,"The first part of our results is the performance of the WSI classification task across the subsets, summarized in Table  As we deemed MCC to better represent the clinical use situation (see Sect. 4.1), it was used for our further evaluations. Overall, the performance is in line with previously published work  Table "
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,6.0,Discussion and Conclusion,"MIL is Affected by Domain Shift. Some previous work claim that MIL is more robust to domain shift as it is trained on more data due to the reduced costs of data annotation  The Proposed F DD K Measure Outperforms Alternatives. The highest Pearson correlation between change in performance and a distance metric is achieved by Fréchet distance with 64 positive evidence features, F DD 64 (see Table "
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,,Conclusion.,"We carried out a study on how clinically realistic domain shifts affect attention-based MIL for digital pathology. The results show that domain shift may raise challenges in MIL algorithms. Furthermore, there is a clear benefit of using attention for feature selection and our proposed F DD K metric for quantification of expected performance drop. Hence, F DD K could aid care providers and vendors in ensuring safe deployment and operation of attention-based MIL in pathology laboratories."
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 16.
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,1.0,Introduction,"Discovering disease characteristics from medical images can yield important insights into pathological changes. However, subtle disease characteristics are difficult to discover from imaging data. The reduction in brain volume due to Alzheimer's Disease (AD) is one such example. In this case, comparing co-registered AD and Cognitively Normal (CN) reference magnetic resonance images (MRIs) is the most effective way to reveal subtle AD characteristics. Unfortunately, collecting such co-registered reference images at scale is not practical. Generative Adversarial Networks (GANs) has the potential to synthesize the reference images required for disease discovery. Specifically, style-based generative frameworks  In this work, we propose Disease Discovery GAN (DiDiGAN), a specialized style-based network for disease characteristic discovery on medical image data. On the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset  -A learnt disease manifold that captures and encodes AD and CN disease state distributions. Style codes sampled from this manifold control the disease expression in the output reference images. -The disease manifold is naturally smooth such that seamless transitions of AD to CN are possible via style interpolation. -The generator uses a low-resolution input image as the source anatomical constraint (to provide coarse structural guidance), it is low-resolution to leave sufficient room for the generator to synthesize disease features. -Anti-aliasing as a key mechanism to maintain anatomical correspondence across generated reference images. Without anti-aliasing, reference images exhibit inconsistent anatomical structures invalidating visual comparisons. -DiDiGAN is a weakly-supervised framework requiring only class labels for images rather than pixel (voxel) labels. DiDiGAN learns to generate representative reference AD and CN images by training on 2D coronal brain slices labelled with AD and CN. The generated reference images and animations clearly show systematic changes in the hippocampus, ventricle, and cortex areas while maintaining anatomical correspondence. The discovered characteristics are corroborated by independent tests which solidify the strengths of the findings. There has not been a previous work that can i) produce visualizations on par with DiDiGAN's and ii) learn a dedicated manifold for disease characteristic discovery."
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,2.0,Related Work,Various generative methods have attempted to synthesize pathological changes for disease modelling. HerstonNet 
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,3.0,Methods,"DiDiGAN projects different disease states onto a common anatomical structure to form disease reference images, and the output images must maintain a consistent anatomical structure. A detailed architecture diagram is shown in Fig. "
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,,Disease Style w c and Anatomical Constraint,"x AC : Let x c denote the images in a dataset and c ∈ {c 1 , c 2 ...c n } are the disease classifications for the images, the generator G's aim is to synthesise reference disease images x c that share a consistent anatomical structure. This is done by injecting disease styles w c into a shared anatomical constraint x AC to produce the output x c = G(w c , x AC ). w c is created following w c = M (emb(c), z) where emb is a learned 512-d embedding specific to each class, z is a 512-d N (0, 1) noise vector and M is a 2-layer mapping network made up of dense layers. Like StyleGAN, the style code space w c is a learned manifold (of disease states), and it can be interpolated for smooth disease states transitions. The reason to include z is to introduce variability such that w c forms a surface instead of a fixed point. x AC is a 4× down-sampled version of x c which only enforces coarse anatomical structure similarities between the reference images, and it lacks resolution on purpose allowing room for disease features from w c to manifest. As already discussed, full anatomical correspondence requires anti-aliasing to enforce."
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,,Alias-Free Style Generator G:,"Generator G is alias-free as the feature maps are processed carefully following the Shannon-Nyquist sampling theorem. Like StyleGAN 3, G's convolutional (AA Conv) layers apply anti-aliasing to the leakyrelu activation function. This process involves first interpolating discrete feature maps to continuous domains. Then, low-pass filtering is used to remove offending frequencies. Finally, the activation function is applied, and the signal is resampled back to the discrete domain "
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,,Multi-head Discriminator D:,"The discriminator D is an encoder architecture with consecutive convolution and down-sampling. There are three prediction heads D R/F , D c , and D AC which predict a realness logit, disease classification, and reconstruction of constraint x AC , respectively. The main loss functions are these outputs are 1) the standard non-saturating GAN loss L R/F for generating realistic images, 2) the adversarial classification loss L c which supervises the conditioning on c 3) the anatomical reconstruction loss L AC ensuring the generated reference images share the high-level structure of the input anatomical constraint. These losses are as follows: min where x c is a real image of class c, x c is a fake image of the same class, w c = M (emb(c), z), and x AC is a down-sampled version of x c . There are also other standard regularization loss functions including the pathlength regularization L pl  Quantitative Image Quality Evaluation Using Perceptual Metrics. The quality of the generated reference images (based on the test set, ignoring classes) from all the methods is assessed using Frechet Inception Distance (FID), Learned Perceptual Image Patch Similarity (LPIPS), and Kernel Inception Distance (KID) as they do not require paired data. As Table "
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,,AD Characteristic Extraction and Visualization Using DiDiGAN.,"Figure  In these examples, we also manually segmented the ventricle and hippocampus, and the notable size changes marked in the figure. Anti-aliasing is crucial for maintaining anatomical correspondence across DiDiGAN's reference images. As an ablation study, a DiDiGAN without anti-aliasing was trained. The generated AD and CN pairs exhibit significant anatomical disturbances (see S1 for examples). Comparing AD Characteristic Visualization with Baselines. The baseline methods produce reference images via direct image translation from CN to an AD reference. The results in Fig.  Disease Manifold Formation and Interpolation. DiDiGAN's disease manifold formation is visualized using UMAP  Corroborating Tests: Due to the lack of paired medical images as ground truths, we perform the following tests (I, II, III) to verify DiDiGAN's findings. I: DiDiGAN was independently applied to the sagittal view slices, and similar hippocampus shrinkage and ventricle enlargement were clearly observed (see examples S2). These independent findings corroborate the coronal view visualizations produced by DiDiGAN's reference images. II: SPM segmentation was applied to the test set and DiDiGAN's reference images (generated based on the test set) to help compare the magnitudes of brain volume loss. For each image, the grey matter and white matter pixels were treated as brain mass. As Fig. "
Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery,5.0,Conclusion,"DiDiGAN demonstrated disease characteristic discovery by generating reference images that clearly depict relevant pathological features. The main technical novelties of DiDiGAN are i) the use of a learned disease manifold to manipulate disease states AD ii) the ability to interpolate the manifold to enhance visualization and iii) mechanisms including the structural constraint and anti-aliasing to maintain anatomical correspondence without direct registration. In the experiments involving the ADNI dataset, DiDiGAN discovered key AD features such as hippocampus shrinkage, ventricular enlargement, and cortex atrophy where other frameworks failed. DiDiGAN shows potential to aid disease characteristic discovery across time of other chronic diseases such as osteoarthritis."
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,1.0,Introduction,"Liver cancer is the third leading cause of cancer death world-wide in 2020  Many researchers have developed algorithms to automatically segment  In this paper, we build a comprehensive framework to address both tumor screening and diagnosis. (1) Tumor screening involves finding tumor patients in a large pool of healthy subjects and patients. Most existing works in tumor segmentation and detection did not explicitly consider it since their training and testing images are all tumor patients. Such models may generate false positives in real-world screening scenario when facing diverse tumor-free images. We collect a large-scale dataset with both tumor and non-tumor subjects, where the non-tumor subjects includes not only healthy ones, but also patients with various diffuse liver diseases such as steatosis and hepatitis to improve the robustness of the algorithm. (2) Most works studied liver tumor segmentation alone without differentiating tumor types, while a few works classify liver tumors on cropped tumor patches  Algorithms for liver tumor segmentation have focused on improving the feature extraction backbone of a fully-convolutional CNN  We collected a large-scale multi-phase dataset containing 810 non-tumor subjects and 939 tumor patients. 4010 tumor instances of eight types are extensively annotated based on pathological reports. On the non-contrast tumor screening and diagnosis task, PLAN achieves 95.0%, 96.4%, and 0.965 in patient-level sensitivity, specificity, and average AUC for malignant and benign patients, in contrast to 94.4%, 93.7%, and 0.889 for the widely-used nnU-Net "
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,2.1,Preliminary on Mask Transformer,"Mask transformers are a series of latest works achieving superior accuracy on various segmentation tasks  Mask transformers have various advantages when applied to our task. They can classify a lesion as a whole instead of classifying each pixel, thus can view each lesion holistically. Cross-attention is used to aggregate global features for each lesion. Inter-lesion relation can also be exploited by self-attention operations. In liver CT, inter-lesion relation is diagnostically useful, e.g., metastases and cysts are often multiple. Therefore, We pioneer mask transformers' adaptation for lesion segmentation and classification in 3D medical images. Given a groundtruth or a predicted lesion mask image, we perform connected component (CC) analysis and treat each CC as a lesion instance for training and evaluation."
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,2.2,Pixel-Lesion-Patient Network (PLAN),"Our goal is to segment the mask and classify the type of each tumor in a liver CT. We also hope to make patient-level diagnoses for each CT scan. PLAN is inspired by Mask2Former  Pixel Branch and Anchor Queries. The pixel branch is a convolutional layer after the pixel decoder and learns to predict pixel-wise segmentation maps similar to traditional segmentators. We do CC analysis to the predicted mask to extract lesion instances, and then average the pixel embeddings inside each predicted lesion to obtain a feature vector. The feature vectors are regarded as anchor queries and work the same way as the randomly initialized queries in the lesion branch. Compared to the random queries in the original Mask2Former, the anchor queries contain prior information of the lesions to be segmented, helping the lesion branch to match with the lesion targets more easily  Lesion Branch and Foreground-Enhanced Sampling Loss. Similar to Mask2Former, the lesion branch predicts a binary mask and a class label for each query, see Fig.  Patient Branch. A patient-level diagnosis is useful for triage. For example, diagnosing the subject as normal, benign, or malignant will result in completely different treatments  A lesion-patient consistency loss is further proposed to encourage coherence of the lesion and patient-level predictions. Inspired by multi-instance learning  The overall loss of PLAN is listed in Eq. 1, where L pixel is the combined crossentropy (CE) and Dice loss for the pixel branch as in nnU-Net "
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,3.0,Experiments,"Data. Our dataset contains 810 normal subjects and 939 patients with liver tumors. Each normal subject has a non-contrast (NC) CT, while each patient has a dynamic contrast-enhanced (DCE) CT scan with NC, arterial, and venous phases. We use DEEDS  We use the RAdam optimizer with an initial learning rate of 0.0001. Each training batch contains two patches of size 256 × 256 × 24. For DCE CT, the three phases form a 3-channel image as the network input. Extensive data augmentation is applied including random cropping, scaling, flipping, elastic deformation, and brightness adjustment  During training, we first pretrain the backbone and the pixel branch for 500 epochs, and then train the whole network for another 500 epochs. Patient-Level Results. This paper has three major goals: tumor screening in NC CT (classifying a subject as normal or tumor), preliminary diagnosis in NC CT (predicting the existence of malignant and benign tumors), and fine-grained diagnosis in DCE CT (predicting the existence of 8 tumor types). Among the 8 tumor types, HCC, ICC, meta, and hepato are malignant; heman, FNH, and cyst are benign. ""Others"" can be either malignant or benign, thus are excluded in the preliminary diagnosis task. The NC test set contains 198 tumor cases, 202 completely normal cases, and 100 ""hard"" non-tumor cases which may have larger image noise, artifact, ascites, diffuse liver diseases such as hepatitis and steatosis. These cases are used to test the robustness of the model in real-world screening scenario with diverse tumor-free images. We compare PLAN with a widely-used strong baseline, nnU-Net  Lesion and Pixel-Level Results. In lesion-level evaluation, we treat a prediction as a true positive if its overlap with a ground-truth lesion is >0.2 in Dice.  Lesions smaller than 3 mm in radius are ignored. As shown in Table "
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,,Comparison with Radiologists.,"In the reader study, we invited a senior radiologist with 16 years of experience in liver imaging, and a junior radiologist with 2 years of experience. They first read the NC CT of all subjects and provided a diagnosis of normal, benign, or malignant. Then, they read the DCE scans and provided a diagnosis of the 8 tumor types. We consider patients with only one tumor type in this study. Their reading process is without time constraint. In Table  Comparison with Literature. In the pixel level, we obtain Dice scores of 77.2% and 84.2% using NC and DCE CTs, respectively. The current state of the art (SOTA) of LiTS "
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 8.
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,1.0,Introduction,"Nephropathy is a progressive and incurable disease with high mortality, occurring commonly in the general adult population, with a world-wide prevalence of 10%  There exist only a few IF image based DNN methods, probably because the properties of IF images are complicated and have not been fully exploited. Different from the natural and other medical images, a IF sequence usually include multiple modalities from different types of fluorescent "
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,,Modality,"To address above issue, this paper proposes a novel customized multi-teacher knowledge distillation framework for nephropathy diagnosis on IF images. Specifically, we establish a large-scale IF image dataset including 7 types of nephropathy from 1,582 patients. By mining our dataset, we conduct experiments to explore the importance of a individual modality contributing to each nephropathy, as the empirical medical prior (see Fig.  -We establish a large-scale IF dataset containing 7 nephropathy categories and  hospital including 69 IF sequences with 348 images. Note that our work is conducted according to the Declaration of Helsinki. Compared with the existing IF image datasets "
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,2.2,Dataset Analysis,"Based on our main dataset, we further conduct data analysis to obtain the following findings about the relationship between different modalities and nephropathy."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,,Finding 1: The proportions of each IF modality vary greatly in different nephropathy.,"Analysis: As introduced above, the collected modalities of a IF sequence are usually incomplete. This is partially due to the importance of each IF modality for the specific nephropathy, since the patient may not have the anti-body of the useless fluorescent. Therefore, we count the proportions of 7 IF modalities on each nephropathy over 1,582 IF sequences in our main set. As shown in Fig. "
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,,"Finding 2: For certain combinations, the single-modality IF image achieves better diagnosis accuracy than multi-modality IF sequences over DNN models.","Analysis: To explore the impact of each modality in DNN models, we conduct experiments to evaluate the effectiveness of single IF modality for diagnosing each nephropathy. Specifically, we compare the nephropathy diagnosis accuracy of the same DNN model, when trained and tested over multi-modality IF sequences versus single-modality IF images. First, two widely-used classification models (ResNet-18  Thus, the higher error weight indicates that the single-modality can achieve more accurate diagnosis compared with using all modalities. Table "
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,3.1,Nephropathy Diagnosis Network,"Here, we introduce the detailed structure of the nephropathy diagnosis network, as the backbone structure for both student and teacher networks. Note that the backbone structure is flexible, and we implement 4 advanced DNNs in the experimental section. Taking ResNet-18 for the student network as an example, as illustrated in Fig.  (2) In  (3)"
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,3.2,Customized Recruitment Module,"A customized recruitment module is developed to adaptively select the effective teacher networks, on the top of the medical priors from our findings. As shown in Fig.  For the medical prior part, we first construct the adjacency matrix A ∈ R M ×N , in which M and N indicate the number of IF modalities and nephropathy. In A, the corresponding element is set as 1, when the IF modality is found to have positive influence over 2 DNN models in Table  where α and β are rescaling hyper-parameters. Additional, for the learnable part, the last level feature of the student network FS 4 is passed through a max pooling layer to obtain the representation v s ∈ R K×1 for student network, in which K is channel number of FS 4 . Let θ and {v t,i } M i=1 denote the learnable k-element rescaling weights and M teacher network representations. Then, the correlation between student network and teacher network can be formulated as the inner product between vector representations v t,i , θ v s . In summary, the learnable importance weights w l can be presented as where V is the matrix of {v t,i } M i=1 , while denotes element-wise multiplication. Note that V and θ are initialized with 1, and optimized during training. Finally, a overall modality importance weight w t can be obtained as"
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,3.3,Multi-level Knowledge Distillation,"Here, a multi-level knowledge distillation is developed between teacher and student networks. Based on the modality importance weight w t from our recruitment module, the multi-level features {FC i } M i=1 and predicted logits l c of are fused from multiple teacher networks: In above equation, w j t , l j t , FT j i are the importance weight, predicted logits, and the i-th level features for the j-th teacher networks, respectively. After that, to transfer the learned knowledge from teacher to student network, the logit loss is introduced by calculating the Kullback-Leibler (KL) divergence between the fused l c and predicted logits l s from the student network: Meanwhile, in order to make the student network fully learn from diagnosis processes of teacher networks, mean square error (MSE) losses are conducted on each level of fused {FC i } 4 i=1 and student network features {FS i } 4 i=1 : Finally, the overall loss function for the student network can be written as where λ fea , λ logits and λ cls are the hyper-parameters for balancing single losses."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,4.1,Experimental Settings,"In our experiments, all IF images are resized to 512 × 512 for consistency. During training, the parameters are updated by Adam optimizer with an initial learning rate of 0.0001. Then, each teacher network is pre-trained for 70 epochs, and then the student network with our distillation method is trained for 460 epochs. Finally, our and 8 other compared methods are trained over the training set of main set, and evaluated over the main test set and the external set, by adopting 3 evaluation metrics of accuracy, kappa and F1-score."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,4.2,Evaluation on Knowledge Distillation,"To evaluate the effectiveness of the proposed customized knowledge distillation method, we implement it over 4 different backbone models of ResNet-18 "
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,4.3,Comparisons with the State-of-the-Art Models,"We evaluate the diagnosis performance of our method over our main dataset, compared with 10 DNN based methods, i.e., ShuffleNet "
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,4.4,Ablation Study,"We ablate different components of our method to thoroughly analyze their effects on nephropathy diagnosis. Specifically, the accuracy degrades 0.028, 0.023, and 0.031, when ablating medical prior, learnable weights, and recruitment module (equal distillation), respectively. Besides, other ablations, such as the number of teacher networks and distillation loss, are also analyzed. The ablation experiments are reported in Fig. "
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,5.0,Conclusion,"In this paper, we propose a customized knowledge distillation method for IF based nephropathy diagnosis. Different from the existing methods that averagely integrate information of different IF modalities, we propose a knowledge distillation framework to transfer knowledge from the trained single-modality teacher networks to a multi-modality student network. In particular, a recruitment module and multi-level knowledge distillation are developed to dynamically select and fuse the knowledge from teacher networks. The extensive experiments on several backbone networks verify the effectiveness of our proposed framework."
Recruiting the Best Teacher Modality: A Customized Knowledge Distillation Method for if Based Nephropathy Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_51.
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,1.0,Introduction,"The clinical courses of neurodegenerative pathologies such as Alzheimer's or Parkinson's Diseases span multiple years and encompass intricate evolution of patients' cognitive abilities, physiological biomarkers and brain structure. Longitudinal studies are an essential tool for clinicians to uncover the diseases' mechanisms. In such studies, biomarkers and cognitive scores of patients are repeatedly measured at different times and need to be analyzed together, usually with a two-sided scope. First, to describe the general process at play across a whole cohort of patients: this is population-level modelling and allows to describe the average course of the disease. A second layer aims at explaining and predicting the variability observed among individuals: this is personalized-level modelling. Mixed-effect frameworks are widely adopted to address these multi-layered prospects, offering to disentangle fixed effects (population level) from random effects (individual level) to explain the variability of the disease. Linear mixedeffects models are the simplest instances of such models. Generalized linear and non-linear mixed-models are now often prefered to account for the neurodegenerative diseases' peculiarities, and most state of the art disease progression models (e.g.  Inter-patient variability is thereby modelled through random perturbations β i around a fixed reference α. However, it is known that some of this clinical variability between patients is explained by external factors (and thus hardly explained entirely by random perturbations). Genetic mutations or external factors such as gender, family history, education or socio-economics levels can influence the course of pathologies. Accumulating evidence suggests that the variability induced by such covariates stems from general mechanisms shared across the population, for instance in Alzheimer's and Parkison's diseases  Our contribution is to propose a slight change in this mixed-effect paradigm to allow non-linear models to also be influenced by these variables. Instead of estimating a fixed effect α (parametrizing the average disease course) as well as random effects β i , we introduce a link function f ϕ that can predict, given a set of covariates c i (e.g. sex, education level, SNP arrays, genetic risk scores), an expected trajectory of the disease conditionned by these covariates. The main difference between the standard approach and our method is that the previously introduced fixed-effects α are now estimated for each subject as a deterministic function of their covariates f ϕ (c i ). It also differs from accounting for the heterogeneity through hierarchical progression models  We demonstrate the value of this approach by adapting a general modelling framework, namely a non-linear Bayesian model: the Disease Course Mapping (DCM) "
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,2.0,Method,We derive here an algorithm that learns to model repeated observations while accounting for the heterogeneity explained by additional covariates.
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,2.1,A Generic Mixed-Effects Geometric Model,In their seminal paper  The choice of the manifold's metric shapes the geodesic trajectories and thus the disease model  which gives the trajectories of Eq. (  (1) Biomarker 2 = = = = Fig. 
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,2.2,Covariate Association and Statistical Framework,"We provide here a statistical instantiation of the previous geometric model. As described, given a geodesic trajectory γ 0 (fully specified by its position p 0 and speed v 0 at initial time t 0 ), and a set of random effect ψ i and w i , individual trajectories of an individual i observed at times (t i,j ) j are modelled by the curve η wi γ0 (ψ i (t i,j )). We propose that γ 0 (which represents the reference disease course, as a fixed-effect of the model) is to be computed for each subject i from the measured covariates c i as: where f belongs to a parametrized family of functions and ϕ are its parameters treated as the new fixed-effect of the model. The individual effects to register this computed γ 0,(i) onto observations are characterized by two random effects: an acceleration factor ξ i and a time-shift τ i such that ψ i (t) = e ξi t -t 0,(i) -τ i . On top of these are space-shifts w i ∈ R N , computed thanks to an ICA: w i = As i , where A is a latent matrix of independent directions (fixed effect) and s i is the corresponding individual latent source vector (random effect). Our hierarchical statistical model treats the fixed and random effects as a set of latent variables z which is the reunion of the population and individual variables z pop = {ϕ, A} and We posit the following priors on these latent parameters, where θ hyper = {σ ϕ , σ A } are fixed hyperparameters and θ model = ϕ, A, σ 2 τ , σ 2 ξ , σ 2 are the parameters of the model to be estimated: A non-informative prior is used over these model parameters due to the lack of a-priori knowledge. We seek to maximize a posteriori the joint-likelihood under the following additive Gaussian noise modelling prior on model parameters (taken non informative) It can be shown that the model's likelihood function lies in the curved exponential family. That is there exist two smooth functions Φ and Ψ functions of θ model and a measurable sufficient statistics function S(y, z) of the data and the latent realizations such that log q(y, z, θ model ) factors as: This allows estimating our model with a Monte-Carlo Markov-Chain Stochastic Approximation version of the Expectation Maximization algorithm (MCMC-SAEM) while enjoying theoretical guarantees of convergence  We choose to parametrize the link function as a linear mapping between covariates and dynamic parameters f ϕ (c i ) = ϕ slope • c i + ϕ intercept . This will provide an interpretable model to explain the general processes linking covariates to dynamic features such as the base pace of the disease or average onset time. The coefficients of ϕ that correspond to the mapping between the covariates c i and v 0 measure how much a given covariate impact the progression speed of each feature, and can be analyzed easily. Model parameters are initialized by setting their intercept to the models learned by a regular DCM model without considering covariates, while latent parameters are initialized at random."
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,3.1,Simulated Data,"We used the generative abilities of the DCM  -as binary covariates that directly dictated which hardcoded model is used to simulate the repeated measurements (covariate thought as a mutation-status or sex for instance). -as continuous covariates, influencing the simulated progression by using them as convex coefficients in a combination of the reference models. The covariates are seen as continuous risk factors of following one form or another. Our simulated datasets typically included 500 subjects with an average of 5 visits and an average follow-up duration of about 5±2 years and a measurement noise of around 5%. Such an experiment is summarized in Fig.  In this example, our calibrated model correctly matches each covariate to its simulated effect. For instance: the coefficients of the link function related to the disease initial speed v 0 (we refer to Fig.  Similarly, the coefficients associated with the irrelevant covariate did not capture any significant effect (factors of 1.03 [0.94, 1.12], 1.05 [0.98, 1.12] and 1.04 [0.97, 1.11] for the memory, motor and language features), which validates the ability of the model to discard covariates without influence on the disease dynamic."
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,3.2,Multimodal Clinical Data,"Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). ADNI was launched in 2003 as a public-private partnership led by Michael W. Weiner, MD. For up-to-date information, see https://www.adni-info.org. We selected subjects that eventually converted to an MCI or AD stage during their follow-up. This amounted to 1440 patients for a total of 9343 visits. The follow-up duration was 4.069 (± 3.190) years, with a baseline age of 73.683 (± 7.508) years old. We processed and included biomarkers relevant to monitor AD progression: -Two cognitive scores: the Mini-Mental State Exam (MMSE) and the AD Assessment Scale-Cognitive (ADAS-Cog). We normalized and inverted them so that they both cover the [0, 1] interval, (1 being the highest abnormality). -Hippocampus and Ventricles volumes, measured by structural T1 MRI and normalized by patient's Intracranial Volume (ICV). As for the cognitive scores, these measurements were rescaled to [0, 1] interval. -Contrasted PET imaging derived brain-averaged amyloid β42 and phosphorylated τ proteins loads, also rescaled to [0, 1]. We also sample an irrelevant covariate that is never used to modulate the disease course. In (a) is the resulting model, which we can visualize for any combination of covariates (two combinations are presented). We also plot the credible intervals (at 95%) for the coefficients linking covariates to the speed of progression on each feature (multiplicative effect, thus 1.0 stands for no influence while a coefficient of 1.5 stands for an expected progression speed greater by 50%)."
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,,APOE-ε4,". We calibrated our model by including a covariate known to modulate Alzheimer's Disease course, namely the patient's APOE mutation status. The results of this model are showcased in Fig.  Features can be grouped into a cognitive scores group, more impacted than the other features by the mutation (  Fig.  SNP Associations. We selected a subset of 69 Single Nucleotide Polymorphisms (SNP) among the top associations with AD diagnosis from a reference Genome-Wide Association Study (GWAS) "
A Multimodal Disease Progression Model for Genetic Associations with Disease Dynamics,4.0,Conclusion,"We proposed a framework to adapt a state of the art Bayesian non-linear mixedeffect disease progression model to capture the effects of external covariates into the disease dynamic. We implemented an estimation algorithm, and show that it reliably provides new interpretable measures of interaction between covariates and the disease course. For instance, we recover the (clinically known) association between the APOE-ε4 mutation and cognitive dysfunction. In particular, its use on genetic data (either single mutation status or SNP arrays) could help to go beyond associations with the sole diagnosis and provide complementary tools to GWAS."
Self-supervised Learning for Endoscopic Video Analysis,1.0,Introduction,"Endoscopic operations are minimally invasive medical procedures which allow physicians to examine inner body organs and cavities. During an endoscopy, a thin, flexible tube with a tiny camera is inserted into the body through a small orifice or incision. It is used to diagnose and treat a variety of conditions, including ulcers, polyps, tumors, and inflammation. Over 250 million endoscopic procedures are performed each year globally and 80 million in the United States, signifying the crucial role of endoscopy in clinical research and care. A cardinal challenge in performing endoscopy is the limited field of view which hinders navigation and proper visual assessment, potentially leading to high detection miss-rate, incorrect diagnosis or insufficient treatment. These limitations have fostered the development of computer-aided systems based on artificial intelligence (AI), resulting in unprecedented performance over a broad range of clinical applications  This study introduces Masked Siamese Networks (MSNs "
Self-supervised Learning for Endoscopic Video Analysis,2.0,Background and Related Work,"There exist a wide variety of endoscopic applications. Here, we focus on colonoscopy and laparoscopy, which combined covers over 70% of all endoscopic procedures. Specifically, our study addresses two important common tasks, described below. Cholecystectomy Phase Recognition. Cholecystectomy is the surgical removal of the gallbladder using small incisions and specialized instruments. It is a common procedure performed to treat gallstones, inflammation, or other conditions affecting the gallbladder. Phase recognition in surgical videos is an important task that aims to improve surgical workflow and efficiency. Apart from measuring quality and monitoring adverse event, this task also serves in facilitating education, statistical analysis, and evaluating surgical performance. Furthermore, the ability to recognize phases allows real-time monitoring and decision-making assistance during surgery, thus improving patient safety and outcomes. AI solutions have shown remarkable performance in recognizing surgical phases of cholecystectomy procedures  Optical Polyp Characterization. Colorectal cancer (CRC) remains a critical health concern and significant financial burden worldwide. Optical colonoscopy is the standard of care screening procedure for preventing CRC through the identification and removal of polyps  3 Self-supervised Learning for Endoscopy SSL approaches have produced impressive results recently "
Self-supervised Learning for Endoscopic Video Analysis,3.1,Masked Siamese Networks,"SSL has become an active research area, giving rise to efficient learning methods such as SimCLR  During pretraining, on each image x i ∈ R n of a mini-batch of B ≥ 1 samples (e.g. laparoscopic images) we apply two sets of random augmentations to generate anchor and target views, denoted by x a i and x t i respectively. We convert each view into a sequence of non-overlapping patches and perform an additional masking (""random"" or ""focal"" styles) step on the anchor view by randomly discarding some of its patches. The resultant anchor and target sequences are used as inputs to their respective image encoders f θ a and f θ t . Both encoders share the same Vision Transformer (ViT  where 0 < τ t < τ a < 1 are temperatures and Q ∈ R K×d is a matrix whose rows are the prototypes. The probabilities are promoted to be the same by minimizing the cross-entropy loss H(p t i , p a i ), as illustrated in Fig.  . Thus, the overall training objective to be minimized for both θ a and Q is where λ > 0 is an hyperparameter and the gradients are computed only with respect to the anchor predictions p a i,m (not the target predictions p t i ). Applying MSNs on the large datasets described below, generates representations that serve as a strong basis for various downstream tasks, as shown in the next section."
Self-supervised Learning for Endoscopic Video Analysis,3.2,Private Datasets,"Laparoscopy. We compiled a dataset of laparoscopic procedures videos exclusively performed on patients aged 18 years or older. The dataset consists of 7,877 videos recorded at eight different medical centers in Israel. The dataset predominantly consists of the following procedures: cholecystectomy (35%), appendectomy (20%), herniorrhaphy (12%), colectomy (6%), and bariatric surgery (5%). The remaining 21% of the dataset encompasses various standard laparoscopic operations. The recorded procedures have an average duration of 47 min, with a median duration of 40 min. Each video recording was sampled at a rate of 1 frame per second (FPS), resulting in an extensive dataset containing 23.3 million images. Further details are given in the supplementary materials. Colonoscopy. We have curated a dataset comprising 13,979 colonoscopy videos of patients aged 18 years or older. These videos were recorded during standard colonoscopy procedures performed at six different medical centers between the years 2019 and 2022. The average duration of the recorded procedures is 15 min, with a median duration of 13 min. To identify and extract polyps from the videos, we employed a pretrained polyp detection model "
Self-supervised Learning for Endoscopic Video Analysis,4.0,Experiments,"In this section, we empirically demonstrate the power of SSL in the context of endoscopy. Our experimental protocol is the following: (i) first, we perform SSL pretraining with MSNs over our unlabeled private dataset to learn informative and generic representations, (ii) second we probe these representations by utilizing them for different public downstream tasks. Specifically, we use the following two benchmarks. (a) Cholec80  Implementation Details. For SSL we re-implemented MSNs in JAX using Scenic library "
Self-supervised Learning for Endoscopic Video Analysis,4.1,Results and Discussion,"Scaling Laws of SSL. We explore large scale SSL pretraining for endoscopy videos. Table  As seen, SSL-based models provide enhanced robustness to limited annotations. When examining the cholecystectomy phase recognition task, it is evident that we can achieve comparable frame-level performance by using only 12% of the annotated videos. Using 25% of the annotated videos yields comparable results to the fully supervised temporal models. Optical polyp characterization results show a similar trend, but with a greater degree of variability. Using small portions of PolypSet (12% and 25%) hindered the training process and increased sensitivity to the selected portions. However, when using more than 50% of PolypSet, the training process stabilized, yielding results comparable to the fully supervised baseline. This feature is crucial for medical applications, given the time and cost involved in expert-led annotation processes."
Self-supervised Learning for Endoscopic Video Analysis,4.2,Ablation Study,Table 
Self-supervised Learning for Endoscopic Video Analysis,5.0,Conclusion,"This study showcases the use of Masked Siamese Networks to learn informative representations from large, unlabeled endoscopic datasets. The learnt representations lead to state-of-the-art results in identifying surgical phases of laparoscopic procedures and in optical characterization of colorectal polyps. Moreover, this methodology displays strong generalization, achieving comparable performance with just 50% of labeled data compared to standard supervised training on the complete labeled datasets. This dramatically reduces the need for annotated medical data, thereby facilitating the development of AI methods for healthcare."
Self-supervised Learning for Endoscopic Video Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_55.
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,1.0,Introduction and Related Works,"Deep learning algorithms have shown success in performing computer-aided diagnosis (CAD) tasks using high-dimensional medical images, such as classification  The diagnosis of COVID-19 is challenging. Convolutional Neural Networks (CNNs) and their variants, such as 3D CNNs  One of the major challenges is achieving consistency training in GCNs while preserving the integrity of the slice information. Existing graph model  Besides, various approaches have been developed to locate key slices in CT volumes under weak supervision. For instance, "
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,2.1,Problem Statement,"j=1 , where N V (i) is the cardinality and varies for each volume. The volume-level label is given by y i . We first extract the native descriptors of the slices using a spatial feature extractor, where the output of F ext is an m 0dimensional vector. To perform volume-level prediction under the guidance of shared semantic vectors, we introduce the Global Semantic-guided Dual-stream Graph (GSDG) model: ŷ = F GSDG (X, C). C indicate semantic vectors and will be introduced in the following. Figure "
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,2.2,Constructing Super-Nodes,"We introduce a method for grouping native nodes X into super-nodes H which we denote as H = F Gro (X, C). To accomplish this, we propose semantic vectors correspond to K groups and are shared across all volumes, end-to-end updated with the model. In an unsupervised setting, previous work  To achieve semantic uniformity, we reformulate p(y c |x i,j ) as p(y c |x i,j , c yc ): where τ is a temperature hyper-parameter. The mapping from native nodes to semantic vectors is described by Eq. 2. We represent this mapping using Q ∈ R K×N V (i) , and optimize it to maximize the similarity between the native node features and the semantic vectors of their corresponding groups. Therefore, the optimization objective of F Gro can be formulated as follows: min p,q E(p, q) s.t. ∀y c : q(y c |x i,j ) ∈ {0, 1} and We utilize the Sinkhorn-Knopp (SK) algorithm "
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,2.3,Bi-level Adjacency Matrices,"We aim to train GCN on a dataset of variable-length volumes; and have already grouped the native-nodes into super-nodes, H. Then, we could depict the adjacency relationships between nodes in H by semantic vectors or native nodes. Inspired by the multi-resolution model design in CNNs, we explicitly model the global and local adjacency relations: A G , A L = F Adj (X, S, C). A G represents the global semantic adjacency matrix, while A L the local sequence adjacency matrix of the learned super-nodes from X. Global Adjacency Matrix Based on Grouping Semantic Vectors. The existing study  /s , in the global adjacency matrix, g 1 i,j , g 2 i,j ∼ Gumbel(0, 1) and s is a hyper-parameter."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,,Local Adjacency Matrix Based on Native Sequence Association.,"To account for the associations varying with relative distance between slices within a volume, we utilize exponential smoothing to create a sequence adjacency matrix A ∈ R N V (i) ×N V (i) for each volume. The adjacency value between native nodes i and j is calculated by: A i,j = tanh( ). Here, s represents the output of the sigmoid function applied to a learnable vector w L ∈ R D , and D is a hyper-parameter. We combine the connectivities of native nodes belonging to the same group using the allocation matrix S introduced in Sect. 2.2 and obtain the reduced local adjacency matrix appliable to super-nodes: A L = S T AS."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,2.4,Dual-Stream Graph Classifier,"We introduce a graph classification module, denoted as ŷ = F Cls (H, A G , A L ), consisting of stacked Base Graph Modules (BGM) and a classifier. The BGM comprises two parallel isomorphic graph convolutions, a global feature GCN layer and a local sequence GCN layer, and a Multi-graph Bilinear Pooling (MgBP) module. Base Graph Module. The two GCN layers pass messages between super-nodes from distinct perspectives and output H G , H L ∈ R m×K . Then the MgBP module extracts fine-grained graph-level representation, F, using a low-rank multimodal bilinear module: Classification Head. To obtain hierarchical features, we concatenate F i from each BGM layer along the feature dimension, resulting in F f inal = NG i=1 F i ∈ R m×K . We then compute the mean and max along the node dimension separately, resulting in two length-m vectors. These vectors are concatenated and passed through a 2-layer MLP and softmax activation for classification. Weakly-Supervised Informative Slice Localization. Firstly, we obtain the predicted probability p base for the target class from F f inal using the Classification Head. Then, we mask each super-node in turn to create K sub-matrices of size m × (K -1). The Head is utilized again to calculate the new probabilities, which results in a vector p ∈ R K . The groups are ranked by d sn = p basep. Within a group, the distances between the native nodes and the super-node are measured using the dot product and normalized to the interval [0,1], which results in d rn . Slices' global importance within the group i is d sn(i) /d rn . We repeat this procedure for all groups and select the top k slices globally."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,3.1,Dataset and Pre-processing,Our experiment used a public CT volume dataset 2019nCoVR  We chosen ResNet-50 
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,3.2,"Differential Diagnosis, Prognosis and Weakly-Supervised Localization",Table  Our proposed method outperforms the state-of-the-art weakly-supervised graph model GCN-DAP  It is worth noting that our model required fewer training epochs than 
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,3.3,Visualization of Grouping and Slice Localization,"We visualized the grouping of native slices in Supplementary Material S.1 and the weakly-supervised slice localization for two patients in S.2. It can be observed that slices belonging to the same group display a remarkable degree of visual resemblance. Besides, the group importance distribution of NCP and CP cases are more similar to each other than to the Normal case, which reflects patterns differences between positive and negative cases. Within the positive cases, our model exhibits a tendency to concentrate more on the lung base in the CP case, as evidenced by columns 4 and 5 of Fig. "
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,3.4,Ablation Study,The ablation study was conducted to evaluate the effectiveness of different node alignment methods and graph structures on the performance of the proposed model for variable-length volumes. The results are presented in Tables 
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,4.0,Conclusion,"This paper proposes a novel approach for handling variable-length volume while preserving the integrity of the data by not discarding any slices, which is a departure from the previous method. Our approach first introduces a shared global semantic vectors-guided native node grouping scheme. Then we present an efficient and effective dual-stream graph module for simultaneously learning representations from global semantic vectors and sequence associations specific to each volume. Additionally, our approach offers informative slice localization and visually-consistent grouping outcomes, which enhances interpretability for clinical purposes. Moreover, the current dataset format prevents us from using existing semantic segmentation techniques to remove non-pulmonary noise. We will delve deeper into this direction to enhance localization accuracy."
GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 45.
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,1.0,Introduction,"The wiring system within the human brain can be modeled as a complex graph, where the anatomical regions of interest (ROIs) are represented as nodes, and the white matter connectomes define edges between them  Recently, variants of graph neural networks (GNNs) have been successful in brain network analysis with feature aggregation and message-passing mechanism on graph nodes  In order to analyze the connectivity of brain networks directly, we propose a novel graph learning framework that allows a neural network to utilize the topological features by representing the brain network as a simplicial complex. We utilize the Hodge 1-Laplacian, i.e., L 1 ; in geometry, the 1-simplex denotes a line segment, and the Hodge Laplacian L 1 includes connection between different line segments (i.e., edges) in a graph depending on their directions. Leveraging the Hodge Laplacian lets us obtain directed relationships between graph edges (i.e., a brain network) as an undirected graph (i.e., Hodge Laplacian), and the edge weights in the original graph become a signal on the nodes comprehended by the Hodge Laplacian. Spatial convolution with the L 1 combines directed edge weights of the original graph based on the nodes that they are sharing. Together with a spatial graph convolution formulation, we construct our Hodge-Graph Neural Network (Hodge-GNN) which predicts labels of graphs purely based on the topology of the graphs without any node measures. The contributions of our work are 1) proposing a novel graph edge-learning framework on higher-order connectivity (i.e., connectivity between edges) of graphs with Hodge Laplacian, 2) defining spatial edge convolution layer that operates on graph edges directly, and 3) demonstrating superior performance on graph classification with brain connectivity from Alzheimer's Disease Neuroimaging Initiative (ADNI) with interpretability. Using Hodge-GNN, we depict brain connectivities that are highly associated with AD classification, which are corroborated by prior AD literature."
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,2.0,Related Work,"Higher-order connectivity in Spatial domain. To capture the relation of higher-order graph structures, Morris et al.  Line Graphs. Line graph transformation interchanges the nodes and edges in the original graph respectively, allowing the node-wise graph convolution to be performed edge-wise, which makes the learning of edge-embeddings feasible  Spectral Filtering of Graph Edges. Huang et al. "
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,3.0,Preliminaries: Simplicial Complex Representation,"A simplicial complex is a collection of simplices with various dimensional representations, where simplices refer to the basic blocks to represent objects in topological space. In detail, each simplex of various dimension can be seen as nodes (0-simplex), edges (1-simplex), triangles (2-simplex), and other higher dimensional counterparts. A simplicial complex composed of only 0-simplices is called a 0-skeleton, likewise, p-skeleton is composed of 0 to p-simplices. A graph, therefore, is a 1-skeleton with 0-and 1-simplices (nodes and edges)  In a simplicial complex, a p-chain is defined as a sum of p-simplicies, denoted as c = i α i σ i , where σ i are the p-simplices and the α i are either 0 or 1  . Also, the boundary operator ∂ p is represented using a boundary matrix B p to facilitate efficient computation of the Hodge Laplacian. The p-th boundary matrix B p can be defined as  where σ p i is the i-th p-simplex, and ∼ and denote similar and dissimilar orientations respectively. The boundary matrix B p relates the two adjacent simplices, i.e., p-and (p -1)-simplex, which will be used to define the Hodge Laplacian for higher-order graph representation in Sec. 4.1."
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,4.0,Proposed Method,"The proposed method is composed of two components; graph transformation of adjacency matrix to Hodge Laplacian L 1 with edge-wise features, and edgewise graph convolution using the L 1 . With the traditional graph convolution formulation, the network can conduct transform of topological features directly instead of using them as indirect measures in previous GNNs."
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,4.1,Hodge Laplacian of Brain Network Data,"Let G = (V, E) be a directed weighted graph, where V is a set of nodes, and E is a set of directed edges consisting of ordered tuples, (u, v), s.t. u, v ∈ V, which denotes an edge from u to v. E is indexed with {e i } E i=1 , |E| = E is the number of edges, and |V| = N is the number of nodes. Hodge Laplacian L p , also known as the p-Laplacian is a generalization of graph Laplacian on higher simplices, i.e., nodes (0-simplices) to p-simplices. The Hodge Laplacian L p is defined using the B p (i, j) in Eq. (  From Eq. (  where B 1 ∈ R N ×E is a boundary matrix for the 1simplex, i.e., an incidence matrix relating nodes to edges. To enable the graph representation to hold connectivity over edges, we construct Hodge Laplacian L 1 . As a 1-skeleton, i.e. topological graph, is composed of 0 and 1 simplex only (i.e., B 2 = 0), the L 1 ∈ R E×E is derived as: Considering the vertices u, v, t ∈ V, s.t. u =v =t, each element L 1 (i, j) is defined as: A directed weighted graph G can be represented as a binary adjacency matrix A ∈ R N ×N and a weight matrix W ∈ R N ×N for A. Since W holds features for each edge, we can extract the non-zero components of W, which serves as a signal W E ∈ R E on L 1 ."
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,4.2,Convolving Graph Edges via Hodge Laplacian L 1,"GCN  where A ∈ R N ×N is an adjacency matrix, and W (l) is a parameter matrix of the l-th layer. H (l) ∈ R N ×K is the output of the l-th convolution layer with K features, where H (0) is the input of node feature vectors, and σ(•) is a nonlinear activation function. From a spatial perspective, the graph convolution relates the neighboring node features to generate the node embedding utilizing the adjacency matrix as a relational matrix that provides the direct neighboring information. From a weighted adjacency matrix A ∈ R N ×N , we can extract Hodge Laplacian L 1 ∈ R E×E and W E which is a vector of edge weights considered as measurements on the nodes of L 1 . With L 1 , we construct a Hodge graph neural network (Hodge-GNN) whose l-th layer is defined as: where W (l) is a learnable weight parameter and H (l) ∈ R E×K is the output from the l-th convolution layer, with H (0) = W E . The key component here is L 1 H (l) , described as Edge-wise Convolution in Fig.  When it comes to graph analysis, most of existing GNN methods assume that features on the nodes exist for node-wise analysis. However, when the measurements on the nodes do not exist, and the analysis must be performed solely with the graph topology and edge information, other GNN methods must define an auxiliary node-wise measures such as node degree and clustering coefficients. Unlike the previous approaches, our framework enables the information from adjacent edges to be given different weights, either positive or negative, depending on the topology of the graph, and the edge-wise convolution can now relate the edge features and generate edge embeddings, allowing the network to utilize the hidden topological features that were not seen in the original input graph form. Finally, the class prediction Ŷ c for each class c is obtained by flattening the H (L) ∈ R E×K and passing it through multi-layer perceptron (MLP), and applying a softmax yields The objective function defined by cross-entropy over all T samples is: where"
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,4.3,Interpretability of the Connectomes in Brain Dysfunction,"To provide interpretability to the framework, we define gradient-based class activation map on the graph edges using the formulation in  Performing the edge-wise convolution from Hodge Laplacian L 1 , this heatmap H c holds the contribution of each connectome to the classification of developmental stages in brain dysfunction (i.e., Alzheimer's Disease)."
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,5.1,Dataset and Experimental Settings,"Dataset. Our dataset contains structural brain connectivity data derived from Diffusion Tensor Images (DTI) in Alzheimer's Disease Neuroimaging Initiative (ADNI) with tractography. Each sample is given as a directed weighted graph whose weights denote the number of white matter fiber tracts connecting two different ROIs and its corresponding diagnostic label. The ROIs and their connectomes were defined by the Destrieux atlas  Baselines. Our method is validated on various approaches, including conventional classification methods, neural networks, and graph methods of both spatial and spectral domain. In detail, we used support vector machine (SVM), single layer perceptron (SLP), multi-layer perceptron (MLP), and GCN  Evaluation. 4-way classification is performed on the AD-specific groups. All models are evaluated using 5-fold cross validation (CV) for unbiased results. On the 4-way classification task for AD-specific groups, average accuracy, Macroprecision, Macro-recall, and Macro-F1-score (Table "
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,5.2,Experimental Results,"Our Hodge-GNN is evaluated based on the 4-way classification of CN, EMCI, LMCI, and AD. The quantitative comparisons are shown in Tab. 1. As k-GNN, MEMET, and GCN with G L capture higher-order connectivity information, they performed better than the conventional GCN with G. However, Hodge-GNN was more effective in the AD classification, achieving the highest performance in all measures by ∼3%p over the second best method. "
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,5.3,Interpretation of AD via Trained Hodge-GNN,"Using the computed importance from Eq. (  In addition, the depicted edges showed several symmetry found in both left and right hemispheres, such as pallidum-putamen and amygdala-hippocampus connectomes, both of which play a crucial role in the development of AD "
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,6.0,Conclusion,"We proposed a novel framework for extracting edge-to-edge relations in graph spatial domain using Hodge 1-Laplacian, i.e., Hodge-GNN. The Hodge-GNN performs graph convolution on edges via shared nodes among edges and allows a downstream predictor to accurately classify different stages of AD. The validation experiment showed superiority of performance in prediction together with interpretable outcomes depicting specific connectomes and ROIs for effective AD analysis. "
Convolving Directed Graph Edges via Hodge Laplacian for Brain Network Analysis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 76. 
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,1.0,Introduction,"By virtue of the human body's complexity, most diseases present phenotypic variability in terms of symptoms, rate of progression and imaging findings, which challenges diagnostic criteria. Among many hard to diagnose diseases, Chronic Obstructive Pulmonary Disease (COPD) stands out, as it is extensively underand misdiagnosed  Considering the limitations of spirometry as the standard diagnostic method  Instead of attempting to learn all possible complex manifestations of the disease, we ask: Could COPD be more accurately detected if considered as an anomaly from the distribution of healthy lungs? As previously reported for anomaly detection  1. We show the benefit of reformulating COPD prediction as an anomaly detection task. Inspired by  To the best of our knowledge, this work is the first to investigate anomaly detection in the context of a heterogeneous lung disease classification and has the potential to be applied to a wide range of diffuse diseases affecting large body areas. "
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,2.0,Method,"Our proposed method cOOpD aims at reformulating COPD classification as anomaly detection. It is a self-supervised anomaly detection framework, inspired by the strategy of "
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,2.1,Patch-Level Representations Using Contrastive Learning,"The latent representations of the encoder are learned with a self-supervised contrastive task, creating clusters based on semantic information. For this, we follow the contrastive training described in "
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,2.2,Generative Models Operating on Representation Space,"Once having extracted the latent representations, the distribution of normal representations is modeled, by fitting a generative model p(z) on the representations of purely normal patches. Patch normality is defined by % emphysema < 1% strictly applied to normal individuals, a very restrictive bound to guarantee that no intensity alterations could be present in the definition of normality. % emphysema is defined as the percentage of low attenuation areas less than a threshold of -950 Hounsfield units "
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,3.0,Experiment Setup,Dataset & Preprocessing: Paired inspiratory and expiratory volumetric CT images were used from two nationwide multi-center studies (COPDGene  Lung Parenchyma Segmentation for Patch Extraction: Lung masks were generated on the inspiratory image space using a nnU-Net model  Intensity Normalization: Inter-scanner variability was addressed by normalizing the intensity values to a scale between 0 (air) and 1 (tissue)  Baselines: State-of-the-art (SotA) baselines were applied to 2D slices and 3D patches. A 2D-CNN 
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,,Contrastive Representations Ablation:,"The contrastive latent representations' usefulness was evaluated with a supervised method (ReContrastive) that maps the latent representations back to their position in the original image, producing a 4D image, where the 4 th dimension is the length of the latent representation vector (Suppl). The produced image is then used as input for a CNN classifier. Training was performed for 500 epochs using the SGD Optimizer, a learning rate of 1e-2, Cosine Annealing  Evaluation Metrics: We used Area Under Receiver Operator Curve (AUROC) and Area Under Precision Recall Curve (AUPRC) as the default multi-threshold metric for classification. AUROC is used as the main evaluation metric since it is less sensitive to class balance changes. Final Method Configurations: These were chosen based on the highest AUROC on three experiment runs on the validation set. The best patch extraction configuration for all tested 3D methods was two-channel (inspiratory and registered expiratory) with 20% patch overlap. The best performance was always achieved with a ResNet34. For our proposed cOOpD method, GMM with κ = 4 was found to be the best performing generative model."
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,,Real-World Prevalence Ablation:,Given the global prevalence of COPD at 10.3% 
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,4.0,Results,As shown in Table 
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,5.0,Discussion,"Final Method Configurations: The best working method configurations reflect the following properties of the task: Using both inspiratory and expiratory images provides information about pulmonary vascular alterations and airway wall thickness not visible on the inspiratory scan alone, as in line with  Should COPD Binary Classification be Formulated as Anomaly Detection? cOOpD performance shows to be significantly superior compared to all tested methods, on the COPDGene (internal) and COSYCONET (independent external) test sets. The lower performance in the external test set was consistent with all other methods. There are several potential explanations for this. Besides being a highly imbalanced dataset, all patients in COSYCONET have a diagnosis of COPD and only 15% are categorized into GOLD 0 due to normal lung function. We hypothesize that these 15% ""healthy"" individuals have early signs of disease that are not captured by voxel-based methods but are being encoded by the latent representations. Considering that cOOpD was trained only on healthy representations from the COPDGene dataset, whose normal class consisted of never-smokers and GOLD 0 subjects, it can still outperform all the other methods, since the unseen traits of the disease are seen as anomalies. The advantage of solely modeling the healthy distribution is further highlighted by the real-world experiments (Fig.  Are Self-supervised Patch-Level Latent Representations Advantageous over Voxels? Both methods working on the representation space (cOOpD and ReContrastive) outperform all voxel-based baselines on the internal test set. Although for ReContrastive this improvement is no longer seen for the external test set, being the worst performing method, the early signs of disease for the healthy class of COSYCONET are likely being encoded by the latent representations, as mentioned earlier. We hypothesize that this performance drop stems from the problem of supervised models depicted in Fig. "
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,6.0,Conclusion,"Our proposed reformulation of COPD binary classification into an anomaly detection task (cOOpD) demonstrated superior performance compared to SotA methods. Additionally, the advantage of using latent representations was demonstrated. The cOOpD approach also demonstrated stability in performance when trained on datasets with simulated real-world class imbalance. Future work should focus on further validation on larger and more diverse datasets, longitudinal evaluation, and exploring its application to other heterogeneous diseases where annotated diseased data is scarce and access to healthy data is abundant."
cOOpD: Reformulating COPD Classification on Chest CT Scans as Anomaly Detection Using Contrastive Representations,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 4.
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,1.0,Introduction,"Viral or metabolic chronic liver diseases that cause liver fibrosis impose great challenges on global health. Accurate staging for the severity of liver fibrosis is essential in the diagnosis of various liver diseases. Current deep learning-based methods  The aim of multi-view learning is to exploit complementary information from multiple features  To enhance the interpretability and reliability of multi-view learning methods, recent works have proposed uncertainty-aware decision-level fusion strategies. Typically, they first estimate uncertainties through Bayesian methods such  as Monte-Carlo dropout  In this work, we propose an uncertainty-aware multi-view learning method with an interpretable fusion strategy of liver fibrosis staging, which captures both global features across views and local features in each independent view. The road map for this work is shown in Fig.  Our contribution has three folds. First, we are the first to formulate liver fibrosis staging as a multi-view learning problem and propose an uncertaintyaware framework with an interpretable fusion strategy based on Dempster-Shafer Evidence Theory. Second, we propose to incorporate global representation in the multi-view learning framework through the data-efficient transformer network. Third, we evaluate the proposed framework on enhanced liver MRI data. The results show that our method outperforms existing multi-view learning methods and yields lower calibration errors than other uncertainty estimation methods."
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,2.0,Methods,The aim of our method is to derive a distribution of class probabilities with uncertainty based on multiple views of a liver image. The pipeline for view extraction is shown in Fig. 
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,2.1,Subjective Logic for Uncertainty Estimation,"Subjective logic, as a generalization of the Bayesian theory, is a principled method of probabilistic reasoning under uncertainty  1 , e k 2 , ..., e k C ] with non-negative elements for C classes is estimated through the evidential network, which is implemented using a classification network with softplus activation for the output. According to subjective logic, the Dirichlet distribution of class probabilities Dir(p k |α k ) is determined by the evidence. For simplicity, we follow  where and predicted probabilities pk can be derived in an end-to-end manner."
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,2.2,Combination Rule,"Based on opinions derived from each view, Dempster's combination rule  where N = 1i =j b 1 i b 2 j is the normalization factor. According to Eq. (  For opinions from K local views and one global view, the combined opinion could be derived by applying the above rule for K times, i.e., D = D 1 "
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,2.3,Global Representation Modeling,"To capture the global representation, we apply a data-efficient transformer as the evidential network for the global view. We follow  As shown in Fig.  LSA modifies self-attention in ViT by sharpening the distribution of the attention map to pay more attention to important visual tokens. As shown in Fig.  where q, k, v are the query, key, and value vectors obtained by linear projections of X. M is the diagonal masking operator that sets the diagonal elements of qk T to a small number (e.g.,-∞). τ ∈ R is the learnable scaling factor."
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,2.4,Training Paradigm,"Theoretically, the proposed framework could be trained in an end-to-end manner. For each view k, we use the integrated cross-entropy loss as in  where ψ is the digamma function and y k is the one-hot label. We also apply a regularization term to increase the uncertainty of misclassified samples, where λ is the balance factor which gradually increases during training and αk = y k + (1 -y k ) α k . The overall loss is the summation of losses from all views and the loss for the combined opinion, where L Combined and L Global are losses of the combined and global opinions, implemented in the same way as L k . In practice, we pre-train the evidential networks before training with Eq. ( "
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,3.1,Dataset,The proposed method was evaluated on Gd-EOB-DTPA-enhanced 
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,3.2,Implementation Details,"Augmentations such as random rescale, flip, and cutout  The transformer network was pre-trained for 200 epochs using the same setting. The framework was implemented using Pytorch and was run on one Nvidia RTX 3090 GPU."
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,3.3,Results,"Comparison with Multi-view Learning Methods. To assess the effectiveness of the proposed multi-view learning framework for liver fibrosis staging, we compared it with five multi-view learning methods, including Concat  As shown in Table  Comparison with Uncertainty-Aware Methods. To demonstrate reliability, we compared the proposed method with other methods. Specifically, these methods estimate uncertainty using Monte-Carlo dropout (Dropout)  Interpretability. The proposed framework could explain which view of the input image contains more decisive information for liver fibrosis staging through uncertainties. To evaluate the quality of explanations, we compared the estimated uncertainties with annotations from experienced physicians. Views that contain more signs of fibrosis are supposed to have lower uncertainties. According to Fig. "
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,4.0,Conclusion,"In this work, we have proposed a reliable and interpretable multi-view learning framework for liver fibrosis staging. Specifically, uncertainty is estimated through subjective logic to improve reliability, and an explicit fusion strategy is applied which promotes interpretability. Furthermore, we use a data-efficient transformer to model the global representation, which improves the performance."
A Reliable and Interpretable Framework of Multi-view Learning for Liver Fibrosis Staging,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 18.
