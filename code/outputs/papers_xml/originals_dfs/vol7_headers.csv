Paper Title,Header Number,Header Title,Text
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,1,Introduction,"Sarcopenia is a prevalent musculoskeletal disease characterized by the inevitable loss of skeletal muscle, causing increased risks of all-cause mortality and disability that result in heavy healthcare costs  In this study, we propose MSKdeX: Musculoskeletal (MSK) decomposition from a plain X-ray image for the fine-grained estimation of lean muscle mass and volume of each individual muscle, which are useful metrics for evaluating muscle diseases including sarcopenia. Figure "
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,2,Method,
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,2.1,Dataset Preparation,"Figure  Since muscles deform depending on the joint angle, they are not aligned. Instead, we exploited the invariant property of muscles using the newly proposed intensity-sum loss."
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,2.2,Model Training,"We train a decomposition model G to decompose an X-ray image into the DRRs = {V DRR, M DRR, W V DRR} to infer the lean muscle mass and muscle volume, adopting CycleGAN  Structural Consistency. We call the summation of a DRR over all the channels (objects) the virtual X-ray image defined as , where I DRR i is the i-th object image of a DRR. We applies reconstruction GC loss  to maintain the structure consistency between an X-ray image and decomposed DRR, where G(I X ) W V DRR is the decomposed WVDRR. However, we do not apply reconstruction GC loss for VDRR and MDRR because of lacking attenuation coefficient information. Instead, we propose inter-DRR/intra-object GC loss L β GC defined as to chain the structural constraints from WVDRR to VDRR and MDRR, where the , and G(I X ) MDRR i are i-th object image of the decomposed WVDRR, VDRR, and MDRR, respectively. The st(•) operator stops the gradient from being back-propagated in which the decomposed VDRR and MDRR are expected to be structurally closer to WVDRR (not vice-versa) to stabilize training. Thus, our structural consistency constant L up GC is defined as where the λ gca balances the two GC losses. Intensity Sum Consistency. Unlike general images, our DRRs embedded specific information so that the intensity sum represents physical metrics (mass and volume). Furthermore, the conventional method did not utilize the paired information of an X-ray image and DRR (obtained from the same patient). We took advantage of the paired information, proposing the object-wise intensitysum loss, a simple yet effective metric invariant to patient pose and projection direction, for quantitative learning. The OWIS loss L IS is defined as: where I DRR i and S(•) are the i-th object image of DRR and the intensity summation operator (sum over the intensity of an image), respectively. The H and W are the image height and weight, respectively, served as temperatures for numeric stabilizability. The intensity consistency objective L all IS is defined as Partially Aligned Training. A previous study  where the K is a set of indexes containing aligned bone indexes. The N b is the size of the set K. The λ l1 tries to balance structural faithfulness and quantitative accuracy. The objective of partially aligned pixel-wise learning is defined as Full Objective. The full objective, aiming for realistic decomposition while maintaining structural faithfulness and quantitative accuracy, is defined as where the λ is re-weights the penalty on the proposed OWIS loss. "
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,3,Experiments and Results,"The automatic segmentation results of 552 CTs were visually verified, and 13 cases with severe segmentation failures were omitted from our analysis, resulting in 539 CTs. Four-fold cross-validation was performed, i.e., 404 or 405 training data and 134 or 135 test data per fold. The baseline of our experiment was the vanilla CycleGAN with the reconstruction GC loss proposed in  The average PCC for the muscles was improved from 0.457 to 0.826 by adding the OWIS loss (λ is = 100) and to 0.796 by adding the bone loss, while their combination achieved the best average PCC of 0.855, demonstrating the superior ability of quantitative learning of the proposed MSKdeX. The results also suggested that the weight balance for loss terms needs to be made to achieve the best performance. More detailed results are shown in supplemental materials.  "
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,4,Summary,"We proposed MSKdeX, a method for fine-grained estimation of the lean muscle mass and volume from a plain X-ray image (2D) through the musculoskeletal decomposition, which, in fact, recovers CT (3D) information. Our method decomposes an X-ray image into DRRs of objects to infer the lean muscle mass and volume considering the structural faithfulness (by the gradient correlation loss chain) and quantitative accuracy (by the object-wise intensity-sum loss and aligned bones training), outperforming the conventional method by a large margin as shown in Sect. 3. The results suggested a high potential of MSKdeX for opportunistic screening of musculoskeletal diseases in routine clinical practice, providing a new approach to accurately monitoring musculoskeletal health. The aligned bone DRRs positively affected the quantification of the density and volume of the muscles as shown in the ablation study in Sect. 3, implying the deep connection between muscles and bones. The prediction of muscles overlapped with the pelvis in the X-ray image can leverage the strong pixel-wise supervision by the aligned pelvis's DRR, which can be considered as a type of calibration. Our future works are the validation with a large-scale dataset and extension to the decomposition into a larger number of objects."
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,,Fig. 1 .,
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,,Fig. 2 .,
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,,Fig. 3 .,
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,,Fig. 4 .,
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,,Table 1 .,
MSKdeX: Musculoskeletal (MSK) Decomposition from an X-Ray Image for Fine-Grained Estimation of Lean Muscle Mass and Muscle Volume,,Table 2 .,
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,1,Introduction,"Abdominal organ segmentation from medical images is an essential work in clinical diagnosis and treatment planning of abdominal lesions  Training CNNs for segmentation with scribble annotations has been increasingly studied recently. Existing methods are mainly based on pseudo label learning  Differently from most existing weakly supervised methods that are designed for 2D slice segmentation with a single or few organs, we propose a highly optimized 3D triple-branch network with one encoder and three different decoders, named TDNet, to learn from scribble annotations for segmentation of multiple abdominal organs. Particularly, the decoders are assigned with different dilation rates  2) We propose two novel consistency loss functions, i.e., Uncertainty-weighted Soft Pseudo label Consistency (USPC) loss and Multi-view Projection-based Class-similarity Consistency (MPCC) loss, to regularize the prediction from the pixel-wise and class-wise perspectives respectively, which helps the segmentation network obtain reliable predictions on unannotated pixels. 3) Experiments results show our proposed method outperforms five existing scribble-supervised methods on the public dataset WORD "
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,2,Method,"Figure  For the convenience of following description, we first define several mathematical symbols. Let X, S be a training image and the corresponding scribble annotation, respectively. Let C denote the number of classes for segmentation, and Ω = Ω S ∪ Ω U denote the whole set of voxels in X, where Ω S is the set of labeled pixels annotated in S, and Ω U is the unlabeled pixel set."
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,2.1,Triple-Branch Multi-Dilated Network (TDNet),As shown in Fig. 
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,2.2,Pixel-Wise and Class-Wise Consistency,"Uncertainty-Weighted Soft Pseudo Label Consistency (USPC). As the three decoders capture features at different scales that are complementary to each other, an ensemble of them would be more robust than a single branch. Therefore, we take an average of P 1 , P 2 , P 3 to get a better soft pseudo label P = (P 1 + P 2 + P 3 )/3 that is used to supervise each branch during training. However, P may also contain noises and be inaccurate, and it is important to highlight reliable pseudo labels while suppressing unreliable ones. Thus, we propose a regularization term named Uncertainty-weighted Soft Pseudo label Consistency (USPC) between P n (n = 1, 2, 3) and P : where Pi refers to the prediction probability at voxel i in P , and Pn,i is the corresponding prediction probability at voxel i in Pn . KL() is the Kullback-Leibler divergence. w i is the voxel-wise weight based on uncertainty estimation: where the uncertainty is estimated by entropy. c is the class index, and P c i means the probability for class c at voxel i in the pseudo label. Note that a higher uncertainty leads to a lower weight. With the uncertainty-based weighting, the model will be less affected by unreliable pseudo labels."
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,,Multi-view Projection-Based Class-Similarity Consistency (MPCC).,"For multi-class segmentation tasks, it is important to learn inter-class relationship for better distinguishing them. In addition to using L USP C for pixel-wise supervision, we consider making consistency on class relationship across the outputs of the decoders as illustrated in Fig.  Similarly, P n is projected in the sagittal and coronal views, respectively, and the corresponding normalized class affinity matrices are denoted as Q sagittal n and Q coronal n , respectively. Here, the affinity matrices represents the relationship between any pair of classes along the dimensions. Then we constraint the consistency among the corresponding affinity matrices by Multi-view Projection-based Class-similarity Consistency (MPCC) loss: where v ∈ {axial, sagittal, coronal} is the view index, and Qv is the average class affinity matrix in a certain view obtained by the three decoders."
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,2.3,Overall Loss Function,"To learn from the scribbles, the partially Cross-Entropy (pCE) loss is used to train the network, where the labeled pixels are considered to calculate the gradient and the other pixels are ignored  where S represents the one-hot scribble annotation, and Ω S is the set of labeled pixels in S. The total object function is summarized as: where α t and β t are the weights for the unsupervised losses. Following "
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,3,Experiments and Results,
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,3.1,Dataset and Implementation Details,We used the publicly available abdomen CT dataset WORD  Our framework was implemented in PyTorch  Table 
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,,Organ,FullySup 
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,3.2,Comparison with Other Methods,"We compared our method with five weakly supervised segmentation methods with the same set of scribbles, including pCE only  Compared with the second best method DMPLS "
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,3.3,Ablation Experiment,"We then performed ablation experiments to investigate the contribution of each part of our method, and the quantitative results on the validation set are shown in Table  Additionally, Table "
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,4,Conclusion,"In this paper, we proposed a scribble-supervised multiple abdominal organ segmentation method consisting of a 3D triple-branch multi-dilated network with two-level consistency constraints. By equipping each decoder with different dilation rates, the model leverages features at different scales to obtain high-quality soft pseudo labels. In addition to mine knowledge from unannotated pixels, we also proposed USPC Loss and MPCC Loss to learn unsupervised information from the uncertainty-rectified soft pseudo labels and class affinity matrix information respectively. Experiments on a public abdominal CT dataset WORD demonstrated the effectiveness of the proposed method, which outperforms five existing scribble-based methods and narrows the performance gap between weakly-supervised and fully-supervised segmentation methods. In the future, we will explore the effect of our method on sparser labels, such as a volumetric data with scribble annotations on one or few slices."
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,,Fig. 1 .,
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,,Fig. 3 .,
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,,Table 2 .,
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,1,Introduction,"Breast cancer is the most prevalent form of cancer among women and can have serious physical and mental health consequences if left unchecked  Deep neural networks have been widely adopted for breast cancer diagnosis to alleviate the workload of radiologists. However, these models often require a large number of manual annotations and lack interpretability, which can prevent their broader applications in breast cancer diagnosis. Radiologists typically focus on areas with breast lesions during mammogram reading  Radiologists' eye movements can be automatically and unobtrusively recorded during the process of reading mammograms, providing a valuable source of data without the need for manual labeling. Previous studies have incorporated radiologists' eye-gaze as a form of weak supervision, which directs the network's attention to the regions with possible lesions  Mammography primarily detects two types of breast lesions: masses and microcalcifications  In this work, we propose a novel diagnostic model, namely Mammo-Net, which integrates radiologists' gaze data and interactive information between CC-view and MLO-view to enhance diagnostic performance. To the best of our knowledge, this is the first work to integrate gaze data into multi-view mammography classification. We utilize class activation map (CAM)  Our contributions can be summarized as follows: • We emphasize the significance of low-cost gaze to provide weakly-supervised positioning and visual interpretability for the model. Additionally, we develop a pyramid loss that adapts to the supervised process. • We propose a novel breast cancer diagnosis model, namely Mammo-Net. This model employs transformer-based attention to mutualize information and uses BFL to integrate task-related information to make accurate predictions. • We demonstrate the effectiveness of our approach through experiments using mammography datasets, which show the superiority of Mammo-Net. 2 Proposed Method"
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,2.1,Overall Architecture,"The pipeline of Mammo-Net is illustrated in Fig.  The fusion network combines multi-view feature representations using a stack of linear-activation layers and a fully connected layer, resulting in a classification output."
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,2.2,Gaze Supervision,"In this module, we utilize CAM to calculate the attention map for the network by examining gradient-based activations in back-propagation. After that, we employ pyramid loss to make the network attention being consistent with the supervision of radiologists' gaze heat maps, guiding the network to focus on the same lesion areas as the radiologists. This module guides the network to accurately extract pathological features."
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,,Class Activation Map.,"At the final convolutional layer of our model, the activation of the ith feature map f i (x, y) at coordinates (x, y) is associated with a weight w k i for class k. This allows us to generate the attention map H k for class k as: Pyramid Loss. To enhance the learning of important attention areas, we propose a pyramid loss constraint that requires consistency between the network and gaze attention maps. The pyramid loss is based on using a pyramid representation of the attention map: where H is the network attention map generated by the CAM and R is the radiologist's gaze heat map. square error (MSE) between the attention maps generated by the radiologist and the model at each level of the Gaussian pyramid. This allows the model to mimic the attention of radiologists and enhance diagnostic performance. Moreover, the pyramid representation enables the model to learn from the important pathological regions on which radiologists are focusing, without the need for precise pixel-level information. Layernorm is also employed to address the issue of imprecise gaze data. This reduces noise in the consistency process by performing consistency loss only in the regions where radiologist spent most time."
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,2.3,Interactive Information,"Transformer-Based Mutualization Model. We use transformer-based attention to mutualize information from the two views at the level of the spatial feature map. For each attention head, we compute embeddings for the source and target pixels. Our model does not utilize positional encoding, as it encodes the relative position of each pixel and is not suitable for capturing information between different views of mammograms  Subsequently, the output is transformed into attention-based feature maps X and mutualized with the feature maps Y from the other view. The mutualized feature maps are normalized and used for subsequent calculations: Bidirectional Fusion Learning. To enable the fusion network to retain more of the shared features between the two views and filter out noise, we propose to use BFL to learn a fusion representation that maximizes the cross-view mutual information. The optimization target is to generate a fusion representation I from multi-view representations p v , where v ∈ {cc, mlo}. We employ the Noise-Contrastive Estimation framework  where s(I, p v ) evaluates the correlation between multi-view fused representations and single-view representations  where N (I) is a reconstruction of p v generated by a fully connected network N from I and the Euclidean norm || • || 2 is applied to obtain unit-length vectors. In contrastive learning, we consider the same patient mammograms as positive samples and those from different patient mammograms in the same batch P i v = P v \{p i v } as negative samples  In short, we require the fusion representation I to reversely reconstruct multiview representations p v so that more view-invariant information can be passed to I. By aligning the prediction N (I) to p v , we enable the model to decide how much information it should receive from each view. The overall loss function for this module is the sum of the losses defined for each view:"
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,2.4,Loss Function,"We use binary cross entropy loss (BCE) between the network prediction and the ground-truth as the classification loss. In conclusion, we have proposed a total of three loss functions to guide the model training: L BCE , L BF L , and L P yramid . The overall loss function is defined as the sum of these three loss functions, with coefficients λ and μ used to adjust their relative weights: 3 Experiments and Results"
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,3.1,Datasets,Mammogram Dataset. Our experiments were conducted on CBIS-DDSM  Eye Gaze Dataset. Eye movement data was collected by reviewing all cases in INbreast using a Tobii Pro Nano eye tracker. The scenario is shown in Appendix and can be accessed at https://github.com/JamesQFreeman/MicEye. Participated radiologist has 11 years of experience in mammography screening.
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,3.2,Implementation Details,We trained our model using the Adam optimizer 
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,3.3,Results and Analysis,"Visualization. Figure  The results of the visualization demonstrate that the model's capability in localizing lesions becomes more precise when radiologist attention is incorporated in the training stage. The pyramid loss improves the model's robustness even when the radiologist's gaze data is not entirely focused on the breast. This intuitively demonstrates the effectiveness of training the model with eye-tracking supervision. Ablation Study. We perform an ablation analysis to assess each component (radiologist attention, cross-view attention and BFL) in Mammo-Net. Table "
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,4,Conclusion and Discussion,"In this paper, we have developed a breast cancer diagnosis model to mimic the radiologist's decision-making process. To achieve this, we integrate gaze data as a form of weak supervision for both lesion positioning and interpretability of the model. We also utilize transformer-based attention to mutualize multi-view information and further develop BFL to fully fuse multi-view information. Our experimental results on mammography datasets demonstrate the superiority of our proposed model. In future work, we intend to explore the use of scanning path analysis as a means of obtaining insights into the pathology-relevant regions of lesions."
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,,Fig. 1 .,
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,,Fig. 2 .,
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,,Table 1 .,
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,,.889 0.849 Performance Comparison.,
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 7.
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,1,Introduction,The increased availability of radiological data and rapid advances in medical image analysis has led to an exponential growth in prediction models that utilize features extracted from clinical imaging scans to detect and diagnose diseases and predict response to treatment 
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,2,Methods,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,2.1,Datasets,"This study used two unique datasets: (1) the UCLA low-dose chest CT dataset, a collection of 186 exams acquired using Siemens CT scanners at an equivalent dose of 2 mGy following an institutional review board-approved protocol. The raw projection data of scans were exported, and Poisson noise was introduced, as described in Zabic et al. "
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,2.2,Normalizing Flows,"In this section, we describe the normalizing flows and modifications that were made to improve computational efficiency. Deterministic approaches to image translation (e.g., using a convolutional neural network) attempt to find a mapping function y = g θ (x) that takes an input image x and outputs an image y that mimics the appearance of a target condition. For example, x could be an image acquired using a low dose protocol (e.g., 25% dose, smooth kernel), and y represents the image acquired at the target acquisition and reconstruction parameter (e.g., 100% dose, medium kernel). Flow-based image translation aims to approximate the density function y|x (y|x, θ) using maximum likelihood estimation.   which can be trained by maximizing the log-likelihood. In practice, a multilayer flow operation is preferred because a single-layer flow cannot represent complex non-linear relationships within the data. f is decomposed into a series of invertible neural network layers h n where h n = f n θ (h n-1 ; e(x)), n represents the number of layers, and e(x) represents a deep convolutional neural network that extracts salient feature maps of x upon which the flow layers are conditioned. For an N-layer flow model, the objective is to maximize Once the training is complete, the decoding function g θ (z; x) is applied using random latent variable z, which is drawn from the independent and identically distributed Gaussian density function. The use of z allows us to generate a range of possible restored images y , conditioned on the same input image x. Flow Layers. Flow layers must meet two requirements: 1) be invertible and 2) be a tractable Jacobian determinant. To compute the second term in Eq. 2, we apply the triangulation trick developed by Dinh et al.  Thus, by definition, Jacobian of h n+1 is a lower triangular matrix. Figure  • Activation normalization: A channel-wise batch normalization  • Feature conditional affine. We compute the scale and shift factor from e(x) again using a shallow CNN to apply the n-th layer flow transformation h. The motivation is to impose a relationship between feature maps extracted e(x) and activation maps h. The deep convolutional neural network extractor e(x) is based on Residual-in-Residual Dense Blocks (RRDB)  Multiscale Architecture. Since the flow approach is invertible, input x and latent space vector z must have the same dimensions. However, in most cases, y|x (y|x, θ) is a lowdimensional manifold in high-dimensional input space. Computation is inefficient when a flow model is imposed with a higher dimensionality than the dimension of true latent space. Given the multiscale architecture in RealNVP, we can simplify the model and improve the density estimation at multiple levels. The overall multiscale architecture is depicted in Fig.  Network Training. We trained CTFlow using a batch size of 16 and 50k iterations. The learning rate was set to 1e-4 and halved at 50%, 75%, 90%, and 95% of the total training steps. A negative log-likelihood loss was used."
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,2.3,Experiments,"We conducted two experiments to evaluate CTFlow: image quality metrics and impact on the performance of a lung nodule computer-aided detection (CADe) algorithm. Image Quality. Using the Grand Challenge dataset, we assessed image quality and compared it with other previously published low-dose CT denoising techniques. We computed image quality metrics using the peak signal-to-noise ratio (PSNR), structural similarity (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS)  Nodule Detection. We evaluated the ability of CTFlow to harmonize differences in reconstruction kernels and their effect on the performance of a lung nodule detection algorithm. Our CADe system was based on the RetinaNet model, a composite model comprised of a backbone network called feature pyramid net and two subnetworks responsible for object classification with bounding box regression. The model was trained and validated on the LIDC-IDRI dataset, a public de-identified dataset of diagnostic and low-dose CT scans with annotations from four experienced thoracic radiologists. As part of the training process, we only considered nodules annotated by at least three readers in the LIDC dataset. A total of 7,607 slices (with 4,234 nodule annotations) were used for training and 2,323 slices (with 1,454 nodule annotations) for testing in a single train-test split. A bounding box was then created around the union of all the annotator contours to serve as the reference for the detection model. After training for 200 epochs with Focal loss and Adam optimizer, the model achieved an average precision (AP@0.5) of 0.62 on the validation set. We hypothesized that the CTFlow models should yield better consistency in lung nodule detection performance compared with not normalizing or other state-of-the-art methods. As a comparison, we trained a 3D SNGAN model using the same training and validation set as CTFlow to perform the same task. We trained three separate CTFlow and SNGAN models to map scans reconstructed using smooth, medium, or sharp kernels to a reference condition. We computed the F1 score (the harmonic mean of the CADe algorithm's precision and recall) when executing the model on the CTFlow and SNGAN normalized scans. We then determined the Concordance Correlation Coefficient "
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,3,Results,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,3.1,Network Training,"On the Grand Challenge dataset, CTFlow took 3 days to train on an NVIDIA RTX 8000 GPU. The peak GPU memory usage was 39 GB. Unlike GANs that required two loss functions, our network was optimized with only one loss function. The negative log-likelihood loss was stable and decreased monotonically. "
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,3.2,Image Quality,Table 
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,3.3,Nodule Detection,Table 
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,4,Conclusion,"We developed CTFlow, a normalizing flows approach to mitigating variations in CT scans. We demonstrated that CTFlow achieved consistent performance across image quality metrics, yielding the best perceptual quality score. Moreover, CTFlow was better than a GAN-based method in maintaining consistent lung nodule detection performance. Compared to generative models, the normalizing flows approach offers exact and efficient likelihood computation and generates diverse outputs that are closer to the target distribution. We note several limitations of this work. In our evaluations, we trained separate CTFlow and comparison models for each mapping (e.g., transforming a 'smooth' kernel to a 'medium' kernel scan), allowing us to troubleshoot models more easily. A single model conditioned on different doses and kernels would be more practical. Also, CTFlow depends on tuning a variance parameter; better PSNR and SSIM may have been achieved with the optimization of this parameter. Finally, this study focused on mitigating the effect of a single CT parameter, either dose (in image quality) or kernel (in nodule detection). In the real world, multiple CT parameters interact (dose and kernel); these more complex interactions are being investigated as part of future work. One underexplored area of normalizing flow is its ability to generate the full distribution of possible outputs. Using this information, we can estimate where high uncertainty exists in the model output, providing information to downstream image processing steps, such as segmentation, object detection, and classification. For example, Chan et al. "
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,,Fig. 1 .,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,,Figure 1,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,,Fig. 2 .,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,,Fig. 3 .,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,,Table 1 .,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,,Table 2 .,
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,,Acknowledgments,. This work was supported by the 
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,1,Introduction,"Retinal diseases are one of the most common eye disorders, which can lead to vision impairment and blindness if left untreated. Computer-aided diagnosis has been increasingly used as a tool to detect ophthalmic diseases at the earliest possible time and to ensure rapid treatment. Optical Coherence Tomography (OCT)  Traditional methods manually design OCT features and adopt machine learning classifiers for prediction  Recent works have attempted to include additional modalities for classification through multi-modal learning shown in Fig.  To this end, we propose Fundus-enhanced Disease-aware Distillation Model (FDDM) for retinal disease classification from OCT images, as shown in Fig.  Our main goal is to extract disease-related information from a fundus teacher model and transfer it to an OCT student model, all without relying on paired training data. To achieve this, we propose a class prototype matching method to align the general disease characteristics between the two modalities while also eliminating the adverse effects of a single unreliable fundus instance. Moreover, we introduce a novel class similarity alignment method to encourage the student to learn similar inter-class relationships with the teacher, thereby obtaining additional label co-occurrence information. Unlike existing works, our method is capable of extracting valuable knowledge from any accessible fundus dataset without additional costs or requirements. Moreover, our approach only needs one modality during the inference process, which can help greatly reduce the prerequisites for clinical application. To summarize, our main contributions include 1) We propose a novel fundusenhanced disease-aware distillation model for retinal disease classification via class prototype matching and class similarity alignment; 2) Our proposed method offers flexible knowledge transfer from any publicly available fundus dataset, which can significantly reduce the cost of collecting expensive multi-modal data. This makes our approach more accessible and cost-effective for retinal disease diagnosis; 3) We validated our proposed method using a clinical dataset and other publicly available datasets. The results demonstrate superior performance when compared to state-of-the-art alternatives, confirming the effectiveness of our approach for retinal disease classification."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,2,Methodology,"Our approach is based on two ideas: class prototype matching, which distills generalized disease-specific knowledge unaffected by individual sample noise, and class similarity alignment, which transfers additional label co-occurrence information from the teacher to the student. Details of both components are discussed in the sections below. An overview of our framework is shown in Fig.  We denote the fundus dataset as , and the OCT dataset as To utilize knowledge from the fundus modality during training, we build a teacher model, denoted F t , trained on D f . Similarly, an OCT model F s is built to learn from OCT images D o using the same backbone architecture as the fundus model. We use binary cross-entropy loss as the classification loss L CLS for optimization, to allow the same input to be associated with multiple classes. During inference time, only OCT data is fed into the OCT model to compute the probabilities p = {p c } C c=1 for each disease, c."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,2.1,Class Prototype Matching,"To distill features from the teacher model into the student model, we aim to ensure that features belonging to the same class are similar. However, we note that individual sample features can be noisy since they contain variations specific to the sample instance instead of the class. In order to reduce noise and ensure disease-specific features are learnt, we compress features of each class into a class prototype vector to represent the general characteristics of the disease. During the training per batch, the class prototype vector is the average of all the feature vectors belonging to each category, which is formulated as: where e c f and e c o denote the prototype vector for class c of the fundus and OCT modality respectively, v f,i (v o,j ) represents the feature vector of the input image, and y c f,i y c o,j is a binary number which indicates whether the instance belongs to class c or not. P demotes an MLP projector that projects OCT features into the same space as fundus features. In the class prototype matching stage, we apply softmax loss to the prototype vectors of fundus modality to formulate soft targets E c f = σ(e c f /τ ), where τ is the temperature scale that controls the strength to soften the distribution. Student class prototypes E c o are obtained in the same way. KL divergence is then used to encourage OCT student to learn matched class prototypes with fundus teacher: By doing so, the OCT model is able to use the global information from fundus modality for additional supervision. Overall, our approach adopts class prototypes from fundus modality instead of noisy features from individual samples, which provides more specific knowledge for OCT student model."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,2.2,Class Similarity Alignment,"We also note that for multi-label classification tasks, relationships among different classes also contain important information, especially since label cooccurrence is common for eye diseases. Based on this observation, we additionally propose a class similarity alignment scheme to distill knowledge concerning inter-class relationships from fundus model to OCT model. First, we estimate the disease distribution by averaging the obtained logits of fundus and OCT model in a class-wise manner to get Then, to transfer information on inter-class relationships, we enforce cosine similarity matrices of the averaged logits to be consistent between teacher and student model. The similarity matrix for teacher model is calculated as  In this way, disease distribution knowledge is distilled from fundus teacher model, forcing OCT student model to learn additional knowledge concerning inter-class relationships, which is highly important in multi-label scenarios."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,2.3,Overall Framework,"The overall loss is the combination of classification loss and distillation enhancement loss: where α and β are loss weights that control the contribution of each distillation loss. Admittedly, knowledge distillation strategies in computer vision "
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,3,Experiments,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,3.1,Experimental Setup,"Dataset. To evaluate the effectiveness of our approach, we collect a new dataset TOPCON-MM with paired fundus and OCT images from 369 eyes of 203 patients in Guangdong Provincial Hospital of Integrated Traditional Chinese and Western Medicine using a Topcon Triton swept-source OCT featuring multimodal fundus imaging. For fundus images, they are acquired at a resolution of 2576 × 1934. For OCT scans, the resolution ranges from 320 × 992 to 1024 × 992. Specifically, multiple fundus and OCT images are obtained for each eye, Implementation Details. Following prior work  All the images are resized to 448 × 448 before feeding into the network. For a fair comparison, we apply identical data processing steps, data augmentation operations, model backbones and running epochs in all the experiments. We use SGD to optimize parameters with a learning rate of 1e-3, a momentum of 0.9, and a weight decay of 1e-4. The batch size is set to 8. For weight parameters, τ is set to 4, α is set to 2 and β is set to 1. All the models are implemented on an NVIDIA RTX 3090 GPU. We split the dataset into training and test subsets according to the patient's identity and maintained a training-to-test set ratio of approximately 8:2. To ensure the robustness of the model, the result was reported by five-fold cross-validation. Evaluation Metrics. We follow previous work "
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,3.2,Compare with State-of-the-Arts,"To prove the effectiveness of our proposed method, we compare our approach with single-modal, multi-modal, and knowledge distillation methods. From Table  To further demonstrate the efficiency of our proposed distillation enhancement approach, we validate our method on a publicly available multi-modal dataset with fundus and OCT images, MMC-AMD "
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,3.3,Results Trained with Other Fundus Datasets,"Since we implement distillation in a disease-aware manner, multi-modal fundus and OCT training data do not need to be paired. Theoretically, any publicly available fundus dataset could be applied as long as it shares a label space that overlaps with our OCT data. To verify this hypothesis, we separately reproduce our methods with fundus images from two datasets, MMC-AMD "
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,3.4,Ablation Studies,Table 
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,4,Conclusion,"Our work proposes a novel fundus-enhanced disease-aware distillation module, FDDM, for retinal disease classification. The module incorporates class prototype matching to distill global disease information from the fundus teacher to the OCT student, while also utilizing class similarity alignment to ensure the consistency of disease relationships between both modalities. Our approach deviates from the existing models that rely on paired instances for multi-modal training and inference, making it possible to extract knowledge from any available fundus data and render predictions with only OCT modality. As a result, our approach significantly reduces the prerequisites for clinical applications. Our extensive experiments demonstrate that our method outperforms existing baselines by a considerable margin."
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,,Fig. 1 .,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,,Fig. 2 .,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,,Fig. 3 .,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,,Table 1 .,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,,Table 2 .,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,,Table 3 .,
Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 60.
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,1,Introduction,"Coronary artery disease (CAD) is one of the most prevalent critical cardiovascular diseases with up to 32% mortality rate  In fact, a cardiac cycle has two phases, i.e., diastole and systole. The reconstructed arteries in the two-phased CCTA images are incomplete coronary trees, but they complement each other. By accurately aligning the arteries in both phases, the complete coronary tree can be reconstructed. Nevertheless, there are three challenges for successful coronary reconstruction. 1) Since the heart beats vigorously, its surrounding arteries can be squeezed by heart chambers and become invisible in one of the phases, easily causing the misalignment of a significant number of arteries in the two-phased images (short for component variation), as pointed by yellow (visible in diastole only) and cyan (visible in systole only) arrows in Fig.  In this paper, we propose a structural point registration network (SPR-Net) to align coronary arteries from the systolic and diastolic phases. The SPR-Net is designed to exploit both image-based and point-cloud-based features, in which the image and point cloud are encoded as intrinsic features. Additionally, we propose a transformer-based feature fusion module to fully exploit the obtained intrinsic features in extracting structural points, i.e., key points that delineate the anatomical morphology of arteries across the two phases and are solely used to compute the deformation field. For those obtained structural points, a simple thin-plate spline "
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,2,Method,"We propose the SPR-Net method, which simultaneously utilizes geometric features extracted from point clouds and image features extracted from CCTA images with the goal of generating structural points to align arteries across systole and diastole, with shape, location, and component variations. In this section, we first introduce the extraction of geometric features (Sect. 2.1), then the extraction of image features (Sect. 2.2), next the extraction of structural points and their usage in registration procedures (Sect. 2.3), and finally the loss function (Sect. 2.4)."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,2.1,Geometric Feature Learning,"The coronary arteries share a tubular structural shape. The point cloud network has the advantages of effectively learning the spatial geometric shape of arteries and providing accurate relative positional relationships of points  Given the input diastolic and systolic point clouds P and Q, we first use a sampling layer in the point cloud encoder to obtain the down-sampled points respectively. P and Q are then filled into the multi-scale grouping layer to aggregate its neighboring points within different radii r. After that, the multi-scale aggregated points are fed into the PointNet layer to extract geometric features."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,2.2,Geometric and Image Feature Encoding,"Point clouds can provide good geometric shapes and spatial location information, but they lack sufficient semantic features of coronary arteries. Meanwhile, the images contain rich contextual information that can complement the geometric features. Therefore, we design a transformer-based module to integrate both advantages. Specifically, 1) we employ a shallow 3D vision transformer (ViT)  where E i and I i respectively indicate the position encoding and image features for the i-th rectangular volume. (a, b, c) ∈ R 3 is the point coordinates. f img i ∈ R l is the self-attention input of transformer layer, and l is the feature dimension. 2) Geometry and Image Co-embedding. Given concatenated features of pointwise and image features, four transformer layers are employed to further explore comprehensive contextual features between the two phases. The transformer layer incorporates an encoder and decoder block, which are based on a multi-head attention mechanism. We use the concatenated features of the diastolic phase as input to the transformer encoder and decoder respectively, and the opposite for the systolic phase, to learn the feature dependencies between the two phases."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,2.3,Registration via Structural Point Correspondences,"1) Integration of Structural Points. The input of MLP is the contextual features extracted by the transformer, and the output is the probability of each point. Specifically, given the sampled points P with the fused features F P from diastole, we input the features into the shared MLP to generate the probability maps Thus, the diastolic structural points S p can be calculated as follows: Note that, the systolic structural points S q are calculated in the same way as the diastolic structural points. 2) Structural Points based Registration using TPS. Based on the correspondence established between the structural points S p and S q in the two phases, we apply a simple but effective idea of the TPS method to interpolate the dense deformation field. For the two sets of structural points, S p and S q , the nearest projection from structural points S q to the S p is calculated, and the S q is warped to the S p in the diastolic phase. Eventually, each systolic point is re-meshed by the closest point to the structural point and further warped to the original points Q using the estimated dense deformation field."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,2.4,Loss Function,"We design a structure-constrained registration loss for SPR-Net, where, Here L rec is chamfer distance, and X and Y denote two point clouds respectively. The first part L rec (S p , P ), and the second part L rec (S q , Q) assure the predicted structural points in two different phases are close to their corresponding original point clouds. The third part L rec (S p , S q ) encourages an accurate alignment of structural points between the two phases, ensuring that structural points with the same semantics align on the same vessel branch."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,3,Experiments and Results,
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,3.1,Dataset and Evaluation Metrics,"Data Processing. In our experiments, we collected 58 pairs of CCTA images with both diastolic and systolic phases. All coronary artery masks are first extracted using  where P o and Q o denote the set of coronary branches common to diastolic and systolic phases, respectively. Moreover, the Dice coefficient (Dice), Chamfer distance (CD), and Hausdorff distance (HD) are also employed for evaluation."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,3.2,Implementation Details,"The initial inputs of the SPR-Net contain 4096 point clouds for each phase, and a volume size of 16 × 16 × 8 is cropped around each point. The point cloud encoder consists of two set abstraction blocks with 1024 and 256 grouping centers respectively. In each set abstraction block, we utilize the grouping layer with two scales r to combine the multi-scale features, containing scales (0.1, 0.2, 0.4) and (0.2, 0.4, 0.8) respectively. The transformer blocks we used are composed of vanilla transformer layers. The outputs of the point cloud encoder and ViT have 512-D and 128-D features, respectively, which are concatenated together to form 640-D contextual features. The configuration of the MLP block in the structural point integration depends on the number of structural points. All experiments were implemented using Pytorch on 1 NVIDIA Tesla A100 GPU. We trained the networks using Adam optimizer with an initial learning rate of 10 -4 , epoch of 600, and batch size of 8."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,3.3,Comparison with State-of-the-Art Methods,"Our SPR-Net was quantitatively and qualitatively evaluated, compared with eight SOTA registration methods, which belong to three categories:1) imagebased registration, including SyN  Quantitative Results. The quantitative results are listed in Table "
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,3.4,Ablation Study,"We also conduct the ablation studies with the same backbone point cloud encoder by following three groups of configurations: 1) Whether using the four transformer layers, denoted as CoF, to encode and fuse the systolic and diastolic geometry. 2) Whether fusing the geometry features of point cloud with imagelevel semantic features, denoted GIF. 3) Testing the network on different numbers of structural points (Number-SP). Table  If without employing CoF and GIF, only the backbone encoder is used to generate structural points. 1) With the same 768 structural points, we can find the individual modules of CoF and GIF can both improve the Dice performance. Meanwhile, combining the two modules lead to the best performance, which may suggest the importance of fusing the two different aspects of features. 2) By equipping both CoF and GIF, we can find that SPR-Net's performance has been improved when the structural points number increases from 256 to 768. However, the performance decreases when it is further increased to 1024, indicating that dense structural points negatively affect the results, which is probably caused by the increasing number of outlier points. It can also be found that SPR-Net demonstrates inferior performance than both backbone+CoF and backbone+GIF when using 256 structural points, which is probably caused by the sparsity of structural points that are largely located at the endpoints and bifurcation positions, which cannot well delineate the morphology of vessel tree. Therefore, the number of structural points is a key parameter that affects registration performance. Through extensive experiments, we determine the optimal number of structural points to ensure one-to-one correspondences between diastole and systole (Table "
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,4,Conclusion,"In this paper, we have proposed an intrinsic structural point learning-based framework for systolic and diastolic coronary artery registration. The framework identifies structural points in the arteries across the two different phases using both the spatial geometric features extracted by the point cloud network and the complementary image semantic information extracted by ViT. By strategically fusing the image and point geometric features through a transformer, structural points with strong correlations in two different phases are extracted and used to guide the registration process. Compared with the existing image-based registration methods and point cloud-based methods, our integrated method achieves superior performance and outperforms the state-of-the-art methods by a large margin, which suggests the potential applicability of our framework in real-world clinical scenarios for CAD diagnosis."
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,,Fig. 1 .,
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,,Fig. 2 .,
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,,Fig. 3 .,
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,,Table 1 .,
SPR-Net: Structural Points Based Registration for Coronary Arteries Across Systolic and Diastolic Phases,,Table 2 .,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,1,Introduction,"The automatic segmentation of abdominal multiple organs is clinically significant in extremely that can significantly reduce clinical resource costs. However, the task of abdominal organ segmentation is difficult. The number of abdominal organs is large, and these multiple organs show diverse characteristics among themselves. For example, the shape of the stomach varies greatly even in the same individual at different times, making precise pixel segmentation extremely challenging. Accurate and automatic segmentation of readable results from abdominal multiple organs can provide accurate evidence of reality for surgical navigation, visual enhancement, radiation therapy, and biomarker measurement systems. Therefore, how to accurately make the segmentation results more readable in the case of multiple organs influencing each other has a great contribution to clinical examination and diagnosis. Abdominal multi-organ network models based on deep neural networks (DNN) are difficult to train. Training such a good enough model usually requires a large amount of labeled data, or the model performance is likely to meet a heavy drop. However, manual annotation of organs requires doctors to make accurate judgments based on their professional knowledge and rich experience, this leads to making manual labeling both expensive and time-consuming. In addition to pixel-level annotated datasets, deep neural networks can also benefit from other types of supervision. For example, boundary-level annotation can provide more detailed boundary information. In addition, weakly supervised  However, the practical process of collecting additional annotations remains challenging, as it may require clinicians to repeatedly provide specific and refined annotations to fine-tune the network model. There is a need to minimize the impact of the annotation process on clinical work. To address this, we investigate novel annotation information that can be used for abdominal multi-organ segmentation. In the context of medical image analysis, it has been observed that radiologists tend to focus their attention on specific regions of interest (ROIs) or lesions when interpreting medical images. Specifically, our method utilizes eye gaze information collected by an eye-tracker during radiologists' image interpretation as a source of additional supervision. In clinical practice, experienced radiologists can usually quickly locate specific organs when reading abdominal images. In this process, the doctor's eye movement information can reflect the location information of organs to a certain extent. Compared with manual label-ing, this information is cheap and fast and can be used as effective supervision information to assist the localization and segmentation of each organ. The literature studies have implied that the potential of the radiologist's gaze data can be high in improving disease diagnosis  In this paper, we propose a novel eye-guided multi-organ segmentation network for diverse abdominal organ images. The network model is forced to focus on relevant objects or features required for the segmentation task by fully and synergistically utilizing the radiologist's cognitive information about the abdominal image. This method of information collection is convenient and can make the positioning of each organ more accurate. The overall architecture is shown in Fig. "
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,2,Methodology,As shown in Fig. 
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,2.1,Wavelet Transform for Composite Information,"Wavelet transform is able to obtain global information and edge information in different directions in gaze attention heatmaps so that the network can effectively fuse the composite information in the heatmaps. In the clinic, when radiologists read abdominal images, the more important location, the longer the radiologists' gaze. We convert this information into a heatmap representation. The heatmap reflects the rough position information of the target to be segmented. The single heatmap is unable to reflect the composite information it contains, therefore DWT is utilized for extracting it. Discrete wavelet transform "
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,2.2,Multi-feature Skip Connection,"Multi-feature skip connection (MSC) comprehensively utilizes multiple features to guide the segmentation results of each abdominal organ toward accurate internal details. As shown in Fig.  where F i and F g represent the output features from the down-sampling layers of the two paths, and F fusion denotes the final multiple fusion features. Following the MSC, the dimension of the concatenated multiple features remains the same as the dimension of the upsampled features. "
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,2.3,Cross-Attention Transformer Module,"The cross-attention transformer module (CATM) creatively enables the communication between network semantic perception and human semantic perception. Different from the traditional self-attention mechanism in transformer block  where F i and F g represent the final output features of the encoder on the image attention and gaze attention encoding pathways. Our experimental results demonstrate that the cross-attention operation within the CATM design efficiently enhances the communication of information between these two paths. The convolution operation of CATM is used to fuse the feature information from two paths, which makes up for the possible information shortage in the decoding process. The CTB is the core design of the CATM, which is a variant of the Transformer  ) , and V in the image and gaze attention path, respectively; B denotes the learnable relative positional encoding; d is the dimension of K, and we set the number of head of multi-headed self-attention is 12."
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,3,Experiments,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,3.1,Datasets and Evaluation,"Our experiments use the Synapse multi-organ segmentation dataset(Synapse). Each CT volume consists of 85 -198 slices of 512 × 512 pixels, with a voxel spatial resolution of ([0.54 -0.54] × [0.98 -0.98] × [2.5 -5.0]) mm 3 . We use the 30 abdominal CT scans and split it 18 training cases and 12 testing cases randomly. Following  Comparison with Existing Methods Performance. As shown in Table  Qualitative Visualization Results. As shown in Fig. "
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,4,Conclusion,"In this paper, we propose a novel network that can realize the interactive communication between network semantic perception and human semantic perception, and apply it to the task of abdominal multi-organ segmentation for information interactive collaboration. The network is innovatively built with 1) a dual-path encoder that integrates human cognitive information; 2) a cross-attention transformer module (CATM) that communicates information in network semantic perception and human semantic perception; and 3) multi-feature skip connection (MSC), which effectively combines spatial information during down-sampling to offset the internal details of segmentation. Extensive experiments with promising results reveal gaze attention has great clinical value and potential in multi-organ segmentation."
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,Fig. 1 .,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,Fig. 2 .,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,Fig. 3 .,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,Fig. 4 .,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,Table 1 .,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,.16 3.2 Results and Analysis Overall Performance.,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,Table 2 .,
Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen,,Acknowledgements,. This study was supported by the 
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,1,Introduction,"Coronary Computerized Tomography Angiography (CCTA) is a commonly used non-invasive approach for the diagnosis of potential coronary artery diseases  In this paper, we propose a novel framework called TopoLab to perform topology-preserving automatic labeling of coronary arteries. Our model mainly contains two components: the hierarchical feature extraction module and the anatomy-aware connection classifier. The hierarchical feature extraction module introduces the segment query to achieve intra-segment feature aggregation via Transformer  To the best of our knowledge, there is currently no publicly available dataset with annotations for artery labeling. In this work, we contribute high-quality annotations to the orCaScore dataset  Our contributions can be summarized as follows. "
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,2,Related Work,Traditional methods 
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,3,Methodology,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,3.1,Overview,"We start by extracting centerlines from the vessel segmentation annotations in a CCTA image, using a traditional 3D thinning algorithm  As illustrated in Fig. "
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,3.2,Hierarchical Feature Extraction,"We first feed the CCTA image X ∈ R H×W ×D into an image encoder (e.g. U-Net  For each segment S i , trilinear interpolation in the downsampled feature map F is adopted for the centerline point v ∈ S i to obtain the corresponding point features Intra-Segment Feature Aggregation. To extract the segment-level features for labeling, aggregation of sequential point features belonging to the same segment is required. Previous studies  We introduce Transformer  Inter-Segment Feature Interaction. The branching structure of coronary arteries is inherently graph-like, with each segment serving as a node and the connections between segments serving as edges. Thus, we leverage graph convolutional network  Specifically, let Ê ∈ R N ×C denotes the aggregated segment features where the i-th item of Ê is Êi , and A ∈ R N ×N is the adjacency matrix for the vessel segment graph derived from the segment connections C. The process of GCN layers is as follows: where σ is the ReLU activation, W l ∈ R C×C is the learnable parameters for the l-th GCN layer, the input for the first layer is Ê0 = Ê. Finally, we fuse the input segment features Ê and the output of the final GCN layer Êf to obtain E = [ Êf , Ê]W f ∈ R N ×C with the parameters W f ∈ R (C+C)×C . The enhanced segment features {E 1 , E 2 , ..., E N } are forwarded to the classifier for segment labeling, where E i ∈ R C is the i-th item of E."
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,3.3,Anatomy-Aware Connection Classifier,"The direct approach for labeling the coronary arteries is to use a linear layer to classify each segment independently as in previous methods. To incorporate the category topological trees into the classifier design, we propose to conduct the classification task for every connected segment pair. We begin by defining the ground truth segment connections which are composed of the topology-conforming connections derived from the category topological trees (like LM→LAD, LCX→OM, etc.). Note that the self-connections (e.g. RCA→RCA) are also considered as the ground truth connections. The ground truth segment connections have the corresponding template embeddings for classification, which are represented by G ∈ R Ng×2C , where N g is the number of ground truth segment connections. Denote g i = Concate(Enc(x), Enc(y)) ∈ R 2C as the i-th item of G, where x and y stand for the segment classes indexed by the i-th ground truth connection, Enc denotes the sinusoidal encoding as in  The connection templates G are used to enable classification for the connected segment pairs. Then, given the segment features {E i } N i=1 , and all of the connected segment pairs C = {(i 1 , j 1 ), (i 2 , j 2 ), ..., (i Nc , j Nc )}, the connection features P ∈ R Nc×2C are obtained by rearrangement of the segment features, where the k-th item of P is Concate(E i k , E j k ) ∈ R 2C . We use an MLP layer to further fuse the features Training Loss. The loss function can be written as: where y i is the ground truth of the i-th segment connection, pi denotes the i-th item of P . sim in Eq. 3 stand for the cosine similarity, sim(x, y) = x T y ||x||2||y||2 , and the temperature τ is a hyperparameter which is set as 0.05 by default. Inference. During the inference stage, for each segment, we first select the connection with the largest confidence score among all segment connections that have covered the given segment. And then the corresponding category indexed by the selected connection serves as the prediction of the specific segment."
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,4,Experiments,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,4.1,Setup,Datasets. We train and evaluate our method on two datasets. The orCaScore 
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,4.2,Implementation Details,3D ResUNet 
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,4.3,Comparison with Other Methods,Quantitative Results. We compare TopoLab with other deep learning based approaches including TaG-Net  Qualitative Results. In Fig. 
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,4.4,Ablation Study,"In this subsection, we explore the effectiveness of different components in Topo-Lab on orCaScore dataset, as shown in Table  Intra-segment Feature Aggregation (IFA). Transformer  Inter-segment Feature Interaction (IFI). We use GCN to establish intersegment feature interactions due to the natural graph structure of coronary arteries. When directly removing this module, the performance drops by 1.57% in F1 score and 19.14% in Viola c as illustrated in the second line of Table  Anatomy-Aware Connection Classifier (ACC). AC-Classifier which exploits the prior knowledge from category topological trees is adopted to classify each connected segment pair. We replace it with a commonly used linear layer to enable classification for single segments as in previous methods, and the performance drops by 1.33% in Viola and 15.42% in Viola c , which effectively demonstrates the superiority of the proposed method. "
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,5,Conclusion,"In this study, we review the essential task of coronary artery labeling and exploit the prior knowledge of the predetermined anatomical connections. The proposed strategies of intra-and inter-segment feature aggregation guarantee effective feature extraction, while the AC-Classifier preserves the clinical logic in the network design. The extensive experiments on orCaScore dataset and in-house dataset reveal that the proposed TopoLab has achieved new state-of-the-art performance. We hope our paper could encourage the community to explore the use of clinical priority to facilitate the design of more effective algorithms."
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,,Fig. 1 .,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,,Fig. 2 .,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,,Fig. 3 .,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,,Table 1 .,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,,Table 2 .,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,,Table 3 .,
Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_71.
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,1,Introduction,"The 3D dental mesh segmentation is aimed to accurately separate the dental mesh into distinct components, namely individual teeth and gums. Therefore stable and accurate 3D dental mesh segmentation plays an essential role in various areas of oral medicine, including orthodontics and denture design, where precise tooth segmentation is of great importance for subsequent procedures and treatments. However, this task is accompanied by notable challenges arising from the inherent limitations in scan accuracy and the presence of considerable noise within the reconstructed 3D dental mesh, consequently leading to the blurring of tooth boundary. To solve these challenges, some methods have been widely explored. Some conventional methods usually utilize specific geometric properties  In recent years, many deep learning-based methods have been proposed to perform more accurate automated dental mesh segmentation. Some methods  First of all, these methods still have some difficulty on the tooth boundary especially for the crowded teeth. Although there always exist individual differences in the shape of teeth from person to person, the teeth at different positions do have distinctive shape priors that distinguish them from other teeth, such as canine teeth and molar teeth. If we can make full use of the shape priors attached to the semantic information of the teeth, these segmentation errors on the boundaries can be decreased greatly. Thus it provides a more promising way to perform the segmentation guided by the semantic information. Secondly, these methods are often confused at the molars since they only use graph convolution to model the dependencies. Therefore a better alternative would be utilizing the local and non-local semantic information at the same time to further enhance the features, that means the long distance dependencies also need to be considered. Lastly, the existing methods always directly perform concatenation on the features from different angles, resulting in the incorrect segmentation on the misaligned teeth. This is due to the fact that the importance of features from different perspectives can vary a lot in different regions. Hence regressing a specific weight and fusing the features with these weight parameters can effectively eliminate the feature imbalance and pay more emphasis on salient features. To address these issues, we propose a novel semantics-based feature learning network to fully utilize the semantic information and grasp the local and non-local dependencies. We first follow the TSGCNet  To conclude, our contributions are three-fold: "
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,2,Method,
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,2.1,Overview,"Our network mainly consists of a semantics-based graph-transformer module and an adaptive cross-domain feature fusion module, as shown in Fig. "
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,2.2,Semantics-Based Graph-Transformer,"The semantics-based graph-transformer module aims to generate the multi-scale cell-wise feature vectors in C-domain embedded with N-domain which can represent the geometric features of dental mesh at different positions accurately and discriminatively. We denote the initial feature vectors extracted from the dental mesh as a N × 24 matrix, N is the number of the cells and the 24-dimensional vector consists of 12-dimensional relative coordinates and 12-dimensional normal vectors. Then through a normal STN module  Semantics Prediction. The semantic prediction is mainly used to generate a pseudo cell-wise label for each cell which can effectively extract the semantic information. Denote the C-domain feature as C that has a shape of N × k, and the N-domain feature as N that has a shape of N ×k, and we regress a C-domain weight and a N-domain weight which indicates the weights of the domain fusion. So we have the cross-domain features F adaptively fused as: where ⊕ is the channel-wise concatenation, and j indicates the layer of the semantics-based graph-transformer modules, and c j , n j is the adaptive weights of C-domain and N-domain in layer j, and C j , N j is the output from the previous layer. And after that we perform a simple MLP to generate the final pseudo semantic cell-wise label formed as: where softmax can get the probability that the cells belong to each class and max outputs the index of the maximum value. Thus we have the cell-wise pseudo semantic label which can be used to make the difference between the features of cells belonging to different categories greater."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,,Graph-Transformer.,"The graph-transformer is composed of a semantic KNN and a Transformer Encoder Block. For the input C-domain, we first construct a KNN graph based on the semantic biased Euclidean distance formed as: where cell i and cell j indicate the ith cell and jth cell and the Semantic Dist function measures the difference of the two cells which is formulated as: where λ is a positive parameter that can be set according to the specific task and l i indicates the pseudo label of the ith cell. Then we perform a transformer encoder block on each cell to get the local dependencies which can enhance the local features belong to each class. The attention we used is a standard multihead attention, and we set the query and value as the matrices of neighbour features of the cells and the key as the distance matrix between cells and their neighbours."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,2.3,Adaptive Cross-Domain Feature Fusion,"The adaptive cross-domain feature fusion module aims to fuse the C-domain and N-domain features for the cell-wise segmentation. Through the above semanticsbased graph-transformer module we have obtained the accurate multi-scale Cdomain features embedded by the N-domain features. Then we need to fuse the features to integrate the spatial information and the geometric information of the cells. Therefore, we first perform the concatenation on the features of different scales and use MLP to fuse the multi-scale features together which can balance the features at different scales. This process is formulated as: where ⊕ is the channel-wise concatenation while c and n represent C-domain and N-domain features. Then through a global graph-transformer block, we just use the standard multi-head attention on the C-domain and N-domain respectively, so as to capture long distance dependencies and have global knowledge of the semantic information. Since the learnable weights can fuse the cross-domain features adaptively, we use the same cross-domain feature fusion strategy similar to that we used in the semantics prediction module. Further more, we use a single MLP to generate a feature mask and perform the dot product on the feature and the mask which is formulated as: where is the element-wise multiplication and F is the fused cross-domain features for segmentation."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,3,Experiments and Results,
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,3.1,Implementation Details,"Our network is implemented with PyTorch 1.11.0 on four NVIDIA GeForce RTX 3090 GPUs. The input meshes are all downsampled to 12000 cells. And in the training process, we optimize the network through minimizing the cross-entropy loss which is very commonly used in the segmentation task. The learning rate was empirically set as 10 -3 , and reduced by 0.2 decay every 20 epochs."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,3.2,Dataset,"Our dataset consists of 200 3D dental meshes obtained by an intraoral scanner on real orthodontic patients from hospital and each raw mesh contains even more than 150,000 cells, so we down sample the raw mesh to 12000 cells while preserving the surface topology. We randomly split the whole dataset as a training set with 160 meshes and a testing set with 40 meshes. And we segment the raw mesh into 14 teeth and gums, following the FDI World Dental Federation notation  For evaluation, overall accuracy (OA) and mean intersection over union (mIoU) are adopted for quantitative comparison."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,3.3,Comparing with SOTA Methods,"We compare our network against five recent methods, including PointNet++  The segmentation results are shown in Table "
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,3.4,Ablation Study,"We evaluate the effectiveness of semantics prediction, graph-transformer and adaptive feature fusion as three critical components of our method. We perform the evaluation by excluding one of these critical components each time. Specifically, when we remove the semantics prediction module, we will use a naive KNN to construct the KNN graph. When we remove the graph-transformer module, we replace it with max-pooling layers. In the absence of the adaptive feature fusion module, we directly concatenate the features from C-domain and N-domain for cross-domain feature fusion. The results of the ablation study are presented in Table "
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,4,Conclusion,"We propose a novel semantics-based feature learning to make full use of local and unlocal semantic information to enhance the features extracted. This architecture can decouple the spatial and geometric features into C-domain and Ndomain and embed the N-domain into C-domain to further utilize the semantic pseudo label to perform the local graph-transformer module. Lastly we fuse the features from spatial and geometric domain adaptively by using the global graph-transformer module and adaptive feature fusion module. The effectiveness of our proposed method is evaluated on the real dental mesh from real orthodontic patients. In the future work, we will try to utilize the feature decoupling and fusing strategy in other segmentation tasks."
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,,Fig. 1 .,
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,,Fig. 2 .,
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,,Table 1 .,
3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer,,Table 2 .,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,1,Introduction,"Colorectal cancer (CRC) remains a major health burden with elevated mortality worldwide  Traditional machine learning approaches in polyp segmentation primarily focus on learning low-level features, such as texture, shape, or color distribution  Despite significant progress made by these binary mask supervised models, challenges remain in accurately locating polyps, particularly in complex clinical scenarios, due to their insensitivity to complex lesions and high false-positive rates. More specifically, most polyps have an elliptical shape with well-defined boundaries. However, supervised segmentation learning solely based on binary masks may not be effective in discriminating polyps in complex clinical scenarios. Endoscopic images often contain pseudo-polyp objects with strong boundaries, such as colon folds, blood vessels, and air bubbles, which can result in false positives. In addition, sessile and flat polyps have ambiguous and challenging boundaries to delineate. To address these limitations, Qadir et al.  Therefore, the primary challenge lies in enhancing polyp segmentation performance in complex scenarios by precisely preserving the polyp segmentation boundaries, while simultaneously maximizing the decoder's attention on the overall pattern of the polyps. In this paper, we propose a novel transformer-based polyp segmentation framework, PETNet, which addresses the aforementioned challenges and achieves SOTA performance in locating polyps with high precision. Our contributions are threefold: • We propose a novel Gaussian-Probabilistic guided semantic fusion method for polyp segmentation, which improves the decoder's global perception of polyp locations and discrimination capability for polyps in complex scenarios. • We evaluate the performance of PETNet on five widely adopted datasets, demonstrating its superior ability to identify polyp camouflage and small polyp scenes, achieving state-of-the-art performance in locating polyps with high precision. Furthermore, we show that PETNet can achieve a speed of about 27FPS in edge computing devices (Nvidia Jetson Orin). • We design several polyp instance-level evaluation metrics, considering that conventional pixel-level calculation methods cannot explicitly and comprehensively evaluate the overall performance of polyp segmentation algorithms."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,2,Methods,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,2.1,Architecture Overview,As shown in Fig. 
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,2.2,Encoder Group,"To balance the trade-off between computational speed and feature representation capability, we utilize the pre-trained PVTv2-B2 model  Mixed transformer attention(MTA) layer is composed of Local-Global Gaussian-Weighted Self-Attention (LGG-SA) and External Attention (EA). We add a MTA layer to encode the last level features, enhancing the model's semantic representation and accelerating the training process "
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,2.3,Gaussian-Probabilistic Modeling Group,"To incorporate both polyp location probability and surface pattern information in a progressive manner, we propose the Gaussian Probabilistic-induced Transition (GIT) method. This method involves the interaction between a Gaussian auxiliary decoder and multiple binary decoders in a layer-wise fashion, as shown in Fig.  Gaussian Probabilistic Mask. Inspired by  W ×H×1 by utilizing elliptical Gaussian kernels. Specifically, for every polyp in a binary mask, after masking other polyp pixels as background, we calculate where (x o , y o ) is the mass of each polyp in the binary image f (x, y). To rotate the output 2D Gaussian masks according to the orientation, we set a, b, c as followings, where σ 2 x and σ 2 y are the polyp size-adaptive standard deviations  Gaussian Guided UNet-Like Decoder Branch. The Gaussian Guided UNet-like decoder branch(GUDB) module is a simple UNet-like decoding branch supervised by Gaussian Probabilistic masks. We employ four levels of encoder output features, and adjust encoder features with channels of [C, 2C, 2C, 2C] in each level. At the final layer, a 1 × 1 convolution is used to convert the feature vector to one channel, producing a size of H × W × 1 Gaussian mask. Gaussian Probabilistic-Induced Transition Module. We use the Gaussian probabilistic-induced transition module(GIT) to achieve transition between binary features and gaussian features. Given the features originally sent to the Decoder as binary features {X B i } 4 i=1 , and the transformed encoder features sent to GUDB as X G . We first splits 4 levels of X B and X G into fixed groups as: where M is the corresponding number of groups. Then, we periodically arrange groups of X B i,m and X G i,m for each level, and generate the regrouped feature Q i ∈ R (Ci+Cg)×Hi×Wi in an Multi-layer sandwiches manner. Soft grouping convolution  Ci×Hi×Wi is obtained for the UDB decoder. Considering the computation cost, The binary features X B ← X E for the Fus decoder have channel numbers of [4C, 4C, 4C, 4C]. The Fus decoder and CFM share identical transited output features, while CFM exclusively utilizes the last three levels of features."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,2.4,Ensemble Binary Decoders Group,"During colonoscopy, endoscopists often use the two-physician observation approach to improve the detection rate of polyps. Building on this manner, we propose the ensemble method that integrates multiple simple decoders to enhance the detection and discrimination of difficult polyp samples. We demonstrate the effectiveness of our approach using three commonly used convolutional decoders. After GIT process, diverse level of Gaussian probabilistic-induced binary features were sent to these decoders. The output mask P is obtained by element-wise summation of P i , where i represents the binary decoder index. Fusion Module. As shown in Fig.  UNet Decoder Branch and CFM Module. The structure of the UDB is similar to that of the GUDB, except for the absence of channel reduction prior to decoding. In our evaluation, we also examine the decoder CFM utilized in  3 Experiments"
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,3.1,Datasets Settings,"To evaluate models fairly, we completely follow P raNet "
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,3.2,Loss Setting,"Our loss function formulates as L = N i=1 L i + λL g , and where N is the total number of binary decoders, L g represents the L1 loss between the ground truth Gaussian mask G G and GUDB prediction mask P G . λ is a hyperparameter used to balance the binary and Gaussian losses. Furthermore, we employ intermediate decoder outputs to calculate auxiliary losses for convergence acceleration."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,3.3,Evaluation Metrics,"Conventional evaluation metrics for polyp segmentation are typically limited to pixel-level calculations. However, metrics that consider the entire polyp are also crucial. Here we assess our model from both pixel-level and instance-level perspectives. Pixel-level evaluation is based on mean intersection over union (mIoU ), mean Dice coefficient (mDic), and weighted F 1 score (wF m ). For polyp instance evaluation, a true positive (TP) is defined when the detection centroid is located within the polyp mask. False positives (FPs) occur when a wrong detection output is provided for a negative region, and false negatives (FNs) occur when a polyp is missed in a positive image. Finally, we compute sensitivity nSen = T P/(T P + F N) × 100, precision nPre = T P/(T P + F P ) × 100, and nF 1 = 2 × (Sen × Pre)/(Sen + Pre) × 100 based on the number count for instance evaluation."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,3.4,Results,Training and Learning Ability. Table  Generalization Ability. The generalization results are shown in Table  Small Polyp Detection Ability. The detection capability results of small polyps are shown in Table  Ablation Analysis. Table 
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,3.5,Comparative Analysis,Fig. 
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,3.6,Running in the Real World,"Furthermore, we deployed P ET Net on the edge computing device Nvidia Jetson Orin and optimized its performance using TensorRT. Our results demonstrate that P ET Net achieves real-time denoising and segmentation of polyps with high accuracy, achieving a speed of 27 frames per second on the device(Video S1)."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,4,Conclusion,"Based on intrinsic characteristics of the endoscopic polyp image, we specifically propose a novel segmentation framework named PETNet consisting of three key module groups. Experiments show that P ET Net consistently outperforms most current cutting-edge models on five challenging datasets, demonstrating its solid robustness in distinguishing other intestinal analogs. Most importantly, P ET Net shows better sensitivity to complex lesions and diminutive polyps."
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,,Fig. 1 .,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,,Fig. 2 .,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,,Table 1 .,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,,Table 2 .,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,,Table 3 .,
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_54.
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,1,Introduction,"Post-ischemic Ventricular Tachycardia (VT) is an arrhythmia that occurs after a myocardial infarction event. During Myocardial Infarction (MI), the blood flow to an area of the heart is blocked, producing tissue death and scarring  Catheter ablation is recommended for ischemic heart disease patients, in the presence of recurrent monomorphic VT despite anti-arrhythmic drug therapy. Ablation therapy aims at destroying a part of the re-entry path (channel) that sustains the VT  Virtual-Heart Arrhythmia Ablation Targeting (VAAT) methods have been developed to improve the utility of pre-operative imaging by using image-based computational models to reproduce the patient-specific re-entrant dynamics of the arrhythmia and thus increase precision in the localization of ablation targets  In this paper, we present a validation study of a novel VAAT method based on efficient phenomenological models of electrophysiology for high-fidelity simulation of VT. We successfully reproduce the inducibility of sustained monomorphic VT (more than 30 s) in two animal models of scar-related VT. In addition, we reproduce the ECG signature of each measured VT circuit, in terms of the polarity in the QRS complex of each lead as well as the VT cycle length. Finally, we demonstrate the feasibility of using this method in clinical practice by showing considerable speed up compared to the state-of-the art."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,2,Methods,
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,2.1,Data Description,Two porcine models of MI 
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,2.2,Anatomical Model Generation,"The right ventricular (RV) endocardium, left ventricular (LV) endocardium, and LV epicardium were manually contoured and reviewed by an electrophysiologist. The computational domain for the electrophysiology model was defined as a Cartesian grid of isotropic 0.5 mm resolution, obtained by rasterization of the segmented surfaces  The scar and BZ were semi-automatically segmented by an expert and validated by an electrophysiologist. The full-width half-maximum method "
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,2.3,Electrophysiology Simulation,"The electrophysiology model is based on the monodomain equation using the modified Mitchell-Schaeffer cellular model  Scar tissue was modeled as a non-conductive material. The BZ tissue was modeled as conductive, isotropic, and with a longer Action Potential Duration (APD) than the healthy tissue  To reproduce the fast conduction produced by the Purkinje network, a Fast Endocardial Conductive (FEC) layer of 3 mm was added to the model to account for the increased transmurality of Purkinje networks in porcine models "
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,2.4,Parameter Personalization,"The model parameters were tuned to match the ECG measurements during sinus rhythm. To simulate sinus rhythm, a one-second-long simulation was run with four pacing points placed in the basal and apical areas of the LV and RV septal walls to produce depolarization patterns consistent with previously published observations  Myocardial diffusivity was sampled in a range of values corresponding to CV between 0.4 and 0.75 m/s, with 0.01 m/s steps, consistent with previous approaches  The reference QRS and QT duration in the measured ECGs were annotated by an electrophysiologist in a digital trace plotted with a paper speed of 50 mm/s. The QRS and QT duration in the simulated ECGs were visually annotated by an expert and the set of parameters leading to the best match with the measured ECG were validated by an electrophysiologist. Due to the infarction features present in the ECG, such as ST elevation  Electrophysiology properties of the BZ were not personalized based on sinus rhythm ECG measurements, since we observed minimal effect on simulated sinus rhythm QRS and QT duration from variations of the BZ parameters within the range of previously reported values "
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,2.5,VT Induction Procedure,"The simulation of a programmed stimulation experiment included three phases: model preconditioning, artificial stimulation delivery, and spontaneous activity simulation. First, the internal states of the model were pre-conditioned by simulating 5 s of sinus rhythm at 60 bpm. Then, 8 stimuli were delivered at a constant period S1, which ranged between 0.3 and 0.6 s, followed by three extra stimuli delivered with delays S2, S3, and S4. The pacing times S2, S3, and S4 were automatically reduced until VT was induced or no activation was produced, following the procedure described in the Supplementary Materials. The stimuli were delivered from one point at a time, which could be located in the RV outflow tract, RV apex, or each AHA region barycenter. Other points in proximity to areas that were visually identified as possible re-entry channels were additionally tested. For efficiency purposes, the internal states of the simulation were saved after the preconditioning phase and after the S1 stimuli train and were re-used for later simulations. To elucidate the role of Electrophysiological (EP) properties of the BZ on the generation and maintenance of VT, multiple programmed stimulation experiments were conducted with varying BZ parameters. BZ diffusivity was sampled in a range of values corresponding to CV between 0.14 and 0.23 m/s, with 0.01 m/s steps, consistent with previous approaches  All simulations were performed on a high-performance GPU cluster with 24 GPUs (Tesla-V100-SXM2, NVIDIA Corporation) scheduled to perform multiple experiments in parallel. Additionally, to save computation time, the simulations were stopped 1 s after the last delivered stimulus if no spontaneous activity was present. If spontaneous activation was present, the simulations were computed for a maximum of 30 s after the last delivered stimulus. Each simulation was classified as inducible with sustained activity if the spontaneous activity lasted for 30 s after the last stimulus. For these simulations, the pseudo-ECG was computed and the intracardiac potentials sampled in each of the AHA region barycenters were saved for later analysis."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,2.6,Analysis of VT Inducibility,"The intracardiac potentials of each inducible simulation were analyzed by calculating the Cycle Length (CL) of spontaneous activity using a moving window of 4 s. The mean and standard deviation time between Action Potential (AP) peaks in the moving window was computed. This was done to study the evolution of the sustained VT during its duration. Additionally, this allowed us to determine whether the VT was monomorphic, in which case CL standard deviation is minimized."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,3,Results,
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,3.1,Personalization Results,"The personalization resulted in a myocardial CV of 0.68 m/s and 0.64 m/s, which produced a QRSd consistent with the measurements of 70 ms and 80 ms, for case 1 and 2 respectively. The personalized APDs of 0.155 s for both cases produced a QTd of 370 ms and 380 ms for case 1 and case 2 respectively, which were in accordance with the measurements. The personalization was done with a BZ CV of 0.23 m/s and an APD increase of 0.03 s for both cases."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,3.2,Inducibility of VT,"The personalized values obtained in Sect. 3.1 were used for the VT inducibility simulations following the protocol described in Sect. 2.4. For case 1, the artificial stimulation was delivered from 3 points as tested in the clinical scenario: the RV apex, AHA regions 1 and 2. An additional point in the LV apex was placed near a visually identified channel. A total of 7050 virtual programmed stimulation experiments were performed with an average compute time of 5 h and 31 min per GPU, resulting in 201 induced VTs. All VTs shared the same ECG signature, and were induced after LV apex pacing. The ECG signature was consistent with that of the measured VT. Additionally, one parameter combination produced VT with the same CL as the measured VT (0.27 s). This was achieved with a BZ CV of 0.21 m/s and BZ APD of 0.17 s. The 12-lead-ECG polarity matched in all leads. A qualitative comparison is shown in Fig. "
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,3.3,Comparison with Previous Modeling Approaches,The modeling configuration reported by Mendonca Costa et al. 
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,4,Discussion,"In this manuscript, a pipeline and methodology to simulate monomorphic VT reentrant dynamics was described. This method utilized fast and computationally efficient methods to solve the monodomain equation for cardiac electrophysiology, and phenomenological models for the cellular AP description. The results of this paper show that this approach allows to match clinical measurements of monomorphic VT, matching both the cycle length and ECG lead polarity, after myocardial parameter personalization and BZ parameter exploration. The optimal BZ tissue properties, maximizing the agreement between simulated and measured VT, could not be identified based on commonly adopted modeling assumptions (e.g. as a pre-defined ratio of myocardial tissue properties). Instead, we observed significant case-by-case variability, suggesting that improved methods for estimation of BZ properties are necessary to increase fidelity of Virtual Heart models for monomorphic VT simulation. Although this was only validated in 2 cases, we plan to add more in the continuation of the project. To our knowledge, this is the largest validation study of Virtual Heart models focusing on VT ECG. Lopez Perez et al. performed a similar study only using one case, showing a good agreement of VT morphology but not CL matching  Our results show that BZ parameter exploration is necessary to reproduce the clinical VT. Accurately determining BZ tissue properties in a prospective setting, when VT ECG measurements are not available, requires future investigation. We hypothesize that it is possible to identify BZ features in sinus rhythm ECG measurements, possibly using more complex signal features than QRS and QT duration, although this has not been investigated in our study. Additionally, we reproduced the alternative modeling approach described by Mendonca Costa et al.  The proposed pipeline is potentially capable of delivering results with times compatible with clinical practice: for a given parameter combination, a virtual programmed stimulation experiment requires 20 min of computation on a single GPU (i.e. 41 s per simulated second). This is faster than other approaches: 1 h per simulated second  Clinical applicability will also require the full automation of several steps of this pipeline, including the tissue segmentation, parameter personalization, and unique VT signature characterization. At the current stage of our research, we have focused on careful manual curation of the input data to help reduce the potential impact of uncertainty due to data quality or algorithm performance. An additional limitation of this study is that the virtual ECG model is not able to produce the full range of ECG morphologies observed in measured signals, in particular high frequency features in ECG leads including changes in slope, narrowing and notches within the QRS complex, as observed in Figs. 1a, 1b, and 1c. These signal characteristics might be produced by tissue properties heterogeneity which we do not include in our model. Nonetheless, the good agreement achieved in lead polarity suggests that the simulated re-entrant VTs follows a similar re-entry pathway as the corresponding measured VT. The Virtual Heart model was not capable to match all measured VTs in case 2 in this study, characterized by a comparatively larger infarct extent, suggesting that additional VT exit sites may have been manifest in vivo, while not being produced in the model. As observed in previous computational modeling studies, we still have an incomplete understanding of the role of uncertainty on scar and BZ extent, as determined by the image processing and segmentation pipeline, on the model fidelity. This is a limitation of the current study to be addressed with larger validation studies including a wide variety of infarction presentations in pre-operative imaging."
Virtual Heart Models Help Elucidate the Role of Border Zone in Sustained Monomorphic Ventricular Tachycardia,,Fig. 1 .,
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,1,Introduction,"Breast cancer (BC) is the most common cancer in women and incidence is increasing  Authenticated by the BI-RADS lexicon  The question of ""what the Bi-MG would look like if they were symmetric?"" is often considered when radiologists determine the symmetry of Bi-MG. It can provide valuable diagnostic information and guide the model in learning the diagnostic process akin to that of a human radiologist. Recently, two studies explored generating healthy latent features of target mammograms by referencing contralateral mammograms, achieving state-of-the-art (SOTA) classification performance  In this work, we present a novel end-to-end framework, DisAsymNet, which consists of an asymmetric transformer-based classification (AsyC) module and an asymmetric abnormality disentanglement (AsyD) module. The AsyC emulates the radiologist's analysis process of checking unilateral and comparing Bi-MG for abnormalities classifying. The AsyD simulates the process of disentangling the abnormalities and normal glands on pixel-level. Additionally, we leverage a self-adversarial learning scheme to reinforce two modules' capacity, where the feedback from the AsyC is used to guide the AsyD's disentangling, and the AsyD's output is used to refine the AsyC in detecting subtle abnormalities. To "
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,2,Methodology,"In this study, the ""asymmetric"" refers to the visual differences on perception level that can arise between the left and right breasts due to any abnormality, including both benign and malignant lesions. Thus, a paired Bi-MG is considered symmetrical only if both sides are normal and the task is different from the malignancy classification study "
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,2.1,Asymmetric Transformer-Based Classification Module,"The AsyC module consists of shared encoders ψ e and asymmetric transformer layers ψ asyt to extract features and learn bilateral-view representations from the paired mammograms. In this part, we first extract the starting features f of each side (f r , f l represent the right and left features respectively) through ψ e in the latent space for left-right inspection and comparison, which can be denoted as f = ψ e (x). Then the features are fed into the ψ asyt . Unlike other MV transformer methods  with the number of heads h = 8, which is a standard component in transformers and has already gained popularity in medical image fields  While in the CA, we replace the key and value vectors with those from the contralateral features, Then, the starting feature f , and the attention features f SA and f CA are concatenated in the channel dimension and fed into the FFN layers to fuse the information and maintain the same size as f . The transformer block is repeated N = 12 times to iteratively integrate information from Bi-MG, resulting in the output feature f r out , f l out = ψ N =12 asyt (f r , f l ). To predict the abnormal probability ŷ of each side, the output features f out are fed into the abnormal classifier. For the asymmetry classification of paired mammograms, we compute the absolute difference of the output features between the right and left sides (f asy out = abs(f r out -f l out ), which for maximizing the difference between the two feature) and feed it into the asymmetry classifier. We calculate the classification loss using the binary cross entropy loss (BCE) L bce , denoted as L diag = L cls (y asy , y r , y l , x r , x l ) = L bce (y asy , ŷasy )+L bce (y, ŷ)."
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,2.2,Disentangling via Self-adversarial Learning,What would the Bi-MG look like when the asymmetrical abnormalities have been removed? Unlike previous studies 
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,2.3,Asymmetric Synthesis for Supervised Reconstruction,"To alleviate the lack of annotation pixel-wise asymmetry annotations, in this study, we propose a random synthesis method to supervise disentanglement. Training with synthetic artifacts is a low-cost but efficient way to supervise the model to better reconstruct images  The alpha weights α k is a 2D Gaussian distribution map, in which the co-variance is determined by the size of k-th tumor t, representing the transparency of the pixels of the tumor. Some examples are shown in Fig.  When training the model on other datasets, we use the tumor set collected from the INBreast dataset. Thus, the supervised reconstruction loss is L syn = L l1 (x|real, x n |fake), where x|real is the real image before synthesis and x n |fake is the disentangled normal image from the synthesised image x|fake."
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,2.4,Loss Function,"For each training step, there are two objectives, training AsyC and AsyD module, and then is the refinement of AsyC. For the first, the loss function can be denoted by The values of weight terms λ 1 , λ 2 , λ 3 , and λ 4 are experimentally set to be 1, 0.1, 1, and 0.5, respectively. The loss of the second objective is L ref ine as aforementioned."
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,3,Experimental,
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,3.1,Datasets,This study reports experiments on four mammography datasets. The INBreast dataset  Table 
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,3.2,Experimental Settings,"The mammogram pre-processing is conducted following the pipeline proposed by  To assess the performance of different models in classification tasks, we calculate the area under the receiver operating characteristic curve (AUC) metric. The 95% confidence interval (CI) of AUC is estimated using bootstrapping (1,000 times) for each measure. For the segmentation task , we utilize Intersection over Union (IoU), Intersection over Reference (IoR), and Dice coefficients. For the localization task , we compute the mean accuracies of IoU or IoR values above a given threshold, following the approach "
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,3.3,Experimental Results,"We compare our proposed DisAsymNet with single view-based baseline ResNet18, attention-driven method HAM "
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,,Comparison of Performance in Different Tasks:,"For the classification task, the AUC results of abnormal classification are shown in Table "
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,4,Conclusion,"We present, DisAsymNet, a novel asymmetrical abnormality disentangling-based self-adversarial learning framework based on the image-level class labels only. Our study highlights the importance of considering asymmetry in mammography diagnosis in addition to the general multi-view analysis. The incorporation of pixel-level normal symmetric breast view generation boosts the classification of Bi-MG and also provides the interpretation of the diagnosis. The extensive experiments on four datasets demonstrate the robustness of our DisAsymNet framework for improving performance in classification, segmentation, and localization tasks. The potential of leveraging asymmetry can be further investigated in other clinical tasks such as BC risk prediction."
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,,Fig. 1 .,
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,,Fig. 2 .,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,1,Introduction,"Multi-slice cine cardiovascular magnetic resonance (CMR) scanning is a common method for the accurate diagnosis and evaluation of cardiovascular diseases  To better evaluate heart disease, plan interventions, and monitor heart disease, a 3D heart model can be created from cine CMRs, which is a digitized heart object visualized as triangular meshes  To this end, we proposed a Recurrent Graph Neural Network based method, ModusGraph, to fully automate the reconstruction of 4D heart models from cine CMR. This includes i) a voxel processing module with Modality Handles (Modhandle) and ResNet decoder for super-resolution and correction of motion artefacts from the acquired cine CMR, ii) a Residual Spatial-temporal Graph Convolution module (R-StGCN) for 4D mesh models generation by hierarchically spatial deformation and temporal motion estimation, and iii) a Signed Distance Sampling process bridge voxel features from segmentation and vertex features from deformation."
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,2,Related Works,"Surface meshing involves constructing polygonal representations of geometric objects or surfaces, and creating high-quality and feature-aware surface meshes for medical imaging applications it is of particular interest. With available large-volume training data and advanced computational resources, more studies harness the strength of deep learning and traditional methods to avoid user supervision. Aubert et al.  In contrast to those methods applying directly to the surface manifold, others, similar to our method, combine image segmentation with explicit surface representations and mesh deformation with coarse to fine controls. Wickramasinghe et al. "
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,3,Method,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,3.1,Voxel Processing Module,"Modality Handles. Compared to cine CMRs, CT imaging provides higher spatial resolution and enables easier high-resolution segmentation of the heart. The developed network to generate such segmentation is transferable to process cine CMRs, allowing for comparable cardiac structural information to be extracted from similar patient populations. The following module is thus proposed to estimate high-resolution segmentations from unpaired CT and cine CMR image volumes. An input image volume X CT ∈ R H×W×D is cropped around the anatomy of interest to the size of 128 × 128 × 128, and then down-sampled following bilinear interpolation method to X' CT ∈ R 16×16×16 , which enables a common low-resolution segmentation space for cine CMRs. Its predicted segmentation Ỹ' CT ∈ R 16×16×16 is generated by a CT Modality Handle (CT Mod-handle, h(X CT )), using ResNet blocks followed by ReLU non-linear activity and a oneby-one convolution layer. A MR Modality Handle (MR Mod-handle, h(X MR )) generates predicted segmentation ỸMR from cine CMRs in the same way. Super-Resolution Decoder. Ỹ' CT is passed to a decoder for super-resolution reconstruction to a size of 128×128×128. The decoder ψ includes three layers of up convolution followed by ResNet blocks. The high-resolution segmentation of cine CMR is generated similarly through ỸMR = W ψ W h X MR , where W ψ and W h are trainable weights of decoder and Mod-handle, respectively. To include heart morphological features in the graph convolution process, the signed distance is calculated from the decoder's output. This signed distance is computed as geodesic distances from each voxel to the surface boundary "
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,3.2,Residual Spatial-Temporal Graph Convolution Module (R-StGCN),"Graph convolution networks can be utilized to reshape a heart mesh model, but regressing the surface near sharp edges or areas with aggressive Laplacian changes is challenging due to the networks' lack of awareness of the position relationships. We borrowed the idea of graph construction from human joints to overcome this issue  Spatial Deformation. Given a dynamic mesh at level l and frame t, i.e. M l t = (V l t , E l t ), where V l t and E l t are N vertices and M edges, respectively. A dense adjacency matrix A k ∈ R N ×N denotes the edges between every two vertices. A data-dependent matrix C k ∈ R N ×N determines the similarity of every two vertices as normalized Gaussian function, . W θk and W φk are the parameters of the 1 × 1 convolution layer θ and φ, respectively. f in ∈ R 3×T ×N is input feature matrix of the convolution layer, where T is the number of frames for each cardiac cycle. The sampling area of convolution is a 1-connected neighbourhood includes 3 subsets, which conforms with the aforementioned mesh topology. It is described as f out is output feature of the convolution layer, W k is trainable weights for the convolution. Following the AGC layer, we used graph convolution with first-order Chebyshev polynomial approximation. It is formalized as , where W θ0 , W θ1 are trainable weights and L = 2L norm /λ max -I, L ∈ R N ×N is the scaled and normalized Laplacian matrix  Temporal Deformation. The deformation field vector V t-1→t and V t→t-1 are learnt through temporal convolutions, where the sampling neighbourhood is defined as a vertex in consecutive frames. It is a T × 1 convolution performed on the output feature matrix f out in a bidirectional manner. V is regularized following the principle of motion estimation. With vertices of meshes at consecutive frames V 0 and V 1 , the vertices of intermediate mesh V t , 0 < t < 1 is approximated under symmetric assumption  ), and we measure the L1 difference between Ṽt and V t ."
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,3.3,Training Scheme,"Generally, the MR Mod-handle was trained on cine CMRs and down-sampled segmentation, described as L seg,MR (h(X MR ), Y' MR ). The CT Mod-handle and ResNet decoder were trained on CT image volumes, segmentation and their down-sampled counterparts using dice loss and cross-entropy loss, i.e. L seg,CT (ψ• h(X CT ), Y CT , Y' CT ). Supervised by the ground-truth point clouds from CT segmentation, meshes were predicted from the R-StGCN module, where Chamfer distance is minimized together with surface regularization  ). P t was generated via Marching Cubes "
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,4,Results and Discussion,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,4.1,Datasets,The training and validation data consisted of CT image volumes from the SCOT-HEART study 
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,4.2,Evaluation of Whole Heart Meshes Quality,"1.79 ± 0.68 2.39 ± 0.49 1.17 ± 0.14 0.43 ± 0.29 0.18 ± 0.10 1.50 ± 1.80 We evaluated the quality of meshes generated by ModusGraph, by comparing them to those produced by other state-of-the-art methods using the SCOT-HEART dataset. ModusGraph, Voxel2Mesh  Regarding the Dice score and Average Surface Distance (ASD) in Table "
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,4.3,Dynamic Mesh for Biomechanical Simulation,"We evaluated ModusGraph and other methods on the task of creating dynamic ventricle myocardium meshes. When them to ground-truth short-axis slices segmentation, the Average Surface Distance (ASD) at end-diastole (ED) and  Mesh quality for biomechanical simulations, including aspect ratio, min and max angle, and Jacobian of surface mesh triangular cells, was also evaluated. ModusGraph showed less distorted cells, leading to faster convergence for mechanical simulations, as shown in Table "
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,5,Conclusion,"Our proposed method, ModusGraph, automates 4D heart model reconstruction from cine CMR using a voxel processing module, a Residual Spatial-temporal Graph Convolution module and a Signed Distance Sampling process. Modus-Graph outperforms other state-of-the-art methods in reconstructing accurate 3D heart models from high-resolution segmentations on computed tomography images, and generates 4D heart models suitable for biomechanical analysis, which will aid in the understanding of congenital heart disease. This approach offers an efficient and automated solution for creating 3D and 4D heart models, with potential benefits for heart disease assessment, intervention planning, and monitoring."
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,,Fig. 1 .,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,,Fig. 2 .,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,,Fig. 3 .,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,,Table 1 .,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,,.92 ± 0.02 0.82 ± 0.05 0.91 ± 0.03 0.89 ± 0.03 0.90 ± 0.04 0.87 ± 0.05,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,,Table 2 .,
ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency,,Acknowledgements. YD was funded by the,Kings-China Scholarship Council PhD Scholarship Program. HX was funded by 
Full Image-Index Remainder Based Single Low-Dose DR,1,Introduction,"Digital Radiography (DR) and Computed Tomography (CT) techniques are extensively utilized in the diagnosis of various clinical conditions  Traditional image denoising methods have relied on exploring spatial pixel features and properties in the transform domain  With the advancement of neural networks, deep learning-based denoising techniques have shown superior performance compared to traditional methods  Toovercomethelimitationsofsupervisedtechniques,self-supervisedorunsupervised denoising methods have been proposed and developed, leveraging the similaritybetweennoisyimages  The remaining sections are organized as follows. Sections 2 and 3, introduce our proposed FIRE method in terms of the network architectures, sampler design, and loss functions. In Sect. 4, we will test our model on large-scale low-dose DR and CT image datasets and compare it with other well-performing methods. Finally, a summary will be given."
Full Image-Index Remainder Based Single Low-Dose DR,2,Methodology,
Full Image-Index Remainder Based Single Low-Dose DR,2.1,Related Theories,"Given a clean image x and its corresponding noisy image y, the supervised image denoising methods try to train the denoising network f parameterized by θ minimizing the loss function below: ( Implementing the supervised denoising network on noisy and clean image pairs, the usual loss function can be formulated as: However, in actual situations, it is difficult to obtain the corresponding clean and noisy image pairs, especially in medical imaging scans. To address this issue, Noise2Noise proposed a self-denoising method that does not require real clean images, but it depends on pairs of independent noisy images of the same scene. The method proves that the results obtained by minimizing the following equation are the same as the results obtained by the supervised case above: where y and z are two independent noisy images conditioned on x. In previous studies "
Full Image-Index Remainder Based Single Low-Dose DR,2.2,Framework,"In this section, we propose a self-supervised framework for training a single noisy image-denoising network. First, we design a new subspace sampling technique for generating subspace image groups to train the network. Next, for recovering finer image details and features, a specialized loss function consisting of reconstruction terms and regularization terms is proposed and used for training the network. The overall of our FIRE framework is shown in Fig.  The details of the remainder sampler G can be developed as follows: a) Making the sampled pictures contain all the pixels within the original noisy image, it is necessary to ensure that the length and width of the image y can be divisible by N, which is chosen as 4; b) Encoding pixel index of the image y from the left-upper corner, all image pixels can be accessed by the abscissa i and ordinate j. The image indexes can range from (0, 0) to (W -1, H -1). c) Calculating the remainder of (2×i+j)%4 and defined it as k. k is an integer in [0, 3]. All pixel coordinations satisfying k = 0 are retained, and the remaining pixel values are set to 0 to generate the first mask. The second mask is obtained with pixel coordinations satisfying k = 1. The third and fourth masks are obtained by satisfying k = 2 and k = 3. d) Selecting the reserved pixel value from the four pixels in each 2×2 area as the pixel value for the corresponding position of the subsampled image, and put it into (g 1 , g 2 , g 3 and g 4 ). By doing this, four sampled pictures are obtained, i.e., (g 1 (y), g 2 (y), g 3 (y) and g 4 (y)). The width and length of all sampled subspace images are W/2 and H/2 respectively. To clearly demonstrate the generation diagram of the subspace image group, Fig.  Regularization Optimization: For self-supervised training, the following minimization optimization problem is formulated by taking advantage of the constraint According to E x,y = E x E y|x , it can be converted into the following regularization optimization problem Finally, the loss function incorporating the regularization term is proposed to train the denoising network with where f θ is the denoising network and λ is a hyperparameter to balance the regularization term. "
Full Image-Index Remainder Based Single Low-Dose DR,3,Experiments,"In this section, we first introduce the details and configuration of clinical experiments. To evaluate the effectiveness of this method, several advanced imagedenoising methods for CT and DR were involved in the comparison. The experiments were conducted on several large-scale CT and DR data sets. Experimental Configuration: The network architecture of our proposed FIRE framework was a modified U-Net  In terms of the comparisons, we include BM3D "
Full Image-Index Remainder Based Single Low-Dose DR,,Influence of Regularization Term:,"We proposed a regularization term in Sect. 3, and the hyperparameter λ is used to adjust the regularization term. Figure "
Full Image-Index Remainder Based Single Low-Dose DR,4,Conclusion,"We proposed a full image-index remainder method (FIRE) using only a single noisy image. The proposed FIRE first divided the whole high-dimensional image space into a series of low-dimensional sub-image spaces with the full image-index remainder technique. Our FIRE retains the complete information within the original noisy image. In addition, we proposed a new regularization optimization function to regularize sub-space image training by reducing the gap between the self-supervised and supervised denoising networks. Quantitative and qualitative experiment results indicate that the proposed FIRE is effective in both DR and CT image denoising."
Full Image-Index Remainder Based Single Low-Dose DR,,,
Full Image-Index Remainder Based Single Low-Dose DR,,Fig. 1 .,
Full Image-Index Remainder Based Single Low-Dose DR,,Fig. 2 .,
Full Image-Index Remainder Based Single Low-Dose DR,,Fig. 3 .,
Full Image-Index Remainder Based Single Low-Dose DR,,4 .,
Full Image-Index Remainder Based Single Low-Dose DR,,Fig. 4 .,
Full Image-Index Remainder Based Single Low-Dose DR,,Fig. 5 .,
Full Image-Index Remainder Based Single Low-Dose DR,,Fig. 6 .,
Full Image-Index Remainder Based Single Low-Dose DR,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 44.
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,1,Introduction,"Catheter ablation surgery is a common operation that treats premature ventricular contraction arrhythmia effectively, and it mainly includes two steps: electrical physiological measurement and radiofrequency ablation. For electrical physiological measurement, it requires doctors to insert an electrode catheter in patient's heart, analyze the causes and parts of arrhythmia through the electrical signal obtained by the catheter, and finally determine the specific location of ectopic pacing and catheter ablation  There is a trend to use computational tools based on 12-lead electrocardiogram (ECG) to analyze ablation location since ECG has been proven that can provide information about pacing areas in patients  Rapid developments in computer performance and theoretical knowledge have enabled detailed, physiologically realistic whole-heart simulations of arrhythmias and pacing  In this paper, inspired by work in  1. Propose a framework that is trained based on ECG simulation data from the specific patient's CT for noninvasive cardiac ectopic pacing localization. It can eliminate the effect of insufficient clinical data and patient variance error on location accuracy. 2. Propose a network that combines time-frequency information and local-global information to achieve precise ectopic pacing location based on a small training data set. 3. Proposed method achieves great performance on PVC patient data, which demonstrates its potential for clinical cardiac treatment."
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,2,Methodology,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,2.1,Overview of Location Framework,Figure 
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,2.2,Process of Forward-Solution Obtaining,"Establishing a specific heart-torso model requires the geometric structure of both patient's heart and torso, which can be constructed from CT imaging data from the common preoperative scanning. The solution to the forward problem is obtained from the process of calculating torso surface potentials based on known cardiac source parameters  where y denotes the torso surface and, x denotes the cardiac source, A is a transfer function dependent on the source model, which has been demonstrated in prior research that can be expressed as a transfer matrix H ∈ R N * M . . This transfer matrix can be calculated through the resolution of the relationship between the cardiac and torso domains based on the finite element method (FEM) or boundary element method (BEM). In terms of the cardiac source, the two most common models are the activation-based model and the potential-based model  where u is the transmembrane potentials, v is the conduction current, D is the diffusion tensor dependent on 3-D myocardial structure, and tissue conductive anisotropy, ∇(D∇u) is the diffusion term. Functions f 1 and f 2 produce TMP shapes, which can be represented as: According to "
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,2.3,Network Structure,"Figure  where N is the number of samples and P (x, y, z) is the labeled coordinates. To minimize the loss function, we can finally get a well-trained model. The training uses the Adam optimizer with a learning rate of 1e-4 and a batch size of 32. Feature Fusion in Time-Frequency Domain. The electrocardiogram signal is a temporal signal, and therefore its analysis can be performed in both time and frequency domains. The algorithm proposed in reference  where r k and θ k , respectively, denote amplitude and phase. We then separate amplitude matrix {r 0 , r 1 , . . . , r T -1 } and phase matrix {θ 0 , θ 1 , . . . , θ T -1 } as new features that are sent to the network along with signal matrix Local-Global Feature Extraction Module. Clinical diagnosis of heart disease based on 12-lead ECG mainly depends on its local features, such as P wave, T wave, or QRS complex. Convolutional neural network (CNN) has proven effective for extracting local information of ECG waveforms  Inspired by these, our feature extraction module is divided into local feature extraction module and global feature extraction module, ensuring the acquisition of detailed and comprehensive information and improving the network's ability to encode ECG signals. The local feature extraction block consists of three Conv1D blocks, each containing a 1DConv-BN-Relu layer and a max pooling layer. The Conv1D-BN-Relu layer comprises a one-dimensional convolutional layer, a batch normalization layer, and an activation layer with a Relu activation function. The kernel size of the convolutional layer is sequentially set to 7, 5, and 3; the stride is sequentially set to 2, 2, and 1. Figure  In order to extract a broader dependence relationship, we introduced the additive self-attention mechanism after the GRU layer. Its network structure is shown in Fig.  3 Experimental Result"
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,3.1,Overall Performance,"The experimental data of this part are generated by ECGSIM software based on the UDL source model  In this section, we added Gaussian white noise with 5 dB, 15 dB, and 25 dB to the testing 12-lead data, respectively. From the results, we can see that network has certain robustness to different noise levels. At 25 dB noise, the prediction error is still only 2.97 mm. Though the error reached 10 mm at 5 dB, it is still within the clinically acceptable range considering the small signal-to-noise ratio (SNR).  To further demonstrate the capacity of the forward-solution aided method for ectopic pacing localization, we compared our method with the typical inverse solution methods shown in "
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,3.2,Clinical Data Experiment,Previous experiments confirmed the efficacy of our prediction framework on simulated cardiac data and the network's superior ability to learn the relationship between 12-lead data and ectopic pacing location. We now transfer the method to clinical PVC patients' data using the double variation cardiac source for calculating the forward solution of the specific heart-torso model and use them as the training data. Table 
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,4,Discussion,"In this paper, we developed a forward solution-aided deep learning framework for analyzing ectopic pacing from 12-lead ECG data. Only CT data is needed to establish the specific heart-torso model for simulating ECG data as the training set for the designed network. Time-frequency fusion module and local-global feature extraction module are the core component of the network. Experiments have shown that the framework performs well on both simulated and clinical data. In the future, to enhance the robustness of our proposed method, additional datasets and comprehensive simulations involving a broader spectrum of cardiac conditions should be incorporated. This paper primarily emphasizes the clinical potential of the proposed approach rather than extensively comparing actual and simulated data. The discrepancies observed could be attributed to noise, slight variations in physiological parameters, and consistent electrode placement in clinical practice. Further investigation and discussion on these aspects will be addressed in future research endeavors."
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Fig. 1 .,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Fig. 2 .,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Fig. 3 .,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Fig. 4 .,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Fig. 5 .Fig. 6 .,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Fig. 7 .,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Table 1 .,
Forward-Solution Aided Deep-Learning Framework for Patient-Specific Noninvasive Cardiac Ectopic Pacing Localization,,Table 2 .,
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,1,Introduction,"Intra-operative X-ray imaging supports the assessment of fracture reduction and implant positioning, which significantly increases surgical precision  The main contribution of the paper can be summarized as follows: -proposition of a derivable calculation of the image rotation angle derived from the HT heatmap in the manner of a differentiable spatial to numerical transform (DSNT) "
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,2,Materials and Methods,
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,2.1,Determination of Rotation Angle,"In Kunze et al.  Angle Regression Exploiting the Hough Transform. As pointed out in the introduction, the training with a cost function based on the HM does not enforce a narrow destinct line, but any structure with a defined main axis contributes to a low cost. A cost function which enforces this behaviour would potentially more distinctive heatmap. A common approach to detect lines in pattern recognition applications is the HT: Given a line L ρ,θ , with ρ the angle between line and the x-axis and θ its distance from the origin, its corresponding pixels {x ρ,θ,i , y ρ,θ,i } vote in the Hough space for the closest bin (ρ, θ). In contrast to the HM based direction calculation, the maximum of the HT is only obtained for a straight line of given extent. The extent can be controlled by the bin width of the HT. This feature of the HT enables the training of a narrow structure with a defined width punishing wide structures. However, the HT does not return an angle itself but returns a new 2-dimensional representation of the heatmap, with the line to be sought represented as its maximum. When this representation shall be integrated in an end-to-end trainable network, a differentiable calculation of the position of this maximum is needed. Thereto, we follow the idea of the DSNT layer introduced by Nibali et al.  Equation (  To overcome this shortcoming, A non-linear function needs to be applied to the Hough histogram to assign high pixel values a greater weight than small ones. Keeping pixel values between 0 and 1 while weighting maximum values and suppressing low values, a steep sigmoid function is employed, which maps input values to a range from 0 to 1. At best, only values from roughly 0.8 to 1 should be used. The steeper the sigmoid function is, the more it approximates the step function, which ideally assigns a weight of 1 to values of 0.8 and higher. For the end-to-end training, the same issue holds as for the direct regression: the angular value θ has a discontinuity for θ = -90 • and θ = 90 • . Both θ values represent the same line. To pass this boundy problem, the cosine-and sine-value are computed instead of the angular value itself. This can be incorporated in Eq. (  where α = 20 and β = 17 were determined heuristically. θ can be calculated by the atan2 function from its sine and cosine value. We call the layer, that implements Eq. (  Neither the HT layer parameters nor the DSAT layer have parameters, that need to be trained: The HT can be written as a multiplication of a vector representing the values of the image with a sparse matrix mapping the input image into a [N ρ × N θ ] Hough histogram Y , where N ρ and N θ are the numbers of discrete offsets and angles "
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,2.2,Data,"For the experiments, three different datasets containing the spine, the knee and the wrist, were used. The spinal dataset consists of 958 images derived from 148 patients, where 289 images represent the cervical, 150 the lumbar, and 519 the thoracic region. The wrist dataset contains 257 and the knee dataset 113 images. The 16-bit, gray-scale X-ray images were selected randomly retrospectively from anonymized databases which were acquired using the Cios Spin mobile C-arm system of Siemens Healthcare GmbH during orthopedic surgeries. The images are depicted in the AP view and may contain screws, plates, and other surgical tools. In these images, the centerline was marked using the labelme tool "
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,2.3,"Training Protocol, Experiments, and Evaluation","To identify the effect of the post processing method on the angle regression error, an ablation study was set up. Following Kunze et al.  where λ ∈ R is a multiplicative weighting term. Using this we examined the following 3 cases: 1. For λ = 1, the training corresponds to one without the regression loss. In this case, the performance of the HM based and the HT based angle regression methods can be compared on the heatmap. 2. With a constant weighting term λ = 0.5, the regression loss is considered along with the segmentation loss during the training. 3. When decreasing lambda over the epochs from 1 to 0, we obtain a training which is kick-started using the segmentation loss only. But in the end, only the regression loss is used for the training. For the parameter selection of the HT layer, a grid search was performed on synthetic line heatmaps, resulting in discretization values dθ = 0.4 • and dρ = 1 8px. For comparison reasons, the direct regression method based on a ResNet-34 was trained using the MSE based on the sine-and cosine-vales as cost function. During all trainings, online augmentation for rotation (α ∈ [-45 • , 45 • ], p = 0.5), inverting (p = 0.5), shifting (s ∈ [-40px, 40px], p = 0.5) and contrast enhancement was applied. After the augmentation, the images were scaled to the dimension [H:256×W:256] px, using zero-padding the image to preserve the ratio. Thereafter, sample-wise data normalization using z-scoring was used and combined with batch normalization  For the evaluation, different matrices based on the angular error were used. The mean error (ME) reveals a directional offset of the algorithm. Further on, the mean absolute error (MSE) and the standard deviation of the absolute error (Std.) are reported. Practically more relevant are the percentiles P 50 , P 90 and P 95 based on the absolute error as they reveal to what extent manual rotation corrections are required after the automatic correction was performed. Based on the MAE a Wilcoxon signed-rank test was performed to verify the significance of the observed differences. To evaluate the performance of the presented methods, a 5-fold crossvalidation scheme is employed. The mean performance score across all folds served as final result of the evaluated algorithm. Training was performed using the Adam optimizer for 500 epochs with a learning rate of l = 0.001 divided by two every 50 epochs. The batch size was selected to be 8. The weights of the segmentation network as well as of the direct regression model were initialized by He's method "
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,3,Results,"The results of the experiments are shown in Table  The results show for all three anatomies, that the HT based methods are superior to the HM based method. So already as post-processing method, it significantly outperforms the HM based method (spine: p = 1.0 × 10 -95 , wrist: p = 5.6 × 10 -21 , knee: p = 6.7 × 10 -8 ). Further improvements of the angle regression results can be realized by incorporating the regression loss in the training (spine: p = 9.5 × 10 -91 , wrist: p = 5.7 × 10 -21 , knee: p = 4.5 × 10 -9 ). Best results except for wrist are obtained when the final training is performed on the regression loss only (spine: p = 1.3 × 10 -2 , wrist: p = 7.7 × 10 -1 , knee: p = 2.1 × 10 -2 ). For the wrist adding the regression loss to the overall loss does not improve the training. The comparable P 50 value for most of the anatomies shows, that HT based and HM based angle regression both work well for standard images. For all anatomies, an improvement in the 90 th percentile of the absolute angular error can be observed. While for the spine the improvement is about 17%, for the wrist and knee, this value an reduction by a factor of 4.6 and 3.1, respectively, can be observed. A case-by-case analysis shows that on images for which the centerline is well determined, the HT analysis returns roughly the same result as the HM analysis. However, since the dataset also contains many challenging cases, e.g., images with cropped anatomical structures of interest or images containing metal implants, the centerline cannot always be well determined by the segmentation network. Then occasionally, the heatmap generates short and fragmented estimates of the centerline. For these images, the error of the regressed angle by the HT based method typically is thinner compared to that by the HM computed one (Fig. "
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,4,Discussion,"We introduced a new HT based method to calculate the rotation of straight heatmaps. For the implementation, special care needs to be taken to cope with constant first-order moments of the HT and boundary problems at θ = ±90 • . Doing so, the proposed method has similar performance to HM for X-ray images of good quality. In more difficult cases, when the segmentation algorithm returns multiple and only short segments, like for the wrist or knee, the proposed algorithm is more robust than the HM based one. The largest benefit of the HT based image rotation calculation can be achieved with its embedding into the cost function of a segmentation network. Then it helps to improve the accuracy compared to a segmentation-only approach. Especially outliers can be reduced resulting in a lower MAE and standard deviation of the error. The presented results confirm the work of Kordon et al.  The inspection of the generated heatmaps reveals that the HT based approach can enforce more pronounced, thinner heatmaps compared to the HM -as desired. This observation can be explained by the property of the HT based cost function: it penalizes structures that are not oriented along the desired direction. Only thin straight heatmaps along the direction have a positive effect on the loss term. Structures that are parallel, like the centerlines of the radius and ulna for the wrist images, are penalized. In contrast to that, the HM derived cost term does not punish broad heatmaps as long as the largest main axis points toward the correct direction. Thus, this regression term does not enforce one single, thin line. Also, clusters are tolerated as the results depict. That explains the reduced number of outliers for the HT bases method compared to the HM based, indicated by P 90 and P 95 values. The fact that the adaptation of the parameter λ during the training has only a small effect can be explained by the regression loss being larger compared to the segmentation loss by magnitudes as soon as a certain segmentation level is reached. So the segmentation loss only at the beginning of the training has a distinct influence on the training. Thus, the training process is adapting the influence of the segmentation loss by itself. Finally, the results confirm the finding of Kunze et al.  Data Use Declaration: The data was obtained retrospectively from anonymized databases and not generated intentionally for the study. The acquisition of data from patients had a medical indication. Informed consent was not required."
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,,Fig. 1 .,
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,,Fig. 2 .,
Robust Hough and Spatial-To-Angular Transform Based Rotation Estimation for Orthopedic X-Ray Images,,Table 1 .,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",2,Method,"As shown in Fig.  Vanilla Encoder and Co-attention Encoder. Two basic encoders enable effective intra-and inter-modal interaction. As shown in Eqs. 1-2, the Vanilla encoder duplicates the input stream into query, key and value (Q m , K m and V m ) components and concentrates on intra-modal interaction, while the Co-attention receives two input streams from different modalities and focuses on inter-modal interaction, with the primary stream acting as query, Q m , and the subordinate stream replicated as key and value (K m, V m). Due to the high computational cost of the self-attention mechanism in the OCT branch, we propose block embedding, partitioning the OCT volume into n OCT blocks and each block will be embedded as where m and m denote different modalities, CFP or OCT in this task. d k denotes dimension of self-attention."
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",2.1,Bilateral Contrastive Alignment,"The BCA module aims to align the extracted features with the self-supervised strategy before interaction. The semantic gap between CFP and OCT is huge, and direct interaction between different modalities without alignment will lead to a mismatch. ALBEF  Considering preserving the stereo information of OCT, each block-level token, T k O blk , is averaged in the token dimension before being projected. To equally align both branches, 4 projectors are followed symmetrically in both branches to map the tokens to contrastive space. Different augmentation will cause huge discrepancies, especially for OCT images. Therefore, as shown in Fig.  where q m,u ∈ R dim (dim is 256 by default) denotes modality m with augmentation u in contrastive space with the query encoder. k m,ū denote the vectors from the key encoder. preditor m,m,u denotes the predictor to map modality m to another modality m. ctr() denotes ""ctr loss"" "
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",2.2,Multiple Instance Learning Representation,"Direct interaction between different modalities with unbalanced amounts is computational-consuming, and the cross-modality relationship is difficult to build. Therefore, we conjecture that the OCT block-level tokens(features) can be formulated as an embedding-level MIL problem in two aspects:  where depth is the depth of the Vanilla Encoder in MILR."
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",2.3,Hierarchical Attention Fusion,"Before HAF, each modality interacts within its internal modality. To extract modal-specific features and modal-agnostic features respectively, HAF implements a mid-fusion strategy consisting of two fusion stages, Merged-attention and Cross-attention. As shown in Fig. "
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",3,Experiments,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",3.1,Datasets,"For multi-modal glaucoma recognition, the existing public dataset, GAMMA "
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",3.2,Experiment Details,"Due to the high computational cost, the fixed sampling interval technique is employed to extract 32 OCT images. To avoid over-fitting, we reduce the depths of the encoders to 3 layers. For a fair comparison between all models in this study, we use the following standard setup: initializing with the pre-trained weight of ViT-Base-16 on ImageNet. In the first experiment, the existing multi-modal methods and classical baselines are compared with MM-RAF on the private test set. The baseline includes ResNet, ViT, DeiT "
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",3.3,Experimental Results,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Compared with Single-Modal and Multi-modal Solutions. As shown in Table,"Ablation Study. The ablation study on the private dataset examines the contribution of three modules, the order of HAF, and the depth of each module. From Table .3, MILR and HAF modules bring 0.03 and 0.02 AP increases, respec-  tively. Reversing the order of the HAF module brings a decrease, which indicates that the modal-agnostic features should be extracted after the Merged-attention."
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",3.4,Visualization,"For visualization, we employ a class-dependent relevance-based method "
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",4,Conclusion,"The challenges in multi-modal glaucoma recognition include the huge discrepancies, the unbalanced amounts, and the lack of spatial information interaction between different modalities. To this end, we propose MM-RAF, a pure selfattention multi-modal framework consisting of three modules dedicated to the problems: BCA fills the semantic gap between CFP and OCT and promotes robustness. MILR and HAF complete semantic aggregation and comprehensive relationship probing with better performance. While MM-RAF outperforms other solutions in multi-modal glaucoma recognition, the performance can be further improved with sufficient data. Our next direction is to utilize a lightweight transformer to leverage more information from both modalities. Besides, addressing the issue of uncertainty measurement and preventing the bias of any specific modality from influencing the overall decision in the multi-modal recognition scenario is crucial, especially when diagnosing glaucoma using OCT for its limited specificity. Cross-modal uncertainty measurement is also our further research direction."
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Fig. 1 .,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Fig. 2 .,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Fig. 3 .,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Table 1 .,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Table 2 .,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,"transfer learning, BCA 0.8467",
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Table 3 .,
"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition",,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 66.
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,1,Introduction,"Minimally Invasive Endovascular Procedures (MIEP) are becoming increasingly popular due to their non-invasive nature and quicker recovery time compared to traditional open surgeries. MIEP encompasses various applications, from peripheral artery disease treatments to complex procedures in kidneys, liver, brain, aorta, and heart. Catheter tracking is an essential component of these procedures, allowing for accurate guidance of the catheter through the vasculature  Electromagnetic (EM) tracking is a widely used technology for catheter tracking in MIEP. However, accurate registration between preoperative images and the EM tracking system remains a challenge "
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,1.1,Related Work,"Electromagnetic Tracking is a popular tracking technology that uses integrated sensors in the catheter tip and a field generator to enable localization of the sensor's pose in 3D. EM tracking does not require line-of-sight, which makes it particularly advantageous for MIEP  Registration is an essential step that enables intraoperative guidance of procedures through tracking technologies such as EM. Numerous methods have been developed and presented for EM tracking registration, particularly in vascular procedures  Other registration methods include the use of the EM-tracked catheter paths and registration to the vessel's centerline, using Iterative Closest Point (ICP) algorithms, as described in  Dynamic Time Warping is a widely-used technique in signal processing to compare and align two time-dependent sequences. Dynamic Time Warping (DTW) calculates the similarity between the sequences by optimally aligning them in a nonlinear fashion, taking into account time differences and sequence sampling rates  Despite the well-known advantages of DTW as a technique for signal alignment, its application as a registration method in EM-tracked MIEP is still unexplored. Previous works have reported using DTW and EM tracking; however, these works have primarily used DTW for data processing and evaluation due to its advantages in interpreting intravariability, rather than for registration purposes "
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,2,Method,"This paper introduces a novel approach for automatically registering EM tracking systems to preoperative images using DTW. The introduced method includes a preoperative phase, during which the targeted vasculature is segmented from preoperative images such as Magnetic Resonance Imaging (MRI), computed Tomography (CT), or CT Angiography (CTA), and the centerlines are extracted using the SlicerVMTK toolkit. In the intraoperative phase, the catheter with an integrated EM sensor in its tip is guided through the respective branch of the vascular tree and records a 3D EM path. In this particular implementation, a catheter-shaped EM sensor is used instead. The recorded EM path is then processed using DTW and warped to the centerline, providing point correspondences between the EM path and the centerline. These correspondences are used to perform a closed-form solution using Coherent Point Drift (CPD) algorithm, translating the EM path to the centerline, resulting in the registration of the two coordinate spaces. A detailed overview of the method is presented in Fig. "
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,2.1,EM Tracking System,"The tracking technology used in this paper is the Aurora tracking system from Northern Digital Inc. -NDI (Waterloo, Ontario, Canada). It comprises of a system control unit, sensor interface unit, and a tabletop field generator, which allows for tracking of EM sensors in a 420 × 600 × 600 mm space. This tracking system is capable of recording data with a frequency of up to 40 Hz. The Aurora 5DOF FlexTube, which has a 1 mm diameter, was the catheter-shaped EM sensor used in this paper. It is highly versatile in applications since it can be navigated independently or integrated into catheters. The EM sensor was used without a catheter and was navigated in the phantom through its long cable. The tracking data is recorded using ImFusion Suite software (ImFusion GmbH, Munich, Germany), running on a laptop computer with the following specifications: Windows 11Pro, Intel Core i7-8565U CPU, 16 GB RAM, and Intel UHD 620 Graphics. Figure "
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,2.2,Phantom,"For EM tracking data acquisition purposes of this paper, an STL model obtained from "
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,2.3,Dynamic Time Warping Registration,"During the intraoperative phase, two preoperative components are used, namely the segmented vascular model and its corresponding branch centerline points. The centerline points are referred to as p n c ∈ P 3 preop , where n represents the number of points, and P 3 preop represents the 3D preoperative coordinate space. Firstly, the EM catheter-shaped sensor is guided through the vascular phantom, and the resulting EM path is recorded. The EM path points are referred to as p m em ∈ P 3 em , where m represents the number of points, and P 3 em represents the 3D coordinate space of the EM tracker. In order to use DTW with 3D signals, the number of points must be consistent across the signals; therefore, the signal with fewer points is linearly interpolated to match the number of points in the other signal. Here, the centerlines are interpolated to match the EM paths, p n c → p m c . In the next step of the introduced method, the centerline and the EM path signals are normalized between -1 and 1 to bring the signals into a temporary common coordinate space. The DTW algorithm registration process assumes that the orientation of the phantom (patient) and the preoperative model are similar. Here forwards, DTW decomposes both 3D signals into their respective axes, namely x, y, and z over time, and matches each point from the EM path to the centerline's counterpart. This iterative process stretches the two signals until the sum of the euclidean distances between corresponding points is minimized. The output warp paths represent the corresponding indices that have been warped from the DTW algorithm, creating a set of minimum cost correspondences between the EM path and centerline, c u i,j = (p i c , p j em ). The variable c in the equation refers to the set of corresponding points between the two signals, u represents the total number of correspondences, while i and j represent the indices of the corresponding matched points. For each point in the EM path, there exist one or more points in the centerline that have been warped together and vice versa. For further reading on the DTW algorithm used in this paper, please refer to the following works with more implementation details  In order to register the two signals, we leverage the point correspondences generated by the DTW algorithm to select three sets of equally distributed points from three equal segments of the signals, c 3 i,j = (p i c , p j em ). The correspondences in each signal segment are selected based on the minimum cost return function of the DTW algorithm, where the sum of the euclidean distances between corresponding points is minimal. Utilizing the three segments facilitates the equitable distribution of point matching across the entirety of the signal. This step is crucial to ensure high confidence matching and produce a reliable registration that does not rely entirely on one part of the vasculature. The selected correspondence points are then used to find the rigid transformation between them using the CPD algorithm, as described in "
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,3,Evaluation,"In this study, the introduced DTW method for registration of EM-guided MIEP is evaluated using the mean registration error criterion. The method is compared to path-based ICP registration methods from state-of-the-art. To evaluate the DTW method for registration accuracy, we conducted experiments by recording EM-tracked paths while navigating through each of the six branches of the phantom five times. For each run, the EM catheter-shaped sensor was manually pulled from the main inlet of the phantom towards the outlets of each branch, with an average speed of 1-2 cm/s. Subsequently, the recorded EM paths were registered to the phantom's centerline using the introduced DTW method. The ground truth registration used in this study is a marker-based method, which is considered a benchmark in the literature. Ten unique easily-identifiable landmarks throughout the phantom are used to register the preoperative model to the EM tracking system for calculating the ground truth."
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,3.1,Mean Registration Error,"This criterion is employed to assess the accuracy of the introduced DTW method compared to the ground truth registration. The mean registration error is computed by summing the euclidean distance of each DTW-registered EM path point to its closest point in the ground truth EM-registered path. Equation (1) represents the mathematical expression employed for this computation. The p i em represents an EM path point transformed by the introduced DTW registration method, p j gt represents the closest point from ground truth to p i em , m represents the total number of points in both signals, and || • || represents the Euclidean norm. The results of this evaluation criterion are presented in Fig. "
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,4,Results and Discussion,"Based on the experimental setup and evaluation criterion mentioned above, the results of this proof-of-concept study are presented in detail in Fig.  The introduced methods offers the advantage of automation in the registration process, with no changes in the intraoperative workflow. Unlike markerbased registration methods, which require manual interactions to match point correspondences, the DTW method automatically warps the signals, selects corresponding points, and performs registration. Additionally, the DTW method is not dependent on initialization compared to other path-based registration methods, which may fail to provide a transformation if not correctly initialized. In this paper, the ICP method was consistently initialized from the registered position of the DTW method. This step was necessary as the direct application of ICP registration would not converge to provide a transformation due to the significant distance between the signals. Furthermore, unlike other path-based registration methods, the DTW registration method does not rely on registering the entire EM path to the centerline. In ICP-based methods, the aim is to minimize the difference between all points to be registered, which means that any deformations in one part of the signal would affect the entire registration. In contrast, the DTW method matches points between signals beforehand and employs algorithms to check the confidence of the matched correspondences, ensuring that the set of points to be registered would result in a reliable registration. In comparison to prior studies, the proposed method aligns well with other state-of-the-art approaches in the research community. Marker-based techniques outlined in  Future research directions for the introduced DTW registration include conducting additional evaluations with various phantoms and potentially in-vivo studies, which may provide more evidence of the method's effectiveness in clinical settings. In addition to exploring the impact of catheter movements on DTW matching, another research direction involves improving the approach to registering EM paths to centerlines. In time-dependent series, alternating forward or backward catheter movement can change the signal appearance and result in incorrect matching. To overcome this, one solution is to use the 3D localization and motion-capturing abilities of EM to detect forward and backward movements and then backward warp the signal when the catheter direction changes. Last, advanced algorithms that more accurately depict catheter motion dynamics could be implemented to improve registration accuracy. Current solutions still exhibit significant variability between ground truth catheter movement and the centerline."
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,5,Conclusion,"Accurate catheter tracking is essential in MIEP, and this paper introduces a novel catheter registration method using DTW. As far as the authors know, this study represents the first attempt at using temporal analysis of 3D EM signals for catheter registration. The method is evaluated on a vascular phantom with marker-based registration as the ground truth. The results indicate that the DTW registration method achieves accurate and reliable registration, outperforming path-based ICP registration methods. Furthermore, it provides several advantages compared to existing solutions, including high registration accuracy, registration process automation, preservation of procedural workflow, and elimination of the need for initialization. The method introduced in this paper is a proof-of-concept study, and further experiments by the research community are necessary to establish its applicability and effectiveness in clinical settings. Overall, the introduced DTW registration method has the potential to enhance the accuracy and reliability of catheter tracking in MIEP."
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,,Fig. 1 .,
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,,Fig. 2 .,
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,,Fig. 3 .,
WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 75.
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,1,Introduction,"Over 430,000 new cases of renal cancer were reported in 2020 in the world  Segmentation of kidney tumors on NCCT images adds challenges compared to contrast-enhanced CT (CECT) images, due to low contrast and lack of multiphase images. On CECT images, the kidney tumors have different intensity values compared to the normal tissues. There are several works that demonstrated successful segmentation of kidney tumors with high precision  3D U-Net  In this work, we present a novel framework that is capable of capturing the protuberances in the kidneys. Our goal is to segment kidney tumors including isodensity types on NCCT images. To achieve this goal, we create a synthetic dataset, which has separate annotations for normal kidneys and protruded regions, and train a segmentation network to separate the protruded regions from the normal kidney regions. In order to segment whole tumors, our framework consists of three networks. The first is a base network, which extracts kidneys and an initial tumor region masks. The second protuberance detection network receives the kidney region mask as its input and predicts a protruded region mask. The last fusion network receives the initial tumor mask and the protruded region mask to predict a final tumor mask. This proposed framework enables a better segmentation of isodensity tumors and boosts the performance of segmentation of kidney tumors on NCCT images. The contribution of this work is summarized as follows: 1. Present a pioneering work for segmentation of kidney tumors on NCCT images. 2. Propose a novel framework that explicitly captures protuberances in a kidney to enable a better segmentation of tumors including isodensity types on NCCT images. This framework can be extended to other organs (e.g. adrenal gland, liver, pancreas). 3. Verify that the proposed framework achieves a higher dice score compared to the standard 3D U-Net using a publicly available dataset. "
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,2,Related Work,"The release of two public CT image datasets with kidney and tumor masks from the 2019/2021 Kidney and Kidney Tumor Segmentation challenge  Looking at the top 3 teams from each challenge  In terms of focusing on protruded regions in kidneys, our work is close to  The second protuberance detection network is the same as the base network except it starts from 8 channels instead of 16. We train this network using synthetic datasets. The details of the dataset and training procedures are described in Sect. 3.2. The last fusion network combines the outputs from the base network and the protuberance detection network and makes the final tumor prediction. In detail, we perform a summation of the initial tumor mask and the protruded region mask, and then concatenate the result with the input image. This is the input of the last fusion network, which also has the same architecture as the base network with an exception of having two input channels. This fusion network do not just combine the outputs but also is responsible for removing false positives from the base network and the protuberance detection network. Our combined three network is fully differentiable, however, to train efficiently, we train the model in 3 steps."
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,3.1,Step1: Training Base Network,"In the first step, we train the base network, which is a standard segmentation network, to extract kidney and tumor masks from the images. We use a sigmoid function for the last layer. And as a loss function, we use the dice loss "
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,3.2,Step2: Training Protuberance Detection Network,"In the second step, we train the protuberance detection network alone to separate protruded regions from the normal kidney masks. Here, we only use the crossentropy loss and label smoothing with a smoothing factor of = 0.01. Synthetic Dataset. To enable a segmentation of protruded regions only, a separate annotation of each region is usually required. However, annotating such areas is time-consuming and preparing a large number of data is challenging. Alternatively, we create a synthetic dataset that mimics a kidney with protrusions. The synthetic dataset is created through the following steps: 1. Randomly sample a kidney mask without protuberance and a tumor mask. 2. Apply random rotation and scaling to the tumor mask. 3. Randomly insert the tumor mask into the kidney mask. 4. If both of the following conditions are met, append to the dataset. where k i is a voxel value (0 or 1) in the kidney mask and t i is a voxel value in the tumor mask. Equation 1 ensures that only up to 30% of the kidney is covered with a tumor. Equation "
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,3.3,Step3: End-to-End Training with Fusion Network,"In the final step, we train the complete network jointly. Although our network is fully differentiable, since there is no separate annotation for protruded regions other from the synthetic dataset, we freeze the parameters in protuberance detection network. The output of the protuberance detection network will likely have more false positives than the base network since it has no access to the input image. Thus, when the output of the protuberance detection network is concatenated with the output of the base network, the fusion network can easily reduce the loss by ignoring the protuberance detection network's output, which is suboptimal. To avoid this issue, we perform summation not concatenation to avoid the model from ignoring all output from the protuberance detection network. We then clip the value of the mask to the range of 0 and 1. As a result, the input to the fusion network has two channels. The first channel is the input image, and the second channel is the result of summation of the initial tumor mask and the protruded region mask. We concatenate the input image so that the last network can remove false positives from the predicted masks as well as predicting the missing tumor regions from the protuberance detection network. We use the dice loss and the cross-entropy loss as loss functions for the fusion network. We also keep the loss functions in the base network for predicting kidneys and tumors. The loss function for tumors in the base network acts like an intermediate supervision. Our network shares some similarities with the stacked hourglass network "
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,4,Experiments,No prior work exists that uses NCCT images from KiTS19 
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,4.1,Datasets and Preprocessing,"We used a dataset from KiTS19  The images were first clipped to the intensity value range of [-90, 210] and normalized from -1 to 1. The voxel spacings were normalized to 1 mm. During the training, the images were randomly cropped to a patch size of 128×128×128 voxels. We applied random rotation, random scaling and random noise addition as data augmentation. During the Step2 phase of the training, where we used the synthetic dataset, we created 10,000 masks using the method from Sect. 3.2. We applied some augmentations during training to input masks to simulate the incoming inputs from the base network. The output of the base network is not binarized to keep gradient from flowing, so the values are in the range [0, 1] and the edge of kidneys are usually smooth. Therefore, we applied gaussian blurring, gaussian noise addition and intensity value shifting. "
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,4.2,Training Details and Evaluation Metrics,"Our model was trained using SGD with a 0.9 momentum and a weight decay of 1e-7. We employed a learning rate scheduler, which we warm-up linearly from 0.0001 to 0.1 during the first 30% (for Step1 and Step3) or 10% (for Step2) of the total training steps and decreased following the cosine decay learning rate. A mini-batch size of 8, 16 and 4 were used, and trained for 250k, 100k and 100k steps during Step1 to 3 respectively. We conducted our experiments using JAX (v.0.4.1)  For the experiment on CECT images, we used the dice score as our evaluation metrics following the same formula from KiTS19. For the experiment on NCCT images, we also evaluated the sensitivity and false positives per image (FPs/image). We calculated as true positive when the predicted mask has the dice score greater than 0.5, otherwise we calculated as false negative. On the other hand, false positives were counted when the predicted mask did not overlap with any ground truth masks."
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,5,Results,
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,5.1,Performance on CECT Images,"To show that our model is properly tuned, we compare our baseline model with an existing method using CECT images. As can be seen from Table "
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,5.2,Performance on NCCT Images,Table 
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,6,Conclusion,"In this paper, we proposed a novel framework for kidney tumor segmentation on NCCT images. To cope with isodensity tumors, which have similar intensity values to their surrounding tissues, we created a synthetic dataset to train a network that extracts protuberance from the kidney masks. We combined this network with the base network and fusion network. We evaluated our method using the publicly available KiTS19 dataset, and showed that the proposed method can achieve a higher sensitivity than existing approach. Our framework is not limited to kidney tumors but can also be extended to other organs (e.g., adrenal gland, liver, pancreas)."
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,,Fig. 1 .,
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,,Fig. 2 .,
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,,Fig. 3 .,
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,,Table 1 .,
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,,Table 2 .,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,1,Introduction,"The assessment of fractional flow reserve (FFR) is significant for diagnosing coronary artery disease (CAD) and determining the patients and lesions in need of revascularization  Deep learning has become a promising approach for the assessment of FFR, due to its high computation efficiency in contrast to the computational fluid dynamics  Physics-informed neural networks (PINNs) have the potential to address the aforementioned challenge of lacking appropriate priors. PINNs add priors to the loss function by penalizing the residual of physical equations and the boundary conditions  In this paper, we apply a prior to the inputs. Adding boundary conditions as prior to the inputs resulting in a strong relationship between learned features and boundary conditions. Therefore, the learned features contain more direct and powerful boundary condition information. Additionally, a graph network is introduced as a prior to enforce the coronary topology constraint, because the interaction between the nodes on the graph is similar to the interaction of FFR between the spatial points on the coronary. To this end, we propose a conditional physics-informed graph neural network (CPGNN) for FFR assessment under the constraint of the morphology and boundary conditions. CPGNN adds morphology and boundary conditions as priors to the inputs for learning the conditioned features, besides adding priors to the loss function by penalizing the residual of physical equations and the boundary conditions. Specially, CPGNN consists of a multi-scale graph fusion module (MSGF) and a physics-informed loss. The purpose of MSGF is to generate the features constrained by the coronary topology and better represents the different-range dependence. The physics-informed loss uses the finite difference method to calculate the residuals of physical equations. The main contributions in the paper are three-fold: (1) CPGNN provides FFR assessment under the condition of morphology and boundary. (2) CPGNN introduces a prior by adding the information of the morphology and boundary into inputs and a multi-scale graph fusion module is designed to capture the conditional features related to those information. "
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,2,Method,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,2.1,Problem Statement,"The purpose of CPGNN is to add the appropriate prior, which makes sure that the prediction of blood pressure and flow meet boundary conditions and the conservation of physical principle. The idea is to find a loss term describing the rules of blood pressure and flow. There exists the hemodynamic theory  where Ω is the domain of the coronary, ∂Ω is the boundary, θ is the morphology parameters, γ is boundary condition, F and B are operators to describe the control equation and boundary constraints respectively. A neural network with parameter ω is introduced to approximate the pressure and flow, defined as Q ω (z) and P ω (z). Then, the residuals of the Eq. (  Further, considering the feature learned by Eq. ( "
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,2.2,Conditional Physics-Informed Graph Neural Network,"As shown in the Fig.  According to hemodynamic theory  is obtained by the element-wise fusion on up-sampling feature graphs while the feature H q (z|θ, γ) ∈ R Nv×Nq×C is obtained by concatenating feature graphs at the branch channel. The N q is the concatenated dimension according to the number of feature graphs used to fuse. Conditional Feature Decoding. A serial of 1D convolution and up-sampling is used to decode the feature H p (z|θ, γ) and generate the pressure prediction P (z|θ, γ) ∈ R Nv×N d ×1 . Considering the conversation of the mass, the points share the same prediction on branch. Thus, a full connection layer is used to decode the feature H q (z|θ, γ) to generate the flow prediction Q(z|θ, γ) ∈ R Nv×1 Momentum Loss. The pointset D = (Z, S, Q, P ) contain the coordinate Z, cross-sectional area S, flow Q, and pressure P of the points at the coronary. The vessel wall is assumed to be rigid and blood is Newtonian fluid. The momentum loss term combines the hemodynamic equation and finite differences  where (i, j) denote the j-th point at the i-th branch , N d is point number on branch, N v is branch number on coronary, h i is the point interval on i-th branch defined as , C is a coefficient describing the stenosis influence defined in  where N j is the number of junction on coronary tree, N i p is the number of points of the i-th junction, (i, j) denotes the j-th point at the i-th junction and the 1-st point is the nearest point to the coronary inlet. Boundary Loss. Given the pointset B = (Q, P, γ) containing flow Q, pressure P and condition γ of the points at the boundary, the boundary constraint is penalized by where N b is the number of boundary, the 1-st point is inlet, the rest points are outlet. Eventually, the objective of our CPGNN is to minimize the total loss L total : where λ 1 and λ 2 are the trade-off parameters."
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,3,Experiments and Results,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,3.1,Materials and Experiment Setup,"The experiment contains one synthetic dataset and two in-vivo datasets. (1) the synthetic data. We generate 6600 synthetic coronary trees for training. The parameters of geometry and boundary conditions are randomly set in the appropriate ranges, including vessel radius, the length and number of branches, inlet pressure, and outlet resistances  All experiments run on a platform with NVIDIA RTX A6000 48 GB GPU. The Adam optimizer is used with 16 batch size per step. Initial learning rate is 0.001 and the decay rate is 0.95. The ratio of the training and verification of the synthetic dataset is 8:2. CPGNN is trained on the synthetic dataset and tested on both synthetic and clinical datasets. CPGNN is compared with six state-of-the-art methods, GCN "
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,3.2,Results on Synthetic Data,"Comparison of Pressure Prediction: The performance of CPGNN is closer to the GT with an overall MAE of 0.74, RMSE of 0.86, and MAPE of 0.79. Table "
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Ablation of Conditional Inputs and Multi-scale Mechanism:,As shown in Table 
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,3.3,Results on In-Vivo Data,Comparison of FFR Assessment: As shown in Fig. 
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Ablation of CPGNN Components:,As shown in Table 
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,4,Conclusion,"In this paper, we propose a conditional physics-informed graph neural network (CPGNN) for FFR assessment under the condition of morphology and boundary. Compared to the current reduce-order computation method "
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Fig. 1 .,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Fig. 2 .,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Fig. 3 .,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Fig. 4 .,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Table 1 .,
Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment,,Table 2 .,
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,1,Introduction,"Cerebral X-ray digital subtraction angiography (DSA) is a widely used imaging modality in interventional radiology for blood flow visualization and therapeutic guidance in endovascular treatments  Over the last three decades, various motion correction techniques have been proposed to mitigate the impact of body motion retrospectively  Recent generative models, such as pix2pix  To overcome these limitations, we introduce AngioMoCo, a fast learningbased motion correction method for DSA that avoids severe contrast distortion. We employ a supervised CNN module that distinguishes between motion displacement and contrast intensity change. The output contrast-removed image and the pre-contrast image are then input to a subsequent self-supervised learning-based registration model for deformable registration, where a deformation regularization loss limits the local irregularity. By excluding contrast enhancements from the deformation learning processing, AngioMoCo avoids undesired distortion of the vessels. This results in trustworthy visualization of continuous blood flow and promises to assist in automated analysis of flow-based biomarkers relevant to endovascular treatments. Overall, classical non-rigid registration methods use various regularization strategies to limit vessel distortion, but are prohibitively time-consuming. Recent learning-based methods are fast, but do not explicitly model the motion between frames, and as a result can negatively distort the very clinical information we aim to highlight. We build on the strengths of both directions while avoiding their limitations. Specifically, we propose a novel learning-based strategy that is significantly faster than traditional non-rigid registration methods. AngioMoCo not only removes subtraction artifacts on each frame but does so by explicitly compensating for motion between frames, which is not available in existing image-to-image models. We demonstrate that AngioMoCo achieves high-quality registration while avoiding undesirable contrast reduction or vessel erasure. We define a contrast extraction module f θ f (x t ) = c t with parameters θ f that takes as input a post-contrast frame x t . This function separates x t into a contrast image c t and a contrast-removed image m t where m t = x t -c t . The values in c t are within [-1, 0] as the injected contrast medium can only lead to a decrease in pixel intensity relative to the input image with an intensity range of [0, 1]. The contrast extraction module aims to reduce contrast discrepancies between the pre-and post-contrast frames. Such image-to-image modules can lead to hallucination and may not fully capture distal vessels, relatively less contrasted vessels, and vessels behind bone structures. Therefore, in AngioMoCo, we only employ this module to enable easier registration of the frame x t to the precontrast x 0 using the intermediate contrast-extracted m t image."
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,2,Method,
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,2.1,Model,"We define a registration function r θr (x 0 , m t ) = φ t with parameters θ r to estimate the deformation φ t . We obtain the motionless subtraction angiography y t by subtracting the pre-contrast frame x 0 from the warped post-contrast frame w t : y t = w t -x 0 = x t • φ t -x 0 , where • defines a spatial warp."
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,2.2,Training,"We train the contrast extraction f θ f (•) and deformable registration r θr (•, •) modules separately. The contrast extraction module is trained on a motionless subset of data with an MSE loss between the ground truth contrast, estimated via subtraction between post-and pre-contrast frames (x t -x 0 ), and the predicted c t : We train the deformable registration module on a motion subset, with the pretrained contrast extraction module frozen, using a loss function that combines an MSE loss between m t and x 0 and a smoothness loss L smooth , weighted by λ: where L smooth is the mean squared horizontal and vertical gradients of displacement u t in deformation field φ t , that enforces spatial smoothness of deformation: (3)"
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,2.3,Architecture,"We design the contrast extraction module f θ f (•, •) using a U-Net architecture, which includes a contracting path (encoder) and an expanding path (decoder) connected by skip connections. The encoder stage comprises eight convolutional and max-pooling layers with the number of channels being 8, 16, 32, 64, 128, 256, 512, and 512 respectively. The convolutions operate with a 3 × 3 kernel size and a stride of 2. Similarly, the decoding path employs eight upsampling, 3 × 3 convolution, and concatenation operations with 32 feature maps per layer to restore the spatial dimension up to the input size. Each convolution is accompanied by an instance normalization and a LeakyReLU activation layer. We also use three additional 3 × 3 convolutions. The final convolution employs a negative sigmoid activation, confining the output pixel intensity to [-1, 0]. We employ a deformable registration module r θr (•, •) based on VoxelMorph to learn motion correction in DSA "
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,3,Experiments,"We assess AngioMoCo in terms of vessel contrast preservation, artifact removal, and computation efficiency compared to existing approaches."
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,3.1,Experimental Setup,"Data. We identified 272 patients with unsubtracted cerebral angiographic images available from MR CLEAN registry  Based on visual assessment, we categorized the dataset into two subsets: motionless and motion. We use the motionless subset, consisting of 107 series (1933 frames) from 21 patients, for pre-training the contrast extraction module. The motion subset, which contains 681 series (14708 frames) from 251 patients, is used for overall training and evaluation. We split data on the patient level independently on the motionless and motion subsets, with a ratio of 50%, 20%, and 30% for training, validation, and testing, respectively. Baselines. We compare AngioMoCo with two widely used image registration approaches, elastix-based affine registration and VoxelMorph  Training Details. We use an NVIDIA 2080 Ti GPU (11 GB), the Adam optimizer  Evaluation. We carry out both qualitative and quantitative analyses on the hold-out test set of the motion subset. A key challenge is to minimize motion and subtraction artifacts while retaining clinically important features. We use mean squared intensity (MSI) as a proxy to quantify the preservation of contrast intensity within vessels and the ability of motion correction outside vessels. As ground truth deformations are not available for image sequences with motion, we manually segment the blood vessels in post-contrast frames (Supplemental Fig. "
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,3.2,Results,"Quantitative Analysis. The optimal outcome is represented by the top left corner of Fig.  AngioMoCo(λ = 0.001) achieves similar vessel preservation (P = 0.2), while substantially decreasing the MSI outside vessels (by about half). Compared to Vox-elMorph, AngioMoCo demonstrates substantial improvement, with higher vessel preservation and better (more to the left) artifact removal. While the imageto-image U-Net yields the lowest MSI outside vessels, it sacrifices a substantial amount (30%) of contrast inside vessels, harming the precise clinical signal we are interested in. Qualitative Analysis. Figure  Runtime. Compared to iterative registration methods, deep-learning-based registration methods, including AngioMoCo, require orders of magnitude less time. For example, AngioMoCo takes less than a second to process a series on GPU, while iterative registration methods are mostly implemented on CPU where they require minutes. "
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,4,Discussion,"We find that AngioMoCo achieves high-quality motion correction in DSA, while preserving vessel details, which is of critical clinical importance. While the imageto-image U-Net resulted in fewer artifacts, it substantially degrades the vessel contrast, harming its usability in clinical usefulness. These results suggest that AngioMoCo is clinically relevant for endovascular applications, enhancing the utility of DSA in diagnosis and treatment planning. The tool can extract contrast flow while outputting smooth bi-directional deformation fields that provide interpretability. Unlike image-to-image models, the contrast flow visualization is driven by motion-compensation of the post-contrast frames to the pre-contrast image, and hence avoids undesirable hallucinations and modifications of vessel contrast. We also examined the end-to-end training strategy of AngioMoCo, which did not yield superior results to VoxelMorph or the modularly trained AngioMoCo (Supplemental Fig. "
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,5,Conclusion,"We have presented AngioMoCo, a deep learning-based strategy towards motionfree digital subtraction angiography. The approach leverages a contrast extraction module to disentangle contrast flow from body motion and a deformable registration module to concentrate on motion-induced deformations. The experimental results on a large clinical dataset demonstrate that AngioMoCo outperforms iterative affine registration, learning-based VoxelMorph, and imageto-image U-Net. Overall, AngioMoCo achieves high registration accuracy while preserving vascular features, improving the quality and clinical utility of DSA for diagnosis and treatment planning in endovascular procedures."
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,,Fig. 1 .,
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,,Figure 2 Fig. 2 .,
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,,Fig. 4 .,
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,,,
AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_72.
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,1,Introduction,"Optical coherence tomography angiography (OCTA) is a noninvasive ophthalmic imaging modality that captures the thin vessels and capillaries, named as microvasculature, present around the fovea and parafovea regions at various retinal depths. The 2D en face OCTA images have been increasingly used in clinical investigations for inspecting retinal eye diseases and systemic conditions at the capillary level resolution. More specifically, the morphological changes of retinal vasculature distributed within parafovea regions are associated with diseases such as diabetic retinopathy, early-stage glaucomatous optic neuropathy, macular telangiectasia type 2, uveitis, and age-related macular degeneration  The automated delineation of retinal vasculature from OCTA images encounters several problems such as low signal-to-noise ratio (SNR), projection and motion artifacts, and inhomogeneous image background. A few methodologies have been used in the literature for detecting vessel trees in OCTA images. The majority of the vessel delineation approaches have been presented for color fundus images. Generally, the existing methods for vessel extraction from OCTA images can be categorized into supervised  This paper introduces a new modulatory elongated model to advance the delineation methodology of retinal microvasculature in OCTA images via addressing the above problems. The contributions of this work are summarized as follows. (1) A modulatory function is proposed to include two simultaneous facilitatory and inhibitory delineation processes that distinguish vascular trees more conspicuously from background. (2) The responses of the elongated representation encode the intrinsic profile of the vessels and capillaries, elongated-like shape, which retains the subtle intensity changes of vessels and capillaries for solving the continuity issue. (3) The proposed method disambiguates the region surrounding vessel structures for addressing the disturbance of noise at vascular regions. (4) Our method achieves the best quantitative and qualitative results over the state-of-the-art vessel delineation benchmarks."
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,2,Methodology,"The primary visual cortex (V1) has an essential role in the perception of objects in the visual system. The physiological studies revealed that the response of a V1 neuron to a stimulus in its receptive field can be either increased or decreased when more stimuli are added in the region surrounding the receptive field. The stimuli falling outside the classical receptive field (CRF) can exert modulatory effects on the activities of neurons in V1. The surrounding area beyond the CRF is named as a non-classical receptive filed (nCRF) modulation region, which captures long-range contextual information for modulating the neuron responses  Responses of V1 Neurons. We propose to describe the responses of orientationselectivity V1 neurons to the stimuli placed within the classical receptive field (CRF) by an elongated representation. The elongated kernels  where σ > 0 is the scaling factor that determines the width of the kernel and ρ > 1 represents the anisotropic index that controls the elongation of kernel profiles. T is the matrix transpose and R θ denotes the rotation matrix with angle θ while (x, y) is a point location. In our implementation, we set σ ∈ [1 : 0.5 : 2.5], ρ ∈ (1 : 0.1 : 1.5] and θ ∈ π 16 , 2π 16 , 3π 16 , . . . , 15π 16 , π . θ σ,ρ (x, y) forms a pool of neuron responses for processing the local stimuli within the CRF. For an input OCTA image I(x, y), the final CRF response of a V1 neuron is obtained as: where the notation represents the convolution operation in the spatial domain. Contextual Influences for V1 Neurons. Physiological findings  where F and F -1 represent forward and inverse discrete Fourier transforms, respectively while . 1 and . 2 indicate L 1 norm and L 2 norm, respectively. . denotes a truncated function that replaces negative values with zero. W(x, y) is a distance weighting function that is calculated by a 2D non-negative difference of Gaussian function DoG(x, y) "
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,,Contrast Influence on Contextual Information.,"Physiological experiments  where C l is a local range measure that returns the range value (maximum value-minimum value) using a patch of N × N neighborhood around the pixel (x, y) in the response of ψ. S d computes the local standard deviation of the N × N patch around the corresponding input pixel of ψ response. We use N = 5 in our implementation. Modulatory Function. The essence of the proposed modulatory model is that the response of the elongated representation at a specific point is modulated by the response of the representation presented in the area outside the region of the representation interest. We integrate the elongated responses with the contextual influences at each pixel to produce spatial coherent responses that enhance vessel structures more conspicuously from their background while reducing noise disturbances with vascular regions. The proposed modulatory model (M) is defined as: where is a small value to avoid division by zero. When there are no spurious signals in the region surrounding the elongated responses, the modulatory influence from (H i and S i ) produces a weak response and the numerator (ψ -S i .H i ) of M becomes almost equal to the ψ. As a result, the term (H i + S i ) in the denominator together performs a faciliatory process for improving the responses of ψ. However, the influence of (H i and S i ) has a strong response when containing spurious signals in the surroundings. Then, the (H i and S i ) behave as an inhibitory process in both the numerator and denominator, which drops off the contribution of ψ to almost zero response."
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,3,Experimental Results,Datasets and Metrics. The proposed method is assessed on the ROSE-1 database 
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,,Results and Comparisons,. Figure  Performance on Challenging Cases. In Fig. 
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,4,Conclusion,"This paper presents a modulatory elongated model that exploits contextual modulatory effects presented for tuning the responses of neurons in V1. The proposed method incorporates the elongated responses, neuron responses, at a certain location with either a facilitatory or inhibitory process, contextual information, to reliably enable the delineation of vasculatures in OCTA images and the suppression of background inhomogeneities simultaneously. The validation phase on clinically relevant OCTA images illustrates the effectiveness of our method in producing promising vessel delineation results. The proposed method not only obtains the better quantitative results over the state-of-the-art benchmarks, but also produces an encouraging performance for retaining the continuity of vessels and capillaries, detecting the low-visibility capillaries, and handling spurious signals of noise and artifacts that interfere with vessel structures. Encouraging experimental results demonstrate the effectiveness of the proposed modulatory elongated model in improving the vessel delineation performance. Further, the proposed model is a non-learning approach that does not require any annotated samples or training procedures for performing vasculature detection. As the proposed method is not only responsive to the intrinsic profile of vessels, but also is sensitive to the region surrounding vessel structures, it can be a better alternative to the existing vessel delineation approaches that depend on surroundings-unaware operators."
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,,Fig. 1 .,
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,,Fig. 2 .,
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,,Fig. 3 .,
A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images,,Table 1 .,
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,1,Introduction,"Segmenting the left atrium (LA) from magnetic resonance images (MRI) is critical in treating atrial fibrillation. Recent success of deep learning (DL)-based methods usually requires a large amount of high-quality (HQ) labeled data (termed as Set-HQ). However, since labeling medical images is expertisedemanding and laborious, acquiring massive HQ labeled data from experts is expensive and not always feasible. Without sufficient HQ labels, the DL approaches often struggle with inferior performance. Despite the recent success of semi-supervised learning (SSL) that leverages abundant unlabeled data  The existing works on mining LQ labeled data for medical image segmentation can be categorized by two distinct application scenarios: (i) HQ-agnostic, e.g., Set-HQ and Set-LQ are mixed as one dataset  Tailoring for the HQ-aware scenario, in this work, we propose the Prototypical Label Isolation Learning (PLIL) framework for left atrium segmentation, enabling effective expert-amateur collaboration. Specifically, PLIL is built upon the popular teacher-student framework. Besides the prime supervised signals from HQ labeled data, PLIL robustly exploits the additional LQ labeled data via two steps: (i) Considering the structural characteristics that semantic regions of the same class are often highly correlated and the higher noise tolerance in the high-level feature space "
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,2,Methods,
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,2.1,Problem Formulation,"Our PLIL framework is depicted in Fig.  that only contains M samples, and abundant non-expert LQ noisy labeled data S l = x l(i) , y l(i) N i=M +1 that consists of N -M (usually M ) samples, where x h(i) , x l(i) ∈ R Ωi denote the images and y h(i) , y l(i) ∈ {0, 1} Ωi×C are the given HQ or LQ label (C denotes the class number). Our goal is to learn segmentation with scarce Set-HQ and abundant Set-LQ by optimizing the following loss function: where L HQ and L LQ denote the guidance from HQ and LQ labeled data, respectively. λ is a trade-off weight for L LQ , scheduled by the time-dependent ramp-up Gaussian function  , where t is the current iteration and t max is the maximal iteration. Since our method heavily relies on the manipulation in the feature space, such weighting schedule can reduce the interference of LQ labeled data to the feature space learning at the early training stage. The HQ labeled data provides prime HQ supervised guidance L hs , i.e., L HQ = L hs . Following "
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,2.2,Prototypical Label Isolation for Adaptive Learning,"Teacher-Student Architecture. Our framework is built upon the popular teacher-student architecture  Multi-scale Voting-Based Prototypical Label Isolation. Considering the structural characteristics that the targeted segmentation regions of the same class are often highly correlated and the higher noise tolerance in the high-level feature space  , where the predicted probabilities of object p obj t(v) from the teacher model weight the contribution of voxel v to prototype generation. Similarly, the background prototype q bg i can be also obtained. Then, the relative feature distances d obj i(v) and d bg i(v) between the feature vector of voxel v and the prototypes are defined as: and Intuitively, if the given label y l(v) at voxel v is object (background) yet its feature vector e v lies closer to the background (object) prototype than the object (background) prototype, this voxel will be isolated to the noisy group. Otherwise, it will be selected as the clean labeled one. Formally, the i-th scale determines the clean-label selection mask m i for image x l as: We select the last three scales of features from the teacher model to perform multi-scale voting. Thus, for the final clean-label selection mask, Adaptive Learning Scheme for Isolated Voxels. As shown in Fig.  For the noisy group, since it is extremely difficult to perfectly find out the noisy labels, we do not advocate label refinement as in  The design of m-masked stability loss is motivated by the fact that the estimated noisy group correlates with the voxels with large intra-class variation, wherein these voxels often exhibit difficult and ambiguous nature, which potentially have serious instability problem. Besides, compared to "
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,3,Experiments and Results,Materials. The left atrium (LA) segmentation dataset 
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,,Implementation and Evaluation Metrics. The framework is based on,"PyTorch using an NVIDIA GeForce RTX 3090 GPU. 3D V-Net  Comparison Study. The quantitative results are presented in Table  Ablation Study and Discussions. To further investigate how our method works, we perform an ablation study under the 4-HQ-sample setting (as presented in Table "
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,4,Conclusion,"In this work, we proposed a novel Prototypical Label Isolation Learning (PLIL) framework to robustly learn left atrium segmentation from scarce highquality labeled data and massive low-quality labeled data. Taking advantage of our multi-scale voting-based prototypical label isolation and adaptive learning scheme for clean and suspected noisy labeled voxels, our approach can robustly exploit the additional low-quality labeled data (e.g., via cheap crowdsourcing), which enables effective expert-amateur collaboration. Comprehensive experiments on the left atrium segmentation benchmark demonstrated the superior performance of our method as well as the effectiveness of each proposed component."
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,,Fig. 1 .,
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,,,
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,,Fig. 2 .,
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,,Table 1 .,
Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels,,Table 2 .,
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,1,Introduction,"Heart failure is usually characterized by the inability of the heart to supply enough oxygen and blood to other organs of the body  Cardiac MRI scans contain high-dimensional spatial and temporal features generated throughout the cardiac cycle. The small number of samples compared to the high-dimensional features poses a challenge for machine learning classifiers. To address this issue, Multilinear Principal Component Analysis (MPCA)  Our main contributions are summarized as follows: 1) Methodology: We developed a fully automatic pipeline for PAWP prediction using cardiac MRI data, which includes automatic landmark detection with uncertainty quantification, an uncertainty-based binning strategy for training sample selection, tensor feature learning, and multimodal feature integration. 2) Effectiveness: Extensive experiments on the cardiac MRI scans of 1346 patients with various heart diseases validated our pipeline with a significant improvement (ΔAUC = 0.1027, ΔAccuracy = 0.0628, and ΔMCC = 0.3917) over the current clinical baseline. 3) Clinical utility: Decision curve analysis indicates the diagnostic value of our pipeline, which can be used in screening high-risk patients from a large population. "
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,2,Methods,As shown in Fig. 
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Cardiac MRI Preprocessing:,"The preprocessing of cardiac MRI contains (1) normalization of scans, (2) automatic landmark detection, (3) inter-subject registration, and (4) in-plane downsampling. We standardize cardiac MRI intensity levels using Z-score normalization "
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Landmark Detection and Uncertainty-based Sample Binning:,"We utilize supervised learning to automate landmark detection using an ensemble of Convolutional Neural Networks (CNNs) for each modality (short-axis and fourchamber). We use the U-Net-like architecture and utilize the same training regime implemented in  A minor error in landmark prediction can result in incorrect image registration  Tensor Feature Learning: To extract features from processed cardiac scans, we employ tensor feature learning, i.e. Multilinear Principal Component Analysis (MPCA)  where P n < I n , and × n denotes a mode-wise product. Therefore, the feature dimensions are reduced from I 1 × I 2 × I 3 to P 1 × P 2 × P 3 . We optimize the projection matrices {U (n) } by maximizing total scatter Y m is the mean tensor feature and ||.|| F is the Frobenius norm  Multimodal Feature Integration: To enhance performance, we perform multimodal feature integration using features extracted from the short-axis, fourchamber, and Cardiac Measurements (CM). We adopt two strategies for feature integration, namely the early and late fusion of features  Performance Evaluation: In this paper, we use three primary metrics: Area Under Curve (AUC), accuracy, and Matthew's Correlation Coefficient (MCC), to evaluate the performance of the proposed pipeline. Decision Curve Analysis (DCA) is also conducted to demonstrate the clinical utility of our methodology. "
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,3,Experimental Results and Analysis,"Study Population : Patients with suspected pulmonary hypertension were identified after institutional review board approval and ethics committee review. A total of 1346 patients who underwent Right Heart Catheterization (RHC) and cardiac MRI scans within 24 hours were included. Of these patients, 940 had normal PAWP (≤ 15 mmHg), while 406 had elevated PAWP (> 15 mmHg). Table  Cardiac MRI and Measurement: MRI scans were obtained using a 1.5 Tesla whole-body GE HDx MRI scanner (GE Healthcare, Milwaukee, USA) equipped with 8-channel cardiac coils and retrospective electrocardiogram gating. Two cardiac MRI protocols, short-axis and four-chamber, were employed, following standard clinical protocols to acquire cardiac-gated multi-slice steadystate sequences with a slice thickness of 8 mm, a field of view of 48 × 43.2, a matrix size of 512 × 512, a bandwidth of 125 kHz, and TR/TE of 3.7/1.6 ms. Following "
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Experimental Design:,"We conducted experiments on short-axis and fourchamber scans across four scales. To determine the optimal parameters, we performed 10-fold cross-validation on the training set. From MPCA, we selected the top 210 features. We employed early and late fusion on short-axis and fourchamber scans, respectively, while CM features were only fused using the late fusion strategy. We divided the data into a training set of 1081 cases and a testing set of 265 cases. To simulate a real testing scenario, we designed the experiments such that patients diagnosed in the early years were part of the training set, while patients diagnosed in recent years were part of the testing set. We also partitioned the test into 5 parts based on the diagnosis time to perform different runs of methods and report standard deviations of methods in comparison results. For SVM, we selected the optimal hyper-parameters from {0.001, 0.01, 0.1, 1} using the grid search technique. The code for the experiments has been implemented in Python (version 3.9). We leveraged the cardiac MRI preprocessing pipeline and MPCA from the Python library PyKale "
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Uncertainty-Based Sample Binning:,"To improve the quality of training data, we used quantile binning to remove training samples with uncertain landmarks. The landmarks were divided into 50 bins, and then removed one bin at a time in the descending order of their uncertainties. Figure "
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Unimodal Study:,The performance of three models on single-modality is reported in Table 
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Bi-modal Study:,"In this experiment, we compared the performance of bimodal models. As shown in Table "
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Effectiveness of Tri-modal:,"In this experiment, we performed a fusion of CM features with the bi-modal models to create two tri-modal models. The first trimodal is tri-modal late (CM with a late fusion of short-axis and four-chamber) and the second tri-modal is a tri-modal hybrid (CM with an early fusion of shortaxis and four-chamber). As shown in Fig.  Decision Curve Analysis (DCA)  Feature Contributions: Our model is interpretable. The highly-weighted features were detected in the left ventricle and interventricular septum in cardiac MRI. For cardiac measurements, left atrial volume (0.778/1) contributed more than left ventricular mass (0.222/1) to the prediction."
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,4,Conclusions,"This paper proposed a tensor learning-based pipeline for PAWP classification. We demonstrated that: 1) tensor-based features have a diagnostic value for PAWP, 2) the integration of CM features improved the performance of unimodal and bi-modal methods, 3) the pipeline can be used to screen a large population, as shown using decision curve analysis. However, the current study is limited to single institutional data. In the future, we would like to explore the applicability of the method for multi-institutional data using domain adaptation techniques."
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Fig. 1 .,
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Fig. 2 .Fig. 3 .,
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Fig. 4 .,
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Fig. 5 .,
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Table 1 .,
Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI,,Table 2 .,
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,1,Introduction,"Medical imaging technology has revolutionized the field of cardiac disease diagnosis, enabling the assessment of both cardiac anatomical structures and motion, including the creation of 3D models of the heart  In order to eliminate the step for optimizing the latent code and improve the image generation quality, we propose a morphology-guided diffusion-based 3D cardiac volume reconstruction method that improves the axial resolution of 2D cMRIs through global semantic and regional morphology latent code interpolation as indicated in Fig. "
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,2,Methods,Figure 
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,2.1,DMCVR Architecture,"Our DMCVR is composed of a global semantic encoder E sem , a regional moprphology network (E mor , D mor ) and a diffusion-based generator G. The generating process G is defined as follows: given input x T , sem , mor , which are the stochastic, global semantic and regional morphology latent codes, we want to reconstruct the image x 0 recursively as follows: where θ (x t , t, sem , mor ) is the noise prediction network and f θ is defined as removing the noise from x t or Tweedie's formula  Here, the term α t is a function of t affecting the sampling quality. The forward diffusion process takes the noise x T as input and produces x 0 the target image. Since the change in x T will affect the details of the output images, we can treat x T as the stochastic latent code. Therefore, finding the correct stochastic latent code is crucial for generating image details. Thanks to DDIM proposed by Song et al.  Although using the stochastic latent variables we are able to reconstruct the image accurately, the stochastic latent space does not contain interpolatable high-level semantics. Here we utilize a semantic encoder proposed by Preechakul et al.  One drawback of the global semantic encoder is that it encodes the general high-level features, but tends to pay little attention to the cardiac region. This is due to the relatively small area of LVC, LVM and RVC in the cMRI slice. However, the generation accuracy of the cardiac region is crucial for the cardiac reconstruction task. For this reason, we introduce the regional morphology encoder E mor that embeds the image into the latent space containing necessary information to produce the segmentation map of the target cardiac tissues. With this extra morphology information, we are able to guide the generative model to focus on the boundary of the ventricular cavity and myocardium region, which will produce increased image accuracy in the cardiac region and the downstream segmentation task. Here, we do not assume any particular architecture for the segmentation network. However, in our experiments, we utilize the segmentation network MedFormer proposed by Gao et al.  The training of DMCVR contains the training of the segmentation network and the training of the generative model. We first train the segmentation model with summation of focal loss and dice loss "
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,2.2,3D Volume Reconstruction and Latent-Space-Based Interpolation,"Due to various limitations, the gap between consecutive cardiac slices in cMRI is large, which results in an under-sampled 3D model. In order to output a smooth super-resolution cine image volume, we generate the missing slices by using the interpolated global semantic, regional morphology and stochastic latent codes. For global semantic and regional morphology latent code , since it is similar to the idea of latent code in StyleGAN, we utilize the same interpolation strategies as in the original paper between adjacent slices. Assume that k < j -i, i < j, For interpolating the stochastic latent variable, it is important to consider that the distribution of stochastic noise is high-dimensional Gaussian, as shown in Eq. ( "
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,3,Experiments,
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,3.1,Experimental Settings,"In this study we use data from the publicly available UK Biobank cardiac MRI data  LVC, LVM and RVC are manually annotated on SAX images at the end-diastolic (ED) and end-systolic (ES) cardiac phases. We use 808 cases containing 484,800 2D SAX MR slices for training and 200 cases containing 120,000 2D images for testing. To evaluate the 3D volume reconstruction performance, we randomly choose 50 testing 2D LAX cases to evaluate the 3D volume reconstruction task. All models are implemented on PyTorch 1.13 and trained with 4×RTX8000."
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,3.2,Evaluation of the 2D Slice Generation Quality,We provide peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) 
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,3.3,Evaluation of the 3D Volume Reconstruction Quality Through Latent Space Interpolation,"In this section, we exploit the relationship between SAX and LAX images and leverage the LAX label to evaluate the volume reconstruction quality. In cardiac MRI, long axis (LAX) slices typically comprise 2-chamber (2ch), 3-chamber (3ch), and 4-chamber (4ch) views. To evaluate the performance of different interpolation methods on LAX slices, we conducted the following experiments: 1) Nearest Neighbor resampling of short-axis (SAX) volume to each LAX view, 2) Image-based Linear Interpolation, 3) DeepRecon 1k , and 4) our DMCVR. Table "
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,4,Conclusion,"Integrating analysis of cMRI holds significant clinical importance in understanding and evaluating cardiac function. We propose a diffusion-model-based volume reconstruction method. Our finding shows that through an interpolatable latent space, we are able to improve the spatial resolution and produce meaningful MR images. In the future, we will consider incorporating LAX slices as part of the generation process to help refine the latent space."
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,,Fig. 1 .,
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,,Fig. 2 .,
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,,Fig. 3 .,
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,,Table 2 .Fig. 4 .,
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,,Table 1 .,
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,,929 12.940 0.250 3.236 0.254,
DMCVR: Morphology-Guided Diﬀusion Model for 3D Cardiac Volume Reconstruction,,865 23.636 0.282 3.519 0.267,
Skin Lesion Correspondence Localization in Total Body Photography,1,Introduction,"Evolution, the change of pigmented skin lesions, is a risk factor for melanoma  Several techniques have been proposed to match skin lesions across pairs of 2D images  Several works have been proposed for tackling the skin lesion tracking problem over the full body  We propose a novel framework for finding skin lesion correspondence iteratively using geometric and texture information (Fig. "
Skin Lesion Correspondence Localization in Total Body Photography,2,Methods,"Given a set of lesions of interest (LOIs) X in the source mesh, we would like to find their corresponding positions Y in the target mesh. Formally, we assume we are given source and target meshes, M 0 and M 1 , with vertex sets We achieve this by computing a dense correspondence map Φ L, : V 0 → V 1 , initially defined using geometric information and refined using textural information. Then we use that to define a map taking lesions of interest on the source to positions on the target Φ : X → V 1 ."
Skin Lesion Correspondence Localization in Total Body Photography,2.1,Landmark-Based Correspondences,"We define an initial dense correspondence between source and target vertices by leveraging the sparse landmark correspondences  Concretely, we define maps k : V k → R S , associating a vertex v ∈ V k with an S-dimensional feature descriptor that describes the position of v relative to the landmarks: where is the geodesic distance function on M k . We use the reciprocal of geodesic distance so that landmarks closer to v contribute more significantly to the feature vector. Given this mapping, we create an initial dense correspondence between the source and target vertices, Φ L, : V 0 → V 1 by mapping a source vertex v ∈ V 0 to the target vertex with the most similar feature descriptor (with similarity measured in terms of the normalized cross-correlation):"
Skin Lesion Correspondence Localization in Total Body Photography,2.2,Texture-Based Refinement,"While feature descriptors of corresponding vertices on the source and target mesh are identical when 1) the landmarks are in perfect correspondence, and 2) the source and target differ by an isometry, neither of these assumptions holds in realworld data. To address this, we use local texture to assign an additional feature descriptor to each vertex and use these texture-based descriptors to refine the coarse correspondence given by Φ L, : V 0 → V 1 . Various texture descriptors have been proposed, e.g. SHOT  We achieve this as follows: To every source vertex v ∈ V 0 we associate a region R v ⊂ V 1 of target vertices that are either close to Φ L, (v) (the corresponding vertex on V 1 as predicted by the landmarks) or have similar geometric feature descriptors: ( Given this region, we define the target vertex corresponding to a source as the vertex within the region that has the most similar ECHO descriptor (using the normalized cross-correlation as before). In practice, we compute the ECHO descriptor over three different radii, obtaining three descriptors for each vertex, The selection of three different radii in ECHO descriptors is done to accommodate different sizes of lesions and their surrounding texture, and the values are empirically determined. This gives a mapping Φ L, : V 0 → V 1 defined in terms of the weighted sum of cross-correlations: where is the texture score of the target vertex v and w i is the weight of the cross-correlation between the ECHO descriptors computed at each radius."
Skin Lesion Correspondence Localization in Total Body Photography,2.3,Iterative Skin Lesion Correspondence Localization Framework,"While each source LOI has a corresponding position on the target mesh as given by Φ L, : , not all correspondences are localized with high confidence when 1) the local texture is not well-preserved across scans and 2) the local region R v does not include the true correspondence. To address this, we adapt our algorithm for computing the correspondence map Φ : X → V 1 by iteratively growing the set of landmarks to include LOI correspondences about which we are confident, similar to the way in which a human annotator would label lesion correspondence (Fig.  , with the superscript denoting the k th iteration. For each map Φ k L, and every LOI x ∈ X, we determine if we are confident in the correspondence {x, Φ k L, (x)} by evaluating a binary function χ k L : X → {0, 1}. Denoting by X the subset of LOIs about which we are confident, we add the pairs {x , Φ k L, (x )} to the landmark set L and remove the LOI x ∈ X from X. We iterate this process until all the correspondences of LOIs are confidently found or a maximum number of iterations (K) have been performed. Lesion correspondence confidence is measured using three criteria: i) texture similarity, ii) agreement between geometric and textural correspondences, and iii) the unique existence of a similar lesion within a region. To quantify uniqueness, we compute the set of target vertices whose textural descriptor is similar to that of the LOI: and consider the diameter of the set (defined in terms of the mean of the distances of vertices in S δ x from the centroid of S δ x ). Putting this together, we define confidence as x ) < ε5,  Final Correspondence Map. Having mapped every high-confidence LOI to a corresponding target vertex, we must complete the correspondence for the remaining low-confidence LOIs. We note that for a low-confidence LOI x ∈ X, the texture in the source mesh is not well-matched to the texture in the target, for any v ∈ R x . (Otherwise the first term in χ k L would be large.) To address this, we would like to focus on landmark-based similarity. However, by definition of R x , for all v ∈ R x , we know that the landmark descriptors of x and v will all be similar, so that C L, will not be discriminating. Instead, we use a standard transformation to turn distances into similarities. Specifically, we define geometric score between a source LOI x and a target vertex v ∈ R x in terms of the geodesic distance between v and the corresponding position of x in V 1 , as predicted by the landmark descriptors: where σ is the maximum geodesic distance from a vertex within R x to Φ K L, (x). Therefore, for a remaining LOI, we define its corresponding target vertex as the vertex with the highest weighted sum of the geometric and texture scores: where w 1 and w 2 are the weights for combining the scores."
Skin Lesion Correspondence Localization in Total Body Photography,3,Evaluation and Discussion,
Skin Lesion Correspondence Localization in Total Body Photography,3.1,Dataset,We evaluated our methods on two datasets. The first dataset is from Skin3D 
Skin Lesion Correspondence Localization in Total Body Photography,3.2,Correspondence Localization Error and Success Rate,"Average correspondence localization error (CLE) for individual subjects, defined as the geodesic distance between the ground-truth and the estimated lesion correspondence, is shown in Fig.  We measured the success rate as the percentage of the correctly localized skin lesions over the total number of skin lesion pairs in the dataset. To compare our result to the existing method  Skin3D "
Skin Lesion Correspondence Localization in Total Body Photography,3.3,Usage of Texture on 3D Surface,We believe that the geometric descriptor only provides a coarse correspondence while the local texture is more discriminating. Figure 
Skin Lesion Correspondence Localization in Total Body Photography,4,Conclusions and Limitations,"The evolution of a skin lesion is an important sign of a potentially cancerous growth and total body photography is useful to keep track of skin lesions longitudinally. We proposed a novel framework that leverages geometric and texture information to effectively find lesion correspondence across TBP scans. The framework is evaluated on a private dataset and a public dataset with success rates that are comparable to those of the state-of-the-art method. The proposed method assumes that the local texture enclosing the lesion and its surroundings should be similar from scan to scan. This may not hold when the appearance of the lesion changes dramatically (e.g. if the person acquires a tattoo). Also, the resolution of the mesh affects the precision of the positions of landmarks and lesions. In addition, the method may not work well with longitudinal data that has non-isometric deformation due to huge variations in body shape, inconsistent 3D reconstruction, or a dramatic change in pose and, therefore, topology, such as an open armpit versus a closed one. In the future, the method needs to be evaluated on longitudinal data with longer duration and new lesions absent in the target. In addition, an automatic method to determine accurate landmarks is desirable. Note that although we rely on the manual selection of landmarks, the framework is still preferable over manually annotating lesion correspondences when a subject has hundreds of lesions. As the 3D capture of the full body becomes more prevalent with better quality in TBP, we expect that the proposed method will serve as a valuable step for the longitudinal tracking of skin lesions."
Skin Lesion Correspondence Localization in Total Body Photography,,Fig. 1 .,
Skin Lesion Correspondence Localization in Total Body Photography,,Fig. 2 .,
Skin Lesion Correspondence Localization in Total Body Photography,,Fig. 3 .,
Skin Lesion Correspondence Localization in Total Body Photography,,Fig. 4 .,
Skin Lesion Correspondence Localization in Total Body Photography,,Fig. 5 .,
Skin Lesion Correspondence Localization in Total Body Photography,,,
Skin Lesion Correspondence Localization in Total Body Photography,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_25.
Retinal Thickness Prediction from Multi-modal Fundus Photography,1,Introduction,"Retinal thickness map (RTM), generated from optical coherence tomography (OCT) volumes, provides a quantitative representation of various retina pathologic conditions  However, OCT exams are only available to limited patients as it is both costly and time-consuming, which impedes the acquisition of RTM and ETDRS grid. The recent advances in deep learning  Fundus photography (FP) is widely used to image the retina, which captures the reflected signal of emitted signal from the retinal surface with a flash of light  Recently, Holmberg et al.  Furthermore, we obtain the ETDRS grid prediction, i.e. nine values representing averaged thickness in the predefined areas in Fig. "
Retinal Thickness Prediction from Multi-modal Fundus Photography,2,Methodology,"In this study, we exclusively concentrate on DME to explore the predictive capacity of FPs regarding the retinal thickness. The rationale behind this is that, apart from DME, predicting retinal thickness itself has relatively less clinical value. For example, for age-related macular degeneration, the ophthalmologist needs to look for subtle changes in abnormal OCT features (e.g. subretinal fluid, pigmentary epithelial detachments  In standard clinical settings, the ophthalmologist will acquire the C-FP upon patients' arrival. If RTM is deemed necessary for diagnosis, a separate device will capture IR-FP and conduct OCT scanning. Figure  Dovetailed with the clinical settings, M 2 FRT aims to predict the RTM corresponding to the IR-FP, utilizing enriched depth information from pre-collected C-FP. The RTM requires precise pixel-wise correspondences to the IR-FP, while the ETDRS grid is a regional concept. Therefore, we can manage to derive an ETDRS grid prediction using only easier acquired C-FP, even in the absence of IR-FP, which holds importance within clinical scenarios and telemedicine. As mentioned above, the FPs from the two modalities are captured by different machines. So, the FPs are not registered and have a distinct field of view (FoV). The recent advances in vision Transformers "
Retinal Thickness Prediction from Multi-modal Fundus Photography,2.1,Encoder,"The overall pipeline of M 2 FRT is presented in Fig.  The convolution and concatenation pose ""hard"" operations on the spatial dimensions. Thus, whether we concatenate I IR-F P and I C-F P as input or in the feature space under a CNN backbone, the misalignment of I IR-F P and I C-F P will deteriorate the performance for M prediction. In contrast, the spatial information is ""softly"" incorporated into the Transformer architecture, where the subsequent operations in the feature space are location-agnostic. Therefore, we utilize a CNN encoder E C from U-Net  (1)"
Retinal Thickness Prediction from Multi-modal Fundus Photography,2.2,Decoder,"M 2 FRT extracts 2D aligned depth information from C-FP, which enrich the depth representations acquired from IR-FP in an end-to-end learning manner. The extracted features are fused by concatenation and passed to the decoder D M to generate the thickness map prediction M , where With fine-grained thickness information extracted for the RTM prediction task, the encoded features obtained from E T are ready to be decoded to predict the ETDRS grid using a lightweight decoder D G . In D G , the features from multiple levels are combined using a series of convolutions and concatenations. Then the final prediction for G is generated by a linear projection. The predicted ETDRS grid is denoted as G , where G = D G (f C-F P )."
Retinal Thickness Prediction from Multi-modal Fundus Photography,2.3,Loss Functions,"The loss functions L M 1 and L G 1 are employed in the prediction of the RTM and ETDRS grid using L 1 criteria, respectively, as shown in the following equations, where G (i) and G (i) are the i-th number in the ETDRS grid ground truth G and prediction G . The final loss function is 3 Experiments Implementation Details. The M 2 FRT is implemented with PyTorch  The initial learning rate is 0.001 and exponentially decayed with γ = 0.999. Performance Metrics. For the RTM predictions, we use mean absolute error (MAE) and peak signal-to-noise ratio (PSNR) for evaluation in the areas G 1,2,3 as shown in Fig.  The Wilcoxon signed-rank test is employed to compare the performance of M 2 FRT with the baselines."
Retinal Thickness Prediction from Multi-modal Fundus Photography,3.2,Quantitative and Qualitative Evaluations on RTM Predictions,"To better illustrate the problem and our solution, we begin with the most concise design, U-Net  However, the multi-modal FPs are unregistered and have a distinct FoV, in which case a mere concatenation of these inputs would diminish the network's capacity to effectively exploit the thickness information from paired 2D positions. A simple solution is to encode the multi-modal FPs with two separated convolutional encoders E C , where the features are deeply fused along the downsampling path. The unregistered problem is eased by the higher-level features with a larger receptive field, and the MAE/PSNR are improved to 24.80 µm/28.54 dB. After all, 2D convolution and concatenation pose a ""hard"" operation to the spatial dimensions, which is still interfered with by the unregistered problem. As shown in Fig.  Therefore, we employ distinct encoders of a CNN E C and a Transformer E T to IR-FP and C-FP respectively. The attention mechanism in E T is encouraged to gather 2D aligned thickness information from the perturbed patch embeddings, with the guidance from the decoder D M and the L M 1 loss function. With this CNN-Transformer hybrid design, the MAE/PSNR performance are   Since IR-FP acts as a localizer for the OCT scan and RTM, spatially perturbing the features from IR-FP with E T is not appropriate for the accurate prediction of RTM, and thus not yielding better quantitative results, as shown in Table  Our proposed M 2 FRT utilizes a combination of multi-modal IR-FP and C-FP to predict the RTM. M 2 FRT outperforms the state-of-the-art (SOTA) RTM prediction technique, DeepRT  Additionally, when E T is guided to gather aligned features for RTM using the attention mechanism, the deep features from E T are ready to be decoded by D G for ETDRS grid predictions, which involves computing the averaged thickness in nine predefined regions. Notably, the ETDRS grid prediction task does not have a significant impact on the performance of the RTM prediction (the last two rows in Table "
Retinal Thickness Prediction from Multi-modal Fundus Photography,3.3,Quantitative Evaluations on ETDRS Grid Predictions,"Following the clinical settings, we predict the full RTM based on the IR-FP localizer in place of the OCT scanning procedure, which can boost mass screening. We gather enriched thickness information from C-FP and improve the performance with a hybrid CNN-Transformer design, as elaborated in Sect. 3.2. In addition to identifying 2D disease patterns in C-FP, predicting the ETDRS grid solely from C-FP can exploit additional information in the C-FP and hold significant clinical value for rapid diagnosis, especially in the field of telemedicine. To achieve this, we can adopt a conventional learning-based method to predict the nine numbers in the ETDRS grid, i.e. ResNet  However, simply approximating the nine numbers will neglect the fine-grained thickness information. To address this issue, following the design in Sect. 3.2, the encoder E T for C-FP is guided by the encoder E C from the IR-FP part for detailed RTM predictions. Therefore, E T has been trained to extract fine-grained depth information from C-FP, which can be decoded for the averaged thickness for ETDRS grid predictions with D G . The fine-grained thickness supervision from the RTM prediction task can benefit the ETDRS grid prediction task, as shown in the last two rows of Table "
Retinal Thickness Prediction from Multi-modal Fundus Photography,4,Conclusion,"In this paper, we demonstrate the advantages of leveraging multi-modal information from C-FP for RTM prediction with respect to IR-FP, which overcomes the limitations of OCT and has the potential to enhance mass screening. Additionally, we propose a novel method for predicting the ETDRS grids solely from C-FP, which has significant clinical importance for fast diagnosis, telemedicine, etc. Our results indicate that additional fine-grained supervision from the RTM prediction task is beneficial for ETDRS grid prediction, where the ETDRS grid is decoded from the encoder of C-FP by a lightweight decoder during the training procedure of the RTM prediction task. Further research could be conducted for: 1) Predicting RTM of multiple retinal layers simultaneously, and 2) Improving RTM prediction's resolution and detail by acquiring finer OCT as ground truth."
Retinal Thickness Prediction from Multi-modal Fundus Photography,,Fig. 1 .,
Retinal Thickness Prediction from Multi-modal Fundus Photography,,Fig. 2 .,
Retinal Thickness Prediction from Multi-modal Fundus Photography,,Table 1 .,
Retinal Thickness Prediction from Multi-modal Fundus Photography,,Table 2 .,
Retinal Thickness Prediction from Multi-modal Fundus Photography,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_55.
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,1,Introduction,"Magnetic resonance imaging (MRI) has emerged as an important tool for assessing brain development in utero  However, existing UDA methods in medical imaging mainly focus on the gap from different modalities (e.g., CT and MR), and pay less attention to the domain gap from different centres. Due to motion artifacts  To solve the above UDA task, we propose an Appearance and Structure Consistency (ASC) framework. Consider the fact  The contributions of this work are three-fold: "
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,2,Methodology,"In the UDA setting, D s = {X s , Y s } M s=1 denotes a set of source domain images (e.g., fetal brain atlases) and corresponding labels, respectively. D t = {X t } N t=1 denotes a set of target domain images (e.g., images from the FeTA benchmark). We aim to learn a semantic segmentation model for target domain data based on the labeled source and unlabeled target domain data. Usually, this goal is achieved by minimizing the domain gap between source domain samples D s and target domain samples D t . Figure "
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,2.1,Frequency-Based Appearance Transformation,"Atlases are magnetic resonance fetal images with ""average shape"". Domain shifts between the atlases and fetal images are mainly due to the texture, different hospital sensors, illumination or other low-level sources of variability. However, traditional UDA employing GAN to synthetic style-transfer images hardly capture such domain shift. Thus, we align the low-level statistics based on Fourier transformation to narrow the distribution of the two domains. This process is shown in Fig.  where the mask M = I (h,w,d)∈[-βH:βH,-βW :βW,-βD:βD] controls the proportion of the swapped part over the whole amplitude by a parameter β ∈ (0, 1). Here we assume the center of the image is (0, 0, 0). Then we can train a student network with domain alignment images X sf t , the original images X s and the labels Y s by minimizing the dice loss: where P s and P sf t are the prediction of X s and X sf t , respectively."
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,2.2,Appearance Consistency,"The above loss function imposes an implicit regularization before and after frequency-based transformation. In other words, source domain image X s and its transformation image X sf t should predict the same segmentation. However, the label of images from the target domain is not available X t in UDA settings. As a replacement, we propose a teacher model for keeping semantic consistency across domain transformation. Specifically, the target domain image X t and its aligned image X tf s are regarded as representations of an object under different domains. Given the inputs of X t and X tf s of teacher and student models, we expect their predictions to be consistent. Further, considering that appearance transformation may break certain semantic information and make the model learn the wrong mapping relationship, we employ a form of dual consistency, which directs the model to focus on invariant information between the two views. f (•) and f (•) represent the outputs of the student model and the teacher model, respectively. Following the conventional consistency learning methods "
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,2.3,Structure Consistency,"Although frequency-based transformation and appearance consistency align the two domains' styles, the variance of tissue structure in pathological subjects still brings difficulty to domain alignment, which limits the model's gener alization ability. To this end, we utilise the teacher-student model "
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,2.4,Overall Training Strategy,"We calculate appearance and structure consistency using the same teacher model. Its model weight θ is updated with the exponential moving average (EMA) of the student model f (θ), i.e., θ t = αθ t-1 + (1 -α)θ t , where α is the EMA decay rate that reflects the influence level of the current student model parameters. Let λ control the trade-off between the supervised loss and the unsupervised regularization loss, the overall loss is: 3 Experiment"
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,3.1,Dataset and Pre-processing,We evaluated our method on the Fetal Brain Tissue Annotation and Segmentation Challenge (FeTA) 2021 benchmark dataset 
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,3.2,Implementation Details,All models were implemented in PyTorch 1.12 and trained with NVIDIA A100 GPU with CUDA 11.3. Following the top-ranked method in the FeTA2021 competition 
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,3.3,Comparison with the State-of-the-arts,"We implemented several state-of-the-art label-limited segmentation methods for comparison, including Registration-based (SCALE  It is worth noting that registration-based "
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,3.4,Ablation Study and Sensitivity Analysis,"The ablation study is shown in Table  and L str con . ""M3"" denotes the appearance consistency loss L app con(Xt) to align distribution from source to target. ""M4"" indicates the dual-view appearance consistency loss to constrain semantic invariance. ""M5"" denotes the structure consistency L str con . We can see that appearance consistency can boost performance on normal and abnormal fetal MRIs, showing that minimizing the appearance gap between the source domain and target domain is effective. Further, we can obtain better results on the abnormal samples by applying the structure consistency loss. Table "
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,4,Conclusion,"In this paper, we present a novel UDA framework and an atlas-based UDA setting for fetal brain tissue segmentation. Our method integrates appearance consistency encouraging the model to adapt different domain styles to narrow the domain gap and structure consistency making the model robust against the anatomical variations in the target domain. Experiments on the FeTA2021 benchmark demonstrate that our method outperforms the state-of-the-art methods. The proposed novel setting of atlas-based UDA could provide accurate segmentation for the fetal brain MRI data without pixel-wise annotations, greatly reducing the labeling costs."
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Fig. 1 .,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Fig. 2 .,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Fig. 3 .,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Fig. 4 .,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Table 1 .,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,±0.2 81.7 ±0.1 78.5 ±0.1,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Table 2 .,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,±0.2 81.7 ±0.1 78.5 ±0.1,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Table 3 .,
ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 31.
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,1,Introduction,"Cataract is the leading cause of blindness worldwide, and cataract surgery is one of the most common operations in health care. Among different cataract surgery techniques, phacoemulsification cataract surgery (PCS) is the standard of care  Phacoemulsification needs microsurgical skills, which depend on numerous variables, including the amount of practice, inherent manual dexterity, and previous experience. Statistical analyses have demonstrated significant differences in completion and complication rates among ophthalmologists, with variations observed based on factors such as seniority and experience  To bridge the experience gap among ophthalmologists, several intraoperative AR-guided systems have been developed. Zhai et al.  Advancements in video spatiotemporal learning, particularly in surgical phase recognition, present a promising opportunity to switch AR scenes to the current surgical phase automatically. Early attempts used a 2D CNN to extract spatial features to predict each video frame's surgical phase  In this study, we developed a novel intraoperative AR-guide system for PCS. Our contributions are two-fold: "
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,2,Methods,Figure 
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,2.1,Spatiotemporal Network for Microscopic Video Recognition,"Limbus Segmentation and Spatial Feature Extraction. We observe that the region within the limbus displays distinguishable appearances at different phases in surgical microscope videos, whereas other regions like the sclera exhibit similar appearances. We argue that using a limbus region-focused spatial feature extraction network can improve spatiotemporal aggregation. This led us to develop a multi-task network for limbus segmentation and phase recognition in the first stage. As shown in Fig.  The surgical phase recognition branch involves a fully connected layer that is directly connected to the global average pooling layer, followed by a softmax layer. To train this branch, we use cross-entropy loss, which is defined as where g s is the ground truth binary indicator of phase s, p s is the probability of the input frame belonging to phase s. The limbus segmentation branch incorporates a decoder with upsampling and concatenation, resembling the U-net  where y c i and p c i are the pixel-level ground truth and prediction result respectively, α is a hyper-parameter to balance the loss. The final loss function for training the first stage is defined as where β is a hyper-parameter to balance the loss. After training the first stage, we obtain the spatial feature s t ∈ R 2048 for frame t by outputting the average pooling layer of the spatial feature extractor. Spatiotemporal Features Aggregation. We argue that the surgical phase recognition method used for intraoperative AR should fulfill the following requirements: 1) online recognition for real-time intraoperative guidance, and 2) sufficient stability to avoid distracting ophthalmologists with incorrect phase recognition. As shown in Fig.  where W 3 and W 4 are the weights of the 1×1 convolution, and b 3 and b 4 are bias vectors. Inspired by  where w s is the weight and is inversely proportional to the surgical phase frequencies "
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,2.2,Phase-Specific AR Guidance in PCS,"The proposed spatiotemporal learning network enables real-time limbus segmentation and surgical phase, facilitating the development of our intraoperative AR guidance system for PCS. The limbus contour can be fitted as an ellipse  To accomplish this, we follow the steps shown in Fig.  2) extracting the contour of the maximum connected region and sampling the contour points; 3) removing contour points near the video boundaries; 4) fitting the remaining contour points to an ellipse and outputting the length and rotation of the long and short axes of the ellipse. We segment PCS into nine phases "
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,3,Experiments and Results,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,3.1,Dataset and Implementation Details,Dataset. We evaluate our methods on CATARACTS 
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,3.2,Comparisons with Strong Baselines,We compare our method with several strong baselines in surgical phase recognition: 1) ResNet-50 
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,3.3,Ablation Study,Effect of Multi-task Feature Extractor We performed experiments to evaluate the effect of the multi-task feature extractor. ResNet-50  Effect of the TP-SFA Module We evaluate the number of the connected TP-SFA modules. The quantitative results are listed in Table 
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,3.4,AR Guidance Evaluation,"Our method achieves real-time intraoperative processing at a speed of 36 fps. This makes it suitable for meeting the demands of online intraoperative AR guidance, as the acquired microscope video stream has a speed of 30 fps. We show some typical failed scenes in Fig. "
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,4,Conclusion,"We proposed a two-stage spatiotemporal network for online microscope video recognition. Furthermore, we developed a phase-specific intraoperative AR guidance system for PCS. Our developed system has the potential for clinical applications to enhance ophthalmologists' intraoperative skills."
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,Fig. 1 .,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,Fig. 2 .,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,Table 1 .,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,Table 2 .,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,Table 3 .,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,Table 4 .,
Efficient Spatiotemporal Learning of Microscopic Video for Augmented Reality-Guided Phacoemulsification Cataract Surgery,,Table 5 .,
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,1,Introduction,"Anterior segment optical coherence tomography (AS-OCT) is a widely used noninvasive imaging modality for ocular disease  To suppress speckle noise in AS-OCT images, commercial scanners  Although previous studies have achieved outstanding performances, deploying AS-OCT despeckling algorithms remains challenging due to several reasons: (1). Gathering massive paired data for supervised learning is difficult because clinical data acquisition is time-consuming and expensive. (2). Speckle noise in AS-OCT images strongly correlates with the real signal, making the additive Gaussian assumption on the speckle pattern to remove noise impractically. (3). Unsupervised algorithms can easily miss inherent content, and structural content consistency are vital for clinical intervention in AS-OCT  To address these challenges, we propose a Content-Preserving Diffusion Model for AS-OCT despeckling, named CPDM, which removes speckle noise in AS-OCT images while preserving the inherent content simultaneously. Firstly, we efficiently remove noise via a conditioned noise predictor by truncated diffusion model "
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,2,The Statistical Characteristic of Speckles,"Speckle noise is inherent in coherent imaging systems  where Γ (•) is the Gamma function and M is the number of multilook  To transform the multiplicative noise into an additive one, logarithmic transform  e Mw e -e w M . According to the central limit theorem and analyzing the statistical distribution of transformed one in  (2) Diffusion Model. The diffusion model can subtly capture the semantic knowledge of the input image and prevails in the pixel-level representation  where ε θ is an approximator intended to predict noise ε from x t and I ∼ N (0,1)."
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,,Truncated Diffusion Model.,"As mentioned in the previous section, speckle noise follows a gamma distribution and can be transformed into a Gaussian distribution via a logarithmic function. This transformation enables matching the Markov chain procedure in the reverse diffusion process. To speed up the sampling process, this work introduces a truncated reverse procedure that can directly obtain satisfying results from posterior sampling  CPDM Integrated Fidelity Term. Inspired by the fact that the score-based reverse diffusion process is a stochastic contraction mapping so that as long as the data consistency imposing mapping is non-expansive, data consistency incorporated into the reverse diffusion results in a stochastic contraction to a fixed point  The Bayesian maximum a posteriori (MAP) formulation leads to the image despeckling optimization with data fidelity and regularization terms. arg min where R() is the regularization term, and λ is the regularization parameter. The unconstrained minimization optimization problem can be defined as a constrained formulation by variable splitting method  Motivated by the iterative restoration methods with prior information to tackle various tasks become mainstream, we explore the fidelity term Eq. 4 from the posterior distribution of observed images into the iterative reverse diffusion procedure. The fidelity can guarantee data consistency with original images and avoid falling into artificial artifacts. Moreover, we learn reasonable prior from DDPM reverse recover procedure, which can ensure the flexibility with iterative fidelity term incorporated into the loop of prior generation procedure. As shown in Fig.  where the hyperparameter u control the degree of freedom. It is worth mentioning that Eq. 7 is obtained with the trained CPDM model, and Eq. 8 can be solved by the Newton method "
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,4,Experiment,"To evaluate the performance of the proposed CPDM for AS-OCT image despeckling, we conduct the comparative experiment and a ablation study in despeckling three evaluations, including despeckling evaluation, subsequent CM segmentation or SS localization. Dataset Preparation. A series of unsupervised methods including generative adversarial networks (GAN) and diffusion models aim at learning the noise distribution rather than the signal. Therefore, we collect images by averaging 16 repeated B-scans as noisy-free data collected from AS-OCT, the CASIA2 (Tomey, Japan). This study obeyed the tenets of the Declaration of Helsinki and was approved by the local ethics committee. AS-Casia dataset contains 432 noisy image and 400 unpaired clean image with the size of 2131 × 1600, which are views of the AS structure, including lens, cornea, and iris. 400 noisy data and 400 clean images were used for training, and the rest were for testing. The SS location in the noisy image is annotated by ophthalmologists. CM-Casia dataset consists of 184 noisy images and 184 unpaired clean data with the size of 1065 × 1465 that show the scope of CM tissue. 160 noisy images and 160 clean data are utilized for training network, with the remaining images reserved for testing. Moreover, ophthalmologists annotated the CM regions on the noisy images.  Implementation Settings. The backbone of our model is a simplified version of that in  Comparison on CM-Casia Dataset. We conduct the experiment of image despeckling and the following CM segmentation task to validate the clinical benefit with CPDM. Specifically, we train a U-Net segmentation model  Ablation Study. Table  From Table "
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,5,Conclusions,"Due to the impact of speckles in AS-OCT images, monitoring and analyzing the anterior segment structure is challenging. To improve the quality of AS-OCT images and overcome the difficulty of supervised data acquisition, we propose a content-preserving diffusion model to achieve unsupervised AS-OCT image despeckling. We first analyze the statistical characteristic of speckles and transform it into Gaussian distribution to match the reverse diffusion procedure. Then the posterior distribution knowledge of AS-OCT image is designed as a fidelity term and incorporated into the iterative despeckling process to guarantee data consistency. Our experiments show that the proposed CPDM can efficiently suppress the speckles and preserve content superior to the competing methods. Furthermore, we validate that the CPDM algorithm can benefit medical image analysis based on subsequent CM segmentation and SS localization task."
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,,Fig. 1 .,
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,,Fig. 2 .,
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,,Fig. 3 .,
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,,Fig. 4 .,
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,,Table 1 .,
Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling,,Acknowledgments,. This work was supported in part by 
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,1,Introduction,"Data augmentation (DA) is a key factor in the success of deep neural networks (DNN) as it artificially enlarges the training set to increase their generalization ability as well as robustness  State-of-the-art approaches still rely on simplistic spatial transformations, like translation, rotation, cropping, and scaling by globally augmenting the MRI sequences  Both lesion and prostate shape geometrical appearance influence the clinical assessment of Prostate Imaging-Reporting and Data System (PI-RADS)  Model-driven transformations attempting to simulate organ functions -like respiration, urinary excretion, cardiovascular-and digestion mechanics -offer a high degree of diversity while also providing realistic transformations. Currently, the finite element method (FEM) is the standard for modeling biomechanics  In this work we propose an anatomy-informed spatial augmentation, which leverages information from adjacent organs to mimic typical deformations of the prostate. Due to its lightweight computational requirements, it can be easily integrated into common DA frameworks. This technique allows us to simulate different physiological states during the training and enrich our dataset with a wider range of organ and lesion shapes. Inducing this kind of soft tissue deformation ultimately led to improved model performance in patient-and lesion-level PCa detection on an independent test set. "
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2,Methods,
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2.1,Mathematical Model of the Anatomy-Informed Deformation,"Model-driven spatial transformations simulate realistic soft-tissue deformations, which are part of the physiology, and can highly affect the shape of the prostate as well as the lesions in it. As the computation of state-of-the-art FEM models does not scale to on-the-fly DA, we introduce simplifications to be able to integrate such a biomechanical model as an online DA into the model training: -soft tissue deformation of the prostate is mostly the result of morphological changes in the bladder and rectal space  we apply isotropic deformation to them, -we assume similar elastic modulus between the prostate and surrounding muscles  Based on them, we define the vector field V for the transformation as the gradient of the convolution between the Gaussian kernel G σ and the indicator function S organ , multiplied by a scalar C to control deformation amplitude and direction: The resulting V serves as the deformation field for an MRI sequence I(x, y, z): It allows us to simulate the distension or evacuation of the bladder or rectal space. We refer to this transformation as anatomy-informed deformation. We make it publicly available in Batchgenerators "
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2.2,Experimental Setting,"We evaluate our anatomy-informed DA qualitatively as well as quantitatively. First, we visually inspect whether our assumptions in Sect. 2.1 regarding pelvic biomechanics resulted in realistic transformations. We apply either our proposed transformation to the rectum or the bladder, random deformable or no transformation in randomly selected exams and conduct a strict Turing test with clinicians having different levels of radiology expertise (a freshly graduated clinician (C.E.) and resident radiologists (C.M., K.S.Z.), 1.5 -3 years of experience in prostate MRI) to determine if they can notice the artificial deformation. Finally, we quantify the effect of our proposed transformation on the clinical task of patient-level PCa diagnosis and lesion-level PCa detection. We derive the diagnosis through semantic segmentation of the malignant lesions following previous studies  1. Basic DA setting of nnU-Net  3. Proposed anatomy-informed transformation in addition to the simple DA scheme "
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2.3,Prostate MRI Data,"774 consecutive bi-parametric prostate MRI examinations are included in this study, which were acquired in-house during the clinical routine. The ethics committee of the Medical Faculty Heidelberg approved the study (S-164/2019) and waived informed consent to enable analysis of a consecutive cohort. All experiments were performed in accordance with the declaration of Helsinki "
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2.4,Training Protocol,"774 exams were split into 80% training set (619 exams) and 20% test set (155 exams) by stratifying them based on the prevalence of csPCa (36.3%). The MRI sequences were registered using B-spline transformation based on mutual information to match the ground-truth segmentations across all modalities  As the limited number of exams with csPCa and the small lesion size compared to the whole image can cause instability during training, we adapted the cropping strategy from "
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,3,Results,"The anatomy-informed transformation produced highly realistic soft tissue deformations. Figure  At the selected patient-and object-level working points, the model with the proposed rectum-and bladder-informed DA scheme reached the best results with significant improvements (p < 0.05) compared to the model with the basic DA setting by increasing the F 1 -score with 5.11% and identifying 4 more lesions (5.3%) from the 76 lesions in our test set. The time overhead introduced by anatomy-informed augmentation caused no increase in the training time, the GPU remained the main bottleneck."
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,4,Discussion,"This paper addresses the utilization of anatomy-informed spatial transformations in the training procedure to increase lesion, prostate, and adjacent organ shape variability for the task of PCa diagnosis. For this purpose, a lightweight mathematical model is built for simulating organ-specific soft tissue deformations. The model is integrated into a well-known DA framework and used in model training for enhanced PCa detection. Towards Radiologists' Performance. Inducing lesion shape variability via anatomy-informed augmentation to the training process improved the lesion detection performance and increased the sensitivity value towards radiologistlevel performance in PCa diagnosis in contrast to the training with the basic DA setting. These soft tissue deformations are part of physiology, but only one snapshot is captured from the many possible functional states within each individual MR examination. Our proposed DA simulates examples of physiologic anatomical changes that may have occurred in each of the MRI training examples at the same exam time points, thereby aiding the generalization ability as well as the robustness of the network. We got additional, but slight improvements by extending the DA scheme with bladder distensions. A possible explanation for this result is that less than 30% of the lesions are located close to the bladder, and our dataset did not contain enough training examples for more improvements. Realistic Modeling of Organ Deformation. Our proposed anatomyinformed transformation was designed to mimic real-world deformations in order to preserve essential image features. Most of the transformed sequences successfully passed the Turing test against a freshly graduated clinician with prostate MRI expertise, and some were even able to pass against radiology residents with more expertise. To support the importance of realism in DA quantitatively, we compared the performance of the basic and our anatomy-informed DA scheme with that of the random deformable transformation. The random deformable DA scheme generated high lesion shape variability, but it resulted in lower performance values. This could be due to the fact that it can also cause implausible or even harmful image warping, distorting important features, and producing counterproductive training examples. In comparison, our proposed anatomy-informed DA outperformed the basic and random deformable DA, demonstrating the significance of realistic transformations for achieving superior model performance. High Applicability with Limitations. The easy integration into DA frameworks and no increase in the training time make our proposed anatomy-informed DA highly applicable. Its limitation is the need for additional organ segmentations, which requires additional effort from the annotator. However, pre-trained networks for segmenting anatomical structures like nnU-Net "
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,5,Conclusion,"In this work, we presented a realistic anatomy-informed augmentation, which mimics typical organ deformations in the pelvis. Inducing realistic soft-tissue deformations in the model training via this kind of organ-dependent transformation increased the diagnostic accuracy for PCa, closely approaching radiologistlevel performance. Due to its simple and fast calculation, it can be easily integrated into DA frameworks and can be applied to any organ with similar distension properties. Due to these advantages, the shown improvements in the downstream task strongly motivate to utilize this model as a blueprint for other applications."
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,,Fig. 1 .,
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,,Fig. 2 .,
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,,Fig. 3 .,
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,,Table 1 .,
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_50.
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,1,Introduction,"Achieving complete tumor resection in surgical oncology like breast conserving surgery (BCS) is challenging as boundaries of tumors are not always visible/ palpable  The success of clinical deployment of learning models heavily relies on approaches that are not only accurate but also interpretable. Therefore, it should be clear how models reach their decisions and the confidence they have in such decision. Studies suggest that one way to improve these factors is through data centric approaches i.e. to focus on appropriate representation of data. Specifically, representation of data as graphs has been shown to be effective for medical diagnosis and analysis  Biological data, specially those acquired intra-opertively, are heterogeneous by nature. While the use of ex-vivo data collected under specific protocols are beneficial to develop baseline models, intra-operative deployment of these models is challenging. For iKnife, the ex-vivo data is usually collected from homogeneous regions of resected specimens under the guidance of a trained pathologist, versus the intra-operative data is recorded continuously while the surgeon cutting through tissues with different heterogeneity and pathology. Therefore, beyond predictive power and explainable decision making, intra-operative models must be able to handle mixed and unseen pathology labels. Uncertainty-aware models in computer-assisted interventions can provide clinicians with feedback on prediction confidence to increase their reliability during deployment. Deep ensembles "
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,2,Materials and Methods,Figure 
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,2.1,Data Curation,"Ex-vivo: Data is collected from fresh breast tissue samples from the patients referred to BCS at Kingston Health Sciences Center over two years. The study is approved by the institutional research ethics board and patients consent to be included. Peri-operatively, a pathologist guides and annotates the ex-vivo pointburns, referred to as spectra, from normal or cancerous breast tissue immediately after excision. In addition to spectral data, clinicopathological details such as the status of hormone receptors is also provided post-surgically. In total 51 cancer and 149 normal spectra are collected and stratified into five folds (4 for cross validation and 1 prospectively) with each patient restricted to one fold only."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Intra-operative:,"A stream of iKnife data is collected during a BCS case (27 min) at Kingston Health Sciences Center. At the sampling rate of 1 Hz, a total of 1616 spectra are recorded. Each spectrum is then labeled based both on surgeons comments during the operation and post-operative pathology report. Preprocessing: Each spectrum is converted to a hierarchical graph as illustrated in Fig. "
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,2.2,Network Architecture and Training,
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Graph Transformer Network:,"The GTN consists of a node embedding layer, L Graph Transformer Layers (GTL), a node aggregation layer, multiple dense layers, and a prediction layer  where Q k,l , K k,l , and V k,l are trainable linear weights. The weights w kl ij defines the k-th attention that is paid by node j to update node i at layer l. The concatenation of all H attention heads multiplied by trainable parameters O l generates final attention ĥl+1 i , which is passed through batch normalization and residual layers to update the node features for the next layer. After the last GTL, features from all nodes are aggregated, then passed to the dense layers to construct a final prediction output."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Evidential Graph Transformer:,"Evidential deep learning provides a welldefined theoretical framework to jointly quantify classification prediction and uncertainty modeling by assuming the class probability follows a Dirichlet distribution  In the context of surgical margin assessment, the attentions reveal the relevant metabolic ranges to cancerous tissue, while uncertainty helps identify and filter data with unseen pathology. Specifically, the attentions affect the predictions by selectively emphasizing the contributions of relevant nodes, enabling the model to make more accurate predictions. On the other hand, the spread of the outcome probabilities as modeled by the Dirichlet distribution represents the confidence in the final predictions. Combining the two provides interpretable predictions along with the uncertainty estimation. Mathematically, the Dirichlet distribution is characterized by α = [α 1 , ..., α C ] where C is the number of classes in the classification task. The parameters can be estimates as α = f (x i |Θ) + 1 where f (x i |Θ) is the output of the Evidential Graph Transformer parameterized by Θ for each sample(x i ). Then, the expected probability for the c-th class p c and the total uncertainty u for each sample (x i ) can be calculated as p c = αc S , and u = C S , respectively, where S = C c=1 α c . To fit the Dirichlet distribution to the output layer of our network, we use a loss function consisting of the prediction error L p i and the evidence adjustment where λ is the annealing coefficient to balance the two terms. L p i can be crossentropy, negative log-likelihood, or mean square error , while L e i (Θ) is KL divergence to the uniform Dirichlet distribution "
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,2.3,Experiments,
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Network/Graph Ablation:,We explore the hyper-parameters of the proposed model in an extensive ablation study. The attention parameters include the number of attention heads ( 1-15 with step size of 2) and the number of hidden features 
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Ex-vivo Evaluation:,"The performance of the proposed network is compared with 3 baseline models including GTN, graph convolution network  Clinical Relevance: Hormone receptor status plays an important role in determining breast cancer prognosis and tailoring treatment plans for patients "
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Intra-operative Deployment:,"To explore the intra-operative capability of the models, we deploy the ensemble models of the proposed method as well as the baselines from the cross-validation study to the BCS iKnife stream."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,3,Results and Discussion,"Ablation Study and Ex-vivo Evaluation: According to our ablation study, hyper parameters of 11 attention heads, 11 hidden features per attention head, the cross entropy loss function, and annealing coefficient of 30, result in higher performances when compared to other configurations (370k learnable parameters). The performance of EGT in comparison with the mentioned baselines are summarized in Table  The estimated probabilities in evidence based models are directly correlated with model confidence and therefore more interpretable. To demonstrate this, Table "
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Intra-operative Deployment:,The raw intra-operative iKnife data (y-axis is m/z spectral range and x-axis is the surgery timeline) along with the temporal reference labels extracted from surgeon's call-outs and pathology report are shown in Fig. 
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,4,Conclusion,"Intra-operative deployment of deep learning solutions requires a measure of interpretability as well as predictive confidence. These two factors are particularly importance to deal with heterogeneity of tissues which represented as mixed or unseen labels for the retrospective models. In this paper, we propose an Evidential Graph Transformer for margin detection in breast cancer surgery using mass spectrometry with these benefits in mind. This structure combines the attention mechanisms of graph transformer with predictive uncertainty. We demonstrate the significance of this model in different experiments. It has been shown that the proposed architecture can provide additional insight and consequently clearer interpretation of surgical margin characterization and clinical features like status of hormone receptors. In the future, we plan to work on other uncertainty estimation approaches and further investigate the graph conversion technique to be more targeted on the metabolic pathways, rather than regular conversion."
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Fig. 1 .,
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Fig. 2 .,
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Fig. 3 .,
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Fig. 4 .,
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,,Fig. 5 .,
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,1,Introduction,"Diabetic retinopathy(DR) is a common long-term complication of diabetes that can lead to impaired vision and even blindness as the disease worsen  The challenges mainly lie in: (I) The diagnosis of fundus diseases relies more on local pathological features (haemorrhages, microaneurysms, etc.) than on the global information. How can contrastive learning enable models to extract features of lesion information more effectively on the large datasets with only imagelevel annotation? (II) The false negatives tend to disrupt the feature extraction of contrastive learning  To address the aforementioned issues, we propose the lesion-aware CL framework for DR grading. Specifically, to eliminate false negatives during contrastive learning introduced in automatic disease diagnosis and ensure that samples having similar semantic information stay close in the joint embedding space, we first capture lesion regions in fundus images using a pre-trained lesion detector. Based on the detected regions, we construct a lesion patch set and a healthy patch set, respectively. Then, we develop an encoder and a momentum encoder  To the best of our knowledge, this is the first work to rethink the potential issues of contrastive learning for medical image analysis. In summary, our contributions can be summarized as follows. (1) A new scheme of constructing positives and negatives is proposed to prevent false negatives from disrupting the extraction of lesion features. This design can be easily extended to other types of medical images with less prominent physiological features to achieve better lesion representation. (2) To enhance the capability of CL in extracting lesion features for medical fundus image analysis and improve the quality of learned feature embeddings, a lesion-aware CL framework is proposed for sufficiently exploiting hard negatives. (3) We evaluate our framework on the large-scale EyePACS dataset for DR grading. The experimental results indicate the proposed method leads to a performance boost over the state-of-the-art DR grading methods. Fig. "
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,2.1,Construction of Positives and Negatives,"In this section, we provide a detailed description regarding the construction of positives and negatives. As opposed to traditional CL working on the whole medical images, it is essential to enable the model to focus more on the lesion regions in the images. Our goal is to eliminate the effect of false negatives on contrastive learning for obtaining a better representation of the lesion features. Specifically, given a training dataset X with five labels (1-4 indicating the increasing severity of DR, 0 indicating healthy). We first divide dataset X into lesion subset X L and healthy subset X H based on the disease grade labels of X. Then, we apply a pre-trained detector f det (•) only on X L and obtain high-confidence detection regions. Finally, the construction process of positives P = {p 1 , p 2 , . . . , p j } and negatives N = {n 1 , n 2 , . . . , n k } can be represented as P = Ω(f det (X L ) > conf) and N = Randcrop(X H ), where conf denotes the confidence threshold of detection results, Ω(•) indicates the operation of expanding the predicted boxes of f det (•) to 128*128 for guaranteeing that the lesions are included as much as possible, and Randcrop(•) indicates randomly cropping images into patches with 128*128 from the healthy images."
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,2.2,Dynamic Hard Negatives Mining Enhances Contrastive Learning,"Given the constructed positives P and negatives N , a negatives sampling scheme based on offline knowledge distillation is developed to enable contrastive learning to dynamically exploit hard negatives, and we adjust the update mechanism of the negatives queue(i.e. only enqueue and dequeue N to avoid confusion with P ) to better adapt contrastive learning to the medical image analysis task. Training the Teacher Network. With the positives P , we obtain two views P = {p 1 , p2 , p3 ...p j } and P = {p 1 , p 2 , p 3 ...p j } by data augmentation(i.e. color distortion rotation, cropping followed by resize). Correspondingly, with the negatives N , to increase the diversity of the negatives, we apply a similar data augmentation strategy to obtain the augmented negatives Ñ = {ñ 1 , ñ2 , ñ3 ...ñ k } (where k j). We feed P and P + Ñ to the encoder En(•) and the momentum encoder MoEn(•) to obtain their embeddings Z = {z 1 , ..., z j |z j = En(p j )}, Z = {z 1 , ..., z j |z j = MoEn(p j )} and Z = {z 1 , ..., zk |z k = MoEn(ñ k )}. Then, we calculate the positive and negative similarity matrix by the samples of Z, Z and Z. According to the similarity matrix, the contrastive loss L cl-t of the teacher model training process can be defined as: where sim z j , z j /z k = dot(zj ,z j /z k ) zj 2 z j /z k 2 , τ denotes a temperature parameter, A t p and A t n represent the similarity matrix of positives and negatives, respectively. In order to create a positive sample view different from that of En(•), it should be noted that the parameters θ q of En(•) are updated using gradient descent, while MoEn(•) introduces an extra momentum coefficient m = 0.99 to update its parameters Training the Student Network. Previous works  where γ = δ/(cos( πs 2S ) + 1) represents the number of the current hard negatives, s and S denotes the current and maximum training step, respectively. As s increases during the training process, we dynamically adjust the number of hard negatives such that the difficulty of distillation learning proceeds from easy to hard. Based on the index in A t n , the elements at the corresponding positions in A s n are obtained and a resampled negatives similarity matrix A s n is constructed. Hence, the CL loss L cl-s in training process of student can be formulated as: In addition, to improve the quality of embeddings learned by the student model, we leverage the generated similarity matrices to facilitate the richer knowledge distilled from the teacher to the student. Formally, the KL-divergence loss L kd between A t p , A t n and A s p , A s n is represented as follows. where C(•) denotes the matrix concatenation. The final loss of the student model is L = L cl-s + λ 1 L kd , where the λ 1 is a positive parameter controlling the weight of the knowledge distillation loss L kd ."
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,2.3,DR Grading Task,"To evaluate the effectiveness of the proposed method, we take the encoder of the pre-trained student model as a backbone and fine-tune it for the downstream DR grading task. Considering that the proposed contrastive learning framework is trained with patches, whereas the downstream grading task relies on entire fundus images, an additional attention mechanism is incorporated to break the gap between the inputs of pretext and downstream tasks. Specifically, we first fragment the entire fundus image into patches x = {x p1 , . . . , x pi }. Then, feature embedding v i of x pi is generated by the encoder. Meanwhile, an attention module with two linear layers is utilized in the DR grading task to obtain the attention weight α i of each patch x pi . where W 1 , W 2 are the parameters of the two linear layers , LayerN orm is the layernorm function. Finally, α i is assigned to the corresponding patch's embedding v i to highlight the contribution of patch x pi , and the predicted results of DR obtained by ŷ = W T 3 • N i=1 α i v i , and W T 3 is parameter of the grading layer."
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,3,Experiments,
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,3.1,Datasets and Implementation Details,EyePACS  Implementation Details. The proposed framework is implemented by Pytorch on two Tesla T4 Tensor core GPUs. We employ the IDRiD dataset 
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,3.2,Comparison with the State-of-the-Art,"In this section, we provide qualitative and quantitative comparisons with various DR grading methods and demonstrate the effectiveness of the proposed method. As shown in Table "
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,3.3,Ablation Study,"To more comprehensively evaluate the Lesion-aware CL, we conduct ablation studies to analyze the correlation among DR grading, the construction of positives and negatives(CPN) and dynamic hard negatives mining(DHM). We compare the proposed method with its several variants. The results of ablation study are reported in Table "
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,4,Conclusion,"In this paper, we propose a novel lesion-aware CL framework for DR grading. The proposed method first overcomes the false negatives problem by reconstructing positives and negatives. Then, to improve the quality of learned feature embeddings and enhance the awareness for lesion regions, we design the dynamic hard negatives mining scheme based on knowledge distillation. The experimental results demonstrate that the proposed framework significantly improves the latest results of DR grading on the benchmark dataset. Furthermore, our approaches are migratable and can be easily applied to other medical image analysis tasks."
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,,Fig. 1 .,
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,,,
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,,Table 1 .,
Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis,,Table 2 .,
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,1,Introduction,"Atrial fibrillation (AF) is a cardiac disease characterized by rapid, irregular heartbeats  Clinical studies have discovered a strong relationship between AF and epicardial adipose tissue (EAT), a fat depot layer on the surface of the myocardium that can cause inflammation and disrupt cardiac function  Deep learning has achieved outstanding results on medical imaging analysis tasks, largely due to its ability to learn task-specific features and complex relations between them  In this work, we propose a novel approach to atrial fibrillation sub-type classification from CT volumes by integrating radiomic and deep learning methods. We note that textural radiomic features identified by feature selection methods can serve as an information prior to supplement low-level features from DNNs, since they are designed to capture low-level context and have predictive power "
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,2,Methodology,"We combine radiomic and deep learning approaches using two novel components: 1) feature fusion of local radiomic features and low-level DNN features to improve local context, 2) encouraging complementary deep and radiomic features through feature de-correlation. These are illustrated in Fig.  includes N samples of input x i and binary label y i , where 0 indicates PaAF and 1 indicates PeAF. x i has two channels, one consisting of the 3D CT volume centered around the left atrium and the other the binary region-of-interest (ROI) mask indicating EAT. The ROI is obtained through Hounsfield value thresholding such that all voxels valued between -250 and 0 are identified as EAT "
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,2.1,Feature Fusion of Locally Computed Radiomic Features with Low-Level DNN Features for Improved Local Context,"Under the radiomics pipeline, a large set of features, typically more than a thousand, is first extracted by performing calculations over the volume and ROI input x i . Feature selection methodologies such as mutual information (MI), principal component analysis (PCA), or LASSO regularization, are then used to identify predictive features for classification  Our method applies feature calculations locally to cubic patches centered around each voxel, such that features are obtained on a voxel basis and reflect the statistics of the neighbouring region. For a cubic patch with radius p and input x i , the local feature at location (h, w, d), denoted by r p i,(h,w,d) , is obtained by performing R on the cubic patch in x i centered around (h, w, d): where the input of F r is the cubic sub-volume. This process is illustrated in Fig.  where z i is the fused feature, ⊕ is channel concatenation, and ⊗ is element-wise multiplication. The learned attention tensor A(r l i ⊕ z i ) has dimensions (C + L) × 1 × 1 × 1 and is broadcasted along the volume dimension, such that attention is applied channel-wise and spatial feature distributions are preserved."
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,2.2,Encouraging Complementary Deep and Radiomic Features Through Feature De-Correlation,"Global radiomic features are also included in our model by concatenation with high-level DNN features before the classification layer. Unlike existing approaches however, we encourage our DNN to learn features complementary to radiomic features by enforcing de-correlation between the two. This ensures that different variations are captured, which provides a more comprehensive signal to the classification layer. Accurate approximation of correlation requires large batches sizes however, which requires large GPU memory and can affect model convergence  The first B samples, where B is the batch size, belong to the training sample of the current iteration, and their losses are back-propagated to encourage deep features to have zero correlation with radiomic features. This process is illustrated in Fig. "
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,2.3,Overall Framework,"The DNN model uses raw CT volumes concatenated with ROI masks as input. Global and local radiomic features are pre-computed for input into the feature layer. Binary cross-entropy is used for AF sub-type classification loss L cls : where ŷi is the model prediction for sample y i . The model is trained together with feature de-correlation loss L corr and its loss weighting, w corr . To provide further regularization and prevent over-fitting, we perform an additional selfreconstruction task, using loss L rec , which we describe in more detail in the supplementary materials. The overall loss function is then: 3 Experiments"
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,3.1,Implementation Details,"Dataset. We use a dataset of 172 patients containing 94 PaAF and 78 PeAF cases collected from the Sun Yat-Sen Memorial Hospital in China. CT volumes are centered on the left atrium and normalized to between -1 and 1. ROI masks for EAT are obtained through Hounsfield value thresholding between -250 and 0. Volumes are resized to the same aspect ratio to ensure consistent dimensions across samples. We use an input size of 96 × 128 × 128 voxels and apply zero padding for smaller volumes. We use five-fold cross-validation and report average test performance across folds. Cross-validation is implemented by splitting the dataset into five equal subsets and using three subsets for training, one subset for validation, and one subset for testing. A rolling scheme is used such that different validation and test subsets are used for each of the five folds. Data acquisition procedures and statistics are given in the supplementary materials. Setup. We use the PyRadiomic package "
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,3.2,Comparison with State-of-the-Art Methodologies,"We compare our method with alternative state-of-the-art approaches based on radiomics, deep learning, and hybrid techniques. Deep and hybrid volume-based classification methods  Table "
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,,Type,"Selector We can see that hybrid methods outperform radiomic and deep methods in general. Our method, RIDL, achieves the best results across all metrics however and improves AUC by 1.1% over the baseline method (86.9% v.s. 85.8%) and 3.5% over the best radiomics approach (86.9% v.s. 83.4%)."
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,3.3,Ablation,"Component Analysis. We perform ablation experiments to demonstrate improvements from using local radiomic features, global radiomic features, and feature de-correlation loss. Results are shown in Table  We can see that including local radiomic features, improving AUC by up to 1.6% when included with a standard DNN (78.8% v.s. 77.2%). Using feature de-correlation further boosts performance and leads to the best overall results. Effectiveness of Radiomic Feature Selection. To demonstrate the effectiveness of radiomic feature selection as prior knowledge for feature fusion, we compare with results from using features discarded by radiomics feature selection. We randomly select three discarded features to generate local features r l i as input whilst keeping other components constant. Results are shown in Table  Table "
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,4,Conclusion,"In this work, we propose a new approach to atrial fibrillation sub-type classification from CT volumes by integrating radiomic and deep learning approaches through a radiomics-informed deep learning method, RIDL. Our method is based on two key ideas: feature fusion of locally computed radiomic features with lowlevel DNN features to improve local context, and encouraging complementary deep and radiomic features through feature de-correlation. Unlike existing hybrid approaches, our method specifically addresses the advantages and limitations of both techniques to improve feature extraction. We achieve state-of-the-art results on AF sub-type classification and outperform existing radiomic, deep learning, and hybrid methods. Future improvements to RIDL can be made by introducing more sophisticated local radiomic features selection methods, given the large set features to choose from. Experiments on larger datasets or alternative tasks can also be done to provide more empirical support, since current results show only slight improvements over baseline. These issues may be addressed in future works. Overall, our method is a novel way of combining radiomic and deep learning approaches, and can be used to improve accuracy of PeAF screening from CT volumes for better preventive care of high-risk patients."
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,,Fig. 1 .,
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,,Fig. 2 .,
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,,9 ± 0.6 86.3 ± 0.6 74.7 ± 1.5 76.9 ± 1.0,
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,,Table 2 .,
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 15.
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,1,Introduction,"Magnetic resonance imaging (MRI) has become an increasingly important tool to investigate prenatal equivocal neurological situations, as it provides excellent anatomical details  Several post-processing techniques have been proposed to combine multiple motion-corrupted LR series and leverage the information redundancy from orthogonal orientations to reconstruct a single, 3D isotropic HR motion-free volume of the fetal brain  Hxx LR 2 + αR(x),  This work proposes the first approach to optimize the setting of the regularization parameter α based on numerical simulations of imaging sequences tailored to clinical ones. We take advantage of a recent Fetal Brain MR Acquisition Numerical phantom (FaBiAN)  Our contributions are twofold. First, using synthetic, yet realistic data, we study the sensitivity of the regularization to three common variables in inverse SRR problem in fetal MRI: (i) the number of LR series used as input, (ii) the magnetic field strength which impacts also the in-plane through-plane spatial resolution ratio, and (iii) the gestational age (GA), which leads to substantial changes in brain anatomy. Secondly, we qualitatively illustrate the practical value of our framework, by translating our approach to clinical MR exams. We show that α * estimated by our simulated framework echoes a substantial improvement of image quality in the clinical SRR. To generalize the validity of our findings, we perform our study using two stateof-the-art SRR pipelines, namely MIALSRTK "
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,2,Materials and Methods,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,2.1,Simulated Acquisitions,We use FaBiAN 
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,2.2,Super-Resolution Reconstruction Methods,"Two widely adopted reconstruction pipelines, MIALSRTK  Remark. Contrary to NiftyMIC  Quality Assessment. Solving Problem 1 yields a SR-reconstructed image xHR whose quality can be compared against the reference x HR using various metrics. We use two common metrics for SRR assessment "
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,2.3,Experiment 1 -Controlled in Silico Environment,"In this first experiment, we study the sensitivity of the parameter α to common variations in the acquisition pipeline. Dataset. For every STA subject, nine LR series (three per anatomical orientation) are simulated at 1.5T and 3T with little amplitude of stochastic 3D rigid motion. Experimental Setting. We define four configurations based on the number of LR series given as input to the SRR pipeline (three or six series) and the magnetic field strength (1.5 or 3T). Note that the inter-magnetic field difference is especially captured in the image resolution, with a through-plane/in-plane ratio of 3.3/1.1 = 3 at 1.5T and 3.3/0.5 = 6.6 at 3T. In each configuration, individual brains are repeatedly reconstructed (n = 3) from a selection of different LR series among the nine series available per subject. The grid of parameters searched for NiftyMIC consists of 10 values geometrically spaced between 10 -3 and 2, plus the default parameter α def = 0.01. For MIALSRTK, we use α ∈ {1/0.75, 1/1.0, 1/1.5, 1/2.0, 1/2.5, 1/3.0, 1/3.5, 1/5.0} (8 values, with default parameter α def = 1/0.75). At the end of the experiment, the best parameter, for either of the pipelines, is referred to as α * 1 . Statistical Analysis. The optimal regularization parameters evaluated for the different SRR configurations are compared using the Wilcoxon rank sum test. The difference between the metrics performance obtained with default or optimal parameters is tested with a paired Wilcoxon rank sum test. The p-value for statistical significance is set to 0.05."
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,2.4,Experiment 2 -Clinical Environment,"Clinical MR fetal exams are prone to substantial inter-subject variation and heterogeneity. In particular, the number of LR series available for reconstruction, as well as the amplitude of fetal motion, greatly vary from one subject to the other  Dataset. Twenty fetal brain MR exams conducted upon medical indication were retrospectively collected from our institution. All brains were finally considered normal. Fetuses were aged between 21 and 34 weeks of GA (mean ± standard deviation (sd): 29.7 ± 3.6) at scan time. For each subject, at least three orthogonal series were acquired at 1.5T (voxel size: 1.125 × 1.125 × 3 mm 3 ). After inspection, four to nine series (mean ± sd: 6.3 ± 1.5) were considered exploitable for SRR. The local ethics committee approved the retrospective collection and analysis of MRI data and the prospective studies for the collection and analysis of the MRI data in presence of a signed form of either general or specific consent. The same 20 subjects are simulated using exam-specific parameters to mimic as closely as possible the corresponding clinical acquisitions. In particular, we match the number and the orientation of the LR series, as well as the amplitude of fetal motion (from little to moderate), and the GA of each subject. Experimental Setting. We consider the same regularization parameter space as in Experiment 1 (Sect. 2.3), and evaluate both clinical and simulated data on this parameter grid."
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Statistical Analysis.,"We compare the similarity between the images reconstructed by MIALSRTK and NiftyMIC using both default and optimized parameters. In this experiment, no reference images are available. Statistical significance of the performance difference is tested using a paired Wilcoxon rank sum test (p < 0.05 for statistical significance)."
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,3,Results,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,3.1,Experiment 1 -Controlled in Silico Environment,Optimal Regularization Parameter. Figure 
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,MIALSRTK,Gestational Age-Based Analysis. Since the human brain undergoes drastic morphological changes throughout gestation 
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,3.2,Experiment 2 -Clinical Environment,"In this experiment, we compare two differently optimized regularization parameters. First, we use the optimal value α * 1 (from Fig. "
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,4,Discussion and Conclusion,"In this paper, we propose a novel simulation-based approach that addresses the need for automated, quantitative optimization of the regularization hyperparameter in ill-conditioned inverse problems, with a case study in the context of SRR fetal brain MRI. Our estimated regularization weight shows both qualitative and quantitative improvements over widely adopted default parameters. Our results also suggest that subject-specific parameter tuning -which is computationally expensive to run -might not be necessary, but that an acquisition setting-specific tuning, ran only once, might be sufficient in practice. As such, the proposed methodology demonstrates a high practical value in a clinical setting where fetal MR protocols are not standardized, leading to heterogeneous acquisition schemes across centers and scanners. Besides, we show that our simulation-based optimization approach reduces the variability in image quality and appearance between the two SRR pipelines studied. We expect this behavior to contribute to mitigating the domain shift currently inherent to any reconstruction technique, a key challenge in the development of automated tissue segmentation methods "
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Fig. 1 .,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Fig. 2 .,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Fig. 3 .,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Fig. 4 .,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Fig. 5 .,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Fig. 6 .,
Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 32.
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,1,Introduction,"Estimation of patient-specific hemodynamic features in coronary arteries is an essential step in providing personalized and accurate diagnosis and treatment of CAD which is one of the main causes of death in the world  To solve the computation time drawback, AI-based solutions have been proposed to lower the estimation time of vFFR down to minutes, or even seconds at the cost of accuracy  In this work, we tackle the problem of estimating hemodynamic features such as pressure drops and vFFR along the coronary arteries extracted from CTA. We propose a novel CenterlinePointNet++ architecture that is tailored towards the processing of complex, elongated structures such as coronary arteries, that can be represented as a surface point cloud and a centerline graph along branches. In our approach, implicit feature extraction is guided by the proposed centerline grouping aggregation. It is a replacement for the commonly used topology-agnostic "
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,2,Method,"In this section, we provide a detailed description of the proposed CenterlinePoint-Net++ architecture (see Fig. "
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,2.1,Encoder,"The encoder is built out of the proposed CSA blocks in a hierarchical manner as it is shown in Fig.  from the previous block and a centerline graph C, where N stands for the number of points, F stands for the number of features, i + 1 is the index of the current CSA block, i is the index of the previous block, and (e) stands for ""encoding"". The CSA block is described in Fig.  For each point r ∈ R, a centerline grouping procedure g(r, C) is performed to extract their point neighborhoods G r ⊆ P. In the last step, all extracted neighbourhoods are processed independently with a PointNet  Centerline Grouping: We propose a novel centerline-guided point grouping scheme for point cloud structures. Commonly used Euclidean distance based grouping strategies do not take into account an underlying surface manifold formed by the points. Thus they tend to fail when the point cloud topology is complex, by considering points in the close Euclidean distance to be neighbours while their distance along the formed manifold is much larger (see Fig.  For a given point cloud P, its representation R and its centerline C, the centerline grouping procedure g(r, C) is performed independently for each r ∈ R. We define a mapping function M : P → V C , which assigns a closest, in Euclidean distance sense, centerline node v ∈ V C to each point p ∈ P. Since a representative point cloud R is a subset of point cloud P, the mapping is also defined for each point r ∈ R. Due to the utilization of a mapping function M , the grouping procedure can be performed directly on the centerline graph C where the topological structure of the vessel tree can be more accurately represented. Based on the 3D coordinates of centerline nodes V C , the weights of the edges E C are calculated as distances between the connected nodes -since the centerline graph is not-mutable during training, the weights can be pre-computed in the pre-processing stage. Having a mapping function M we can directly work on the centerline graph itself and thus we define a centerline neighbourhood Q v which is computed independently for each v ∈ M [R] and can be expressed with the following equation: where d C is a centerline distance function which returns the length of the weighted shortest path between two given nodes in the centerline C and t is the distance threshold which marks whether nodes should be considered neighbours or not. Having centerline neighbourhoods Q extracted we need to map them back onto the point cloud P to obtain the point neighbourhoods G for each r ∈ R:"
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,2.2,Decoder,The decoder is built out of the Feature-Propagation (FP) blocks in a similar manner as in PointNet++ 
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,3,Experiments,"In this section, we describe our evaluation of the proposed model architecture in the tasks of pressure drops and vFFR estimation of a synthetic coronary artery geometry with respect to different stenosis severity grades and different biologically relevant ranges of blood flow characteristics. We test our architecture against standard PointNet++ grouping with the same number of adequate layers as a reference due to the impossibility of reproduction "
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,3.1,Dataset,"We utilize a dataset of 1, 700 synthetically generated coronary arteries to train and evaluate the model's performance. We split the data to train, validation and test set of sizes 1,500, 100 and 100, respectively. We generate synthetic coronary arteries using ranges of geometric quantities such as the radii of branches, bifurcation angles, and degree of tapering with distributions similar to other studies  For the ground-truth (GT) labels generation we use a commercial CFD engine of choice  The considered simulations for GT are stationary and take up to two hours on a CPU with 16 processes per synthetic coronary artery. We generate labels from a biologically relevant range that aim to simulate a patient under rest, mild exercise and high-intensity exercise conditions "
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,3.2,Results,We showcase the comparison between PointNet++ and proposed Centerline-PointNet++ in the task of pressure drop and vFFR estimation for the synthetic dataset under different biologically relevant boundary conditions (BC) in Fig.  We show a visual comparison between the PointNet++ and CenterlinePoint-Net++ capabilities in the task of the vFFR in Fig.  We evaluate estimated vFFR with respect to the stenosis severity grade and group stenosis grades into the relevant intervals based on CAD-RADS scale 
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,4,Conclusions,"In this study, we propose a novel point cloud based neural network architecture CenterlinePointNet++ which is tailored towards the analysis of complex vessel trees by incorporating multi-modal input of surface point cloud and centerline graph. We show an improvement in the vFFR time estimation from approx. two hours for a CFD simulation to around 15 s per synthetic coronary artery. Our centerline grouping approach is confronted with PointNet++ in the task of pressure drop and vFFR estimation in synthetic coronary arteries and achieves better results in NMAE and MAE for every set of values of the input flow and pressure of CFD simulation. The evaluation of FFR showcases a correlation of 0.93 with the CFD vFFR. One of the limitations of the method is the fact that the model is trained for the specified set of boundary conditions of underlying CFD simulation. In the future, we plan to expand the approach by incorporating boundary conditions as an additional input to the network. We also aim to conduct a comprehensive study using real patients' geometries for both training and evaluation, while comparing the results with invasive FFR."
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,Fig. 1 .,
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,Fig. 2 .,
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,Fig. 3 .,
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,,
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,Fig. 4 .,
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,Fig. 5 .,
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,Table 1 .,
CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 73.
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,1,Introduction,"Population ageing is a huge health burden worldwide as the risk of morbidity and mortality increases exponentially with age  With the advent of technology, deep learning (DL) algorithms have found great applications in retinal age prediction. For example, Liu et al.  Therefore, in this study, we present an attempt to provide a novel accurate estimate of retinal age by learning adaptive age distribution from multiple cohorts with temporal fundus images available. Instead of learning a model using fixed label distribution as ground truth, we formulate the age estimation as a two-stage LDL task and give an adaptive distribution estimate for individual fundus images. As learning the LDL model with images from different data sources can harm the consistency and ordinality of embedding space, we introduce ordinal constraints to align the image features from different domains. Moreover, to leverage the temporal knowledge from the fundus image sequence, we add a temporal branch to capture the temporal evolution and use this auxiliary information to enhance the predictive performance of our model on snapshot images. We verify our method on a large retinal fundus dataset which consists of approximately 130k images of healthy subjects from the UKB cohort and Chinese cohorts. Extensive experiments prove that our model can achieve lower age prediction errors on multiple cohorts."
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,2,Method,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,2.1,Progressive Label Distribution Learning,"As shown in Fig.  Formally, given a dataset with N images The image encoder first transforms an input image x i into a spatial feature F i ∈ R H×W ×C , then a convolutional projection layer maps F i into the base representation F i ∈ R H×W ×D and the averaged feature f i ∈ R 1×D for the following age distribution learning. We discretize the age classes as ŷi = R (y i /δ d ) * δ d , where R (•) denotes the round operator and δ d is the age bin for tuning the discretization degree. Therefore, the total discretized age class number is . For the coarse-level age estimation, we set a large δ d = 10 which determines the age group queries as Then, we use an FC layer with softmax applied on the f i to calculate the coarse age distribution p i ∈ R 1×C δ d . Different from the previous study  where * ŷc is the expected value of the learned distribution p i , The first term is the cross-entropy loss which helps the model converges in an early training stage, the last two terms encourage the learned distribution to be centered and concentrated at the true age labels. In the refining stage, the mean value of coarse age distribution m i is used to select the age group query from Q coarse to involve the computation of fine-level feature: where GAP (•) is the global averaged pooling and A (•) denote the attention function with θ a as the parameters. The key and value vectors in the atten- tion function come from F i . Finally, we concatenate the f with the mapped coarse age distribution as the final feature embedding to predict the fine-level age distribution on a small age bin of δ d = 1: where f (•) denotes an FC layer with parameters θ f , mlp (•) represents a multilayer perceptron with one hidden layer and the parameter is θ m . The training loss is the same with Eq. 1."
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,2.2,Cross-Domain Ordinal Feature Alignment,"Although existing studies  where D [•] denotes metric of Euclidean distance, m is a dynamic margin depends on the relative age difference gap between |ŷ i -ŷj | and |ŷ i -ŷj |. To align features from different data domains, we directly select samples from same class and push them closer in the embedding space by minimizing the intra-class distance on both coarse-level features and fine-level features: where the I c (i, j) is an indicator function."
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,2.3,Co-Learning with Temporal Fundus Images,"Compared to merely learning from single snapshot images, temporal data capturing more aging information can further boost retinal age prediction. However, in practice, temporal fundus data can be limited because the individuals are often lost to follow-up. Directly learning a temporal model on these small data usually cause poor generalization. Therefore, we propose to co-train our model on limited temporal imaging data and large-scale snapshot imaging data. Our aim is to use the auxiliary knowledge from temporal data to enhance the performance of our model tested on snapshot images. As illustrated in Fig.  where R 2 (•) denotes the distance correlation and the detailed definition refers to "
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,3,Experiment and Results,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,3.2,Quantitative Results,"Comparative Study: We then compare our model with existing popular regression methods which include both direct regression method, classification-Fig.  Ablation Study: Here, we give the ablation results of our model to illustrate how different components affect the final performance. Figure  The results indicate that the model produces high MAE on the tail ages in each cohort. Therefore, a future step to improve our model would be to consider the imbalanced learning techniques or group-wise analysis to reduce the MAE bias."
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,3.3,Visualization Results,In Fig. 
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,4,Conclusion,"In this study, we present a novel accurate modeling of retinal age prediction. Our model is capable of learning adaptive age distribution from multiple cohorts and leveraging temporal knowledge learned from sequencing images to improve age prediction on snapshot image modeling. Our model demonstrated improved performance in four independent datasets, with an overall MAE much lower than previously proposed algorithms."
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,,Fig. 1 .,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,,Fig. 2 .,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,,3. 1,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,,Fig. 3 .,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,,Fig. 6 .,
Retinal Age Estimation with Temporal Fundus Images Enhanced Progressive Label Distribution Learning,,Table 1 .,
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,1,Introduction,"Intraoperative imaging employing a mobile C-arm enables immediate and continuous control during orthopedic and trauma interventions. For optimal fracture reduction and implant placement, correct acquisition of standard views that correspond to a specific C-arm pose relative to the patient is essential "
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,2,Materials and Methods,An overview of the complete framework for fully automated C-arm positioning towards desired standard views during knee surgeries is given in Fig. 
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,2.1,Training Data Simulation,"To address the interventional data scarcity problem, simulated training data was generated from a collection of CT and C-arm volumes using a realistic DRR simulation framework "
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,2.2,Shape-Based Positioning Framework:,The proposed shape-based positioning framework was trained jointly for both standard views (Fig. 
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,(1) Intensity-based multi-task classification and regression module:,"For simultaneous in-plane rotation regression, view recognition, and laterality classification, an EfficientNet-B0 feature extractor  (2) Shape-based pose regression: Following surgical characteristics for recognizing correct standard views of the knee, a view-independent shape-based pose regression framework was developed. The architecture is based on a 2D U-Net "
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,2.3,Real X-Ray Test Data:,"Real X-rays for validation were sampled from single Siemens Cios Spin R sequences generated during 3D acquisition of 6 knee cadavers. Preprocessing consisted of (1) definition of 3D standard reference planes, (2) laterality check, (3) sampling of X-rays around the defined reference standards in the interval α, β ∈ [-30 • , 30 • ], and generation of ground truth pose labels. Since the Spin sequences are orbital acquisition sequences, only the orbital rotation is equidistantly covered in the test set, while the angular rotation is constant for all X-rays sampled from the same sequence. The number of sampled X-rays per standard and view may differ, if the reference standard is located close to the edge of the orbital sequence (range: 102-124). "
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,3.1,Importance of Pipeline Design Choices (RQ1),"In an ablation study, the proposed shape-based view-independent pose regression was compared to view-specific direct intensity-based pose regression "
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,3.2,Importance of Individual Bones on Overall Performance (RQ2),Figure 
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,3.3,Accuracy of View and Laterality Classification (RQ3),"The classificator performances were assessed on the synthetic (3528 DRRs, 4 CTs) and real data (1386 X-rays, 6 C-arm scans). The view classificator (a.-p. / lateral) achieved an accuracy of 100% on the test DRRs and 99% on the X-rays. The laterality classificator (left / right) resulted in an accuracy of 98% on the test DRRs and 98% on the X-rays."
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,4,Discussion and Conclusion,"A complete framework for automatic acquisition of standard views of the knee is proposed that can handle several standard views with a single architecture. The complete pipeline is trained on simulated data with automatically generated annotations and evaluated on real intraoperative X-rays. To bridge the domain gap, different augmentation strategies are suggested that address intraoperative confounding factors, e.g., the OR table. View-independent training and multilabel shape features improve the generalization from simulated training to real X-rays and outperform direct intensity-based approaches. View-independent networks result in more training data which showed to improve the generalization from simulated training to real X-rays. The 2-step approach increases robustness and simultaneously automates necessary preprocessing tasks like laterality and standard view recognition, which can be performed with very high accuracy on simulated (100%, 98%) and real data (99%, 98%). The approach is fast and easy to translate into the operating room as it does not require any additional technical equipment. Assuming that the surgeon acquires the initial X-ray Data use declaration: The data was obtained retrospectively from anonymized databases and not generated intentionally for the study. The acquisition of data from living patients had a medical indication and informed consent was not required. The corresponding consent for body donation for these purposes has been obtained."
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,Fig. 1 .,
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,Fig. 2 .Fig. 3 .,
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,( 2 ),
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,Fig. 4 .,
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,Fig. 5 .,
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,Fig. 6 .,
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,Fig. 7 .,
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 45.
Shape-Based Pose Estimation for Automatic Standard Views of the Knee,3,Experiments and Results,"The proposed pipeline was evaluated considering the following research questions: (RQ1) Does the proposed shape-based pipeline outperform view-specific intensity-based and shape-based pose regression? How does it influence the generalization from synthetic to real data? (Sect. 3.1) (RQ2) How do individual bones influence the overall positioning performance? (Sect. 3.2) (RQ3) How accurate is the performance of the view and laterality classification? (Sect. 3.3) Positioning performance was evaluated based on the angle θ = arccos v pred , v gt between the principal rays of the ground truth v gt and predicted pose v pred and the mean absolute error (AE) of in-plane rotation γ. The interrater variation of the reference standard planes defined by two independent raters serves as an upper bound for the reachable accuracy of a C-arm positioning approach trained on the reference annotations. It was assessed in terms of orientation differences θ (θ a.-p. = 4.1 ± 2.6 • , θ lateral = 1.8 ± 1.3 • ). The models were implemented using PyTorch 1.6.0, trained with an 11 GB GeForce RTX 2080 Ti, and optimized with the Adam optimizer with a base learning rate of η = 10 -4 and batchsize 8, pre-trained independently, and jointly fine-tuned until convergence."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,1,Introduction,"Virtual models of cardiac electrophysiology (EP) have demonstrated significant potential in various clinical tasks, such as stratifying the risk for lethal arrhythmias  Physics-informed neural networks (PINNs) is a new paradigm for solving both the forward and inverse problems of PDEs  What fundamental challenges does 3D bi-ventricular EP present for PINNs? First, 3D bi-ventricular EP simulation requires the PDE solutions to be obtained over a complex geometry domain in space and a long duration in time. Second, the PDE solution to bi-ventricular EP -in the form of the spatiotemporal propagation of transmembrane potential (TMP) activation -exhibits sharp spatial and temporal gradients. These characteristics of bi-ventricluar EP simulations present fundamental challenges to the state-of-the-art PINN framework, which has been shown to face potential failure modes when the PDEs are solved over large or complex solution domains with sharp transitions  In this paper, we present a novel PINN framework to overcome these challenges and enable the forward and inverse solutions to 3D bi-ventricular EP simulations. This is achieved with three key innovations. First, to avoid dealing with higher-order spatial derivatives over the complex 3D geometry, we formulated the PINN over the meshfree representation of the 3D bi-ventricular geometry with a modified PDE residual incorporating the weak form of the original PDE. Second, to enable PINN solutions over the long temporal domain, we present a temporally adaptive and sequential training strategy to guide the PINN to respect the causality of the underlying physics of wave propagation. Finally, we introduce a spatially adaptive training strategy to guide the PINN to exploit the spatiotemporal sparsity of the sharp gradients exhibited in the PDE solution. We experimentally demonstrated that the presented PINN framework was able to enable complete simulation of the bi-ventricular EP activation process that is otherwise not possible with vanilla PINN frameworks. We further conducted detailed ablation studies of the benefits of the presented spatialtemporally adaptive and sequential learning strategies, and demonstrated preliminary feasibility of the presented PINN framework for supporting the inverse parameter estimation of 3D bi-ventricular EP models. These represent an innovative first attempt to enable PINN solutions for 3D bi-ventricular EP applications."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,2,Background: Bi-Ventricular EP Simulations,"Existing ventricular EP models range from macroscopic-level two-variable PDEs to ionic models with tens of variables  where u ∈ [0, 1] is the unit-less TMP and v is the recovery current. The diffusion tensor D describes local conductivity anisotropy determined by fiber structures. Parameters {γ, c, e 0 , μ 1 , μ 2 } control the temporal dynamics of u and v."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,3,Methodology,We present a novel PINN framework to support forward and inverse 3D biventricular EP simulations. As outlined in Fig. 
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,3.1,Weak-Form PDE Residual over Meshfree Representations,"In a vanilla PINN framework, we will use a neural network u θ (x, t) to approximate PDE solutions u(x, t) to the AP model. The network will be optimized by the initial, boundary, and PDE residuals (Loss I , Loss B and Loss R ) over a set of points j=1 sampled in time, space, and boundary domains: ) where λ I , λ B and λ R are the hyperparameters that balance these residuals during training. Because u θ (x, t) lives on the 3D geometry of the ventricles, it is nontrivial to calculate the second-order spatial derivatives. To address this, we utilize the weak form of PDE and spatially discretize it on a bi-ventricular mesh through the Mesh-free method  where vectors U = [u 1 , u 2 , . . . , u NΩ ] T and V = [v 1 , v 2 , . . . , v NΩ ] T consist of u and v from all N Ω mesh-free points inside the myocardium. Matrices M and K represent numerical approximations of the second-order spatial derivatives in Eq. (  ) With this modification, PINN no longer needs to deal directly with the second-order spatial derivatives over the 3D ventricular geometry."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,3.2,Spatial-Temporally Adaptive Training Strategies,"A common challenge in PINN training is the failure to propagate PDE solutions over a large solution domain  Temporally Adaptive Training: We argue that the propagation failure is caused by the inability of vanilla PINNs to respect the causality underlying the physical laws of wave propagation. Therefore, we propose temporal weights {β 1 , β 2 , . . . , β NT } to guide PINNs to respect temporal causality during training. The intuition is that, when the correct solution is propagated to time t c , the previous times {t 1 , t 2 , . . . , t c-1 } should all have lower PDE residuals. We thus propose the temporal weights to be: where t is a threshold to be tuned. This will guide the propagation of correct solutions while avoiding obtaining and propagating trivial intermediate solutions."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,,Spatially Adaptive Training:,"While sharp gradients in PDE solutions are challenging for PINNs  where w h > 1 and s is another threshold to be tuned. With the above strategy, PDE residual is further modified to: Sequential PINN Training: Even when propagation causality and sparse regions of sharp gradients are respected, PINN cannot solve for arbitrarily long time domains because the loss landscape becomes increasingly complex as N T increases. We thus further utilize a sequential learning method where we first uniformly discretize the time domain [0, T ] into n segments, and then train the PINN across these segments sequentially as: where PDE solutions obtained from previous time segments become the initial residual for training the PINN for the next time segment. The complete PDE solution for the entire time domain is obtained at the end of the training."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,3.3,Solving the Forward and Inverse Problems,"In the forward problem, PINNs approximate the solution of the AP model by optimizing the initial residual Loss I and the PDE residual Loss R : In the inverse problem, PINNs utilize partially known solutions to simultaneously optimize the PINN parameters θ and unknown PDE parameters φ with an additional data loss Loss D :"
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,4,Experiments and Results,"In all experiments, the neural network we used has 5 quadratic residual layers  The Effectiveness of Spatial-Temporally Adaptive PINNs: To verify the effectiveness of the spatial-temporally adaptive strategies for PINN training, we solve the forward problem of the AP model on time domain t ∈ [0, 1], t ∈ [0, 5] and t ∈ [0, 10] considering three ablation models: vanilla PINN, PINN + temporal weights, PINN + temporal and spatial weights. Figure "
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,,The Effectiveness of Sequential Training:,"We then compared the computation cost and the accuracy of the PDE solution achieved by the presented PINNs with and without sequence learning, across different lengths of time domains as summarized in Table  PINN for Supporting PDE Parameter Inference: Finally, we tested the feasibility of the presented PINNs to support parameter estimation of the AP model. We assumed measurements of TMP solutions to be available and considered unknown parameter γ in the AP model (Eq. 1) due to its relatively large influence on the PDE solution "
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,5,Conclusion,"We presented a spatial-temporally adaptive PINN framework to solve the forward and inverse problems of cardiac EP over 3D bi-ventricular models, overcoming the current limitations of PINNs in solving PDEs with sharp gradients/transitions over tricky spatial domains and long time domains. However, our PINN framework still has a much larger computational cost than traditional numerical methods, which is a drawback of PINN itself. Future works will pursue the use of this PINN framework for more complex PDEs of bi-ventricular EP, improve its computational efficiency as well as enable inverse EP using surface measurements such as electrocardiograms (ECGs) in real-data settings."
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,,Fig. 1 .,
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,,Fig. 2 .Fig. 3 .,
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,,Fig. 4 .,
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,,Fig. 5 .,
A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference,,Table 1 .,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,1,Introduction,"The thalamus is a critical brain region that relays and modulates information between different parts of the cerebral cortex, and plays a vital role in signal transmission and processing, including pain recognition and reaction  In recent years, landmark-based detection approaches based on deep learning have been employed to measure biometrics from fetal US images. They were used to detect measurement key points for brain structures in the fetal brains, and bony structures such as length of the femur or dimensions in the pelvic floors  However, BiometryNet cannot be directly used to measure FTD and FHC due to two specific difficulties. First being that the ""guitar-shaped"" structure (GsS) by Sridar et al. to measure FTD has similar echogenicity to surrounding brain tissues, resulting in fuzzy boundaries, especially around the wing-tips where measurement landmarks are located  To address the above difficulties, we present a novel Swoosh Activation Function (SAF) designed to enhance the regularization of heatmaps produced by landmark detection algorithms. The SAF takes its name from the Nike TM swoosh logo, which resembles its shape (Supplementary Fig. "
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,2,Method,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,2.1,Swoosh Activation Function,"SAF is introduced to optimize pHs by enforcing an optimum MSE between a pair of pHs and a secondary optimum MSE between a predicted heatmap (pH) and a zero matrix (O). We determine the optimum MSE by computing the MSE between a pair of ground truth heatmaps (gHs). Each ground truth heatmap (gH) represents a measurement landmark by a smaller matrix drawn from a Gaussian distribution that is centered at the landmark coordinates with the peak assigned at the value of 1 (Fig.  In Eq. 1, Min represents a function that ensures SAF minimizes to 0, and it is defined as Min = f 1 ab . The coefficient a determines the slope of SAF around the minimum point in Quadrant 1 of the Cartesian coordinate system (Supplementary Fig. "
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,2.2,SAF Regularization,"The loss function consists of the MSE between pH and gH, and three additional SAF regularization terms. We chose SAF as the activation function to control these regularization terms because SAF's output grows exponentially on either side of the minimum point (Supplementary Fig. "
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,2.3,Datasets,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,,FTD Dataset:,"The dataset used in this study consists of 1111 2D-US images acquired during the second trimester of pregnancies and confirmed by boardcertified ultrasonographers to be suitable for measuring FTD  The intraclass correlation coefficient (ICC) score was computed using IBM TM SPSS TM version 28, with the ICC configuration being Two-Way Random and Absolute Agreement  HC18 Dataset: HC18 dataset is available on the Grand Challenge website "
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,2.4,Experimental Setup,"Training Epochs and Learning Rate. We conducted our experiments using PyTorch version 1.12 on two NVIDIA GTX-1080Ti graphical processing units, each with 11 GB of video memory. For landmark detection training, we used the same learning rate configuration as Avisdris et al.  Pre-processing. Our pre-processing pipeline included random rotation of ±180 • , random re-scaling of ±5%, resizing to 384×384 pixels without preserving aspect ratio, and normalization using ImageNet-derived mean = (0.485, 0.456, 0.406) and standard deviation = (0.229, 0.224, 0.225) for each color channel. SAF Configuration. Given our dataset configuration where each biometry landmark was represented by a 19 × 19 matrix with values derived from a Gaussian distribution with the center point's value peaked at 1, the MSE between a pair of gHs was 0.0061. This configuration followed the standard implementation used in human pose estimation landmark detection  We conditionally activated SAF when the average of MSE(pH 1 , gH 1 ) and MSE(pH 2 , gH 2 ) was less than 0.0009 because SAF is not bounded and early activation would hinder algorithm learning. The proposed SAF algorithm was evaluated using six model configurations, including Vanilla BiometryNet, Biom-etryNet with SAF with coefficient a values of 1, 4, and 8, an EfficientNet, and the EfficientNet with SAF configured with coefficient a value of 4. The model configurations were trained and tested to verify the usefulness of the proposed SAF using both FTD and HC18 datasets."
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,3,Results,"The results of FTD dataset show that BiometryNet with SAF a1 (Biome-tryNet SAF a1) achieved the highest ICC score at 0.737, surpassing the performance of the vanilla BiometryNet, which scored 0.684. Moreover, BiometryNet with SAF a4 (BiometryNet SAF a4) and BiometryNet with SAF a8 (Biome-tryNet SAF a8) also demonstrated superior ICC scores for FTD measurement, albeit to a lesser degree than BiometryNet SAF a1. The impact of SAF on performance was further observed in the modified EfficientNet, where Efficient-Net SAF a4 achieved a higher ICC score of 0.725, compared to the modified EfficientNet without SAF, which only scored 0.688. We also observed that SAF reduced the similarities between a pair of pHs and the dispersiveness of hotspot in the pH. We display such heatmaps produced by BiometryNet in Fig .  For the HC18 dataset, BiometryNet SAF a8 demonstrated the lowest measurement mean difference from the ground truth at 3.86 mm ± 7.74 mm. Additionally, all configurations outperformed the vanilla BiometryNet. The impact on FHC measurement was further observed in the modified EfficientNet, where EfficientNet SAF a4 achieved a lower measurement mean difference at 4.87 mm ± 5.79 mm compared to EfficientNet at 32.76 mm ± 21.01 mm. The FTD dataset ICC scores and HC18 dataset mean differences for each algorithm are presented in Table "
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,4,Discussion,"The main findings of this study are as follows: (1) SAF improved the measurement accuracy of algorithms in both FTD and FHC measurement tasks; (2) SAF regularization is architecture-agnostic, as it improved the measurement accuracy of both BiometryNet and EfficientNet compared to their vanilla forms that do not use SAF; and (3) the optimum configuration of SAF coefficients is taskdependent. For FTD measurement, the most optimum configuration was to use a = 1, while for FHC measurement was with coefficient a = 8. The results demonstrate the performance improvement brought about by SAF regularization, effectively improving the accuracy of landmark detection of fetal biometries in 2D-US images. SAF regularization forces a pair of heatmaps to highlight different areas and reduce the dispersiveness of hotspots in pHs, which results in improved fetal biometry landmark detection accuracy. This is supported by the comparison of heatmaps produced by BiometryNet and BiometryNet SAF a1 displayed in Fig.  As part of our future study, we will explore the effectiveness of SAF regularization in other fetal landmark detection tasks, especially those that suffer from similar issues with fuzzy edges and uncertain landmark locations. Additionally, the optimum configuration of SAF coefficients may vary depending on the specific dataset or imaging modality used."
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,5,Conclusion,"Our study demonstrated the effectiveness of SAF as a novel activation function for regularizing heatmaps generated by fetal biometry landmark detection algorithms, resulting in improved measurement accuracy. SAF outperformed the previous state-of-the-art algorithm, BiometryNet, in both FTD and FHC measurement tasks. Importantly, our results showed that SAF is architecture-agnostic and highly configurable for different tasks through its coefficients, making it a generalizable solution for a wide range of landmark detection problems."
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,,Fig. 2 .,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,,,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,,Fig. 3 .,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,,Fig. 4 .,
Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function,,Table 1 .,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,1,Introduction,"Brain extraction is the first step in fetal Magnetic Resonance Image (MRI) analysis in advanced applications such as brain tissue segmentation  Prevailing methods learning from image-level labels for segmentation commonly produce a coarse localization of the objects based on Class Activation Maps (CAM)  In contrast to one-stage methods, two-stage methods can perform favourably, as they leverage dense labels generated by the classification network to train a segmentation network  In this work, we propose a novel weakly-supervised method for accurate fetal brain segmentation using image-level labels. Our contribution can be summarized as follows: 1) We design an Uncertainty-weighted Multi-resolution CAM  "
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,2,Method,An overview of our method is presented in Fig. 
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,2.1,Psuedo Mask Generation Based on UM-CAM Initial Response via Grad-CAM.,"A typical classification network consists of convolutional layers as a feature extractor, followed by global average pooling and a fully connected layer as the output classifier  where y is classification prediction score for the foreground. i is pixel index, and N is the pixel number in the image."
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,Multi-resolution Exploration and Integration.,"The localization map for each image typically provides discriminative object parts, which is insufficient to provide supervision for the segmentation task. As shown in Fig.  where Âb m and Âf m represent the background and foreground probability of Âm , respectively. w m is the weight map for Âm , and P UM is the UM-CAM for the target."
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,2.2,Robust Segmentation with UM-CAM and SPL,"Though UM-CAM is better than the CAM from the deep layer of the classification network, it is still insufficient to provide accurate object boundaries that are important for segmentation. Motivated by  Concretely, we adopt the centroid and the corner points of the bounding box obtained from UM-CAM as the foreground seeds S f and background seeds S b , respectively. To efficiently leverage these seed points, SPL is generated via Exponential Geodesic Distance (EGD) transform of the seeds, leading to a foreground cue map P f SP L and a background cue map P b SP L . The values of P b SP L and P f SP L represent the similarity between each pixel and background/foreground seed points, which can be computed as: where P i,j is the set of all paths between pixels i and j. D b (i) and D f (i) represent the minimal geodesic distance between target pixel i and background/foreground seed points, respectively. p is one feasible path and it is parameterized by n ∈ [0, 1]. u (n) = p (n) / p (n) is a unit vector that is tangent to the direction of the path. Based on the supervision from UM-CAM and SPL, the segmentation network can be trained by minimizing the following joint object function: where P p is the prediction of the segmentation network, and λ is a weight factor to balance the supervision of UM-CAM and SPL. L CE is the Cross-Entropy (CE) loss."
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,3,Experiments and Results,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,3.1,Experimental Details,"Dataset. We collected clinical T2-weighted MRI data of 115 pregnant women in the second trimester with Single-shot Fast Spin-echo (SSFSE). The data were acquired in axial view with pixel size between 0.5547 mm × 0.5547 mm and 0.6719 mm × 0.6719 mm and slice thickness between 6.50 mm and 7.15 mm. Each slice was resampled to a uniform pixel size of 1 mm × 1 mm. In all experiments, Implementation Details. To boost the generalizability, we applied spatial and intensity-based data augmentation during the training stage, including gamma correction, random rotation, random flipping, and random cropping. For 2D classification, we employed VGG-16 "
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,3.2,Ablation Studies,"Stage1: Quality of Pseudo Masks Obtained by UM-CAM. To evaluate the effectiveness of UM-CAM, we compared different pseudo mask generation strategies: 1) Grad-CAM (baseline): only using CAMs from the last layer of the classification network generated by using Grad-CAM method "
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,Stage2: Training Segmentation Model with UM-CAM and SPL.,"To investigate the effectiveness of SPL, we compared it with several segmentation models: 1) Grad-CAM (baseline): only using the pseudo mask generated from Grad-CAM to train the segmentation model, 2) UM-CAM: only using UM-CAM as supervision for the segmentation model, 3) SPL: only using SPL as supervision, 4) UM-CAM+SPL: our proposed method using UM-CAM and SPL supervision for the segmentation model. Quantitative evaluation results in the second section of Table "
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,3.3,Comparison with State-of-the-art Methods,"We compared three CAM invariants with our UM-CAM in pseudo mask generation stage, including Grad-CAM++ "
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,4,Conclusion,"In this paper, we presented an uncertainty and context-based method for fetal brain segmentation using image-level supervision. An Uncertainty-weighted Multi-resolution CAM (UM-CAM) was proposed to integrate multi-resolution activation maps via uncertainty weighting to generate high-quality pixel-wise supervision. We proposed a Geodesic distance-based Seed Expansion (GSE) method to produce Seed-derived Pseudo Labels (SPL) containing detailed context information. The SPL is combined with UM-CAM for training the segmentation network. The proposed method was evaluated on the fetal brain segmentation task, and experimental results demonstrated the effectiveness of the proposed method and suggested the potential of our proposed method for obtaining accurate fetal brain segmentation with low annotation cost. In the future, it is of interest to validate our method with other segmentation tasks and apply it to other backbone networks."
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,Fig. 1 .,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,(,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,Fig. 2 .,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,Fig. 3 .,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,Table 1 .,
UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation,,Table 2 .,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,1,Introduction,"Alzheimer's disease (AD) is a progressive and debilitating neurological disorder that affects millions of people worldwide. Although primary detection of AD can be achieved through a combination of cognitive function tests and neuroimaging techniques, such as magnetic resonance imaging (MRI) and cerebrospinal fluid (CSF) analysis  Studies on clinical biomarkers of OCTA images are mainly based on regional analysis, e.g., the early treatment of diabetic retinopathy study (ETDRS) grid, which divides a target area into 9 regions with three concentric circles and two orthogonal lines, as shown in the right three sub-figures in Fig.  Over the past few years, deep-learning-based algorithms have achieved remarkable success in the analysis of medical images. As for AD detection, several methods use an integration of multiple modalities "
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,2,Methodology,Figure 
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,2.1,Polar Coordinate Transformation for OCTA Image,"We introduce a method called polar transformation, to realize region-based analysis. As shown in Fig.  The width of the transformed image is equal to the distance R, the minimal length from the center O c (u o , v o ) to the edge in the original image, and the height is 2πR. Since the corners are cropped, the outermost pixels of the region of interest are kept in order to preserve the original information as much as possible, and the part near O c is filled by nearest neighbor interpolation. The polar transformation represents the original image in the polar coordinate system by pixel-wise mapping  1) Approximate Sector-Shaped Convolution. Convolution is widely used in convolutional neural networks (CNNs), where the shape of the convolution kernel is always rectangular. However, in the real world, many semantics are non-rectangular, such as circle and sector, which makes the adaptability of the receptive field in CNNs suboptimal. For the polar transformation, the mapping relationship is fixed, enabling us to approximate the sector convolution with a rectangular convolution kernel at a lower computational cost. The mapping relationship shown in Fig.  2) Equivalent Augmentation. Applying data augmentation to the original image is the same as applying data augmentation in the polar system since the transformation is a pixel-wise mapping "
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,2.2,Network Architecture,"In the transformed image, we can extract features around the FAZ. Rectangular features at different scales correspond to different sectoral features in the retina. Therefore, it is critical to extract information across different sizes of the visual field. To this end, we design the Polar-Net. As shown in Fig.  Each branch starts with a polar feature extractor module (PFEM) and ends with a residual network. To take full advantage of all the branches, middle fusion is used. To generate the region importance matrix, a polar region importance module (PRIM) is proposed, which follows the residual network. Furthermore, Polar-Net can receive a prior knowledge matrix to utilize clinical knowledge."
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Polar Feature Extractor Module (PFEM):,"To extract shallow features in different views, we propose PFEM, which consists of a multi-kernel atrous convolution module (MKAC), a multi-kernel pooling module (MKPM), and a convolutional block attention module (CBAM)  (2)"
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Polar Region Importance Module (PRIM):,"To calculate the region importance, we implement PRIM by applying an average pooling after a Grad-CAM "
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,3,Experiments and Results,"Data Description: An in-house dataset was conducted for this study. It includes 199 images from 114 AD patients and 566 images from 291 healthy subjects. All data were collected with the approval of the relevant authorities and the consent of the patients, following the Declaration of Helsinki. All the patients conform to the standards of the National Institute on Aging and Alzheimer's Association (NIA-AA). The images were captured with a swept-source OCTA (VG200S, SVision Imaging). The images were captured in a 3 × 3 mm 2 area centered on the fovea. We make sure that the images from a single patient will only be used as training or testing sets once. In the cross-validation experiment subset, we sample the categories from each dataset at the same ratio. "
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Model ACC AUROC Kappa,"ResNet-34  The width of the transformed images was resized to 224 pixels. Five-fold crossvalidation was employed to fully utilize the data and make the results more reliable. Since there is no standard way to convert existing prior knowledge into matrices, for prior knowledge, we manually generated a 4 × 2 weight matrix according to the study "
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Evaluation and Interpretability Assessment:,"We evaluate the performance of the model on the test set using the accuracy score (ACC), area under the receiver operating characteristic (AUROC), and kappa. To evaluate the performance, we compared our method with several state-of-the-art methods in the computer vision field and one in the AD detection field. Table  For the sake of simplicity, here we take the EDTRS grid for analysis. As shown in Fig.  Ablation Study: To evaluate the effectiveness of the polar transformation and Polar-Net, we performed an ablation study. To validate the proposed Polar-Net, we removed the PFEM. The results are shown at the bottom of Table  Extended Experiment: To further verify our detection method's stability and generalisability, we conducted an additional experiment on a public dataset OCTA-500 "
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,4,Conclusion,"In this paper, we propose a novel framework for AD detection using retinal OCTA images, leveraging clinical prior knowledge and providing interpretable results. Our approach involves polar transformation, allowing for the use of approximate sector convolution and enabling the implementation of the regionbased analysis. Additionally, our framework, called Polar-Net, is designed to acquire the importance of the corresponding retinal region, facilitating the understanding of the model's decision-making process in detecting AD and assessing its conformity to clinical observations. We evaluate the performance of our method on both private and public datasets, and the results demonstrate that Polar-Net outperforms state-of-the-art methods. Importantly, our approach produces clinically interpretable results, providing a potential tool for disease research to investigate the underlying pathological mechanisms. Our work presents a promising approach to using OCTA imaging for AD detection. Furthermore, we highlight the importance of incorporating clinical knowledge into AI models to improve interpretability and clinical applicability."
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Fig. 1 .,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Fig. 2 .,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Fig. 3 .,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Fig. 4 .,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Table 1 .,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Table 2 .,
Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images,,Table 3 .,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,1,Introduction,"Breast cancer is the most common cancer and the leading cause of cancer death in women  With the development of computer technology, artificial intelligence-based methods have shown potential in image generation and have received extensive attention. Some studies have shown that some generative models can effectively perform mutual synthesis between MR, CT, and PET  Diffusion-weighted imaging (DWI) is emerging as a key imaging technique to complement breast CE-MRI  i From the perspective of method, we innovatively proposed a multi-sequence fusion model, designed for combining T1-weighted imaging and multi-b-value DWI to synthesize CE-MRI for the first time. ii We invented hierarchical fusion module, weighted difference module and multi-sequence attention module to enhance the fusion at different scale, to control the contribution of different sequence and maximising the usage of the information within and across sequences. iii From the perspective of clinical application, our proposed model can be used to synthesize CE-MRI, which is expected to reduce the use of GBCA."
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2,Methods,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.1,Patient Collection and Pre-processing,"This study was approved by Institutional Review Board of our cancer institute with a waiver of informed consent. We retrospectively collected 765 patients with breast cancer presenting at our cancer institute from January 2015 to November 2020, all patients had biopsy-proven breast cancers (all cancers included in this study were invasive breast cancers, and ductal carcinoma in situ had been excluded). The MRIs were acquired with Philips Ingenia All MRIs were resampled to 1 mm isotropic voxels and uniformly sized, resulting in volumes of 352 × 352 pixel images with 176 slices per MRI, and subsequent registration was performed based on Advanced Normalization Tools (ANTs) "
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.2,Model,Figure  Figure 
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,ADC =ln (S,"where S l and S h represent the image signals obtained from lower b value b l and higher b h , f θ l and f θ h represent the corresponding neural networks for DWI with a lower and higher b value. In the multi-sequence attention module, a channel-based attention mechanism is designed to automatically apply weights (A s ) to feature maps (F concat ) from different sequences to obtain a refined feature map (F concat ), as shown in Eq. 3. The input feature maps (F concat ) go through the maximum pooling layer and the average pooling layer respectively, and then are added element-wise after passing through the shared fully connected neural network, and finally the weight map A s is generated after passing through the activation function, as shown in Eq. 4. where ⊗ represents element-wise multiplication, ⊕ represents element-wise summation, σ represents the sigmoid function, θ fc represents the corresponding network parameters of the shared fully-connected neural network, and AvgP ool and M axP ool represent average pooling and maximum pooling operations, respectively. In the synthesis process, the generator G tries to generate an image according to the input multi-sequence MRI (d 1 , d 2 , d 3 , d 4 , t 1 ), and the discriminator D tries to distinguish the generated image G(d 1 , d 2 , d 3 , d 4 , t 1 ) from the real image y, and at the same time, the generator tries to generate a realistic image to mislead the discriminator. The generator's objective function is as follows: and the discriminator's objective function is as follows: where pro data (d 1 , d 2 , d 3 , d 4 , t 1 ) represents the empirical joint distribution of inputs d 1 (DW I b0 ), d 2 (DW I b150 ), d 3 (DW I b800 ), d 4 (DW I b1500 ) and t 1 (T1weighted MRI), λ 1 is a non-negative trade-off parameter, and l 1 -norm is used to measure the difference between the generated image and the corresponding ground truth. The architecture of the discriminator includes five convolutional layers, and in each convolutional layer, 3 × 3 filters with stride 2 are used. Each filter is followed by batch normalization, and after batch normalization, the activation function LeakyReLU (with a slope of 0.2) is used. The numbers of filters are 32, 64, 128, 256 and 512, respectively."
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.3,Visualization,"The T1-weighted images and the contrast-enhanced images were subtracted to obtain a difference MRI to clearly reveal the enhanced regions in the CE-MRI. If the CE-MRI was successfully synthesized, the enhanced region would be highlighted in the difference MRI, otherwise it would not. "
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.4,Experiment Settings,"Based on the ratio of 8:2, the training set and independent test set of the in-house dataset have 612 and 153 cases, respectively. The trade-off parameter λ 1 was set to 100 during training, and the trade-off parameter of the reconstruction loss in the reconstruction module is set to 5. Masks for all breasts were used (weighted by a factor of 100 during the calculation of the loss between generated and real CE-MRI) to reduce the influence of signals in the thoracic area. The batch was set to 8 for 100 epochs, the initial learning rate was 1e-3 with a decay factor of 0.8 every 5 epochs (total run time is about 60 h). Adam optimizer was applied to update the model parameters. MMgSN-Net "
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.5,Evaluation Metrics,"Results analysis was performed by Python 3.7. Structural Similarity Index Measurement (SSIM), Peak Signal-to-Noise Ratio (PSNR) and Normalized Mean Squared Error (NMSE) were used as metrics, all formulas as follows: P SNR = 10 log 10 max 2 (y(x), G(x)) where G(x) represents a generated image, y(x) represents a ground-truth image, μ y(x) and μ G(x) represent the mean of y(x) and G(x), respectively, σ y(x) and σ G(x) represent the variance of y(x) and G(x), respectively, σ y(x)G(x) represents the covariance of y(x) and G(x), and c 1 and c 2 represent positive constants used to avoid null denominators."
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,3,Results,"First, we compare the performance of different existing methods on synthetic CE-MRI using our source data, The quantitative indicators used include PSNR, SSIM and NMSE. As shown in Table  MMgSN-Net "
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,4,Conclusion,"We have developed a multi-sequence fusion network based on multi-b-value DWI to synthesize CE-MRI, using source data including DWIs and T1-weighted fatsuppressed MRI. Compared to existing methods, we avoid the challenges of using full-sequence MRI and aim to be selective on valuable source data DWI. Hierarchical fusion generation module, weighted difference module, and multisequence attention module have all been shown to improve the performance of synthesizing target images by addressing the problems of synthesis at different scales, leveraging differentiable information within and across sequences. Given that current research on synthetic CE-MRI is relatively sparse and challenging, our study provides a novel approach that may be instructive for future research based on DWIs. Our further work will be to conduct reader studies to verify the clinical value of our research in downstream applications, such as helping radiologists on detecting tumors. In addition, synthesizing dynamic contrastenhanced MRI at multiple time points will also be our future research direction. Our proposed model can potentially be used to synthesize CE-MRI, which is expected to reduce or avoid the use of GBCA, thereby optimizing logistics and minimizing potential risks to patients."
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,Fig. 1 .,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,Fig. 2 .,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,Fig. 3 .,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,Table 1 .,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,Table 2 .,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,.93 ± 2.91 28.92 ± 1.63 0.0585 ± 0.026,
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 8.
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,1,Introduction,"Fetal magnetic resonance imaging (MRI) is becoming increasingly common, supplementing ultrasound for clinical decision-making and planning. It has a wide range of functional contrasts, a higher resolution than ultrasound, and can be used from approximately 16 weeks of gestation until birth. The renewed interest in low-field MRI (0.55T) and the increasing availability of commercial low field scanners carries significant advantages for fetal MRI: Low-field MRIs allow a wider bore (widening access to this tool for the increasingly obese pregnant population) while maintaining field homogeneity at the lower field strength, and generally do not require helium for cooling. Low field MRI is especially advantageous for fetal functional imaging (often performed using Echo-Planar-Imaging) as the reduced susceptibility-induced artefacts and longer T2* times allow for longer read-outs and hence more efficient acquisitions. It therefore provides an excellent environment for fetal body T2* mapping  T2* maps of the fetus provide an indirect measurement of blood oxygenation levels due to the differing relaxation times of deoxygenated and oxygenated hemoglobin  However, the wider use of T2* relaxometry in the clinical setting is currently limited by significant methodological barriers such as quality of the data, fetal motion and time-consuming manual segmentations. As a consequence, it has mainly been used in research settings focused on the brain and placenta  Here, we present an automated pipeline for quantitative mean T2* fetal body organs at low field MRI, resulting in normative growth curves from 17-40 weeks. We use a low-resolution dynamic T2* acquisition framework, and then use a novel multi-channel deformable slice-to-volume reconstruction (dSVR) to generate a high-resolution 3D volume of the fetal body and its corresponding T2* map in the standard plane "
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,2,Methods,"Fetal MRI was acquired as part of an ethically approved study (MEERKAT, REC 21/LO/0742, Dulwich Ethics Committee, 08/12/2021) performed between May 2022 and February 2023 at St Thomas' Hospital in London, UK. Participants for this study were recruited prospectively, with inclusion criteria of a singleton pregnancy, maternal age over 18 years. Exclusion criteria were multiple pregnancies, maternal age <18 years, lack of ability to consent, weight >200 kg, and contraindications for MRI such as metal implants."
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,2.1,Image Acquisition,"The subjects were scanned on a clinical 0.55T scanner (MAGNETOM Free.Max, Siemens Healthcare, Germany) using a 6-element blanket coil and a 9-element spine coil in the supine position. A multi-echo whole-uterus dynamic (timeresolved) gradient echo sequence was acquired in the maternal coronal orientation. The sequence parameters are as follows: Field of View (FOV): 400 × 400 mm; resolution: 3.125 mm × 3.125 mm × 3 mm; TE: "
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,2.2,Image Processing,"The acquired images were reviewed to remove dynamics with excess motion. The remaining images were first denoised using MRTRIX3's dwidenoise tool  Reconstructions. The image generated from the second echo was determined to have the best contract for the fetal body organs (an example of the echos can be seen in the Supplementary Material). This second echo image for each dynamic was then used to create a high-resolution 3D volume with isotropic resolution in a standard atlas space using deformable slice-to-volume (dSVR) registration, with the following non-default parameters: no intensity matching, no robust statistics, resolution = 1.2 mm, cp = [12 5], lastIter=0.015  Fetal Lung Reconstruction Validation. In order to validate that the 3D volume reconstructions do not alter the obtained mean T2* values, mean T2* values from 10 of the acquired dynamics were compared with mean lung T2* value of the corresponding 3D volume. The values were considered to be equal if the mean T2* determined from the 3D volume was within two standard deviations of the mean T2* values sampled from the 10 dynamic scans. Segmentation. Initial segmentations of the second echo 3D volumes were generated with an existing in-house U-Net using the MONAI framework  A 3D nnUNet  Growth Curves. Organ-specific volume and T2* growth curves were created using control cases from the generated label maps and the high-resolution T2* volumes, excluding voxels where the T2* fitting failed. A linear regression analysis was performed in order to determine the relationship between the T2* values and GA. The complete fetal body T2* 3D reconstruction and segmentation processing pipeline can be seen in Fig. "
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,3.1,Fetal Body T2* Reconstruction,"41 subjects had a multi-echo T2* dynamic scan (mean GA: 28.46 ± 6.78 weeks; mean body mass index: 29.0 ± 6.26). Dynamics with motion artifact were removed (mean included: 13.6). All cases underwent 3D volume reconstruction. Both control and pathological cases were used during the iterative training process for the segmentation network. Nine cases were excluded from the creation of growth curves due to a pathology impacting the fetal body. A further two cases were excluded after reconstruction due to excessive motion. Fetal Lung Reconstruction Validation. In the lung reconstruction validation experiment, all cases fell within the required range, and therefore are considered to be equal (Table "
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,3.2,Fetal Body T2* Segmentation,The DSC scores of the one-and two-channel networks (Table 
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,3.3,Growth Curves,"The obtained T2* growth curves (Fig.  Table  insight into the development of fetal body organs not yet explored, such as the adrenal glands. A more comprehensive study with more cases at every GA is needed to further validate these normative curves. While structural T2-weighted images are more suited for volumetry, the fact that all organs follow the expected growths trend further validates the segmentations. It also indicates that for some of the smaller organs (kidney pelvis, adrenal glands), our network has difficulties in the segmentation step for younger fetuses. Two organs (thymus, adrenal glands) had poor DSC values (below 0.45), indicating that further work on the segmentation network for these difficult organs is required. The thymus has very poor contrast and is difficult to delineate even manually. The network often identified heart tissue as thymus, skewing the T2* values calculated. The adrenal gland is a very small organ, which makes it difficult to segment. The poor DSC for the adrenal gland does not necessarily translate to incorrect average T2* values, as the DSC is a volumetric metric. However, improved DSC scores would allow for more confidence in the calculated T2* values. The proposed multi-organ pipeline can be successfully run across a wide range of GAs, and requires minimal user interaction. The combination of low field fetal MRI, quantitative imaging, and comprehensive image analysis pipelines could potentially make a substantial impact in our understanding of the development of the fetal body throughout gestation, as well as possibly provide clinical prenatal biomarkers."
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,,Fig. 1 .,
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,,Fig. 2 .,
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,,Fig. 3 .,
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,,Fig. 4 .,
An Automated Pipeline for Quantitative T2* Fetal Body MRI and Segmentation at Low Field,,Table 2 .,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,1,Introduction,"Nasopharyngeal carcinoma (NPC), also known as lymphoepithelioma, is a highly aggressive malignancy that originates in nasopharynx  In recent years, artificial intelligence (AI), especially deep learning, plays a gamechanging role in medical imaging  The clinical evaluation of AI-based techniques is of paramount importance in healthcare. Rigorous clinical evaluations can establish the safety and efficacy of AI-based techniques, identify potential biases and limitations, and facilitate the integration of clinical expertise to ensure accurate and meaningful results  To bridge this bench-to-bedside research gap, in this study, we conducted a series of clinical evaluations to assess the effectiveness of synthetic VCE-MRI in NPC delineation, with a particular focus on assessment in VCE-MRI image quality and primary GTV delineation. This study has two main novelties: (i) To the best of our knowledge, this is the first clinical evaluation study of the VCE-MRI technique in RT; and (ii) multiinstitutional MRI data were included in this study to obtain more reliable results. The success of this study would fill the current knowledge gap and provide the medical community with a clinical reference prior to clinical application of the novel VCE-MRI technique in NPC RT."
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,2,Materials and Methods,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,2.1,Data Description,"Patient data was retrospectively collected from three oncology centers in Hong Kong. This dataset included 303 biopsy-proven (stage I-IVb) NPC patients who received radiation treatment during 2012-2016. The three hospitals were labelled as Institution-1 (110 patients), Institution-2 (58 patients), and Institution-3 (135 patients), respectively. For each patient, T1w MRI, T2w MRI, gadolinium-based CE-MRI, and planning CT were retrieved. MRI images were automatically registered as MRI images for each patient were scanned in the same position. The use of this dataset was approved by the Institutional Review Board of the University of Hong Kong/Hospital Authority Hong Kong West Cluster (HKU/HA HKW IRB) with reference number UW21-412, and the Research Ethics Committee (Kowloon Central/Kowloon East) with reference number KC/KE-18-0085/ER-1. Due to the retrospective nature of this study, patient consent was waived. For model development, 288 patients were used for model development and 15 patients were used to synthesize VCE-MRI for clinical evaluation. The details of patient characteristics and the number split for training and testing of each dataset were illustrated in Table "
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,2.2,VCE-MRI Synthesis Network,The multimodality-guided synergistic neural network (MMgSN-Net) was applied to learn the mapping from T1w MRI and T2w MRI to CE-MRI. The MMgSN-Net was a 2D network. The effectiveness of this network in VCE-MRI synthesis for NPC patients has been demonstrated by Li et al. in 
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,2.3,Clinical Evaluations,"The evaluation methods used in this study included image quality assessment of VCE-MRI and primary GTV delineation. Two board-certified radiation oncologists (with 8 years' and 6 years' clinical experience, respectively) were invited to perform the VCE-MRI quality assessment and GTV delineation according to their clinical experience. Considering the clinical burden of oncologists, 15 patients were included for clinical evaluations. All clinical evaluations were performed on an Eclipse workstation (V5.0.10411.00, Varian Medical Systems, USA) with the same monitor, and the window/level can be adjusted freely by the oncologists. The results were obtained under the consensus of the two oncologists. (i) Distinguishability between CE-MRI and VCE-MRI. To evaluate the reality of VCE-MRI, oncologists were invited to differentiate the synthetic patients (i.e., image volumes that generated from synthetic VCE-MRI) from real patients (i.e., image volumes that generated from real CE-MRI). Different from the previous studies that utilized limited number (20-50 slices, axial view) of 2D image slices for reality evaluation "
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Image,"(ii) Clarity of tumor-to-normal tissue interface. The clarity of tumor-normal tissue interface is critical for tumor delineation, which directly affects the delineation outcomes. Oncologists were asked to use a 5-point Likert scale ranging from 1 (poor) to 5 (excellent) to evaluate the clarity of tumor-to-normal tissue interface. Paired two-tailed t-test (with a significance level of p = 0.05) was applied to analyses if the scores obtained from real patients and synthetic patients are significantly different. (iii) Veracity of contrast enhancement in tumor invasion risk areas. In addition to the critical tumor-normal tissue interface, the areas surrounding the NPC tumor will also be considered during delineation. To better evaluate the veracity of contrast enhancement in VCE-MRI, we selected 25 tumor invasion risk areas according to  The Jaccard index (JI)  where R CE and R VCE represents the set of risk areas that recorded from CE-MRI and corresponding VCE-MRI, respectively. JI measures similarity of two datasets, which ranges from 0% to 100%. Higher JI indicates more similar of the two sets. (iv) Efficacy in primary tumor staging. A critical RT-related application of CE-MRI is tumor staging, which plays a critical role in treatment planning and prognosis prediction  Primary GTV Delineation. GTV delineation is the foremost prerequisite for a successful RT treatment of NPC tumor, which demands excellent precision  To mimic the real clinical setting, contrast-free T1w, T2w MRI and corresponding CT of each patient were imported into the Eclipse system since sometimes T1w and T2w MRI will also be referenced during tumor delineation. Due to both real patients and synthetic patients were involved in delineation, to erase the delineation memory of the same patient, we separated the patients to two datasets, each with the same number of patients, both two datasets with mixed real patients and synthetic patients without overlaps (i.e., the CE-MRI and VCE-MRI from the same patient are not in the same dataset).When finished the first dataset delineation, there was a one-month interval before the delineation of the second dataset. After the delineation of all patients, the Dice similarity coefficient (DSC)  Dice Similarity Coefficient (DSC). DSC is a broadly used metric to compare the agreement between two segmentations  where C CE and C VCE represent the contours delineated from real patients and synthetic patients, respectively."
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Hausdorff Distance (HD).,"Even though DSC is a well-accepted segmentation comparison metric, it is easily influenced by the size of contours. Small contours typically receive lower DSC than larger contours  where d(x, C VCE ) and d(y, C CE ) represent the distance from point x in contour C CE to contour C VCE and the distance from point y in contour C VCE to contour C CE . (i) Distinguishability between CE-MRI and VCE-MRI. The overall judgement accuracy for the MRI volumes was 53.33%, which is close to a random guess accuracy (i.e., 50%). For Institution-1, 2 real patients were judged as synthetic and 1 synthetic patient was considered as real. For Institution-2, 2 real patients were determined as synthetic and 4 synthetic patients were determined as real. For Institution-3, 2 real patients were judged as synthetic and 3 synthetic patients were considered as real. In total, 6 real patients were judged as synthetic and 8 synthetic patients were judged as real. (ii) Clarity of tumor-to-normal tissue interface. The overall clarity scores of tumorto-normal tissue interface for real and synthetic patients were 3.67 with a median of 4 and 3.47 with a median of 4, respectively. No significant difference was observed between these two scores (p = 0.38). The average scores for real and synthetic patients were 3.6 and 3, 3.6 and 3.8, 3.8 and 3.6 for Institution-1, Institution-2, and Institution-3, respectively. 5 real patients got a higher score than synthetic patients and 3 synthetic patients obtained a higher score than real patients. The scores of the other 7 patient pairs were the same. (iii) Veracity of contrast enhancement in tumor invasion risk areas. The overall JI score between the recorded tumor invasion risk areas from CE-MRI and VCE-MRI was 74.06%. The average JI obtained from Institution-1, Institution-2, and Institution-3 dataset were similar with a result of 71.54%, 74.78% and 75.85%, respectively. In total, 126 risk areas were recorded from the CE-MRI for all of the evaluation patients, while 10 (7.94%) false positive high risk invasion areas and 9 (7.14%) false negative high risk invasion areas were recorded from VCE-MRI. (iv) Efficacy in primary tumor staging. A T-staging accuracy of 86.67% was obtained using VCE-MRI. 13 patient pairs obtained the same staging results. For the Institution-2 data, all synthetic patients observed the same stages as real patients."
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,3,Results and Discussion,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,3.1,Image Quality of VCE-MRI,"For the two T-stage disagreement patients, one synthetic patient was staged as phase IV while the corresponding real patient was staged as phase III, the other synthetic patient was staged as I while corresponding real patient was staged as phase III.   "
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,3.2,Primary GTV delineation,"The average DSC and HD between the C CE and C VCE was 0.762 (0.673-0.859) with a median of 0.774, and 1.932 mm (0.763 mm-2.974 mm) with a median of 1.913 mm, respectively. For Institution-1, Institution-2, and Institution-3, the average DSC were 0.741, 0.794 and 0.751 respectively, while the average HD were 2.303 mm, 1.456 mm, and 2.037 mm respectively. Figure "
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,4,Conclusion,"In this study, we conducted a series of clinical evaluations to validate the clinical efficacy of VCE-MRI in RT of NPC patients. Results showed the VCE-MRI has great potential to provide an alternative to GBCA-based CE-MRI for NPC delineation."
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Figure 1,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Fig. 1 .,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Fig. 2 .,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Table 1 .,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Table 2,
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,,Table 2 .,
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,1,Introduction,"Fluorescein Angiography (FA) is a commonly utilized imaging modality for detecting and diagnosing fundus diseases. It is widely used to image vascular structures and dynamically observe the circulation and leakage of contrast agents in blood vessels. Recently, the emergence of Ultra-Wide-angle Fundus (UWF) imaging has enabled its combination with FA and Scanning Laser Ophthalmoscopy (SLO), namely UWF-FA and UWF-SLO. The UWF-FA imaging enables simultaneous and high-contrast angiographic images of all 360 • C of the mid and peripheral retina  Cross-modality medical image generation provides a new method for solving the aforementioned problems. Multi-scale feature maps from different input modalities usually have similar structures. Hence, different contrasts can be merged to generate target images based on multimodal deep learning to provide more information for diagnosis  In this paper, we present the Ultra-Wide-Angle Transformation GAN (UWAT-GAN), a supervised conditional GAN capable of generating UWF-FA from UWF-SLO. To address the image misalignment issue, we employ an automated image registration method and integrate the idea of pix2pixHD  1). To the best of our knowledge, we present the first study to synthesize UWF-FA from UWF-SLO, overcoming the limitations of UWF-FA imaging. 2). We propose a novel UWAT-GAN utilizing multi-scale generators and multiple new weighted losses on different data scales to synthesize high-resolution images with the ability to capture tiny vascular lesion areas. 3). We assess the performance of the UWAT-GAN on a clinical in-house dataset and adopt an effective preprocessing method for image sharpening and registration to enhance the clarity of vascular regions and tackle the misalignment problem. 4). We demonstrate the superiority of the proposed UWAT-GAN against the state-of-the-art models through extensive experiments, comparisons, and ablation studies."
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,2,Methodology,"We propose a supervised conditional GAN for synthesizing UWF-FA from UWF-SLO images, as illustrated in Fig. "
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,,2.1) and a fusion module works on result of both level generators (Sect. 2.2).,"Then the Attention Transmit Module is put forward to improve the U-Net-like architecture (Sect. 2.3). Additionally, we provide a comprehensive description of up-down sampling process and architecture of multi-scaled discriminators (Sec. 2.4). Eventually, we discuss the proposed loss function terms and their impacts in detail (Sect. 2.5). "
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,2.1,Overall Architecture,"UWF-FA has global features such as eyeballs and long-thick blood vessels, as well as local features such as small lesions and capillary blood vessels. However, generating images with both global and local features using a single generator is challenging. To address this issue, we devise two different levels of generators. The coarse generator Gen C extracts global information and generates a result based on this information, while the fine generator Gen F extracts local information. The results of global and local information can be used, alternately, as a reference for each other. Hence, this allows the extraction and utilization of both global and local information. In Fig. "
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,2.2,Patch and Fusion Module,In Fig. 
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,2.3,Attention Transmit Module,In Fig. 
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,2.4,Generator and Discriminator Architectures,"After conducting multiple experiments, we choose three down-sample layers for Gen F and two down-sample layers for Gen C . In addition, each generator includes an initial block, down-sample block, up-sample block, residual block, and attention transmit module, which are shown in Fig. "
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,2.5,Proposed Loss Function,"Denote the two generators Gen C and Gen F as G C and G F , the three discriminators as D C1 , D C2 , and D F , and the paired variables {(c i , x i )}, where c represents the distribution of original input as a condition and x represents the distribution of ground truth (i.e., real UWF-FA image). Given the conditional distribution c, we aim to maximize the loss of D C1 , D C2 , and D F while minimizing the loss of Gen C and Gen F using the following objective function: where L cGAN is given by: We adopt the feature mapping (FM) loss  where T is the total number of layers and N i represents each layer's number of elements. (e.g., convolution, normalization, Leaky-Relu means three elements). Minimizing this loss ensures that each layer can extract the same features from the paired images. Additionally, we use the perceptual loss  where N represents the total number of layers, M i denotes the elements in each layer, and V i is the ith-layer of the VGG19 network. The final cost function is as follows: where λ F MC , λ V GGC , λ F MF , λ V GGF indicate adjustable weight parameters. 3 Experiments and Results"
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,3.1,Data Preparation and Preprocess,"In our experiments, we utilized an in-house dataset of UWF images obtained from a local hospital, comprising UWF-FA and UWF-SLO images. The UWF-SLO are in 3-channel RGB format, whereas the UWF-FA images are in 1-channel format. Each image pair was collected from a unique patient. However, from a clinical perspective, images taken with an interval of more than one day or those with noticeable fresh bleeding were excluded. Additionally, images that contain numerous interfering factors affecting their quality were also discarded. After the quality check, we have 70 paired images with the size of 3900 × 3072, of which 70% were randomly allocated for training and 30% for testing, respectively. Furthermore, we employed image sharpening through histogram equalization to enhance the clarity of images. We then utilized automated image registration software, i2k Retina Pro, to register each pair of images which changed the image size. To standardize the size of each image, we resized the registered images to 2432 × 3702. Subsequently, we randomly cropped the resized images with a size of 608 × 768 into different patches. And 50 patches could be obtained for each image. Finally, we adopted data augmentation using random flip and rotation to increase the number of training images from 49 pairs to 1960 pairs."
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,3.2,Implementation Details,All our experiments were conducted on the PyTorch 1.12 framework and carried out on two Nvidia RTX 3090Ti GPUs. Our model was trained from scratch to 200 epochs. The parameters were optimized by the Adam optimizer algorithm 
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,3.3,Comparisons,We first compared the performance of our model with some state-of-the-art GAN-based models including: Pix2pix 
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,3.4,Ablation Study,"To evaluate the significance of the attention transmit module proposed in UWAT-GAN, we trained our model with and without this module, namely M A and M NA , respectively. Unlike the generated images of M A , we found that M NA was not so distinctive as some vessels were missing and some interference of eyelashes was incorrectly considered as vessels. In Fig. "
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,4,Discussion and Conclusion,"To address the potential adverse effects of fluorescein injection during FA, we propose UWAT-GAN to synthesize UWF-FA from UWF-SLO. Our method can generate high-resolution images and enhance the ability to capture small vascular lesions. Comparison and ablation study on an in-house dataset demonstrate the superiority and effectiveness of our proposed method. However, our model still has a few limitations. First, not every pair of images can be registered since some paired images may have fewer available features, making registration difficult. Second, our model's accuracy in synthesizing very tiny lesions is not optimal, as some lesions cannot be well generalized. Third, the limited size of our dataset is relatively small and may affect the model performance. In the future, we aim to expand the size of our dataset and explore the use of the object detection model, especially for small targets, to push our model pay more attention to some lesions. After further validation, we aim to adopt this method as an auxiliary tool to diagnose and detect fundus diseases."
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,,Fig. 1 .,
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,,Fig. 2 .,
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,,Fig. 4 .,
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,,Table 1 .,
UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN,,Clinical Applications -Vascular,
Automated CT Lung Cancer Screening Workflow Using 3D Camera,1,Introduction,"Lung cancer is the leading cause of cancer death in the United States, and early detection is key to improving survival rates. CT lung cancer screening is a lowdose CT (LDCT) scan of the chest that can detect lung cancer at an early stage, when it is most treatable. However, the current workflow for performing CT lung scans still requires an experienced technician to manually perform pre-scanning steps, which greatly decreases the throughput of this high volume procedure. While recent advances in human body modeling  Since LDCT scans are obtained in a single breath-hold and do not require any contrast medium to be injected, the scout scan consumes a significant portion of the scanning workflow time. It is further increased by the fact that tube rotation has to be adjusted between the scout and actual CT scan. Furthermore, any patient movement during the time between the two scans may cause misalignment and incorrect dose profile, which could ultimately result in a repeat of the entire process. Finally, while minimal, the radiation dose administered to the patient is further increased by a scout scan. We introduce a novel method for estimating patient scanning parameters from non-ionizing 3D camera images to eliminate the need for scout scans during pre-scanning. For LDCT lung cancer screening, our framework automatically estimates the patient's lung position (which serves as a reference point to start the scan), the patient's isocenter (which is used to determine the table height for scanning), and an estimate of patient's Water Equivalent Diameter (WED) profiles along the craniocaudal direction which is a well established method for defining Size Specific Dose Estimate (SSDE) in CT imaging  -A novel workflow for automated CT Lung Cancer Screening without the need for scout scan -A clinically relevant method meeting IEC 62985:2019 requirements on WED estimation. -A generative model of patient WED trained on over 60, 000 patients. -A novel method for real-time refinement of WED, which can be used for dose modulation"
Automated CT Lung Cancer Screening Workflow Using 3D Camera,2,Method,"Water Equivalent Diameter (WED) is a robust patient-size descriptor  Our method jointly predicts the AP and lateral WED profiles. While WED can be derived from CT images, paired CT scans and camera images are rarely available, making direct regression through supervised learning challenging. We propose a semi-supervised approach to estimate WED from depth images. First, we train a WED generative model on a large collection of CT scans. We then train an encoder network to map the patient depth image to the WED manifold. Finally, we propose a novel method to refine the prediction using real-time scan data."
Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.1,WED Latent Space Training,"We use an AutoDecoder  During training, we initialize our latent space to a unit Gaussian distribution as we want it to be compact and continuous. We then randomly sample points along the craniocaudal axis and minimize the L1 loss between the prediction and the ground truth WED. We also apply L2-regularization on the latent vector as part of the optimization process."
Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.2,Depth Encoder Training,"After training our generative model on a large collection of unpaired CT scans, we train our encoder network on a smaller collection of paired depth images and CT scans. We represent our encoder as a DenseNet "
Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.3,Real-Time WED Refinement,"While the depth image provides critical information on the patient anatomy, it may not always be sufficient to accurately predict the WED profiles. For example, some patients may have implants or other medical devices that cannot be guessed solely from the depth image. Additionally, since the encoder is trained on a smaller data collection, it may not be able to perfectly project the depth image to the WED manifold. To meet the strict safety criteria defined by the IEC, we propose to dynamically update the predicted WED profiles at inference time using real-time scan data. First, we use our encoder network to initialize the latent vector to a point in the manifold that is close to the current patient. Then, we use our AutoDecoder to generate initial WED profiles. As the table moves and the patient gets scanned, CT data is being acquired and ground truth WED can be computed for portion of the body that has been scanned, along with the corresponding craniocaudal coordinate. We can then use this data to optimize the latent vector by freezing the AutoDecoder and minimizing the L1 loss between the predicted and ground truth WED profiles through gradient descent. We can then feed the updated latent vector to our AutoDecoder to estimate the WED for the remaining portions of the body that have not yet been scanned and repeat the process. In addition to improving the accuracy of the WED profiles prediction, this approach can also help detect deviation from real data. After the latent vector has been optimized to fit the previously scanned data, a large deviation between the optimized prediction and the ground truth profiles may indicate that our approach is not able to find a point in the manifold that is close to the data. In this case, we may abort the scan, which further reduces safety risks. Overall flowchart of the proposed approach is shown in Fig. "
Automated CT Lung Cancer Screening Workflow Using 3D Camera,3,Results,
Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.1,Data,"Our CT scan dataset consists of 62, 420 patients from 16 different sites across North America, Asia and Europe. Our 3D Camera dataset consists of 2, 742 pairs of depth image and CT scan from 2, 742 patients from 6 different sites across North America and Europe acquired using a ceiling-mounted Kinect 2 camera. Our evaluation set consists of 110 pairs of depth image and CT scan from 110 patients from a separate site in Europe."
Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.2,Patient Preparation,Patient positioning is the first step in lung cancer screening workflow. We first need to estimate the table position and the starting point of the scan. We propose to estimate the table position by regressing the patient isocenter and the starting point of the scan by estimating the location of the patient's lung top. Starting Position. We define the starting position of the scan as the location of the patient's lung top. We trained a DenseUNet  Isocenter. The patient isocenter is defined as the centerline of the patient's body. We trained a DenseNet 
Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.3,Water Equivalent Diameter,"We trained our AutoDecoder model on our unpaired CT scan dataset of 62, 420 patients with a latent vector of size 32. The encoder was trained on our paired CT scan and depth image dataset of 2, 742 patients. We first compared our method against a simple direct regression model. We trained a DenseUNet "
Automated CT Lung Cancer Screening Workflow Using 3D Camera,,Method (lateral),"Mean then measured the performance of our model before and after different degrees of real-time refinement, using the same optimizer and learning rate. We report the comparative results in Table  We observed that our method largely outperforms the direct regression baseline with a mean lateral error 40% lower and a 90 th percentile lateral error over 30% lower. Bringing in real-time refinement greatly improves the results with a mean lateral error over 40% and a 90 th percentile lateral error over 20% lower than before refinement. AP profiles show similar results with a mean AP error improvement of nearly 40% and a 90 th percentile AP error improvement close to 30%. When using our proposed method with a 20 mm window refinement, our proposed approach outperforms the direct regression baseline by over 60% for lateral profile and nearly 80% for AP. Figures 3 highlights the benefits of using real-time refinement. Overall, our approach shows best results with an update frequency of 20 mm, with a mean lateral error of 15.93 mm and a mean AP error of 10.40 mm. Figure  Finally, we evaluated the clinical relevancy of our approach by computing the relative error as described in the International Electrotechnical Commission (IEC) standard IEC 62985:2019 on Methods for calculating size specific dose estimates (SSDE) for computed tomography  where: -Ŵ ED(z) is the predicted water equivalent diameter -W ED(z) is the ground truth water equivalent diameter z is the position along the craniocaudal axis of the patient. IEC standard states the median value of the set of Δ REL (z) along the craniocaudal axis (noted Δ REL ) should be below 0.1. Our method achieved a mean lateral Δ REL error of 0.0426 and a mean AP Δ REL error of 0.0428, falling well within the acceptance criteria."
Automated CT Lung Cancer Screening Workflow Using 3D Camera,4,Conclusion,"We presented a novel 3D camera based approach for automating CT lung cancer screening workflow without the need for a scout scan. Our approach effectively estimates start of scan, isocenter and Water Equivalent Diameter from depth images and meets the IEC acceptance criteria of relative WED error. While this approach can be used for other thorax scan protocols, it may not be applicable to trauma (e.g. with large lung resections) and inpatient settings, as the deviation in predicted and actual WED would likely be much higher. In future, we plan to establish the feasibility as well as the utility of this approach for other scan protocols and body regions."
Automated CT Lung Cancer Screening Workflow Using 3D Camera,,Clinical Applications -Musculoskeletal,
Automated CT Lung Cancer Screening Workflow Using 3D Camera,,Fig. 1 .,
Automated CT Lung Cancer Screening Workflow Using 3D Camera,,Fig. 2 .,
Automated CT Lung Cancer Screening Workflow Using 3D Camera,,Fig. 3 .,
Automated CT Lung Cancer Screening Workflow Using 3D Camera,,Fig. 4 .,
Automated CT Lung Cancer Screening Workflow Using 3D Camera,,,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,1,Introduction,"In-silico trials (ISTs) use computational modelling and simulation techniques with virtual twin or patient models of anatomy and physiology to evaluate the safety and efficacy of medical devices virtually  Virtual populations can be considered to be parametric representations of the anatomy sampled from a generative model. Traditional statistical shape models (SSMs), based on methods such as principal component analysis (PCA), have been widely explored in the past decade  In this study, we address the limitations of the state-of-the-art conditional generative models used to synthesise VPs of anatomical structures. In particular, we propose a method to relax the constraint on modelling the latent distribution as a unimodal multivariate Gaussian, to boost the flexibility of the generative model, and to enable conditional synthesis of diverse and plausible VPs generation. Recent advances in normalising flows "
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,2,Methodology,"In this study, we propose a cVAE model equipped with normalising flows for controllable synthesis of VPs of cardiovascular anatomy. A schematic of the proposed conditional flow VAE network architecture is shown in Fig. "
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Demographic Data,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Mesh Data,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Normalizing Flow,Planner Planner
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,… Demographic Data,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Concatenate,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Latent Flow Model,Conv Norm
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Demographic Data,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Conv ReLU Norm Residual,Basic Block 
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Mesh Data,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Conditional Variational Autoencoder:,"A VAE is a probabilistic generative model/network  Our conditional flow VAE (cVAE-NF) is a graph-convolutional network which takes as input a triangular surface mesh representation of an anatomical structure of interest, i.e., the Left Ventricle (LV) in this study, and its associated covariates/conditioning variables, i.e., the patient demographic data and clinical measurements, such as gender, age, weight, blood cholesterol, etc., and outputs the reconstructed surface mesh. Each mesh is represented by a list of 3D spatial coordinates of its vertices and an adjacency matrix defining vertex connectivity (i.e. edges of mesh triangles). The encoder and decoder contain five residual graph-convolutional blocks, respectively. Each block comprises two Chebyshev graph convolutions, each of which is followed by batch normalisation and ELU activation. A residual connection is added between the input and the output of each graph-convolutional block. Hierarchical mesh down/up-sampling operations proposed in CoMA  Flexible Posterior Using Normalizing Flow: Vanilla cVAEs model the approximate posterior distribution using Gaussian distributions with a diagonal covariance matrix. However, such a unimodal distribution is a poor approximation of the complex true latent posterior distribution in most real-world applications (e.g. for shapes of the LV observed across a population), limiting the anatomical variability captured by the model. In this study, we introduce normalising flows to construct a flexible multi-modal latent posterior distribution by applying a series of differentiable, invertible/diffeomorphic transformations iteratively to the initial simple unimodal latent distribution. As shown in Fig.  where the det ∂f ∂z is the Jacobian determinant of f . Therefore, we can obtain a complex multi-modal density by composing multiple invertible mappings to transform the initial, simple and tractable density sequentially, as follows, The specific mathematical formulation of the normalising flow function is important and must be chosen with care to allow for efficient gradient computation during training, scalable inference, and efficiency in computing the determinant of the Jacobian. In this study, we leverage the planar flow in  where w ∈ R d , u ∈ R d and b ∈ R are learnable parameters; h(•) is a smooth element-wise non-linear function with derivative h (•) (we use tanh in our study) and z denotes the latent variables sampled from the posterior distribution. Therefore, we could compute the log determinant of the Jacobian term in O(D) time as follows: Finally, the network is trained by optimizing the modified ELBO based on Eq. 3: where, ln p(x|c) is the marginal log-likelihood of the observed data x (i.e. here x represents an LV graph/mesh), conditioned on the covariates of interest (i.e. patient demographics and clinical measurements) c; i is the steps of the normalizing flows. p(x|z i , c) is the likelihood of data parameterised by the decoder network, which reconstructs/predicts x given the latent variables z i , transformed by latent (planar) normalising flows, and the conditioning variables c; KL(q(z 0 |x) p(z i )) is the Kullback-Leibler divergence of the approximate posterior initial q(z 0 |x, c) from the prior, p(z) = N (z | 0, I)."
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,3,Experimental Setup and Results,"Data: In this study, we created a cohort of 2360 triangular meshes of the left ventricle (LV) based on a subset of cardiac cine-MR imaging data available from the UK Biobank (UKBB) by registering a cardiac LV atlas mesh "
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Implementation Details:,"The framework was implemented using PyTorch on a standard PC with a NVIDIA RTX 2080Ti GPU. We trained our model using the AdamW optimizer with an initial learning rate of 1e-3 and batch size of 16 for 1000 epochs. The feature number for each graph convolutional block in the encoder was 16, 32, 32, 64, 64, and in reverse order in the decoder. The latent dimension was set at 16. The down/up-sampling factor was four, and we used a warm-up strategy "
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Evaluation Metrics:,"We compared our model (cVAE-NF) with a traditional PCA-based SSM, two generative models without conditioning information including a vanilla VAE and a VAE with normalising flow (VAE-NF) and the vanilla cVAE. Comparison of the vanilla cVAE can also validate the performance of existing approaches  It is essential to capture the distribution of clinically relevant biomarkers (e.g. BPVol) in the synthesised virtual populations (VPs) based on the specified covariates/conditioning information available for real patients, in order to effectively replicate the inclusion/exclusion criteria used during trial design in ISTs. For example, the BPVol of women is known to be lower than that of men  To verify this, we generated VPs using cVAE and our method, conditioned on real patient data (covariates) from the UK Biobank. Figure "
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,4,Conclusion,"We proposed a conditional flow VAE model for the controllable synthesis of VPs of anatomy. Our approach was demonstrated to increase the flexibility of the learnt latent distribution, resulting in VPs that captured greater variability in the LV shape than the vanilla cVAE. Furthermore, our model was able to model the relationship between covariates/conditional variables and the shape of the LV, and synthesise target VPs that fit the desired criteria (in terms of demographics of the patient and clinical measurements) and closely matched the real population in terms of a clinically relevant biomarker (LV BPVol). These results suggest that our approach has potential for the controllable synthesis of diverse, yet plausible, VPs of anatomy. Future work will focus on modelling the whole heart and exploring the impact of individual covariates on VP synthesis in more detail."
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Fig. 1 .,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Fig. 2 .,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Fig. 3 .,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Fig. 4 .,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Table 1 .,
A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 14.
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,1,Introduction,"Accurate segmentation of the optic cup and optic disc in fundus images is essential for the cup-to-disc ratio measurement that is critical for glaucoma screening and detection  Existing SF-UDA solutions can be categorized into four main groups: batch normalization (BN) statistics adaptation  We observe in our experiments (see Fig.  In this paper, we present a novel context-aware pseudo-label refinement (CPR) framework for source-free unsupervised domain adaptation. Firstly, we develop a context-similarity learning module, where context relations are computed from distances of features via a context-similarity head. This takes advantage of the intrinsic clustered feature distribution under domain shift "
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,2,Method,Figure 
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,2.1,Context-Similarity Learning,"In the SF-UDA problem, a source model f s is typically trained with a supervision loss of cross-entropy. Also an unlabeled dataset {x i t } nt i=1 from the target domain D t is given, where x i t ∈ D t . SF-UDA aims to learn a target model f t : X t → Y t with only the source model f s and the target dataset {x i t } nt i=1 . In our fundus segmentation problem, y i ∈ {0, 1} H×W ×C , where C is the number of classes and C = 2 because there are two segmentation targets, namely optic cup and optic disc."
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Architecture of Context-Similarity Head.,"Although the target features generated by source encoder do not align with the source segmentor, features of the same classes tend to be in the same cluster while those of different classes are faraway, as shown in Fig.  ( Computing similarities between every pair of coordinates in a feature map is computationally costly. Thus, for each coordinate i, only similarities with coordinates j lying within the circle of radius r are considered in our implementation. Training of Context-Similarity Head. Given a target image x t , initial pseudolabels and uncertainty mask can be obtained from the source model f s and x t , following previous work  (2) , ω ∈{foreground (fg), background (bg)}, In Eq. 2, Monte Carlo Dropout  Binary similarity label is then obtained. For two coordinates i and j, similarity label S * ij is 1 if pseudo-labels ŷi = ŷj , and 0 otherwise. Note only reliable pseudolabels are considered to provide less noisy supervision. The context-similarity head is trained with S * . To address the class imbalance issue, the loss of each type of similarity (fg-fg, bg-bg, fg-bg) is calculated and aggregated "
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,2.2,Context-Similarity-Based Pseudo-Label Refinement,"Context-Aware Revision. The trained context-similarity head is utilized to refine the initial coarse pseudo-labels. Specifically, context-similarities S ij are computed by passing the target image through the source encoder and the trained head. Then the refined probability for the i-th coordinate is updated as the weighted average of the probabilities in a local circle around the i-th coordinate as where p re i is the revised probability and d(•) is the Euclidean distance. β ≥ 1, in order to highlight the prominent similarities and ignore the smaller ones. By combining neighboring predictions based on context relations, revised probabilities are more robust. Equation 5 is performed iteratively for t rounds, since revised probabilities can be used for further revision. Calibration. The probability update by Eq. 5 might be hurt by inaccurate context relations. We observe that for some classes (optic cup for fundus segmentation) with worse pseudo-labels, the context-similarity for ""fg-bg"" is not learned well. Consequently, the probability of background incorrectly propagates to that of foreground, making the probability of foreground lower. To tackle this issue, the revised probability is calibrated as The decreased probability is rectified by the maximum value in the image, considering the maximum probability (e.g., in the center of a region) after calibration of a class should be close to 1."
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,2.3,Model Adaptation with Denoised Pseudo Labels,"The refined pseudo-labels can be obtained by However, noisy pseudo-labels inevitably exist. The combination of model knowledge and target feature distribution shows the best estimation of sample confidence  in which γ low and γ high are two thresholds for filtering out pseudo-labels without confident probabilities. d fg v and d bg v are the distances to feature prototypes as computed in Eq. 3. The final label selection mask is the intersection of m v,p and m v,c , i.e., m v = m v,p •m v,c . The target model is trained under the supervision of pseudolabels selected by m v , with cross-entropy loss:"
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,3,Experiments,"Datasets. For a fair comparison, we follow prior work  Implementation Details and Evaluation Metrics. Following prior works  Comparison with State-of-the-Arts. Table  Ablation Study on Different Modules. Table "
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,4,Conclusion,"This work presents a novel SF-UDA method for the fundus image segmentation problem. We propose to explicitly learn context semantic relations to refine pseudolabels. Calibration is performed to compensate for the wrong revision caused by inaccurate context relations. The performance is further boosted via the denoising scheme, which provides reliable guidance for adaptation. Our experiments on crossdomain fundus image segmentation show that our method outperforms the stateof-the-art SF-UDA approaches."
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Fig. 1 .,
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Fig. 2 .,
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Fig. 3 .,
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Table 1 .,
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Table 2 .,
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Table 3 .,
Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 58.
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,1,Introduction,"Quantitative T 1 mapping is a magnetic resonance imaging (MRI) technique that allows for the precise measurement of intrinsic longitudinal relaxation time in myocardial tissue  The derivation of accurate T 1 maps necessitates a sequential acquisition of registered images, where each pixel characterizes the same tissue at different timepoints (Fig.  Alignment of the images obtained at different time-points via image registration can serve as a mitigation for residual motion and enable cardiac T 1 mapping with free-breathing sequences such as the slice-interleaved T 1 (STONE) sequence  Deep-learning methods have been also proposed for motion correction by image registration as a pre-processing step in quantitative cardiac T 1 mapping  In this work, we introduce PCMC-T1, a physically-constrained deep-learning model for simultaneous motion correction and T 1 mapping from free-breathing acquisitions. Our network architecture combined an image registration module and an exponential T 1 signal decay model fitting module. The incorporation of the signal decay model into the network architecture encourages physicallyplausible deformations along the longitudinal relaxation axis. Our PCMC-T1 model has the potential to expand the utilization of quantitative cardiac T 1 mapping to patient populations who cannot tolerate breathholding by enabling automatic motion-robust accurate T 1 parameter estimation without additional manual annotation of the myocardium."
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,2,Method,"We formulate the simultaneous motion correction and signal relaxation model estimation for qMRI T 1 mapping as follows: where N is the number of acquired images, M 0 , T 1 are the exponential signal relaxation model parameters, φ i is the i'th deformation field, Φ = {φ} N -1 i=0 , I i is the i'th original image, and t i is the i'th timestamp. However, direct optimization of this problem can be challenging and time-consuming "
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,2.1,Model Architecture,"To overcome this challenge, we propose PCMC-T1, a DNN architecture that simultaneously predicts the deformation fields and the exponential signal relaxation model parameters. Figure  The input of the DNN is a set of acquired images {I i |i = 0 . . . N -1}, stacked along the channel dimension. The first encoder-decoder is an extension of the pair-wise VoxelMorph model  The second encoder-decoder has a similar architecture. It has two output layers representing the exponential signal relaxation model parameters: T 1 and M 0 . The predicted parameters maps are then used, along with the input's timestamps {t i |i = 0 . . . N -1}, as input to a signal generation layer. This layer generates a set of motion-free images {S i |i = 0 . . . N -1} computed directly from the estimated parametric maps (M 0 , T 1 ) at the different inversion times using the signal relaxation model "
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,2.2,Loss Functions,"We encourage predictions of physically-plausible deformation fields by coupling three terms in our loss function as follows: The first term (L fit ) penalizes for differences between the model-predicted images generated by the model-prediction decoder and the acquired images warped according to the deformation fields predicted by the registration decoder. Specifically, we use the mean-squared-error (MSE) between the registered images {R i |i = 0 . . . N -1} and the synthetic images {S i |i = 0 . . . N -1}: where S i are the images generated with the signal model equation (Eq. 2), and the registered images are the output of the registration module. This term encourages deformation fields that are physically plausible by means of a signal relaxation that is consistent with the physical model of T 1 signal relaxation. The second term (L smooth ) encourages the model to predict realistic, smooth deformation fields Φ by penalizing for a large l 2 norm of the gradients of the velocity fields  where Ω is the domain of the velocity field and p are the voxel locations within the velocity field. In addition, we encourage anatomically-consistent deformation fields by introducing a segmentation-based loss term (L seg ) as a third term in the overall loss function  where Seg i , (i ∈ 0, . . . , N -1) is the i th binary segmentation mask of the myocardium, Seg r is the binary segmentation mask of the fixed image, and r is the index of the fixed image. This term can be omitted in cases where the segmentations of the myocardium are not available."
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,2.3,Implementation Details,"We implemented our models in PyTorch. We experimentally fixed the first timepoint image, and predict deformation fields only for the rest of the time points. We optimized our hyperparameters using a grid search. The final setting for the loss function parameters were: λ 1 = 1, λ 2 = 500, λ 3 = 70000. We used a batch size of 8, ADAM optimizer with a learning rate of 2 • 10 -3 . We trained the model for 300k iterations. We used the publicly available TensorFlow implementations of the diffeomorphic VoxelMorph "
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,3,Experiments and Results,
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,3.1,Data,We used the publicly available myocardial T 1 mapping dataset 
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,3.2,Evaluation Methodology,"Quantitative Evaluation: We used a 5-fold experimental setup. For each fold, we divided the 210 subjects into 80% as a training set and 20% as a test set. We conducted an ablation study to determine the added value of the different components in our model. Specifically, we compared our method using a few variations, including a multi-image registration model with a mutual-informationbased loss function (REG-MI)  Clinical Impact: We further assessed the clinical impact of our method by conducting a semi-quantitative ranking of the T 1 maps for the presence of motion artifacts by an expert cardiac MRI radiologist (3 years of experience) who was blinded to the methods used to generate the maps. We randomly selected 29 cases (5 slices per case) from the test set with their associated T 1 maps. The radiologist was asked to rank each slice with 1 in case of a good quality map without visible motion artifacts and with 0 otherwise. We computed overall patient scores by summing the slice grades. The maximum grade per subject was 5 for cases in which no motion artifacts were present in all slices and 0 for cases in which motion artifacts were present in all slices. We assessed the statistical significance with the repeated measures ANOVA test; p<0.05 was considered significant."
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,3.3,Results,Quantitative Evaluation: Table  Clinical Impact: Figure  Table 
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,4,Conclusions,"We presented PCMC-T1, a physically-constrained deep-learning model for motion correction in free-breathing T 1 mapping. Our main contribution is the incorporation of the signal decay model into the network architecture to encourage physically-plausible deformations along the longitudinal relaxation axis. We demonstrated a quantitative improvement by means of fit quality with comparable Dice score and Hausdorff distance. We further assessed the clinical impact of our method by conducting a qualitative evaluation of the T 1 maps produced by our method in comparison to baseline methods by an expert cardiac radiologist. Our PCMC-T1 model holds the potential to broaden the application of quantitative cardiac T 1 mapping to patient populations who are unable to undergo breath-holding MRI acquisitions by enabling motion-robust accurate T 1 parameter estimation. Further, the proposed physically-constrained motion robust parameter estimation approach can be directly extended to quantitative T2 mapping as well as to additional qMRI applications."
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,,Fig. 1 .,
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,,Fig. 2 .,
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,,,
PCMC-T1: Free-Breathing Myocardial T1 Mapping with Physically-Constrained Motion Correction,,Table 1 .,
TabAttention: Learning Attention Conditionally on Tabular Data,1,Introduction,"Many clinical procedures involve collecting data samples in the form of imaging and tabular data. New deep learning (DL) architectures fusing image and nonimage data are being developed to extract knowledge from both sources of information and improve predictive capabilities  We develop such a solution and as an example of application, we use fetal birth weight (FBW) prediction from ultrasound (US) data. It is a challenging task requiring clinicians to collect US videos of fetal body parts and fetal biometry measurements. Currently, abdominal circumference (AC), head circumference (HC), biparietal diameter (BPD), and femur length (FL) are used to estimate FBW with heuristic formulae  In this work, we introduce TabAttention, a novel module designed to enhance the performance of CNNs by incorporating tabular data. TabAttention extends the CBAM to the temporal dimension by adding a Temporal Attention Module (TAM) that leverages Multi-Head Self-Attention (MHSA)  The main contributions of our work are: 1) the introduction of TabAttention, a module for conditional attention learning with tabular data, 2) the extension of CBAM to the temporal dimension via the TAM module, and 3) the validation of our method on the FBW estimation task, where we demonstrate that it is competitive with state-of-the-art methods."
TabAttention: Learning Attention Conditionally on Tabular Data,2,Method,"In this section, we introduce the fundamental components of the TabAttention module. We detail the development of CBAM augmented with a Temporal Attention Module. Then, we elaborate on how TabAttention leverages tabular embeddings to modulate the creation of attention maps and outline how the module can be seamlessly incorporated into the residual block of ResNet. Figure  Here O denotes the output of the module and ⊗ is an element-wise multiplication during which attention maps are broadcasted along all unitary dimensions.  In general, attention maps are computed based on information aggregated by average-and max-pooling along specified dimensions which are then passed through shared layers for refinement (Fig.  (2) Channel Attention Module. We follow the design of the original CBAM  Spatial Attention Module. After splitting the temporal feature maps, we average-and max-pool them along channel dimension to produce feature descriptors (F s avgi , F s maxi ∈ R 1×H×W ). We pass the tabular data through MLP embs with one hidden layer of size R H×W 2 and ReLU activation to embed it into the same dimension as spatial descriptors. We reshape this embedding to the size of feature descriptors and concatenate it with them. We pass the following representation through a 2D convolution layer and the sigmoid activation: Temporal Attention Module. We create temporal descriptors by averageand max-pooling temporal feature maps along all non-temporal dimensions (F t avgi , F t maxi ∈ R T ×1×1×1 ). We embed tabular data with MLP embt with one hidden layer of size R T 2 into the same dimension. We concatenate created vectors and treat them as the embedding of the US sequence which we pass to the MHSA layer (with 2 heads). We create the query (Q), key (K) and value (V) with linear layers and an output size of d (4). We add relative positional encodings  TabAttention can be integrated within any 3D CNN (or 2D CNN in case TAM is omitted). As illustrated in Fig. "
TabAttention: Learning Attention Conditionally on Tabular Data,3,Experiments and Results,"This section describes the dataset used and provides implementation details of our proposed method. We benchmark the performance of TabAttention against several state-of-the-art methods and compare them to results obtained by clinicians. Additionally, we conduct an ablation study to demonstrate the significance of each key component utilized in our approach. Dataset. This study was approved by the Ethics Committee of the Medical University of Warsaw (Reference KB.195/2021) and informed consent was obtained for all subjects. The multi-site dataset was acquired using international standards approved by  We resized the pixel spacing to 0. Implementation Details. We use 3D ResNet-18 as our base model. We implement all experiments with PyTorch and train networks using NVIDIA A100 80GB GPU for 250 epochs with a batch size of 16 and an initial learning rate chosen with grid search from the set of {1 × 10 -2 , 1 × 10 -3 , 1 × 10 -4 }. To minimize the Mean Squared Error loss function, we employ the Adam  XGBoost  TabAttention achieves the lowest MAE, RMSE and MAPE (170 ± 26, 225 ± 37, 5.0 ± 0.8 respectively) among all tested methods. Our approach outperforms clinically utilized heuristic formulae, machine learning, and image-only DL methods (two-tailed paired t-test p-value < 0.05). Results of TabAttention are also best compared with all DL models utilizing tabular and imaging modalities, however, the difference does not reach statistical significance with a p-value around 0.11. Ablation Study. We conduct ablation experiments to validate the effectiveness of key components of our proposed method (Table "
TabAttention: Learning Attention Conditionally on Tabular Data,4,Discussion and Conclusions,"In this work, we present a novel method, TabAttention, that can effectively compete with current state-of-the-art image and/or tabular-based approaches in estimating FBW. We found that it outperformed Clinicians achieving mMAPE of 5.0% vs. 5.9% (p-value < 0.05). A key advantage of our approach is that it does not require any additional effort from clinicians since the necessary data is already collected as part of standard procedures. This makes TabAttention an alternative to the heuristic formulas that are currently used in clinical practice. We should note that while TabAttention achieved the lowest metrics among the DL models we evaluated, the differences between our approach and other DL methods using tabular data were not statistically significant, partly due to the small performance change. This small difference in the performance is likely caused by the fact that the tabular features used in TabAttention are mainly derived from the same modality (i.e. US scans), so they do not carry additional information, but instead can be considered as refined features already present in the scans. To develop TabAttention, we used tabular data as a hint for the network to learn attention maps and gain additional knowledge about essential aspects presented in the scans. This approach significantly improved the performance of baseline methods and demonstrated its practical applicability. Accurate estimation of FBW is crucial in determining the appropriate delivery method, whether vaginal or Cesarean. Low birth weight (less than 2500 g g) is a major risk factor for neonatal death, while macrosomia (greater than 4000 g) can lead to delivery traumas and maternal complications, such as birth canal injuries, as reported by Benacerraf et al.  This study has limitations. Firstly, a relatively small study cohort was used, which may affect the accuracy and generalization of the results. To address this, future work will include a larger sample size by using additional datasets. Secondly, our dataset is limited to only Caucasian women and may not be rep-resentative of other ethnicities. It is important to investigate the performance of our method with datasets from different ethnic groups and US devices to obtain more robust and generalizable results. Lastly, our method relies on fetal biometry measurements that are subject to inter-and intra-observer variabilities. This variability could potentially affect the network's performance and influence the measurements' quality. Future studies should consider strategies to reduce measurement variabilities, such as standardized protocols or automated measurements, to improve the accuracy of the method. To summarize, we have introduced TabAttention, a new module that enables the conditional learning of attention on tabular data and can be integrated with any CNN. Our method has many potential applications, including serving as a computer-aided diagnosis tool for various clinical workflows. We have demonstrated the effectiveness of TabAttention on the FBW prediction task, utilizing both US and tabular data, and have shown that it outperforms other methods, including clinically used ones. In the future, we plan to test the method in different clinical applications where imaging and tabular data are used together."
TabAttention: Learning Attention Conditionally on Tabular Data,,Fig. 1 .,
TabAttention: Learning Attention Conditionally on Tabular Data,,Fig. 2 .,
TabAttention: Learning Attention Conditionally on Tabular Data,,Fig. 3 .,
TabAttention: Learning Attention Conditionally on Tabular Data,,,
TabAttention: Learning Attention Conditionally on Tabular Data,,Table 1 .,
TabAttention: Learning Attention Conditionally on Tabular Data,,Table 2 .,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,1,Introduction,"Medical phrase grounding (MPG) is the task of associating text descriptions with corresponding regions of interest (ROIs) in medical images. It enables machines to understand and interpret medical findings mentioned in medical reports in the context of medical images, which is crucial in medical image analysis and radiological diagnosis. Figure  Although visual grounding has been well studied for natural images  In this work, we propose MedRPG, an end-to-end approach for MPG. MedRPG has a lightweight model architecture and explicitly captures the finding-specific correlations between ROIs and report phrases. Specifically, we propose to stack a few vision-language transformer layers to encode both the medical images and report phrases and directly predict the box coordinates of desired medical findings. Compared to general grounding methods with heavy model architectures, this design is more robust against overfitting for MPG with limited training data. To locate nuanced medical findings with better regionphrase correspondences, we further propose Tri-attention Context contrastive alignment (TaCo). TaCo seeks context alignment to learn finding-specific representations that jointly align the region, phrase, and box prediction under the same context of a vision-language transformer encoder. It pulls both the features and attention outputs close together for semantically relevant region-phrase pairs while pushing those of irrelevant pairs far away. This encourages the alignment between regions and phrases at both feature and attention levels, leading to enhanced finding-identification ability and reduced spurious region-phrase correlations. Experimental results on three medical datasets demonstrate that our MedRPG is more effective in localizing medical findings, achieves better regionphrase correspondences, and significantly outperforms general visual grounding approaches on the MPG task."
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,2,Proposed Method,"The MPG problem can be defined as follows: Given a radiological image I associated with medical phrases T written by specialist radiologists, MPG aims to locate the described findings and then output a 4-dim bounding box (bbox) coordinates b = (x, y, w, h), where (x, y) is the box center coordinates and w, h are the box width and height, respectively."
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,2.1,Model Architecture,"Figure  Vision Encoder: Following the common practice  , where N v is the number of visual tokens and f n v ∈ R Cv is the n-th token of F v . Language Encoder: We leverage pre-trained language models such as BERT  , where N l is the number of text tokens, C l is the feature dimensions, and f n l ∈ R C l is the n-th token of F l . Vision-Language Transformer: After the individual vision and language encoding, we obtain F v and F l . To capture the correspondence between the image and phrase embeddings, we first project them into the common space (channel= C vl ) and then fed them into a Vision-Language Transformer (VLT), together with an extra learnable [REG] token, which is further used to predict the bbox: where ϕ v (•) and ϕ l (•) denote the project functions for vision and language tokens, respectively. r∈R C vl is the [REG] token and VLT(•) denotes the VLT encoder with learnable position embeddings. H ∈ R C vl ×N vl (where is the output of VLT that consist of three parts: vision embeddings , and [REG] embedding h reg . To perform the final grounding results, we further feed h reg into a 3-layer MLP to predict the final 4-dim box coordinates b = MLP(h reg ). Given the grounding-truth box b 0 , we leverage smooth L1 loss  where L box is the box loss. Φ l1 and Φ giou are the smooth L1 and GIoU loss functions, respectively. λ is the trade-off parameter."
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,2.2,Tri-Attention Context Contrastive Alignment,"Medical findings often share subtle differences in texture and brightness level due to the low contrast of the medical images, which makes it challenging for the MPG methods to capture accurate region-phrase correlations. To identify nuanced medical findings with better region-phrase correspondences, we propose the Tri-attention Context Contrastive Alignment (TaCo) strategy to learn finding-specific representations with accurate region-phrase correlations by explicitly aligning relevant regions and phrases at both feature and attention levels. Feature-Level Alignment. The feature-level alignment aims to make visual and textual embeddings with the same semantics meaning to be similar. To this end, given the bbox b 0 related to a given phrase query, we first obtain the positive ROI embeddings ) by aggregating visual embeddings H v within the bbox b 0 . Next, we randomly select K bbox {b k } K k=1 that have low IoUs with b 0 (i.e., regions that are irrelevant to the given phrase query) and obtain K negative region embeddings Let h cls be the features of the input phrases. We want to make the positive ROI embedding h box 0 close to the corresponding phrase embedding h cls whereas negative region embeddings {h box k } K k=1 far away. This is achieved by exploiting the InfoNCE  where L fea denotes the feature-level alignment loss, τ is a temperature hyperparameter and '•' represents the inner (dot) product. Attention-Level Alignment. In addition to the feature-level alignment, we also consider attention-level alignment, which encourages the attention outputs of VLT for relevant region-phrase pairs to be similar. To realize this, we extract the attention weight A ∈ R N vl ×N vl from the last multi-head attention layer of VLT. We denote a reg , a cls and {a box k } K k=0 as the attention weights for the embeddings of the [REG] token, the [CLS] token, and the K + 1 bboxes, respectively, where a box k = Pool(A, b k ). Given the k-th bbox embedding, we calculate the joint attention weights of bbox, [CLS], and [REG] embeddings and then further product H to get the triple-attention context pooling c k as follows: where t k represents the joint attention weights, t (j) k denotes the j-th element of t k , H[:, j] denotes the j-th column of H, and Norm(•) is the L2 normalization operation to constrain the sum of the squared weights to be equal to 1. Such triple-attention context pooling c k characterizes the contextual dependencies among regions, phrases, and box predictions in the VLT. Intuitively, the box prediction of certain medical findings should be made based on its relevant regions and phrases rather than irrelevant ones. Therefore, the attention outputs c 0 for relevant region-phrase pairs should be similar to their individual embeddings h box 0 and h cls , leading to attention-level alignment. Table  Finally, we combine the TaCo loss (  where L MedRP G denotes total loss for MedRPG and μ is the trade-off parameter."
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,3,Experiments,"Dataset. Our experiments are conducted on two public datasets, i.e., MS-CXR  We pre-process MS-CXR to make sure a given phrase query corresponds to only one bounding box, which results in 890 samples from 867 patients. ChestX-ray8 is a large-scale dataset for diagnosing 8 common chest diseases, of which 984 images with pathology are provided with hand-labeled bounding boxes. Due to the lack of finding-specific phrases from medical reports, we use category labels as the phrase queries to build the Image-Report-BBox triples for our task. Our in-house dataset comprises 1,824 Image-Phrase-BBox samples from 635 patients, including 23 categories of chest abnormalities with more complex phrases. For a fair comparison, all datasets are split into train-validation-test sets by 7:1:2 based on the patients. Evaluation Metrics. To evaluate the quality of the MPG task, we follow the standard protocol of nature image grounding  The channel numbers C v , C l , and C vl are 256, 768, and 256. The sample number K is set to 5. The trade-off parameter λ in Eq. 2 and μ in Eq. 6 are set to 1 and 0.05, respectively. The base learning rates for the vision encoder, language encoder, and vision-language transformer are set to 1×10 -5 , 1×10 -5 , and 5×10 -5 , respectively. We train our MedRPG model by the AdamW  This can be attributed to the proposed TaCo strategy in learning finding-specific representations and improving region-phrase alignment. On ChestX-ray8, all methods get degraded results due to the lack of position cues in the phrase queries. Nevertheless, our method still outperforms the second-best method by 4.3% in Acc and 1.6% in mIoU. On the in-house dataset, our method is still the best even when there exist much more types of findings to be grounded. Note that the self-supervised methods BioViL and GloRIA achieve very poor results compared to other methods on all three datasets. Ablation Study. We conduct ablative experiments on the MS-CXR dataset to verify the effectiveness of each component in MedRPG. Table  Qualitative Results. In Fig. "
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,4,Conclusion,"This study introduces MedRPG, a lightweight and efficient method for medical phrase grounding. A novel tri-attention context contrastive alignment (TaCo) is proposed to learn finding-specific representations and improve region-phrase alignment in feature and attention levels. Experimental results show that MedRPG outperforms existing visual grounding methods and achieves more consistent correlations between phrases and mentioned regions."
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,,Fig. 1 .,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,,Fig. 2 .,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,,Fig. 3 .,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,,,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,,Table 2 .,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,,Table 3 .,
Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_35.
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,1,Introduction,"Intelligible speech is produced by the intricate three-dimensional structure of the tongue, composed of localized functional units  Despite recent advances in cross-modal speech processing, translating between varied-size of wide 2D weighting maps and high-frequency 1D audio waveforms remains a challenge. The first obstacle is the inherent heterogeneity of their respective data representations, compounded by the tendency of losing pitch information in audio  To address the aforementioned challenges, in this work, we propose an endto-end translator that generates 2D spectrograms from 2D weighting maps via a heterogeneous plastic light transformer (PLT) encoder and a 2D CNN decoder. The lightweight backbone of PLT can efficiently capture the global dependencies with a wide matrix input in every layer  The main contributions of this work are three-fold: Both quantitative and qualitative evaluation results demonstrate superior synthesis performance over comparison methods. Our framework has the potential to support clinicians and researchers in deepening their understanding of the interplay between tongue movements and speech waveforms, thereby improving treatment strategies for patients with speech-related disorders."
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,2,Methods,
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,2.1,Preprocessing,"During the training phase, we are given M pairs of synchronized tagged MRI sequences t i and audio waveforms a i , i.e., {t i , a i } M i=1 . First, we apply a non-linear transformation using librosa to convert a i into mel-spectrograms, denoted as s i with the function S : a i → s i . This transformation uses a Hz-scale to emphasize human voice frequencies ranging from 40 to 1000 Hz Hz, while suppressing highfrequency instrument noise. Second, for each tagged MRI sequence t i , we use a phase-based diffeomorphic registration method  where λ and η denote the weights associated with the manifold and sparse regularizations, respectively, and Tr(•) represents the trace of a matrix. The graph Laplacian is denoted by L."
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,2.2,Encoding Variable Size H i with Plastic Light-Transformer,"Directly modeling correlations among any two elements in a given weighting map The recent efficient vision transformers (ViTs)  , each of which is flattened to a token vector with a length of d = P x × P y  where vectors Ni×d are produced by the linear projections of query (W q ), key (W k ), and value (W v ) branches, respectively  x and y directions δ x a,b = x ax b + P x , δ y a,b = y ay b + M y as the index in p. Furthermore, the product relative position bias utilized in this work can distinguish between vertical or horizontal offsets, whereas the popular cross relative position bias  Therefore, for global attention, we can aggregate the information of local tokens by modeling their global dependencies with"
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,,Ĝi,"Then, these global dependencies are broadcasted to every local token: By adding H local i and H global i , each token can benefit from both local and global features, while maintaining linear complexity with respect to the input size. This brings noticeable improvements with negligible FLOPs increment. However, the sequentially proportional patch merging used in  We cascade four PLT layers with SSPP as our encoder to extract the feature representation f i ∈ R 8×8×d . For the decoder, we adopt a simple 2D CNN with three deconvolutional layers to synthesize the spectrogram si ."
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,2.3,Overall Training Protocol,"We utilize the intermediate pairs of {H i , s i } M i=1 to train our translator T , which consists of a PLT encoder and a 2D CNN decoder. The quality of the generated spectrograms si is evaluated using the mean square error (MSE) with respect to the ground truth spectrograms s i : Additionally, we utilize the utterance consistency in the latent feature space as an additional optimization constraint. Specifically, we propose to disentangle f i into two parts, i.e., utterance-related f u i and subject-related f s i . In practice, we split the utterance/subject-related parts channel-wise using tensor slicing method. Following the idea of deep metric learning  Of note, the f s i is implicitly encouraged to incorporate the subject-related style of the articulation other than f u i with a complementary constraint  A GAN model can be further utilized to boost the realism of si . A discriminator D is employed to differentiate whether the mel-spectrogram is real s i = S(a i ) or generated si = T (H i ) with the following binary cross-entropy loss: In adversarial training, the translator T attempts to confuse D by optimizing Of note, T does not involve real spectrograms in log(D(s i ))  where β and λ represent the weighting parameters. Notably, only T is utilized in testing, and we do not need pairwise inputs for utterance consistency. Recovering audio waveform from mel-spectrogram can be achieved by the well-established Griffin-Lim algorithm "
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,3,Experiments and Results,"For evaluation, we collected paired 3D tagged MRI sequences and audio waveforms from a total of 29 subjects, while performing the speech words ""a souk"" or ""a geese,"" with a periodic metronome-like sound as guidance  The resulting H matrix varied in size from 20 × 5,745 to 20 × 11,938 (we set one dimension to a constant value of 20.) The audio waveforms had varying lengths between 21,832 to 24,175. To augment the dataset, we employed a sliding window technique on each audio, allowing us to crop sections with 21,000 time points, resulting in 100 audio waveforms. Then, we utilized the Librosa library to convert all audio waveforms into mel-spectrograms with a size of 64 × 64. For our evaluation, we utilized a subject-independent leave-one-out approach. For the data augmentation of the H matrix, we randomly drop the column to round Y i to the nearest hundred, e.g., 9,882 to 9,800, generating 100 versions of H. We utilized the leave-one-out evaluation, following a subject-independent manner. In our implementation, we set P x = 1 and P y = 20, i.e., d = 20. Our encoder consisted of four PLT encoder layers with SSPP, to extract a feature f i with the size of 8 × 8 × 20. Specifically, the first 8 × 8 × 4 component was set as the utterance-related factors, and the remaining 16 channels were for the subject-specific factors. Then, the three 2D de-convolutional layers were applied as our decoder to generate the 64 × 64 mel-spectrogram. The activation units in our model were rectified linear units (ReLU), and we normalized the final output of each pixel using the sigmoid function. The discriminator in our model consisted of three convolutional layers and two fully connected layers, and had a sigmoid output. A detailed description of the network structure is provided in the supplementary material, due to space limitations. Our model was implemented using PyTorch and trained 200 epochs for approximately 6 h on a server equipped with an NVIDIA V100 GPU. Notably, the inference from a H matrix to audio took less than 1 s, depending on the size of H. Also, the pairwise utterance consistency and GAN training were only applied during the training phase and did not affect inference. For our method and its ablation studies, we consistently set the learning rates of our heterogeneous translator and discriminator to lr T = 10 -3 and lr D = 10 -4 , respectively, with a momentum of 0.5. The loss trade-off hyperparameters were set as β = 0.75, and we set λ = 1. It is important to note that without NMF, generating intelligible audio with a small number of subjects using video-based audio translation models, such as Lip2AudSpect  Figure  Following "
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,4,Conclusion,"This work aimed to explore the relationship between tongue movements and speech acoustics by translating weighting maps, which represent the functional units of the tongue, to their corresponding audio waveforms. To achieve this, we proposed a deep PLT framework that can handle variable-sized weighting maps and generated fixed-sized spectrograms, without information loss or dimension expansion. Our framework efficiently modeled global correlations in wide matrix input. To improve the realism of the generated spectrograms, we applied pairwise utterance consistency with MMD constraint and adversarial training. Our experimental results demonstrated the potential of our framework to synthesize audio waveforms from weighting maps, which can aid clinicians and researchers in better understanding the relationship between the two modalities."
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,,Fig. 1 .,
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,,Fig. 2 .,
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,,Fig. 3 .,
Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer,,Table 1 .,
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,1,Introduction,"High myopia (HM) has become a global concern for public health, with its markedly growing prevalence  Accurate quantification of vision loss is integral to the early detection and timely treatment for MM and other HM-related complications  Conversely, imaging techniques, such as fundus photography (a.k.a., fundus), provide a relatively objective and robust measurement of the retinal morphology, which likely corresponds to the VF with an underlying ""structure-function relationship""  Therefore, utilizing machine learning models to estimate VF from fundus becomes a promising and feasible alternative for HM subjects in clinical practice. To the best of our knowledge, there is no existing approach to estimate VF from fundus. Some studies have been proposed to estimate the global indices (e.g., mean deviation) of VF from fundus  Actually, estimating VF with conventional regression  To tackle this challenge, we propose a novel method for estimating VF for HM using fundus, namely VF-HM. In general, VF-HM incorporates VF properties and is additionally regularized by an auxiliary task, thereby learning relatively high-entropy feature representations (see the orange line in Fig.  Our contributions are summarized as follows: -We propose a novel method, VF-HM, for estimating VF from fundus for HM. VF-HM more accurately detects the local vision loss and significantly outperforms conventional regression by 16.61% in the MAE metric on a real dataset. -VF-HM is the first work for VF estimation using fundus for HM, allowing for more convenient and cost-efficient detection of vision loss in HM, which could be useful for not only clinics but also large-scale vision screenings."
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,2,Problem Formulation,"Let D = {(x i , m i )} denote the training set, where x i ∈ X denotes the fundus, m i ∈ M denotes its corresponded VF. And A = {(x i , y i )} denotes the auxiliary set, where y i ∈ Y denotes the MM severity category of a given x i . The objective is to learn a model f : X -→ M by utilizing both D and A. The novelty of this formulation is additionally utilizing the auxiliary set to improve the model's generalization. And challenges mainly come from the following two aspects. First, how to design the model f , as mentioned earlier, conventional regression fails to predict local vision loss. Second, how to properly utilize the auxiliary set to assist the generalization of f , as the auxiliary information is not always helpful during the learning progress, i.e., sometimes may interfere  3 Proposed Method: VF-HM In this section, we first present an overview of the proposed method. Then, we introduce the details of different components."
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,3.1,Overview,"We present an overview of the proposed method in Fig.  where L pri and L aux denote the loss function for T pri and T aux , respectively. λ ∈ (0, 1] is a hyper-parameter to control the importance of L aux ."
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,3.2,Primary Task: VF Estimation,"The overall interest is only the primary task T pri , which is parameterized by f (•; θ, φ) : X -→ M. Specifically, we formulate T pri as an ordinal classification (aka, rank learning) problem, where each VF point m j i represents an ordinal variable/rank rather than a continuous one. Such a formulation incorporates the distinct properties of VF, which include: 1) Discretization: i , there is a relative order among VF values. To achieve this goal, we extend the ordinal variable/rank into binary labels  where L BCE (•) denotes the binary cross-entropy loss In addition, we propose to reuse the features from different blocks, as they contain distinct spatial information. Specifically, we propose Multi-scale Feature Fusion (MFF) for aggregating features from different blocks. As highlighted in orange in Fig. "
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,3.3,Auxiliary Task: MM Classification,"The auxiliary task T aux is introduced only to assist the generalization of T pri . Specifically, T aux is to predict MM severity category y i from fundus x i , which is parameterized by g(•; θ, ψ) : X -→ Y. MM is highly correlated to vision loss  However, the T aux is not always helpful for T pri because of the negative transfer  T aux becomes harmful for T pri , when the cosine similarity between ∇ θ L pri and ∇ θ L aux becomes negative "
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,4,Experiments,"In this section, we conduct experiments on a clinic-collected real-world dataset to evaluate the performance of our proposed method"
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,4.1,The Studied Data,"The studied data comes from a HM population, including 75 patients, each with diagnosis information for both eyes. For each eye, there are one fundus, VF, and MM severity category. Specifically, the fundus is captured in colorful mode, the VF is measured in the 24-2 mode with 52 effective points, and MM category is labeled by registered ophthalmologists. Besides, 34 patients (i.e., 68 eyes) have SD-OCT scans in the macular region. For these SD-OCT scans, we extract the retinal thickness with the pre-trained model "
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,4.2,Experimental Setup,"Data Pre-processing. We choose the left eye pattern as our base. For fundus, VF and retinal thickness are not in the left eye pattern, we convert them using the horizontal flip. Data Augmentation. Following  Evaluation Methods. For quantitative evaluation, we utilize three metrics  Implementation Details. We utilize the ResNet-18 "
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,4.3,Experimental Results,Main Results. Table 
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,4.4,Ablation Study,"To get a better understanding of the effectiveness of the main components in our proposed method, we conduct a series of ablation studies. Effectiveness of Main Components. We first examine the effectiveness of the main components by ablating them. The results are reported in Table  Impact of Hyper-parameter λ. We study the impact of the hyper-parameter λ with K-fold cross validation on training data. We choose λ ∈ {1.0, 0.1, 0.01, 0.001, 0.0001}. According to the results shown in Fig.  Different Methods for Mitigating the Negative Transfer. We consider three alternatives to refine the auxiliary gradient for mitigating the negative transfer: (1) weighted cosine (WC) similarity "
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,5,Conclusion,"In this work, we propose VF-HM for estimating VF from fundus for HM, which is the first work for VF estimation in HM; and it provides a more convenient and cost-effective way to detect HM-related vision loss. The major limitations include: first, our sample size is limited; second, we utilize both eyes from one patient as two independent inputs, which ignores their similarity; third, we only include the MM severity as the auxiliary information. Future work could be conducted as follows. First, collecting more data from different clinical sites. Second, modeling the relationship between both eyes from the same patient  Third, exploring more auxiliary information. Besides, studying how to adapt our method to different domains is a crucial problem "
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,,Fig. 1 .,
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,,Fig. 2 .,
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,,Fig. 3 .,
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,,Table 1 .,
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,,Table 2 .,
VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_61.
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,1,Introduction,"Automatic airway labeling aims to assign the corresponding anatomical names to the branches in airway trees. The identification of peripheral branches plays an essential role in bronchoscopic navigation. Figure  Various methods have been proposed for airway labeling  To resolve the first problem, in this work, we propose to fully use the latent relationships within the tree structure and airway nomenclature by a novel network named AirwayFormer. The tree structure provides inherent message transmission roads, while both global and local information is critical for anatomical labeling. To this end, we first adopt a neighborhood information encoding module based on the transformer block to aggregate both global and local features within the tree structure. Another structural relationship appears in the nomenclature of bronchial trees. As shown in Fig.  Our main contributions can be summarized as follows: (1) AirwayFormer is proposed for accurate airway labeling up to subsegmental level by exploiting the latent structural relationships. (2) A weight generator is designed to mitigate the overfitting caused by individual variation via adaptive decision boundary adjustment. ( "
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,2,Method,"An overview of our proposed method is illustrated in Fig.  denotes the input feature of the corresponding transformers. The node number and feature dimension are denoted by n and d, respectively. The number of categories of the m-th classifier is c m . More details about our method are introduced in the following sections."
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,2.1,Structure-Aware Transformers,"We propose a transformer-based model named AirwayFormer to exploit the structural relationships in airway labeling. The tree structure is introduced into the self-attention calculation, while the class consistency in the hierarchical nomenclature is encoded in the U-shape layout. Self-attention module in transformers enables nodes to aggregate messages from a global scale while neglecting the local structure prior. Recent studies  Here, Q ∈ R d×d and K ∈ R d×d are trainable parameters. Matrix D encodes the neighborhood information of the tree structure, and the network can adaptively concentrate on the local structure by optimizing C m . To simultaneously utilize the bi-directional correspondence between different levels, AirwayFormer adopts a U-shape layout. Concretely, the network performs nodes classification hierarchically from lobar level to subsegmental level and then back to lobar level: (3) Here, G m ∈ R n×d is the output feature of the m-th transformer blocks, and CAT (•) denotes the concatenate operation. Z m (•) is the m-th classifier consisting of a simple linear layer while P m ∈ R n×cm is the prediction result. The advantages of this design are two-fold. First, the output of coarser level is used as the input of finer level, which is helpful for the network to learn the corresponding relationship of the categories from coarse to fine. Second, the results of coarser level can be directly deduced from the predictions of finer level. Fine-grained features feedback to coarser level reduces the classification difficulty of coarser level and promotes the labeling performance. Besides, transformers conducting labeling of the same level are connected directly using highways. The same-level features can prevent performance degradation and ameliorate training stability. We further stop gradient back-propagation from segmental level to subsegmental level. The reason is that the severe distribution overlap in subsegments makes the feature learning in this level markedly different from others. This is further discussed in the supplementary materials."
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,2.2,Boundary-Adaptive Classifier,"Individual variations cause the distribution of adjacent classes to intersect, especially in subsegmental level. Although most subsegmental bronchi in the same tree are separable according to their relative positions, the inter-individual differences seriously affect the generalization performance of the model. To introduce case-specific information, we propose a boundary-adaptive classifier that adopts a novel generator to predict case-specific weights for each subsegmental category. Let G k ∈ R n×d denote the subsegmental output feature. We first use the next segmental level feature G k+1 ∈ R n×d and prediction P k+1 ∈ R n×c k+1 to obtain coarser representation for each segmental class: Here, S k+1 ∈ R n×c k+1 denotes the probability matrix of the segmental level while α is a learnable cluster parameter. H = (h 1 , h 2 , ..., h c k+1 ) T ∈ R c k+1 ×d are representations for c k+1 segmental categories. However, the segmental prediction P k+1 is not always perfect and may contain some classification errors. To avoid potential error propagation and obtain better class representations, these cluster centers are refined using a vanilla transformer. Specifically, we take H as query vectors and G k+1 as key and value vectors: After getting the enhanced segmental class representations, a generator network T (•) is used to produce classifier weights for subsegmental categories. Since each segment contains six subsegmental bronchi, T (•) transforms each segmental center into the seven (including itself) classifier weights: Here, Gelu(•) denotes the GELU activation function  We use cross-entropy loss with labeling smoothing  where γ m is the weight of the m-th loss function."
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,3,Experiments and Results,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,3.1,Dataset and Implementation Details,We evaluated our method on the public Airway Tree Labeling (ATL) Dataset
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,3.2,Evaluations,Table  Qualitative results are displayed in Fig. 
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,4,Conclusion,"This paper presented a transformer-based method named AirwayFormer for airway anatomical labeling. A U-shape layout integrating graph information is used to exploit the latent relationships fully. Meanwhile, a weight generator that produces the dynamic decision boundaries is designed to capture the relative relationships between sibling categories. Extensive experiments showed that our proposed method achieved superior performance in the bronchi labeling of all three levels, leading to an efficient clinical tool for intra-operative navigation."
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,,Fig. 1 .,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,,Fig. 2 .,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,,Fig. 3 .,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,,Table 1 .,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,,98.8 98.2 98.5 95.7 95.9 95.9 95.9 86.0 83.5 84.5 84.0,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,,Table 2 .,
AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 37.
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,1,Introduction,"Cataract has been the leading cause of vision loss. As the only treatment option, cataract surgery is one of the most commonly performed surgeries worldwide. Nevertheless, not all patients achieve complete visual recovery after surgery, which can be due to various factors, such as pre-existing eye conditions, surgical complications, and postoperative inflammation. The ability to accurately predict visual recovery can help clinicians identify high-risk patients and provide appropriate interventions to improve their visual outcomes. Over the past decades, many efforts have been made to predict the Best Corrected Visual Acuity (BCVA) after cataract surgeries. Most of them are based on traditional approaches like retinometer  Transformers  great promise in computer vision applications, such as image classification  The main contributions of this work can be summarized as follows: (1) We develop a novel framework that uses multimodal data to predict BCVA as well as tackle the complex modality-missing issue. (2) An auxiliary classification loss is adopted to extract more comprehensive pathological features for images. Also, discrete textual words are combined into sentences to better fit the text transformer input. "
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,2,Method,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,2.1,Framework Overview,As shown in Fig. 
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,2.2,Monomodal Feature Extraction,"Text Encoder. The text encoder is a pre-trained CLIP  text encoder. For instance, the text ""male, 67 years old, preoperative visual acuity 0.52 logMAR"" will be combined into the sentence ""A 67-year-old male patient with preoperative visual acuity of 0.52 logMAR"". By combining the text in this way, the physiological texts of all patients are fed into the text transformer in a unified manner. Compared with directly concatenating the texts and inputting them into the model, the combined sentences are more in line with the real scene and are easier to be understood by the model to extract key semantic information. Moreover, the CLIP text encoder is trained on a large text corpus, enabling excellent generalization performance. Thus, during the training of the overall model, the weights of the text encoder can be fixed. Image Encoder. The image encoder adopts ViT  where W i equals 1 if the i-th modality is available and equals 0, otherwise. By adding the classification loss, the final loss to train the whole model is: where L MSE is the mean square error loss between the predicted BCVA and the ground truth, α is a hyper-parameter and set to 0.5."
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,2.3,Multimodal Feature Fusion,"We use a cross-modal Transformer as the multimodal fusion network. The obtained modality-specific tokens are projected into the same dimension and concatenated into an input sequence. In contrast to the modality-specific Transformer, besides adding positional embeddings to all input tokens, we also add learnable modality-specific embeddings to all tokens indicating the modality information. Complete Multimodal Learning. In the collected dataset, all cases have corresponding text modality, and almost all cases have corresponding OCT modality. Therefore, we built a complete multimodal prediction model based on the text modality and OCT modality. In such a situation, only the OCT encoder and text encoder will be preserved, while the SLO encoder and Ultrasound encoder will be dropped. Since transformers can handle sequences of any input length, we don't need to make any changes to the fusion network. Using complete multimodal learning, we can compare our methods to other BCVA prediction approaches which do not consider the incomplete multimodal scenario."
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Incomplete Multimodal,"Learning. Not all cases have complete modalities for the three image modalities (i.e., OCT, SLO, and Ultrasound). For the missing modalities, one possible way is to simply represent them by 0 values  The proposed attentional masks are easy to implement. As shown in Eq. (  in which, Q, K, and V are queries, keys, and values obtained from tokens, respectively. d z is the projection dimension. To avoid interactions between irrelevant tokens, the masked self-attention is computed as: in which, M is the mask matrix. For each element in M , it will be 0 if the interactions should be included, or it will be negative infinity to avoid unnecessary interactions. By adding negative infinity, the results of softmax will be very close to 0. Figure "
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,2.4,Implementation Details,"To validate the effectiveness of the proposed method, we have conducted extensive experiments implemented with Pytorch and 8×RTX 3090 GPUs. The input images are resized to 224 × 224. The Adam optimizer is adopted with an initial learning rate of 0.001 and β 1 = 0.9, β 2 = 0.99. The mini-batch size is set to 32. We train the model for 100 epochs in total, and the learning rate will be decayed by 0.1 every 20 epochs. Besides, we randomly split all samples to 80% for training and 20% for testing. All experiments are conducted with 5-fold cross-validation to produce more solid results."
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,3,Experiments,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,3.1,Datasets and Evaluation Metrics,"The collected dataset consists of 1960 patients (2685 eyes) having cataract surgeries at Aier eye hospital of Wuhan University. The collected modalities are texts and images. The images contain 2635 Optical Coherence Tomography (OCT), 2615 Ultrasound, and 988 Scanning Laser Ophthalmoscopy (SLO). The textual information includes sex, age, preoperative and postoperative visual acuity. For each image, three ophthalmologists will label it to obtain the diagnosis (i.e., clinical diagnosis keywords) of 14 retinal diseases. The retinal diseases include normal, vitreous opacity, posterior staphyloma, stellate vitreous degeneration, pathological myopia changes, retinal atrophy, macular degeneration, epiretinal membrane, ellipsoid band partially missing, retinoschisis, retinal hemorrhage, macular edema, macular hole, and retinitis pigmentosa. We use Mean Absolute Error (MAE), Symmetric Mean Absolute Percentage Error (SMAPE), and prediction accuracy as the metrics. For simplicity, we consider predictions to be accurate if the prediction errors are within ± 0.10 logMAR."
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,3.2,Quantitative Performance,We have compared the results on the collected dataset with other approaches. The compared methods include state-of-the-art methods which aim to predict BCVA using OCT like CTT-Net 
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,3.3,Ablation Study,As shown in Table 
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,4,Conclusion,"In this paper, we present a new framework for BCVA prediction on the collected incomplete multimodal dataset. We take full advantage of multimodal information through our framework. The text modality is better utilized through the combination of the words. Moreover, image modality is explored effectively by the auxiliary classification loss. The attentional mask addresses the modalitymissing issue. Extensive experiments have proved the effectiveness and superiority of our method."
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Fig. 1 .,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Fig. 3 .,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Fig. 4 .,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,"specific encoder Multimodal Fusion Network 12 Layers, C=768 6 Layers, C=384 Tokens Fused Features",
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Predicted Diseases Predicted Diseases Predicted Diseases Predicted BCVA Auxiliary Classification Network,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Table 1 .,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Table 2 .,
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Acknowledgements,. This work is partially supported by 
Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 69.
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",1,Introduction and Related Work,"Skin diseases are a major public health concern that impacts millions of people worldwide. The first step towards diagnosis and treatment of skin diseases often involves visual inspection and analysis of the lesion by dermatologists or other medical experts. However, this process is often subjective, time-consuming, costly, and inaccessible for many people, especially in low-resource communities or remote areas. It is estimated that around 3 billion people lack adequate access to dermatological care  Skin lesion semantic segmentation and malignancy classification are essential for providing accurate and explainable diagnosis information for patients with skin diseases, and recently Artificial Intelligent (AI) systems have led the stateof-the-art for these tasks. However, these systems are commonly based on data and training methods that are prone to racial biases  -Data scarcity: Annotated medical images are often scarce and expensive to obtain due to privacy issues, cost, and expert availability. This limits the amount of data available for training Deep Learning (DL) models, which may result in overfitting, especially in a medical context, as shown in  Denoising Diffusion Probabilistic Models (DDPMs) have been introduced  We introduce the FEDD framework, a denoising diffusion-based approach trained on small, skin tone-balanced, Diverse Dermatology Images (DDI) "
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",2,Description of Data,"The most commonly used dataset to train and evaluate fairness in dermatology AI is Fitzpatrick17k  We draw 4 balanced subsets of DDI for training, each representing approximately 5% (10 samples per skin tone), 10% (20 samples per skin tone), 15% (30 samples per skin tone), and 20% (40 samples per skin tone) of DDI. The smaller training sets are subsets of the larger ones, this is to say 5% ⊆ 10% ⊆ 15% ⊆ 20% ⊆ DDI. For classification we draw validation and test sets, each containing 30 samples (10 samples per skin tone). Further, we test model checkpoints trained on each DDI subset on all remaining DDI images (476 samples), accuracy results from this larger test set are reported on the paper text and on Table "
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",3,Approach,The UNet architecture was introduced for diffusion  Recent work 
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",3.1,"Lesion, Marker, Ruler, Skin, and Background Segmentation","We obtain image encodings from blocks on the decoder side of the DDPM-UNet architecture, then apply bi-linear up-sampling up to some target output resolution (256 × 256 in this case) and concatenate them before feeding them to MLPs for per-pixel classification following "
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",3.2,Malignancy Classification,"For classification, we down-sample block encodings using a combination of 2D and 1D max pooling operations until the feature vectors are one-dimensional and of size 512. We note that the total number of pooling operations varies depending on the sampled block, as deeper blocks are smaller with more channels, while shallower blocks are larger with fewer channels. The vectors are then passed onto a 3-layer MLP of size 64, 32, and 1. We include batch normalization and dropout between each layer with 50% and 25%. This classification network is trained to predict the malignancy of the input image from the down-sampled feature vector. A summary of our approach is shown in Fig. "
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",4,Results and Discussion,
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",4.1,"Lesion, Marker, Ruler, Skin, and Background Segmentation","Our segmentation results are evaluated by Intersection over Union (IoU) performance and are compared against other architectures pre-trained on ImageNet: DenseNet121  We further compare the FEDD's IoU performance on light and dark skin tone images to showcase algorithmic fairness. We note that all architectures show similar performance for both skin tones, suggesting that our balanced data subsets play a larger role in fairness than the choice of neural network. Finally, we plot test set performance when only considering the lesion class split between light and dark tones. We find that FEDD's performance is significantly better at segmenting the lesion class compared to other architectures. We believe this is due to the greater morphology variation of different skin lesions being harder to learn than other more consistent targets like the ruler. Since DDI contains a diversity of skin conditions, FEDD's efficiency becomes very useful for highquality lesion segmentation of this morphologically changing class. These results are shown in Fig.  In Fig. "
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",4.2,Malignancy Classification,"We ablate the performance of each individual block per timestep in the classification context as, to the best of our knowledge, it has not been done before. It is also not entirely intuitive which block depth and timestep combination would produce the best representations for classification, as well as how that performance varies as we introduce more data. We train FEDD's classifier on block embeddings produced between timestep 0 and 1000 of the backward diffusion process. We then record the accuracy of the classifier per block in increments of 50 timesteps. Figure  We select the best-performing block and timestep combination at each fraction of data for the rest of our experiments. The previous classification state-of-Fig.  the-art on the DDI dataset is reported on "
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",5,Conclusion,We introduce the FEDD framework for skin lesion segmentation and malignancy classification that outperforms state-of-the-art methods and an ensemble of board-certified dermatologists across a diverse spectrum of skin tones and malignancy conditions under limited data scenarios. Our proposed methodology can improve the diagnosis and treatment of skin diseases while maintaining fair segmentation outcomes for under-represented skin tones and accurate malignancy predictions for rare malignancy conditions. We freely release our code and annotations to encourage further research around dermatological AI fairness.
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",,Fig. 1 .,
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",,Fig. 2 .,
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",,Fig. 3 .,
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",,Fig. 4 .,
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",,Fig. 5 .,
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 26.
"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification",,Clinical Applications -Fetal Imaging,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,1,Introduction,"Although these days smart healthcare devices such as smartwatches can be used to monitor a single-lead electrocardiogram (ECG) for Lead I and detect cardiovascular diseases (CVDs) such as atrial fibrillation (AF), multichannel ECGs such as twelve-lead signals are still required to diagnose more complex CVDs such as left and right bundle branch blocks (LBBBs and RBBBs) or myocardial infarction. To proactively deal with such intricate CVDs, therefore, one may need to undergo a 12-lead ECG measurement at a hospital and utilize 12-lead ECG-based deep learning algorithms for predicting CVDs  Although the ECG synthesis problem is not new, most previous studies have focused on utilizing it for data augmentation purpose as it is difficult to collect a sufficient amount of labeled ECGs (with CVDs) for developing prediction models. For example, many researchers have focused on synthesizing realistic ECGs using variants of autoencoders and GANs  Our work differs significantly from previous studies, as we generate all 11 remaining leads simultaneously from only a single lead. Thus, our method can be used to bridge commonly available wearable devices that can measure only Lead I and high-performance deep learning-based prediction models using 12lead ECGs. To the best of our knowledge, our work is the first to reconstruct all 12 leads simultaneously from a real single lead. By using the approach in "
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,2,Methods,"In this section, we introduce a novel conditional GAN for ECG reconstruction, called EKGAN, which is based on Pix2Pix "
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,2.1,Generator,Inference Generator. The inference generator G I is based on Pix2Pix's 2D U-Net generator and consists of an encoder and a decoder (Fig. 
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Label,Generator. An autoencoder-based model like U-Net 
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,2.2,Discriminator,"In 12-lead ECGs, each lead has its own characteristics and thus it is important for a discriminator to analyze each lead signal individually at the pixel level. If a standard 2D convolution-based discriminator is used, as for image-to-image translation, which uses 2D patches, then the unique characteristics of each lead signal may be intermixed with others, which may in turn degrade the reconstruction quality. To prevent this problem, instead, we use a 1D U-Net discriminator D, which has the same layer architecture as G L . D takes as input either G I 's output or an original 12-lead ECG, which is concatenated with G I 's input. In the encoder, the numbers of convolution filters are 32, 64, 128, 256, 512, kernel sizes are 64, 32, 16, 8, 4, and stride sizes are 4, 4, 4, 2, 2. The decoder has the inverse structure of the encoder as in the two generators, except that it uses a sigmoid activation function at the last layer."
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,2.3,Loss,"The objective of EKGAN is similar to that of other conditional GANs except that it uses the label generator G L to train the inference generator G I . Let us write e i for 12 replicated ECG segments from the input single-lead ECG for G I and e o for the corresponding ground-truth 12-lead ECG segments. In addition, let z i and z l be the latent vectors produced by the encoders of G I and G L , respectively. We use the following adversarial loss and L1 losses: where the last one is the latent vector loss for the inference generator. Then, the objective of EKGAN is defined as: where λ and α control the relative importance of each function. Note that L L1 (G L ) is used solely for training G L and thus not included in the objective equation. Through a grid search, we determined λ = 50 and α = 1, and these values have been used in the following experiments unless otherwise stated."
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,3,Evaluation,"This section introduces our dataset and its preprocessing. Then, we extensively evaluate EKGAN in terms of its reconstruction performance. We also assess its applicability using an existing prediction model with proven performance "
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,3.1,Experimental Setup,"Datasets. To develop and evaluate EKGAN, we used about 326,000 ECGs collected from Ewha Womans University Mokdong and Seoul Hospitals between May 23, 2017, and November 30, 2022. Specifically, we first selected ECGs with LBBB, RBBB, and AF from the dataset and randomly selected normal sinus rhythm (NSR) ECGs in a 1:1 ratio to reduce the bias of the generative models. For the label information, we simply used the interpretation result of the ECG machine. Next, for a test set for CVD multi-label classification and cardiologists' examination, we randomly chose 100 ECGs for each disease and 300 NSR ECGs. Then, the remaining dataset was randomly divided into train and validation sets in an 8:2 ratio. Table "
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Training and Test.,"All experiments were conducted on a workstation with an NVIDIA RTX 8000 and using TensorFlow 2.8. For comparisons, we implemented not only EKGAN but also Pix2Pix "
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,3.2,Reconstruction Performance,Table  Figure 
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,3.3,Reconstruction Quality Evaluation,"To evaluate the applicability and quality of ECG signals reconstructed by EKGAN, we use a highly accurate prediction model for CVDs  Table "
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,4,Conclusion,"This paper studies a novel problem of reconstructing 12-lead ECGs from singlelead ECGs. To address this problem, we propose a novel conditional GAN, called EKGAN, based on Pix2Pix, which consists of two generators and one 1D U-Net discriminator. Experimental results show that EKGAN significantly outperforms other representative generative models such as Pix2Pix, CycleGAN, and Car-dioGAN, and is able to reconstruct 12-lead ECGs that faithfully capture the essential characteristics of the original 12-lead ECGs useful for predicting CVDs. Therefore, we expect that numerous deep learning models based on 12-lead ECGs with proven performance could be applied to smart healthcare devices that can measure only single-lead signals. It would be also interesting to investigate if our method is applicable to more complex CVDs such as acute myocardial infarction, which require a more detailed analysis of 12-lead ECGs by cardiologists."
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Fig. 1 .,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Fig. 2 .,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Table 1 .,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Table 2 .,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,proposed) 0.32 0.25 6.95 0.04 × 10 -3 20.51Table 3 .,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Table 4 .,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Table 5 .,
Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 18.
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,1,Introduction,"Placenta Accreta Spectrum (PAS) occurs when the placenta becomes abnormally adherent to the myometrium rather than the uterine decidua  In the field of computer-aided PAS diagnosis for prenatal screening, some work based on traditional machine learning has been proposed  On the other hand, deep learning stands out in terms of automatic deep feature extraction  Another gap to fill is the inherent diversity of MRI sequences. There are T1weighted and T2-weighted MRI scans, upon which three planes with multiple slices are further present. Therefore, each patient corresponds to dozens of slice images. The physicians usually need to inspect a large portion of these images in order to identify the focus of suspected PAS and reach the patient-level diagnosis. For the existing work, a single (or few) slice image is selected to represent the information of a patient, which can lead to potentially biased results. Multi-Instance Learning (MIL), as a potential solution, has been widely adopted in lesion classification and localization  To deal with the issues above, a novel end-to-end Hierarchical Attention and Contrastive Learning Network (HACL-Net) is proposed under the formulation of a multi-instance problem. Slice-level spatial residual attention with prior knowledge is designed to extract context-aware deep semantic features from individual MRI slices of a patient. Patient-level attention-based pooling is then applied to aggregate these features into patient representation for PAS prediction. To make such features more discriminative, a plug-and-play contrastive loss is further included. Extensive experiments with ablation study show the proposed network can achieve state-of-the-art performance on a real clinical dataset involving 359 distinct patients."
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,2,Methodology,"Consider each patient bag P i includes K i valid 2D MRI slices from axial, sagittal, and coronal views (possibly both T1-weighted and T2-weighted scans exist). In the training dataset, there is an associated binary label Y i indicating whether PAS is reported at delivery. Denote i is the j-th MRI slice of i-th patient. Note that the number K i can vary with different patients. The proposed HACL-Net aims to learn the relationship between X i and Y i in a weekly-supervised and end-to-end manner (Fig. "
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,2.1,Slice-Level Spatial Residual Attention,"Trunk Branch. The trunk branch aims to extract global semantic information from each MRI slice that is important for the PAS prediction task. It is composed of the top convolution layer, batch normalization layer, and ReLU layer from ResNet18. For each grayscale MRI slice X (j) i ∈ R 256×256×1 , a preliminary feature map T (j) i ∈ R 128×128×64 is generated. Note that to realize the spatial residual attention with the mask branch, shallow features are first derived since they can be aligned with the context features. Deep features will be extracted later after the spatial residual attention layer. Mask Branch. The mask branch aims to extract the context information that indicates the location of ROI. Unlike the traditional implementation where a general bottom-up top-down network is employed, here the U-Net dedicated for medical image segmentation "
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Spatial Residual Attention and Deep Feature Extraction. The mask branch output M (j),"i has been aligned with the trunk branch output T (j) i in terms of the field of view. Therefore, M (j) i is multiplied in an element-wise manner with T (j) i as the attention mechanism. Besides, to avoid potential performance drop with attention, residual learning is further considered  i . Note that the attention is not performed directly on deep features because the ROI considered has explicit physical meanings instead of being ""hidden"". The remaining layers of pre-trained ResNet18 (excluding the top three layers used previously) are employed to further extract deep features, as shown in Fig.  } are eventually generated, where F (j) i ∈ R 512 . These features capture the context-aware semantic information of MRI slices."
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,2.2,Patient-Level Attention-Based Pooling,"Due to the MIL nature of the studied problem, MRI slices in a patient bag contribute unevenly to the PAS diagnosis. There are no detailed annotations to reflect the role of each slice. Therefore, patient-level attention mechanism is further proposed to aggregate the deep features from each slice in a task-specific manner, i.e., deriving the patient-wise features G i based on a set of slice-wise features F i . Some recent work  where the calculation of attention weight a j involves trainable parameters w ∈ R 64×1 and V ∈ R 64×512 . Such weight quantifies the degrees of ""activation"" toward the final prediction for each MRI slice. Note that the weight calculation resembles the softmax operation, and can deal with the varying number of slices for different patients unless the variation is not significant. The aggregated patient-wise features G i are then mapped to an output vector Ŷi with an FC layer. Binary Cross Entropy (BCE) is used as the classification loss function,"
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,2.3,Contrastive Learning for Discriminative Deep Features,"For the studied problem, the MRI slices of a positive patient (with PAS) that reflect the clues about PAS can account for only a small portion of all slices, while most ones look normal. Meanwhile, some slices of a negative patient (without PAS) can manifest suspected patterns. Therefore, the extracted patient-wise deep features should be more discriminative under the existence of inherent noises. In addition to aggregating the slice-wise features with attention, contrastive learning is further considered to realize that  where c is the positive margin constant that quantifies the distance. It is added to the BCE losses of the samples in a batch: L total = L e + L c ."
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,3,Experiment,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,3.1,Dataset and Implementation Details,"The experiments are conducted on a real clinical dataset collected from a large obstetrics and gynecology hospital. The dataset involves 359 distinct patient subjects with a total of 15,186 MRI slices. A 5-fold cross-validation is performed and the average metrics values with standard deviations over the folds are reported. To pre-train the U-Net in the mask branch, 185 MRI slices are annotated with placenta by the experts. The ResNet18 is pre-trained on ImageNet. HACL-Net is trained end to end with Adam optimizer and batch size N b = 3 for 100 epochs. The learning rate of the slice-level attention module and final FC layer is set as 10 -6 and 10 -4 respectively, while that of the rest parts is set as 10 -5 . PyTorch 1.10.1, and an NVIDIA GeForce RTX 2080 Ti GPU with CUDA 11.3 are used to train our method. More details can be found in Section B of supplementary. The code is publicly available on https://github.com/LouieBHLu/HACL-Net."
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,3.2,Performance Comparison of PAS Diagnosis,Section C of supplementary material illustrates the effect of different selections of trunk branch. Table  To evaluate the performance of machine learning methods with Pyradiomics features and deep learning methods 
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,3.3,Ablation Study,Table 
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,4,Conclusion,"This paper presents a novel end-to-end HACL-Net for MRI-based computeraided PAS diagnosis that facilitates more efficient prenatal screening. The proposed network utilizes slice-level spatial residual attention to extract contextaware deep semantic features, and aggregate them as a comprehensive representation of a patient with patient-level attention-based pooling. Moreover, a contrastive loss is added to improve the discriminating power of learned patient-wise features. Extensive experiments on a real clinical dataset show that HACL-Net achieves superior performance, with great potential for clinical applications."
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Fig. 1 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Fig. 2 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Fig. 3 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Fig. 4 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Fig. 5 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Fig. 6 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Table 1 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Table 2 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Table 3 .,
HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 29.
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,1,Introduction,The inception of deep neural networks has revolutionized the landscape of medical image segmentation 
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,,Background,"In this work, we propose a novel SFDA framework for cross-modality medical image segmentation. Our framework contains two sequentially conducted stages, i.e., Prototype-anchored Feature Alignment (PFA) stage and Contrastive Learning (CL) stage. As previous works "
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,2,Methods,"We are first provided a segmentation model M s trained on N s labeled samples {(x s n , y s n )} Ns n=1 from the source domain D s , and an unlabeled dataset with N t samples {x t m } Nt m=1 from the target domain D t , where x s , x t ∈ R H×W ×D , y s n ∈ R H×W , H and W are the height and width of the samples. The goal of SFDA is to adapt the source model M s with only unlabeled x t to predict pixel-wise label y t for the target domain data. In general, the segmentation model consists of two parts: 1) a feature extractor F θ : that projects pixel feature into the semantic label space with C classes. In the SFDA task, the source classifier φ s encounters a domain shift problem when classifying the target domain feature. To tackle this challenge, we propose a novel SFDA framework mainly including two stages, shown in Fig. "
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,2.1,Prototype-Anchored Feature Alignment,"Since source data is not available, explicit feature alignment that directly minimizes the domain gap between the source and target data like many UDA methods  of the source domain classifier φ s can be interpreted as the source prototypes, which characterize the features of each class. Thus, we introduce a bi-directional transport cost to align the target features with these prototypes instead of the unaccessible source features. Following  where τ is the temperature parameter, and p (µ c ) is the prior distribution (i.e., class proportion) over the C classes for the target domain. As the true class distribution is unavailable in the target domain, we use the EM algorithm to infer p (µ c ) instead of using a uniform prior distribution (see more details in  With the conditional distribution and point-to-point transport cost, we can derive the target-to-prototype (T2P) expected cost of moving the target features in this mini-batch to source prototypes, In this target-to-prototype direction, we assign each target pixel to the prototypes according to their similarities and the class distribution. However, like many entropy minimization methods  Then, combining the conditional transport cost in these two directions, we define the total prototype-anchored feature alignment (PFA) loss: Similar to "
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,2.2,Contrastive Learning Using Unreliable Predictions,"After the PFA stage, the clusters of target features are shifted towards their corresponding source prototypes, which brings remarkable improvements for the initial noisy prediction (see Fig.  With this intuition, we denote p t m,i as the softmax probabilities generated by model M t0 for the target data x t m,i . Then, for each class c, we construct three components, named query samples, positive prototypes, and negative samples, to explore those unreliable predictions as "
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,,Query Samples.,"During training, we employ the per-pixel entropy as uncertainty metric  where H(•) is the entropy of the input probabilities and γ c is the entropy threshold for class c. Here we set γ c as the α c -th percentile of all the entropy values of pixels assigned a pseudo label c. Positive Prototypes. The positive prototype is the same for all query pixels from the same class. Instead of using the center of query samples like  where r l is the low rank threshold and is set to 3 in our task. With the above definition, we have the pixel-level contrastive loss as: where K is the number of query samples, and z c,k ∈ P c denotes the k-th query sample from class c. Each query sample is paired with a positive prototype z + c and N negative samples z - c,k,j ∈ N c ."
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,3,Experiments and Results,
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,3.1,Experimental Setup,"Datasets and Evaluation Metrics. We evaluate our SFDA approach on a cross-modality abdominal multi-organ segmentation task. For the abdominal datasets, we obtain 20 MRI volumes from the 2019 CHAOS Challenge  For the evaluation, two main metrics, dice similarity coefficient (Dice) and average symmetric surface distance (ASSD) are used to quantitatively evaluate the segmentation results "
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,3.2,Results of Source-Free Domain Adaptation,"Comparision with Other Methods. In our experiments, ""no adaptation"" lower bound denotes learning a model on the source domain and directly test on the target domain without adaptation. And ""supervised"" upper bound means training and testing in the same target domain. We compared our methods with recent SFDA methods all designed for medical image segmentation scenarios, including a denoised pseudo-labeling approach (DPL)  The quantitative evaluation results are presented in Table "
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,4,Conclusion,"In this paper, we propose a novel two-stage framework to address the source-free domain adaptation problem in medical image segmentation. We first introduce a bi-directional transport cost to encourage the alignment between target features and source class prototypes in the prototype-anchored feature alignment stage. Also, a contrastive learning stage using unreliable predictions is further devised to learn a more compact target feature distribution. Sufficient experiments on the cross-modality abdominal multi-organ segmentation task validate the effectiveness and superiority of our method against other strong SFDA baselines, even some classical UDA approaches."
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,,Fig. 1 .,
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,,Fig. 2 .,
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,,Fig. 3 .,
Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning,,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 1. Q. Yu et al. ✔ 70.1 ± 6.9 52.9 ± 14.2 65.7 ± 12.5 70.9 ± 13.2 64.9 ± 8. of 86.1% and the lowest average ASSD of 1.4. Moreover, compared with recent UDA methods, our method obtains competitive results on average Dice and ASSD, which may be due to the use of unreliable predictions. As for ""CT to MRI"" direction, our method similarly shows great superiority on most organs as well, achieving the best performance in terms of both the average Dice (89.2%) and ASSD (1.3) among all SFDA methods. Figure  Ablation Study. In Fig. "
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,1,Introduction,"Bone shapes vary significantly across the world's population  Our work contributes several key steps to create personalized plates: (i) we first design an optimal PHILOS 3D plate shape for the humerus bone, fulfilling the clinical constraints given by a senior surgeon; (ii) we propose an anatomyaware transfer of the plate shape into any new bone, resulting in a personalized plate shape; (iii) we propose a method to position an arbitrary plate on an arbitrary bone according to clinical constraints, allowing surgical compatibility to be evaluated; (iv) we leverage the compatibility assessment to obtain a compact set of plates that accommodates a given population of bones; (v) we validate our methodology with extensive experiments on ex-vivo and 3D-printed bones, demonstrating the relevance of the designed personalized plate shapes for the clinical setting; (vi) we make available for research purposes the humerus and plate 3D models, as well as the plate extraction and fitting code  The creation of surgical plates consists of several tasks, which have been partially addressed by the existing state of the art. In Table  Most methods start with a base plate or template that is designed for a specific bone surgery (tibia, humerus, clavicle, etc.)  Once the plate is positioned, one needs to evaluate if the plate can be used in surgery. Some works compute plate-to-bone metrics  Once the plate is positioned on a bone, and the fit evaluated, one can further consider the question of how to deform the plate to fulfill the fit criterion. For instance,  Prior work proposes a set of plate shapes that, together, can accommodate a range of patients "
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,2,Method,"Dataset. To create and evaluate our plate designs, we use a dataset consisting of 97 3D meshes of humerus bones, divided into two groups (A and B). Group A has 54 bones scanned with a FARO laser scanner (25 females and 29 males, 50% Black and 50% White, age range 17 and 45). Group B has 43 Computed Tomography (CT) scans of bones from autotomized body donors. The CT volumes were segmented and cleaned to reconstruct a mesh of the bone. Left-side humerus scans were mirrored along the z-axis to work with right-side humeri only. Plate Design. Given a bone, our goal is to automatically generate an optimal plate shape and determine its position on the bone. The optimality of our plate design is defined by an experienced surgeon, who established fixation points, areas to avoid, and plate-to-bone distance tolerances. These choices were validated by a second expert surgeon. A plate fulfilling these constraints on a bone is considered optimal for surgery. To obtain the plate design we 3D printed 7 bone meshes from our dataset with diverse shapes and asked the surgeon to annotate each bone with a marker. The surgeon indicated the bone areas that should be in contact with an ideal plate and the areas to be avoided (Fig. "
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,right).,"Bone Registration for Optimal Plate Transfer. Once the optimal plate is defined on the template bone, the goal is to register any new bone to the template, by preserving the relevant anatomic regions for the plate fixation. To do so, we adapt the registration technique designed for vertebrae  We first register each bone of set A by optimizing their Eqs. 1, 2 and 3 from  Second, we register the whole dataset using the learned bone shape model by minimizing where S denotes the scanned bone mesh, t and r are, respectively, the 3D rigid translation and rotation applied to the bone mesh vertices, and F is a 3D pervertex offset applied to each mesh vertex. E p2m (S, .) is the point-to-triangle distance between the scan vertices and the mesh triangles. The function Δ(T) is the mesh Laplacian operator, which is used to regularize the offsets F. We minimize Eq. (  Due to the elongated shape of the humeri and their axial rotation similarity, the registrations B i can contain sliding, i.e. the same vertex does not correspond precisely to the same anatomic location on two different bones. To correct this, we perform a second pass of registration using smooth shells  To validate the bone correspondences we define anatomic regions on the bone template T, and transfer them to all registrations B i on a per-vertex index basis. Figure  Positioning a Plate on a Bone. Now that we can create personalized plates, we want to study if a created plate can be used for surgery on another bone. For that, given an arbitrary plate P i and bone registration B j , we need to position the plate and determine if it fits. We automate the plate positioning by minimizing the distance between the plate and bone fixation points. We start by computing an initial 3D rigid transformation r 0 , t 0 by optimizing Eq. 2 in Sup. Mat. and then obtaining the final positioning r f , t f by optimizing Eq. 3 in Sup. Mat. These equations enforce the fixation areas of the plate P i h to match B j h and P i s to match B j s (see Fig. "
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,Plate Shapes Set.,A single plate design can not accommodate the whole population due to its morphological variance 
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,3,Experiments,
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,3.1,Numerical Evaluations,"Bone Registration Accuracy. We validate that our registrations B i accurately match the scans S i by computing the mean distance (MD) between each registration to the closest vertex in the scan. We obtain a MD of 0.08 mm (std=0.04) for group A, and a MD of 0.27 mm (std=0.55) for group B. For both sets, the registered bones match the scans with sub-millimeter accuracy. In addition, all distances are less than 1mm on the surface where the plate is positioned (Fig. "
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,left).,"Plate Set Coverage. One application of our design is that it can generate a plate set that could be preprinted in the hospital and be readily available for immediate use. Using our greedy algorithm on our bone dataset, a set P N =5 S made of the first 5 plates accommodates already 51.04% of the bones and P N =10 S accommodates 73.96% (Fig. "
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,3.2,Ex-Vivo Evaluations,"Comparison with State-of-the-Art Plates. We CT-scanned an isolated bone (not included in the original dataset) and asked a surgeon to manually contour a state-of-the-art plate, as done in clinical practice. We compare it to the custom plate P C and the best set plate P S by computing the plate-to-bone distances (Fig.  Ex-vivo Surgery. We performed an ex-vivo experiment on a cadaveric arm, mimicking an actual minimally-invasive surgical operation setting. One goal was to test whether the designed 3D plates can be properly inserted along the bone under the muscles. We CT-scanned 3 cadaveric arms (not included in the original dataset), reconstructed the bone scans and registered them to obtain B 1 , B 2 and B 3 . For each bone we generated and 3D printed their custom plate P 1,2,3 C and the best fitting set plate P 1,2,3 S . The plates were drilled and coated with metallic paint to be visible on the CT scan. During the experiment, the plates were inserted in the cadaver arm, as it would be done in a standard operation, and fixed to the humerus with screws. For the 3 bones, the surgeon estimated that the fit of both the custom and set plates was satisfactory. He noted that the surface roughness of the 3D print and metallic coating was too high, making the plate hard to slide across the humerus into place. We CT-scanned each plate screwed to the bone and used the scans to asses the fit quality. Figure "
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,3.3,Evaluations with 3D-Printed Bones,"To further evaluate our design, we generated 7 bones {B i } i∈  First, we presented pairs of bones and plates to the surgeon and asked whether they were fit for surgical use. Then, we asked them to position the plates on the 3D bones to evaluate our positioning strategy. Last, to evaluate the adequacy of a plate set, we presented one bone to the surgeon and asked them to choose the best plate from the set. Ready-for-Surgery Evaluation. For each bone B i , the surgeon considered both plates as candidates and evaluated if their shape was suitable for surgery. The surgeon stated that 100% of the plates were fit for surgery and that the custom plates P C were always clearly better fitting than the plates P S from the set. Positioning Evaluation. To evaluate the plate positioning computed by our algorithm, we compare it to the surgeon's positioning. We asked the surgeon to position the set plates P S on each bone, secured them with blue tack and scanned the whole with a FARO hand scanner. Figure  Choosing from the Plate Set. Our algorithm iteratively positions the plates P i S from i = 0 to i = N and picks the best-fitting plate, so we asked the surgeon to perform a similar trial-and-error task to evaluate the relevance of our plate set. Out of the 7 bones, the surgeon only chose the same plate as our algorithm for two of them (B 7 and B 10 ). For the other cases, the surgeon compared his selection to the algorithms' choice and confirmed that both plates were fit for surgery: different plates with slightly different placements can work for the same bone. Most interestingly, some plates selected by the surgeon did not fit the bone according to the strict numeric fit criteria. We discuss this finding in the next section. The surgeon noted that the plates of our set were always preferred over the state-of-the-art plates."
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,4,"Conclusions, Take-Aways and Future Work","Conclusion. We propose to automatically generate a custom humerus plate that specifically matches the 3D shape of a bone and specifically meets the requirements of a surgeon for minimally invasive surgery. We also generate a set of plate shapes that accommodates a given bone population. We extensively evaluate our approach on cadaverous arms and 3D printed bones. Experimental Takeaways. All the proposed plates (individual and set plate) match the shape of the bones, while the individual ones are considered the best by the surgeon. Furthermore, our experiments show the importance of evaluating on 3D-printed bones and ex-vivo arms. With the former, the surgeon can easily assess the plate-to-bone fit. With the latter, the insertion procedure can affect the plate placement. Moreover, our results on 3D printed bones argue for a relaxation of the theoretical fitting constraints: the plate-to-bone distance could be higher in some areas while still obtaining a proper surgical fit. Loosening the tolerances would allow more bones to be fit with the same plates and potentially the number of plates required in the plate set could be further reduced. Future Work. To reach actual clinical use, two elements need consideration: (i) the current method takes as input the shape of a healthy bone, whereas patients have a fracture. One could perform a CT scan of the other arm and generate the symmetric bone or leverage methods that reconstruct a full bone from partial observations "
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,Fig. 1 .,
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,Fig. 2 .,
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,Fig. 3 .,
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,Fig. 4 .,
Optimizing the 3D Plate Shape for Proximal Humerus Fractures,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 46.
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,1,Introduction,"Age-related macular degeneration (AMD) is the leading cause of blindness in the elderly, affecting nearly 200 million people worldwide  the retina. However, the widely adopted AMD grading system  In their search for new biomarkers, clinicians have annotated known biomarkers in longitudinal datasets that monitor patients over time and mapped them against disease progression  Our Contribution: In this work, we present a method to automatically propose biomarkers that capture temporal dynamics of disease progression in longitudinal datasets (see Fig. "
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,2,Related Work,"Current AMD Grading Systems: Ophthalmologists' current understanding of progression from early to late AMD largely involves drusen, which are subretinal lipid deposits. Drusen volume increases until suddenly stagnating and regressing, which often precedes the onset of late AMD  Tracking Evolution of Known Biomarkers: Few research efforts have aimed at quantifying and tracking known AMD biomarkers, mostly drusen, over time  Automated Discovery of Unknown Biomarkers: Prior work for automated biomarker discovery in AMD explores the latent feature space of encoders trained for image reconstruction "
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3,Materials and Methods,
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3.1,OCT Image Datasets,We use two retinal OCT datasets curated in the scope of the PINNACLE study 
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3.2,Self-supervised Feature Space Using Contrastive Learning,"We use the BYOL contrastive loss  where the output of the momentum updated 'teacher' network f is passed through a stop-gradient, so that only the student network f is updated. As several of the contrastive transformations designed for natural images are inapplicable to medical images, such as solarisation, colour shift and greyscale, we use the set tailored for retinal OCT images by Holland et al.  Fig. "
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3.3,Extracting Sub-trajectories via Partitioning,"Naively clustering whole time series of patients ignores two characteristics of longitudinal data. Firstly, individual time series are not directly comparable as patients enter and leave the study at different stages of their overall progression. Secondly, longer time series can record multiple successive transitions in disease stage. Inspired by TRACLUS  For each eye, we first form piecewise-linear trajectories by linking points in feature space that were derived from consecutively acquired OCT images. We then extract sub-trajectories by finding all sequences of images spanning 1.0 ± 0.5 years of elapsed time within each trajectory. Next, to avoid oversampling trajectories with a shorter time interval between images, we randomly sample at most one sub-trajectory in every 0.5-year time interval."
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3.4,Sub-trajectory Distance Functions and Clustering,"In order to find common patterns of disease progression among sub-trajectories we cluster them. To this end we introduce a new distance function between subtrajectories that incorporates three distinct temporal criteria (see Fig.  Since all sub-trajectories cover a similar temporal duration, D transition also differentiates between fast and slow progressors and stable periods of no progression. However, by ignoring intermediary images, this metric does not respect the disease pathway along which patients progress. To incorporate this, we include a second metric that measures path dissimilarity, calculated using dynamic time warping (DTW)  The third and final temporal criteria is to match time series that progress in the same relative direction, regardless of absolute disease states. We weight the contribution of this with φ ∈ R, 0 ≤ φ ≤ 1 in Eq. 4: Spectral Clustering: As the non-linearity of D subtraj prohibits the use of k-means for clustering, we instead use spectral clustering "
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,3.5,Qualitative and Quantitative Evaluation of Clusters,"Initially, we tune the hyperparameters, λ, φ and K, on the Development dataset by heuristically selecting values that result in higher uniformity between subtrajectories within each cluster. Two teams of two ophthalmologists then review 20 sub-trajectories from distinct patients in each cluster, interpreting and summarising any consistently observed temporal dynamics. Next, using the same hyperparameters we apply the method directly to the Unseen dataset. The ophthalmologists then review these clusters and confirm whether they capture the same temporal biomarkers observed in the Development dataset. In addition to the qualitative evaluation, we also validate the utility of our clusters as biomarkers that stratify risk of disease progression. We test this by predicting the time until conversion to late AMD and its subtypes, CNV and cRORA. Additionally, we predict current visual acuity. For prediction, each sub-trajectory is characterised by a vector of size K that encodes proportional similarity to each cluster. This vector is then used by a Lasso linear regression model. Similarly, we fit an equivalent linear regression model to the static biomarkers from the established grading system detailed in Sect. 3.1. We also include a demographic baseline using age and sex. We also add a temporally agnostic baseline that clusters only single time points. Finally, to demonstrate the performance gap between our interpretable approach and black-box supervised learning algorithms, we include a fully supervised deep learning baseline by fitting an SVR directly to the feature space. Each experiment uses 10-fold cross validation on random 80/20 partitions, while ensuring a patient-wise split. Finally, we repeat the entire method, starting from sub-trajectory extraction, followed by clustering and then regression experiments, using 7 random seeds and report the means and standard deviations. "
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,4,Experiments and Results,"Sub-trajectory Clusters are Candidate Temporal Biomarkers: By first applying our method to the Development dataset we found that using λ = 0.75, φ = 0.75 and K = 30 resulted in the most uniform and homogeneous clusters while still limiting the total number of clusters to a reasonable amount. Achieving the same cluster quality with smaller values of φ required many more clusters in order to encode all combinations of possible start and end disease states. The expert ophthalmologists remarked that many of the identified clusters capture dynamics that have already been linked to the progression of AMD, even though they are not currently included in any clinical grading system. Using the same hyperparameters our method generalised to the Unseen dataset which yielded clusters with equivalent dynamics and quality (see Fig.  Sub-trajectory Clusters Predict Conversion to Late AMD: Next, we validated that our clusters are predictive of progression to late AMD. Our subtrajectory clusters were comparable to, and in some cases outperformed, the current widely adopted grading system in predicting risk of conversion (see Table "
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,5,Discussion and Conclusion,"Motivated to improve inadequate grading systems for AMD that do not incorporate temporal dynamics we developed a method to automatically propose biomarkers that are time-dependent, interpretable, and predictive of conversion to late-stage AMD. We applied our method to two large longitudinal datasets, cataloguing 3,218 total years of disease progression. The found time-dependent clusters were subsequently interpreted by four ophthalmologists. They found them to capture distinct patterns of disease progression that have been previously linked to AMD, but are not currently included in clinical grading systems. Furthermore, we experimentally demonstrated that the found clusters predict conversion to late-stage AMD on par with the established grading system. In the future, biomarkers identified by our method can be further refined by clinicians. We will also use the full volumetric image to model progression dynamics outside the macular. As late stage patients were overrepresented in our datasets, we also intend to apply our method to datasets with greater numbers of patients progressing from earlier disease stages. Ultimately, we envision that proposals from our method may inform the next generation of grading systems for AMD that incorporate the temporal dimension intrinsic to this dynamic disease."
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,,Fig. 3 .,
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,,,
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,,Table 1 .,
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_68.
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,1,Introduction,"Diffuse glioma is the most common and aggressive primary brain tumors in adults, accounting for more deaths than any other type  Recently, deep learning has achieved success in diagnosing various tumors  This paper proposes a deep learning model (DeepMO-Glioma) for glioma classification based on WSIs, aiming to reflect the molecular pathology paradigm. Previous methods are proposed to integrate histology and genomics for tumour diagnosis  Moreover, multiple molecular markers are needed for classifying cancers, due to complex tumor biology. To reflect real-world clinical scenarios, we formulate predicting multiple molecular markers as a multi-label classification (MLC) task. Previous MLC methods have successfully modeled the correlation among labels  Lastly, we focus on modeling the interaction between molecular markers and histology. Specifically, we devise a novel inter-omic interaction strategy to model the interaction between the predictions of molecular markers and histology, e.g., IDH mutation and NMP, both of which are relevant in diagnosing glioblastoma. Particularly, we design a dynamical confidence constraint (DCC) loss that constrains the model to focus on similar areas of WSIs for both tasks. To the best of our knowledge, this is the first attempt to classify diffuse gliomas via modeling the interaction of histology and molecular markers. Our main contributions are: "
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,2,Preliminaries,Database: We use publicly available TCGA GBM-LGG dataset 
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,3,Methodology,Figure 
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,3.1,Hierarchical Multi-task Multi-instance Learning,"To extract global information from input {X i } N  1 , we propose a hierarchical multi-task multi-instance learning (HMT-MIL) framework for both histology and molecular marker predictions. Different from methods using one "
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,3.2,"Co-occurrence Probability-Based, Label-Correlation Graph","In predicting molecular markers, i.e., IDH, 1p/19q and CDKN, existing MLC methods based on label correlation may ignore the co-occurrence of the labels. In the genomic marker prediction module, we proposed a co-occurrence probabilitybased, label-correlation graph (CPLC-Graph) network and a label correlation (LC) loss for intra-omic modeling of the co-occurrence probability of the three markers. 1) CPLC-Graph network: CPLC-Graph (Fig.  as input nodes, we construct a co-occurrence probability based correlation matrix A ∈ R 3×3 to reflect the relationships among each node feature, with a weight matrix W g ∈ R C×C to update the value of F in . Formally, the output nodes F mid ∈ R 3×C are formulated by a single graph convolutional network layer as In (1), δ(•) is an activation function and p(F in i |F in j ) denote the probability of the status of i-th marker given the status of j-th marker. Besides, residual structure is utilized to generate the final output F out of CPLC-Graph network, defined as where α is a graph balancing hyper-parameter."
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,2) LC loss:,"In order to fully exploit the co-occurrence probability of different molecular markers, we further devise the LC loss that constrains the similarity between any two output molecular markers F out i and F out j to approach their correspondent co-occurrence probability A j i . Formally, the LC loss is defined as In "
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,3.3,Dynamical Confidence Constraint,"In the cross-omics interaction module, we design a dynamical confidence constraint (DCC) strategy to model the interaction between molecular markers and histological features. Taking IDH and NMP as an example, the final outputs for IDH widetype where S(ω k wt , ωnmp ) is the indicator function taking the value 1 when the k-th important patch of IDH widetype is in the set of top K m important patches for NMP, and vice versa. In addition, to facilitate the learning process with DCC loss, we adopt a curriculum-learning based training strategy dynamically focusing on hard-to-learn patches, regarded as the patches with higher decision importance weight, as patches with lower confidence weight, e.g., patches with fewer nuclei, are usually easier to learn in both tasks. Hence, K m is further defined as In ( "
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,4,Experiments and Results,
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,4.1,Implementation Details,"The proposed DeepMO-Glioma is trained on the training set for 70 epochs, with batch size of 8 and initial learning rate of 0.003 with Adam optimizer "
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,4.2,Performance Evaluation,"1) Glioma classification. We compare our model with five other state-of-theart methods: CLAM  The left panel of Table  2) Predictions of genomic markers and histology features. From the left panel of Table  3) Network interpretability. An additional visualization experiment is conducted based on patch decision scores to test the interpretability of our method. Due to the page limit, the results are presented in supplementary Fig. "
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,4.3,Results of Ablation Experiments,1) CPLC-Graph network. The right panels of Table  2) LC loss. The right panels of Table  3) DCC loss. From Table 
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,5,Summary,"The paradigm of pathology diagnosis has shifted to integrating molecular makers with histology features. In this paper, we aim to classify diffuse gliomas under up-to-date diagnosis criteria, via jointly learning the tasks of molecular marker prediction and histology classification. Inputting histology WSIs, our model incorporates a novel HMT-MIL framework to extract global information for both predicting both molecular markers and histology. We also design a CPLC-Graph network and a DCC loss to model both intra-omic and inter-omic interactions. Our experiments demonstrate that our model has achieved superior performance over other state-of-the-art methods, serving as a potentially useful tool for digital pathology based on WSIs in the era of molecular pathology."
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,Fig. 1 .,
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,"2 ,",
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,Fig. 2 .,
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,Fig. 3 .,
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,Table 1 .,
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,Table 2 .,
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_52.
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,1,Introduction,"Depiction of white matter fiber tracts is of paramount importance for brain characterization in health and disease. Diffusion-weighted magnetic resonance imaging (dMRI) is the method of choice to study axon bundles that connect different brain regions. Several models have been proposed to map the 4-dimensional diffusion signal to objects such as tensors or fiber orientation distribution functions (FODs)  These acquisitions require prolonged scans that are not affordable for newborn and fetal subjects because of the sensitivity of these cohorts and the increased risk of motion. Acquisitions have to be fast to freeze in-plane motion; yet data dropout rates are high in these cohorts because of motion artifacts. Reconstructing FODs in developing brains has been performed  Deep learning models, first suggested in  To the best of our knowledge, no learning-based method to predict FODs has been reported for newborn and fetal brains. In this work, we demonstrate that a deep convolutional neural network with a large field of view (FOV) can accurately estimate FODs using only 6-12 diffusion-weighted measurements. Our contribution is three-fold. We first show that a deep learning method can achieve an accuracy level that is comparable with the agreement between the state-ofthe-art methods, while drastically reducing the number of measurements, for developing brains. We then show a low agreement between state-of-the-art methods in terms of different metrics using data from a highly controlled setting, namely the developing Human Connectome Project (dHCP). This addresses the need to build reproducible and reliable pipelines for white matter characterization "
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,2,Methodology,
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,2.1,Paradigm,The method is based on directly learning a mapping between the raw diffusion signal and the FOD in a supervised manner (Fig. 
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,2.2,Data Processing,"dHCP Newborns -We have selected two subsets from the developing Human Connectome Project (dHCP) dataset (1) 100 subjects (weeks:  Clinical Newborns and Fetuses -Acquisitions of 8 neonates ([38.1, 39.4, 40.1, 40.4, 40.7, 40.9, 41.8, 42] weeks), were performed during natural sleep at 3T (Siemens Trio and Skyra). Five b0 images and 30 b = 1000 s/mm 2 were acquired. The TR-TE were 3700-104 ms and voxel size was 2 mm isotropic. Eight fetal subjects ( "
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,2.3,Training,"Two networks, DL n and DL f (see Subsect. 2.1 above), were trained using Adam optimizer  We used 70% of the subjects for training, 15% for validation, and 15% for testing. We used the number of FOD peaks (extracted from Dipy "
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,2.4,Evaluation of dHCP Newborns,"Comparison with State-of-the-Art Methods -In addition to comparing our network (DL n ) prediction with FODs estimated using MSMT-CSD of 300 directions (considered as ground truth, GT), we have assessed the agreement between two mutually exclusive subsets extracted from the ground truth (gold standards 1 and 2, respectively GS1 and GS2). Each subset contains 150 directions (b ∈ {0, 400, 1000, 2600} s/mm 2 ) with respectively 10, 32, 44, and 64 measurements (half measurements of GT data). GS1 and GS2 subsets can be considered as independent high-quality scans, and differences in terms of subsequent metrics can be considered as an upper bound error for the different methods deployed. Furthermore, we have computed three state-of-the-art methods: (1) Constrained spherical deconvolution (CSD)  Error Metrics -Quantitative validation was performed based on the number of peaks, the angular error and the apparent fiber density (AFD) "
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,2.5,Evaluation of Clinical Datasets,"DL f was tested on fetal volumes whereas DL n was tested on the clinical newborn dataset. Due to the lack of ground truth for both clinical datasets, we qualitatively assess the network predictions with 12 and 6 measurements, as compared to CSD using all available measurements (SH-L max order 4 and 8), respectively for fetuses and newborns."
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,3,Results,"The networks consistently learned a mapping between the six/twelve diffusion measurements and the ground truth FOD constructed with 300 measurements across 4 b-values, as evaluated on the independent test data (Figs. "
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,3.1,In-Domain Quantitative Evaluation in Newborns dHCP,"Number of Peaks -We first observe a low agreement (AR) between the two gold standard acquisitions (GS1 vs. GS2), that is more pronounced for multiple fibers voxels. For instance, 1-peaks AR is 80.4%, 30.3% for 2-peaks and 27.9% for 3-peaks. SFM achieves a relatively high 1-peaks agreement with the GT of 83% and the lowest with multiple fibers voxels (10% and 3.5% for 2-and 3-peaks, respectively). In contrast, CSD estimates a high number of multiple fibers (16.5% and 5.9% for 1-and 2-peaks respectively) and achieves the lowest 1-peaks AR with 11.7%. In fact, the latter is biased towards multiple peaks estimation with more than 90% of the voxels modeled as either two or three peaks. This might be explained by the high b-value (b = 2600 s/mm 2 ) containing high levels of noise. Our method, DL n , achieves an agreement for 1-, 2-and 3-peaks of respectively 79%, 16% and 3% that is globally the closest to the agreement between the gold standards when compared to other methods. We believe that the relatively low agreement for multiple intravoxel fiber orientations is due to their incongruence across GT subjects, and hence the absence of a Table "
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,,Method b-values (s/mm,"It is worth noting that the agreement between the different methods (CSD vs. CSA, SFM vs. CSA, CSD vs. DL n , etc.) was also low. The confusion matrices for ΔGS agreement and the different methods can be found in Table  Angular Error -The agreement in terms of the number of peaks does not guarantee that the fibers follow the same orientation. Table  Apparent Fiber Density -The last column in Table "
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,3.2,Generalizability to Clinical Acquisitions (newborns and fetuses),DL f successfully generalized to fetal data as can be seen in Fig.  The radial coherence of cortical plate at early gestation 
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,4,Conclusion,"We have demonstrated how a deep neural network can successfully reconstruct high angular multi-shell FODs from a reduced number (6 to 12) of diffusion measurements. The substantially lower number of samples is compensated by learning from high-quality training data and by exploiting the spatial neighborhood information. The network was quantitatively evaluated on the dHCP dataset which was acquired in a highly controlled setting that cannot be reproduced in clinical settings. We showed that our method relying on six measurements can be leveraged to reconstruct plausible FODs of clinical newborn and fetal brains. We compared our model to commonly used methods such as CSD and MSMT-CSD between two gold standard datasets. The results exhibit low agreements between the different methods, particularly for multiple fiber orientations, despite using high angular multi-shell data. This highlights the need to build robust and reproducible methods for microstructure estimation in developing brains."
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,,Fig. 1 .,
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,,Fig. 2 .,
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,,Fig. 3 .,
Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 28.
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_48.
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,1,Introduction,"Esophageal cancer is a significant contributor to cancer-related deaths globally  In the clinical setting, patients may undergo a second round of RT treatment to achieve complete tumor control when initial treatment fails to completely eradicate cancer  Recently, advances in deep learning  In this paper, we present a comprehensive study on accurate GTV delineation for the second course RT. We proposed a novel prior Anatomy and RT information enhanced Second-course Esophageal GTV segmentation network (ARTSEG). A region-preserving attention module (RAM) is designed to effectively capture the long-range prior knowledge in the esophageal structure, while preserving regional tumor patterns. To the best of our knowledge, we are the first to reveal the domain gap between the first and second courses for GTV segmentation, and explicitly leverage prior information from the first course to improve GTV segmentation performance in the second course. The medical images are labeled sparsely, which are isolated by different tasks  nature of GTV on esophageal locations. To achieve this, we efficiently exploit knowledge from multi-center datasets that are not tailored for second-course GTV segmentation. Our training strategy does not specific to any tasks but challenges the network to retrieve information from another encoder with augmented inputs, which enables the network to learn from the above three aspects. Extensive quantitative and qualitative experiments validate our designs."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,2,Network Architecture,"In the first course of RT, a CT image denoted as I 1 is utilized to manually delineate the esophageal GTV, G 1 . During the second course of RT, a CT image I 2 of the same patient is acquired. However, I 2 is not aligned with I 1 due to soft tissue movement and changes in tumor volume that occurred during the first course of treatment. Both images I 1/2 have the spatial shape of H × W × D. Our objective is to predict the esophageal GTV G 2 of the second course. It would be advantageous to leverage insights from the first course, as it comprises comprehensive information pertaining to the tumor in its preceding phase. Therefore, the input to encoder E 1 consists of the concatenation of I 1 and G 1 to encode the prior information (features f d 1 ) from the first course, while encoder E 2 embeds both low-and high-level features f d 2 of the local pattern of I 2 (Fig.  where the spatial shape of , with 2 d+4 channels. Region-Preserving Attention Module. To effectively learn the prior knowledge in the elongated esophagus, we design a region-preserving attention module (RAM), as shown in Fig.  Since MHA perturbs the positional information, we preserve the tumor local patterns by concatenating original features to the attentive features at the channel dimension, followed by a 1 × 1 × 1 bottleneck convolution ξ 1×1×1 to squeeze the channel features (named as RAM), as shown in the following equations, where the lower-level features from both encoders are fused by concatenation. The decoder D generates a probabilistic prediction ) with skip connections (Fig. "
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,3,Training Strategy,"The network should learn from three aspects: 1) Tumor volume variation: the structural changes of the tumor from the first to the second course; 2) Cancer cell proliferation: The tumor in esophageal cancer tends to infiltrate into the adjacent tissue; 3) Reliance of GTV on esophageal anatomy: The anatomical dependency between esophageal GTV and the position of the esophagus. Medical images are sparsely labeled which are isolated by different tasks  In order to fully leverage both public and private datasets, the training objective should not be specific to any tasks. Here, we denote G 1 /G 2 as prior/target annotations respectively, which are not limited only to the GTV areas. As shown in Fig. "
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,3.1,Tumor Volume Variation,"The differences in tumor volume between the first and second courses following an RT treatment can have a negative impact on the state-of-the-art (SOTA) learning-based techniques, which will be discussed in Sect. 4.2. To adequately monitor changes in tumor volume and integrate information from the initial course into the subsequent course, a paired first-second courses dataset S p = {i 1  p , i 2 p , g 1 p ; g 2 p } is necessary for training. In S p , i 1 p and i 2 p are the first and second course CT images, while g 1 p and g 2 p are the corresponding GTV annotations."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,3.2,Cancer Cell Proliferation,"The paired dataset S p for the first and second courses is limited, whereas an unpaired GTV dataset S v = {i v ; g v } can be easily obtained in a standard clinical workflow with a substantial amount. S v lacks its counterpart for the second course, in which i v /g v are the CT image and the corresponding annotation for GTV. To address this, we apply two distinct randomized augmentations, P 1 , P 2 , to mimic the unregistered issue of the first and second course CT. The transformed data is feed into the encoders E 1/2 as shown in the following equations: , P 1 (g e ), P 2 (i e ), P 2 (g e ), when i e , g e ∈ S e . (4) The esophageal tumor can proliferate with varying morphologies into the surrounding tissues. Although not paired, S v contains valuable information about the tumor. Challenging the network to query information within GTV will enhance the capacity to retrieve pertinent information for the tumor positions."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,3.3,Reliance of GTV on Esophageal Anatomy,"To make full use of the datasets of relevant tasks, we incorporate a public esophagus segmentation dataset, denoted as S e = {i e ; g e }, where i e /g e represent the CT images and corresponding annotations of the esophagus structure. By augmenting the data as described in Eq. (  In summary, our training strategy is not dataset-specific or target-specific, thus allowing the integration of prior knowledge from multi-center esophageal GTV-related datasets, which effectively improves the network's ability to retrieve information for the second course from the three key aspects stated in Sect. 3."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4,Experiments,
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.1,Experimental Setup,"Datasets. The paired first-second course dataset, S p , is collected from Sun Yat-Sen University Cancer Center (Ethics Approval Number: B2023-107-01), comprising paired CT scans of 69 distinct patients from South China. We collected the GTV dataset S v from MedMind Technology Co., Ltd., which has CT scans from 179 patients. For both S p and S v , physicians annotated the esophageal cancer GTV in each CT. The GTV volume statistics (cm 3 , mean ± std.) in S v is 40.60 ± 29.75, and is 83.70 ± 55.97/71.66 ± 49.36 for the first/second course RT in S p respectively. Additionally, we collect S e from SegTHOR  Performance Metrics. Dice score (DSC), averaged surface distance (ASD) and Hausdorff distance (HSD) are used as metrics for evaluation. The Wilcoxon signed-rank test is used to compare the performance of different methods."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.2,Domain Gap Between the First and Second Course,"As previously mentioned, the volume of the tumors changes after the first course of RT. To demonstrate the presence of a domain gap between the first and second courses, we train SOTA methods with datasets S train p and S v , by feeding the data sequentially into the network. We then evaluate the models on S test p . The results presented in Table  Figure "
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.3,Evaluations of Second-Course GTV Segmentation Performance,"Combination of Various Datasets. Table  By subsequently incorporating S e for structural esophagus prior knowledge, the DSC improved to 69.42%. Meanwhile, the esophageal tumor comprises two primary regions, the original part located in the esophagus and the extended part that has invaded the surrounding tissue. As shown in Fig.  When S v is incorporated for learning tumor proliferation, the DSC improved to 72.64%. We can observe from Case 2 in Fig.  Region-Preserving Attention Module. Although introducing the esophageal structural prior knowledge using S e can improve the performance in DSC and ASD (Table  However, there is no performance gain with MHA as shown in Table  To tackle the aforementioned problem, we propose RAM which involves the concatenation of the original features with attention outputs, allowing for the preservation of convolution-generated regional tumor patterns while effectively comprehending long-range prior knowledge specific to the esophagus. Finally, our proposed ARTSEG with RAM achieves the best DSC/HSD of 75.26%/19.75 mm, and outperforms its ablations as well as other baselines, as shown in Table  Limitations. For the method's generalizability, analysis of diverse imaging protocols and segmentation backbones are inadequate. Besides, ARTSEG requires more computational resources due to its dual-encoder and attention design."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,5,Conclusion,"In this paper, we reveal the domain gap between the first and second courses of RT for esophageal GTV segmentation. To improve the accuracy of GTV declination in the second course, we explicitly incorporated the naturally existing prior information from the first course. Besides, to efficiently leverage prior knowledge contained in various medical CT datasets, we train the network in an information-querying manner. We proposed RAM to capture long-range prior knowledge in the esophageal structure, while preserving the regional tumor patterns. Our proposed ARTSEG incorporates prior knowledge of the tumor volume variation, cancer cell proliferation, and reliance of GTV on esophageal anatomy, which enhances the GTV segmentation accuracy in the second course RT. Our future research includes accurate delineation for multiple targets in the second course and knowledge transferring through the time series of multiple courses."
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,,Fig. 1 .,
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,,Fig. 2 .,
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,,Fig. 3 .,
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,,Table 1 .,
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,,Table 2 .,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,1,Introduction,"Lung cancer is one of the most fatal diseases worldwide, and early diagnosis of the pulmonary nodule has been identified as an effective measure to prevent lung cancer. Deep learning-based methods for lung nodule classification have been widely studied in recent years  However, the aforementioned methods still face challenges in distinguishing visually similar samples with adjacent rank labels. For example, in Fig.  To integrate text annotations into the image-domain learning process, an effective text encoder providing accurate textual features is required. Fortunately, recent advances in vision-language models, such as contrastive languageimage pre-training (CLIP)  The contributions of this paper are summarized as follows. 1) We propose CLIP-Lung for lung nodule malignancy prediction, which leverages clinical textual knowledge to enhance the image encoder and classifier. 2) We design a channel-wise conditional prompt module to establish consistent relationships among the correlated text tokens and feature maps. 3) We simultaneously align the image features with class and attribute features through contrastive learning while generating more explainable attention maps. "
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,2,Methodology,"m=1 is the set of attribute embeddings, where each element a m ∈ R d×1 is a vector representing the embedding of an attribute word such as ""spiculation"". Then, for a given sample {I i , y i }, our aim is to learn a mapping f θ : I i → y i , where f is a deep neural network parameterized by θ. CLIP-Lung. In Fig. "
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,2.2,Instance-Specific Attribute Weighting,"For the attribute annotations, all the lung nodules in the LIDC-IDRI dataset are annotated with the same eight attributes: ""subtlety"", ""internal structure"", ""calcification"", ""sphericity"", ""margin"", ""lobulation"", ""spiculation"", and ""texture""  where v m denotes the annotated value for a m . Then the weight vectors of the i-th sample is represented as . Hence, the elementwise multiplication w i • A i is unique to I i ."
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,2.3,Channel-Wise Conditional Prompt,CoCoOp 
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,2.4,Textual Knowledge-Guided Contrastive Learning,"Recall that our aim is to enable the visual features to be similar to the textual features of the annotated classes or attributes and be dissimilar to those of irrelevant text annotations. Consequently, we accomplish this goal through contrastive learning  Image-Class Alignment. First, the same to CLIP, we align the image and class information by minimizing the cross-entropy (CE) loss for the sample {I i , y i }: where and "" "" denotes concatenation, i.e., C k,: is conditioned on learnable prompts l t + l t . σ(•, •) calculates the cosine similarity and τ is the temperature term. Therefore, L IC implements the contrastive learning between channel-wise features and corresponding class features, i.e., the ensemble of grouped image-class alignment results. Image-Attribute Alignment. In addition to image-class alignment, we further expect the image features to correlate with specific attributes. So we conduct image-attribute alignment by minimizing the InfoNCE loss  Hence, L IA indicates which attribute the F t,: is closest to since each vector F t,: is mapped from the t-th group of feature maps through the context network h(•). Therefore, certain feature maps can be guided by specific annotated attributes. Class-Attribute Alignment. Although the image features have been aligned with classes and attributes, the class embeddings obtained by the pre-trained CLIP encoder may shift in the latent space, which may result in inconsistent class space and attribute space, i.e., annotated attributes do not match the corresponding classes, which is contradictory to the actual clinical diagnosis. To avoid this weakness, we further align the class and attribute features: and this loss implies semantic consistency between classes and attributes. Finally, the total loss function is defined as follows: where α and β are hyperparameters for adjusting the losses and are set as 1 and 0.5, respectively. L CE denotes the cross-entropy loss between predicted probabilities obtained by the classifier and the ground-truth labels. Note that during the inference phase, test images are only fed into the trained image encoder and classifier. As a result, CLIP-Lung does not introduce any additional computational overhead in inference."
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,3,Experiments,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,3.1,Dataset and Implementation Details,"Dataset. LIDC-IDRI  Experimental Settings. In this paper, we apply the CLIP pre-trained ViT-B/16 as the text encoder for CLIP-Lung, and the image encoder we used is ResNet-18 "
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,3.2,Experimental Results and Analysis,"Performance Comparisons. In Table  CoCoOp and CLIP-Lung alleviate this phenomenon, which demonstrates that the learnable prompts guided by nodule classes are more effective than fixed prompt engineering. Unlike CLIP-Lung, CoCoOp does not incorporate attribute information in prompt learning, leading to increased false negatives in the latent space. From the attention maps, we can observe that CLIP cannot precisely capture spiculation and lobulation regions that are highly correlated with malignancy. Simultaneously, our CLIP-Lung performs better than CoCoOp, which demonstrates the guidance from textual descriptions such as ""spiculation"". Ablation Studies. In Table "
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,4,Conclusion,"In this paper, we proposed a textual knowledge-guided framework for pulmonary nodule classification, named CLIP-Lung. We explored the utilization of clinical textual annotations based on large-scale pre-trained text encoders. CLIP-Lung aligned the different modalities of features generated from nodule classes, attributes, and images through contrastive learning. Most importantly, CLIP-Lung establishes correlations between learnable prompt tokens and feature maps using the proposed CCP module, and this guarantees explainable attention maps localizing fine-grained clinical features. Finally, CLIP-Lung outperforms compared methods, including CLIP on LIDC-IDRI benchmark. Future work will focus on extending CLIP-Lung with more diverse textual knowledge."
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,Fig. 1 .,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,Fig. 2 .,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,Fig. 3 .,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,2.1 Overview Problem Formulation. In,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,Table 1 .,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,Table 2 .,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,Table 3 .,
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,,Acknowledgements. This work was supported in part by,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,1,Introduction,"Atrial fibrillation (AF) has been one of the most common types of cardiovascular diseases and is closely related to the left atrium (LA)  Till now, many researches have focused on the automatic segmentation of the LA  Diffusion is a physical model aimed at minimizing the spatial concentration difference  (1) We introduce a new LA & LAA CT dataset. And as far as we are concerned, this is the first work based on deep neural networks, to model relative relations between the LA and LAA. "
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,2,Methodology,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,2.1,Preliminaries,"Diffusion is a physical phenomenon, in which molecules spread from regions with higher concentrations toward regions with lower concentrations  where D is the diffusivity function determining the diffusion speed along each direction, ∇ is the gradient operator. In our application, the stable state of F (t) will show a more accurate localization for uncertain boundaries of the LAA. Linear isotropic diffusion (D is equal to a constant) cannot be applied to complex scenes, because the diffusion velocity is the same in all directions. For a spatial-dependent function D = D(x, y, z), the process is linear anisotropic. However, if we aim to extract refined boundary features, adopting linear diffusion processes will smooth both backgrounds and the edges. A more feasible solution is to devise complex diffusion functions D = D(F ) with nonlinear characteristics  Detailedly, given a feature F where regions of uncertain boundaries are not highlighted, it is updated by the diffusion process in infinite time. The diffusion adjacent to ambiguous boundaries should be restrained, while the diffusion far away from boundaries is promoted. And the final state of the diffused feature will accurately localize uncertain boundaries of the LAA. "
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,2.2,Semantic Difference Module,"To localize fuzzy boundaries of the LAA, especially the interface between the LA and LAA, we propose the semantic difference module (SDM) to refine boundary features from these regions. Motivated by the diffusion process, we formulate the process of enhancing boundary features as solving a second-order partial differential equation. Due to the fact that semantic information is required to guide the localization of uncertain boundaries, we introduce the deep feature G from the precedent decoder layer to the diffusion process in each SDM. Here we adopt ∇G as the semantic guidance map. And the square term h(|∇G| 2 ) is deployed as function D to model nonlinear characteristics of the diffusion process, where h is a projection function. In terms of  where p is the index of feature maps, δ p is the local neighborhood centered at p, λ and ν are weighting coefficients. Indeed, F t p -F t p is the differential information of original feature F t at point p, representing abundant boundary information, which contains complicated boundary features of anatomies as shown in Fig.  We design the semantic difference module based on Eq. 3 as illustrated by Fig.  where α p and β p refer to the learnable edge operator for feature F and semantic feature G respectively. And ω p means a vanilla 3 × 3 × 3 convolution kernel."
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,2.3,Connectivity-Refined Network,"Due to the CT imaging noise, some cases show the phenomenon that the LAA is separated from the LA. To deal with disconnections between the LA and LAA, we propose the second-stage network called connectivity-refined network (CRN). Inspired by the metric of 95% Hausdorff distance (HD 95 )  where D means the Euclidean distance, σ is the sigmoid function. S is a scaling coefficient, and we set it as 20 according to the ablation study on this hyperparameter (Please refer to supplementary material for more quantitative results). Besides, λ is set as 1 if the training epoch reaches more 300 epochs, or it is 0. When there is no LAA predictions in a cropped patch, L c is equal to 0. On the ground that cases with a poor connectivity need to be refined in the training process of CRN, we increase sampling ratios of the whole LAA region from coarse masks of validation datasets. By cropping patches containing the boundary interface as network inputs, CRN will better learn the connectivity prior from the mapping between coarse predictions and expert annotations. In the inference stage, final decoded features F d are applied with the softmax operator to attain predicted masks. However, different channels of F d bear different maximum values, which will affect segmentation performance. Thus, we propose the concept of channel calibration (CC). Before per-pixel selecting maximum values between channels, we uniform the maximum value of F d channel by channel."
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,3,Experiment,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,3.1,Experimental Settings,"Dataset. To evaluate the performance of our network, we conduct experiments on a new dataset, containing accurate annotations of LA and LAA provided by multiple experts. In detail, we collect 80 CT scans from 80 patients, which are split into 45/15/20 for training, validation and testing cases in Stage 1. In Stage 2, 50 predictions of the validation dataset in Stage 1 are generated by various models, in which there are 30 predicted masks with disconnections. Then we randomly split them as 35/15 for training and validation. Moreover, we choose the Dice score and HD 95 as quantitative metrics. Implementation Details. In the first stage, we choose the vanilla 3D UNet as our baseline model, trained for 2000 epochs. And we utilize a combination of cross entropy loss and Dice loss followed by  And we utilize Dice loss and connectivity loss as illustrated by Eq. 6. We train all models using AdamW optimizer. With the linear warm-up strategy, the initial learning rate is set as 5e-4 with a cosine learning rate decay scheduler, and weight decay is set as 1e-5. The size of cropped patches is 160 × 160 × 192. All models are implemented based on Pytorch and trained on 2 NVIDIA Tesla V100 GPUs."
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,3.2,Experimental Results,"Table  Another phenomenon worth to mention is that our model shows an ordinary performance on the HD 95 of LAA, which is owing to the appearance of outliers. And not only our model but other CNNs and Transformer-based models suffer from the existence of outliers in predictions (More visualization results can be found in supplementary material). We will address this issue in the future research. In Table "
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,3.3,Ablation Studies,"Table  For the first row, our model give a more accurate localization for the boundary interface, which proves the effectiveness of SDM. The second row is a strong proof that CRN can effectively improve the connectivity between LA and LAA. Then we probe into the efficacy of connectivity loss L c by removing it from CRN, which results in a 0.51mm increase on the HD 95 of LAA, which is reasonable because L c is indeed a distance regularization on LAA boundaries. In Table  (1) Without learnable difference kernels for differential maps of feature F and semantic feature G, the Dice score for LAA declines sharply, which reveals we need to focus on the anisotropic distribution of this dataset. "
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,4,Conclusion,"In this paper, we introduce a new CT dataset on LA and LAA, then carry out the segmentation task. Detailedly, we explain the refined process for the segmentation of uncertain boundaries via diffusion theory. Based on this, we apply SDM to successfully improve the segmentation for uncertain boundaries between LA and LAA. Then CRN with the connectivity loss can deal with the poor connectivity between two structures. Detailed quantitative and qualitative results have demonstrated the efficacy of two proposed elements."
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,Fig. 1 .,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,Fig. 2 .,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,Fig. 3 .,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,Table 1 .,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,both sides of '|' represent evaluation metrics for each stage of our model,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,indicates that loss func- tions do not change Parameters and FLOPs of the baseline model).,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,130 81.85 95.96 88.91 ±6.9 8.87 6.63 7.75 ±2,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,Table 2 .,
Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 12.
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,1,Introduction,"Pulmonary diseases pose significant health risks, and computed tomography (CT) analysis of pulmonary airways and vessels has become a valuable clinical tool for revealing tomographic patterns  In recent years, deep learning methods have spawned research on airway and vessel segmentation. Convolutional neural networks (CNNs) have been widely employed in various existing studies to learn robust and discriminative features for automatic airway/artery/vein segmentation  In this paper, we formulate the problem of disconnected pulmonary tubular structures as a key point detection task. The primary objective is to repair the topology structures of two disconnected components by accurately identifying the centers of the disconnected parts located at both ends of the components. Endpoints corresponding to the broken centerline of the pulmonary tubular structure are treated as two key points. The identification of these key points is critical in recognizing disconnections in pulmonary tubular structures for diagnosing pulmonary diseases, which has significant research implications. To address this issue, we propose a training data synthesis pipeline that generates disconnected data from complete pulmonary structures. We further explore the training strategy and thus build a strong basline based on 3D-UNet to predict the key points that can bridge disconnected components. Our contributions can be briefly summarized as follows: -A novel formulation of a practical research problem: We have formulated the problem of pseudo disconnection pulmonary tubular structures as a key point detection task, which is a significant contribution to the field as it has not been extensively explored before. "
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,2,Method,"In this section, we present a comprehensive analysis of our approach for detecting pulmonary tubular interruptions as a keypoint detection task. We start by formulating the problem, followed by a description of the data simulation process used to construct the dataset. The dataset construction process is explained in detail to provide insight into the methods used for generating realistic data samples. We then introduce the simple two-channel 3D-UNet, and describe its architecture, key features, training objective, and implementation details."
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,2.1,Problem Formulation,"Segmentation of thoracic tubular structures, such as airways and vessels, from lung Computed Tomography (CT) scans is vital for diagnosing pulmonary diseases. Over the years, various deep learning-based segmentation methods have demonstrated the potential of Convolutional Neural Networks (CNNs) in handling this task. However, accurately segmenting pulmonary airways, arteries, and veins without interruption remains challenging due to the unique properties of the thoracic tubular structure. The trachea and blood vessels constitute only a small fraction of the whole thoracic CT image, which leads to severe class imbalance between the tubular foreground and background, hindering 3-D CNNs learning from sufficient supervisory signals "
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,2.2,Training Data Synthesis,"We generated synthetic data from lung CT scans with carefully annotated pulmonary airways, arteries, and veins, as no public medical dataset was available for the task at hand. The synthetic data simulates the scenario of vascular/trachea disconnection and serves as a benchmark dataset for the keypoint detection task. To generate the data, binary masks of the tubular structures were extracted from 800 CT scans "
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,2.3,The Keypoint Detection Network,"The framework and training pipeline of our network are depicted in Fig.  Data Sampling. The generated raw data is too large for training the network due to the high-resolution nature of CT scans, which have dimensions of 512 × 512 for the x-y plane and variable dimensions for the z plane. Directly feeding the entire 3-D volume into the network can cause significant memory overhead and slow down the training convergence, especially with limited computing resources. Therefore, we crop a subvolume with a size of 80 × 80 × 80 around where the disconnection occurs from the original volume. Specifically, since the location of interrupted blood vessels cannot be known in advance and the small connected component where KP 2 is located can be found using morphological operations, we randomly select a point in that small object as the center point of our subvolume. This approach also serves as a new form of data augmentation. For each selected branch in an original volume, we randomly crop one subvolume for training purposes and three subvolumes for validation and testing. Network Design. We propose an encoder-decoder network that is based on the widely used 3D U-Net architecture. As depicted in Fig.  The network receives two binarized subvolumes I ∈ R 2×D×H×W as inputs, where D, H, W represent the spatial dimensions of the cropped volume. In this study, we decided to set the output heatmaps to the same size as the inputs, without downsampling, in order to avoid the loss of coordinate accuracy. In the neural network design phase, we prioritized formulating the problem, constructing an open-source dataset, and proposing a comprehensive training and testing pipeline. We refrained from incorporating sophisticated modules, such as attention mechanisms, transformer blocks, or distillation, and fine-tuning hyper-parameters. Hence, we do not delve into detailed network architecture design in this paper. However, we obtained promising results using a simple two-channel 3D-UNet model and explored various training techniques. Our work lays a solid foundation for future researchers to improve upon our findings by incorporating advanced techniques and innovative modules. Loss Function. We adopt the state-of-the-art keypoint detection framework to represent the problem as heatmap estimation, where the coordinate with the highest confidence in each heatmap of H ∈ R k×D×H×W corresponds to the location of the kth keypoint. The ground-truth heatmaps are generated by placing a 3D Gaussian kernel at the center of each ground-truth keypoint location. For simplicity, we define the Keypoint Mean-Squared Error (KMSE) loss function as follows: where H k and Ĥk refer to the ground-truth and the predicted heatmaps for the kth keypoint, and K is fixed to 2 in our study. To reduce memory cost, we limit the size of subvolumes to 80 × 80 × 80, which may result in invisible keypoints if the branch is long and the two keypoints are too far apart. Here, V k indicates the visibility of the ground truth keypoint, where V k = 1 and δ(V k ) = 1 if the keypoint is visible, and vice versa. Implementation Details. During the training phase, we employ a sampling strategy that randomly crops one volume, which introduces data augmentation, mitigates overfitting, and ensures training convergence. To reduce testing time and enable fair comparisons between models, we generate and save three random crops for each vessel branch during validation and testing. The size of the groundtruth heatmaps is 80 × 80 × 80, and the sigma of the 3D Gaussian kernel used to generate them is set to 2.5. All networks were trained using AdamW optimizer with a learning rate of 0.0001 and beta hyperparameters of 0.5 and 0.999. The training was performed on a single NVIDIA 3090ti GPU with a batch size of 16. PyTorch framework was used for implementation, and early stopping strategy was adopted to prevent overfitting. To speed up training, we initialize the artery and vessel models with the trained airway model. We combined artery and vein training data to increase the training samples and reduce the training time."
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,2.4,Model Inference,"Models trained using our proposed training paradigm may not be directly applicable to real-world data due to several assumptions made during training. Specifically, during training, we assume that the interrupted segmentation mask consists of only two continuous components representing KP 1 and KP 2 , and that the location of KP 2 is known a priori, which is used to randomly crop subvolumes. Additionally, we limit the subvolume's size to ensure efficient training. However, in real-world scenarios, the location of KP 1 and KP 2 components is unknown, and there may be small disconnected objects and noises scattered throughout the volume's entire original size. The only prior knowledge available is that KP 1 is located in the volume's largest connected component (i.e., the main vessel/airway), and KP 2 is in one of the small isolated components. To address this issue, we have developed an algorithm that bridges the gap between model training and inference, and accurately predicts disconnections in realworld situations. The pseudo-code of the inference algorithm is detailed in the supplementary materials."
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,3,Experiments,
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,3.1,Datasets,"A total of 800 CT scans with annotations of pulmonary airways, arteries, and veins are utilized to construct our dataset. The CT scans from multiple medical centers are manually annotated by a junior radiologist and confirmed by a senior radiologist "
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,3.2,Evaluation Metrics,"Based on the definition of Object Keypoint Similarity (OKS) in pose estimation tasks, we have adapted this metric to align with the features of our dataset. Our modifications to the OKS are reflected in the following metric formulations: Here d k is the Euclidean distance between the predicted keypoint and the ground-truth keypoint, along with the vessel volume S of the corresponding branch. To maintain a consistent scale for OKS, we have introduced λ, a constant which we set to 0.2. OKS k refers to the OKS of kth keypoint (k = 2 in our study). where OKS i denotes the OKS of ith sample and V k is the visibility flag. where we use standard AP τ , which measures prediction precision of a model given a specific threshold τ . In order to provide a comprehensive and nuanced evaluation of the model's performance, we report average precision across various thresholds. Specifically, we report AP 50 (AP at τ = 0.5), AP 75 (AP at τ = 0.75), AP (the mean AP across 10 τ positions, where τ = {0.5, 0.55, ..., 0.95}), AP S (for small vessels with edge radius within the range of (0, 2]), AP M (for medium vessels with edge radius within the range of (2, 3]), AP L (for large vessels with edge radius greater than 3), AP k1 (AP for KP 1 ), AP k2 (AP for KP 2 ), E d (mean "
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,3.3,Results,"To analyze the performance of our methods on topology repairing of disconnected pulmonary airways and vessels, we report several methods on the proposed PTR dataset, as shown in Table  The study demonstrates that the two-channel 3D-UNet model surpasses the performance of the one-channel counterpart on airway and vessel segmentation tasks. Specifically, the two-channel model yields significant improvements in AP of approximately 7%, 9%, and 15% for airway, artery, and vein tasks, respectively. Additionally, the two-channel model achieves the highest performance on all evaluation metrics for all three tasks. These results suggest that the separation of KP 1 and KP 2 components as two-channel input can effectively improve their interaction in multiple feature levels, leading to improved performances. This is likely due to the high correlation between these two keypoints throughout the topological structure. However, detecting KP 1 was significantly challenging due to the random selection of cropping center points during data sampling, leading to a weaker performance for E d and AP metrics. Additionally, the sparse distribution of keypoints on small pulmonary vessels posed a considerable challenge for capturing subtle features. Notably, the two-channel networks exhibited superior performance over one-channel methods by a substantial margin, which emphasizes the advantages of separating the two components. In the future work, it will be beneficial to design models that capture this characteristic."
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,4,Conclusion,"In this study, we introduce a data-driven post-processing approach that addresses the challenge of disconnected pulmonary tubular structures, which is crucial for the diagnosis and treatment of pulmonary diseases. The proposed approach utilizes the newly created Pulmonary Tree Repairing (PTR) dataset, comprising 800 complete 3D models of pulmonary structures and synthetic disconnected data. A two-channel simple yet effective neural network is trained to detect keypoints that bridge disconnected components, utilizing a training data synthesis pipeline that generates disconnected data from complete pulmonary structures. Our approach yields promising results and holds great potential for clinical applications. While our study primarily focuses on addressing the disconnection issue, we recognize that more complex scenarios, such as handling multiple disconnected components, distinguishing between arteries and veins, and implementing our method in real-world settings, require further investigation in future work. Point or implicit representations "
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,,Fig. 1 .,
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,,Fig. 2 .,
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,,-An effective yet simple baseline with efficient 3D-UNet: We,
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,,Table 1 . Keypoint detection performance on the PTR dataset.,
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,,Acknowledgment. This research was supported by Australian Government Research,"Training Program (RTP) scholarship, and supported in part by a "
Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 36.
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,1,Introduction,"Skin cancer is a serious and widespread form of cancer that requires early detection for successful treatment. Computer-aided diagnosis systems (CAD) using deep learning models have shown promise in accurate and efficient skin lesion diagnosis. However, recent research has revealed that the success of these models may be a result of overly relying on ""spurious cues"" in dermoscopic images, such as rulers, gel bubbles, dark corners, and hairs  To alleviate the artifact bias and enhance the model's generalization ability, we rethink the problem from the domain generalization (DG) perspective, where a model trained within multiple different but related domains are expected to perform well in unseen test domains. As illustrated in Fig.  Previous DG algorithms learning domain-invariant features from source domains have succeeded in natural image tasks  To overcome the above problems, we propose an environment-aware prompt vision transformer (EPVT) for domain generalization of skin lesion recognition. On the one hand, inspired by the emerging prompt learning techniques that embed prompts into a model for adaptation to diverse downstream tasks  Our contributions can be summarized as:  (3) A domain mixup strategy is devised to reduce the co-artifacts specific to dermoscopic images; (4) Extensive experiments on four out-of-distribution skin datasets and six biased ISIC datasets demonstrate the outperforming generalization ability and robustness of EPVT under heterogeneous distribution shifts."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,2,Method,"In domain generalization (DG), the training dataset D train consists of M source domains, denoted as D train = {D k |k = 1, ..., M }. Here, each source domain D k is represented by n labeled instances {(x k j , y k j )} n j=1 . The goal of DG is to learn a model G : X → Y from the M source domains so that it can generalize well in unseen target domains D test . The overall architecture of our proposed model, EPVT, is shown in Fig. "
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,2.1,Domain-Specific Prompt Learning with Vision Transformer,"To enable the pre-trained vision transformer (ViT) to capture knowledge from different domains, as shown in Fig.  , where d is the same size as the feature embedding of the ViT and each prompt P m corresponds to one domain (i.e. dark corners). To incorporate these prompts into the model, we follow the conventional practice of visual prompt tuning  where F is the feature encoder of the ViT, X 0 denotes the class token, E 0 is the image patch embedding, F m is the feature extracted by ViT with the m-th prompt, and 0 is the index of the first layer. Domain prompts P D are a set of learnable tokens, with each prompt P m being fed into the vision transformer along with the image and corresponding class tokens from a specific domain. Through optimizing, each prompt becomes a domain expert only responsible for the images from its own domain. By the self-attention mechanism of ViT, the model can effectively capture domain-specific knowledge from the domain prompt tokens."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,2.2,Cross-Domain Knowledge Learning,"To facilitate effective knowledge sharing across different domains while maintaining its own parameters of each domain prompt, we propose a domain prompt generator, as depicted in Fig.  where P m represents the domain-specific prompt, computed by Hadamard product of P * and P k . Here, P * ∈ R s×d is utilized to learn general knowledge, with s and d representing the dimensions of the prompt vector and feature embedding respectively. On the other hand, P k is computed using domain-specific trainable vectors: u k ∈ R s and v k ∈ R d . These vectors capture domain-specific information in a low-rank space. The decomposition of domain prompts into rank-one subspaces ensures that the model effectively encodes domain-specific information. By using the Hadamard product, the model can efficiently leverage cross-domain knowledge for target domain prediction."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,2.3,Mitigating the Co-artifacts Issue,"The artifacts-based domain labels can provide domain information for dermoscopic images. However, a non-trivial issue arises due to the possible cooccurrence of different artifacts from other domains within each domain. To address this issue, we employ a domain mixup strategy  where x mix = λx k i +(1-λ)x q j ; x k i and x q j are samples from two different domains k and q, and y k i and y q j are the corresponding labels. This strategy can overcome the challenge of ambiguous domain labels in dermoscopic images and improve the performance of our model."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,2.4,Optimization,"So far, we have introduced L mixup in Eq. 3 for optimizing our model. However, since our goal is to generalize the model to unseen environments, we also need to take advantage of each domain prompt. Instead of assigning equal weights to each domain prompt, we employ an adapter  where A represents an adapter containing a two-layer MLP with a softmax layer, and w m denotes the learned weights. To train the adapter A, we simulate the inference process for each image in the source domain by treating it as an image from the pseudo-target domain. Specifically, we first extract features from the ViT: Fm (x) = F ([X 0 , E 0 ]). Then we calculated the adapted prompt P adapted for the pseudo-target environment image x using the adapter A: P adapted = A( Fm (x)). Next, we extract features from ViT using the adapted prompt: Fm (x) = F ([ Fm (x), P adapted , E 0 ]). Finally, the classification head H is applied to predict the label y: y = H( Fm (x)). Additionally, the inferece process is the same as the simulated inference process and our final prediction will be conditioned on the adapted prompt P adapted . To ensure that the adapter learns the correct linear correlation between the domain prompts and the target image, we use the domain label from source domains to directly supervise the weights w m . We also use the cross-entropy loss to maintain the model performance with the adapted prompt: (5) where Fm (x) is the obtained feature map conditioned on the adapted prompt P adapted , and H is the classification head. The total loss is then defined as L total = L mixup + L adapted ."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,3,Experiments,"Experimental Setup: We consider two challenging melanoma-benign classification settings that can effectively evaluate the generalization ability of our model in different environments and closely mimic real-world scenarios. (1) Outof-distribution evaluation: The task is to evaluate the model on test sets that contain different artifacts or attributes compared to the training set. We train and validate all algorithms on ISIC2019  Implementation Details: For a fair comparison, we train all models using ViT-Base/16  Out-of-Distribution Evaluation: Table "
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,,Ablation Study:,"We perform ablation studies to analyze each component of our model, as shown in Table  Trap Set Debiasing: In Fig.  The graph shows that the ERM baseline performs better than our EPVT when the bias is low (0 and 0.3). However, this is because ERM relies heavily on spurious correlations between artifacts and class labels, leading to overfitting on the training set. As the bias degree increases, the correlation between artifacts and class labels decreases, and overfitting the train set causes the performance of ERM to drop dramatically on the test set with a significant distribution difference. In contrast, our EPVT exhibits greater robustness to different bias levels. Notably, our EPVT outperforms the ERM baseline by 9.4% on the bias 1 dataset. Prompt Weights Analysis: To verify whether our model has learned the correct domain prompts for target domain prediction, we analyze and plot the results in Fig.  We then calculate the Frechet distance "
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,4,Conclusion,"In this paper, we propose a novel DG algorithm called EPVT for robust skin lesion recognition. Our approach addresses the co-artifacts problem using a domain mixup strategy and cross-domain learning problems using a domain prompt generator. Compared to other competitive domain generalization algorithms, our method achieves outstanding results on three out of four OOD datasets and the second-best on the remaining one. Additionally, we conducted a debiasing experiment that highlights the shortcomings of conventional training using empirical risk minimization, which leads to overfitting in dermoscopic images due to artifacts. In contrast, our EPVT model effectively reduces overfitting and consistently performs better in different biased environments."
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,,Fig. 1 .,
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,,Fig. 2 .,
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,,Fig. 3 .,
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,,Table 1 .,
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,,Table 2 .,
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_24.
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,1,Introduction,"Computer-aided Diagnosis (CAD) systems have achieved success in many clinical tasks  In order to overcome above challenges, some studies  Considering these real-world challenges, we propose a multi-label model named Self-feedback Transformer (SFT), and validate our method on a realworld pNENs dataset. The main contributions of this work are listed: 1) A transformer multi-label model based on self-feedback mechanism was proposed, which provided a novel method for multi-label tasks in real-world medical application; 2) The structure is flexibility and interactivity to meet the needs of realworld clinical application by using four inference modes, such as expert-machine combination mode, etc.; 3) SFT has good noise resistance, and can maintain good performance under noisy label input in expert-assisted mode."
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,2,Method,Transformer has achieved success in many fields 
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,2.1,Transformer-Based Multi-label Model,"Image Embeddings F . Given input image x ∈ R L×W ×H , the feature vector k ∈ R C is extracted by a CNN after Global Average Pooling (GAP), where the output channel C = 256. Then k is split along the channel dimension into N (N = 8) sub-feature vectors F = {f 1 , f 2 , . . . , f N }, f i ∈ R d , d = C/N for tokenization. We choose 3D VGG8, a simple CNN with 8 convolution layers. Label Embeddings L. In order to realize the information interaction among labels, and between labels and image features, we embed labels by an embedding layer. Each image x has M labels, and all labels are embedded into a vector set L = {l 1 , l 2 , . . . , l M }, l i ∈ R d by the learnable embedding layer of size d × M ."
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Soft State Embeddings S.,"There is a correlation between labels, e.g. the lesions with indistinct borders tend to be malignant. Therefore, we hypothesize that the states (GT values) of some labels can be a context for helping predict the remaining labels. We use a soft state embedding method. Specifically, we first embed the positive and negative states into s p and s n , both ∈ R d , and then the final state embedding s i is the weighted sum of s p and s n as shown in Equation  Multi-label Inference with Transformer Encoder. In a transformer, each output token is the integration of all input tokens. Taking advantage of this structure, we use a transformer encoder to integrate image embeddings and label embeddings, and used the output label tokens to predict label value. Specifically, embedding set are the input tokens, the attention value α and output token e are computed as follows: where e i is from E, W q , W k and W v are weight matrices of query, key and value, respectively, W r and W o are transformation matrices, and b 1 and b 2 are bias vectors. This update procedure is repeated for L layers, where the e i are fed to the successive transformer layer. Finally, all e i which are label output tokens are fed into M independent FC layers for predicting value of each label. The states of unknown labels cannot provide context, thus, the information interaction between known labels and unknown labels may be weaken. To overcome this problem, we propose a Self-feedback Strategy (SFS) inspired by Recurrent Neural Networks (RNN) to enhance the interaction of labels."
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,2.2,Self-feedback Strategy,"Training Progress and Loss Function. As shown in Fig.  is combined with f i as the initial input, and then the output predicted value is converted into state embedding s 0 i by Equation  Label Mask Strategy. To avoid predicting with labels' own input state, we use a Label Mask Strategy (LMS) during training phase to randomly mask a certain proportion a of known labels, which causes the labels' states to be embedded as zero vectors. Meanwhile, only the loss of the masked known label is calculated."
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3,Experiments and Results,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.1,Dataset and Evaluation,"Real-World pNENs Dataset. We validated our method on a real-world pNENs dataset from two centers. All patients with arterial phase Computed Tomography (CT) images were included. The dataset contained 264 and 28 patients in center 1 and center 2, and a senior radiologist annotated the bounding boxes for all 408 and 28 lesions. We extracted 37 labels from clinical reports, including survival, immunohistochemical (IHC), CT findings, etc. Among them, 1)RECIST drug response (RS), 2)tumor shrink (TS), 3)durable clinical benefit (DCB), 4)progression-free survival (PFS), 5)overall survival (OS), 6)grade (GD), 7)somatostatin receptor subtype 2(SSTR2), 8)Vascular Endothelial Growth Factor Receptor 2 (VEFGR2), 9)O6-methylguanine methyltransferase (MGMT), 10)metastatic foci (MTF), and 11)surgical recurrence (RT) are main tasks, and the remaining are auxiliary tasks. 143 and 28 lesions were segmented by radiologists, and the radiomics features of them were extracted, of which 162 features were selected and binarized as auxiliary tasks because of its statistically significant correlation with the main labels. The label distribution and the overlap ratio (Jaccard index) of lesions between pairs of labels are shown in Fig. "
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.2,Implementation Details,The CT window width and window level were adjusted to 310 and 130 HU refer to 
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.3,Comparison and Ablation Experiments,"We compared our method with Single-task(ST), Parameters Sharing (PS), ML-Decoder "
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.4,Performance of Different Inference Modes,As shown in the Fig. 
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.5,Analysis of Noise Resistance,"To explore the noise resistance of SFT in EA mode, we selected 20, 40, 60, 80, and 100 percent of the known labels respectively, and further negated 0, 20, 40, 60, 80, 100 percent of the selected labels to simulate noisy labels. The way to negate a label is to change the label value x to 1x. As shown in Fig. "
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,4,Conclusion,"We proposed a novel model SFT for multi-label prediction on real-world pNENs data. The model integrates label and image informations based on a transformer encoder, and iteratively uses its own prediction based on a self-feedback mechanism to improve the utilization of missing labels and correlation among labels. Experiment results demonstrated our proposed model outperformed other multilabel models, showed flexibility by multiple inference modes, and had a certain ability to maintain performance when the input context noise was less than 40%."
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Fig. 1 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Fig. 2 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Fig. 3 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Fig. 4 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Fig. 5 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Fig. 6 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Fig. 7 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Table 1 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Table 2 .,
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_49.
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,1,Introduction,"Placenta accreta spectrum (PAS) refers to the abnormal invasion of trophoblast cells into the myometrium at different depths of infiltration  However, due to the lack of open-source dataset, the research on computeraided diagnosis of PAS is very limited. Previous studies  To address the above issues, we propose a novel geometry-adaptive PAS detection method, which utilizes the shape prior of placenta to predict highquality PAS bounding boxes and further refines inaccurate annotations. The prior knowledge is mainly reflected in the aspect ratio of lesion boxes. Specifically, to take advantage of the geometry prior, we design a Geometry-adaptive Label Assignment (GA-LA) strategy and a Geometry-adaptive RoI Fusion (GA-RF) module. The GA-LA strategy dynamically selects positive proposals by calculating the optimal IoU threshold for each lesion. The GA-RF module fuses multi-scale RoI features from different pyramid layers. To reduce the impact of lesions with large aspect ratio, the module generates fusion weights through the geometry distribution of proposals. Furthermore, in order to alleviate the reliance on accurate annotations, we construct a Lesion-aware Detection Head (LA-Head), which leverages the geometry-guided predictions to iteratively refine bounding box labels by multiple instance learning. To the best of our knowledge, this is the first work to automatically detect PAS disorders on MR images. The contributions of this paper can be summarized as follows: (1) A Lesion-aware Detection Head (LA-Head) is designed, which employs a new multiple instance learning approach to improve the robustness to inaccurate annotations. (2) A flexible Geometry-adaptive Label Assignment (GA-LA) strategy is proposed to select positive PAS candidates according to the shape of lesions. (3) A statisticbased Geometry-adaptive RoI Fusion (GA-RF) module is developed for aggregating multi-scale features based on the geometry distribution of proposals."
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,2,Method,"The central idea of our geometry-adaptive network is to refine inaccurate bounding boxes with geometry-guided predictions. To this end, we propose a Geometryadaptive Label Assignment (GA-LA) strategy and a Geometry-adaptive RoI Fusion (GA-RF) module to introduce geometry prior to detection. Then we develop a Lesion-aware Detection Head (LA-Head) to achieve the refinement of inaccurate annotations using multiple instance learning. The overview of our method is illustrated in Fig. "
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,2.1,Geometry-Adaptive Label Assignment,"The static label assignment strategy adopted by Faster R-CNN predefines the IoU threshold τ to match objects and background to each anchor, but ignores the different shapes of PAS regions. To relieve the problem, we propose a GA-LA strategy to dynamically calculate the IoU threshold τ i for each lesion bounding box g i according to its aspect ratio r i . A previous study demonstrated that the aspect ratio is larger, the detection performance is better with a low IoU threshold  where α is a hyper-parameter. For each lesion g i , the proposals with an IoU greater than or equal to the threshold τ i are selected as positive samples. The labeled proposals are then used to train the network."
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,2.2,Geometry-Adaptive RoI Fusion,"RoIAlign maps each proposal to a single feature pyramid level, which fails to leverage multi-scale information from other levels. Some works  where {N l } L l=1 is the number of proposals with large aspect ratio in each layer. In this way, we take full advantage of the multi-scale information and the geometry distribution prior to enrich the feature representation for PAS prediction."
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,2.3,Lesion-Aware Detection Head,"Motivated by  In this work, we regard PAS detection as a multiple instance learning (MIL) problem. In the standard paradigm for object detection, a MIL method treats an image as a bag and proposals in the image as instances  As shown in Fig.  where j * is the index of the instance with the highest score. To leverage the classification information of predicted bounding boxes, we fuse the most positive instance p j * i and the ground-truth instance g i to obtain a high-quality positive instance for training. The final selected instance is defined as below: where δ s is the weighting factor. Intuitively, the weight assigned to p * i should be higher when φ(p j * i ; θ s ) is larger, and the weight assigned to g i should have a lower bound γ to ensure that g i can provide prior knowledge. Thus δ s is calculated as: where β and γ are hyper-parameters. With the selected instances, instance classifier is trained to classify other instances as positive or negative. Then these proposals can serve as refined bounding box annotations for the detector. Furthermore, the detector generates instances to train the detection head and highquality proposals can improve the detection performance. Thus this enables the self-feedback relationship between the detector and the LA-Head. During training, instance selector, instance classifier and detector are jointly optimized based on the loss function: where L s is the loss of instance selector which is defined as hinge loss: The second term L c is the loss of instance classifier and denoted as follows: where ψ(p j i ; θ c ) is the probability that p j i contains lesions, and y j i is the label of p j i and calculated as follows: where τ * i is the dynamic IoU threshold of p j i . The third term L d is defined as: where 1(x) is the indicator function, set to 1 if x = 1; otherwise, set to 0. We adopt the smooth L 1 loss as the loss function L reg for regression."
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,3,Experiments and Results,
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,3.1,"Dataset, Evaluation Metrics and Implementation Details","Dataset. Owing to the lack of open-source PAS dataset, our experiments are performed on a private dataset. We collected 110 placenta MRI scans of different patients upon the approval of Xiangya Hospital of Central South University. All T2-weighted image volumes were sliced along the sagittal plane. Two experienced radiologists with 20 and 14 years of clinical experience in medical imaging and PAS diagnosis selected the slices with PAS and manually annotated the lesion bounding boxes using LabelImg  where Δx, Δy, Δw, and Δh follow the uniform distribution U (-λ, λ). Evaluation Metrics. We adopt Average Precision (AP) and Sensitivity to evaluate the detection performance. In detail, AP is calculated over IoU threshold ranges from 0.25 to 0.95 at an interval of 0.05. Sensitivity denotes the proportion of correct prediction results in all ground-truths and is computed with an IoU threshold of 0.25 at 1 false positive per image. Implementation Details. The proposed network is implemented with MMDetection 2.4.0  The hyper-parameters are set as α = 2, β = 0.2, γ = 0.8 and R = 2."
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,3.2,Ablation Study,The results of ablative experiments are listed in Table 
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,3.3,Comparison with State-of-the-Art Methods,"We compare the geometry-adaptive network with seven object detection methods on both clean and noisy datasets. Faster R-CNN, Cascade R-CNN "
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,4,Conclusion,"In this paper, we present a geometry-adaptive network for robust PAS detection. We point out that the geometry prior missing problem and inaccurate annotations could deteriorate the performance of detectors. To solve the problem, a Geometry-adaptive Label Assignment strategy (GA-LA) and a Geometryadaptive RoI Fusion (GA-RF) module are proposed to fully utilize the geometry prior of lesions to predict high-quality proposals. Moreover, a Lesion-aware Detection Head (LA-Head) is developed to alleviate the impact of inaccurate annotations by leveraging the classification information of predicted boxes. The experimental results under both clean and noisy labels demonstrate that our method surpasses other state-of-the-art detectors."
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,,Fig. 1 .,
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,,Fig. 2 .,
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,,Table 1 .,
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,,Table 2 .,
Geometry-Adaptive Network for Robust Detection of Placenta Accreta Spectrum Disorders,,Clinical Applications -Breast,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,1,Introduction,"Breast cancer is a serious health problem with high incidence and wide prevalence for women throughout the world  Therefore, there is a high demand for automatic and robust methods to achieve accurate breast tumor segmentation. However, due to speckle noise and shadows in ultrasound images, breast tumor boundaries tend to be blurry and are difficult to be distinguished from background. Furthermore, the boundary and size of breast tumors are always variable and irregular  Various approaches based on deep learning have been developed for tumor segmentation with promising results  To address these challenges, we present, to the best of our knowledge, the first work to adopt multi-scale features collected from large set of clinical ultrasound images for breast tumor segmentation. The main contributions of our work are as follows: "
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,2,Method,"We demonstrate the architecture of our proposed network in Fig.  where SN represents the segmentation network and CN represents the involved convolutional network. θ SN and θ CN refer to the parameters in the segmentation and convolutional network, respectively. P (n) represents the segmentation maps obtained from the n-th stage in the segmentation network, and G refers to the corresponding ground truth. p SN (proba) denotes the distribution of probability maps. CN θCN (SN θSN (P (2) )) represents the probability for the input of CN coming from the predicted maps rather than the real ones. The parameters α 1 is empirically set as 1, α 2 , α 3 , α 4 are set as 0.1, and β 1 , β 2 , β 3 and β 4 are set as 0.05. It should be noted that, in UNet, there are 4 stages and hence we employ 4 CNNs for each of them without sharing their weights. Meanwhile, the adversarial loss for each of the CNN is defined as: where p CN (truth) denotes the distribution of original samples. CN θCN (G) represents the probability for the input of CN coming from the original dataset. In the implementation, we update the segmentation network and all the discriminators alternatingly in each iteration until both the generator and discriminators are converged."
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,3,Experiments,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,3.1,Dataset and Implementation Details,"We collected 10927 cases for this research from Yunnan Cancer Hospital. Each scan is with resolution of 1 × 1 mm 2 and size of 512 × 480. The breast tumors of each case are delineated by three experienced experts. Five-fold cross validation is performed on the dataset in all experiments to verify our proposed network. For external validation, we further test our model on two independent publicly-available datasets collected by STU-Hospital (Dataset 1) "
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,3.2,Comparison with State-of-the-Art Methods,"To verify the advantages of our proposed model for breast tumor segmentation in ultrasound images, we compare our deep-supervised convolutional network with the state-ofthe-art tumor segmentation methods, including DeepRes  Representative segmentation results using different methods are provided in Fig. "
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,3.3,Ablation Studies,"We demonstrate the efficacy of the deep supervision strategy using ablation studies. Four groups of frameworks (Stage I, Stage II, Stage III and Stage IV) are designed, with the numerals denoting the level of deep supervision counting from the last deconvolutional layer. We test these four frameworks on the in-house breast ultrasound dataset, and verify their segmentation performance using the same five evaluation criteria. The evaluation metrics from all cases are presented by the scatter plots in Fig. "
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,4,Conclusion,"In this paper, we have developed a large pre-trained model for breast tumor segmentation from ultrasound images. In particular, two constraints are proposed to exploit both image similarity and space correlation information for refining the prediction maps. Moreover, our proposed deep supervision strategy is used for quality control at each decoding stage, optimizing prediction maps layer-by-layer for overall performance improvement. Using a large clinical dataset, our proposed model demonstrates not only state-of-the-art segmentation performance, but also the outstanding generalizability to new ultrasound data from different sites. Besides, our large pre-trained model is general and robust in handling various tumor types and shadow noises in our acquired clinical ultrasound images. This also shows the potential of directly applying our model in real clinical applications."
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,,Fig. 1 .,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,,Fig. 2 .,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,,Fig. 3 .,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,,Table 1 .,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,,Table 2 .,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,,97 ± 0.88 81.64 ± 1.30 82.19 ± 1.61 68.83 ± 1.38 0.68 ± 0.06 3.4 Performance on Two External Public Datasets In,
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,,Table 3 .,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,1,Introduction,"Mitral Regurgitation. Mitral regurgitation (MR)  Towards Machine Learning for MR Diagnosis. Although quantitatively assessing mitral regurgitant volume requires specific CMR imaging sequences and expert analysis, four-chamber (4CH) CMR images provide a comprehensive view of all four heart chambers, including the mitral valve as it opens and closes, as shown in Fig.  Our Approach. We propose an automated five stage pipeline named Cardio-vascular magnetic resonance U-Net localized Self-Supervised Predictor (CUSSP). Our approach incorporates several different preexisting neural network architectures in the pipeline, discussed in Sect. 2.3, to address the challenges inherent to the MR classification task. Specifically, we use a U-Net  During training, CUSSP leverages a large amount of unlabeled CMR images, and minimal supervision, in the form of a comparatively small set of MR labels manually annotated by cardiologist. However, at test time CUSSP is fully automated.  "
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,2,Methods,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,2.1,Segmentation of the Cardiac Magnetic Resonance Images,"The CMR imaging data from the UK Biobank that is relevant to MR detection includes long-axis 2-chamber (2CH) view and long-axis 4-chamber (4CH) view, which are all shown in Fig.  As a pre-processing step, we performed semantic segmentation on the CMR imaging data, using masks (Fig.  We manually labeled 100 CMR images for each view and trained a supervised segmentation model with the TernausNet "
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,2.2,Baseline Models,"We first considered a random forest (RF) classifier  Next, we developed a deep learning baseline model for MR classification, a weakly supervised CNN-LSTM, following the principles in Fries et al.  The CNN-LSTM pipeline, shown in Fig.  In the CNN-LSTM model architecture, the CNN serves as the frame encoder, which encodes each frame of each sequence into a representation vector. The model uses DenseNet-121 pre-trained on ImageNet as the CNN. To better learn  the attention span of the frame encoder, we added an attention layer to the DenseNet-121 after the first convolutional layer. After the bi-directional LSTM, a multi-layer perceptron (MLP) performs the final classification."
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,2.3,The CUSSP Framework,"Conceptualization. To better encode the blood flow information relevant to MR classification from the 4CH CMR view, we investigated self-supervised representation learning methods which can leverage all the unlabeled CMR sequences present in the UK Biobank. Typically, self-supervised representation learning for visual data involves maximizing the similarity between representations of various distorted versions of a sample. Among the many self-supervised architectures, SimCLR  Test-Time Pipeline. Our CUSSP method consists of five main steps, shown in Fig.  Training Process. The first step involves training a representation encoder in a Barlow Twins network using over 30,000 unlabeled pre-processed sequences. We chose Barlow Twins for its versatility and robustness to distortions such as blurring, different sizes of the relevant areas and intensity variations, which are common in CMR images. We used a ResNet-18 model pretrained on ImageNet as an encoder. Its output vector is 512-dimensional. We selected ResNet-18 because we found that more complex encoders would easily memorize the limited labeled dataset, reducing the effectiveness of the embeddings. The projector network has three fully connected layers, all with 2048 output units. After training the encoder with the unlabeled dataset, it is fine-tuned in a siamese network using a comparatively smaller labeled set, as indicated in Sect. 3. During training, two sequences are sampled from the labeled dataset, with the first being non-MR and the second being either MR or non-MR. The two sequences are passed through the representation encoder to obtain embeddings, which are then used to calculate the contrastive loss. The model is trained to maximize contrastive loss when the two samples are non-MR and MR and to minimize it when both are non-MR. Contrastive learning helps because it uses the very limited labels in our possession to obtain representations focused on encoding differences between classes. Once the representation encoder is fine-tuned in the siamese network, it is combined with a 3-layer multi-layer perceptron (MLP) network to form a classifier, which is trained on the same labeled dataset. To improve computation efficiency and training accuracy, we also tested the framework using a smaller window of 25 frames, since MR occurs between diastole and systole. The code is available at https://github.com/Information-Fusion-Lab-Umass/CUSSP UKB MR."
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,3,Experiments and Discussion,"Experimental Setup. 4CH CMR images were used to conduct experiments with both the CNN-LSTM method and the CUSSP method. We used a total of 704 labeled sequences, with 525 sequences selected for the training set, including 452 labeled as non-MR and 73 labeled as MR. The remaining 179 sequences were used for testing, with 154 labeled as non-MR and 25 labeled as MR. Considering the substantial class imbalance, we opted to use F1 score as our primary evaluation metric, along with precision and recall. Random Forest Classification Results. The random forest model is trained with 10-fold cross validation, with a random search over a parameter grid of 10-100 estimators, 2-16 depth, 2-8 min samples. The optimal hyper-parameter setting found is: 20 estimators, log2 max features, max depth of 9 and a minimum of 2 samples per leaf node. The best results obtained are presented in Table "
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,,CUSSP Classification Results,"Fig.  We evaluated various configurations of the CUSSP model, to determine the relative benefits of different components. In the first configuration, the ResNet18 model was combined with a 3-layer MLP to train a classifier using the labeled training set after being trained in the Barlow-Twins network with the unlabeled dataset. During the classifier training, the cross-correlation loss from the Barlow-Twins network and the cross-entropy loss from the binary classification were weighted using three different configurations. For CUSSP-1 the crosscorrelation loss has a weight of 0.9, while the cross-entropy loss has a weight of 0.1. For CUSSP-2, the weights are 0.5 and 0.5, while for CUSSP-3 they are 0.1 and 0.9, respectively. Both CUSSP-1 and CUSSP-3 outperform CUSSP-2, though the performance is low, indicating the importance of fine-tuning, described below. In the second scenario, we fine-tuned the encoder with a siamese network to enhance the quality of the encoded representations after training the Barlow Twins network. To prevent overfitting of the model and to limit its capacity, we froze the parameters of all layers except the last block of the ResNet18 encoder when training the siamese network and the classifier. The resulting model, CUSSP-SIAM, showed a significant improvement in performance. In the final configuration CUSSP-SIAM-25, the number of frames in the training sequences was truncated from 50 frames to the 25 frames that correspond to the interval when mitral regurgitation occurs. The results are summarized in Table  CUSSP attains an F1 score of 0.69, and an ROC AUC of 0.88, outperforming the CNN-LSTM approach. This is dues to CUSSP's focus on the area around the valve to capture the blood flow through the valve, combining the advantages of Barlow Twins and contrastive learning. Meanwhile, the CNN-LSTM relies on attention, which does not seem to work as well. Additionally, using only the frames relevant to the task reduces the number of parameters and makes the model sample-efficient. This is essential for attaining high performance in the low label setting. In the future, we aim to use the pipeline on the large unlabeled dataset to scan for and adjudicate more MR cases. In conclusion, we present the first automated mitral regurgitation classification system using CMR imaging sequences. The CUSSP model we developed, trained with limited supervision, operates on 4CH CMR imaging sequences and attains an F1 score of 0.69 and an ROC AUC of 0.88, opening up the opportunity for large-scale screening for MR."
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,,Fig. 1 .,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,,Fig. 2 .,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,,Fig. 3 .,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,,Fig. 4 .,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,,Table 1 .,
Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging,,Table 2 .,
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,1,Introduction,"Color fundus imaging can detect and monitor various ocular diseases, including agerelated macular degeneration (AMD), glaucoma, and pathological myopia (PM)  In a similar manner, color funduscopy imaging has found applications beyond the confines of the planet. For example, astronauts onboard the International Space Station (ISS) utilize this imaging to identify spaceflight-associated neuro-ocular syndrome (SANS)  Image super-resolution and compression using different upsampling filters  Our Contributions: Considering all the relevant factors, we introduce the novel Swin-FSR architecture. It incorporates low-frequency feature extraction, deep feature extraction, and high-quality image reconstruction modules. The low-frequency feature extraction module employs a convolution layer to extract low-level features and is then directly passed to the reconstruction module to preserve low-frequency information. Our novelty is introduced in the deep feature extraction where we incorporated Depthwise Channel Attention block (DCA), improved Residual Swin-transformer Block -(iRSTB), and Spatial and Channel Attention block (SCA). To validate our work, we compare three different SR architectures for four Fundus datasets: iChallenge-AMD "
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,2,Methodology,
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,2.1,Overall Architecture,"As shown in Fig.  It has been reported that the convolution layer helps with better spatial feature extraction and early visual processing, guiding to more steady optimization in transformers  Finally, we combine all three features from our previous modules, namely, F LF , F SF , and F P LF and apply a final high-quality (HQ) image reconstruction module to generate a high-quality image I HQ as given in Eq. 3. where H REC is the function of the reconstruction block. Our low-frequency block extracts shallow features, whereas the two parallel depth-wise channel-attention and improved residual swin-transformer blocks extract spatially and channel-wise dense features extracting lost high-frequencies. With these three parallel residual connections, SwinFSR can propagate and combine the high and low-frequency information to the reconstruction module for better super-resolution results. It should be noted that the reconstruction module consists of a 1 × 1 convolution followed by a Pixel-shuffle layer to upsample the features."
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,2.2,Depth-Wise Channel Attention Block,"For super-resolution architectures, channel-attention  One of the most significant drawbacks of the transformer layer is it works on patch-level tokens where the spatial dimensions are transformed into a linear feature. To retain the spatial information intact and learning dense features effectively, we propose depthwise channel attention given in Eq. 4. Here, δ is ReLU activation, and φ is Sigmoid activation functions. The regular channelattention utilizes a 2D Conv with 1 × 1 × C weight vector C times to create output features 1×1×C. Given that adaptive average pooling already transforms the dimension to 1 × 1, utilizing a spatial convolution is redundant and shoots up computation time. To make it more efficient, we utilize depth-wise attention, with 1 × 1 weight vector applied on each of the C features separately, and then the output is concatenated to get our final output 1 × 1 × C. We use four DCA blocks in our architecture as given in Fig. "
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,2.3,Improved Residual Swin-Transformer Block,"Swin Transformer  Here, σ is Layer-normalization and ConvMLP has two 1D convolution followed by GELU activation. To capture spatial local contexts for patch-level features we utilize a patch-unmerging layer in parallel path and incorporate SCA (spatial and channel attention) block. The block consists of a convolution (k = 1, s = 1), a dilated convolution (k = 3, d = 2, s = 1) and a depth-wise convolution (k = 1, s = 1) layer. Here, k= kernel, d =dilation and s =stride. Moreover all these features are combined to get the final ouptut. By combining repetitive SCA, ST Lc blocks and a identity mapping we create our improved reisdual swin-transformer block (iRSTB) illustrated in Fig. "
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,2.4,Loss Function,"For image super resolution task, we utilize the L1 loss function given in Eq. 6. Here, I RHQ is the reconstructed output of SwinFSR and I HQ is the original high-quality image. 3 Experiments"
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,3.1,Dataset,
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,,Data use Declaration and Acknowledgment:,"The AMD and PALM dataset were released as part of REFUGE Challenge, PALM Challenge. The G1020 was published as technical report and benchmark "
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,3.2,Hyper-parameter,"We utilized L1 loss for training our models for the super-resolution task. For optimizer, we used Adam "
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,3.3,Qualitative Evaluation,"We compared our architecture with some best-performing CNN and Transformer based SR models, including RCAN  In the second experiment, we show results for ×4 reconstruction for all SR models in Fig. "
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,3.4,Quantitative Bench-Marking,"For quantitative evaluation, we utilize Peak Signal-to-Noise-Ratio (PSNR) and Structural Similarity Index Metric (SSIM), which has been previously employed for measuring similarity between original and reconstructed images in super-resolution tasks  Table "
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,3.5,Clinical Assessment,"We carried out a diagnostic assessment with two expert ophthalmologists and test samples of 80 fundus images (20 fundus images per disease classes: AMD, Glaucoma, Pathological Myopia and SANS for both original x2 and x4 images, and superresolution enhanced images). Half of the 20 fundus images were control patients without disease pathologies; the other half contained disease pathologies. The clinical experts were not provided any prior pathology information regarding the images. And each of the experts was given 10 images with equally distributed control and diseased images for each disease category. The accuracy and F1-score for original x4 images are as follows, 70.0% and 82.3% (AMD), 75% and 85.7% (Glaucoma), 60.0% and 74.9% (Palm), and 55% and 70.9% (SANS). The accuracy and F1-score for original x2 are as follows, 80.0% and 88.8% (AMD), 80% and 88.8% (Glaucoma), 70.0% and 82.1% (Palm), and 65% and 77.4% (SANS). The accuracy and F1-score for our model Swin-FSR's output from ×4 images are as follows, 90.0% and 93.3% (AMD), 90.0% and 93.7% (Glaucoma), 75.0% and 82.7% (Palm), and 75% and 81.4% (SANS). The accuracy and F1-score for Swin-FSR's output from ×2 images are as follows, 90.0% and 93.3% (AMD), 90.0% and 93.7% (Glaucoma), 80.0% and 85.7% (Palm), and 80% and 85.7% (SANS). We also tested SWIN-IR, ELAN, and RCAN models for diagnostic assessment, out of which SWIN-IR upsampled images got the best results. For x4 images, the model's accuracy and F-1 score are 80% and 87.5% (AMD), 85.0% and 90.3% (Glaucoma), 70.0% and 80.0% (Palm), and 70% and 76.9% (SANS). For x2 images, the model's accuracy and F-1 score are 80% and 87.5% (AMD), 80% and 88.8% (Glaucoma), 70.0% and 80.0% (Palm), and 75% and 81.4% (SANS). Based on the above observations, our model-generated images achieves the best result."
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,3.6,Ablation Study,"Effects of iRSTB, DCA, and SCA Number: We illustrate the impacts of iRSTB, DCA, and SCA numbers on the model's performance in Supplementary Fig.  Presence and Absence of iRSTB, DCA, and SCA Blocks: Additionally, we provide a comprehensive benchmark of our model's performance with and without the novel blocks incorporated in Supplementary Table "
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,4,Conclusion,"In this paper, we proposed Swin-FSR by combining novel DCA, iRSTB, and SCA blocks which extract depth and low features, spatial information, and aggregate in image reconstruction. The architecture reconstructs the precise venular structure of the fundus image with high confidence scores for two relevant metrics. As a result, we can efficiently employ this architecture in various ophthalmology applications emphasizing the Space station. This model is well-suited for the analysis of retinal degenerative diseases and for monitoring future prognosis. Our goal is to expand the scope of this work to include other data modalities."
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,,Fig. 1 .,
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,,Fig. 2 .,
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,,Fig. 3 .,
Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology,,,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,1,Introduction,"Retinal fundus images and Optical Coherence Tomography (OCT) are common 2D and 3D imaging techniques used for eye disease screening. Multimodality learning usually provides more complementary information than unimodality learning  Uncertainty estimation is an effective way to provide a measure of reliability for ambiguous network predictions. The current uncertainty estimation methods mainly include Bayesian neural networks, deep ensemble methods, and deterministic-based methods. Bayesian neural networks  In this work, we propose a novel multimodality eye disease screening method, called EyeMoSt, that conducts Fundus and OCT modality fusion in a reliable manner. Our EyeMoSt places Normal-inverse Gamma (NIG) prior distributions over the pre-trained neural networks to directly learn both aleatoric and epistemic uncertainty for unimodality. Moreover, Our EyeMoSt introduces the Mixture of Student's t (MoSt) distributions, which provide robust classification results with global uncertainty. More importantly, MoSt endows the model with robustness under heavy-tailed property awareness. We conduct sufficient experiments on two datasets for different eye diseases (e.g., glaucoma grading, age-related macular degeneration, and polypoid choroidal vasculopathy) to verify the reliability and robustness of the proposed method. In summary, the key contributions are as follows: 1) We propose a novel multimodality eye disease screening method, EyeMoSt, which conducts reliable fusion of Fundus and OCT modalities.  "
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,2,Method,"In this section, we introduce the overall framework of our EyeMoSt, which efficiently estimates the aleatoric and epistemic uncertainty for unimodality and adaptively integrates Fundus and OCT modalities in principle. As shown in Fig.  Given a multimodality eye dataset and the corresponding label y i , the intuitive goal is to learn a function that can classify different categories. Fundus and OCT are common imaging modalities for eye disease screening. Therefore, here M = 2, x i 1 and x i 2 represent Fundus and OCT input modality data, respectively. We first train 2D encoder Θ of Res2Net "
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,2.1,Uncertainty Estimation for Unimodality,"We extend the deep evidential regression model  where Γ -1 is an inverse-gamma distribution, γ m ∈ R, δ m > 0, α m > 1, β m > 0 are the learning parameters. Specifically, the multi-evidential heads will be placed after the encoders Θ and Φ (as shown in Fig.  Then, given the evidence distribution parameter p m , the marginal likelihood is calculated by marginalizing the likelihood parameter: Interacted by the prior and the Gaussian likelihood of each unimodality  with "
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,2.2,Mixture of Student's t Distributions (MoSt),"Then, we focus on fusing multiple St Distributions from different modalities. How to rationally integrate multiple Sts into a unified St is the key issue. To this end, the joint modality of distribution can be denoted as: In order to preserve the closed St distribution form and the heavy-tailed properties of the fusion modality, the updated parameters are given by  More intuitively, the above formula determines the modality with a stronger heavy-tailed attribute. That is, according to the perceived heavy-tailed attribute of each modality, the most robust modality is selected as the fusion modality. Finally, the prediction and uncertainty of the fusion modality is given by:"
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,2.3,Learning the Evidential Distributions,"Under the evidential learning framework, we expect more evidence to be collected for each modality, thus, the proposed model is expected to maximize the likelihood function of the model evidence. Equivalently, the model is expected to minimize the negative log-likelihood function, which can be expressed as: Then, to fit the classification tasks, we introduce the cross entropy term L CE m : where λ is the balance factor set to 0.5. For further information on the selection of hyperparameter λ, please refer to Supplementary S2. Similarly, for the fusion modality, we first maximize the likelihood function of the model evidence as follows: ) Complete derivations of Eq. 8 are available in Supplementary S1.2. Then, to achieve better classification performance, the cross entropy term L CE m is also introduced into Eq. 8 as below: Totally, the evidential learning process for multimodality screening can be denoted as: In this paper, we mainly consider the fusion of two modalities, M = 2."
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,3,Experiments,"Datasets: In this paper, we verify the effectiveness of EyeMoSt on the two datasets. For the glaucoma recognition, We validate the proposed method on the GAMMA "
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Compared Methods and Metrics:,"We compare the following six methods: For different fusion stage strategies, a) B-EF Baseline of the early fusion "
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Comparison and Analysis:,"We reported our algorithm with different methods on the GAMMA and in-house datasets in Table  As shown in Table  Understanding Uncertainty for Unimodality/Multimodality Eye Data: To make progress towards the multimodality ophthalmic clinical application of uncertainty estimation, we conducted unimodality and multimodality uncertainty analysis for eye data. First, we add more Gaussian noise with varying variances to the unimodality (Fundus or OCT) in the GAMMA and in-house datasets to simulate OOD data. The original samples without noise are denoted as in-distribution (ID) data. Figure "
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,4,Conclusion,"In this paper, we propose the EyeMoSt for reliable and robust screening of eye diseases using evidential multimodality fusion. Our EyeMoSt produces the uncertainty for unimodality and then adaptively fuses different modalities in a distribution perspective. The different NIG evidence priors are employed to model the distribution of encoder observations, which supports the backbones to directly learn aleatoric and epistemic uncertainty. We then derive an analytical solution to the Student's t distributions of the NIG evidence priors on the Gaussian likelihood function. Furthermore, we propose the MoSt distributions in principle adaptively integrates different modalities, which endows the model with heavy-tailed properties and is more robust and reliable for eye disease screening. Extensive experiments show that the robustness and reliability of our method in classification and uncertainty estimation on GAMMA and in-house datasets are competitive with previous methods. Overall, our approach has the potential to multimodality eye data discriminator for trustworthy medical AI decision-making. In future work, our focus will be on incorporating uncertainty into the training process and exploring the application of reliable multimodality screening for eye diseases in a clinical setting."
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Fig. 1 .,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,2 ),
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,1,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Fig. 2 .,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Fig. 3 .,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Table 1 .,
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Acknowledgements,. This work was supported by the 
Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions,,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 56.
