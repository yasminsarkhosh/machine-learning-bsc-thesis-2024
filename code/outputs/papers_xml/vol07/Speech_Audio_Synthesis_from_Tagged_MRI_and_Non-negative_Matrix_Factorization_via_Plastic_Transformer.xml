<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer</title>
				<funder ref="#_3USEbEb #_SvpPES3 #_pKStDgS #_JA9vBD6">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Massachusetts General Hospital</orgName>
								<orgName type="institution" key="instit2">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fangxu</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Massachusetts General Hospital</orgName>
								<orgName type="institution" key="instit2">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maureen</forename><surname>Stone</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiachen</forename><surname>Zhuo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sidney</forename><surname>Fels</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of British Columbia</orgName>
								<address>
									<settlement>Vancouver</settlement>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jerry</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Georges</forename><surname>El Fakhri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Massachusetts General Hospital</orgName>
								<orgName type="institution" key="instit2">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonghye</forename><surname>Woo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Massachusetts General Hospital</orgName>
								<orgName type="institution" key="instit2">Harvard Medical School</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="435" to="445"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">85103588FB5DD93A877AE63C7A332AD8</idno>
					<idno type="DOI">10.1007/978-3-031-43990-2_41</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The tongue's intricate 3D structure, comprising localized functional units, plays a crucial role in the production of speech. When measured using tagged MRI, these functional units exhibit cohesive displacements and derived quantities that facilitate the complex process of speech production. Non-negative matrix factorization-based approaches have been shown to estimate the functional units through motion features, yielding a set of building blocks and a corresponding weighting map. Investigating the link between weighting maps and speech acoustics can offer significant insights into the intricate process of speech production. To this end, in this work, we utilize two-dimensional spectrograms as a proxy representation, and develop an end-to-end deep learning framework for translating weighting maps to their corresponding audio waveforms. Our proposed plastic light transformer (PLT) framework is based on directional product relative position bias and single-level spatial pyramid pooling, thus enabling flexible processing of weighting maps with variable size to fixed-size spectrograms, without input information loss or dimension expansion. Additionally, our PLT framework efficiently models the global correlation of wide matrix input. To improve the realism of our generated spectrograms with relatively limited training samples, we apply pair-wise utterance consistency with Maximum Mean Discrepancy constraint and adversarial training. Experimental results on a dataset of 29 subjects speaking two utterances demonstrated that our framework is able to synthesize speech audio waveforms from weighting maps, outperforming conventional convolution and transformer models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Intelligible speech is produced by the intricate three-dimensional structure of the tongue, composed of localized functional units <ref type="bibr" target="#b25">[26]</ref>. These functional units, when measured using tagged magnetic resonance imaging (MRI), exhibit cohesive displacements and derived quantities that serve as intermediate structures linking tongue muscle activity to tongue surface motion, which in turn facilitates the production of speech. A framework based on sparse non-negative matrix factorization (NMF) with manifold regularization can be used to estimate the functional units given input motion features, which yields a set of building blocks (or basis vectors) and a corresponding sparse weighting map (or encoding) <ref type="bibr" target="#b26">[27]</ref>. The building blocks can form and dissolve with remarkable speed and agility, yielding highly coordinated patterns that vary depending on the specific speech task at hand. The corresponding weighting map can then be used to identify the cohesive regions and reveal the underlying functional units <ref type="bibr" target="#b24">[25]</ref>. As such, by elucidating the relationship between the weighting map and intelligible speech, we can gain valuable insights for the development of speech motor control theories and the treatment of speech-related disorders.</p><p>Despite recent advances in cross-modal speech processing, translating between varied-size of wide 2D weighting maps and high-frequency 1D audio waveforms remains a challenge. The first obstacle is the inherent heterogeneity of their respective data representations, compounded by the tendency of losing pitch information in audio <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. By contrast, transforming a 1D audio waveform into a 2D spectrogram provides a rich representation of the audio signal's energy distribution over the frequency domain, capturing both pitch and resonance information along the time axis <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>. Second, the input sizes of the weighting maps vary between 20 × 5,745 and 20 × 11,938, while the output spectrogram has a fixed size for each audio section. Notably, fully connected layers used in <ref type="bibr" target="#b0">[1]</ref> require fixed size input, while the possible fully convolution neural networks (CNN) can have varied output sizes and unstable performance <ref type="bibr" target="#b22">[23]</ref>. Third, modeling global correlations for the long column dimension of the weighting map and the lack of spatial local neighboring relationships in the row dimension presents further difficulties for conventional CNNs that rely on deep hierarchy structure for expanding the reception field <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref>. Furthermore, the limited number of training pairs available hinders the large model learning process.</p><p>To address the aforementioned challenges, in this work, we propose an endto-end translator that generates 2D spectrograms from 2D weighting maps via a heterogeneous plastic light transformer (PLT) encoder and a 2D CNN decoder. The lightweight backbone of PLT can efficiently capture the global dependencies with a wide matrix input in every layer <ref type="bibr" target="#b13">[14]</ref>. Our PLT module is designed with directional product relative position bias and single-level spatial pyramid pooling to enable flexible global modeling of weighting maps with variable sizes, producing fixed-size spectrograms without information loss or dimension expansion due to cropping, padding, or interpolation for size normalization. To deal with a limited number of training samples, we explore pair-wise utterance consistency as prior knowledge with Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b7">[8]</ref> in a disentangled latent space as an additional optimization objective. Additionally, a generative adversarial network (GAN) <ref type="bibr" target="#b9">[10]</ref> can be incorporated to enhance the realism of the generated spectrograms.</p><p>The main contributions of this work are three-fold: Both quantitative and qualitative evaluation results demonstrate superior synthesis performance over comparison methods. Our framework has the potential to support clinicians and researchers in deepening their understanding of the interplay between tongue movements and speech waveforms, thereby improving treatment strategies for patients with speech-related disorders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessing</head><p>During the training phase, we are given M pairs of synchronized tagged MRI sequences t i and audio waveforms a i , i.e., {t i , a i } M i=1 . First, we apply a non-linear transformation using librosa to convert a i into mel-spectrograms, denoted as s i with the function S : a i → s i . This transformation uses a Hz-scale to emphasize human voice frequencies ranging from 40 to 1000 Hz Hz, while suppressing highfrequency instrument noise. Second, for each tagged MRI sequence t i , we use a phase-based diffeomorphic registration method <ref type="bibr" target="#b28">[29]</ref> to track the internal motion of the tongue. This allows us to generate corresponding weighting maps denoted as H i , which are based on input motion features X i , including the magnitude and angle of each track, by optimizing the following equation.</p><formula xml:id="formula_0">E = 1 2 X i -W i H i 2 F + 1 2 λTr(H i L i H i ) + η H i 1/2 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where λ and η denote the weights associated with the manifold and sparse regularizations, respectively, and Tr(•) represents the trace of a matrix. The graph Laplacian is denoted by L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Encoding Variable Size H i with Plastic Light-Transformer</head><p>Directly modeling correlations among any two elements in a given weighting map</p><formula xml:id="formula_2">H i ∈ R Xi×Yi can impose quadratic complexity of O(X 2 i Y 2 i ).</formula><p>The recent efficient vision transformers (ViTs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32]</ref> usually adopt a local patch design to compute local self-attention and correlate patches with CNNs. Specifically, the input is divided into</p><formula xml:id="formula_3">N i = Xi Px × Yi Py patches 1</formula><p>, each of which is flattened to a token vector with a length of d = P x × P y <ref type="bibr" target="#b6">[7]</ref>. The local self-attention is then formulated with a complexity of O(N i d 2 = X i Y i d) as follows:</p><formula xml:id="formula_4">H local i = Attn(H q i , H k i , H v i ) = SoftMax( H q i H k i √ d )H v i , ∈ R Xi×Yi ,<label>(2)</label></formula><p>where vectors</p><formula xml:id="formula_5">H q i , H k i , H v i ∈ R</formula><p>Ni×d are produced by the linear projections of query (W q ), key (W k ), and value (W v ) branches, respectively <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32]</ref>. The global correlation of ViTs with CNN <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> or window shifting <ref type="bibr" target="#b19">[20]</ref>, however, may not be efficient for our wide matrix H i , which lacks explicit row-wise neighboring features and may have a width that is too long for hierarchical convolution modeling. To address these challenges, we follow the lightweight ViT design <ref type="bibr" target="#b13">[14]</ref>, which uses a global embedding G ∈ R T ×d with T N i randomly generated global tokens as the anchor for global information aggregation Ĝi . The aggregation is performed with attention of G q , H k i , H v i , which is then broadcasted with attention of H q i , Ĝk i , Ĝv i to leverage global contextual information <ref type="bibr" target="#b13">[14]</ref>. While LightViT backbones have been shown to achieve wide global modeling within each layer <ref type="bibr" target="#b13">[14]</ref>, they are not well-suited for our variable size input and fixed size output translation. Although the self-attention scheme used in ViTs does not constrain the number of tokens, the absolute patch-position encoding in conventional ViTs <ref type="bibr" target="#b6">[7]</ref> can only be applied to a fixed N i <ref type="bibr" target="#b31">[32]</ref>, and the attention module will keep the same size of input and output. Notably, the number of tokens N i will change depending on the size of X i × Y i . As such, in this work, we resort to the directional product relative position bias <ref type="bibr" target="#b27">[28]</ref> to add R i ∈ R Ni×Ni , where element r a,b = p δ x a,b ,δ y a,b is a trainable scalar, indicating the relative position weight between the patches a and b 2 . We set the offset of patch position in 1 The bottom-right boundary is padded with 0 to ensure Xi%Px = 0 and Yi%Py = 0. 2 a learnable matrix p ∈ R (2Px-1)×(2Py -1) is initialized with trunc normal , where Px = 20 and Py = 12000 20 = 600 are the maximum patch dimensions in our task.</p><p>x and y directions δ x a,b = x ax b + P x , δ y a,b = y ay b + M y as the index in p. Furthermore, the product relative position bias utilized in this work can distinguish between vertical or horizontal offsets, whereas the popular cross relative position bias <ref type="bibr" target="#b27">[28]</ref> in computer vision tasks does not need to differentiate between time and spatial neighboring relationships in two dimensions.</p><p>Therefore, for global attention, we can aggregate the information of local tokens by modeling their global dependencies with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ĝi</head><formula xml:id="formula_6">= Attn(G q , H k i , H v i ) = SoftMax( G q H k i + R i √ d )H v i , ∈ R Xi×Yi . (<label>3</label></formula><formula xml:id="formula_7">)</formula><p>Then, these global dependencies are broadcasted to every local token:</p><formula xml:id="formula_8">H global i = Attn(H q i , Ĝk i , Ĝv i ) = SoftMax( H q i Ĝk i + R i √ d ) Ĝv i , ∈ R Xi×Yi . (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>By adding H local i and H global i , each token can benefit from both local and global features, while maintaining linear complexity with respect to the input size. This brings noticeable improvements with negligible FLOPs increment. However, the sequentially proportional patch merging used in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref> still generates output sizes that vary with input sizes. Therefore, we utilize the single-level Spatial Pyramid Pooling (SSPP) <ref type="bibr" target="#b12">[13]</ref> to extract a fixed-size feature for arbitrary input sizes. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the output of our channel-wise SSPP module with 20 × 256 bins has the size of 20 × 256 × d, which can be a token merging scheme that adapts to the input size. Therefore, the final output of a layer is given by</p><formula xml:id="formula_10">H i = SSPP(H local i + H global i ) ∈ R Xi×Yi . (<label>5</label></formula><formula xml:id="formula_11">)</formula><p>We cascade four PLT layers with SSPP as our encoder to extract the feature representation f i ∈ R 8×8×d . For the decoder, we adopt a simple 2D CNN with three deconvolutional layers to synthesize the spectrogram si .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Overall Training Protocol</head><p>We utilize the intermediate pairs of {H i , s i } M i=1 to train our translator T , which consists of a PLT encoder and a 2D CNN decoder. The quality of the generated spectrograms si is evaluated using the mean square error (MSE) with respect to the ground truth spectrograms s i :</p><formula xml:id="formula_12">L MSE = ||s i -s i || 2 2 = ||T (H i ) -S(a i )|| 2 2 . (<label>6</label></formula><formula xml:id="formula_13">)</formula><p>Additionally, we utilize the utterance consistency in the latent feature space as an additional optimization constraint. Specifically, we propose to disentangle f i into two parts, i.e., utterance-related f u i and subject-related f s i . In practice, we split the utterance/subject-related parts channel-wise using tensor slicing method. Following the idea of deep metric learning <ref type="bibr" target="#b18">[19]</ref>, we aim to minimize the discrepancy between the latent features f u i and f u j of two samples t i and t j that belong to the same utterance. Therefore, we use MMD <ref type="bibr" target="#b7">[8]</ref> as an efficient discrepancy loss L MMD = γMMD(f u i , f u j ), where γ = 1 or 0 for same or different utterance pairs, respectively.</p><p>Of note, the f s i is implicitly encouraged to incorporate the subject-related style of the articulation other than f u i with a complementary constraint <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref> for reconstruction. Therefore, the decoder, which takes f s i conditioned on f u i can be considered as the utterance-conditioned spectrogram distribution modeling. This approach follows a divide-and-conquer strategy <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref> for each utterance and can be particularly efficient for relatively few utterance tasks.</p><p>A GAN model can be further utilized to boost the realism of si . A discriminator D is employed to differentiate whether the mel-spectrogram is real s i = S(a i ) or generated si = T (H i ) with the following binary cross-entropy loss:</p><formula xml:id="formula_14">L GAN = E si {log(D(s i ))} + E si {log(1 -D(s i ))}. (<label>7</label></formula><formula xml:id="formula_15">)</formula><p>In adversarial training, the translator T attempts to confuse D by optimizing</p><formula xml:id="formula_16">L T GAN = E si {-log(1 -D(s i ))}.</formula><p>Of note, T does not involve real spectrograms in log(D(s i )) <ref type="bibr" target="#b23">[24]</ref>. Therefore, the overall optimization objectives of our translator T and discriminator D are expressed as:</p><formula xml:id="formula_17">min T L MSE + βL MMD + λL T GAN ; min D L GAN ,<label>(8)</label></formula><p>where β and λ represent the weighting parameters. Notably, only T is utilized in testing, and we do not need pairwise inputs for utterance consistency. Recovering audio waveform from mel-spectrogram can be achieved by the well-established Griffin-Lim algorithm <ref type="bibr" target="#b10">[11]</ref> in the Librosa toolbox.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>For evaluation, we collected paired 3D tagged MRI sequences and audio waveforms from a total of 29 subjects, while performing the speech words "a souk" or "a geese," with a periodic metronome-like sound as guidance <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30]</ref>. The tagged-MRI sequences consisted of 26 frames, which were resized to 128 × 128.</p><p>The resulting H matrix varied in size from 20 × 5,745 to 20 × 11,938 (we set one dimension to a constant value of 20.) The audio waveforms had varying lengths between 21,832 to 24,175. To augment the dataset, we employed a sliding window technique on each audio, allowing us to crop sections with 21,000 time points, resulting in 100 audio waveforms. Then, we utilized the Librosa library to convert all audio waveforms into mel-spectrograms with a size of 64 × 64. For our evaluation, we utilized a subject-independent leave-one-out approach. For the data augmentation of the H matrix, we randomly drop the column to round Y i to the nearest hundred, e.g., 9,882 to 9,800, generating 100 versions of H. We utilized the leave-one-out evaluation, following a subject-independent manner. In our implementation, we set P x = 1 and P y = 20, i.e., d = 20. Our encoder consisted of four PLT encoder layers with SSPP, to extract a feature f i with the size of 8 × 8 × 20. Specifically, the first 8 × 8 × 4 component was set as the utterance-related factors, and the remaining 16 channels were for the subject-specific factors. Then, the three 2D de-convolutional layers were applied as our decoder to generate the 64 × 64 mel-spectrogram. The activation units in our model were rectified linear units (ReLU), and we normalized the final output of each pixel using the sigmoid function. The discriminator in our model consisted of three convolutional layers and two fully connected layers, and had a sigmoid output. A detailed description of the network structure is provided in the supplementary material, due to space limitations.</p><p>Our model was implemented using PyTorch and trained 200 epochs for approximately 6 h on a server equipped with an NVIDIA V100 GPU. Notably, the inference from a H matrix to audio took less than 1 s, depending on the size of H. Also, the pairwise utterance consistency and GAN training were only applied during the training phase and did not affect inference. For our method and its ablation studies, we consistently set the learning rates of our heterogeneous translator and discriminator to lr T = 10 -3 and lr D = 10 -4 , respectively, with a momentum of 0.5. The loss trade-off hyperparameters were set as β = 0.75, and we set λ = 1.</p><p>It is important to note that without NMF, generating intelligible audio with a small number of subjects using video-based audio translation models, such as Lip2AudSpect <ref type="bibr" target="#b0">[1]</ref>, is not feasible. As an alternative, we pre-processed the input by cropping, padding with zeros, or using bi-cubic interpolation to obtain a fixed-size input H. We then compared the performance of our encoder module with conventional CNN or LightViT <ref type="bibr" target="#b13">[14]</ref>.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows a qualitative comparison of our PLT framework with CNN and LightViT <ref type="bibr" target="#b13">[14]</ref> using bi-cubic interpolation. We can observe that our generated spectrogram and the corresponding audio waveforms demonstrate superior alignment with the ground truth. It is worth noting that the CNN model or the CNN-based global modeling ViTs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31]</ref> require deep models to achieve large receptive fields <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref>. Moreover, the interpolation process adds significant computational complexity for both CNN and LightViT, making it difficult to train on a limited dataset. In Fig. <ref type="figure" target="#fig_2">3</ref>(a), we show that our proposed PLT framework achieves a stable performance gain along with the training and outperforms CNN with the crop, which lost the information of some functional units.</p><p>Following <ref type="bibr" target="#b0">[1]</ref>, we used 2D Pearson's correlation coefficient (Corr2D) <ref type="bibr" target="#b3">[4]</ref>, and Perceptual Evaluation of Speech Quality (PESQ) <ref type="bibr" target="#b21">[22]</ref> as our evaluation metrics to measure the synthesis quality of spectrograms in the frequency domain, and waveforms in the time domain, respectively. The numerical comparisons of different encoder structures with conventional CNN or LightViT with different crop or padding strategies and our PLT framework are provided in Table <ref type="table" target="#tab_0">1</ref>. The standard deviation was obtained from three independent random trials. Our framework outperformed CNN and lightViT consistently. In addition, the synthesis performance was improved by pair-wise disentangled utterance consistency MMD loss and GAN loss, as demonstrated in our ablation studies. Furthermore, it outperformed the in-directional cross relative position bias <ref type="bibr" target="#b27">[28]</ref>, since two dimensions in the weighting map indicate time and spatial relationship, respectively. Notably, even though CNN with SSPP can process varied size inputs, it suffers from limited long-term modeling capacity <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref> and unstable performance <ref type="bibr" target="#b22">[23]</ref>. The sensitivity analysis of our loss weights are given in Fig. <ref type="figure" target="#fig_2">3</ref>(b) and (c), where the performance was relatively stable for β ∈ [0.75, 1.5] and λ ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This work aimed to explore the relationship between tongue movements and speech acoustics by translating weighting maps, which represent the functional units of the tongue, to their corresponding audio waveforms. To achieve this, we proposed a deep PLT framework that can handle variable-sized weighting maps and generated fixed-sized spectrograms, without information loss or dimension expansion. Our framework efficiently modeled global correlations in wide matrix input. To improve the realism of the generated spectrograms, we applied pairwise utterance consistency with MMD constraint and adversarial training. Our experimental results demonstrated the potential of our framework to synthesize audio waveforms from weighting maps, which can aid clinicians and researchers in better understanding the relationship between the two modalities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of our translation framework. Only the NMF and translator with heterogeneous PLT encoder and 2D CNN decoder are used for testing.</figDesc><graphic coords="3,56,46,54,50,339,40,158,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparisons of our PLT with CNN and LightViT using bi-cubic interpolation. We show H i for compact layout. Audios are attached in supplementary.</figDesc><graphic coords="7,56,46,54,29,339,52,222,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) Comparison of Corr2D using our plastic light transformer and CNN with crop. Sensitivity analysis of β (b) and λ (c).</figDesc><graphic coords="9,56,46,54,41,339,76,105,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Numerical comparisons during testing using leave-one-out evaluation</figDesc><table><row><cell>Encoder Models</cell><cell cols="2">Corr2D for spectrogram ↑ PESQ for waveform ↑</cell></row><row><cell>CNN (Crop)</cell><cell>0.614±0.013</cell><cell>1.126±0.021</cell></row><row><cell>CNN (Padding 0)</cell><cell>0.684±0.010</cell><cell>1.437±0.018</cell></row><row><cell>CNN (Bi-Cubic)</cell><cell>0.689±0.012</cell><cell>1.451±0.020</cell></row><row><cell>CNN+SSPP</cell><cell>0.692±0.017</cell><cell>1.455±0.022</cell></row><row><cell>LightViT (Crop)</cell><cell>0.635±0.015</cell><cell>1.208±0.022</cell></row><row><cell>LightViT (Padding 0)</cell><cell>0.708±0.011</cell><cell>1.475±0.015</cell></row><row><cell>LightViT (Bi-Cubic)</cell><cell>0.702±0.012</cell><cell>1.492±0.018</cell></row><row><cell>Ours</cell><cell>0.742±0.012</cell><cell>1.581±0.020</cell></row><row><cell>Ours with cross embedding</cell><cell>0.720±0.013</cell><cell>1.550±0.021</cell></row><row><cell cols="2">Ours w/o Pair-wise Disentangle 0.724±0.010</cell><cell>1.548±0.019</cell></row><row><cell>Ours w/o GAN</cell><cell>0.729±0.011</cell><cell>1.546±0.020</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is supported by <rs type="funder">NIH</rs> <rs type="grantNumber">R01DC014717</rs>, <rs type="grantNumber">R01DC018511</rs>, <rs type="grantNumber">R01CA133015</rs>, and <rs type="grantNumber">P41EB022544</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3USEbEb">
					<idno type="grant-number">R01DC014717</idno>
				</org>
				<org type="funding" xml:id="_SvpPES3">
					<idno type="grant-number">R01DC018511</idno>
				</org>
				<org type="funding" xml:id="_pKStDgS">
					<idno type="grant-number">R01CA133015</idno>
				</org>
				<org type="funding" xml:id="_JA9vBD6">
					<idno type="grant-number">P41EB022544</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lip2audspec: speech reconstruction from silent lip movements video</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="2516" to="2520" />
			<date type="published" when="2018">2018</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computing receptive fields of convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Norris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep verifier networks: verification of deep discriminative models with deep generative models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiresolution spectrotemporal analysis of complex sounds</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="887" to="906" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Twins: revisiting the design of spatial attention in vision transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9355" to="9366" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-54184-6_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-54184-66" />
	</analytic>
	<monogr>
		<title level="m">ACCV 2016</title>
		<editor>
			<persName><forename type="first">S.-H</forename><surname>Lai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10112</biblScope>
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.03906</idno>
		<title level="m">Training generative neural networks via maximum mean discrepancy optimization</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vid2speech: speech reconstruction from silent video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5095" to="5099" />
			<date type="published" when="2017">2017</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Signal estimation from modified short-time Fourier transform</title>
		<author>
			<persName><forename type="first">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image2audio: facilitating semi-supervised audio emotion recognition with facial expression image</title>
		<author>
			<persName><forename type="first">G</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="912" to="913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05557</idno>
		<title level="m">Lightvit: towards light-weight convolution-free vision transformers</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semi-automatic segmentation of the tongue for 3D motion analysis with dynamic MRI</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Murano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1465" to="1468" />
		</imprint>
		<respStmt>
			<orgName>ISBI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mutual information regularized feature-level Frankenstein for discriminative recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vijayakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="5243" to="5260" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain generalization under conditional and label shifts via variational Bayesian inference</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Feature-level Frankenstein: eliminating variations for discriminative recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="637" to="646" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adaptive deep metric learning for identity-aware facial expression recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="20" to="29" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Perceptual evaluation of speech quality PESQ): an objective method for end-to-end speech quality assessment of narrow-band telephone networks and speech codecs</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Recommendation</surname></persName>
		</author>
		<idno>ITU-T P. 862</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">(Input) size matters for CNN classifiers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Byttner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Krumnack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wiedenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schallner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shenk</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-86340-1_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-86340-111" />
	</analytic>
	<monogr>
		<title level="m">ICANN 2021</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Farkaš</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Masulli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Otte</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Wermter</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12892</biblScope>
			<biblScope unit="page" from="133" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2234" to="2242" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A sparse non-negative matrix factorization framework for identifying functional units of tongue behavior from MRI</title>
		<author>
			<persName><forename type="first">J</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="730" to="740" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A deep joint sparse non-negative matrix factorization framework for identifying the common and subject-specific functional units of tongue motion during speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page">102131</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Identifying the common and subject-specific functional units of speech movements via a joint sparse non-negative matrix factorization framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing</title>
		<imprint>
			<biblScope unit="volume">11313</biblScope>
			<biblScope unit="page" from="446" to="451" />
			<date type="published" when="2020">2020. 2020</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking and improving relative position encoding for vision transformer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10033" to="10041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Phase vector incompressible registration algorithm for motion estimation from tagged magnetic resonance images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2116" to="2128" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3D tongue motion from tagged and cine MR images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Z</forename><surname>Murano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Prince</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-40760-4_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-40760-46" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2013</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Sakuma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Barillot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8151</biblScope>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Focal self-attention for local-global interactions in vision transformers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00641</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rest: an efficient transformer for visual recognition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15475" to="15485" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
