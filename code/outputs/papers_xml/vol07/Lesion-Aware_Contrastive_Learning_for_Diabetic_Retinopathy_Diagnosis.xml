<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis</title>
				<funder ref="#_4JdB8MM">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Pnc76Dj">
					<orgName type="full">Science Project of Liaoning province</orgName>
				</funder>
				<funder ref="#_v3TUreD">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingshan</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Cao</surname></persName>
							<email>caopeng@mail.neu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">National Frontiers Science Center for Industrial Intelligence and Systems Optimization</orgName>
								<address>
									<postCode>110819</postCode>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinzhu</forename><surname>Yang</surname></persName>
							<email>yangjinzhu@cse.neu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">National Frontiers Science Center for Industrial Intelligence and Systems Optimization</orgName>
								<address>
									<postCode>110819</postCode>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoli</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Osmar</forename><forename type="middle">R</forename><surname>Zaiane</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Alberta Machine Intelligence Institute</orgName>
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lesion-Aware Contrastive Learning for Diabetic Retinopathy Diagnosis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3F386CD762CDCB1AD62DA4F35C9B1EA7</idno>
					<idno type="DOI">10.1007/978-3-031-43990-263.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Diabetic Retinopathy</term>
					<term>Contrastive Learning</term>
					<term>Hard Negative Mining</term>
					<term>Knowledge Distillation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Early diagnosis and screening of diabetic retinopathy are critical in reducing the risk of vision loss in patients. However, in a real clinical situation, manual annotation of lesion regions in fundus images is time-consuming. Contrastive learning(CL) has recently shown its strong ability for self-supervised representation learning due to its ability of learning the invariant representation without any extra labelled data. In this study, we aim to investigate how CL can be applied to extract lesion features in medical images. However, can the direct introduction of CL into the deep learning framework enhance the representation ability of lesion characteristics? We show that the answer is no. Due to the lesion-specific regions being insignificant in medical images, directly introducing CL would inevitably lead to the effects of false negatives, limiting the ability of the discriminative representation learning. Essentially, two key issues should be considered: (1) How to construct positives and negatives to avoid the problem of false negatives? (2) How to exploit the hard negatives for promoting the representation quality of lesions? In this work, we present a lesion-aware CL framework for DR grading. Specifically, we design a new generating positives and negatives strategy to overcome the false negatives problem in fundus images. Furthermore, a dynamic hard negatives mining method based on knowledge distillation is proposed in order to improve the quality of the learned embeddings. Extensive experimental results show that our method significantly advances state-of-the-art DR grading methods to a considerable 88.0%ACC/86.8% Kappa on the EyePACS benchmark dataset. Our code is available at https://github.com/IntelliDAL/Image.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Diabetic retinopathy(DR) is a common long-term complication of diabetes that can lead to impaired vision and even blindness as the disease worsen <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b15">14]</ref>. Hence, conducting large-scale screening for early DR is an essential step to prevent visual impairment in patients. Screening fundus images by the ophthalmologist alone is not sufficient to prevent DR on a large scale, and the diagnosis of DR heavily relies on the experience of the ophthalmologist <ref type="bibr" target="#b0">[1]</ref>. Therefore, the automatic DR diagnosis on retinal fundus images is urgently needed <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">25]</ref>. Recently, in light of the powerful feature extraction and representation capabilities of convolutional neural networks, deep learning technology has developed rapidly in medical image analysis <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">22]</ref>. However, leveraging only the image-level grading annotation hinders deep learning algorithms from extracting features of suspicious lesion regions, which further affects the diagnosis of diseases. For these reasons, some previous work <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b20">19]</ref> considers the introduction of pixel-level lesion annotation to improve the model's feature extraction capability for lesion regions. Despite the methods have achieved promising results, the large-scale pixel-level annotation process is time-intensive and error-prone which imposes a heavy burden on the ophthalmologist. To address this problem, contrastive learning(CL) <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b24">23]</ref> has received a great deal of attention in medical images, but how to harness the power of CL in the medical applications remains unclear.</p><p>The challenges mainly lie in: (I) The diagnosis of fundus diseases relies more on local pathological features (haemorrhages, microaneurysms, etc.) than on the global information. How can contrastive learning enable models to extract features of lesion information more effectively on the large datasets with only imagelevel annotation? (II) The false negatives tend to disrupt the feature extraction of contrastive learning <ref type="bibr" target="#b27">[26]</ref>, resulting in the issue of inaccurate alignment of feature distributions <ref type="bibr" target="#b19">[18]</ref> (i.e. similar samples have dissimilar features). How to address the issue of false negatives caused by introducing contrastive learning into automatic disease diagnosis? (III) The performance of contrastive learning benefits from the hard negatives <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">16]</ref>. How to effectively exploit hard negatives for improving the quality of the learned feature embeddings?</p><p>To address the aforementioned issues, we propose the lesion-aware CL framework for DR grading. Specifically, to eliminate false negatives during contrastive learning introduced in automatic disease diagnosis and ensure that samples having similar semantic information stay close in the joint embedding space, we first capture lesion regions in fundus images using a pre-trained lesion detector. Based on the detected regions, we construct a lesion patch set and a healthy patch set, respectively. Then, we develop an encoder and a momentum encoder <ref type="bibr" target="#b6">[6]</ref> for extracting the features of positives (lesion patches) and negatives (healthy patches). The introduced momentum encoder enables the contrastive learning to maintain consistency in critical features while creating different perspectives for the positive samples. Secondly, considering the critical role of hard negatives in the contrastive learning, we formulate a two-stage scheme based on knowledge distillation <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b22">21]</ref> to dynamically exploit hard negatives, which further enhances the lesion-aware capability of the diagnosis models, and further improves the quality of the learned feature embeddings. Finally, we fine-tune the proposed framework in the DR grading task to demonstrate its effectiveness.</p><p>To the best of our knowledge, this is the first work to rethink the potential issues of contrastive learning for medical image analysis. In summary, our contributions can be summarized as follows. (1) A new scheme of constructing positives and negatives is proposed to prevent false negatives from disrupting the extraction of lesion features. This design can be easily extended to other types of medical images with less prominent physiological features to achieve better lesion representation. (2) To enhance the capability of CL in extracting lesion features for medical fundus image analysis and improve the quality of learned feature embeddings, a lesion-aware CL framework is proposed for sufficiently exploiting hard negatives. (3) We evaluate our framework on the large-scale EyePACS dataset for DR grading. The experimental results indicate the proposed method leads to a performance boost over the state-of-the-art DR grading methods. Fig. <ref type="figure" target="#fig_0">1</ref> shows the illustration of the proposed framework. In stage 1, we construct positives and negatives based on a pre-trained lesion detector pre-trained on a auxiliary dataset (IDRiD <ref type="bibr" target="#b16">[15]</ref>) with pixel lesion annotation, to avoid the effect of false negatives on the learned feature embeddings while aligning samples with similar semantic features. In stage 2, a dynamically sampling method is developed based on knowledge distillation to effectively exploit hard negatives and improve the quality of the learned feature embeddings. In the last stage, we finetune our model on the downstream DR grading task. Remarkably, to bridge the gap between local patches in the pretext task and global images in the downstream task, we introduce an attention mechanism on the fragmented patches to highlight the contributions of different patches on the grading results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Construction of Positives and Negatives</head><p>In this section, we provide a detailed description regarding the construction of positives and negatives. As opposed to traditional CL working on the whole medical images, it is essential to enable the model to focus more on the lesion regions in the images. Our goal is to eliminate the effect of false negatives on contrastive learning for obtaining a better representation of the lesion features. Specifically, given a training dataset X with five labels (1-4 indicating the increasing severity of DR, 0 indicating healthy). We first divide dataset X into lesion subset X L and healthy subset X H based on the disease grade labels of X. Then, we apply a pre-trained detector f det (•) only on X L and obtain high-confidence detection regions. Finally, the construction process of positives P = {p 1 , p 2 , . . . , p j } and negatives N = {n 1 , n 2 , . . . , n k } can be represented as P = Ω(f det (X L ) &gt; conf) and N = Randcrop(X H ), where conf denotes the confidence threshold of detection results, Ω(•) indicates the operation of expanding the predicted boxes of f det (•) to 128*128 for guaranteeing that the lesions are included as much as possible, and Randcrop(•) indicates randomly cropping images into patches with 128*128 from the healthy images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dynamic Hard Negatives Mining Enhances Contrastive Learning</head><p>Given the constructed positives P and negatives N , a negatives sampling scheme based on offline knowledge distillation is developed to enable contrastive learning to dynamically exploit hard negatives, and we adjust the update mechanism of the negatives queue(i.e. only enqueue and dequeue N to avoid confusion with P ) to better adapt contrastive learning to the medical image analysis task.</p><p>Training the Teacher Network. With the positives P , we obtain two views P = {p 1 , p2 , p3 ...p j } and P = {p 1 , p 2 , p 3 ...p j } by data augmentation(i.e. color distortion rotation, cropping followed by resize). Correspondingly, with the negatives N , to increase the diversity of the negatives, we apply a similar data augmentation strategy to obtain the augmented negatives Ñ = {ñ 1 , ñ2 , ñ3 ...ñ k } (where k j). We feed P and P + Ñ to the encoder En(•) and the momentum encoder MoEn(•) to obtain their embeddings Z = {z 1 , ..., z j |z j = En(p j )}, Z = {z 1 , ..., z j |z j = MoEn(p j )} and Z = {z 1 , ..., zk |z k = MoEn(ñ k )}. Then, we calculate the positive and negative similarity matrix by the samples of Z, Z and Z. According to the similarity matrix, the contrastive loss L cl-t of the teacher model training process can be defined as:</p><formula xml:id="formula_0">L cl-t = - log exp A t p /τ exp(A t p /τ ) + exp (A t n /τ ) = - j log exp sim z j , z j /τ exp(sim(z j , z j )/τ ) + k exp (sim (z j , zk ) /τ ) ,<label>(1)</label></formula><p>where sim z j , z j /z k = dot(zj ,z j /z k ) zj 2 z j /z k 2 , τ denotes a temperature parameter, A t p and A t n represent the similarity matrix of positives and negatives, respectively. In order to create a positive sample view different from that of En(•), it should be noted that the parameters θ q of En(•) are updated using gradient descent, while MoEn(•) introduces an extra momentum coefficient m = 0.99 to update its parameters</p><formula xml:id="formula_1">θ k ← mθ k + (1 -m)θ q .</formula><p>Training the Student Network. Previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">16]</ref> reveal that not all negatives are useful for the contrastive learning. Moreover, the hard negatives may exhibit more semantically similar to the positives than the normal negatives, indicating that hard negatives provide more potentially useful information for facilitating the following DR grading. Meanwhile, the number of hard negatives significantly affects the difficulty of training the model, in other words, the network should be capable of dynamically adjust the optimisation process by controlling the number of hard negatives. In light of the above two points, we formulate and introduce a well-balanced strategy of hard negatives during the training phase of the student model. Specifically, based on the trained teacher model, we first input P and N into both the teacher and student models to generate similarity matrices A t p , A t n and A s p , A s n , respectively. According to the negative similarity matrix A t n produced by the teacher model, we prioritise the negatives that are likely to be confused with the positives in descending order and only select the top δ samples for distillation learning during the student model's training phase. For each negative zk in A t n , the resampled negative set A t n can be defined as:</p><formula xml:id="formula_2">A t n = zk | zk ∈ Sort(A t n ), sim (z j , zk ) ≥ sim(z j , zγ }, (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where γ = δ/(cos( πs 2S ) + 1) represents the number of the current hard negatives, s and S denotes the current and maximum training step, respectively. As s increases during the training process, we dynamically adjust the number of hard negatives such that the difficulty of distillation learning proceeds from easy to hard. Based on the index in A t n , the elements at the corresponding positions in A s n are obtained and a resampled negatives similarity matrix A s n is constructed. Hence, the CL loss L cl-s in training process of student can be formulated as:</p><formula xml:id="formula_4">L cl-s = - log exp A s p /τ exp(A s p /τ ) + k exp (A s n /τ ) ,<label>(3)</label></formula><p>In addition, to improve the quality of embeddings learned by the student model, we leverage the generated similarity matrices to facilitate the richer knowledge distilled from the teacher to the student. Formally, the KL-divergence loss L kd between A t p , A t n and A s p , A s n is represented as follows.</p><formula xml:id="formula_5">L kd = -τ 2 C(A t p , A t n ) log C(A s p , A s n ) , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where C(•) denotes the matrix concatenation. The final loss of the student model is L = L cl-s + λ 1 L kd , where the λ 1 is a positive parameter controlling the weight of the knowledge distillation loss L kd .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">DR Grading Task</head><p>To evaluate the effectiveness of the proposed method, we take the encoder of the pre-trained student model as a backbone and fine-tune it for the downstream DR grading task. Considering that the proposed contrastive learning framework is trained with patches, whereas the downstream grading task relies on entire fundus images, an additional attention mechanism is incorporated to break the gap between the inputs of pretext and downstream tasks. Specifically, we first fragment the entire fundus image into patches x = {x p1 , . . . , x pi }. Then, feature embedding v i of x pi is generated by the encoder. Meanwhile, an attention module with two linear layers is utilized in the DR grading task to obtain the attention weight α i of each patch x pi .</p><formula xml:id="formula_7">α i = sof tmax W T 2 max LayerN orm W 1 v T i , 0 ,<label>(5)</label></formula><p>where W 1 , W 2 are the parameters of the two linear layers , LayerN orm is the layernorm function. Finally, α i is assigned to the corresponding patch's embedding v i to highlight the contribution of patch x pi , and the predicted results of DR obtained by ŷ = W T 3 • N i=1 α i v i , and W T 3 is parameter of the grading layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Implementation Details</head><p>EyePACS <ref type="bibr" target="#b3">[4]</ref>. EyePACS is the largest public fundus dataset which contains 35,126 training images and 53,576 testing images with only image-level DR grading labels. According to the severity of DR, images are classified into five grades: 0 (normal), 1 (mild), 2 (moderate), 3 (severe), and 4 (prolifera-tive).</p><p>Implementation Details. The proposed framework is implemented by Pytorch on two Tesla T4 Tensor core GPUs. We employ the IDRiD dataset <ref type="bibr" target="#b16">[15]</ref> for the pretraining of the lesion detector f det (•). During the sample construction stage, considering the diversity of sizes of the original fundus images, all images are resized to 768 × 768, and the data enhancement strategies include random rotation, flipping and color distortion. During the phase of dynamically mining hard negatives, the Adam optimizer with momentum 0.9 is applied to train and update the parameters of the framework with 800 epochs, the initial learning rate of 1×10 -3 and the batch size of 400. The designed hyper-parameter δ is set to be 4,000 after extra experiments (please refer to the supplementary materials for more details). In the downstream DR grading task, we fine-tune the encoder(i.e. ResNet50) for 25 epochs with an initial learning rate of 1×10 -4 and a batch size of 32. In addition to the normal classification accuracy, we also introduce the quadratic weighted kappa metric to reflect the performance of the proposed method and a range of comparable methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with the State-of-the-Art</head><p>In this section, we provide qualitative and quantitative comparisons with various DR grading methods and demonstrate the effectiveness of the proposed method.</p><p>As shown in Table <ref type="table" target="#tab_0">1</ref>, we conduct a comprehensive comparison of the proposed method with three types of comparable methods: covering the popular backbone network <ref type="bibr" target="#b7">[7]</ref>, the top two places of Kaggle challenge <ref type="bibr" target="#b3">[4]</ref> and the current SOTA DR grading methods <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b25">24]</ref>. From the Table <ref type="table" target="#tab_0">1</ref>, it can be observed that our method consistently achieves the best results with respect to both the Kappa and Accuracy. The results show that our framework presents a notably better DR grading performance than the SOTA methods due to improve quality of the learned lesion embeddings by eliminating the false negatives and dynamically mining hard negatives, and in turn enhancing the lesion-awareness of CL, which is beneficial for DR grading. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>To more comprehensively evaluate the Lesion-aware CL, we conduct ablation studies to analyze the correlation among DR grading, the construction of positives and negatives(CPN) and dynamic hard negatives mining(DHM). We compare the proposed method with its several variants. The results of ablation study are reported in Table <ref type="table" target="#tab_1">2</ref>. We can draw conclusions from several aspects: (1) CL shows the worst performance and the performance of Lesion-aware CL w/o CPN is obviously degraded compared to Lesion-aware CL (i.e. kappa reduces 1.5% ). The results suggest that CPN is critical for improving the performance when contrastive learning is introduced in fundus images. Without false negatives disrupting the feature extraction procedure of lesions, the model is able to extract a better representation for the regions of lesions and thus achieve better DR grading performance. (2) Lesion-aware CL w/o DHM performs worse than Lesion-aware CL. As opposed to the common CL methods which uses all negative samples, our model takes into account the difference of the negatives with difficulty level. The teacher network is able to dynamically exploit the hard negatives and transfer the learned knowledge to the student, thereby improving the quality of the feature embeddings in subsequent Fig. <ref type="figure">2</ref>. Visualization results from GradCAM between the four representative methods contrastive learning. Figure <ref type="figure">2</ref> shows the visualization results from GradCAM of four representative methods including Resnet50, the common CL methods (MoCo, CL-DR), and the Lesion-aware CL. Two cases with proliferate DR(DR-4) are visualized by four representative models. The intensity of the heatmap indicates the importance of each pixel in the corresponding image for making the prediction. In case 1, both Resnet and typical CL methods focus on the optic disc where has obvious physiological characteristics, while our method focuses more on the lesion regions and less on the structural aspects of the fundus image. In case 2, our method provides a promising perception of the lesion regions than other methods, suggesting that our approach allows the DR grading model to learn better representation of lesion and thus be sensitive to the DR grading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel lesion-aware CL framework for DR grading. The proposed method first overcomes the false negatives problem by reconstructing positives and negatives. Then, to improve the quality of learned feature embeddings and enhance the awareness for lesion regions, we design the dynamic hard negatives mining scheme based on knowledge distillation. The experimental results demonstrate that the proposed framework significantly improves the latest results of DR grading on the benchmark dataset. Furthermore, our approaches are migratable and can be easily applied to other medical image analysis tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The overall architecture of the proposed framework. Stage 1: The construction of positives and negatives based on the pre-trained lesion detector. Stage 2: Dynamic hard negatives mining enhances contrastive learning. Stage 3: Fine-tuning our model on the downstream diabetic retinopathy grading task.</figDesc><graphic coords="3,70,98,292,19,310,54,196,33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(1) CL: the proposed model is trained without CPN and DHM, it indicates a basic CL method. (2) Lesionaware CL w/o CPN: the model is trained without CPN. (3) Lesion-aware CL w/o DHM: the model is trained without DHM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The comparison between our method and the SOTA methods in DR grading task on EyePACS dataset</figDesc><table><row><cell>Model</cell><cell cols="2">Kappa Accuracy Model</cell><cell cols="2">Kappa Accuracy</cell></row><row><cell>Resnet50 [7]</cell><cell>0.823 0.845</cell><cell>AFN(2019) [12]</cell><cell>0.859</cell><cell>-</cell></row><row><cell>Min-pooling [4]</cell><cell>0.849 -</cell><cell>DeepMT-DR(2021) [19]</cell><cell>0.839</cell><cell>0.857</cell></row><row><cell>o O [4]</cell><cell>0.844 -</cell><cell>CL-DR(2021) [10]</cell><cell>0.832</cell><cell>0.848</cell></row><row><cell cols="2">Zoom-in-Net(2017) [20] 0.854 0.873</cell><cell>CLEAQ-DR(2022) [9]</cell><cell>0.863</cell><cell>-</cell></row><row><cell>MMCNN(2018) [24]</cell><cell>0.841 0.862</cell><cell cols="3">Lesion-aware CL (Ours) 0.868 0.880</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The ablation experiment results of the proposed framework on EyePACS</figDesc><table><row><cell>Method</cell><cell>Kappa</cell><cell>Accuracy</cell></row><row><cell>CL</cell><cell>0.844</cell><cell>0.858</cell></row><row><cell cols="3">Lesion-aware CL w/o CPN 0.853(+0.9%) 0.864(+0.6%)</cell></row><row><cell cols="3">Lesion-aware CL w/o DHM 0.857 (+1.3%) 0.871(+1.3%)</cell></row><row><cell>Lesion-aware CL</cell><cell cols="2">0.868(+2.4%) 0.880(+2.2%)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This research was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No.<rs type="grantNumber">62076059</rs>), the <rs type="funder">Science Project of Liaoning province</rs> under Grant (<rs type="grantNumber">2021-MS-105</rs>) and the 111 Project (<rs type="grantNumber">B16009</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4JdB8MM">
					<idno type="grant-number">62076059</idno>
				</org>
				<org type="funding" xml:id="_Pnc76Dj">
					<idno type="grant-number">2021-MS-105</idno>
				</org>
				<org type="funding" xml:id="_v3TUreD">
					<idno type="grant-number">B16009</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Expert-validated estimation of diagnostic uncertainty for deep neural networks in diabetic retinopathy detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ayhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kühlewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aliyeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Inhoffen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ziemssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page">101724</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06682</idno>
		<title level="m">Are all negatives created equal in contrastive instance discrimination? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Collaborative learning of weaklysupervised domain adaptation for diabetic retinopathy grading on retinal images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zaiane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page">105341</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Diabetic retinopathy detection</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W C</forename></persName>
		</author>
		<ptr target="https://kaggle.com/competitions/diabetic-retinopathy-detection" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluation of retinal image quality assessment networks in different color-spaces</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="48" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-76" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="38" to="39" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image quality assessment guided collaborative learning of image enhancement and classification for diabetic retinopathy grading</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaiane</surname></persName>
		</author>
		<idno type="DOI">10.1109/JBHI.2022.3231276</idno>
		<ptr target="https://doi.org/10.1109/JBHI.2022.3231276" />
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inform</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lesion-based contrastive learning for diabetic retinopathy grading from fundus images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-311" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-supervised feature learning via exploiting multi-modal data for retinal disease diagnosis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4023" to="4033" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A framework for identifying diabetic retinopathy based on anti-noise detection and attention-based fusion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11071</biblScope>
			<biblScope unit="page" from="74" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00934-2_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00934-29" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Digit. Health</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="271" to="e297" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">IDF diabetes atlas: global estimates of undiagnosed diabetes in adults for 2021</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ogurtsova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetes Res. Clin. Pract</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="page">109118</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Indian diabetic retinopathy image dataset (idrid): a database for diabetic retinopathy screening research</title>
		<author>
			<persName><forename type="first">P</forename><surname>Porwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pachade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kamble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kokare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meriaudeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Contrastive learning with hard negative samples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling and enhancing low-quality retinal fundus images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="996" to="1006" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6827" to="6839" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep multi-task learning for diabetic retinopathy grading in fundus images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2826" to="2834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Zoom-in-Net: deep mining lesions for diabetic retinopathy detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66179-7_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-66179-7" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2017</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Descoteaux</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Franz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Jannin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Collins</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Duchesne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10435</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge distillation meets self-supervision</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58545-7_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58545-734" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12354</biblScope>
			<biblScope unit="page" from="588" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MIL-VT: multiple instance learning enhanced vision transformer for fundus image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-35" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automated detection of diabetic retinopathy using a binocular Siamese-like convolutional network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-cell multi-task convolutional neural networks for diabetic retinopathy grading</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2724" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A benchmark for studying diabetic retinopathy: segmentation, grading, and transferability</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="818" to="828" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CrossCLR: cross-modal contrastive learning for multi-modal video representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1450" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
