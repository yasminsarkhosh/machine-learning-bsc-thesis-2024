<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Alexandre</forename><surname>Cafaro</surname></persName>
							<email>a.cafaro@therapanacea.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">TheraPanacea</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Gustave Roussy</orgName>
								<orgName type="institution">Paris-Saclay University</orgName>
								<address>
									<addrLine>Inserm 1030</addrLine>
									<settlement>Villejuif</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Radiation Oncology</orgName>
								<orgName type="department" key="dep2">Centre Léon Bérard</orgName>
								<address>
									<settlement>Lyon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quentin</forename><surname>Spinat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TheraPanacea</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amaury</forename><surname>Leroy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TheraPanacea</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Gustave Roussy</orgName>
								<orgName type="institution">Paris-Saclay University</orgName>
								<address>
									<addrLine>Inserm 1030</addrLine>
									<settlement>Villejuif</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Radiation Oncology</orgName>
								<orgName type="department" key="dep2">Centre Léon Bérard</orgName>
								<address>
									<settlement>Lyon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pauline</forename><surname>Maury</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Munoz</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Gustave Roussy</orgName>
								<orgName type="institution">Paris-Saclay University</orgName>
								<address>
									<addrLine>Inserm 1030</addrLine>
									<settlement>Villejuif</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Radiation Oncology</orgName>
								<orgName type="department" key="dep2">Centre Léon Bérard</orgName>
								<address>
									<settlement>Lyon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Beldjoudi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Radiation Oncology</orgName>
								<orgName type="department" key="dep2">Centre Léon Bérard</orgName>
								<address>
									<settlement>Lyon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charlotte</forename><surname>Robert</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Gustave Roussy</orgName>
								<orgName type="institution">Paris-Saclay University</orgName>
								<address>
									<addrLine>Inserm 1030</addrLine>
									<settlement>Villejuif</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Deutsch</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Gustave Roussy</orgName>
								<orgName type="institution">Paris-Saclay University</orgName>
								<address>
									<addrLine>Inserm 1030</addrLine>
									<settlement>Villejuif</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Grégoire</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Radiation Oncology</orgName>
								<orgName type="department" key="dep2">Centre Léon Bérard</orgName>
								<address>
									<settlement>Lyon</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="institution" key="instit1">LIGM</orgName>
								<orgName type="institution" key="instit2">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">TheraPanacea</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="699" to="709"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">F11BD90AB700D7462D3CD712CF939AF3</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_66</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Image reconstruction</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an unsupervised deep learning method to reconstruct a 3D tomographic image from biplanar X-rays, to reduce the number of required projections, the patient dose, and the acquisition time. To address this ill-posed problem, we introduce prior knowledge of anatomic structures by training a generative model on 3D CTs of head and neck. We optimize the latent vectors of the generative model to recover a volume that both integrates this prior knowledge and ensures consistency between the reconstructed image and input projections. Our method outperforms recent methods in terms of reconstruction error while being faster and less radiating than current clinical workflow. We evaluate our method in a clinical configuration for radiotherapy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tomographic imaging estimates body density using hundreds of X-ray projections, but it's slow and harmful to patients. Acquisition time may be too high for certain applications, and each projection adds dose to the patient. A quick, low-cost 3D estimation of internal structures using only bi-planar X-rays can revolutionize radiology, benefiting dental imaging, orthopedics, neurology, and more. This can improve image-guided therapies and preoperative planning, especially for radiotherapy, which requires precise patient positioning with minimal radiation exposure.</p><p>However, this task is an ill-posed inverse problem: X-ray measurements are the result of attenuation integration across the body, which makes them very Fig. <ref type="figure">1</ref>. Current methods vs our method. Feed-forward methods do not manage to predict a detailed and matching tomographic volume from a few projections. Iterative methods based on neural radiance fields lack prior for good reconstruction. By learning an embedding for the possible volumes, we can recover an accurate volume from very few projections with an optimization based on a Bayesian formulation.</p><p>ambiguous. Traditional reconstruction methods require hundreds of projections to get sufficient constraints on the internal structures. With very few projections, it is very difficult to disentangle the structures for even coarse 3D estimation. In other words, many 3D volumes may have generated such projections a priori.</p><p>Classical analytical and iterative methods <ref type="bibr" target="#b7">[8]</ref> fail when very few projections are available. Several works have attempted to largely decrease the number of projections needed for an accurate volumetric reconstruction. Some deep learning methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">30]</ref> predict directly a 3D volume in a forward way from very few projections. The volume is however not guaranteed to be consistent with the projections and it is not clear which solution is retrieved. Other recent methods have adapted NeRFs <ref type="bibr" target="#b19">[20]</ref> to tomographic reconstruction <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">31]</ref>. These non-learning methods show good results when the number of input projections remains higher than a dozen but fail when very few projections are provided, as our experiments in Sect. 3.3 show.</p><p>As illustrated in Fig. <ref type="figure">1</ref>, to be able to reconstruct a volume accurately given as low as two projections only, we first learn a prior on the volume. To do this, we leverage the potential of generative models to learn a low-dimensional manifold of the target body part. Given projections, we find by a Bayesian formulation the intermediate latent vectors conditioning the generative model that minimize the error between synthesized projections of our reconstruction and these input projections. Our work builds on Hong et al. <ref type="bibr" target="#b9">[10]</ref>'s 3D style-based generative model, which we extend via a more complex network and training framework.</p><p>Compared to other 3D GANs, it is proven to provide the best disentanglement of the feature space related to semantic features <ref type="bibr" target="#b1">[2]</ref>.</p><p>By contrast with feed-forward methods, our approach does not require paired projections-reconstructions, which are very tedious to acquire, and it can be used with different numbers of projections and different projection geometries without retraining. Compared to NeRF-based methods, our method exploits prior knowledge from many patients to require only two projections. We evaluate our method on reconstructing cancer patients' head-and-neck CTs, which involves intricate and complicated structures. We perform several experiments to compare our method with a feed-forward-based method <ref type="bibr" target="#b30">[30]</ref> and a recent NeRF-based method <ref type="bibr" target="#b22">[23]</ref>, which are the previous state-of-the-art methods for the very few or few projections cases, respectively.</p><p>We show that our method allows to retrieve results with the finest reconstructions and better matching structures, for a variety of number of projections. To summarize, our contributions are two-fold: (i) A new paradigm for 3D reconstruction with biplanar X-rays: instead of learning to invert the measurements, we leverage a 3D style-based generative model to learn deep image priors of anatomic structures and optimize over the latent space to match the input projections; (ii) A novel unsupervised method, fast and robust to sampling ratio, source energy, angles and geometry of projections, all of which making it general for downstream applications and imaging systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Figure <ref type="figure">2</ref> gives an overview of the pipeline we propose. We first learn the lowdimensional manifold of CT volumes of a target body region. At inference, we estimate the Maximum A Posteriori (MAP) volume on this manifold given very few projections: we find the latent vectors that minimize the error between the synthetic projections from the corresponding volume on the manifold and the real ones. In this section, we formalize the problem, describe how we learn the manifold, and detail how we optimize the latent vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>Given a small set of projections {I i } i , possibly as few as two, we would like to reconstruct the 3D tomographic volume v that generates these projections. This is a hard ill-posed problem, and to solve it, we need prior knowledge about the possible volumes. To do this, we look for the maximum a posteriori (MAP) estimate given the projections {I i } i :</p><formula xml:id="formula_0">v * = argmax v p(v|{I i } i ) = argmax v p(v)p({I i } i |v) = argmin v i L(v, I i ) + R(v) .</formula><p>(1) Term L(v, I i ) is a log-likelihood. We take it as:</p><formula xml:id="formula_1">L(v, I i ) = λ 2 A i • v -I i 2 + λ p L p (A i • v, I i ) ,<label>(2)</label></formula><p>Fig. <ref type="figure">2</ref>. Our pipeline. We first learn the low-dimensional manifold of 3D structures using a generative model. Then, given projections, we find the latent vectors that minimize the error between the projections of our generation and the input projections.</p><p>where A i is an operator that projects volume v under view i. We provide more details about operator A in Sect. 2.3. L p is the perceptual loss <ref type="bibr" target="#b12">[13]</ref> between projection of v and the observed projection</p><formula xml:id="formula_2">I i . Term R(v) is a regularization term.</formula><p>It is crucial as it is the term that embodies prior knowledge about the volume to reconstruct. As discussed in the introduction, we rely on a generative model, which we describe in the next section. Then, we describe how exactly we use this generative model for regularization term R(v) and how this changes our optimization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Manifold Learning</head><p>To regularize the domain space of solutions, we leverage a style-based generative model to learn deep priors of anatomic structures. Our model relies on Style-GAN2 <ref type="bibr" target="#b14">[15]</ref> that we extend in 3D by changing the 2D convolutions into 3D ones as done in 3DStyleGAN <ref type="bibr" target="#b9">[10]</ref> except that we start from the StyleGAN2 architecture.</p><p>Our generator G generates a volume v given a latent vector w and Gaussian noise vectors n = {n j } j : v = G(w, n). Latent vector w ∈ N (w|μ, σ) is computed from an initial latent vector z ∈ N (0, I ) mapped using a learned network m: w = m(z). w controls the global structure of the predicted volumes at different scales by its components w i , while the noise vectors n allow more fine-grained details. The mean μ and standard deviation σ of the mapped latent space can be computed by mapping over initial latent space N (0, I ) after training. The mapping network learns to disentangle the initial latent space relatively to semantic features which is crucial for the inverse problem. We train this model using the non-saturating logistic loss <ref type="bibr" target="#b4">[5]</ref> and path length regularization <ref type="bibr" target="#b14">[15]</ref>. For the discriminator, we use the non-saturating logistic loss with R1 regularization <ref type="bibr" target="#b18">[19]</ref>. We implement adaptive discriminator augmentation from StyleGAN-ADA <ref type="bibr" target="#b13">[14]</ref> to improve learning of the model's manifold with limited medical imaging data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reconstruction from Biplanar Projections</head><p>Since our generative model provides a volume v as a function of vectors w and n, we can reparameterize our optimization from Eq. ( <ref type="formula">1</ref>) into:</p><formula xml:id="formula_3">w * , n * = argmin w,n i L(G(w, n), I i ) + R(w, n) . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>Note that by contrast with <ref type="bibr" target="#b17">[18]</ref> for example, we optimize on the noise vectors n as well: as we discovered in our early experiments, the n are also useful to embed high-resolution details. We take our regularization term R(w, n) as:</p><formula xml:id="formula_5">R(w, n) = λ w L w (w) + λ c L c (w) + λ n L n (n) . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Term L w (w) =k log N (w k |μ, σ) ensures that w lies on the same distribution as during training. N (•|μ, σ) represents the density of the standard normal distribution of mean μ and standard deviation σ.</p><p>Term L c (w) =i,j log M(θ i,j |0, κ) encourages the w i vectors to be collinear so to keep the generation of coarse-to-fine structures coherent. M(•; μ, κ) is the density of the Von Mises distribution of mean μ and scale κ, which we take fixed, and θ i,j = arccos( wi•wj wi wj ) is the angle between vectors w i and w j .</p><p>Term L n (n) =j log N (n j |0, I ) ensures that the n j lie on the same distribution as during training, i.e., a multivariate standard normal distribution. The λ * are fixed weights.</p><p>Projection Operator. In practice, we take operator A as a 3D cone beam projection that simulates X-ray attenuation across the patient, adapted from <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>. We model a realistic X-ray attenuation as a ray tracing projection using material and spectrum awareness:</p><formula xml:id="formula_7">I atten = E I 0 e -m µ(m,E)tm , (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>with μ(m, E) the linear attenuation coefficient of material m at energy state E that is known <ref type="bibr" target="#b10">[11]</ref>, t m the material thickness, I 0 the intensity of the source X-ray.</p><p>For materials, we consider the bones and tissues that we separate by threshold on electron density. A inverts the attenuation intensities I atten to generate an X-ray along few directions successively. We make A differentiable using <ref type="bibr" target="#b20">[21]</ref> to allow end-to-end optimization for reconstruction.</p><p>3 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Preprocessing</head><p>Manifold Learning. We trained our model with a large dataset of 3500 CTs of patients with head-and-neck cancer, more exactly 2297 patients from the publicly available The Cancer Imaging Archive (TCIA) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b32">32]</ref> and 1203 from private internal data, after obtention of ethical approbations. We split this data into 3000 cases for training, 250 for validation, and 250 for testing. We focused CT scans on the head and neck region above shoulders, with a resolution of 80 × 96 × 112, and centered on the mouth after automatic segmentation using a pre-trained U-Net <ref type="bibr" target="#b21">[22]</ref>. The CTs were preprocessed by min-max normalization after clipping between -1024 and 2000 Hounsfield Units (HU).</p><p>3D Reconstruction. To evaluate our approach, we used an external private cohort of 80 patients who had undergone radiotherapy for head-and-neck cancer, with their consent. Planning CT scans were obtained for dose preparation, and CBCT scans were obtained at each treatment fraction for positioning with full gantry acquisition. As can be seen in Fig. <ref type="figure" target="#fig_0">3</ref> and the supplementary material, all these cases are challenging as there are large changes between the original CT scan and the CBCT scans. We identified these cases automatically by comparing the CBCTs with the planning CTs. To compare our reconstruction in the calibrated HU space, we registered the planning CTs on the CBCTs by deformable registration with MRF minimization <ref type="bibr" target="#b3">[4]</ref>. We hence obtained 3D volumes as virtual CTs we considered as ground truths for our reconstructions after normalization. From these volumes, we generated projections using the projection module described in Sect. 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>Manifold Learning. We used Pytorch to implement our model, based on Style-GAN2 <ref type="bibr" target="#b14">[15]</ref>. It has a starting base layer of 256 × 5 × 6 × 7 and includes four upsamplings with 3D convolutions and filter maps of 256, 128, 64, 32. We also used 8 fully-convolutional layers with dimension 512 and an input latent vector of dimension 512, with tanh function as output activation. To optimize our model, we used lazy regularization <ref type="bibr" target="#b14">[15]</ref> and style mixing <ref type="bibr" target="#b14">[15]</ref>, and added a 0.2 probability for generating images without Gaussian noise to focus on embedding the most information. We augmented the discriminator with vertical and depthoriented flips, rotation, scaling, motion blur and Gaussian noise at a probability of 0.2. Our training used mixed precision on a single GPU Nvidia Geforce GTX 3090 with a batch size of 6, and we optimized the generator, discriminator, and mapping networks using Adam at learning rates 6e-5 and 1e-5 to avoid mode collapse and unstable training. After training for 4 weeks, we achieved stabilization of the Fréchet Inception Distance (FID) <ref type="bibr" target="#b8">[9]</ref> and Multi-scale Structural Similarity (MS-SSIM) <ref type="bibr" target="#b29">[29]</ref> on the validation set.</p><p>3D Reconstruction. For the reconstruction, we performed the optimization on GPU V100 PCI-E using Adam, with learning rate of 1e-3. By grid search on the validation set, we selected the best weights that well balance between structure and fine-grained details, λ 2 = 10, λ p = 0.1, λ w = 0.1, λ c = 0.05, λ n = 10. We perform 100 optimization steps starting from the mean of the mapped latent space, which takes 25 s, enabling clinical use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and Discussion</head><p>Manifold Learning. We tested our model's ability to learn the low-dimensional manifold. We used FID <ref type="bibr" target="#b8">[9]</ref> to measure the distance between the distribution of generated volumes and real volumes, and MS-SSIM <ref type="bibr" target="#b29">[29]</ref> to evaluate volumes' diversity and quality. We obtained a 3D FID of 46 and a MS-SSIM of 0.92. For reference, compared to 3DStyleGAN <ref type="bibr" target="#b9">[10]</ref>, our model achieved half their FID score on another brain MRI dataset, with comparable MS-SSIM. This may be due to a more complex architecture, discriminator augmentation, or simpler anatomy.</p><p>Baselines. We compared our method against the main feed-forward method X2CT-GAN <ref type="bibr" target="#b30">[30]</ref> and the neural radiance fields with prior image embedding method NeRP <ref type="bibr" target="#b22">[23]</ref> meant for modest sparsely-sampled reconstruction. Recent methods like <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b11">[12]</ref> were excluded because they provide only minor improvements compared to X2CT-GAN <ref type="bibr" target="#b30">[30]</ref> and have similar constraints to feed-forward methods. Additionally, no public implementation is available. <ref type="bibr" target="#b25">[26]</ref> uses a flow-based generative model, but the results are of lower quality compared to GANs and similar to X2CT-GAN <ref type="bibr" target="#b30">[30]</ref>.</p><p>3D Reconstruction. To evaluate our method's performance with biplanar projections, we focused on positioning imaging for radiotherapy. Figure <ref type="figure" target="#fig_0">3</ref> compares our reconstruction with those of the baselines from biplanar projections. Our method achieves better fitting of the patient structure, including bones, tissues, and air separations, almost matching the real CT volume. X2CT-GAN <ref type="bibr" target="#b30">[30]</ref> produced realistic structures, but failed to match the actual structures as it does not enforce consistency with the projections. In some clinical procedures, an earlier CT volume of the patient may be available and can be used as an additional input for NeRP <ref type="bibr" target="#b22">[23]</ref>. Without a previous CT volume, NeRP lacks the necessary prior to accurately solve the ill-posed problem. Even when initialised with a previous CT volume, NeRP often fails to converge to the correct volume and introduces many artifacts when few projections are used. In contrast, our method is more versatile and produces better results. We used quantitative metrics (PSNR and SSIM) to evaluate reconstruction error and human perception, respectively. Table <ref type="table" target="#tab_0">1</ref> shows these metrics for our method and baselines with 1 to 8 cone beam projections. Deviation from projections, as in X2CT-GAN, leads to inaccurate reconstruction. However, relying solely on projection consistency is inadequate for this ill-posed problem. NeRP matches projections but cannot reconstruct the volume correctly. Our approach balances between instant and iterative methods by providing a reconstruction in 25 s with 100 optimization steps, while ensuring maximal consistency. In contrast, NeRP requires 7 min, and X2CT-GAN produces structures instantly but unmatching. Clinical CBCT acquisition and reconstruction by FDK <ref type="bibr" target="#b2">[3]</ref> take about 1-2 min and 10 s respectively. Our approach significantly reduces clin-ical time and radiation dose by using instant biplanar projections, making it promising for fast 3D visualization towards complex positioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion, Limitations, and Future Work</head><p>We proposed a new unsupervised method for 3D reconstruction from biplanar X-rays using a deep generative model to learn the structure manifold and retrieve the maximum a posteriori volume with the projections, leading to stateof-the-art reconstruction. Our approach is fast, robust, and applicable to various human body parts, making it suitable for many clinical applications, including positioning and visualization with reduced radiation.</p><p>Future hardware improvements may increase resolution, and our approach could benefit from other generative models like latent diffusion models. This approach may provide coarse reconstructions for patients with rare abnormalities, as most learning methods, but a larger dataset or developing a prior including tissue abnormalities could improve robustness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Visual comparison of 3D reconstruction from biplanar projections by our model and baselines. Without a previous CT volume, NeRP fails by lack of constraints. When initialized with an earlier CT (left), NeRP tends to create artefacts to match the projections rather than really change the anatomy. Our method produces better matching structures than X2CT-GAN, almost matching the CT volume deformed on the CBCT volume (GT, right).</figDesc><graphic coords="7,58,98,53,69,334,48,119,02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,43,29,54,47,337,36,187,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,43,29,54,62,337,45,274,09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Metrics for our method and baselines, for reconstruction from 1 to 8 cone beam projections. Standard deviations are provided in parentheses.</figDesc><table><row><cell>Method</cell><cell>1 Projection</cell><cell>2 Projections</cell></row><row><cell></cell><cell>PSNR (dB)↑ SSIM↑</cell><cell>PSNR (dB)↑ SSIM↑</cell></row><row><cell cols="3">NeRP (w/o prior volume) 14.8 (±2.7) 0.12 (±0.10) 18.4 (±3.8) 0.17 (±0.10)</cell></row><row><cell cols="3">NeRP (w/ prior volume) 22.5 (±3.2) 0.29 (±0.07) 23.5 (±3.5) 0.30 (±0.06)</cell></row><row><cell>X2CT-GAN</cell><cell cols="2">20.7 (±2.4) 0.57 (±0.07) 21.8 (±2.5) 0.72 (±0.08)</cell></row><row><cell>Ours</cell><cell cols="2">23.2 (±2.8) 0.79 (±0.09) 25.8 (±3.2) 0.85 (±0.10)</cell></row><row><cell></cell><cell>4 Projections</cell><cell>8 Projections</cell></row><row><cell cols="3">NeRP (w/o prior volume) 19.9 (±2.6) 0.21 (±0.04) 20.0 (±2.5) 0.23 (±0.05)</cell></row><row><cell cols="3">NeRP (w/ prior volume) 24.2 (±2.7) 0.32 (±0.05) 24.9 (±4.9) 0.34 (±0.08)</cell></row><row><cell>Ours</cell><cell cols="2">28.2 (±3.5) 0.89 (±0.10) 30.1 (±3.9) 0.92 (±0.11)</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_66.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Beichel</surname></persName>
		</author>
		<title level="m">Data from QIN-HEADNECK</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evaluation of 3D GANs for Lung Tissue Modelling in Pulmonary CT</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Practical cone-beam algorithm</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Feldkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kress</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A-Opt. Image Sci. Vis</title>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dense image registration through MRFs and efficient linear programming</title>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tziritas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="731" to="741" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HNSCC</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Anderson Cancer Center Head and Neck Quantitative Imaging Working Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rasche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Single-image tomography: 3D volumes from 2D cranial X-rays</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Herman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-84628-723-7</idno>
		<ptr target="https://doi.org/10.1007/978-1-84628-723-7" />
		<title level="m">Fundamentals of Computerized Tomography: Image Reconstruction from Projections</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3D-StyleGAN: a style-based generative adversarial network for generative modeling of three-dimensional medical images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-88210-5_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-88210-5_3" />
	</analytic>
	<monogr>
		<title level="m">DGM4MICCAI/DALI -2021</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Engelhardt</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13003</biblScope>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tables of X-Ray Mass Attenuation Coefficients 1 keV to 20 MeV for Elements Z=1 to 92 and 48 Additional Substance of Dosimetric Interest</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hubbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NISTIR</title>
		<imprint>
			<biblScope unit="volume">5632</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MFCT-GAN: multi-information network to reconstruct CT volumes for security screening</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Manuf. Spec. Equip</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="30" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46475-6_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46475-6_43" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Data from the ACRIN 6685 Trial HNSCC</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kinahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bialecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Coombs</surname></persName>
		</author>
		<idno>FDG-PET/CT</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Data from Radiomic Biomarkers to Refine Risk Models for Distant Metastasis in Oropharyngeal Carcinoma</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y Y</forename><surname>Kwan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Marinescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Bayesian Image Reconstruction Using Deep Generative Models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Which training methods for GANs do actually converge?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">NeRF: representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">XraySyn: realistic view synthesis from a single radiograph through CT priors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NeRP: implicit neural representation learning with prior embedding for sparsely sampled image reconstruction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A geometry-informed deep learning framework for ultra-sparse 3D tomographic image reconstruction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Capaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page">105710</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Patient-specific reconstruction of volumetric computed tomography images from a single projection view via deep learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="880" to="888" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the simulation of ultra-sparse-view and ultra-low-dose computed tomography with maximum a posteriori reconstruction using a progressive flow-based deep generative model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shibata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tomography</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2129" to="2152" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DeepDRR -a catalyst for machine learning in fluoroscopyguided procedures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Unberath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-3_12" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Vallières</surname></persName>
		</author>
		<title level="m">Data from Head-Neck-PET-CT</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thrity-Seventh Asilomar Conference on Signals, Systems &amp; Computers</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">X2CT-GAN: reconstructing CT from biplanar X-rays with generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">NAF: neural attenuation fields for sparse-view CBCT reconstruction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_42" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="442" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The Cancer Genome Atlas Head-Neck Squamous Cell Carcinoma Collection TCGA-HNSC</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Zuley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
