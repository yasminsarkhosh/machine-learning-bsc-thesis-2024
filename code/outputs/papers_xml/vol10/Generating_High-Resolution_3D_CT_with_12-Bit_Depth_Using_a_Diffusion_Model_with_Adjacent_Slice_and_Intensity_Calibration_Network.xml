<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiheon</forename><surname>Jeong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Asan Medical Institute of Convergence Science and Technology</orgName>
								<orgName type="institution">University of Ulsan College of Medicine</orgName>
								<address>
									<postCode>05505</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ki</forename><forename type="middle">Duk</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Convergence Medicine</orgName>
								<orgName type="institution">Asan Medical Center</orgName>
								<address>
									<postCode>05505</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujin</forename><surname>Nam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Asan Medical Institute of Convergence Science and Technology</orgName>
								<orgName type="institution">University of Ulsan College of Medicine</orgName>
								<address>
									<postCode>05505</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kyungjin</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Asan Medical Institute of Convergence Science and Technology</orgName>
								<orgName type="institution">University of Ulsan College of Medicine</orgName>
								<address>
									<postCode>05505</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiseon</forename><surname>Kang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Convergence Medicine</orgName>
								<orgName type="institution">Asan Medical Center</orgName>
								<address>
									<postCode>05505</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gil-Sun</forename><surname>Hong</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Asan Medical Center</orgName>
								<address>
									<postCode>05505</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Namkug</forename><surname>Kim</surname></persName>
							<email>namkugkim@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Convergence Medicine</orgName>
								<orgName type="institution">Asan Medical Center</orgName>
								<address>
									<postCode>05505</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Asan Medical Center</orgName>
								<address>
									<postCode>05505</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generating High-Resolution 3D CT with 12-Bit Depth Using a Diffusion Model with Adjacent Slice and Intensity Calibration Network</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="366" to="375"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">0A1D65119F48C3D363052423740F2DD3</idno>
					<idno type="DOI">10.1007/978-3-031-43999-5_35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Score-based Diffusion Model</term>
					<term>Adjacent Slice-based 3D Generation</term>
					<term>Intensity Calibration Network</term>
					<term>12-bit Depth DICOM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Since the advent of generative models, deep learning-based methods for generating high-resolution, photorealistic 2D images have made significant successes. However, it is still difficult to create precise 3D image data with 12-bit depth used in clinical settings that capture the anatomy and pathology of CT and MRI scans. Using a score-based diffusion model, we propose a slice-based method that generates 3D images from previous 2D CT slices along the inferior direction. We call this method stochastic differential equations with adjacent slice-based conditional iterative inpainting (ASCII). We also propose an intensity calibration network (IC-Net) that adjusts the among slices intensity mismatch caused by 12-bit depth image generation. As a result, Frechet Inception Distance (FIDs) scores of FID-Ax, FID-Cor and FID-Sag of ASCII(2) with IC-Net were 14.993, 19.188 and 19.698, respectively. Anatomical continuity of the generated 3D image along the inferior direction was evaluated by an expert radiologist with more than 15 years of experience. In the analysis of eight anatomical structures, our method was evaluated to be continuous for seven of the structures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unlike natural images that are typically processed in 8-bit depth, medical images, including X-ray, CT, and MR images, are processed in 12-bit or 16-bit depth to retain more detailed information. Among medical images, CT images are scaled using a quantitative measurement known as the Hounsfield unit (HU), which ranges from -1024 HU to 3071 HU in 12-bit depth. However, in both clinical practice and research, the dynamic range of HU is typically clipped to emphasize the region of interest (ROI). Such clipping of CT images, called windowing, can increase the signal-to-noise ratio (SNR) in the ROI. Therefore, most research on CT images performs windowing as a pre-processing method <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Recent advancements in computational resources have enabled the development of 3D deep learning models such as 3D classification and 3D segmentation. 3D models have attracted much attention in the medical domain because they can utilize the 3D integrity of anatomy and pathology. However, the access to 3D medical imaging datasets is severely limited due to the patient privacy. The inaccessibility problem of 3D medical images can be addressed by generating high quality synthetic data. Some researches have shown that data insufficiency or data imbalance can be overcome using a welltrained generative model <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. However, generating images with intact 3D integrity is very difficult. Moreover, generating high-quality images <ref type="bibr" target="#b4">[5]</ref> in the 12-bit depth, which is used in real clinical settings, is even more challenging.</p><p>The present study proposes a 2D-based 3D-volume generation method. To preserve the 3D integrity and transfer spatial information across adjacent axial slices, prior slices are utilized to generate each adjacent axial slice. We call this method Adjacent Slicebased Conditional Iterative Inpainting, ASCII. Experiments demonstrated that ASCII could generate 3D volumes with intact 3D integrity. Recently, score-based diffusion models have shown promising results in image generation <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>, super resolution <ref type="bibr" target="#b12">[13]</ref> and other tasks <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. Therefore, ASCII employs a score-based diffusion model to generate images in 12-bit depth. However, since the images were generated in 12-bit depth, errors in the average intensity arose when the images were clipped to the brain parenchymal windowing range. To solve this issue, we propose a trainable intensitycalibration network (IC-Net) that matches the intensity of adjacent slices, which is trained in a self-supervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Score-based generative models <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> and denoising diffusion probabilistic models (DDPMs) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> can generate high-fidelity data without an auxiliary network. In contrast, generative adversarial networks (GANs) <ref type="bibr" target="#b17">[18]</ref> require a discriminator and variational auto-encoders (VAEs) <ref type="bibr" target="#b18">[19]</ref> require a Gaussian encoder. Score-based generative models and diffusion models have two processing steps: a forward process that creates perturbed data with random noise taken from a pre-defined noise distribution in each step, and a backward process that denoises the perturbed data using a score network. The perturbation methods were defined as the stochastic differential equation (SDE) in <ref type="bibr" target="#b7">[8]</ref>. A continuous process was defined as {x(t)} T t=0 with x(0) ∼ p data and x(T ) ∼ p T , where t ∈ [0, 1] and p data , p T are the data distribution and prior noise distribution, respectively. The forward process was defined as the following SDE:</p><formula xml:id="formula_0">dx = f (x, t)dt + g(t)dw,<label>(1)</label></formula><p>where f and g are the coefficients of the drift and diffusion terms in the SDE, respectively, and w induces the Wiener process (i.e., Brownian motion). The backward process was defined as the following reverse-SDE:</p><formula xml:id="formula_1">dx = f (x, t) -g(t) 2 ∇xlogp t (x) dt + g(t)dw, (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where w is the backward Wiener process. We define each variance σ t as a monotonically increasing function. To solve the reverse-SDE given by above equation we train a score network S θ (x, t) to estimate the score function of the perturbation kernel ∇xlogp t (x t |x 0 ). Therefore, the objective of the score network is to minimize the following loss function: 3 Adjacent Slice-Based Conditional Iterative Inpainting, ASCII</p><formula xml:id="formula_3">θ * = argmin θ λ(t)E x(0) E x(t)|x(0) S θ (x(t), t) -∇xlogp t (x(t)|x(0)) 2 2 dt,<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generating 12-Bit Whole Range CT Image</head><p>VESDEs experimented on four noise schedules and two GAN models <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> were compared for 12-bit whole range generation. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, setting σ min to 0.01 generated a noisy image (third and fifth column) whereas the generated image was well clarified when σ min was reduced to 0.001 (forth and last column). However, in two GAN models, the important anatomical structures, such as white matter and grey matter, collapsed in the 12-bit generation. The coefficient of variation (CV), which is a measure used to compare variations while eliminating the influence of the mean, measured in the brain parenchyma of the generated images (excluding the bones and air from the windowing) at both noise levels <ref type="bibr" target="#b22">[23]</ref>. It is observed that the anatomical structures tended to be more distinguishable when the CV was low. Furthermore, a board-certified radiologist also qualitatively assessed that the images generated with a lower σ min showed a cleaner image. It can be interpreted that reducing σ min lowers CV and therefore improves the image quality.</p><p>For quantitative comparison, we randomly generated 1,000 slices by each parameter and measured the CV. As shown Fig. <ref type="figure" target="#fig_0">1</ref>, the variance of CV was lowest when σ min and σ max were set to 0.001 and 68, respectively. Also, setting σ max to 1,348 is theoretically plausible according to previous study <ref type="bibr" target="#b8">[9]</ref> because we preprocessed CT slices in the range of -1 to 1. Finally, we fixed σ min and σ max to 0.001 and 1,348, respectively in subsequent experiments.</p><p>In the case of GAN, as the convolution operator can be described as a high-pass filter <ref type="bibr" target="#b23">[24]</ref>, GANs trained through the discriminator's gradient are vulnerable to generating precise details in the low-frequency regions. Therefore, while anatomy such as bone (+1000HU) and air (-500HU) with strong contrast and high SNR are well-generated, anatomy such as parenchyma (20HU-30HU) with low contrast and SNR are difficult to generate accurately. As shown in the first and second column of Fig. <ref type="figure" target="#fig_0">1</ref>, we can see that the GAN models seem to generate anatomy with strong contrast and high SNR well but failed to generate the others. The SDE's scheduling can impact how the score-based diffusion model generates the fine-grained regions of images. To generate high-quality 12-bit images, the diffusion coefficient must be set to distinguish 1HU (0.00049). By setting σ max to 1,348 and σ min to 0.001, the final diffusion coefficient (0.00017) was set to a value lower than 1HU. In other words, setting the diffusion coefficient (0.00155) to a value greater than 1HU can generate noisy images. The detailed description and calculation of diffusion coefficient was in Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adjacent Slice-Based Conditional Iterative Inpainting</head><p>To generate a 3D volumetric image in a 2D slice-wise manner, a binary mask was used, which was moved along the channel axis. A slice x 0 = {-1024HU } D filled with intensity of air was padded before the first slice x 1 of CT and used as the initial seed. Then, the input of the model was given by x t : x t+K-1 , where t ∈ [0, N s -K + 1] and N s and K are the total slice number of CT and the number of contiguous slices, respectively. In addition, we omit augmentation because the model itself might generate augmented images. After training, the first slice was generated through a diffusion process using the initial seed. The generated first slice was then used as a seed to generate the next slice in an autoregressive manner. Subsequently, the next slices were generated through the same process. We call this method adjacent slice-based conditional iterative inpainting, ASCII.</p><p>Two experiments were conducted. The first experiment preprocessed the CT slices by clipping it with brain windowing [-10HU , 70HU ] and normalizing it to [-1, 1] with σ min set to 0.01 and σ max set to 1,348, while the second experiment preprocessed the whole windowing [-1024HU , 3071HU ] and normalized it to [-1, 1] with σ min set to 0.001 and σ max set to 1,348. As shown Fig. <ref type="figure" target="#fig_1">2</ref>, the white matter and gray matter in the first experiment (brain windowing) could be clearly distinguished with maintained continuity of slices, whereas they were indistinguishable and remained uncalibrated among axial slices in the second experiment (whole range).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Intensity Calibration Network (IC-Net)</head><p>It was noted that the intensity mismatch problem only occurs in whole range generation. To address this issue, we first tried a conventional non-trainable post-processing, such as histogram matching. However, since each slice has different anatomical structure, the histogram of each slice image was fitted to their subtle anatomical variation. Therefore, anatomical regions were collapsed when the intensities of each slice of 3D CT were calibrated using histogram matching. Finally, we propose a solution for this intensity mismatching, a trainable intensity calibration network: IC-Net.</p><p>To calibrate the intensity mismatch, we trained the network with a self-supervised manner. First, adjacent two slices from real CT images, x t , x t+1 were clipped using the window of which every brain anatomy HU value can be contained. Second, the intensity of x t+1 in ROI is randomly changed and the result is x t+1 c = x t+1x t+1 * µ + x t+1 , where x t+1 and μ are the mean of x t+1 and shifting coefficient, respectively. And µ was configured to prevent the collapse of anatomical structures.</p><p>Finally, intensity calibration network, IC-Net was trained to calibrate the intensity of x t+1 to the intensity of x t . The objective of IC-Net was only to calibrate the intensity of x t and preserve both the subtle texture and the shape of a generated slice. The IC-Net uses the prior slice to calibrate the intensity of generated slice. The objective function of IC-Net is given by,</p><formula xml:id="formula_4">L IC = E t E μ∼U [-0.7,1.3] ICNet x t+1 μ , x t -x t+1<label>(4)</label></formula><p>As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, some important anatomical structures, such as midbrain, pons, medulla oblongata, and cerebellar areas, are blurred and collapsed when histogram matching was used. It can be risky as the outcomes vary depending on the matching seed image. On the other hand, the anatomical structure of the IC-Net matched images did not collapse. Also, the IC-Net does not require to set the matching seed image because it normalizes using the prior adjacent slice. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ASCII with IC-Net in 12-Bit Whole Range CT Generation</head><p>The description of the dataset and model architecture is available in the Supplementary Material. We experimented ASCII on continuous K slices of K = 2 and 3 and called them ASCII(2) and ASCII(3), respectively. We generated a head &amp; neck CT images via ASCII(2) and ASCII(3) with and without IC-Net, and slice-to-3D VAE <ref type="bibr" target="#b24">[25]</ref>. Figure <ref type="figure" target="#fig_3">4</ref> demonstrate the example qualitative images. The 3D generated images were shown both in whole range and brain windowing range. The results showed that the both ASCII(2) and ASCII(3) were well calibrated using IC-Net. Also, anatomical continuity and the 3D integrity is preserved while the images were diverse enough. However, there was no significant visual difference between ASCII(2) and ASCII <ref type="bibr" target="#b2">(3)</ref>. Although the results in whole range appear to be correctly generated all models, the results in brain windowing range showed the differences. The same drawback of convolution operation addressed in the 12-bit generation of GAN based models, which was shown in Fig. <ref type="figure" target="#fig_0">1</ref>, was also shown in slice-to-3D VAE.</p><p>The quantitative results in whole range are shown in Table <ref type="table" target="#tab_1">1</ref>. The mid-axial slice, midsagittal slice, and mid-coronal slice of the generated volumes were used to evaluate the Fréchet Inception Distance (FID) score, which we designated as FID-Ax, FID-Sag, and FID-Cor, respectively. And multi-scales structural similarity index measure (MS-SSIM) and batch-wise squared Maximum Mean Discrepancy (bMMD 2 ) were also evaluated for quantitative metrics. In general, quantitative results indicate that ASCII(2) performs better than ASCII(3). It's possible that ASCII(3) provides too much information from prior slices, preventing it from generating sufficiently diverse images. Additionally, IC-Net significantly improved generation performance, especially in the windowing range. The FID-Ax of ASCIIs was improved by IC-Net from 15.250 to 14.993 and 18.127 to 16.599 in the whole range, respectively. Also, the performance of FID-Cor and FID-Sag had significantly improved when IC-Net was used. The MS-SSIM showed that ASCIIs can generated it diverse enough. The FID-Ax, FID-Cor, and FID-Sag scores of ASCIIs with IC-Net were improved in windowing range. The FID-Ax of ASCIIs was improved by IC-Net from 15.770 to 14.656 and 20.145 to 15.232 in the windowing range, respectively. On the other hand, ASCIIs without IC-Net had poor performance in the windowing range and this means that even when IC-Net is used, structures do not collapse. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Calibration Robustness of IC-Net on Fixed Value Image Shift</head><p>To demonstrate the performance of IC-Net, we conducted experiments with the 7, 14, 21 and 28th slices, which sufficiently contain complex structures to show the calibration performances. The previous slice was used as an input to the IC-Net along with the target slice whose pixel values were to be shifted. And the absolute errors were measured between GT and predicted slice using IC-Net.</p><p>As shown in Fig. <ref type="figure" target="#fig_4">5</ref>, it worked well for most shifting coefficients. The mean absolute error was measured from 1HU to 2HU when the shifting coefficient was set from 0.7 to 1.1. However, the errors were exploded when shifting coefficient was set to 1.2 or 1.3. It was because the images were collapsed when shifting coefficient increases than 1.2 since the intensity deviates from the ROI range [-150HU , 150HU ]. Nevertheless, IC-Net can calibrate intensity to some extent even in the collapsed images as shown Fig. <ref type="figure" target="#fig_4">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visual Scoring Results of ASCII with IC-Net</head><p>Due to the limitations of slice-based methods in maintaining connectivity and 3D integrity, an experienced radiologist with more than 15 years of experience evaluated the images. Seeded with the 13th slices of real CT scans, in which the ventricle appears, ASCII(2) with IC-Net generated a total of 15 slices. Visual scoring shown in Table . 2 on a three-point scale was conducted blindly for 50 real and 50 fake CT scans, focusing on the continuity of eight anatomical structures. Although most of the fake regions were scored similarly to the real ones, the basilar arteries were evaluated with broken continuity. The basilar artery was frequently not generated, because it is a small region. As the model was trained on 5-mm thickness non-contrasted enhanced CT scans, preserving the continuity of the basilar artery is excessively demanding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a high-performance slice-based 3D generation method (ASCII) and combined it with IC-Net, which is trained in a self-supervised manner without any annotations. In our method, ASCII generates a 3D volume by iterative generation using previous slices and automatically calibrates the intensity mismatch between the previous and next slices using IC-Net. This pipeline is designed to generate high-quality medical image, while preserving 3D integrity and overcoming intensity mismatch caused in 12-bit generation. ASCII had shown promising results in 12-bit depth whole range and windowing range, which are crucial in medical contexts. The integrity of the generated images was also confirmed in qualitative and quantitative assessment of 3D integrity evaluations by an expert radiologist. Therefore, ASCII can be used in clinical practice, such as anomaly detection in normal images generated from a seed image <ref type="bibr" target="#b25">[26]</ref>. In addition, the realistic 3D images generated by ASCII can be used to train deep learning models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> in medical images, which frequently suffer from data scarcity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Qualitative and quantitative generation results according to σ min and σ max</figDesc><graphic coords="4,68,46,148,22,315,40,97,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Results of ASCII with windowing range (red) and 12-bit whole range (blue)</figDesc><graphic coords="5,50,31,56,36,324,04,66,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Post-processing results of ASCII(2) in windowed range of whole range generation.</figDesc><graphic coords="6,120,48,56,51,211,96,141,88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Qualitative comparison among ASCII(2) and ASCII(3) with/without IC-Net calibration and Slice-to-3D VAE models.</figDesc><graphic coords="7,97,80,56,36,228,52,114,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (Left) IC-Net calibration results for fixed value image shift. Shifted images, IC-Net calibration result, and difference map are presented, respectively. Images were shifted with fixed value from 0.7 to 1.3. (Right) IC-Net calibration results for fixed value image shift. All slices were normalized from [-150HU , 150HU ] to [-1, 1].</figDesc><graphic coords="8,97,98,240,47,256,84,117,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison of ASCII(2) and ASCII(3) with/without IC-Net calibration and Slice-to-3D VAE in whole range and windowing range. Whole range and windowing range are set to [-1024HU, 3071HU] and [-10HU, 70HU], respectively.</figDesc><table><row><cell></cell><cell>ASCII(2)</cell><cell>ASCII(2)</cell><cell>ASCII(3)</cell><cell>ASCII(3)</cell><cell>Slice-to-3D VAE [25]</cell></row><row><cell></cell><cell>w/ IC-Net</cell><cell>w/o IC-Net</cell><cell>w/ IC-Net</cell><cell>w/o IC-Net</cell><cell></cell></row><row><cell>Whole Range</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FID-Ax</cell><cell>14.993</cell><cell>15.250</cell><cell>16.599</cell><cell>18.127</cell><cell>29.137</cell></row><row><cell>FID-Cor</cell><cell>19.188</cell><cell>19.158</cell><cell>20.930</cell><cell>21.224</cell><cell>28.263</cell></row><row><cell>FID-Sag</cell><cell>19.698</cell><cell>19.631</cell><cell>21.991</cell><cell>22.311</cell><cell>29.024</cell></row><row><cell cols="2">MS-SSIM 0.6271</cell><cell>0.6275</cell><cell>0.6407</cell><cell>0.6406</cell><cell>0.9058</cell></row><row><cell>bMMD2</cell><cell>425704</cell><cell>429120</cell><cell>428045</cell><cell>432665</cell><cell>311080</cell></row><row><cell cols="2">Windowing Range</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FID-Ax</cell><cell>14.656</cell><cell>15.770</cell><cell>15.232</cell><cell>20.145</cell><cell>28.682</cell></row><row><cell>FID-Cor</cell><cell>18.920</cell><cell>19.830</cell><cell>19.996</cell><cell>24.230</cell><cell>28.828</cell></row><row><cell>FID-Sag</cell><cell>18.569</cell><cell>19.675</cell><cell>19.840</cell><cell>24.511</cell><cell>29.912</cell></row><row><cell cols="2">MS-SSIM 0.5287</cell><cell>0.5384</cell><cell>0.5480</cell><cell>0.5447</cell><cell>0.8609</cell></row><row><cell>bMMD 2</cell><cell>1975336</cell><cell>1854921</cell><cell>2044218</cell><cell>1858850</cell><cell>1894911</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Visual scoring results of integrity evaluation Scale 1 -discontinuity, Scale 2 -strained continuity, Scale 3 -well preserved continuity</figDesc><table><row><cell>Anatomy</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CT image segmentation for inflamed and fibrotic lungs using a multiresolution convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Gerard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrating deep learning CT-scan model, biological and clinical variables to predict severity of COVID-19 patients</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lassau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frid-Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gan augmentation: augmenting training data using generative adversarial networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bowles</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10863</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d-stylegan: a style-based generative adversarial network for generative modeling of three-dimensional medical images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Generative Models, and Data Augmentation, Labelling, and Imperfections</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456</idno>
		<title level="m">Score-based generative modeling through stochastic differential equations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12438" to="12448" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01073</idno>
		<title level="m">Sdedit: Image synthesis and editing with stochastic differential equations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07636</idno>
		<title level="m">Image super-resolution via iterative refinement</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<title level="m">Diffwave: a versatile diffusion model for audio synthesis</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">WaveGrad: estimating gradients for waveform generation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00713</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diffusion probabilistic models for 3d point cloud generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Symbolic music generation with diffusion models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16091</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12104" to="12114" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="852" to="863" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How noise affects the synchronization properties of recurrent networks of inhibitory neurons</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hansel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1066" to="1110" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06709</idno>
		<title level="m">How Do Vision Transformers Work? arXiv preprint</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modelling the distribution of 3D brain MRI using a 2D slice VAE</title>
		<author>
			<persName><forename type="first">A</forename><surname>Volokitin</surname></persName>
		</author>
		<author>
			<persName><surname>Erdil</surname></persName>
		</author>
		<author>
			<persName><surname>Ertunc</surname></persName>
		</author>
		<author>
			<persName><surname>Karani</surname></persName>
		</author>
		<author>
			<persName><surname>Neerav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kerem</forename><surname>Tezcan</surname></persName>
		</author>
		<author>
			<persName><surname>Can</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Xiaoran</surname></persName>
		</author>
		<author>
			<persName><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ender</forename><surname>Konukoglu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59728-3_64</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59728-3_64" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2020: 23rd International Conference</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<editor>
			<persName><surname>Abolmaesumi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Purang</surname></persName>
		</editor>
		<editor>
			<persName><surname>Stoyanov</surname></persName>
		</editor>
		<editor>
			<persName><surname>Danail</surname></persName>
		</editor>
		<editor>
			<persName><surname>Mateus</surname></persName>
		</editor>
		<editor>
			<persName><surname>Diana</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria</forename><forename type="middle">A</forename><surname>Zuluaga</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kevin</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Racoceanu</surname></persName>
		</editor>
		<editor>
			<persName><surname>Daniel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Leo</forename><surname>Joskowicz</surname></persName>
		</editor>
		<meeting><address><addrLine>Lima, Peru; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">October 4-8, 2020. 2020</date>
			<biblScope unit="page" from="657" to="666" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">f-AnoGAN: Fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
