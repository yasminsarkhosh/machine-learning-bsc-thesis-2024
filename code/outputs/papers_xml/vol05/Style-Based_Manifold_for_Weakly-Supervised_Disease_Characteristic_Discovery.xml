<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Siyu</forename><surname>Liu</surname></persName>
							<email>siyu.liu1@uq.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linfeng</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The Queensland Brain Institute</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Craig</forename><surname>Engstrom</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuan</forename><forename type="middle">Vinh</forename><surname>To</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The Queensland Brain Institute</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Data Science and AI</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Monash-Airdoc Research Centre</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">AIM for Health Lab</orgName>
								<orgName type="institution">Monash University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stuart</forename><surname>Crozier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fatima</forename><surname>Nasrallah</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The Queensland Brain Institute</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shekhar</forename><forename type="middle">S</forename><surname>Chandra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">10220726BCF06A57E6ABD640B3E32A8E</idno>
					<idno type="DOI">10.1007/978-3-031-43904-936.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Disease learning</term>
					<term>Weak supervision</term>
					<term>Alzheimer&apos;s Disease</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Alzheimer's Disease (AD), interpreting tissue changes is key to discovering disease characteristics. However, AD-induced brain atrophy can be difficult to observe without Cognitively Normal (CN) reference images, and collecting co-registered AD and CN images at scale is not practical. We propose Disease Discovery GAN (DiDiGAN), a stylebased network that can create representative reference images for disease characteristic discovery. DiDiGAN learns a manifold of disease-specific style codes. In the generator, these style codes are used to "stylize" an anatomical constraint into synthetic reference images (for various disease states). The constraint in this case underpins the high-level anatomical structure upon which disease features are synthesized. Additionally, DiDiGAN's manifold is smooth such that seamless disease state transitions are possible via style interpolation. Finally, to ensure the generated reference images are anatomically correlated across disease states, we incorporate anti-aliasing inspired by StyleGAN3 to enforce anatomical correspondence. We test DiDiGAN on the ADNI dataset involving CN and AD magnetic resonance images (MRIs), and the generated reference AD and CN images reveal key AD characteristics (hippocampus shrinkage, ventricular enlargement, cortex atrophies). Moreover, by interpolating DiDiGAN's manifold, smooth CN-AD transitions were acquired further enhancing disease visualization. In contrast, other methods in the literature lack such dedicated disease manifolds and fail to synthesize usable reference images for disease characteristic discovery.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Discovering disease characteristics from medical images can yield important insights into pathological changes. However, subtle disease characteristics are difficult to discover from imaging data. The reduction in brain volume due to Alzheimer's Disease (AD) is one such example. In this case, comparing co-registered AD and Cognitively Normal (CN) reference magnetic resonance images (MRIs) is the most effective way to reveal subtle AD characteristics. Unfortunately, collecting such co-registered reference images at scale is not practical.</p><p>Generative Adversarial Networks (GANs) has the potential to synthesize the reference images required for disease discovery. Specifically, style-based generative frameworks <ref type="bibr" target="#b3">[4]</ref> could be suitable as they employ mechanisms to "stylize" a share anatomical structure into different disease states. However, as we will show, many GAN frameworks (style and non-style based) are unable to produce satisfactory reference AD and CN images for AD characteristic discovery.</p><p>In this work, we propose Disease Discovery GAN (DiDiGAN), a specialized style-based network for disease characteristic discovery on medical image data. On the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset <ref type="bibr" target="#b6">[7]</ref>, DiDi-GAN can create not only reference AD and CN MRIs, but also smooth animated transitions between AD and CN. The highlights of DiDiGAN are:</p><p>-A learnt disease manifold that captures and encodes AD and CN disease state distributions. Style codes sampled from this manifold control the disease expression in the output reference images. -The disease manifold is naturally smooth such that seamless transitions of AD to CN are possible via style interpolation. -The generator uses a low-resolution input image as the source anatomical constraint (to provide coarse structural guidance), it is low-resolution to leave sufficient room for the generator to synthesize disease features. -Anti-aliasing as a key mechanism to maintain anatomical correspondence across generated reference images. Without anti-aliasing, reference images exhibit inconsistent anatomical structures invalidating visual comparisons. -DiDiGAN is a weakly-supervised framework requiring only class labels for images rather than pixel (voxel) labels.</p><p>DiDiGAN learns to generate representative reference AD and CN images by training on 2D coronal brain slices labelled with AD and CN. The generated reference images and animations clearly show systematic changes in the hippocampus, ventricle, and cortex areas while maintaining anatomical correspondence.</p><p>The discovered characteristics are corroborated by independent tests which solidify the strengths of the findings. There has not been a previous work that can i) produce visualizations on par with DiDiGAN's and ii) learn a dedicated manifold for disease characteristic discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Various generative methods have attempted to synthesize pathological changes for disease modelling. HerstonNet <ref type="bibr" target="#b17">[18]</ref> and Bernal et al. <ref type="bibr" target="#b1">[2]</ref> synthesize brain MRIs with atrophy, but they rely on induced atrophy by altering the Partial Volume (PV)-maps and segmentation maps. Hence they do not learn disease characteristics on their own. Ravi et al. <ref type="bibr" target="#b16">[17]</ref> and ADESyn <ref type="bibr" target="#b7">[8]</ref> learn AD characteristics from data, and they can synthesize brain images at different stages of AD. However, ADESyn fails to capture ventricle enlargement which is a key AD while Ravi et al. failed to capture hippocampus shrinkage (which are key AD characteristics). DBCE <ref type="bibr" target="#b15">[16]</ref> learns to deform a healthy brain image into an AD image. However, the resulting atrophy is not sufficiently distinct for AD characteristic visualization. Saliency maps must be further computed to reveal AD regions. Xia et al. <ref type="bibr" target="#b22">[23]</ref> proposed a GAN for ageing brain synthesis. While the results clearly depict ventricle enlargement for advanced ages, other AD characteristics remain unclear as the images show significant anatomical inconsistencies. Another limitation of the above methods is that disease features are intertwined with anatomical features. For disease modelling, a dedicated mechanism (such as a latent space) to store disease features in an explorable manner would be desirable. The objective to learn a dedicated disease latent space naturally aligns with style-based <ref type="bibr" target="#b3">[4]</ref> frameworks, which have mostly been used for medical image translation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>. Style-based frameworks have the potential to enhance generative disease modelling. Fetty et al. <ref type="bibr" target="#b4">[5]</ref> have shown that StyleGANs can learn a dedicated latent space (manifold) of medical image features. This latent space can be smoothly interpolated to manipulate synthesis. Other works like StarGAN <ref type="bibr" target="#b3">[4]</ref> have shown that this latent can be disentangled from the content. Similarly, in medical image analysis, Chartsias et al. <ref type="bibr" target="#b2">[3]</ref> have shown modality information can be disentangled from anatomical content for multi-modal image translation. These findings motivate DiDiGAN to use style to learn a manifold of disease features and then apply the manifold to "stylized" an input anatomical constraint into AD and CN images. However, StyleGAN3 <ref type="bibr" target="#b8">[9]</ref> discovered that common style-based methods suffer from aliasing <ref type="bibr" target="#b23">[24]</ref>. When interpolating the manifold, aliasing causes undesirable texture disruptions, and it threatens to disrupt anatomical structures if applied to disease studies. Thus, applying anti-aliasing (as per Shannon-Nyquist theorem) is critical for DiDiGAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>DiDiGAN projects different disease states onto a common anatomical structure to form disease reference images, and the output images must maintain a consistent anatomical structure. A detailed architecture diagram is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disease Style w c and Anatomical Constraint</head><p>x AC : Let x c denote the images in a dataset and c ∈ {c 1 , c 2 ...c n } are the disease classifications for the images, the generator G's aim is to synthesise reference disease images x c that share a consistent anatomical structure. This is done by injecting disease styles w c into a shared anatomical constraint x AC to produce the output x c = G(w c , x AC ). w c is created following w c = M (emb(c), z) where emb is a learned 512-d embedding specific to each class, z is a 512-d N (0, 1) noise vector and M is a 2-layer mapping network made up of dense layers. Like StyleGAN, the style code space w c is a learned manifold (of disease states), and it can be interpolated for smooth disease states transitions. The reason to include z is to introduce variability such that w c forms a surface instead of a fixed point. x AC is a 4× down-sampled version of x c which only enforces coarse anatomical structure similarities between the reference images, and it lacks resolution on purpose allowing room for disease features from w c to manifest. As already discussed, full anatomical correspondence requires anti-aliasing to enforce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alias-Free Style Generator G:</head><p>Generator G is alias-free as the feature maps are processed carefully following the Shannon-Nyquist sampling theorem. Like StyleGAN 3, G's convolutional (AA Conv) layers apply anti-aliasing to the leakyrelu activation function. This process involves first interpolating discrete feature maps to continuous domains. Then, low-pass filtering is used to remove offending frequencies. Finally, the activation function is applied, and the signal is resampled back to the discrete domain <ref type="bibr" target="#b8">[9]</ref>. Architecture wise G is a progressively up-sampling decoder Convolutional Neural Network (CNN) with a starting resolution of 4×4 and an output resolution of 256×256. There are 13 convolutional blocks that gradually perform upsampling and synthesis. All the convolutional layers are modulated convolutions <ref type="bibr" target="#b3">[4]</ref> to inject disease features w c . The anatomical constraint x AC is introduced by concatenating it to the input of every block, and we observed more stable training using this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-head Discriminator D:</head><p>The discriminator D is an encoder architecture with consecutive convolution and down-sampling. There are three prediction heads D R/F , D c , and D AC which predict a realness logit, disease classification, and reconstruction of constraint x AC , respectively. The main loss functions are these outputs are 1) the standard non-saturating GAN loss L R/F for generating realistic images, 2) the adversarial classification loss L c which supervises the conditioning on c 3) the anatomical reconstruction loss L AC ensuring the generated reference images share the high-level structure of the input anatomical constraint. These losses are as follows:</p><formula xml:id="formula_0">min D max G L R/F = E[logD R/F (x c )] -E[log(D R/F (x c ))]<label>(1)</label></formula><p>min</p><formula xml:id="formula_1">D max G L c = - c∈{c1,c2...cn} y c log (D c (X)), where X ∈ {x c , x c } (<label>2</label></formula><formula xml:id="formula_2">)</formula><formula xml:id="formula_3">L AC = ||x AC -D AC (G(w c , x AC ))|| 2 + ||x AC -D AC (x c ))|| 2<label>(3)</label></formula><p>where x c is a real image of class c, x c is a fake image of the same class, w c = M (emb(c), z), and x AC is a down-sampled version of x c . There are also other standard regularization loss functions including the pathlength regularization L pl <ref type="bibr" target="#b8">[9]</ref>, R1 gradient penalty L R1 <ref type="bibr" target="#b13">[14]</ref> and an explicit diversification loss L div = -||G(w, c, z 1 ) -G(w, c, z 2 )|| to prevent the manifold from converging to fixed points. The total loss L total is then defined as  <ref type="bibr" target="#b20">[21]</ref>, skull stripping using SPM <ref type="bibr" target="#b21">[22]</ref>) were applied to the entire dataset. For each 3D scan, the center 40 coronal slices were extracted and zero-padded to 256 × 256 pixels. Down-sampled versions of these slices (64 × 64) were used for X AC . The network was implemented in Pytorch and trained for 60,000 steps (32 images per step) using Adam optimizer. The training hardware is a 32 GB Nvidia V100 graphics card to support a batch size of 32. The baseline methods include a range of style and non-style-based image translation methods. These methods can readily treat reference image generation as unpaired image translation where, for example, a CN brain is translated to an AD version of the same brain. All the methods were trained and evaluated on the exact same data and splits.</p><formula xml:id="formula_4">L total = L R/F + L c + L AC + L div + L pl + L R1 (4)<label>4</label></formula><p>Quantitative Image Quality Evaluation Using Perceptual Metrics. The quality of the generated reference images (based on the test set, ignoring classes) from all the methods is assessed using Frechet Inception Distance (FID), Learned Perceptual Image Patch Similarity (LPIPS), and Kernel Inception Distance (KID) as they do not require paired data. As Table <ref type="table" target="#tab_1">1</ref> shows, DiDiGAN generated the most realistic reference images compared to the baselines. MUNIT <ref type="bibr" target="#b5">[6]</ref> and CUT <ref type="bibr" target="#b14">[15]</ref> failed to reach convergence hence the metrics are omitted.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AD Characteristic Extraction and Visualization Using DiDiGAN.</head><p>Figure <ref type="figure" target="#fig_1">2</ref> provides AD visualizations by generating reference images with DiDi-GAN and the baseline methods. Since DiDiGAN's uses stylization, it can freely generate correlated AD and CN pairs using the manifold. Comparison between the reference pairs reveals AD characteristics <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> such as hippocampus shrinkage, ventricle enlargement, and the thinning of cortical structures. The consistent anatomical structure also facilitates the computation of Jacobian (shrinkage and expansion) maps. For DiDiGAN, the bright regions in DiDi-GAN's Jacobian map more clearly highlight significant hippocampus shrinkage and ventricle enlargement from CN to AD. More examples are shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>In these examples, we also manually segmented the ventricle and hippocampus, and the notable size changes marked in the figure. Anti-aliasing is crucial for maintaining anatomical correspondence across DiDiGAN's reference images. As an ablation study, a DiDiGAN without anti-aliasing was trained. The generated AD and CN pairs exhibit significant anatomical disturbances (see S1 for examples).</p><p>Comparing AD Characteristic Visualization with Baselines. The baseline methods produce reference images via direct image translation from CN to an AD reference. The results in Fig. <ref type="figure" target="#fig_1">2</ref> suggest StarGANv2 <ref type="bibr" target="#b3">[4]</ref>, StyleGAN-SD <ref type="bibr" target="#b9">[10]</ref> and UNIT <ref type="bibr" target="#b11">[12]</ref> were unable to convey useful AD findings as they struggled to maintain anatomical correspondence. CycleGAN's <ref type="bibr" target="#b24">[25]</ref> simpler architecture helped avoid anatomical disruptions. However, the generated AD is almost pixel-identical to the input CN aside from the contrast difference (hence the high FID). The other two methods MUNIT <ref type="bibr" target="#b5">[6]</ref> and CUT <ref type="bibr" target="#b14">[15]</ref> could not reach convergence on the ADNI dataset despite the documentation being carefully followed.</p><p>Disease Manifold Formation and Interpolation. DiDiGAN's disease manifold formation is visualized using UMAP <ref type="bibr" target="#b12">[13]</ref> where style codes sampled from the manifold are reduced to 2-d from 512-d. Figure <ref type="figure" target="#fig_3">4</ref> shows a 2D cluster containing 10,000 AD and 10,000 CN style codes. While the input disease class is a discrete value, the manifold automatically maps it to a disease distribution with AD distributed on the left and CN on the right. The overlap between the two classes suggests AD is a progressive disease. The smoothness of the manifold facilitates smooth CN-AD transition animations by interpolating between an AD style code and a CN style code. Practically, this animation provides a more intuitive visualization of AD pathological changes.</p><p>Corroborating Tests: Due to the lack of paired medical images as ground truths, we perform the following tests (I, II, III) to verify DiDiGAN's findings. I: DiDiGAN was independently applied to the sagittal view slices, and similar hippocampus shrinkage and ventricle enlargement were clearly observed (see examples S2). These independent findings corroborate the coronal view visualizations produced by DiDiGAN's reference images. II: SPM segmentation was applied to the test set and DiDiGAN's reference images (generated based on the test set) to help compare the magnitudes of brain volume loss. For each image, the grey matter and white matter pixels were treated as brain mass. As Fig. <ref type="figure" target="#fig_4">5</ref> shows, DiDiGAN's AD reference images consistently show systematic brain mass reduction across all 40 slice positions. This trend is consistent with that of the real data. The box plot suggests an average brain mass reduction of 17.3% compared to the 12.1% of the real data. Limitations. DiDiGAN is an initial step for style-based disease characteristic discovery. In the future, a more rigours examination of the manifold is needed to fully understand the features and trends learned especially for clinical applications. For example, longitudinal trends and atrophy patterns among the AD population. Although DiDiGAN discovered brain tissue shrinkage as indicated by the SPM segmentation analysis, the learned magnitude is different from that of the real data. This is likely a limitation of GANs as the learnt distribution may not exactly match the real data in all aspects. Nonetheless, DiDiGAN's findings could serve as disease characteristic proposals. Additionally, more datasets and diseases should be tested to more thoroughly assess DiDiGAN's generalizability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>DiDiGAN demonstrated disease characteristic discovery by generating reference images that clearly depict relevant pathological features. The main technical novelties of DiDiGAN are i) the use of a learned disease manifold to manipulate disease states AD ii) the ability to interpolate the manifold to enhance visualization and iii) mechanisms including the structural constraint and anti-aliasing to maintain anatomical correspondence without direct registration. In the experiments involving the ADNI dataset, DiDiGAN discovered key AD features such as hippocampus shrinkage, ventricular enlargement, and cortex atrophy where other frameworks failed. DiDiGAN shows potential to aid disease characteristic discovery across time of other chronic diseases such as osteoarthritis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Generator and discriminator architectures of DiDiGAN. Source image xc is first down-sampled into anatomical constraint xAC , then disease style codes wc (c ∈ {c1, c2...cn}) are injected into G to generate reference images x c for different disease states. All x c s maintain a consistent anatomical structure.</figDesc><graphic coords="4,58,47,61,55,330,64,82,12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. AD feature discovery results for DiDiGAN and the baselines. The baselines perform image translation from an input CN to a reference AD image while DiDiGAN uses its manifold to generate a pair of reference AD and CN images. Jacobian Index heat maps were computed to show shrinkage (cool) or expansion (hot) areas between the AD and CN images. DiDiGAN's reference images reveal key AD characteristics including hippocampus shrinkage and ventricle enlargement. The baselines failed to highlight these characteristics due to anatomical disruptions.</figDesc><graphic coords="6,58,98,237,41,334,72,106,96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. More example Jacobian maps for DiDiGAN's reference AD and CN pairs. The hippocampus and ventricle areas are manually segmented with size changes labelled.</figDesc><graphic coords="6,62,61,505,74,327,55,54,12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. 2D UMAP of 10000 CN and 10000 AD style codes sampled from the disease manifold. A linear path from CN to AD is traversed to generate an CN-AD animation. Changes in key affected regions (with reference points) are shown on the right. To help with visualization, we provide a video of several example animations.</figDesc><graphic coords="8,56,46,62,69,320,02,123,01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Slice-wise brain mass difference (in pixels) between AD and CN as represented by DiDiGAN and real data. AD shows significant brain mass reductions in both cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quality of reference images generated by DiDiGAN and the baselines measured using three popular perceptual metrics. All the baselines were run on the exact same data as DiDiGAN. DiDiGAN achieved the best image quality while CUT and MUNIT failed to converge despite all the training instructions being followed.</figDesc><table><row><cell>Method</cell><cell cols="2">FID↓ LPIPS↑ KID↓</cell></row><row><cell>CUT [15]</cell><cell>N/A N/A</cell><cell>N/A</cell></row><row><cell>MUNIT [6]</cell><cell>N/A N/A</cell><cell>N/A</cell></row><row><cell>CycleGAN [25]</cell><cell>32.52 0.04</cell><cell>0.041</cell></row><row><cell>UNIT [12]</cell><cell>34.93 0.01</cell><cell>0.039</cell></row><row><cell cols="2">StyleGAN-SD [10] 17.23 0.14</cell><cell>0.034</cell></row><row><cell>StarGANv2 [4]</cell><cell>17.82 0.07</cell><cell>0.037</cell></row><row><cell>DiDiGAN</cell><cell>14.48 0.29</cell><cell>0.031</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MedGAN: medical image translation using GANs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Armanious</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compmedimag.2019.101684</idno>
		<ptr target="https://doi.org/10.1016/j.compmedimag" />
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">101684</biblScope>
			<date type="published" when="2019">2020. 2019.101684</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating longitudinal atrophy evaluation datasets on brain magnetic resonance images using convolutional neural networks and segmentation priors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Valverde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kushibar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cabezas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lladó</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12021-020-09499-z</idno>
		<ptr target="https://doi.org/10.1007/s12021-020-09499-z" />
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="477" to="492" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Disentangled representation learning in cardiac image analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chartsias</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2019.101535</idno>
		<ptr target="https://doi.org/10.1016/j.media.2019.101535" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101535</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">StarGAN v2: diverse image synthesis for multiple domains</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr42600.2020.00821</idno>
		<ptr target="https://doi.org/10.1109/cvpr42600.2020.00821" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06">2020. June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent space manipulation for high-resolution medical image synthesis via the StyleGAN</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fetty</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.zemedi.2020.05.001</idno>
		<ptr target="https://doi.org/10.1016/j.zemedi.2020.05.001" />
	</analytic>
	<monogr>
		<title level="j">Z. Med. Phys</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="305" to="314" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-toimage translation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01219-9_11</idno>
		<idno>978-3-030-01219-9 11</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11207</biblScope>
			<biblScope unit="page" from="179" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Alzheimer&apos;s disease neuroimaging initiative (ADNI): MRI methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jack</surname></persName>
		</author>
		<idno type="DOI">10.1002/jmri.21049</idno>
		<ptr target="https://doi.org/10.1002/jmri.21049" />
	</analytic>
	<monogr>
		<title level="j">J. Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="685" to="691" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conditional GAN with an attention-based generator and a 3D discriminator for 3D medical image generation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-131" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="318" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A style-aware discriminator for controllable image translation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18218" to="18227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Style transfer using generative adversarial networks for multi-site MRI harmonization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_30</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-430" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="313" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">UMAP: uniform manifold approximation and projection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grossberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Open Source Softw</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page">861</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Which training methods for GANs do actually converge?</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58545-7_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58545-719" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12354</biblScope>
			<biblScope unit="page" from="319" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DBCE: a saliency method for medical deep learning through anatomically-consistent free-form deformations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV56688.2023.00200</idno>
		<ptr target="https://doi.org/10.1109/WACV56688.2023.00200" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2023-01">January 2023</date>
			<biblScope unit="page" from="1959" to="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Degenerative adversarial neuroimage nets for brain scan simulations: application in ageing and dementia</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Blumberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ingala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Barkhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Oxtoby</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2021.102257</idno>
		<ptr target="https://doi.org/10.1016/j.media.2021.102257" />
		<imprint>
			<date type="published" when="2022-01">January 2022</date>
			<publisher>Elsevier BV</publisher>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">102257</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quantifiable brain atrophy synthesis for benchmarking of cortical thickness estimation methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rusak</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2022.102576</idno>
		<ptr target="https://doi.org/10.1016/j.media.2022.102576" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">102576</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The dynamics of cortical and hippocampal atrophy in Alzheimer disease</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<idno type="DOI">10.1001/archneurol.2011.167</idno>
		<ptr target="https://doi.org/10.1001/archneurol.2011.167" />
	</analytic>
	<monogr>
		<title level="j">Arch. Neurol</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1040" to="1048" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MRI of hippocampal volume loss in early Alzheimer&apos;s disease in relation to ApoE genotype and biomarkers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Schuff</surname></persName>
		</author>
		<idno type="DOI">10.1093/brain/awp007</idno>
		<ptr target="https://doi.org/10.1093/brain/awp007" />
	</analytic>
	<monogr>
		<title level="j">Brain</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1067" to="1077" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Tustison</surname></persName>
		</author>
		<idno type="DOI">10.1109/tmi.2010.2046908</idno>
		<ptr target="https://doi.org/10.1109/tmi.2010.2046908" />
		<title level="m">N4ITK: improved N3 bias correction</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1310" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tzourio-Mazoyer</surname></persName>
		</author>
		<idno type="DOI">10.1006/nimg.2001.0978</idno>
		<ptr target="https://doi.org/10.1006/nimg.2001.0978" />
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="273" to="289" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Consistent brain ageing synthesis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chartsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_82</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32251-982" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="750" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.244</idno>
		<ptr target="https://doi.org/10.1109/iccv.2017.244" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10">October 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
