<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis</title>
				<funder ref="#_53F9sKv">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_5dUpRQt">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_v65fHGe">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
				<funder ref="#_wQujRKG">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Fujian Provincial Hospital</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junyu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University Medical School</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Medical UltraSound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University Medical School</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Medical UltraSound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Ni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University Medical School</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Medical UltraSound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wufeng</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University Medical School</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Medical UltraSound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dongmei</forename><surname>Zhu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Ultrasound</orgName>
								<orgName type="institution">The Affiliated Nanchong Central Hospital of North Sichuan Medical College</orgName>
								<address>
									<settlement>Nanchong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jun</forename><surname>Cheng</surname></persName>
							<email>chengjun583@qq.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="laboratory">National-Regional Key Technology Engineering Laboratory for Medical Ultrasound</orgName>
								<orgName type="institution">Shenzhen University Medical School</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Medical UltraSound Image Computing (MUSIC) Lab</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Marshall Laboratory of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="642" to="651"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">653851DE2853686408806A04F0725398</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_62</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-modal Fusion</term>
					<term>Ultrasound Video</term>
					<term>Object Detection</term>
					<term>Renal Tumor</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Early diagnosis of renal cancer can greatly improve the survival rate of patients. Contrast-enhanced ultrasound (CEUS) is a costeffective and non-invasive imaging technique and has become more and more frequently used for renal tumor diagnosis. However, the classification of benign and malignant renal tumors can still be very challenging due to the highly heterogeneous appearance of cancer and imaging artifacts. Our aim is to detect and classify renal tumors by integrating Bmode and CEUS-mode ultrasound videos. To this end, we propose a novel multi-modal ultrasound video fusion network that can effectively perform multi-modal feature fusion and video classification for renal tumor diagnosis. The attention-based multi-modal fusion module uses crossattention and self-attention to extract modality-invariant features and modality-specific features in parallel. In addition, we design an objectlevel temporal aggregation (OTA) module that can automatically filter low-quality features and efficiently integrate temporal information from multiple frames to improve the accuracy of tumor diagnosis. Experimental results on a multicenter dataset show that the proposed framework outperforms the single-modal models and the competing methods. Furthermore, our OTA module achieves higher classification accuracy than the frame-level predictions. Our code is available at https://github.com/ JeunyuLi/MUAF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Renal cancer is the most lethal malignant tumor of the urinary system, and the incidence is steadily rising <ref type="bibr" target="#b12">[13]</ref>. Conventional B-mode ultrasound (US) is a good screening tool but can be limited in its ability to characterize complicated renal lesions. Contrast-enhanced ultrasound (CEUS) can provide information on microcirculatory perfusion. Compared with CT and MRI, CEUS is radiation-free, cost-effective, and safe in patients with renal dysfunction. Due to these benefits, CEUS is becoming increasingly popular in diagnosing renal lesions. However, recognizing important diagnostic features from CEUS videos to diagnose lesions as benign or malignant is non-trivial and requires lots of experience.</p><p>To improve diagnostic efficiency and accuracy, many computational methods were proposed to analyze renal US images and could assist radiologists in making clinical decisions <ref type="bibr" target="#b5">[6]</ref>. However, most of these methods only focused on conventional B-mode images. In recent years, there has been increasing interest in multi-modal medical image fusion <ref type="bibr" target="#b0">[1]</ref>. Directly concatenation and addition were the most common methods, such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref>. These simple operations might not highlight essential information from different modalities. Weight-based fusion methods generally used an importance prediction module to learn the weight of each modality and then performed sum, replacement, or exchange based on the weights <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">19]</ref>. Although effective, these methods did not allow direct interaction between multi-modal information. To address this, attention-based methods were proposed. They utilized cross-attention to establish the feature correlation of different modalities and self-attention to focus on global feature modeling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. Nevertheless, we prove in our experiments that these attentionbased methods may have the potential risks of entangling features of different modalities.</p><p>In practice, experienced radiologists usually utilize dynamic information on tumors' blood supply in CEUS videos to make diagnoses <ref type="bibr" target="#b7">[8]</ref>. Previous researches have proved that temporal information is effective in improving the performance of deep learning models. Lin et al. <ref type="bibr" target="#b10">[11]</ref> proposed a network for breast lesion detection in US videos by aggregating temporal features, which outperformed other image-based methods. Chen et al. <ref type="bibr" target="#b1">[2]</ref> showed that CEUS videos can provide more detailed blood supply information of tumors allowing a more accurate breast lesion diagnosis than static US images.</p><p>In this work, we propose a novel multi-modal US video fusion network (MUVF-YOLOX) based on CEUS videos for renal tumor diagnosis. Our main contributions are fourfold. <ref type="bibr" target="#b0">(1)</ref> To the best of our knowledge, this is the first deep learning-based multi-modal framework that integrates both B-mode and CEUSmode information for renal tumor diagnosis using US videos. <ref type="bibr" target="#b1">(2)</ref> We propose an attention-based multi-modal fusion (AMF) module consisting of cross-attention and self-attention blocks to capture modality-invariant and modality-specific features in parallel. <ref type="bibr" target="#b2">(3)</ref> We design an object-level temporal aggregation (OTA) module to make video-based diagnostic decisions based on the information from multi-frames. <ref type="bibr" target="#b3">(4)</ref> We build the first multi-modal US video datatset containing B-mode and CEUS-mode videos for renal tumor diagnosis. Experimental results show that the proposed framework outperforms single-modal, single-frame, and other state-of-the-art methods in renal tumor diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of Framework</head><p>The proposed MUVF-YOLOX framework is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. It can be divided into two stages: single-frame detection stage and video-based diagnosis stage. <ref type="bibr" target="#b0">(1)</ref> In the single-frame detection stage, the network predicts the tumor bounding box and category on each frame in the multi-modal CEUS video clips. Dual-branch backbone is adopted to extract the features from two modalities and followed by the AMF module to fuse these features. During the diagnostic process, experienced radiologists usually take the global features of US images into consideration <ref type="bibr" target="#b20">[20]</ref>. Therefore, we modify the backbone of YOLOX from CSP-Darknet to Swin-Transformer-Tiny, which is a more suitable choice by the virtue of its global modeling capabilities <ref type="bibr" target="#b14">[15]</ref>. (2) In the video-based diagnosis stage, the network automatically chooses high-confidence region features of each frame according to the single-frame detection results and performs temporal aggregation to output a more accurate diagnosis. The above two stages are trained successively. We first perform a strong data augmentation to train the network for tumor detection and classification on individual frames. After that, the first stage model is switched to the evaluation mode and predicts the label of each frame in the video clip. Finally, we train the OTA module to aggregate the temporal information for precise diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dual-Attention Strategy for Multimodal Fusion</head><p>Using complementary information between multi-modal data can greatly improve the precision of detection. Therefore, we propose a novel AMF module to fuse the features of different modalities. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the features of each modality will be input into cross-attention and self-attention blocks in parallel to extract modality-invariant features and modality-specific features simultaneously.</p><p>Taking the B-mode as an example, we first map the B-mode features F B and the CEUS-mode features</p><formula xml:id="formula_0">F C into (Q B , K B , V B ) and (Q C , K C , V C</formula><p>) using linear projection. Then cross-attention uses scaled dot-product to calculate the similarity between Q B and K C . The similarity is used to weight V C . Crossattention extracts modality-invariant features through correlation calculation but ignores modality-specific features in individual modalities. Therefore, we apply self-attention in parallel to highlight these features. The self-attention calculates the similarity between Q B and K B and then uses the similarity to weight V B . Similarly, the features of the CEUS modality go through the same process in parallel. Finally, we merge the two cross-attention outputs by addition since they are both invariant features of two modalities and concatenate the obtained sum and the outputs of the two self-attention blocks. The process mentioned above can be formulated as follows:</p><formula xml:id="formula_1">F invar = Sof tmax( Q B K T C √ d )V C + Sof tmax( Q C K T B √ d )V B (<label>1</label></formula><formula xml:id="formula_2">)</formula><formula xml:id="formula_3">F B-spec = Sof tmax( Q B K T B √ d )V B + F B (<label>2</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">F C-spec = Sof tmax( Q C K T C √ d )V C + F C (<label>3</label></formula><formula xml:id="formula_6">)</formula><formula xml:id="formula_7">F AM F = Concat(F B-spec , F invar , F C-spec )<label>(4)</label></formula><p>where, F invar represents the modality-invariant features. F B-spec and F C-spec represent the modal-specific features of B-mode and CEUS-mode respectively. F AM F is the output of the AMF module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Video-Level Decision Generation</head><p>In clinical practice, the dynamic changes in US videos provide useful information for radiologists to make diagnoses. Therefore, we design an OTA module that aggregates single-frame renal tumor detection results in temporal dimension for diagnosing tumors as benign and malignant. First, we utilize a feature selection module <ref type="bibr" target="#b13">[14]</ref> to select high-quality features of each frame from the Cls_conv and Reg_conv layers. Specifically, we select the top 750 grid cells on the prediction grid according to the confidence score. Then, 30 of the top 750 grid cells are chosen by the non-maximum suppression algorithm for reducing redundancy. The features are finally picked out from the Cls_conv and Reg_conv layers guided by the positions of the top 30 grid cells. Let F Cls = {Cls 1 , Cls 2 , ...Cls l } and F Reg = {Reg 1 , Reg 2 , ...Reg l } denote the above obtained high-quality features from l frames. After feature selection, we aggregate the features in the temporal dimension by time attention. F Cls and F Reg are mapped into (Q Cls , K Cls , V Cls ) and (Q Reg , K Reg ) via linear projection. Then, we utilize scaled dot-product to compute the attention weights of V Cls as:</p><formula xml:id="formula_8">T ime_Att. = [Sof tmax( Q Cls K T Cls √ d ) + Sof tmax( Q Reg K T Reg √ d )]V Cls (<label>5</label></formula><formula xml:id="formula_9">)</formula><formula xml:id="formula_10">F temp = T ime_Att. + F Cls (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>After temporal feature aggregation, F temp is fed into a multilayer perceptron head to predict the class of tumor.</p><p>3 Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Materials and Implementations</head><p>We collect a renal tumor US dataset of 179 cases from two medical centers, which is split into the training and validation sets. We further collect 36 cases from the two medical centers mentioned above (14 benign cases) and another center (Fujian Provincial Hospital, 22 malignant cases) to form the test set. Each case has a video with simultaneous imaging of B-mode and CEUS-mode. Some examples of the images are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. There is an obvious visual difference between the images from the Fujian Provincial Hospital (last column in Fig. <ref type="figure" target="#fig_1">2</ref>) and the other two centers, which raises the complexity of the task but can better verify our method's generalization ability. More than two radiologists with ten years of experience manually annotate the tumor bounding box and class label at the frame level using the Pair annotation software package (https://www.aipair. com.cn/en/, Version 2.7, RayShape, Shenzhen, China) <ref type="bibr" target="#b9">[10]</ref>. Each case has 40-50 labeled frames, and these frames cover the complete contrast-enhanced imaging cycle. The number of cases and annotated frames is summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Weights pre-trained from ImageNet are used to initialize the Swin-Transformer backbone. Data augmentation strategies are applied synchronously to B-mode and CEUS-mode images for all experiments, including random rotation, mosaic, mixup, and so on. All models are trained for 150 epochs. The batch size is set to 2. We use the SGD optimizer with a learning rate of 0.0025. The weight decay is set to 0.0005 and the momentum is set to 0.9. In the test phase, we use the weights of the best model in validation to make predictions. All Experiments are implemented in PyTorch with an NVIDIA RTX A6000 GPU. AP 50 and AP 75 are used to assess the performance of single-frame detection. Accuracy and F1-score are used to evaluate the video-based tumor diagnosis.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>Single-Frame Detection. We explore the impact of different backbones in YOLOX and different ways of multi-modal fusion. As shown in Table <ref type="table">2</ref>, using Swin-Transformer as the backbone in YOLOX achieves better performance than the original backbone while reducing half of the parameters. The improvement may stem from the fact that Swin-Transformer has a better ability to characterize global features, which is critical in US image diagnosis. In addition, we explore the role of cross-attention and self-attention blocks in multi-modal tasks, as well as the optimal strategy for combining their outputs. Comparing row 5 with row 7 and row 8 in Table <ref type="table">2</ref>, the dual-attention mechanism outperforms the single crossattention. It indicates that we need to pay attention to both modality-invariant and modality-specific features in our multi-modal task through cross-attention and self-attention blocks. However, "CA+SA" (row 6 in Table <ref type="table">2</ref>) obtains inferior performance than "CA" (row 5 in Table <ref type="table">2</ref>). We conjecture that connecting the two attention modules in series leads to the entanglement of modality-specific and modality-invariant information, which would disrupt the model training. On the contrary, the "CA//SA" method, combining two attention modules in parallel, enables the model to capture and digest modality-specific and modality-invariant features independently. For the same reason, we concatenate the outputs of the attention blocks rather than summing, which further avoids confusing modality-specific and modality-invariant information. Therefore, the proposed method achieves the best performance.</p><p>Table <ref type="table">2</ref>. The results of ablation study. "CA" and "SA" denote cross-attention and selfattention respectively. "//" and "+" mean parallel connection and series connection. Video-Based Diagnosis. We investigate the performance of the OTA module for renal tumor diagnosis in multi-modal videos. We generate a video clip with l frames from annotated frames at a fixed interval forward. As shown in Table <ref type="table" target="#tab_2">3</ref>, gradually increasing the clip length can effectively improve the accuracy. This suggests that the multi-frame model can provide a more comprehensive characterization of the tumor and thus achieves better performance. Meanwhile, increasing the sampling interval tends to decrease the performance (row 4 and row 5 in Table <ref type="table" target="#tab_2">3</ref>). It indicates that continuous inter-frame information is beneficial for renal tumor diagnosis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with Other Methods</head><p>The comparison results are shown in Table <ref type="table" target="#tab_3">4</ref>. Compared to the single-modal models, directly concatenating multi-modal features (row 3 in Table <ref type="table" target="#tab_3">4</ref>) improves AP 50 and AP 75 by more than 15%. This proves that complementary information exists among different modalities. For a fair comparison with other fusion methods, we embed their fusion modules into our framework so that different approaches can be validated in the same environment. CMML <ref type="bibr" target="#b19">[19]</ref> and CEN <ref type="bibr" target="#b16">[17]</ref> merge the multi-modal features or pick one of them by automatically generating channel-wise weights for each modality. They score higher AP in the validation set but lower one in the test set than "Concatenate". This may be because the generated weights are biased to make similar decisions to the source domain, thereby reducing model generalization in the external data. Moreover, CMF only highlights similar features between two modalities, ignoring that each modality contains some unique features. TMM focuses on both modality-specific and modality-invariant information, but the chaotic confusion of the two types of information deteriorates the model performance. Therefore, both CMF <ref type="bibr" target="#b16">[17]</ref> and TMM <ref type="bibr" target="#b8">[9]</ref> fail to outperform weight-based models. On the contrary, our AMF module prevents information entanglement by conducting cross-attention and self-attention blocks in parallel. It achieves AP 50 = 82.8, AP 75 = 60.6 in the validation set and AP 50 = 79.5, AP 75 = 39.2 in the test set, outperforming all competing methods while demonstrating superior generalization ability. Meanwhile, the improvement of the detection performance is beneficial to our OTA module to obtain lesion features from more precise locations, thereby improving the accuracy of benign and malignant renal tumor diagnosis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we create the first multi-modal CEUS video dataset and propose a novel attention-based multi-modal video fusion framework for renal tumor diagnosis using B-mode and CEUS-mode US videos. It encourages interactions between different modalities via a weight-sharing dual-branch backbone and automatically captures the modality-invariant and modality-specific information by the AMF module. It also utilizes a portable OTA module to aggregate information in the temporal dimension of videos, making video-level decisions. The design of the AMF module and OTA module is plug-and-play and could be applied to other multi-modal video tasks. The experimental results show that the proposed method outperforms single-modal, single-frame, and other stateof-the-art multi-modal approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Framework of MUVF-YOLOX. AMF module is used to fuse multi-modal features. OTA module is used to classify the tumor as benign or malignant based on videos. FSM means feature selection module.</figDesc><graphic coords="4,55,98,53,66,340,12,204,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of the annotated B-mode and CEUS-mode US images.</figDesc><graphic coords="6,57,48,142,97,337,60,148,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The details of our dataset. Number of cases in brackets.</figDesc><table><row><cell cols="3">Category Training Validation Test</cell></row><row><cell>Benign</cell><cell>2775(63) 841(16)</cell><cell>875(14)</cell></row><row><cell cols="2">Malignant 4017(81) 894(19)</cell><cell>1701(22)</cell></row><row><cell>Total</cell><cell cols="2">6792(144) 1735(35) 2576(36)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The results of video-based diagnosis.</figDesc><table><row><cell cols="3">Clip Length Sampling Interval Validation</cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Accuracy (%) F1-score (%) Accuracy (%) F1-score (%)</cell></row><row><cell>1</cell><cell>1</cell><cell>81.6</cell><cell>81.6</cell><cell>90.3</cell><cell>89.2</cell></row><row><cell>2</cell><cell>1</cell><cell>82.1</cell><cell>82.1</cell><cell>90.5</cell><cell>89.3</cell></row><row><cell>2</cell><cell>2</cell><cell>82.9</cell><cell>82.9</cell><cell>90.0</cell><cell>88.7</cell></row><row><cell>4</cell><cell>1</cell><cell>83.7</cell><cell>83.7</cell><cell>91.0</cell><cell>90.0</cell></row><row><cell>4</cell><cell>2</cell><cell>82.6</cell><cell>82.6</cell><cell>90.8</cell><cell>89.7</cell></row><row><cell>8</cell><cell>1</cell><cell>84.0</cell><cell>84.0</cell><cell>90.9</cell><cell>89.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Diagnosis results of different methods.</figDesc><table><row><cell cols="2">Fusion Methods Validation</cell><cell></cell><cell>External Test</cell><cell></cell></row><row><cell></cell><cell cols="4">AP50 AP75 Accuracy F1-score AP50 AP75 Accuracy F1-score</cell></row><row><cell>B-mode</cell><cell>59.6 38.0 76.4</cell><cell>76.3</cell><cell>58.1 22.1 80.4</cell><cell>79.1</cell></row><row><cell>CEUS-mode</cell><cell>60.1 30.6 78.2</cell><cell>78.1</cell><cell>52.1 14.1 70.5</cell><cell>69.3</cell></row><row><cell>Concatenate</cell><cell>78.8 50.5 79.6</cell><cell>79.5</cell><cell>76.8 38.8 86.8</cell><cell>85.7</cell></row><row><cell>CMML [19]</cell><cell>80.7 54.4 80.1</cell><cell>80.1</cell><cell>76.0 37.2 87.4</cell><cell>86.2</cell></row><row><cell>CEN [17]</cell><cell>81.4 56.2 83.0</cell><cell>83.0</cell><cell>74.3 36.3 85.1</cell><cell>83.8</cell></row><row><cell>CMF [18]</cell><cell>81.4 54.8 79.7</cell><cell>79.7</cell><cell>75.2 35.2 87.8</cell><cell>86.8</cell></row><row><cell>TMM [9]</cell><cell>80.8 52.7 80.1</cell><cell>80.1</cell><cell>74.3 37.0 84.4</cell><cell>83.2</cell></row><row><cell>Ours</cell><cell>82.8 60.6 84.0</cell><cell>84.0</cell><cell>79.5 39.2 90.9</cell><cell>89.9</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. Our dataset was collected from The <rs type="affiliation">Affiliated Nanchong Central Hospital of North Sichuan Medical College, Shenzhen People's Hospital</rs>, and <rs type="funder">Fujian Provincial Hospital</rs> hospitals. This study was approved by local institutional review boards. This work is supported by the <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> (No. <rs type="grantNumber">2021B1515120059</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62171290</rs>), and <rs type="funder">Shenzhen Science and Technology Program</rs> (No. <rs type="grantNumber">SGDX20201103095613036</rs> and <rs type="grantNumber">20220810145705001</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5dUpRQt">
					<idno type="grant-number">2021B1515120059</idno>
				</org>
				<org type="funding" xml:id="_53F9sKv">
					<idno type="grant-number">62171290</idno>
				</org>
				<org type="funding" xml:id="_v65fHGe">
					<idno type="grant-number">SGDX20201103095613036</idno>
				</org>
				<org type="funding" xml:id="_wQujRKG">
					<idno type="grant-number">20220810145705001</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A review on multimodal medical image fusion: compendious analysis of medical modalities, multimodal databases, fusion techniques and quality metrics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Azam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page">105253</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain knowledge powered deep learning for breast cancer diagnosis based on contrast-enhanced ultrasound videos</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2439" to="2451" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for RGB-D salient object detection. Pattern Recogn</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="376" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weighted concordance index loss-based multimodal survival modeling for radiation encephalopathy assessment in nasopharyngeal carcinoma radiotherapy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_19</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-1_19" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="191" to="201" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08430</idno>
		<title level="m">YOLOx: exceeding YOLO series in 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Analysis of kidney ultrasound images using deep learning and machine learning techniques: a review</title>
		<author>
			<persName><forename type="first">M</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Anita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pervasive Comput. Soc. Networking Proc. ICPCSN</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="183" to="199" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Personalized diagnostic tool for thyroid cancer classification using multi-view ultrasound</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_64</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_64" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quantitative multiparametric breast ultrasound: application of contrast-enhanced ultrasound and elastography leads to an improved differentiation of benign and malignant lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kapetas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Invest. Radiol</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">257</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">TranSiam: fusing multimodal visual features using transformer for medical image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.12185</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">102461</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A new dataset and a baseline model for breast lesion detection in ultrasound videos</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_59</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_59" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="614" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A medical image fusion method based on convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Conference on Information Fusion (Fusion)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">European association of urology guidelines on renal cell carcinoma: the 2019 update</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ljungberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Urol</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="799" to="810" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">YOLOV: making still image object detectors great at video object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.09686</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TransBTS: multimodal brain tumor segmentation using transformer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_11</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_11" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Strasbourg, France; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-10-01">27 September-1 October 2021. 2021</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion by channel exchanging</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4835" to="4845" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Channel exchanging networks for multimodal and multitask dense image prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5481" to="5496" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">RemixFormer: a transformer model for precision skin tumor differential diagnosis via multi-modal imaging and non-imaging data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference, Singapore</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022-09-22">18-22 September 2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="624" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_60</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_60" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comprehensive semisupervised multi-modal learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="4092" to="4098" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrast-enhanced ultrasound (CEUS) of benign and malignant renal tumors: distinguishing CEUS features differ with tumor size</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Med</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2551" to="2559" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
