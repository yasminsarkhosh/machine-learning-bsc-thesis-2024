<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diversity-Preserving Chest Radiographs Generation from Reports in One Stage</title>
				<funder ref="#_3EpWDPw">
					<orgName type="full">Proof of Concept Program of Zhongguancun Science City and Peking University Third Hospital</orgName>
				</funder>
				<funder ref="#_j6aaPRH">
					<orgName type="full">Beijing Natural Science Foundation</orgName>
				</funder>
				<funder ref="#_qCcWGTD">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zeyi</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruixin</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Peking University Third Hospital</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qizheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Peking University Third Hospital</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ning</forename><surname>Lang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Peking University Third Hospital</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xiuzhuang</forename><surname>Zhou</surname></persName>
							<email>xiuzhuang.zhou@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Artificial Intelligence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diversity-Preserving Chest Radiographs Generation from Reports in One Stage</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="482" to="492"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">FD76CF4A684F4D4280F7B0DD1E27C0E2</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_47</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Chest X-ray generation</term>
					<term>Radiology report</term>
					<term>Generative adversarial networks</term>
					<term>One-stage architecture</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automating the analysis of chest radiographs based on deep learning algorithms has the potential to improve various steps of the radiology workflow. Such algorithms require large, labeled and domainspecific datasets, which are difficult to obtain due to privacy concerns and laborious annotations. Recent advances in generating X-rays from radiology reports provide a possible remedy for this problem. However, due to the complexity of medical images, existing methods synthesize low-fidelity X-rays and cannot guarantee image diversity. In this paper, we propose a diversity-preserving report-to-X-ray generation method with one-stage architecture, named DivXGAN. Specifically, we design a domain-specific hierarchical text encoder to extract medical concepts inherent in reports. This information is incorporated into a one-stage generator, along with the latent vectors, to generate diverse yet relevant Xray images. Extensive experiments on two widely used datasets, namely Open-i and MIMIC-CXR, demonstrate the high fidelity and diversity of our synthesized chest radiographs. Furthermore, we demonstrate the efficacy of the generated X-rays in facilitating supervised downstream applications via a multi-label classification task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chest radiography is currently the most common imaging examination, playing a crucial role in epidemiological studies <ref type="bibr" target="#b2">[3]</ref> and clinical diagnosis <ref type="bibr" target="#b9">[10]</ref>. Nowadays, the automated analysis of chest X-rays using deep learning algorithms has attracted increasing attention due to its capability of significantly reducing the workload of radiologists and expediting clinical practice. However, training deep learning models to achieve expert-level performance on various medical imaging tasks requires large, labeled datasets, which are typically difficult to obtain due to data privacy and workload concerns. Developing generative models for high-fidelity X-rays that faithfully represent various medical concepts in radiology reports presents a possible remedy for the lack of datasets in the medical domain <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref>. This approach may substantially improve traditional supervised downstream tasks such as disease diagnosis <ref type="bibr" target="#b9">[10]</ref> and medical image retrieval <ref type="bibr" target="#b7">[8]</ref>.</p><p>Generating chest radiographs based on radiology reports can be thought of as transforming textual input into visual output, while current methods typically rely on text-to-image generation in computer vision. The fidelity and diversity of synthesized images are two major qualities of generative models <ref type="bibr" target="#b0">[1]</ref>, where fidelity means the generated images should be close to the underlying real data distribution, while diversity means the output images should ideally cover a large variability of real-world images. Recently, several works have been extensively proposed to generate high-fidelity images using GANs according to text prompts. StackGAN <ref type="bibr" target="#b24">[25]</ref> stacked multiple generators and discriminators to gradually increase the resolution of the generated images. AttnGAN <ref type="bibr" target="#b22">[23]</ref> synthesized images with fine-grained details by introducing a cross-modal attention mechanism between subregions of images and relevant words. DM-GAN <ref type="bibr" target="#b27">[27]</ref> introduced a memory module to refine fuzzy image contents caused by inferior initial images in stacked architecture. MirrorGAN <ref type="bibr" target="#b15">[16]</ref> reconstructed the text from the synthesized images to preserve cross-domain semantic consistency. Despite the progress of text-to-image generation in the general domain, generating X-rays from radiology reports remains challenging in terms of word embedding, handling the linguistic structure in reports, cross-modal feature fusion, etc.</p><p>The first work to explore generating chest X-rays conditioned on clinical text prompts is XrayGAN <ref type="bibr" target="#b23">[24]</ref>, which synthesized high-fidelity images in a progressive way. Additionally, XrayGAN <ref type="bibr" target="#b23">[24]</ref> proposed a hierarchical attentional text encoder to handle the linguistic structure of radiology reports, as well as a pretrained view-consistency network to constrain the generators. Although impressive results have been presented, three problems still exist: 1) The progressive generation stacks multiple generators of different scales trained separately in an adversarial manner (see Fig. <ref type="figure" target="#fig_0">1</ref>), where visual features of different scales are difficult to be fused uniformly and smoothly, making the final refined X-rays look like a simple combination of blurred contours and some details (see Fig. <ref type="figure" target="#fig_1">2</ref>). 2) A high proportion of reconstruction loss and a pre-trained view-consistency network are used at each layer for the convergence of training stacked generators, which severely limits the diversity of generated X-rays (only one chest radiograph can be generated from one report). 3) The word vectors in <ref type="bibr" target="#b23">[24]</ref> are based on the order in which words appear in the vocabulary, ignoring the information presented in medical-specific structures and the internal structure of words. More recently, another line of works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> investigated adapting pre-trained visuallanguage foundational models to generate chest X-rays. However, transferring the diffusion models <ref type="bibr" target="#b17">[18]</ref> trained with large multi-modal datasets to the medical imaging domain typically has high computational requirements.</p><p>In this paper, we propose a new report-to-X-ray generation method called DivXGAN to address the above issues. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, DivXGAN allows for the synthesis of various X-rays containing relevant clinical concepts from a single report. The following contributions are made: 1) Inspired by the one-stage architecture <ref type="bibr" target="#b20">[21]</ref>, we propose to directly synthesize high-fidelity X-rays without entangling different generators. 2) We discard the pixel-level reconstruction losses and introduce noise vectors in the latent space of the generator to provide the variability, thus allowing the diversity of generated chest radiographs. 3) Lastly, we design a domain-specific hierarchical text encoder to represent semantic information in reports and perform multiple cross-modal feature fusions during X-ray generation. We demonstrate the superiority of our method on two benchmark datasets and a downstream task of multi-label classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Let X and Z denote the image space and the low-dimensional latent space, respectively. Given a training set {x i , r i } N i=1 of N X-ray images, each of which x i is associated with a radiology report r i . The task of report-to-X-ray generation aims to synthesize multiple high-fidelity chest radiographs</p><formula xml:id="formula_0">x(j) i ∈ X j=1,2,...,</formula><p>from the corresponding report r i and latent noises {z j ∈ Z} j=1,2,..., . The generative models are expected to produce X-rays with high fidelity and diversity, so as to be used for data augmentation of downstream applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fidelity of Generated X-Rays</head><p>One-Stage Generation. Existing generative method <ref type="bibr" target="#b23">[24]</ref> uses a stacked structure to progressively synthesize high-fidelity X-rays. The stacked structure stabilizes the training of GANs but induces entanglements between generators trained separately in an adversarial way at different scales, resulting in fuzzy or discontinuous images. We draw inspiration from the one-stage architecture <ref type="bibr" target="#b20">[21]</ref> and propose to directly generate high-fidelity X-rays using a single pair of generator G and discriminator D. The network architecture of our method is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. The generator contains many up-sampling layers to increase the resolution of the synthesized X-ray xi , while the corresponding discriminator also requires many down-sampling operations to compute the adversarial loss. To stabilize the training of deep networks in this design, we introduce residual connections <ref type="bibr" target="#b5">[6]</ref> in both the generator and the discriminator.</p><p>Distill and Incorporate Report Knowledge. Semantic information and medical concepts in radiology reports should be fully interpreted and incorporated into visual features to reduce the distance between the generated data distribution and the real data distribution, thereby improving fidelity. We design a medical domain-specific text encoder with hierarchical structure to extract the embeddings of the free-text reports. At the word level, each sentence is represented as a sequence of T word tokens, plus a special token [SEN T ]. We embed each word token w t with an embedding matric W e , i.e., e t = W e w t . Unlike previous work <ref type="bibr" target="#b23">[24]</ref> that use one-hot embedding, we initialize our word embeddings with the pre-trained biomedical embeddings BioWordVec <ref type="bibr" target="#b26">[26]</ref>, which can capture the semantic information in the medical domain and the internal structure of words. Then, we use a Transformer encoder with positional embedding to capture the contextual information of each word and aggregate the holistic representations of the sentence into the embedding of the special token e [SENT ] :</p><formula xml:id="formula_1">e [SENT ] = T rsEncoder e [SENT ] , e1, e2, ..., eT<label>(1)</label></formula><p>At the sentence level, a report consists of a sequence of S sentences, each of which is represented as e (i)</p><p>[SENT ] using the word-level encoder described above. We also utilize a Transformer to learn the contextual importance of each sentence and encode them into a special token embedding e [REP O] , which serves as the holistic representation of the report:</p><formula xml:id="formula_2">e [REP O] = T rsEncoder e [REP O] , e<label>(1)</label></formula><p>[SENT ] , e <ref type="bibr" target="#b1">(2)</ref> [SENT ] , ..., e (S) <ref type="bibr">[SENT ]</ref> (2) Moreover, we perform cross-modal feature fusion after each up-sampling module of the generator (see Fig. <ref type="figure" target="#fig_0">1</ref>), to make the synthesized X-rays more faithful to the report. The fusion block contains two channel-wise Affine transformations and two ReLU layers. The scaling and shifting parameters of each Affine transformation are predicted by two MLPs (Multilayer Perceptron), using the vector e [REP O] as input. The Affine transformation expands the representation space of the generator G, allowing for better fusion of features from different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Diversity of Generated X-Rays</head><p>A radiology report is a medical interpretation of the corresponding chest radiograph, describing the clinical information included and assessing the patient's physical condition. Reports that describe chest radiographs of different patients with similar physical conditions are often consistent. Ideally, multiple X-ray images with the same health conditions could be generated from a single report, only with some differences in irrelevant factors such as body size, etc.</p><p>To this end, we omit the pixel-wise reconstruction loss and introduce noise vectors z in the latent space Z as one of the inputs to our one-stage generator, thereby providing the model with the necessary variability to ensure the diversity of synthesized X-rays. In this case, the generator G maps the low-dimensional latent space Z into a specific X-ray image space X r , conditioned on the report vector e i [REP O] :</p><formula xml:id="formula_3">x(j) i ← G zj, e i [REP O] , j = 1, 2, ... (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where x(j) i denotes the j-th synthesized X-ray from the i-th report r i . The noise vector z j ∈ Z follows a standard multivariate normal distribution N (0, I). In this way, given a radiology report, noise vectors can be sampled to generate various chest X-rays matching the medical description in the report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning Objectives and Training Process</head><p>Since DivXGAN uses a one-stage generator to directly generate high-fidelity chest radiographs, only one level of generator and discriminator needs to be alternately trained. The discriminator D outputs a scalar representing the probability that the input X-ray came from the real dataset and is faithful to the input report. There are three kinds of inputs that the discriminator can observe: real X-ray with matching report, synthesized X-ray with matching report, and real X-ray with mismatched report. The discriminator D x, e [REP O] ; θ d is trained to maximize the probability of assigning the report vector e [REP O] to the corresponding real X-ray x i , while minimizing the probability of the other two kinds of inputs. Due to multiple down-sampling blocks and residual connections, we employ the hinge loss <ref type="bibr" target="#b12">[13]</ref> to stabilize the training of D:</p><formula xml:id="formula_5">LD = Ex∼p data max 0, 1 -D x, e [REP O] + E G(z,e [REP O]) ∼pg max 0, 1 + D G z, e [REP O] , e [REP O] + Ex∼p mis max 0, 1 + D x, e [REP O] (4)</formula><p>where p data , p g and p mis denote the data distribution, implicit generative distribution (represented by G) and mismatched data distribution, respectively.</p><p>The generator G z, e [REP O] ; θ g builds a mapping from the latent noise distribution to the X-ray image distribution based on the correlated reports, fooling the discriminator to obtain high scores:</p><formula xml:id="formula_6">LG = -E G(z,e [REP O] )∼pg D G z, e [REP O] , e [REP O] (5)</formula><p>It is worth noting that the parameters θ t of the text encoder in Eqs. ( <ref type="formula" target="#formula_1">1</ref>) and ( <ref type="formula">2</ref>) are learned simultaneously during the training of the generator G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Experimental Settings</head><p>We use two public datasets, namely Open-i <ref type="bibr" target="#b3">[4]</ref> and MIMIC-CXR <ref type="bibr" target="#b8">[9]</ref>, to evaluate our generative model. The public subset of Open-i [4] consists of 7,470 chest X-rays with 3,955 associated reports. Following previous works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref>, we select studies with two-view X-rays and a report, then end up with 2,585 such studies. As for MIMIC-CXR <ref type="bibr" target="#b8">[9]</ref>, which contains 377,110 chest X-ray images and 227,827 radiology reports, for a fair comparison with alternative methods, we also conduct experiments on the p10 subset with 6,654 cases to verify the effectiveness of our approach. Moreover, we adopt the same data split protocol as used in XRayGAN <ref type="bibr" target="#b23">[24]</ref> for these two datasets, where the ratio of the train, validation, and test sets are 70%, 10%, and 20%. For consistency, we follow the set-up of XRayGAN <ref type="bibr" target="#b23">[24]</ref> to focus on two major sections in each free-text radiology report, namely the "findings" section and the "impression" section.</p><p>Our network is trained from scratch using the Adam <ref type="bibr" target="#b10">[11]</ref> optimizer with β 1 =0.0 and β 2 =0.9. The learning rates for G and D are set to 0.0001 and 0.0004, respectively, according to the Two Timescale Update Rule <ref type="bibr" target="#b6">[7]</ref>. The hidden dimension of the Transformer in the text encoder is 512. The noise vector z in the latent space is sampled from a standard multivariate normal distribution with a dimension of 100. The resolution of synthesized X-rays is 512 × 512. We implemented our method using PyTorch 1.7 and two GeForce RTX 3090 GPUs. We use Inception Score (IS) <ref type="bibr" target="#b18">[19]</ref> and Fréchet Inception Distance (FID) <ref type="bibr" target="#b6">[7]</ref> to assess the fidelity and diversity of the synthesized X-rays. Typically, IS and FID are calculated using an Inception-V3 model <ref type="bibr" target="#b19">[20]</ref> pre-trained on ImageNet, which might fail in capturing relevant features of the chest X-ray modality <ref type="bibr" target="#b11">[12]</ref>. Therefore, we calculate these metrics from the intermediate layer of a pre-trained CheXNet <ref type="bibr" target="#b16">[17]</ref>. Higher IS and lower FID indicate that the generated X-rays are more similar to the original X-rays. In addition, we also calculate the pairwise Structural Similarity Index Metric (SSIM) <ref type="bibr" target="#b21">[22]</ref> to evaluate the diversity of Xrays generated by different methods. A lower SSIM indicates a smaller structural similarity between images, which combined with a low FID, can be interpreted as higher generative diversity <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Analysis</head><p>We compare our approach with several state-of-the-art methods based on generative adversarial networks, including text-to-image generation: StackGAN <ref type="bibr" target="#b24">[25]</ref>, AttnGAN <ref type="bibr" target="#b22">[23]</ref>, and report-to-X-ray generation: XRayGAN <ref type="bibr" target="#b23">[24]</ref>. The performance of different approaches on the test sets of Open-i and MIMIC-CXR is shown in Table <ref type="table" target="#tab_0">1</ref>. We can observe that XRayGAN <ref type="bibr" target="#b23">[24]</ref> achieves better IS and FID than other text-to-image generation baselines. This is because the pixelwise reconstruction loss imposes strong constraints on the stacked generators at different scales to help avoid generating insensible images. However, this strong constraint severely reduces the diversity of generated X-rays, resulting in the worst SSIM. Our method consistently outperforms other alternatives by achieving both the lowest FID and the lowest SSIM, which means the generated X-rays have better fidelity and diversity. The reason lies in that the latent noise vectors impose the necessary variation factor, and the one-stage generation process and multiple cross-modal feature fusions improve the image quality. We conduct ablation experiments to quantify the impact of different components in DivXGAN. The results in Table <ref type="table" target="#tab_1">2</ref> show that our one-stage architecture definitely improves performance due to better cross-modal feature fusion. The domain-specific encoder outperforms the hierarchical attentional encoder <ref type="bibr" target="#b23">[24]</ref>, regardless of the backbone structure, indicating the advantage of the domainspecific embedding matrix, especially for medical reports with very rare and specific vocabulary. The comparison of SSIM for different components demonstrates that the latent vector input preserves the diversity of synthesized X-rays.</p><p>Visualization of chest X-rays synthesized from a report using different methods is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. As we can see, the X-rays generated by text-to-image baselines are very coarse, and even the outline of the hearts can be barely recognized. XRayGAN <ref type="bibr" target="#b23">[24]</ref> alleviates the blur and generates X-rays with relatively obvious chest contours, because of the strong constraints that prevent the generative model from producing abnormal samples. However, this strongly constrained approach still fails to preserve a clear outline of the heart and ribs, due to the entanglements between generators introduced by the stacked architecture. In particular, the lack of variability in the strong constraints results in only one X-ray being generated per report. Our method prevails over other alternatives as the generated X-rays are obviously clearer and more realistic, and even generate annotation information in the top right corner (seen in almost all samples). This phenomenon indicates the efficacy of the one-stage generation and multiple cross-modal feature fusion in generating high-fidelity X-rays. Furthermore, our method can generate various X-rays from one report, each of which manifests the relevant clinical findings. For example, the regions marked by red arrows in Fig. <ref type="figure" target="#fig_1">2</ref> show that our method can synthesize various different X-rays matching the clinical finding "Cardiomegaly" described in the report. Although our model is capable of generating X-rays based on radiology reports, due to the complexity of medical images, the synthesized X-rays have a limited range of gray-scale values and may not effectively capture high-frequency information such as subtle lung markings.  Furthermore, the ethical implications associated with the misuse of generated X-rays are significant. They should not be solely relied upon for clinical decisionmaking or used to train inexperienced medical students as radiologists. While the direct use of generated X-rays in clinical studies or medical training programs may not be appropriate, they can still serve valuable purposes in research, data augmentation, and other potential applications within the medical field. Here, we train a DenseNet-121 from scratch using various splits of real data (from MIMIC-CXR) and synthesized data (from DivXGAN) to demonstrate that the generated chest radiographs can be used for data augmentation when training downstream tasks. The task is a multi-label classification of four findings ("Cardiomegaly", "Consolidation", "Pleural Effusion" and "No Findings"). We randomly sample 5k real images with corresponding reports from the test set. These 5k real reports are input into our generative model, generating one image per report using a single latent vector, resulting in 5k generated images. When training the multilabel classifier, both real and generated images undergo the same general data augmentation techniques, such as rotation and scaling. As shown in Table <ref type="table" target="#tab_2">3</ref>, compared to the baseline trained exclusively on 5k real X-rays, the AUROC of the classifier trained exclusively on 5k synthesized X-rays drops by 0.019. However, training the classifier with 5k real X-rays and 5k generated X-rays improves AUROC by 0.031, suggesting that the synthesized X-rays can augment real data for supervised downstream applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we have devised a diversity-preserving method for high-fidelity chest radiographs generation from the radiology report. Different from state-ofthe-art alternatives, we propose to directly synthesize high-fidelity X-rays using a single pair of generator and discriminator. A domain-specific text encoder and latent noise vectors are introduced to distill medical concepts and incorporate necessary variability into the generation process, thus generating X-rays with high fidelity and diversity. We show the capability of our generative model in data augmentation for supervised downstream applications. Investigation of capturing high-frequency information of X-rays in generative models can be an interesting and challenging direction of future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Overview of the proposed diversity-preserving report-to-X-ray generation with one-stage architecture. (b) Existing report-to-X-ray generative model<ref type="bibr" target="#b23">[24]</ref>. DivX-GAN discards the stacked structure with strong constraints and incorporates necessary variability via latent vectors, thus synthesizing X-rays with high fidelity and diversity.</figDesc><graphic coords="3,65,79,53,84,292,57,162,88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. X-ray images generated by different methods from a radiology report, where the red arrows mark the clinical finding "Cardiomegaly" described by the underlined sentences in the report. (Color figure online)</figDesc><graphic coords="8,75,48,53,90,301,51,157,81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of different methods on Open-i and MIMIC-CXR.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Open-i</cell><cell>MIMIC-CXR</cell></row><row><cell></cell><cell>IS↑</cell><cell>FID↓ SSIM↓ IS↑</cell><cell>FID↓ SSIM↓</cell></row><row><cell cols="4">StackGAN [25] 1.043 243.4 0.138 1.063 245.5 0.212</cell></row><row><cell cols="4">AttnGAN [23] 1.055 226.6 0.171 1.067 232.7 0.231</cell></row><row><cell cols="4">XRayGAN [24] 1.081 141.5 0.343 1.112 86.15 0.379</cell></row><row><cell>DivXGAN</cell><cell cols="3">1.105 93.42 0.114 1.119 62.06 0.143</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on Open-i (Onestage structure has latent vector input).</figDesc><table><row><cell>Methods</cell><cell>FID ↓ SSIM ↓</cell></row><row><cell>Stack w/ Hia-encoder</cell><cell>141.5 0.343</cell></row><row><cell>Stack w/ Med-encoder</cell><cell>139.3 0.358</cell></row><row><cell cols="2">One-stage w/ Hia-encoder 98.91 0.151</cell></row><row><cell cols="2">One-stage w/ Med-encoder 93.42 0.114</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Classification performance of a DenseNet-121 trained with various splits.</figDesc><table><row><cell cols="4">Experiment Training Data AUROC</cell></row><row><cell></cell><cell cols="2">Real Synthetic</cell><cell></cell></row><row><cell>Real</cell><cell>5k</cell><cell>-</cell><cell>0.683</cell></row><row><cell>Synth</cell><cell>-</cell><cell>5k</cell><cell>0.664(↓0.019)</cell></row><row><cell cols="2">Real+Synth 5k</cell><cell>5k</cell><cell>0.714(↑0.031)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work is partially supported by the <rs type="funder">National Natural Science Foundation of China</rs> under grants <rs type="grantNumber">61972046</rs>, the <rs type="funder">Beijing Natural Science Foundation</rs> under grant <rs type="grantNumber">Z190020</rs>, and the <rs type="funder">Proof of Concept Program of Zhongguancun Science City and Peking University Third Hospital</rs> under grant <rs type="grantNumber">HDCXZHKC2022202</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qCcWGTD">
					<idno type="grant-number">61972046</idno>
				</org>
				<org type="funding" xml:id="_j6aaPRH">
					<idno type="grant-number">Z190020</idno>
				</org>
				<org type="funding" xml:id="_3EpWDPw">
					<idno type="grant-number">HDCXZHKC2022202</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">RoentGen: vision-language foundation model for chest X-ray generation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chambon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.12737</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Adapting pretrained vision-language foundational models to medical imaging domains</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chambon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bluethgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaudhari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.04133</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Standardized interpretation of paediatric chest radiographs for the diagnosis of pneumonia in epidemiological studies</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cherian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. World Health Organ</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="353" to="359" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Preparing a collection of radiology examinations for distribution and retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Med. Inform. Assoc</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="304" to="310" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Assessment of data augmentation strategies toward performance improvement of abnormality classification in chest radiographs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghoraani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Antani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="841" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Energy-based supervised hashing for multimorbidity image retrieval</title>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_20" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="205" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">317</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coronet: a deep neural network for detection and diagnosis of Covid-19 from chest X-ray images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bhat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page">105581</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Kynkäänniemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06026</idno>
		<title level="m">The role of imagenet classes in frechet inception distance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<title level="m">Geometric GAN</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring and distilling posterior and prior knowledge for radiology report generation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13753" to="13762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chest X-ray generation and data augmentation for cardiovascular abnormality classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karargyris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Syeda-Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing</title>
		<imprint>
			<biblScope unit="volume">10574</biblScope>
			<biblScope unit="page" from="415" to="420" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MirrorGAN: learning text-to-image generation by redescription</title>
		<author>
			<persName><forename type="first">T</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1505" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">CheXNet: radiologist-level pneumonia detection on chest Xrays with deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05225</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DF-GAN: a simple and effective baseline for text-to-image synthesis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16515" to="16525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">AttnGAN: fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gireesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10552</idno>
		<title level="m">XrayGAN: consistency-preserving generation of X-ray images from radiology reports</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">StackGAN: text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BioWordVec, improving biomedical word embeddings with subword information and MeSH</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DM-GAN: Dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5802" to="5810" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
