<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-view Vertebra Localization and Identification from CT Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Han</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiadong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhentao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nizhuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiming</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<email>dgshen@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai United Imaging Intelligence Co. Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai Clinical Research and Trial Center</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-view Vertebra Localization and Identification from CT Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="136" to="145"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">0340A26E40CA309F719C345512B30BE1</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_14</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vertebra localization and identification</term>
					<term>Contrastive learning</term>
					<term>Sequence Loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurately localizing and identifying vertebra from CT images is crucial for various clinical applications. However, most existing efforts are performed on 3D with cropping patch operation, suffering from the large computation costs and limited global information. In this paper, we propose a multi-view vertebra localization and identification from CT images, converting the 3D problem into a 2D localization and identification task on different views. Without the limitation of the 3D cropped patch, our method can learn the multiview global information naturally. Moreover, to better capture the anatomical structure information from different view perspectives, a multi-view contrastive learning strategy is developed to pre-train the backbone. Additionally, we further propose a Sequence Loss to maintain the sequential structure embedded along the vertebrae. Evaluation results demonstrate that, with only two 2D networks, our method can localize and identify vertebrae in CT images accurately, and outperforms the state-of-the-art methods consistently. Our code is available at https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localiz ation-and-Identification-from-CT-Images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic Localization and identification of vertebra from CT images are crucial in clinical practice, particularly for surgical planning, pathological diagnosis, and post-operative evaluation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. However, the process is challenging due to the significant shape variations of vertebrae with different categories, such as lumbar and thoracic, and also the close shape resemblance of neighboring vertebrae. Apart from these intrinsic challenges, the arbitrary field-of-view (FOV) of different CT scans and the presence of metal implant artifacts also introduce additional difficulties to this task.</p><p>With the advance of deep learning, many methods are devoted to tackling these challenges. For example, Lessmann et al. <ref type="bibr" target="#b10">[11]</ref> employed a one-stage segmentation method to segment vertebrae with different labels for localization and identification. It is intuitive but usually involves many segmentation artifacts. Building upon this method, Masuzawa et al. <ref type="bibr" target="#b11">[12]</ref> proposed an instance memory module to capture the neighboring information, but the long-term sequential information is not well studied. Recently, two or multi-stage methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>, that first localize the vertebra and further classify the detected vertebra patches, are proposed to achieve the state-of-the-art performance. And some additional modules, such as attention mechanism <ref type="bibr" target="#b4">[5]</ref>, graph optimization <ref type="bibr" target="#b12">[13]</ref>, and LSTM <ref type="bibr" target="#b14">[15]</ref>, are integrated to capture the sequential information of adjacent vertebrae. However, all these methods are performed on 3D patches, where the global information of the CT scan is destroyed and cannot be well-captured. Moreover, due to the lack of pre-trained models in 3D medical imaging, networks trained from scratch using a small dataset often lead to severe overfitting problems with inferior performance.</p><p>In this paper, to tackle the aforementioned challenges, we present a novel framework that converts the 3D vertebra labeling problem into a multi-view 2D vertebra localization and identification task. Without the 3D patch limitation, our network can learn 2D global information naturally from different view perspectives, as well as leverage the pre-trained models from ImageNet <ref type="bibr" target="#b5">[6]</ref>. Specifically, given a 3D CT image, we first generate multi-view 2D Digitally Reconstructed Radiograph (DRR) projection images. Then, a multi-view contrastive learning strategy is designed to further pre-train the network on this specific task. For vertebra localization, we predict the centroid of each vertebra in all DRR images and map the 2D detected centroids of different views back into the 3D CT scan using a least-squares algorithm. As for vertebra identification, we formulate it as a 2D segmentation task that generates vertebra labels around vertebra centroids. Particularly, a Sequence Loss, based on dynamic programming, is introduced to maintain the sequential information along the spine vertebrae in the training stage, which also serves as a weight to vote the multiview 2D identification results into the 3D CT image for more reliable results. Our proposed method is validated on a public challenging dataset <ref type="bibr" target="#b16">[17]</ref> and achieved the state-of-the-art performance both in vertebra localization and identification. Moreover, more evaluation results on a large-scale in-house dataset collected in real-world clinics (with 500 CT images) are provided in the supplementary materials, further demonstrating the effectiveness and robustness of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>An overview of our proposed method for vertebra localization and identification using multi-view DRR from CT scans is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, which mainly consists of three steps. Step 1 is to generate DRR images, followed by a multi-view contrastive learning strategy to pre-train the backbone. Step 2 aims to finish 2D single-view vertebra localization and identification, and step 3 is to map the 2D results back to 3D with a multi-view fusion strategy. We will elaborate our framework in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">DRR Multi-view Contrastive Learning</head><p>DRR Generation. To accurately localize and identify each vertebra in CT images, we convert the 3D task into 2D, where global information can be naturally captured from different views, avoiding the large computation of 3D models. To achieve this, DRR (Digitally Reconstructed Radiograph) technique, a simulation procedure for generating a radiograph similar to conventional X-ray image, is performed by projecting a CT image onto a virtual detector plane with a virtual X-ray source. In this way, we can generate K DRR projection images of a CT image for every 360/K degree. The 3D labeling problem can then be formulated as a multi-view localization and identification task in a 2D manner. Specifically, the 2D ground-truth are generated by projecting the 3D centroids and labels onto the 2D image following the DRR projection settings.</p><p>DRR Multi-view Contrastive Learning. After DRR generation, our goal is to localize and identify the vertebra on DRR images. However, as the dataset for the vertebra task is relatively small due to time-consuming manual annotation, we design a new multi-view contrastive learning strategy to better learn the vertebrae representation from various views. Unlike previous contrastive learning methods, where the pretext is learned from numerous augmented negative and positive samples <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, e.g., random crop, image flip, rotation and resize, the multi-view DRR images generated from the same CT image share consistent anatomical information, which are natural positive samples. Based on this insight, we pre-train our network backbone using the Simsiam <ref type="bibr" target="#b2">[3]</ref> approach to encode two random views from the same CT image as a key and query, as shown in Fig. <ref type="figure" target="#fig_0">1 (b)</ref>, in the aims of learning the invariant vertebrae representation from different views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Single-view Vertebra Localization</head><p>With multi-view DRR images, the 3D vertebra localization problem is converted into a 2D vertebra centroid detection task, followed by a multi-view fusion strategy (as introduced in Sect. 2.4) that transforms the 2D results to 3D. To achieve this, we utilize the commonly-used heatmap regression strategy for 2D vertebra centroid detection. Specifically, for each vertebra in a DRR image, our model is trained to learn the contextual heatmap defined on the ground-truth 2D centroid using a Gaussian kernel. During inference, we apply a fast peak search clustering method <ref type="bibr" target="#b15">[16]</ref> to localize the density peaks on the regressed heatmap as the predicted centroid. Benefiting from the pre-trained models from multiview contrastive learning, our method can capture more representative features from different views. Further, compared to existing 3D methods, our approach performs vertebra localization on several DRR images with a fusion strategy, making it more robust to the situation of missing detection in certain views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Single-View Vertebra Identification</head><p>After the vertebrae localization, we further predict the label of each vertebra using an identification network on multi-view DRR images. Unlike other 3D methods that require cropping vertebra patches for classification, our identification network performs on 2D, allowing us to feed the entire DRR image into the network, which can naturally capture the global information. Specifically, we use a segmentation model to predict the vertebra labels around the detected vertebra centroids, i.e., a 22 mm × 22 mm square centered at the centroid. During the inference of single-view, we analyze the pixel-wise labels in each square and identify the corresponding vertebra with the majority number of labels.</p><p>Sequence Loss. In the identification task, we observe that the vertebra labels are always in a monotonically increasing order along the spine, which implies the presence of sequential information. To better exploit this property and enhance our model to capture such sequential information, we propose a Sequence Loss as an additional network supervision, ensuring the probability distribution along the spine follows a good sequential order. Specifically, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we compute a probability map P ∈ R n×c for each DRR image by averaging the predicted pixel-wise possibilities in each square around the vertebra centroid from the identification network. Here, n is the number of vertebrae contained in this DRR image, and c indicates the number of vertebra categories (i.e., from C1 to L6). Due to the sequential nature of the vertebra identification problem, the optimal distribution of P is that the index of the largest probability in each row is in ascending order (green line in Fig. <ref type="figure" target="#fig_0">1</ref>). To formalize this notion, we compute the largest accumulated probability in ascending order, starting from each category in the first row and ending at the last row, using dynamic programming. The higher accumulated probability, the better sequential structure presented by this distribution. We set this accumulated probability as target profit, and aim to maximize it to enable our model to better capture the sequential structure in this DRR image. The optimal solution (OP T ) based on the dynamic programming algorithm is as:</p><formula xml:id="formula_0">OP T [i, j] = P [i, j] if j = 1 or i = 1 OP T [i -1, j -1] + D otherwise D = max(αP [i, j -1], βP [i, j], αP [i, j + 1]),<label>(1)</label></formula><p>where i ∈ [1, n] and j ∈ <ref type="bibr">[1, c]</ref>. Here, α and β are two parameters that are designed to alleviate the influence of wrong-identified vertebra. Sequence Loss (L s ) is then defined as:</p><formula xml:id="formula_1">L s = 1 - max(OP T [n, :]) βn . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>The overall loss function L id for our identification network is:</p><formula xml:id="formula_3">L id = L ce + γL s ,<label>(3)</label></formula><p>where L ce and L s refer to the Cross-Entropy loss and Sequence Loss, respectively. γ is a parameter to control the relative weights of the two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-view Fusion</head><p>Localization Multi-view Fusion. After locating all the vertebrae in each DRR image, we fuse and map the 2D centroids back to 3D space by a leastsquares algorithm, as illustrated in Fig. <ref type="figure" target="#fig_0">1 (d)</ref>. For a vertebra located in K views, we can track K target lines from the source points in DRR technique to the detected centroid on the DRR images. Ideally, the K lines should intersect at a unique point in the 3D space, but due to localization errors, this is always unachievable in practice. Hence, instead of finding a unique intersection point, we employ the least-squares algorithm to minimize the sum of perpendicular distances from the optimal intersection point to all the K lines, given by:</p><formula xml:id="formula_4">D(p; A, N ) = K k=1 D(p; a k , n k ) = K k=1 (a k -p) T (I -n k n T k )(a k -p),<label>(4)</label></formula><p>where p denotes the 3D coordinate of the optimal intersection point, a k and n k represent the point on the k th target line and the corresponding direction vector. By taking derivatives with respect to p, we get a linear equation of p as shown in Eq. ( <ref type="formula" target="#formula_6">5</ref>), where the optimal intersection point can be obtained by achieving the minimum distance to the K lines.</p><formula xml:id="formula_5">∂D ∂p = K k=1 -2(I -n k n T k )(a k -p) = 0 ⇒ Sp = q, S = K k=1 (I -n k n T k ), q = K k=1 (I -n k n T k )a k .</formula><p>(</p><formula xml:id="formula_6">)<label>5</label></formula><p>Identification Multi-view Voting. The Sequence Loss evaluates the quality of the predicted vertebra labels in terms of their sequential property. During inference, we further use this Sequence Loss of each view as weights to fuse the probability maps obtained from different views. We obtain the final voted identification map V of K views as:</p><formula xml:id="formula_7">V = K k=1 W k P k , W k = (1 -L k s ) K a=1 (1 -L a s ) . (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>For each vertebra, the naive solution for obtaining vertebra labels is to extract the largest probability from each row in voted identification map V . Despite the promising performance of the identification network, we still find some erroneous predictions. To address this issue, we leverage the dynamic programming (described in Eq. ( <ref type="formula" target="#formula_0">1</ref>)) again to correct the predicted vertebra labels in this voted identification map V . Specifically, we identify the index of the largest accumulated probability in the last row as the last vertebra category and utilize it as a reference to correct any inconsistencies in the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Evaluation Metric</head><p>We extensively evaluate our method on the publicly available MICCAI VerSe19 Challenge dataset <ref type="bibr" target="#b16">[17]</ref>, which consists of 160 spinal CT with ground truth annotations. Specifically, following the public challenge settings, we utilize 80 scans for training, 40 scans for testing, and 40 scans as hidden data. To evaluate the performance of our method, we use the mean localization error (L-Error) and identification rate (Id-Rate) as the evaluation metrics, which are also adopted in the challenge. The L-Error is calculated as the average Euclidean distance between the ground-truth and predicted vertebral centers. The Id-Rate is defined as the ratio of correctly identified vertebrae to the total number of vertebrae.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>All CT scans are resampled to an isotropic resolution of 1 mm. For DRR Multi-View Contrastive Learning, we use ResNet50 as encoder and apply the SGD optimizer with an initial learning rate of 0.0125, which follows the cosine decay schedule. The weight decay, SGD momentum, batch size and loss function are set to 0.0001, 0.9, 64, and cosine similarity respectively. We employ U-Net for both the localization and identification networks, using the pre-trained ResNet50 from our contrastive learning as backbone. Adam optimizer is set with an initial learning rate of 0.001, which is divided by 10 every 4000 iterations. Both networks are trained for 15k iterations. We empirically set α = 0.1, β = 0.8, γ = 1. All methods were implemented in Python using PyTorch framework and trained on an Nvidia Tesla A100 GPU with 40 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with SOTA Methods</head><p>We train our method on 70 CT images and tune the hyperparameter on the rest 10 CT images from the training data. We then evaluate it on both testing and hidden datasets, following the same setting as the challenge. In the comparison, our method is compared with four methods which are the first four positions in the benchmark of this challenge <ref type="bibr" target="#b16">[17]</ref>. The experimental results are presented in Table <ref type="table" target="#tab_0">1</ref>. Our method achieves Id-Rate of 98.12% and L-Error of 1.79 mm on the test dataset, and Id-Rate of 96.45% and L-Error of 2.17 mm on the hidden dataset, which achieves the leading performance both in localization and identification tasks with just two 2D networks. Compared to these methods performed on 3D with random cropping or patch-wise method (Payer C. <ref type="bibr" target="#b16">[17]</ref>, Lessmann N.</p><p>[17] and Chen M. <ref type="bibr" target="#b16">[17]</ref>), our 2D strategy can capture more reliable global and sequential information in all 2D projection images which can improve the labeling performance, especially the localization error. Compared to those using 2D MIP (Sekuboyina A. <ref type="bibr" target="#b17">[18]</ref>), our DRR multi-view projection and fusion strategy can provide superior performance by analyzing more views and introducing the geometry information carried by varied DRR projections naturally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>Ablation Study of Key Components. We conduct an ablation study on the VerSe19 dataset to demonstrate the effectiveness of each component. As presented in Table <ref type="table" target="#tab_1">2</ref>, we build the basic network for the vertebra localization and  identification with a bagging strategy, where for each vertebra, we opt for the ID that is predicted by the majority of views, when not using weighted voting, and K = 10, denoted as Baseline. Pre-train, Sequence Loss, and voting in Table <ref type="table" target="#tab_1">2</ref> represent the addition of the multi-view contrastive learning, Sequence Loss, and multi-view voting one by one. Pre-trained from ImageNet is used when not utilizing our contrastive learning pre-trained parameters. Specifically, the Baseline achieves Id-Rate of 84.00% and 83.54% on two datasets. With the contrastive learning pre-trained parameters, we achieve 1.88% and 2.98% improvements over the ImageNet pre-trained, respectively. This shows the pre-trained parameters of the backbone obtained from our contrastive learning can effectively facilitate the network to learn more discriminative features for identification than the model learning from scratch. Sequence Loss provides extra supervision for sequential information, and results in 3.53% and 4.02% increase, illustrating the significance of capturing the sequential information in the identification task. Finally, multi-view weighted voting yields the best results with 98.12% and 96.45% on the two datasets, indicating the robustness of our multi-view voting when the identification errors occurred in a small number of DRR images can be corrected by other DRR prediction results.</p><p>Ablation Study of Projection Number. We also conduct an ablation study on the same dataset to further evaluate the impact of the projection number K. The results are presented in Fig. <ref type="figure" target="#fig_1">2</ref>, indicating a clear trend of performance improvements as the number of projections K increases from 5 to 10. However, when K increases to 20, the performance is just comparable to that of 10. We analyze that using too few views may result in inadequate and unreliable anatomical structure representation, leading to unsatisfactory results. On the other hand, too many views may provide redundant information, resulting in comparable results but with higher computation cost. Therefore, K is set to 10 as a trade-off between accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel multi-view method for vertebra localization and identification in CT images. The 3D labeling problem is converted into a multi-view 2D localization and identification task, followed by a fusion strategy.</p><p>In particular, we propose a multi-view contrastive learning strategy to better learn the invariant anatomical structure information from different views. And a Sequence Loss is further introduced to enhance the framework to better capture sequential structure embedded in vertebrae both in training and inference. Evaluation results on a public dataset demonstrate the advantage of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of the proposed method, including (a) DRR Generation, (b) DRR multi-view contrastive learning, (c) single-view vertebra localization and identification, (d) multi-view localization fusion, and (e) multi-view identification voting. The implementation of Sequence Loss is also illustrated. (Color figure online)</figDesc><graphic coords="3,45,81,53,87,332,32,169,84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) The Id-Rate and L-Error of different K. (b) Comparison between different K from the final predicted CT scan on limited FOV and metal artifacts cases (red for ground truth and green for predictions). (Color figure online)</figDesc><graphic coords="8,73,98,171,38,304,12,198,52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on the VerSe19 challenge dataset.</figDesc><table><row><cell>Method</cell><cell>Test Dataset</cell><cell></cell><cell cols="2">Hidden Dataset</cell></row><row><cell></cell><cell cols="4">Id-Rate(%) L-Error(mm) Id-Rate(%) L-Error(mm)</cell></row><row><cell>Payer C. [17]</cell><cell>95.65</cell><cell>4.27</cell><cell>94.25</cell><cell>4.80</cell></row><row><cell cols="2">Lessmann N. [17] 89.86</cell><cell>14.12</cell><cell>90.42</cell><cell>7.04</cell></row><row><cell>Chen M. [17]</cell><cell>96.94</cell><cell>4.43</cell><cell>86.73</cell><cell>7.13</cell></row><row><cell cols="2">Sekuboyina A. [18] 89.97</cell><cell>5.17</cell><cell>87.66</cell><cell>6.56</cell></row><row><cell>Ours</cell><cell>98.12</cell><cell>1.79</cell><cell>96.45</cell><cell>2.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study results of key components.</figDesc><table><row><cell>Baseline Pre-train Sequence Loss Voting Id-Rate(%)</cell><cell></cell></row><row><cell cols="2">Test Dataset Hidden dataset</cell></row><row><cell>84.00</cell><cell>83.45</cell></row><row><cell>85.58</cell><cell>86.52</cell></row><row><cell>89.41</cell><cell>90.54</cell></row><row><cell>98.12</cell><cell>96.45</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_14.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated detection, localization, and classification of traumatic vertebral body fractures in the thoracic and lumbar spine at CT</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">278</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring simple Siamese representation learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic vertebrae localization and segmentation in CT with a two-stage Dense-U</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Net. Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">VertNet: accurate vertebra localization and identification network from CT images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_27" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="281" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computer-assisted screw size and insertion trajectory planning for pedicle screw placement surgery</title>
		<author>
			<persName><forename type="first">D</forename><surname>Knez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Likar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pernuš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vrtovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1420" to="1430" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robotic assistance and intervention in spine surgery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-12508-4_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-12508-4_16" />
	</analytic>
	<monogr>
		<title level="m">Spinal Imaging and Image Analysis</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="495" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterative fully convolutional neural networks for automatic vertebra segmentation and identification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lessmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Išgum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="142" to="155" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic segmentation, localization, and identification of vertebrae in 3D CT images using cascaded convolutional neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Masuzawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kitamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59725-2_66</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59725-2_66" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12266</biblScope>
			<biblScope unit="page" from="681" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vertebrae localization, segmentation and identification using a graph optimization and an anatomic consistency cycle</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pujades</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-21014-3_32</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-21014-3_32" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning in Medical Imaging</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Lian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Rekik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="307" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coarse to fine vertebrae localization and segmentation with SpatialConfiguration-Net and U-Net</title>
		<author>
			<persName><forename type="first">C</forename><surname>Payer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VISIGRAPP</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="124" to="133" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vertebrae labeling via end-to-end integral regression localization and multi-label classification network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2726" to="2736" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clustering by fast search and find of density peaks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Laio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">344</biblScope>
			<biblScope unit="issue">6191</biblScope>
			<biblScope unit="page" from="1492" to="1496" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">VerSe: a vertebrae labelling and segmentation benchmark for multi-detector CT images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sekuboyina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page">102166</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Btrfly net: vertebrae labelling with energy-based adversarial learning of local spine prior</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sekuboyina</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_74</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-3_74" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
