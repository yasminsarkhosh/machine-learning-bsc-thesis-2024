<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis</title>
				<funder ref="#_QbaMqk8">
					<orgName type="full">Federal Ministry of Education and Research of Germany (BMBF)</orgName>
				</funder>
				<funder ref="#_qvYZHRp">
					<orgName type="full">Bavarian Research Foundation (BFS)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Chantal</forename><surname>Pellegrini</surname></persName>
							<email>chantal.pellegrini@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Keicher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ege</forename><surname>Özsoy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Petra</forename><surname>Jiraskova</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Diagnostic and Interventional Radiology</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rickmer</forename><surname>Braren</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Diagnostic and Interventional Radiology</orgName>
								<orgName type="department" key="dep2">School of Medicine</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="420" to="429"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">3352BD47DC9865E04901FBDE1C095C75</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_41</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Zero-Shot Diagnosis</term>
					<term>Explainability</term>
					<term>Contrastive Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated diagnosis prediction from medical images is a valuable resource to support clinical decision-making. However, such systems usually need to be trained on large amounts of annotated data, which often is scarce in the medical domain. Zero-shot methods address this challenge by allowing a flexible adaption to new settings with different clinical findings without relying on labeled data. Further, to integrate automated diagnosis in the clinical workflow, methods should be transparent and explainable, increasing medical professionals' trust and facilitating correctness verification. In this work, we introduce Xplainer, a novel framework for explainable zero-shot diagnosis in the clinical setting. Xplainer adapts the classification-by-description approach of contrastive vision-language models to the multi-label medical diagnosis task. Specifically, instead of directly predicting a diagnosis, we prompt the model to classify the existence of descriptive observations, which a radiologist would look for on an X-Ray scan, and use the descriptor probabilities to estimate the likelihood of a diagnosis. Our model is explainable by design, as the final diagnosis prediction is directly based on the prediction of the underlying descriptors. We evaluate Xplainer on two chest X-ray datasets, CheXpert and ChestX-ray14, and demonstrate its effectiveness in improving the performance and explainability of zero-shot diagnosis. Our results suggest that Xplainer provides a more detailed understanding of the decision-making process and can be a valuable tool for clinical diagnosis. Our code is available on github: https://github.com/ChantalMP/Xplainer</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computer-aided diagnosis systems have become a prominent tool in medical diagnosis. Yet, their adoption is limited by the need for large amounts of annotated data for training, which hinders their scalability and adaptability to new clinical findings <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref>. Moreover, adapting to a new reporting template or clinical protocol necessitates new annotations, further reducing their feasibility in clinical settings. Recently, zero-shot <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b17">17]</ref> and few-shot <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref> learning methods have been proposed as a potential solution, utilizing contrastive pretraining <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">19]</ref> on pairs of radiology reports and images, and achieving performance on par with radiologists <ref type="bibr" target="#b15">[15]</ref>. However, these methods lack the level of detail of radiology reports and inherent explainability, impeding their adoption in clinical settings <ref type="bibr" target="#b6">[7]</ref>. Particularly, explaining the diagnosis with image descriptors is crucial to increase trust in the system and allow radiologists to verify the results <ref type="bibr" target="#b8">[9]</ref>.</p><p>Inspired by the success of using large language models to predict image descriptors in natural images <ref type="bibr" target="#b9">[10]</ref>, we introduce Xplainer, a novel framework that enhances the explainability of zero-shot diagnosis in the clinical setting. Xplainer leverages the classification-by-description approach <ref type="bibr" target="#b9">[10]</ref> of vision-language models and adapts it to the multi-label medical diagnosis task. Specifically, we task the model to classify the existence of descriptive observations, which a radiologist would examine on an X-Ray scan, instead of directly predicting a diagnosis. This model design imbues our framework with intrinsic explainability, as the final diagnosis prediction is predicated on the underlying descriptor predictions.</p><p>We evaluate Xplainer on two chest X-ray datasets, CheXpert <ref type="bibr" target="#b4">[5]</ref> and ChestX-ray14 <ref type="bibr" target="#b16">[16]</ref>, and demonstrate its efficacy in enhancing the performance and explainability of zero-shot diagnosis in the clinical setting. Our results highlight that Xplainer provides a more comprehensive understanding of the diagnosis prediction process, thereby serving as a valuable tool for clinical decision-making. In summary, Xplainer presents a novel framework for zero-shot diagnosis that not only improves explainability and accuracy but also provides an invaluable tool for computer-aided diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Overview</head><p>We propose Xplainer, an explainable zero-shot classification-by-description approach for diagnosing pathologies from X-Ray scans. Given an image i and a list of clinical observations o p1-n per pathology p, the goal is to make a multi-label prediction indicating the diagnosis for the patient.</p><p>Our zero-shot approach leverages the alignment of image and text embeddings provided by contrastive language-image pretraining (CLIP) <ref type="bibr" target="#b12">[13]</ref> and therefore does not require any labeled data. We built upon BioVil <ref type="bibr" target="#b0">[1]</ref>, a CLIP model pretrained on pairs of radiology reports and images. Employing the text and image encoders from BioVil, we calculate the cosine similarity between an Xray image and each of N pre-defined clinical observations o p1-N describing a pathology. Then we calculate observation probabilities P pos (o pi ) for every observation. Analogously, we calculate probabilities for the absence of all observations P neg (o pi ) by defining negated prompts for all observations. Using the softmax over the positive and negative probability, we calculate the final probability of the presence of an observation P (o pi ). Given these observation probabilities P (o pi ), i ∈ 1, ..., N , we estimate a joined probability to determine the likelihood of the presence of a pathology P (p):</p><formula xml:id="formula_0">P (p) = N i=1 log(P (o pi )) ÷ N (1)</formula><p>We repeat this process for all pathologies we want to diagnose in the image. As the prediction of a pathology diagnosis is directly extracted from the observation probabilities, our method is explainable by design, producing a diagnosis prediction and the detected X-ray observations leading to that prediction. Moreover, the observation probabilities show which observations the model mainly considers for its diagnosis. Figure <ref type="figure" target="#fig_0">1</ref> shows an overview of our framework.</p><p>To integrate multiple images of one patient, we calculate positive and negative observation probabilities for each image and average them before calculating the pathology probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prompt Engineering</head><p>Successful zero-shot inference relies on a good alignment between the contrastive pretraining and the downstream task <ref type="bibr" target="#b12">[13]</ref>. As BioVil <ref type="bibr" target="#b0">[1]</ref> was trained on pairs of radiological images and reports, we need to keep our observation prompts close to the style of medical reports. To initialize our prompts, we employ ChatGPT <ref type="bibr" target="#b10">[11]</ref> and query it to describe observations in X-ray images that would occur in a radiology report indicating specific pathologies. We further refined the prompts with the help of an experienced radiologist, who manually verified and adapted the descriptors. Human refinement cost was low, taking the radiologist only a few hours. We provide a complete list of the descriptors in the supplementary.</p><p>Radiology reports often include both presence and absence of particular observations. When comparing a prompt with an image embedding, it is hard for the model to differentiate between an observation's positive and negative occurrence, as their formulation can be very similar. Previous work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">15]</ref> has shown that introducing negative prompts can circumvent this problem. Therefore, instead of thresholding the similarity between a positive prompt and an image, we prompt the model with both a positive and a negated version of each observation prompt and compare their probabilities. We adapt our prompts in two additional steps to align them with the text in radiology reports. First, we add a disease indication, as radiology reports usually contain observations paired with conclusions. Further, this reduces the ambiguity of our prompts, as in radiology, one sign (e.g., Lung Opacity) can indicate multiple pathologies (e.g., Pneumonia, Atelectasis, or Edema). Additionally, we frame all our observations in a sentence structure sounding more like an actual report by adding "There is/are" before every observation. Putting all of this together, we define the following prompt structure: "There is/are (no) &lt;observation&gt; indicating &lt;pathology&gt;." Lastly, we define contrastive pathology-based prompts to compare to our observation-based prompting. In this setting, only two prompts, one positive and one negative prompt, are used per pathology. Overall, we compare the following styles of prompting to show the benefit of observation-based, contrastive prompting with disease indication and report style: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We evaluate Xplainer in a zero-shot setting on the commonly used chest X-ray datasets, CheXpert <ref type="bibr" target="#b4">[5]</ref>, and ChestX-ray14 <ref type="bibr" target="#b16">[16]</ref>. The CheXpert dataset provides a manually labeled validation and test set with 200 and 500 patients, respectively, and 14 classes, including "No Finding", "Support Devices/Foreign Objects", and 12 pathology labels. ChestX-ray14 is evaluated on 14 pathology labels on a test set of 25.596 images. For both datasets, we use the official validation and test splits. We perform a multi-label classification for both datasets and evaluate the performance via the Area Under the ROC-curve (AUC) between the positive pathology probabilities and the labels. Table <ref type="table" target="#tab_0">1</ref> shows our results compared to previously proposed zero-shot pathology prediction approaches. On CheXpert, we compare with Seibold et al. <ref type="bibr" target="#b13">[14]</ref> on the validation set, as they only reported validation performance. For the comparison with CheXzero <ref type="bibr" target="#b15">[15]</ref>, as well as the ChestX-ray14 dataset, we compare test set results. We outperform both previous works in an out-of-domain setting, where the zero-shot inference is performed on a different dataset than CLIP was trained on. The state-of-the-art results on both datasets show the effectiveness of our observation-based modeling. Further, in Table <ref type="table" target="#tab_1">2</ref>, we provide a detailed breakdown of our results per pathology and dataset. Ablation Studies. In our ablation studies, we investigate the impact of our prompt design and the effect of using multiple images. Table <ref type="table" target="#tab_2">3</ref> shows the results on the CheXpert validation set using different prompting styles. We observe that pathology-based prompting, which reaches an AUC of 76.14%, is significantly worse than observation-based prompting, which reaches an AUC of 84.92%, again highlighting the benefit of observation-based prompting. Comparing the basic observation-based prompting, using only positive prompts per observation, to contrastive prompting, we see a substantial performance gap, showing the importance of using negative prompts to differentiate between positive and negative occurrences. We also show the effect of formulating our prompts unambiguously and in the style of an actual radiology report by adding pathology indication and report style. Adding pathology indication to the contrastive observation-based prompting significantly improves performance, achieving an AUC of 84.35%. Finally, incorporating report style in the prompts leads to the highest AUC of 84.92%, indicating that a contrastive observation-based prompt with pathology indication and report style is the most effective for zero-shot X-ray pathology classification.</p><p>Additionally, we compare the initial ChatGPT output to our refined prompts (Table <ref type="table" target="#tab_3">4</ref>). Refinement was performed by deleting irrelevant, redundant, or incorrect descriptors. We observe an improvement through the refinement, indicating that including domain knowledge further improves our method. Nevertheless, the original ChatGPT prompts already perform quite well, showing the impressive potential of combining large generic language models with large domain-specific contrastive models.</p><p>For the "No Finding" class, we compare to either define specific prompts such as "Clear lung fields" or "Normal heart size and shape" to classify "No Finding" or model it as the absence of all of the other 13 labels (Rule-based).  shows that a rule-based modeling of this class leads to better results. A reason for this could be that there is no clearly defined list of observations that indicate a healthy X-ray scan, which a radiologist would mention in his report. Lastly, we investigate different image aggregation methods for pathology prediction. We compare only using a single frontal view X-ray to using all images available for a patient. For aggregation, we compute positive and negative observation probabilities for every image. In Max aggregation, we then use the highest observation probability. The intuition behind this approach is that an observation might be seen much better from one perspective than another, and then only the perspective where the model is most confident should be used. On the other hand, different views give different insights about which kind of observation a visual cue on the image indicates. To leverage this multi-view information, we test Mean aggregation, where all observation probabilities are averaged over multiple images. The results shown in Table <ref type="table" target="#tab_6">6</ref> indicate Mean aggregation to be superior, while both aggregation methods outperform using just a single image.</p><p>Qualitative Results. Figure <ref type="figure">2</ref> shows qualitative examples of our model's predictions. For the true positive prediction, it can be seen that most of the descriptors are detected, and the model recognizes the descriptor "Mass in the mediastinum" as the main indication for the Enlarged Cardiomediastinum. For the True Negative case, the model, correctly, detected none of the descriptors. For the false positive example, one can clearly see that the model made a mistake because it detected an air bronchogram with relatively high certainty and no consolidation. Therefore, this false positive finding is easily falsified by the radiologist since an air bronchogram is a finding that co-occurs with consolidation (i.e., air-filled bronchi in consolidated areas). Thus, knowing which combination of descriptors leads to such a decision substantially improves explainability. In the false positive case, the model misses the pacemaker but detects some implant, showing the model understands there is some foreign object, but can not identify it, which is easily detected by the radiologist. Overall the classification-bydescription may facilitate a plausibility check of a specific inference result and an understanding of the source of errors.</p><p>Discussion. One downside of modeling a joint probability is that it assumes that all descriptors appear simultaneously and gives all descriptors the same Fig. <ref type="figure">2</ref>. Qualitative results of Xplainer importance. While this estimation leads to good results, the assumption does not always hold, as a pathology does not always present with the same signs. Further, there might be inter-dependencies between the descriptors, e.g., there can be descriptors that strongly correlate with the presence of a disease when combined with one descriptor but much less when combined with another. As a first try to model the importance of descriptors, we look into a supervised, outof-domain approach to model these inter-dependencies. For this, we train a Naive Bayes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">18]</ref> CheXpert classifier on MIMIC-CXR <ref type="bibr" target="#b5">[6]</ref>, predicting a diagnosis given the descriptor probabilities, allowing the model to focus more on more relevant descriptors. While this approach relies on labels for MIMIC, these labels can be automatically generated by the CheXpert labeler <ref type="bibr" target="#b4">[5]</ref>, still not requiring human effort for labeling. We observe a slight performance increase on the test set from 80.58% to 81.37% AUC. This shows that the descriptor importance learned on MIMIC can partially be transferred to an out-of-domain dataset. We believe investigating methods to consider varying importance and complex relations between the descriptors is an essential and exciting direction to investigate in future work. Moreover, as Xplainer is not tied to specific image and text encoders, orthogonal works that lead to better encoders can be used to improve our results further.</p><p>The use of descriptors in Xplainer provides a flexible and adaptive approach to automated diagnosis prediction. By identifying and classifying the presence of descriptive observations, our model can capture the underlying characteristics of a disease without relying on labeled data. This means that our system can easily adapt to new settings with different clinical findings, including new conditions where the symptoms are known, but there is no training data available yet. Additionally, using descriptors allows for adapting the system to specific populations, where the essential descriptors can differ. This is because the model is not constrained by pre-defined labels but rather by the meaningful underlying features of a given diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we present a novel and effective zero-shot approach for chest X-ray diagnosis prediction, which provides an explanation for the model's decision. We leverage BioVil, a pretrained, domain-specific CLIP model, and use contrastive observation-based prompting to make predictions without label supervision. Our approach significantly outperforms previous zero-shot methods on CheXpert and Chest-Xray14, showcasing the effectiveness of our approach. Furthermore, we show that designing informative prompts is crucial to improve model performance. Our ablation studies demonstrate that adding disease indication and report style formulation to observation-based prompts notably enhances performance, underscoring the importance of aligning prompts with the domainspecific language used in medical reports. Additionally, contrastive prompts significantly boost performance, suggesting that the model can benefit from explicitly contrasting positive and negative examples.</p><p>Our work highlights the potential of contrastive pretraining combined with observation-based prompting as a promising avenue for zero-shot medical image classification, where labeled data is scarce or expensive to obtain, and explainability is vital. We envision that our approach can be extended to other medical imaging domains and have practical applications in real-world scenarios. Our findings contribute to the growing body of research to improve the accuracy and interpretability of medical image diagnosis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of Xplainer: In the first step, observation probabilities are calculated based on contrastive CLIP prompting. These are then used to make an explainable diagnosis prediction. The figure depicts an example for Pneumonia.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>Pathology-based: (No) &lt;pathology&gt; • Basic: Only positive prompt per pathology: &lt;observation&gt; • Contrastive: (No) &lt;observation&gt; • Pathology Indication: (No) &lt;observation&gt; indicating &lt;pathology&gt; • Report Style: There is/are (no) &lt;observation&gt; indicating &lt;pathology&gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,58,98,60,77,334,36,152,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>AUC for zero-shot pathology classification on CheXpert and ChestX-ray14 datasets. *in-domain, as the underlying CLIP model was trained the ChestX-ray14</figDesc><table><row><cell></cell><cell>CLIP pretraining data</cell><cell cols="2">CheXpert</cell><cell>ChestX-ray14</cell></row><row><cell></cell><cell></cell><cell>val</cell><cell>test</cell><cell>test</cell></row><row><cell>CheXzero [15]</cell><cell>MIMIC</cell><cell>-</cell><cell cols="2">74.73 -</cell></row><row><cell cols="2">Seibold et al. [14] MIMIC</cell><cell cols="2">78.86 -</cell><cell>71.23</cell></row><row><cell cols="4">Seibold et al. [14] MIMIC, PadChest, ChestX-ray14 83.24 -</cell><cell>78.33*</cell></row><row><cell>Xplainer</cell><cell>MIMIC</cell><cell cols="3">84.92 80.58 71.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>AUC per disease on both datasets</figDesc><table><row><cell></cell><cell cols="3">CheXpert Val CheXpert Test ChestX-ray14</cell></row><row><cell>No Finding</cell><cell>88.82</cell><cell>89.94</cell><cell>-</cell></row><row><cell>Enlarged Cardiomediastinum</cell><cell>79.23</cell><cell>80.60</cell><cell>-</cell></row><row><cell>Cardiomegaly</cell><cell>78.62</cell><cell>83.32</cell><cell>79.71</cell></row><row><cell>Lung Opacity</cell><cell>88.18</cell><cell>91.76</cell><cell>-</cell></row><row><cell>Lung Lesion</cell><cell>91.46</cell><cell>69.33</cell><cell>-</cell></row><row><cell>Edema</cell><cell>84.84</cell><cell>84.55</cell><cell>81.46</cell></row><row><cell>Consolidation</cell><cell>91.56</cell><cell>85.89</cell><cell>71.87</cell></row><row><cell>Pneumonia</cell><cell>85.68</cell><cell>83.73</cell><cell>70.83</cell></row><row><cell>Atelectasis</cell><cell>84.64</cell><cell>85.46</cell><cell>66.86</cell></row><row><cell>Pneumothorax</cell><cell>78.09</cell><cell>83.75</cell><cell>72.18</cell></row><row><cell>Pleural Effusion</cell><cell>88.72</cell><cell>89.30</cell><cell>79.11</cell></row><row><cell>Pleural Other</cell><cell>83.92</cell><cell>58.67</cell><cell>-</cell></row><row><cell>Fracture</cell><cell>-</cell><cell>60.47</cell><cell>-</cell></row><row><cell>Infiltration</cell><cell>-</cell><cell>-</cell><cell>68.81</cell></row><row><cell>Mass</cell><cell>-</cell><cell>-</cell><cell>70.28</cell></row><row><cell>Nodule</cell><cell>-</cell><cell>-</cell><cell>64.74</cell></row><row><cell>Emphysema</cell><cell>-</cell><cell>-</cell><cell>74.02</cell></row><row><cell>Fibrosis</cell><cell>-</cell><cell>-</cell><cell>62.25</cell></row><row><cell>Pleural Thickening</cell><cell>-</cell><cell>-</cell><cell>67.44</cell></row><row><cell>Hernia</cell><cell>-</cell><cell>-</cell><cell>74.60</cell></row><row><cell cols="2">Support Devices/Foreign Objects 80.25</cell><cell>81.15</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of different prompting styles on the validation set of CheXpert</figDesc><table><row><cell></cell><cell>AUC</cell></row><row><cell cols="2">Contrastive pathology-based Prompting 76.14</cell></row><row><cell>Observation-based Prompting:</cell><cell></cell></row><row><cell>Basic Prompt</cell><cell>58.65</cell></row><row><cell>Contrastive Prompt</cell><cell>77.00</cell></row><row><cell>+ pathology Indication</cell><cell>84.35</cell></row><row><cell>+ Report Style</cell><cell>84.92</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of ChatGPT prompts vs. refinement with the help of a radiologist</figDesc><table><row><cell></cell><cell cols="3">CheXpert Val CheXpert Test ChestX-ray14</cell></row><row><cell cols="2">ChatGPT prompts 83.61</cell><cell>79.94</cell><cell>71.40</cell></row><row><cell>Refined Prompts</cell><cell>84.92</cell><cell>80.58</cell><cell>71.73</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Modeling of "No Finding" label with explicit prompts or rule-based definition as lack of other findings</figDesc><table><row><cell></cell><cell>AUC -No Finding</cell></row><row><cell cols="2">Explicit Prompting 79.64</cell></row><row><cell>Rule-based</cell><cell>88.82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Comparison of single-view inference to different methods for multi-image processing</figDesc><table><row><cell></cell><cell>AUC</cell></row><row><cell cols="2">Only single Frontal View 84.19</cell></row><row><cell>All -Max Aggregation</cell><cell>84.77</cell></row><row><cell cols="2">All -Mean Aggregation 84.92</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The authors gratefully acknowledge the financial support by the <rs type="funder">Federal Ministry of Education and Research of Germany (BMBF)</rs> under project <rs type="projectName">DIVA</rs> (<rs type="grantNumber">FKZ 13GW0469C</rs>) and the <rs type="funder">Bavarian Research Foundation (BFS)</rs> under project <rs type="projectName">PandeMIC</rs> (grant <rs type="grantNumber">AZ-1429-20C</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_QbaMqk8">
					<idno type="grant-number">FKZ 13GW0469C</idno>
					<orgName type="project" subtype="full">DIVA</orgName>
				</org>
				<org type="funded-project" xml:id="_qvYZHRp">
					<idno type="grant-number">AZ-1429-20C</idno>
					<orgName type="project" subtype="full">PandeMIC</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9_41.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Making the most of text semantics to improve biomedical vision-language processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Boecking</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-20059-5_1</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-20059-5_1" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022: 17th European Conference</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<meeting><address><addrLine>Tel Aviv, Israel; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-10-27">23-27 October 2022. 2022</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXXVI</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Updating formulae and a pairwise algorithm for computing sample variances</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Leveque</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-51461-6_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-51461-6_3" />
	</analytic>
	<monogr>
		<title level="m">Toulouse 1982: Part I: Proceedings in Computational Statistics</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Caussinus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Ettinger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Tomassone</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="30" to="41" />
		</imprint>
	</monogr>
	<note>COMPSTAT 1982 5th Symposium</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Potential, challenges and future directions for deep learning in prognostics and health management applications</title>
		<author>
			<persName><forename type="first">O</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Svensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dersin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ducoffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page">103678</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GLoRIA: a multimodal globallocal representation learning framework for label-efficient medical image recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3942" to="3951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CheXpert: a large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">317</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Explaining chest X-ray pathologies in natural language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kayser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Emde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Papiez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_67" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore; Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="page" from="701" to="713" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Keicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mullakaeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Czempiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khakzar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15723</idno>
		<title level="m">Few-shot structured radiology report generation using natural language prompts</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">CHiLL: zero-shot custom interpretable feature extraction from clinical notes with large language models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mcinerney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Van De Meent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.12343</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Visual classification via description from large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.07183</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI: Chatgpt. chat.openai.com. Accessed</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2023-03">Mar 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computer-aided detection in chest radiography based on artificial intelligence: a survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Eng. Online</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Breaking with fixed set pathology recognition through report-guided contrastive training</title>
		<author>
			<persName><forename type="first">C</forename><surname>Seibold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reiß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleesiek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022: 25th International Conference</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09-22">18-22 September 2022</date>
			<biblScope unit="page" from="690" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_66</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_66" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Talius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ChestX-ray8: hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.10163</idno>
		<title level="m">MedCLIP: contrastive learning from unpaired medical images and text</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The optimality of Naive Bayes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Florida Artificial Intelligence Research Society Conference</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Barr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Markov</surname></persName>
		</editor>
		<meeting>the Seventeenth International Florida Artificial Intelligence Research Society Conference</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Contrastive learning of medical visual representations from paired images and text</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2" to="25" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
