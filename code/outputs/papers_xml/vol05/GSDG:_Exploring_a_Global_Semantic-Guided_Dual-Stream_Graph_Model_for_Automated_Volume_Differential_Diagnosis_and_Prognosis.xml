<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shouyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tongji University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianping</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yin</forename><surname>Wang</surname></persName>
							<email>yinw@tongji.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tongji University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GSDG: Exploring a Global Semantic-Guided Dual-Stream Graph Model for Automated Volume Differential Diagnosis and Prognosis</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="462" to="471"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">452C5C40D312528F3A2470941BCEDBBB</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_45</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Diagnosis</term>
					<term>Prognosis</term>
					<term>Semantic-guided grouping</term>
					<term>Graph convolutional networks</term>
					<term>Lesion localization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Three-dimensional medical images are crucial for the early screening and prognosis of numerous diseases. However, constructing an accurate computer-aided prediction model is challenging when dealing with volumes of different sizes due to numerous slices (native nodes) in a single case and variable-length slice sequence. We propose a Global Semantic-guided Dual-stream Graph model to address this issue. Our approach differs from the existing solution that aligns volumes with varying numbers of slices through downsampling. Instead, we leverage global semantic vectors to guide the grouping of native nodes, construct supernodes, and build dual-stream graphs by incorporating the sequential association of each volume's unique slices and the feature association of global semantic vectors. Specifically, we propose a shared global semantic vectors-based grouping method that aligns the number and the semantic distribution of nodes among different volumes without discarding slices. Furthermore, we construct a dual-stream graph module that enables Graph Convolutional Networks (GCN) to make clinical predictions from computer tomography (CT) volumes through the natural sequence association between native nodes and, simultaneously, the latent feature association between semantic vectors. We provide interpretability by visualizing the distribution of native nodes within each group and weaklysupervised slice localization. The results demonstrate that our method outperforms previous work in diagnostic (96.74%, +2.81%) and prognostic accuracy (84.56%, +1.86%) while being more interpretable, making it a promising approach for medical image analysis scenarios with limited fine-grained annotation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Related Works</head><p>Deep learning algorithms have shown success in performing computer-aided diagnosis (CAD) tasks using high-dimensional medical images, such as classification <ref type="bibr" target="#b19">[20]</ref>, detection <ref type="bibr" target="#b17">[18]</ref>, and segmentation <ref type="bibr" target="#b18">[19]</ref>. Physicians typically review slices sequentially in CT and diagnose based on changes in lesion morphology and knowledge within the key slices. However, the variability in the number of slices between volumes challenges the CAD model in capturing the complex associations between slices and assisting medical decision-making.</p><p>The diagnosis of COVID-19 is challenging. Convolutional Neural Networks (CNNs) and their variants, such as 3D CNNs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21]</ref> and 2.5D CNNs <ref type="bibr" target="#b17">[18]</ref>, have shown promise, CNNs required pre-extracted regions of interest (ROI) with aligned size, fine-grained annotation, and high computational complexity. For instance, <ref type="bibr" target="#b20">[21]</ref> employed a two-stage process where a segmentation model is trained first using segmentation masks. Then, a fixed-size 3D tensor is cropped from the lung region, transformed into a 4D tensor, and fed into the 3D classification net. Additionally, CT slices have intrinsic non-Euclidean associations, which has led to recent interest in using Transformers and Graph Neural Networks (GNNs) to handle them. For example, ViT <ref type="bibr" target="#b2">[3]</ref>, and Swin Transformer <ref type="bibr" target="#b7">[8]</ref> are variants of Transformers that use multi-head self-attention to learn fullyconnected associations between image patches. However, this architecture had primarily been applied to medical 3D patches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref> rather than the complete sequence. Regarding GNNs, ViG <ref type="bibr" target="#b3">[4]</ref> organized images into patch sequences but had yet to extend the model to variable-length sequences. Another earlier work <ref type="bibr" target="#b6">[7]</ref> used systematic sampling to align the number of slices, introducing sampling bias. We identify a common issue that existed in CNNs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>, Transformers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>, and GNNs <ref type="bibr" target="#b6">[7]</ref> that they cannot be directly trained end-to-end on variable-length slice sequences. Therefore, this paper proposes a graph model to break through this limitation by reconstructing node knowledge at the supernode level.</p><p>One of the major challenges is achieving consistency training in GCNs while preserving the integrity of the slice information. Existing graph model <ref type="bibr" target="#b6">[7]</ref> downsampled slices to align nodes, resulting in the loss of some critical information. This approach also treated slices at the same location after sampling from different volumes equally, assuming they have the same semantics, which contradicts semantic consistency and clinical meanings. Moreover, the complex associations between slices further complicate the modeling. In three-dimensional medical images, the slice sequence dynamics naturally encode critical knowledge about morphology changes, which is of great diagnostic value. A recent study <ref type="bibr" target="#b3">[4]</ref> showed that using sparse connections can improve the efficiency of GNNs, at least for natural images, and lead to better performance than other architectures such as CNNs and Transformers. Existing research mainly utilized GNNs to extract associations among slices or patches, constructing topology connections using methods such as k-nearest neighbors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13]</ref> and cosine similarity <ref type="bibr" target="#b6">[7]</ref>, as well as a learnable adjacency matrix <ref type="bibr" target="#b12">[13]</ref>. Such approaches have limitations in capturing the task-specific local sequential associations between slices and the higher-level global feature associations.</p><p>Besides, various approaches have been developed to locate key slices in CT volumes under weak supervision. For instance, <ref type="bibr" target="#b6">[7]</ref> proposed an end-to-end node masking method. In contrast, <ref type="bibr" target="#b10">[11]</ref> used CNNs as feature extractors and selected a fixed number of substantial slices based on Shannon entropy under a multipleinstance learning framework. This method calculates the average prediction distribution of slices under random noise but cannot be trained end-to-end. This limitation is precisely the problem we aim to address. Our contributions are: <ref type="bibr" target="#b0">(1)</ref> We propose a Global Semantic-guided Dual-stream Graph model for weakly-supervised graph classification tasks. It contains an unsupervised grouping algorithm called Semantic-guided Grouping, which aligns variable-length slice sequences using shared global semantic vectors to enable precise prediction by aligning both the node numbers and semantics. <ref type="bibr" target="#b1">(2)</ref> We develop a dual-stream Base Graph Module incorporating the local slice sequence and global semantic knowledge by learning these two representations jointly. (3) We thoroughly evaluate the effectiveness of our proposed method through comparison and ablation experiments. Our method outperforms the weakly-supervised benchmark GCNs in terms of accuracy of diagnosis and prognosis on a publicly available CT dataset while maintaining similar slice localization performance and offering more interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Statement</head><formula xml:id="formula_0">Given a dataset D = {(V i , y i )} ND n=1 , which consists of N D volumes. Each volume is represented by a set of slice nodes {v i,j } N V (i)</formula><p>j=1 , where N V (i) is the cardinality and varies for each volume. The volume-level label is given by y i . We first extract the native descriptors of the slices</p><formula xml:id="formula_1">X i = {x i,j |x i,j = F ext (v i,j )} N V (i) j=1 ∈ R m0×N V (i)</formula><p>using a spatial feature extractor, where the output of F ext is an m 0dimensional vector. To perform volume-level prediction under the guidance of shared semantic vectors, we introduce the Global Semantic-guided Dual-stream Graph (GSDG) model: ŷ = F GSDG (X, C). C indicate semantic vectors and will be introduced in the following. Figure <ref type="figure" target="#fig_0">1</ref> provides a schematic diagram of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Constructing Super-Nodes</head><p>We introduce a method for grouping native nodes X into super-nodes H which we denote as H = F Gro (X, C). To accomplish this, we propose semantic vectors</p><formula xml:id="formula_2">C = {c 1 , • • • , c K } ∈ R m0×K that</formula><p>correspond to K groups and are shared across all volumes, end-to-end updated with the model. In an unsupervised setting, previous work <ref type="bibr" target="#b16">[17]</ref> has extended cross-entropy minimization to optimal transportation. We build on this approach and draw inspiration from <ref type="bibr" target="#b0">[1]</ref> to propose our unsupervised grouping algorithm, Semantic-guided Grouping (SgG). SgG utilizes semantic vectors to guide the grouping process, ensuring that the resulting super-nodes align both the semantic and the number of nodes simultaneously on the variable-length volumes. However, minimizing cross-entropy in unsupervised classification can result in degeneration, where all slices are assigned to a single label. To address this issue, we encode the grouping label of a slice as the posterior distribution q(y c |x i,j ) and re-express cross-entropy as:</p><formula xml:id="formula_3">E(p, q) = - 1 N D ND i=1 1 N V (i) N V (i) j=1 K yc=1 q(y c |x i,j ) log p(y c |x i,j )<label>(1)</label></formula><p>To achieve semantic uniformity, we reformulate p(y c |x i,j ) as p(y c |x i,j , c yc ):</p><formula xml:id="formula_4">p(y c |x i,j , c yc ) = exp(x i,j c yc /τ ) y c exp(x i,j c y c /τ ) (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where τ is a temperature hyper-parameter. The mapping from native nodes to semantic vectors is described by Eq. 2. We represent this mapping using Q ∈ R K×N V (i) , and optimize it to maximize the similarity between the native node features and the semantic vectors of their corresponding groups. Therefore, the optimization objective of F Gro can be formulated as follows: min p,q E(p, q) s.t. ∀y c : q(y c |x i,j ) ∈ {0, 1} and</p><formula xml:id="formula_6">N V (i) j=1 q(y c |x i,j ) = N V (i) K (3)</formula><p>We utilize the Sinkhorn-Knopp (SK) algorithm <ref type="bibr" target="#b1">[2]</ref> to handle the constraint term, which aims to distribute N V (i) native nodes uniformly into K groups. The SK algorithm produces an assignment matrix S ∈ R NV ×K , where each row is a one-hot vector indicating the group index to which a native node belongs. Consequently, we compute H = F C(X S S 0 ), where F C is a linear layer and • 0 : R N V (i) ×K → R 1×K computes the column-wise 0-norm of a matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bi-level Adjacency Matrices</head><p>We aim to train GCN on a dataset of variable-length volumes; and have already grouped the native-nodes into super-nodes, H. Then, we could depict the adjacency relationships between nodes in H by semantic vectors or native nodes. Inspired by the multi-resolution model design in CNNs, we explicitly model the global and local adjacency relations: A G , A L = F Adj (X, S, C). A G represents the global semantic adjacency matrix, while A L the local sequence adjacency matrix of the learned super-nodes from X.</p><p>Global Adjacency Matrix Based on Grouping Semantic Vectors. The existing study <ref type="bibr" target="#b12">[13]</ref> utilized the Gumbel reparameterization trick <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref> to allow gradient flow through the adjacency matrix. Building upon the global semantic vectors C constructed in Sect. 2.2, we learn representations of the commonality between super-nodes over different volumes. Exactly, a link predictor, constructed by a 2-layer MLP, takes the concatenation of semantic vectors c i and c j as its input and produces the output θ i,j . We calculate the corresponding value,</p><formula xml:id="formula_7">A G(i,j) = sigmoid log (θ i,j / (1 -θ i,j )) + g 1 i,j -g 2 i,j</formula><p>/s , in the global adjacency matrix, g 1 i,j , g 2 i,j ∼ Gumbel(0, 1) and s is a hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Adjacency Matrix Based on Native Sequence Association.</head><p>To account for the associations varying with relative distance between slices within a volume, we utilize exponential smoothing to create a sequence adjacency matrix A ∈ R N V (i) ×N V (i) for each volume. The adjacency value between native nodes i and j is calculated by: A i,j = tanh(</p><formula xml:id="formula_8">D d=1 (1 -s d )s |i-j| d</formula><p>). Here, s represents the output of the sigmoid function applied to a learnable vector w L ∈ R D , and D is a hyper-parameter. We combine the connectivities of native nodes belonging to the same group using the allocation matrix S introduced in Sect. 2.2 and obtain the reduced local adjacency matrix appliable to super-nodes: A L = S T AS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Dual-Stream Graph Classifier</head><p>We introduce a graph classification module, denoted as ŷ = F Cls (H, A G , A L ), consisting of stacked Base Graph Modules (BGM) and a classifier. The BGM comprises two parallel isomorphic graph convolutions, a global feature GCN layer and a local sequence GCN layer, and a Multi-graph Bilinear Pooling (MgBP) module.</p><p>Base Graph Module. The two GCN layers pass messages between super-nodes from distinct perspectives and output H G , H L ∈ R m×K . Then the MgBP module extracts fine-grained graph-level representation, F, using a low-rank multimodal bilinear module: Classification Head. To obtain hierarchical features, we concatenate F i from each BGM layer along the feature dimension, resulting in F f inal = NG i=1 F i ∈ R m×K . We then compute the mean and max along the node dimension separately, resulting in two length-m vectors. These vectors are concatenated and passed through a 2-layer MLP and softmax activation for classification.</p><formula xml:id="formula_9">F = Pσ (UH G • VH L ) + b, where trainable weights U, V ∈ R d×m , P ∈ R m N G ×d , b ∈ R, d &lt; m,</formula><p>Weakly-Supervised Informative Slice Localization. Firstly, we obtain the predicted probability p base for the target class from F f inal using the Classification Head. Then, we mask each super-node in turn to create K sub-matrices of size m × (K -1). The Head is utilized again to calculate the new probabilities, which results in a vector p ∈ R K . The groups are ranked by d sn = p basep. Within a group, the distances between the native nodes and the super-node are measured using the dot product and normalized to the interval [0,1], which results in d rn . Slices' global importance within the group i is d sn(i) /d rn . We repeat this procedure for all groups and select the top k slices globally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Pre-processing</head><p>Our experiment used a public CT volume dataset 2019nCoVR <ref type="bibr" target="#b20">[21]</ref>, which contains complete chest CT scans from 929 COVID-19 (NCP) patients, 964 patients with common pneumonia (CP), and 849 healthy individuals (Normal). Among them, 408 patients are annotated with prognosis labels and some pneumonia patients with slice-level lesion annotations. To make a fair comparison with <ref type="bibr" target="#b6">[7]</ref>, we divided the dataset into training, validation, and testing sets using the same method with 20 random seeds. Each slice was resized to 224 × 224 pixels and normalized with mean = 0.449 and std = 0.226.</p><p>We chosen ResNet-50 <ref type="bibr" target="#b4">[5]</ref> as the F ext corresponding to m 0 = 2048. Only the frozen F ext module was pre-trained on ImageNet, while all other modules were trained end-to-end on the 2019nCoVR dataset. The model hyper-parameters were set to m = 256, K = 6, N G = 2, d = 64, k = 10, s = 0.5, τ = 1, and D = 8. AdamW <ref type="bibr" target="#b8">[9]</ref> served as the optimizer with a learning rate of 3e -3 and a batch size of 64. The model was trained with the cross-entropy loss for 20 epochs. After each feature aggregation, a Dropout <ref type="bibr" target="#b13">[14]</ref> layer with a rate of 0.1 was added. The selection of hyper-parameter values is mainly based on experience and constraints in the formula above. The diagnostic model, which has one output node activated by the sigmoid function, was trained from scratch. In contrast, the prognosis model with three output nodes activated by softmax was initialized with the weights of the trained diagnostic model, except for the classification head. We used the same evaluation metric, precision and recall, for weakly-supervised localization as <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Differential Diagnosis, Prognosis and Weakly-Supervised Localization</head><p>Table <ref type="table" target="#tab_0">1</ref> presents the diagnostic and prognostic performance of two state-of-theart architectures and our proposed method. The clinical AI system based on 3D CNNs <ref type="bibr" target="#b20">[21]</ref> was trained with volume-level and additional pixel-level labels.</p><p>Our proposed method outperforms the state-of-the-art weakly-supervised graph model GCN-DAP <ref type="bibr" target="#b6">[7]</ref> and 3D CNNs <ref type="bibr" target="#b20">[21]</ref> in terms of diagnostic accuracy and AUC scores. Furthermore, GSDG also surpassed <ref type="bibr" target="#b6">[7]</ref> in the prognostic task. These results demonstrate the superiority and effectiveness of our method in modeling full-size variable-length volumes. The left panel of Fig. <ref type="figure">2</ref> compares the performance of GSDG and experienced radiologists <ref type="bibr" target="#b20">[21]</ref> in diagnostic tasks. For the NCP, GSDG outperformed the radiologists. Moreover, when identifying Normal and CP cases, GSDG achieved similarly high levels of AUCs. These results suggest that our method is advantageous over radiologists in NCP diagnosis.</p><p>It is worth noting that our model required fewer training epochs than <ref type="bibr" target="#b20">[21]</ref>, as shown in the right panel of Fig. <ref type="figure">2</ref>, which highlights the faster convergence of GSDG compared to GCN-DAP <ref type="bibr" target="#b6">[7]</ref>. GSDG located the most informative CT slices, achieved 51.60% (2.75%) and 89.27% (8.95%) for precision and recall, respectively, slightly worse than <ref type="bibr" target="#b6">[7]</ref>, 57.39% (3.32%), in precision, but outperformed <ref type="bibr" target="#b6">[7]</ref>, 79.89% (3.94%), in recall. During our experiments, we frequently observed that the model became unstable as the precision score increased further and the predictions degraded to a single category. This may be due to the loss of some information that only exists in the Hounsfield Unit, as the 2019nCoVR dataset uses JPG instead of DCM format to save the slices. With the emergence of possible technologies that can recover the JPGs losslessly to DCMs, training the model on lung-masked slices is expected to produce better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Visualization of Grouping and Slice Localization</head><p>We visualized the grouping of native slices in Supplementary Material S.1 and the weakly-supervised slice localization for two patients in S.2. It can be observed that slices belonging to the same group display a remarkable degree of visual resemblance. Besides, the group importance distribution of NCP and CP cases are more similar to each other than to the Normal case, which reflects patterns differences between positive and negative cases. Within the positive cases, our model exhibits a tendency to concentrate more on the lung base in the CP case, as evidenced by columns 4 and 5 of Fig. <ref type="figure">2 (</ref> Regarding localization, our method's ability to identify lesion slices is superior to its precision performance, as indicated by Figs. 4 and 5 (S.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>The ablation study was conducted to evaluate the effectiveness of different node alignment methods and graph structures on the performance of the proposed model for variable-length volumes. The results are presented in Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table">2</ref> in Supplementary Material, S.3. We compared the performance of our proposed super-node strategy to systematic sampling <ref type="bibr" target="#b6">[7]</ref> for node alignment and found that the former outperformed the latter. We also compared global and local adjacency matrices and found that using both resulted in the best overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposes a novel approach for handling variable-length volume while preserving the integrity of the data by not discarding any slices, which is a departure from the previous method. Our approach first introduces a shared global semantic vectors-guided native node grouping scheme. Then we present an efficient and effective dual-stream graph module for simultaneously learning representations from global semantic vectors and sequence associations specific to each volume. Additionally, our approach offers informative slice localization and visually-consistent grouping outcomes, which enhances interpretability for clinical purposes. Moreover, the current dataset format prevents us from using existing semantic segmentation techniques to remove non-pulmonary noise. We will delve deeper into this direction to enhance localization accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Global Semantic-guided Dual-stream Graph (GSDG) model schematic diagram. Native CT nodes are first transformed into super-nodes guided by shared semantic vectors. Global feature and local sequence adjacency matrices are then generated to facilitate learning in the dual-stream base graph modules. The classification head produces diagnostic or prognostic probabilities and weakly-supervised slice localization upon concatenated multi-modal bilinear pooling features from all graph layers. GF : global feature, LS : local sequence. (Color figure online)</figDesc><graphic coords="3,41,79,54,53,340,54,197,17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and N G is the number of BGM layers. The resulting matrix, F ∈ R m N G ×K , represents the output of one BGM layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>S.1), in comparison to the NCP case, where the attention is around the middle lobe, as demonstrated in columns 4, 5 and 6 of Fig. 1 (S.1). These observations may reveal group semantic differences among positive cases and provide a new perspective for clinical diagnosis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Diagnostic and prognostic performance comparison. * : Results come from the original paper rather than 20 runs. ACC : macro accuracy, AUC : macro area under the receiver operating characteristic curve, SD: standard deviation, CI : confidence interval. Prefix D denotes the diagnosis task, and P stands for the prognosis task. All scores are multiplied by 100 to simplify the table. The left panel shows the diagnostic ROC curves of GSDG and the NCP diagnostic scores of four senior radiologists with 15 to 25 years of clinical experience<ref type="bibr" target="#b20">[21]</ref>.The right panel demonstrates convergence speed and accuracy of GSDG and<ref type="bibr" target="#b6">[7]</ref> on diagnostic and prognostic tasks. The diameter of each circle represents its standard deviation, multiplied by 10 for ease of observation. NCP, CP, and Normal represent COVID-19, common pneumonia, and healthy individuals, respectively (Color figure online)</figDesc><table><row><cell>Method</cell><cell>D-ACC (SD) D-AUC (95% CI)</cell><cell cols="2">P-ACC (SD) P-AUC (95% CI)</cell></row><row><cell cols="3">3D-CNN [21] 92.49 (N/A)  *  98.13 (96.91-99.02)  *  N/A</cell><cell>N/A</cell></row><row><cell cols="2">GCN-DAP [7] 93.93 (0.41) 99.00 (N/A)</cell><cell cols="2">82.70 (3.90) N/A (N/A)</cell></row><row><cell cols="4">GSDG (Ours) 96.74 (0.64) 99.65 (99.61-99.69) 84.56 (2.35) 90.89 (88.77-89.88)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. I would like to thank my wife, <rs type="person">Yang Feng</rs>, for her support during my doctoral studies.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 45.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9912" to="9924" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: lightspeed computation of optimal transport</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An image is worth 16 × 16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.00272</idno>
		<title level="m">Vision GNN: an image is worth graph of nodes</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with Gumbel-Softmax</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond COVID-19 diagnosis: prognosis with hierarchical graph representation learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87234-2_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87234-227" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12907</biblScope>
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: a continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bilateral adaptive graph convolutional network on CT based COVID-19 diagnosis with uncertainty-aware consensus-assisted multiple instance learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">102722</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised contrastive learning based transformer for lung nodule detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page">204001</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discrete graph structure learning for forecasting multiple time series</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=WEHSlH5mOk" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3D self-supervised methods for medical imaging</title>
		<author>
			<persName><forename type="first">A</forename><surname>Taleb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18158" to="18172" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-supervised pre-training of Swin transformers for 3D medical image analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page" from="20730" to="20740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Volumetric attention for 3D medical image segmentation and detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_20</idno>
		<idno>978-3-030-32226-7 20</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sli2Vol: annotate a 3D volume from a single slice with self-supervised learning</title>
		<author>
			<persName><forename type="first">P.-H</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I L</forename><surname>Namburete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-37" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="69" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale robust deep AUC maximization: a new surrogate loss and empirical studies on medical image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3040" to="3049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clinically applicable AI system for accurate diagnosis, quantitative measurements, and prognosis of COVID-19 pneumonia using computed tomography</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1423" to="1433" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
