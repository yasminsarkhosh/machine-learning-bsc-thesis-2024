<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network</title>
				<funder ref="#_Mx9CgaU">
					<orgName type="full">Basic Research Projects of Liaoning Provincial Department of Education</orgName>
				</funder>
				<funder ref="#_WXgfk2C">
					<orgName type="full">Science and Technology Innovation Talent Project in Shenyang</orgName>
				</funder>
				<funder>
					<orgName type="full">National Youth Talent Support Program of China</orgName>
				</funder>
				<funder ref="#_CH5qmSv">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ke</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoli</forename><surname>Yin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Shengjing Hospital of China Medical University</orgName>
								<address>
									<postCode>110004</postCode>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingda</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fakai</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shu</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Shengjing Hospital of China Medical University</orgName>
								<address>
									<postCode>110004</postCode>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawen</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chunli</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Shengjing Hospital of China Medical University</orgName>
								<address>
									<postCode>110004</postCode>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyu</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hupan Lab</orgName>
								<address>
									<postCode>310023</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">DAMO Academy</orgName>
								<orgName type="institution" key="instit2">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Shi</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Shengjing Hospital of China Medical University</orgName>
								<address>
									<postCode>110004</postCode>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="72" to="82"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">43F6152AEF3D7E80B9274269FED39697</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-25T18:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Liver tumor</term>
					<term>Lesion segmentation and classification</term>
					<term>CT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Liver tumor segmentation and classification are important tasks in computer aided diagnosis. We aim to address three problems: liver tumor screening and preliminary diagnosis in non-contrast computed tomography (CT), and differential diagnosis in dynamic contrastenhanced CT. A novel framework named Pixel-Lesion-pAtient Network (PLAN) is proposed. It uses a mask transformer to jointly segment and classify each lesion with improved anchor queries and a foregroundenhanced sampling loss. It also has an image-wise classifier to effectively aggregate global information and predict patient-level diagnosis. A largescale multi-phase dataset is collected containing 939 tumor patients and 810 normal subjects. 4010 tumor instances of eight types are extensively annotated. On the non-contrast tumor screening task, PLAN achieves 95% and 96% in patient-level sensitivity and specificity. On contrastenhanced CT, our lesion-level detection precision, recall, and classification accuracy are 92%, 89%, and 86%, outperforming widely used CNN and transformers for lesion segmentation. We also conduct a reader study on a holdout set of 250 cases. PLAN is on par with a senior human radiologist, showing the clinical significance of our results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Liver cancer is the third leading cause of cancer death world-wide in 2020 <ref type="bibr" target="#b13">[14]</ref>. Early detection and accurate diagnosis of liver tumors may improve overall patient outcomes, in which imaging plays a key role <ref type="bibr" target="#b10">[11]</ref>. Computed tomography (CT) is one of the most important imaging modalities for liver tumors. Dynamic contrast-enhanced (DCE) CT is widely used for diagnostics, but it requires iodine contrast injection which can cause reaction and potential risks in patients. Recently, non-contrast (NC) CT scans are gaining attention as they are cheaper and safer to acquire, thus can be potential tools for opportunistic tumor screening <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>. Meanwhile, finding and diagnosing tumors in NC CTs is also extremely challenging because of the poor contrast between tumors and normal tissues compared to those in DCE CTs. Prior works on pancreas <ref type="bibr" target="#b17">[18]</ref> and esophagus <ref type="bibr" target="#b19">[20]</ref> have shown that latest deep learning techniques can detect subtle texture and shape changes in NC CT that even human eyes may miss. Thus, we aim to investigate the performance of liver tumor segmentation and classification in NC CTs. Such an approach will be helpful to discover asymptomatic incidental tumors <ref type="bibr" target="#b11">[12]</ref> from routine NC CT scans indicated for general diagnostic purposes at no additional cost and radiation exposure. After an incidental tumor is found, the patient may undergo further imaging examination such as a multi-phase DCE CT for differential diagnosis <ref type="bibr" target="#b10">[11]</ref>, which can provide useful discriminative information such as the vascularity of lesions and the pattern of contrast agent enhancement <ref type="bibr" target="#b18">[19]</ref>. Liver is largest solid organ in body and is the site of many tumor types <ref type="bibr" target="#b10">[11]</ref>. Therefore, accurate tumor type classification is important for the decision of treatment plans and prognosis.</p><p>Many researchers have developed algorithms to automatically segment <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">23]</ref> or classify <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">25]</ref> liver tumors in CT to help radiologists improve their accuracy and efficiency. For example, public datasets such as the Liver Tumor Segmentation Benchmark (LiTS) <ref type="bibr" target="#b0">[1]</ref> fostered a series of works aiming to segment liver tumors with improved convolutional neural network (CNN) backbones <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> and lesion edge information <ref type="bibr" target="#b14">[15]</ref>. LiTS only has single-phase CTs (venous phase). Several studies investigated methods to exploit multi-phase CT by methods such as hetero-phase fusion <ref type="bibr" target="#b4">[5]</ref> and modality-aware mutual learning <ref type="bibr" target="#b23">[23]</ref>. There are few work discussing liver tumor analysis in NC CT <ref type="bibr" target="#b4">[5]</ref>. Besides lesion segmentation, CNN-based lesion classification algorithms have been studied to distinguish common lesion types <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">25]</ref>.</p><p>In this paper, we build a comprehensive framework to address both tumor screening and diagnosis. (1) Tumor screening involves finding tumor patients in a large pool of healthy subjects and patients. Most existing works in tumor segmentation and detection did not explicitly consider it since their training and testing images are all tumor patients. Such models may generate false positives in real-world screening scenario when facing diverse tumor-free images. We collect a large-scale dataset with both tumor and non-tumor subjects, where the non-tumor subjects includes not only healthy ones, but also patients with various diffuse liver diseases such as steatosis and hepatitis to improve the robustness of the algorithm. (2) Most works studied liver tumor segmentation alone without differentiating tumor types, while a few works classify liver tumors on cropped tumor patches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">25]</ref>. Meanwhile, we learn tumor segmentation and classification with one network using an instance segmentation framework <ref type="bibr" target="#b2">[3]</ref>. We train two networks for NC and multi-phase DCE CTs, respectively. (3) For evaluation, previous segmentation works typically use pixel-level metrics such as Dice coefficient. Such metrics cannot reflect the lesion-level accuracy (how many lesion instances are correctly detected and classified) and may bias to large lesions when a patient has multiple tumors. Patient-level metrics (e.g. classifying whether a subject has malignant tumors) are also useful for treatment recommendation in clinical practice <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>. Therefore, we assess our algorithm thoroughly with pixel, lesion, and patient-level metrics.</p><p>Algorithms for liver tumor segmentation have focused on improving the feature extraction backbone of a fully-convolutional CNN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">23]</ref>. The pixelwise segmentation architectures may not be optimal for lesion and patient-level evaluation metrics since they cannot consider a lesion or an image holistically. Recently, a series of mask transformer algorithms <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17]</ref> have emerged in the computer vision community and achieved the state-of-the-art performance in instance segmentation tasks. In brief, they use object queries to interact with image feature maps and with each other to produce mask and class predictions for each instance. Inspired by them, we propose a novel end-to-end framework named Pixel-Lesion-pAtient Network (PLAN) for lesion segmentation and classification, as well as patient classification. It contains three branches with bottomup cooperation: The segmentation map from the pixel branch helps to initialize the lesion branch, which is an improved mask transformer aiming to segment and classify each lesion; The patient branch aggregates information from the whole image and predicts image-level labels of each lesion type, with regularization terms to encourage consistency with the lesion branch.</p><p>We collected a large-scale multi-phase dataset containing 810 non-tumor subjects and 939 tumor patients. 4010 tumor instances of eight types are extensively annotated based on pathological reports. On the non-contrast tumor screening and diagnosis task, PLAN achieves 95.0%, 96.4%, and 0.965 in patient-level sensitivity, specificity, and average AUC for malignant and benign patients, in contrast to 94.4%, 93.7%, and 0.889 for the widely-used nnU-Net <ref type="bibr" target="#b7">[8]</ref>. On multi-phase DCE CT, our lesion-level detection precision, recall, and classification accuracy are 92.2%, 89.0%, 85.9%, outperforming nnU-Net <ref type="bibr" target="#b7">[8]</ref> and Mask2Former <ref type="bibr" target="#b2">[3]</ref>. We further conduct a reader study on a holdout set of 250 cases. Our algorithm is on par with a senior radiologist (16 yrs experience), showing the clinical significance of our results. Our codes will be made public upon institutional approval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminary on Mask Transformer</head><p>Mask transformers are a series of latest works achieving superior accuracy on various segmentation tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref>. Different from traditional fullyconvolutional segmentators <ref type="bibr" target="#b7">[8]</ref> that predict a class label for each pixel, mask transformers predict a class label and a binary mask for each object. Take Mask2Former <ref type="bibr" target="#b2">[3]</ref> as an example. It includes a pixel encoder and a pixel decoder that extract a high-resolution pixel embedding tensor P ∈ R M ×D×H×W from the image, where M is the embedding dimension, D × H × W is the shape of the 3D image. A group of Q learnable feature vectors {q i ∈ R M } Q i=1 are randomly initialized as object queries. They are processed by a transformer decoder to interact with multi-scale image features and each other using cross and self-attention operations. After processing, each query is supposed to contain information of one object, which can be used to predict the class probability c ∈ R C+1 of the object. Here C is the number of object classes, and we add 1 to indicate an additional "no-object" class if the query does not match with any object. In training, Mask2Former uses bipartite matching <ref type="bibr" target="#b1">[2]</ref> to assign each query to a ground-truth object (or "no-object"). Multiplying q i with P gives the binary mask m i ∈ R D×H×W of object i. During inference, the class and mask predictions of all queries can be merged by matrix multiplication to obtain the final semantic segmentation result Ŷ ∈ R C×D×H×W . We refer readers to <ref type="bibr" target="#b2">[3]</ref> for more details.</p><p>Mask transformers have various advantages when applied to our task. They can classify a lesion as a whole instead of classifying each pixel, thus can view each lesion holistically. Cross-attention is used to aggregate global features for each lesion. Inter-lesion relation can also be exploited by self-attention operations. In liver CT, inter-lesion relation is diagnostically useful, e.g., metastases and cysts are often multiple. Therefore, We pioneer mask transformers' adaptation for lesion segmentation and classification in 3D medical images. Given a groundtruth or a predicted lesion mask image, we perform connected component (CC) analysis and treat each CC as a lesion instance for training and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pixel-Lesion-Patient Network (PLAN)</head><p>Our goal is to segment the mask and classify the type of each tumor in a liver CT. We also hope to make patient-level diagnoses for each CT scan. PLAN is inspired by Mask2Former <ref type="bibr" target="#b2">[3]</ref> with three key improvements: (1) A pixel branch is added to provide anchor queries to the lesion branch. (2) The lesion branch is composed of the transformer decoder in Mask2Former, and we improve its segmentation loss to enhance recall of small lesions. (3) A patient branch is attached to make dedicated image-level predictions with a proposed lesion-patient consistency loss. Our framework is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Pixel Branch and Anchor Queries. The pixel branch is a convolutional layer after the pixel decoder and learns to predict pixel-wise segmentation maps similar to traditional segmentators. We do CC analysis to the predicted mask to extract lesion instances, and then average the pixel embeddings inside each predicted lesion to obtain a feature vector. The feature vectors are regarded as anchor queries and work the same way as the randomly initialized queries in the lesion branch. Compared to the random queries in the original Mask2Former, the anchor queries contain prior information of the lesions to be segmented, helping the lesion branch to match with the lesion targets more easily <ref type="bibr" target="#b9">[10]</ref>.</p><p>Lesion Branch and Foreground-Enhanced Sampling Loss. Similar to Mask2Former, the lesion branch predicts a binary mask and a class label for each query, see Fig. <ref type="figure" target="#fig_0">1</ref>. Mask2Former calculates its segmentation loss on K sampled pixels instead of on the whole image, which is shown to both improve accuracy and reduce GPU memory usage <ref type="bibr" target="#b2">[3]</ref>. However, in lesion segmentation, some tumors are very small compared to the whole 3D image. The importance sampling strategy <ref type="bibr" target="#b2">[3]</ref> can hardly select any foreground pixels in such cases, so the loss only contains background pixels, degrading the segmentation recall of small lesions. We propose a simple approach to remedy this issue by sampling an extra n foreground pixels for each lesion.</p><p>Patient Branch. A patient-level diagnosis is useful for triage. For example, diagnosing the subject as normal, benign, or malignant will result in completely different treatments <ref type="bibr" target="#b24">[24]</ref>. Intuitively, we can also infer patient-level labels from segmentation results by checking if there is any lesion in the predicted mask. However, certain tumors are often related to signs outside the tumor, e.g. hepatocellular carcinoma and cirrhosis, cholangiocarcinoma and bile duct dilatation, etc. We equip PLAN with a dedicated patient branch to aggregate such global information to make better patient-level prediction. Since one patient can have multiple liver tumors of different types, in our problem, we give each image several hierarchical binary labels. The first label classifies normal and tumor subjects (whether the image contains any tumor); The second and third labels indicate the existence of respectively benign and malignant tumors; The rest C labels suggest the existence of C fine-grained types of tumors. We employ the dual-path transformer block <ref type="bibr" target="#b16">[17]</ref> to fuse multi-scale features from the pixel encoder and decoder to generate a feature map, followed by global average pooling and a linear classification layer to predict the C + 3 labels.</p><p>A lesion-patient consistency loss is further proposed to encourage coherence of the lesion and patient-level predictions. Inspired by multi-instance learning <ref type="bibr" target="#b5">[6]</ref>, we compute a pseudo patient-level prediction c ∈ R C from the lesion-level predictions by max-pooling the class probability of each class across all lesion queries (discarding the no-object class). We also have the probability vector from the patient branch p ∈ R C corresponding to the C fine-grained classes. Then, we compute the L2 loss between them:</p><formula xml:id="formula_0">L consist = p -c 2 .</formula><p>The overall loss of PLAN is listed in Eq. 1, where L pixel is the combined crossentropy (CE) and Dice loss for the pixel branch as in nnU-Net <ref type="bibr" target="#b7">[8]</ref>; L lesion-class is the CE loss <ref type="bibr" target="#b2">[3]</ref> for lesion classification in the lesion branch; L lesion-mask is the combined CE and Dice loss <ref type="bibr" target="#b2">[3]</ref> for binary lesion segmentation in the lesion branch with the foreground-enhanced sampling strategy; L patient is the binary CE loss for the multi-label classification task in the patient branch.</p><formula xml:id="formula_1">L = λ 1 L pixel + λ 2c L lesion-class + λ 2m L lesion-mask + λ 3 L patient + λ 4 L consist . (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Data. Our dataset contains 810 normal subjects and 939 patients with liver tumors. Each normal subject has a non-contrast (NC) CT, while each patient has a dynamic contrast-enhanced (DCE) CT scan with NC, arterial, and venous phases. We use DEEDS <ref type="bibr" target="#b6">[7]</ref> to register NC and arterial phases to the venous phase, and then invite a senior radiologist with 10 years of experience to annotate on the multi-phase CTs using CT Labeler <ref type="bibr" target="#b15">[16]</ref>. The 3D mask and the type of all liver tumors are annotated based on pathological reports and magnetic resonance scans if necessary. Eight tumor types are considered in our study: hepatocellular carcinoma (HCC), intrahepatic cholangiocarcinoma (ICC), metastasis (meta), hepatoblastoma (hepato), hemangioma (heman), focal nodular hyperplasia (FNH), cyst, and others (all other tumor types). If a lesion's type cannot be determined according to image signs <ref type="bibr" target="#b10">[11]</ref> and pathology, it will be marked as "unknown" and ignored in training and evaluation. In total, 4010 tumor instances are annotated, whose volumes range from 11 to 3.7×10 6 mm 3 . Detailed statistics and examples of the lesions are shown in the supplementary material. We train two separate networks for NC and DCE CTs. In the former setting, both normal and patient data are used and randomly split into 1149 training, 100 validation, and 500 testing. In the latter one, only patient data are used with 641 training, 100 validation, and 200 testing. Another hold-out set of 150 patients and 100 normal CTs are used for reader study to compare our accuracy with two radiologists. Implementation Details. Each CT is resampled to 0.7×0.7×5mm in spacing. We first train an nnU-Net on public datasets to segment liver and surrounding organs (gallbladder, hepatic vein, spleen, stomach, and pancreas), and then crop the liver region to train PLAN. To help PLAN differentiate liver tumors and other organs, we train the network to segment both tumors and organs using the predicted organ labels. PLAN is built on top of the nnU-Net framework <ref type="bibr" target="#b7">[8]</ref>. Its pixel encoder is a U-Net encoder, whereas its pixel decoder is a light-weight feature pyramid network <ref type="bibr" target="#b2">[3]</ref>. The lesion branch incorporates three transformer decoder blocks with masked attention <ref type="bibr" target="#b2">[3]</ref> which use feature maps of strides 16, 8, 4 from the pixel decoder. The number of random queries is Q = 20; the embedding dimension is M = 64; the number of sampled pixels is K = 12544 <ref type="bibr" target="#b2">[3]</ref>, foreground pixels n = 3; the loss weight is 0.1 for the no-object class while 1 for other classes in the lesion branch <ref type="bibr" target="#b2">[3]</ref>. The weights in Eq. 1 are</p><formula xml:id="formula_2">λ 1 = λ 2c = 2, λ 2m = 5, λ 3 = 1, λ 4 = 0.1.</formula><p>We use the RAdam optimizer with an initial learning rate of 0.0001. Each training batch contains two patches of size 256 × 256 × 24. For DCE CT, the three phases form a 3-channel image as the network input. Extensive data augmentation is applied including random cropping, scaling, flipping, elastic deformation, and brightness adjustment <ref type="bibr" target="#b7">[8]</ref>.</p><p>During training, we first pretrain the backbone and the pixel branch for 500 epochs, and then train the whole network for another 500 epochs.</p><p>Patient-Level Results. This paper has three major goals: tumor screening in NC CT (classifying a subject as normal or tumor), preliminary diagnosis in NC CT (predicting the existence of malignant and benign tumors), and fine-grained diagnosis in DCE CT (predicting the existence of 8 tumor types). Among the 8 tumor types, HCC, ICC, meta, and hepato are malignant; heman, FNH, and cyst are benign. "Others" can be either malignant or benign, thus are excluded in the preliminary diagnosis task. The NC test set contains 198 tumor cases, 202 completely normal cases, and 100 "hard" non-tumor cases which may have larger image noise, artifact, ascites, diffuse liver diseases such as hepatitis and steatosis. These cases are used to test the robustness of the model in real-world screening scenario with diverse tumor-free images. We compare PLAN with a widely-used strong baseline, nnU-Net <ref type="bibr" target="#b7">[8]</ref>. The recent mask transformer, Mask2Former <ref type="bibr" target="#b2">[3]</ref>, is also adapted to 3D for comparison. For the baselines, patient-level labels are inferred from their predicted masks by counting lesion pixels. As displayed in Table <ref type="table" target="#tab_0">1</ref>, PLAN achieves the best accuracy on all tasks, especially in NC preliminary diagnosis tasks, which demonstrates the effectiveness of its dedicated patient branch that can explicitly aggregate features from the whole image.</p><p>Lesion and Pixel-Level Results. In lesion-level evaluation, we treat a prediction as a true positive if its overlap with a ground-truth lesion is &gt;0.2 in Dice.  Lesions smaller than 3 mm in radius are ignored. As shown in Table <ref type="table" target="#tab_1">2</ref>, the pixellevel accuracy of nnU-Net and PLAN are comparable, but PLAN's lesion-level accuracy is consistently higher than nnU-Net. In this work, we focus more on patient and lesion-level metrics. Although NC images have low contrast, they can still be used to segment and classify lesions with ∼ 80% precision, recall, and classification accuracy. It implies the potential of NC CT, which has been understudied in previous works. Mask2Former has higher precision but lower recall in NC CT, especially for small lesions, while PLAN achieves the best recall using the foreground-enhanced sampling loss. Both PLAN and Mask2Former achieve better classification accuracy, which illustrates the mask transformer architecture is good at lesion-level classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Radiologists.</head><p>In the reader study, we invited a senior radiologist with 16 years of experience in liver imaging, and a junior radiologist with 2 years of experience. They first read the NC CT of all subjects and provided a diagnosis of normal, benign, or malignant. Then, they read the DCE scans and provided a diagnosis of the 8 tumor types. We consider patients with only one tumor type in this study. Their reading process is without time constraint. In Table <ref type="table" target="#tab_2">3</ref> and Fig. <ref type="figure" target="#fig_1">2</ref>, all methods get good specificity probably because the normal subjects are completely healthy. Our model achieves comparable accuracy with the senior radiologist but outperforms the junior one by a large margin in sensitivity and classification accuracy. An ablation study for our method is shown in Table <ref type="table" target="#tab_3">4</ref>. It can be seen that our proposed anchor queries produced by the pixel branch, FES loss, and lesionpatient consistency loss are useful for the final performance. The efficacy of the lesion and patient branches has been analyzed above based on the lesion and patient-level results. Due to space limit, we will show the accuracy for each tumor type and more qualitative examples in the supplementary material.</p><p>Comparison with Literature. In the pixel level, we obtain Dice scores of 77.2% and 84.2% using NC and DCE CTs, respectively. The current state of the art (SOTA) of LiTS <ref type="bibr" target="#b0">[1]</ref> achieved 82.2% in Dice using CTs in venous phase; <ref type="bibr" target="#b23">[23]</ref> achieved 81.3% in Dice using DCE CT of two phases. In the lesion level, our precision and recall are 80.1% and 81.9% for NC CT, 92.2% and 89.0% for DCE CT, at 20% overlap. <ref type="bibr" target="#b25">[25]</ref> achieved 83% and 93% for DCE CT. SOTA of LiTS achieved 49.7% and 46.3% at 50% overlap. <ref type="bibr" target="#b20">[21]</ref> classified lesions into 5 classes, achieving 84% accuracy for DCE and 49% for NC CT. We classify lesions into 8 classes with 85.9% accuracy for DCE and 78.5% for NC CT. In the patient level, <ref type="bibr" target="#b4">[5]</ref> achieved AUC=0.75 in NC CT tumor screening, while our AUC is 0.985. In summary, our results are superior or comparable to existing works. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Framework of the Pixel-Lesion-pAtient Network (PLAN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. ROC curve of our method versus 2 radiologists' performance.</figDesc><graphic coords="8,240,33,207,74,145,09,107,77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Patient-level performance on the test set of 500 cases. Spec. 1: specificity on the 202 completely normal cases; Spec. 2: specificity on the 100 hard non-tumor cases.</figDesc><table><row><cell></cell><cell cols="5">NC tumor screening (%) NC diagnosis AUC DCE diagnosis AUC</cell></row><row><cell></cell><cell cols="2">Sens. Spec. 1 Spec. 2</cell><cell cols="3">Malignant Benign 8-class Average</cell></row><row><cell>nnU-Net [8]</cell><cell>94.4 95.1</cell><cell>91.0</cell><cell>0.948</cell><cell>0.829</cell><cell>0.863</cell></row><row><cell cols="2">Mask2Former [3] 93.9 97.0</cell><cell>94.0</cell><cell>0.924</cell><cell>0.828</cell><cell>0.873</cell></row><row><cell>PLAN (ours)</cell><cell>95.0 97.5</cell><cell>94.0</cell><cell>0.961</cell><cell cols="2">0.968 0.898</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Lesion-level performance (precision, recall, recall of lesions with different radius, classification accuracy of 8 tumor types), and pixel-level performance (Dice per case). Precision, recall, and Dice are computed without considering the tumor types.Prec. Recall R&lt;5 mm 5∼10 10∼20 &gt;20 mm Acc. Dice</figDesc><table><row><cell>NC nnU-Net</cell><cell>78.8 77.3</cell><cell>19.7</cell><cell>63.6 90.1</cell><cell>96.5</cell><cell>75.7 78.3</cell></row><row><cell cols="2">Mask2Former 85.7 74.0</cell><cell>10.0</cell><cell cols="2">60.5 91.9 97.4</cell><cell>77.9 76.4</cell></row><row><cell>PLAN</cell><cell cols="2">80.1 81.9 21.9</cell><cell>64.6 90.1</cell><cell>98.3</cell><cell>78.5 77.2</cell></row><row><cell>DCE nnU-Net</cell><cell>88.1 88.3</cell><cell>22.5</cell><cell>76.4 93.7</cell><cell>98.3</cell><cell>83.1 84.2</cell></row><row><cell cols="2">Mask2Former 90.3 83.5</cell><cell>11.7</cell><cell cols="2">74.4 94.6 97.4</cell><cell>84.8 82.9</cell></row><row><cell>PLAN</cell><cell cols="2">92.2 89.0 25.6</cell><cell cols="2">74.9 94.6 98.3</cell><cell>85.9 84.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Reader study results on 150 tumor cases and 100 normal cases. 3-class acc. means classification accuracy of normal vs. benign vs. malignant.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on NC data. FES loss: foreground enhanced sampling loss. screening (%) Prelim. diagnosis AUC Lesion and pixel-level (%) Three tasks are investigated in this paper: liver tumor screening and preliminary diagnosis in NC CT, and the diagnosis of 8 tumor types in DCE CT. The pixellesion-patient network is proposed that can accomplish lesion-level segmentation and classification, and patient-level classification. Comprehensive evaluation on a large-scale dataset confirms the effectiveness and clinical significance of our method. It can serve as a powerful tool for automated screening and diagnosis of various liver tumors. Our future work includes further improving the specificity of hard non-tumor cases and sensitivity of small lesions.</figDesc><table><row><cell></cell><cell>Tumor Sens. Spec.</cell><cell cols="2">Malignant Benign</cell><cell cols="3">Precision Recall Acc. Dice</cell></row><row><cell>PLAN (proposed)</cell><cell>95.0 96.4</cell><cell>96.1</cell><cell>96.8</cell><cell>80.1</cell><cell>81.9</cell><cell>78.5 77.2</cell></row><row><cell>w/o anchor queries</cell><cell>94.4 95.4</cell><cell>94.9</cell><cell>93.5</cell><cell>78.9</cell><cell>78.1</cell><cell>77.1 75.0</cell></row><row><cell>w/o FES loss</cell><cell>93.4 96.0</cell><cell>94.0</cell><cell>96.4</cell><cell>86.6</cell><cell>75.1</cell><cell>77.7 77.2</cell></row><row><cell cols="2">w/o consistency loss 93.9 96.7</cell><cell>95.4</cell><cell>96.3</cell><cell>79.1</cell><cell>80.7</cell><cell>78.2 76.6</cell></row><row><cell cols="2">4 Conclusion</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>Partially supported by the <rs type="funder">National Natural Science Foundation of China</rs> (grant <rs type="grantNumber">82071885</rs>), <rs type="funder">Basic Research Projects of Liaoning Provincial Department of Education</rs> (<rs type="grantNumber">LJKMZ20221160</rs>), the <rs type="funder">National Youth Talent Support Program of China</rs>, and <rs type="funder">Science and Technology Innovation Talent Project in Shenyang</rs> (<rs type="grantNumber">RC210265</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CH5qmSv">
					<idno type="grant-number">82071885</idno>
				</org>
				<org type="funding" xml:id="_Mx9CgaU">
					<idno type="grant-number">LJKMZ20221160</idno>
				</org>
				<org type="funding" xml:id="_WXgfk2C">
					<idno type="grant-number">RC210265</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43904-9 8.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">The Liver Tumor Segmentation Benchmark (LiTS). Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58452-8_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58452-813" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12346</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Masked-attention mask transformer for universal image segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1280" to="1289" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Per-Pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="17864" to="17875" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A flexible threedimensional hetero-phase computed tomography hepatocellular carcinoma ( HCC ) detection algorithm for generalizable and practical HCC screening</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hepatol. Commun</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Not-so-supervised: a survey of semisupervised, multi-instance, and transfer learning in medical image analysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cheplygina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="280" to="296" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MRF-based deformable registration and ventilation estimation of lung CT</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1239" to="1248" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">H-DenseUNet: hybrid densely connected UNet for liver and tumor segmentation from CT volumes</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2663" to="2674" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DAB-DETR : dynamic anchor boxes are better queries for DETR</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
		<respStmt>
			<orgName>ICLR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Americal college of gastroenterology: ACG clinical guideline: the diagnosis and management of focal liver lesions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Marrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rajender Reddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Gastroenterol</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1328" to="1347" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incidentally detected focal liver lesions-a common clinical management dilemma revisited</title>
		<author>
			<persName><forename type="first">A</forename><surname>Semaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anticancer Res</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2923" to="2932" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modified U-Net (mU-Net) with incorporation of object-dependent high level features for improved liver and liver-tumor segmentation in CT images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bassenne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1316" to="1325" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Global cancer statistics 2020 : GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA: Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="249" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">E 2 Net: an edge enhanced network for accurate liver and tumor segmentation on CT scans</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_50</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59719-150" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="512" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A Cascaded Approach for Ultraly High Performance Lesion Detection and False Positive Removal in Liver CT Scans</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2306.16036" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">MaX-DeepLab : end-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5463" to="5474" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective pancreatic cancer screening on non-contrast CT scans via anatomy-aware transformers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_25</idno>
		<idno>978-3-030-87240-3 25</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="259" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A knowledge-guided framework for fine-grained classification of liver lesions based on multi-phase CT images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="386" to="396" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective opportunistic esophageal cancer screening using noncontrast CT imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-833" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="344" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning with convolutional neural network for differentiation of liver masses at dynamic contrast-enhanced CT: a preliminary study</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yasaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiryu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="887" to="896" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">K-means mask transformer</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2022</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13689</biblScope>
			<biblScope unit="page" from="288" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19818-2_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19818-217" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modality-aware mutual learning for multi-modal medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-256" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Cotin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Essert</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="589" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">3D graph anatomy geometry-integrated network for pancreatic mass segmentation, diagnosis, and quantitative patient management</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13738" to="13747" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic detection and classification of focal liver lesions based on deep convolutional neural networks: a preliminary study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Oncol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
