<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LOTUS: Learning to Optimize Task-Based US Representations</title>
				<funder ref="#_QemTKtC">
					<orgName type="full">Qatar National Research Fund</orgName>
				</funder>
				<funder>
					<orgName type="full">Qatar Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yordanka</forename><surname>Velikova</surname></persName>
							<email>dani.velikova@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">Farid</forename><surname>Azampour</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Sharif University of Technology</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Walter</forename><surname>Simson</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">Stanford University School of Medicine</orgName>
								<address>
									<settlement>Stanford</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vanessa</forename><surname>Gonzalez Duque</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory" key="lab1">LS2N Laboratory at Ecole Centrale Nantes</orgName>
								<orgName type="laboratory" key="lab2">UMR CNRS 6004</orgName>
								<address>
									<settlement>Nantes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Computer Aided Medical Procedures</orgName>
								<orgName type="institution">John Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LOTUS: Learning to Optimize Task-Based US Representations</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="435" to="445"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">721A79530E1DBC7627043F8DA0FA5774</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_42</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ultrasound</term>
					<term>Unsupervised Domain Adaptation</term>
					<term>Segmentation</term>
					<term>Task Driven</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anatomical segmentation of organs in ultrasound images is essential to many clinical applications, particularly for diagnosis and monitoring. Existing deep neural networks require a large amount of labeled data for training in order to achieve clinically acceptable performance. Yet, in ultrasound, due to characteristic properties such as speckle and clutter, it is challenging to obtain accurate segmentation boundaries, and precise pixel-wise labeling of images is highly dependent on the expertise of physicians. In contrast, CT scans have higher resolution and improved contrast, easing organ identification. In this paper, we propose a novel approach for learning to optimize task-based ultrasound image representations. Given annotated CT segmentation maps as a simulation medium, we model acoustic propagation through tissue via ray-casting to generate ultrasound training data. Our ultrasound simulator is fully differentiable and learns to optimize the parameters for generating physics-based ultrasound images guided by the downstream segmentation task. In addition, we train an image adaptation network between real and simulated images to achieve simultaneous image synthesis and automatic segmentation on US images in an end-to-end training setting. The proposed method is evaluated on aorta and vessel segmentation tasks and shows promising quantitative results. Furthermore, we also conduct qualitative results of optimized image representations on other organs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ultrasound (US) imaging is a widely used modality in medical diagnosis for screening and follow-up examinations. Hence, precise segmentation of the target organs is crucial for diagnosing or tracking disease progression. Recently, the application of deep learning for ultrasound image segmentation has emerged as a powerful tool. However, accurate segmentation of US images remains a challenging task due to the complexity of the modality, as it has limited resolution and often contains clutter, shadowing and reverberation artefacts. This leads to a general lack of annotated data, and additionally, due to varying operator skills, there is high heterogeneity of ground truth data labels, which is the primary factor hampering solid segmentation performance <ref type="bibr" target="#b5">[6]</ref>.</p><p>On the other hand, large pixel-level labeled CT datasets are freely available online. Thus to overcome the lack of ground truth ultrasound data, researchers have utilized ultrasound simulators to generate large sets of ultrasound-like images from CT label maps and use them for training <ref type="bibr" target="#b9">[10]</ref>. Simulated ultrasound data automatically provides a labeled pair of the tissue distribution and the resulting b-mode image and can be augmented with rotational, brightness, contrast, probe, and scanner variations.</p><p>Generally, ultrasound simulators can be categorized into two types based on their modeling techniques: finite difference models of the wave equation, modeling the mechanical propagation of sound waves through tissues, and simulating ray casting through tissue maps represented by ultrasound tissue properties <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11]</ref>. Although the former can model higher-order non-linear effects, producing realistic images, generating a single image can take hours. The latter, on the other hand, is much faster and can be integrated into other systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. While leveraging automatically generated ultrasound simulations with corresponding labels for training has benefits, models trained on simulations fail when applied directly to real, as they cannot perfectly simulate ultrasound images without distinguishable differences from real ones.</p><p>Thus, one main challenge when working with simulated data is reducing the domain shift between simulated and real data. In a supervised sense, many works have investigated the realistic parametrization of ultrasound simulators to reduce the domain shift between simulated and real data ultrasound data <ref type="bibr" target="#b11">[12]</ref> and augmentation of ultrasound b-modes <ref type="bibr" target="#b12">[13]</ref>. Recent domain adaptation models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref> employing generative adversarial networks have shown promise in improving image synthesis in an unsupervised manner. Moreover, recent works show their application in combination with segmentation or registration tasks between Xray and CT or MRI scans <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. Further works show their application in ultrasound by closing the real-simulation gap via translation from simulated images to "realistic" ones that match the target domain, thereby enabling the application of trained segmentation networks on real images <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>However, those methods require separate training for each part of the architecture, limiting the models' flexibility. Notably, <ref type="bibr" target="#b14">[15]</ref> proposes using an intermediate representation image with common properties between CT and US for the task of aorta segmentation. However, the intermediate image is not formulated differentiably but is statically calibrated, and the whole pipeline is not trained end-to-end.</p><p>Contributions. In this paper, we propose a novel approach for learning to optimize task-based ultrasound image representations. During training, we render an intermediate US image representation from segmented public CT scans and use it as input to a segmentation network. Our ultrasound renderer is fully differentiable and learns to optimize the parameters necessary for physics-based ultrasound simulation, guided by the downstream segmentation task. At the same time, we train an image style transfer network between real and simulated data to achieve simultaneous image synthesis as well as automatic segmentation on US images in an end-to-end training setting. In addition, no labels are required for the real ultrasound images, which are also unpaired with the simulated ultrasound images. We evaluate our method on aorta and vessel segmentation. Our quantitative and qualitative results demonstrate that our method learns the optimal image for the task of interest. The source code for our method is publicly available<ref type="foot" target="#foot_0">1</ref> .  Building on the mathematical foundations of ray tracing and ultrasound echo generation proposed by <ref type="bibr" target="#b1">[2]</ref>, we adopt those equations and modify them to be differentiable while still accurately depicting the physics behind generating US B-mode images. Input to the renderer is a 2D label map with tissue labels. Each tissue label has assigned five parameters with default values<ref type="foot" target="#foot_1">2</ref> which describe ultrasound-specific tissue characteristics and control the whole rendering generation -attenuation coefficient α, acoustic impedance Z, as well as three parameters that define the speckle distributionμ 0 , μ 1 , σ 0 . For each 2D label map, we use these parameters to define three sub-maps: attenuation, reflection, and scatter maps. We generate those maps by modeling ultrasound waves as rays starting from the transducer, which is the top of the label map, and propagating through media using physical laws. Ray casting is simulated by defining a function E i (d) for each scanline i at a distance d from the transducer, which describes the recorded ultrasound echo signal as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Differentiable Ultrasound Renderer</head><formula xml:id="formula_0">E i (d) = R i (d) + B i (d)<label>(1)</label></formula><p>where R i (d) is the energy reflected from the interfaces between two tissues as the beam passes through them and B i (d) represents the energy backscattered from the scattering points along the scanline. The reflection of the ray is described as:</p><formula xml:id="formula_1">R i (d) = |I i (d) * Z i (d)| * P (d) ⊗ G(d) (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where I i (d) is the remaining energy of the ray, which gets attenuated during tissue traversal. We model I i (d) by approximating the Beer-Lambert Law as:</p><formula xml:id="formula_3">I i (d) = e -αd</formula><p>, where α is the attenuation coefficient of the medium and d the distance travelled. To construct the final 2D attenuation map, we calculate, for each ray, the cumulative product of the attenuation as it traverses through various tissues, thereby modeling how the signal's strength diminishes. The reflection coefficient Z = (Z 2 -Z 1 ) 2 /(Z 2 +Z 1 ) 2 , is computed from the acoustic impedances of two adjacent tissues: Z 1 and Z 2 . The P (d) is the Point Spread Function (PSF) along the ray, and G(d) is a boundary map, where 1 is assigned for points on the boundary of the surface and 0 otherwise. For simplicity, we model the PSF as a two-dimensional normalized Gaussian. The amount of the reflected signal, denoted by φ r , equals the result of multiplying the reflection coefficient by the boundary condition. To build our final 2D reflection map, for each ray, we compute the cumulative product of the residual signal, defined as 1φ r . The output represents the fraction of the signal that propagates forward.</p><p>In additionally to the reflection term, a backscattered energy term B i (d) in the returning echo is calculated:</p><formula xml:id="formula_4">B i (d) = I i (d) * P (d) ⊗ T (x, y)<label>(3)</label></formula><p>the residual ultrasound wave energy I i (d) is multiplied with the PSF P (d), which has been convolved with a texture T of random scatterers for each (x, y), where:</p><formula xml:id="formula_5">T (x, y) = S(x, y) if T 1 (x, y) ≤ μ 1 (x, y) 0 otherwise (4) S(x, y) = T 0 (x, y) * σ 0 + μ 0<label>(5)</label></formula><p>This texture is constructed using two random textures T 0 (x, y) and T 1 (x, y) with Gaussian normalized distributions and the parameters μ 0 , μ 1 , and σ 0 , which represent the brightness, density and standard deviation of scatterers respectively.</p><p>To make the function fully differentiable, we replace the conditional operation T 1 ≤ μ 1 with a differentiable approximation:</p><formula xml:id="formula_6">T (x, y) = σ(β • (μ 1 (x, y) -T 1 (x, y))) • S(x, y)<label>(6)</label></formula><p>where σ(z) = 1 1+e -z is the sigmoid function and β is a scaling factor that adjusts its steepness. The resulting function is fully differentiable as the sigmoid function smoothly approximates the step function and all operations involved are differentiable. Additionally, we apply temporal gain compensation (TGC) to enhance tissues deeper in the image. The final rendered ultrasound image is constructed from the three sub-maps (see Fig. <ref type="figure" target="#fig_1">2</ref>) and additionally warped to produce the desired fan shape. At the beginning of the training, we set the default tissuespecific values, which during the training, get changed, guided from the downstream task, and generate optimal US simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">End-to-End Learning</head><p>The proposed method's architecture is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. During training, our method follows two main paths: Real → Reconstructed US and CT label map → Segmentation. We explain the meaning of these paths in the order shown in the figure . 
Real → Reconstructed US. Since there is an appearance gap between real and our rendered ultrasound images we incorporate an unpaired and unsupervised image-to-image translation network, CUT <ref type="bibr" target="#b7">[8]</ref>, which uses a contrastive learning scheme. Given a source image, the Generator learns a function G : X → Y that translates the corresponding image into the target's appearance. We have two domains of unpaired instances: real US images as the source X and rendered US as the target Y. The generator's encoder G enc extracts relevant content characteristics, while the decoder G dec learns to create the desired goal appearance. The Generator network employs an adversarial loss:</p><formula xml:id="formula_7">L GAN = E y log D(y) + E x log(1 -D(G(x))) (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>where the generated images G(x) resemble images from domain Y, and D(.) differentiates between translated and real images y. However, the adversarial loss alone does not ensure that the translated image will preserve the structure of the anatomy. An additional contrastive loss must be imposed, which maximizes mutual information across corresponding image patches from the source and the output image. We use the Patch Sampler from CUT to extract image patches and calculate the contrastive NCE (L NCE ) loss <ref type="bibr" target="#b7">[8]</ref>. The final loss is defined as:</p><formula xml:id="formula_9">L CUT (X, Y ) = L GAN (X, Y ) + L NCE (X, G(X)) + L NCE (Y, G(Y ))<label>(8)</label></formula><p>where, the L NCE is calculated on two pairs, a sample from the source domain (x) paired with the generated output G(x) and a sample from the target domain (y) paired with the G(y) which we denote as the identity image. The loss over the second pair serves as an identity loss and prevents the generator from making unnecessary changes to the image.</p><p>CT Labelmap → Segmentation: The segmentation network forward pass has a nested structure. First, we obtain a 2D slice from the CT label map and pass it to the differentiable ultrasound renderer. The resulting rendered US is passed through the frozen Generator network, and the identity image output of the Generator is used as an input to the segmentation network to ensure the same distribution as the target domain. We update both the segmentation network and the Renderer using dice loss. The label for computing the dice loss comes directly from the input label map used for generating the rendered US.</p><p>Stopping Criterion: Once the segmentation network validation loss converges, we employ a small subset of 10 labeled images from the real US domain as a stopping indicator for the entire training pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>CT Dataset: We use 12 CT volumes from a publicly available dataset Synapse<ref type="foot" target="#foot_2">3</ref>  <ref type="bibr" target="#b6">[7]</ref> for training. The data comes with labels for multiple organs. These labels were additionally augmented with labels of bones, fat, skin, and lungs using TotalSegmentor <ref type="bibr" target="#b16">[17]</ref> to complete the label maps.</p><p>In-vivo Images: We acquired abdominal ultrasound sweeps from eleven volunteers of age 26 ± 3 (m:7/f:4). For each person, one sweep was acquired with a convex probe <ref type="foot" target="#foot_3">4</ref> . Per sweep, 50 frames were randomly sampled and used for training the CUT network. To compare against a supervised approach, additional images were annotated (500 for the aorta, 400 for vessels) from all volunteers to train 5-fold cross-validation. From each set of annotated images, 100 images were randomly sampled as test sets for both segmentation tasks.</p><p>Training Details: We train the network with a learning rate of 10 -5 for the segmentation network, 10 -3 for the US Renderer, and 5 -6 for the image adaptation network, with a batch size of 1, Adam optimizer and dice loss. We employ rotation, translation, and scaling augmentations on the CT label maps and split them randomly in an 80-20% ratio for training and validation, respectively. For the supervised approach, we trained the networks, for 120 epochs, with a learning rate of 10 -3 and the Adam optimizer. Experiments. We test the proposed framework quantitatively for two segmentation tasks: all vessels and the aorta only. We evaluate the accuracy of the proposed method by comparing it to a supervised network. For this, we train a 5-fold cross-validation U-Net <ref type="bibr" target="#b8">[9]</ref>, test on three hold-out subjects, and report the average DSC. We also compare to a fixed rendered image by freezing the US renderer instead of optimizing it. Additionally, we show qualitative results of the proposed method when the downstream tasks are changed Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>In Table <ref type="table" target="#tab_0">1</ref>, we compare the performance of LOTUS against a fully supervised approach and against a frozen renderer's parameters and report the DSC and Hausdorff distance (HD) in mm for aorta segmentation and DSC only for all vessels segmentation. Our proposed method achieved the highest DSC score of 89.24 ± 0.13 and the lowest HD score of 2.52 ± 1.18 mm for aorta segmentation.</p><p>For the task of vessels segmentation it also achieved the best DSC of 90.9 ± 0.06. Figure <ref type="figure" target="#fig_2">3</ref> depicts the images obtained during the optimization of the proposed method for different target organs. The upper row shows the rendered US image with default parameters, and the bottom row displays the optimized image representations for the corresponding target organ learned during optimization. It can be observed that the spine, kidney and liver appear brighter, while for vessel and aorta segmentation, the vessels darken and the background becomes uniformly homogeneous. This highlights the ability of the proposed method to learn optimal representations for each downstream task.</p><p>The results presented in this work demonstrate the effectiveness of LOTUS for segmenting organs in ultrasound images. Our physics-based simulator generates synthetic training data, which is especially useful in scenarios where obtaining labeled data is time-consuming or costly. We believe that learning from transferred labels from CT contributes to a more accurate model since CT data is more accessible and labels are more refined. Our quantitative results indicate that LOTUS can achieve accurate segmentation of aorta boundaries and other vessels. Furthermore, the end-to-end framework enables the differentiable US renderer and the unsupervised image translation to get optimized dynamically during the training. Thus, the intermediate representation image is not static but changes during the training. This illustrates the adaptivity of the proposed method to the downstream task, highlighting its prospective applicability across diverse applications and anatomies.</p><p>Moreover, rather than directly using the rendered US image as an input to the segmentation network, we use the identity image output from the Generator. This yielded significant improvement in the segmentation result as it learns from a distribution consistent with the reconstructed US while looking similar to the rendered US. As a result, during inference stage, the distribution of the translated real US is closer to the distribution the segmentation network was trained on, thereby improving the performance of the model.</p><p>One of the challenges when employing generative adversarial networks is that the loss is not an indicator of the best result. We determine the optimal model by utilizing a small subset of labeled images after the convergence of the segmentation network, to ensure robustness during inference. Further stopping criteria can be studied to achieve higher automation of the pipeline.</p><p>Currently, our model incorporates the basic physics of ultrasound imaging without considering artifacts explicitly. Thus, exploring the robustness of the method against artifacts could yield valuable future improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents a novel approach to learning task-based ultrasound image representations. LOTUS leverages CT labelmaps to simulate ultrasound data via differentiable ray-casting. The proposed ultrasound simulator is fully differentiable and learns to optimize the parameters for generating physics-based ultrasound images guided by the downstream segmentation task. We also introduce an image adaptation network to achieve simultaneous image synthesis and automatic segmentation on US images in an end-to-end training setting without needing paired real and simulated images. Our method is evaluated on aorta and vessel segmentation tasks and shows promising quantitative results. Furthermore, we demonstrate the potential of our approach for other organs through qualitative results of optimized image representations. The ability to learn from unlabeled data and simulate the ultrasound modality has the potential for various clinical tasks beyond segmentation. We believe that our work has the potential to improve ultrasound imaging interpretation and learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed framework. During training, we render online US simulation images from CT label maps and use them as input to a segmentation network. Our ultrasound renderer is fully differentiable and learns to optimize the parameters based on the downstream segmentation task. At the same time, we train an unpaired and unsupervised image style transfer network between real and rendered images to achieve simultaneous image synthesis as well as automatic segmentation on US images in an end-to-end training setting.</figDesc><graphic coords="3,55,98,298,25,340,24,139,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the Differentiable Ultrasound Renderer.</figDesc><graphic coords="4,41,79,117,47,340,12,106,36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Image representations of segmentation tasks for different target organs, learned during the optimization. Top row: rendered US with default parameters, bottom row: final optimized rendered US for each specific organ. From left to right: spine, kidney, liver, vessels, aorta only.</figDesc><graphic coords="7,61,98,308,36,328,48,116,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of DSC and Hausdorff distance for the task of aorta and vessels segmentation of our proposed method with supervised network and with frozen renderer.</figDesc><table><row><cell></cell><cell cols="2">Supervised Frozen Renderer LOTUS</cell></row><row><cell>DSC -Aorta</cell><cell>80.65 ± 2.35 84.67 ± 0.14</cell><cell>89.24 ± 0.13</cell></row><row><cell cols="2">Hausdorff (mm) 17.61 ± 1.32 11.08 ± 18.64</cell><cell>2.52 ± 1.18</cell></row><row><cell>DSC -Vessel</cell><cell>83.56 ± 4.16 89.05 ± 0.09</cell><cell>90.9 ± 0.06</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/danivelikova/lotus.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/Blito/burgercpp/blob/master/examples/ircad11/liver.scene.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://www.synapse.org/#!Synapse:syn3193805/wiki/89480.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>cQuest Cicada US scanner, Cephasonics, Santa Clara, CA, US.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We would like to thank <rs type="person">Magdalena Wysocki</rs> for the insightful discussions and <rs type="person">Dr. Magdalini Paschali</rs> for helping with refining and improving the manuscript. The authors were partially supported by the grant <rs type="grantNumber">NPRP-11S-1219-170106</rs> from the <rs type="funder">Qatar National Research Fund</rs> (a member of the <rs type="funder">Qatar Foundation</rs>). The findings herein are however solely the responsibility of the authors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QemTKtC">
					<idno type="grant-number">NPRP-11S-1219-170106</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_42.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reverberation noise suppression in ultrasound channel signals using a 3D fully convolutional neural network</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Brickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jakovljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1184" to="1195" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time GPU-based ultrasound simulation using deformable mesh models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bettinghausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Radle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hesser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="609" to="618" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised cross-modality domain adaptation of convnets for biomedical image segmentations with adversarial loss</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Beamforming and speckle reduction using neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Brickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Looby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ultrason. Ferroelectr. Freq. Control</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="910" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new approach to calculating spatial impulse responses</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Ultrasonics Symposium Proceedings. An International Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997. 1997</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1755" to="1759" />
		</imprint>
	</monogr>
	<note>Cat. No. 97CH36118</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tracked 3D ultrasound and deep neural network-based thyroid segmentation reduce interobserver variability in thyroid volumetry</title>
		<author>
			<persName><forename type="first">M</forename><surname>Krönke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">268550</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MICCAI multi-atlas labeling beyond the cranial vault-workshop and challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Igelsias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Langerak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</title>
		<meeting>MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Contrastive learning for unpaired image-to-image translation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comparison of real-time ultrasound simulation models using abdominal CT images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Vera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larrabide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>D'amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Larrabide</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2255741</idno>
		<ptr target="https://doi.org/10.1117/12.2255741" />
	</analytic>
	<monogr>
		<title level="m">12th International Symposium on Medical Information Processing and Analysis</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Romero</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lepore</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Brieva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Brieva</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">L</forename></persName>
		</editor>
		<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10160</biblScope>
			<biblScope unit="page">1016009</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Patient-specific 3D ultrasound simulation based on convolutional ray-tracing and appearance optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prevost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24571-3_61</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24571-3_61" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9350</biblScope>
			<biblScope unit="page" from="510" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Investigating pulse-echo sound speed estimation in breast ultrasound with deep learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Simson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sideri-Lampretsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.03064</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rethinking ultrasound augmentation: a physics-inspired approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tirindelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eilers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Simson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paschali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Azampour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_66</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-3_66" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="690" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Content-preserving unpaired translation from simulated to realistic ultrasound images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Portenier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Goksel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="659" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cactuss: common anatomical CT-us space for us examinations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Velikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Simson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Azampour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Paprottka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="492" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving realism in patientspecific abdominal ultrasound simulation using cycleGANs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vitale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Orlando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Iarussi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Larrabide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="183" to="192" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Totalsegmentator: Robust segmentation of 104 anatomic structures in CT images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wasserthal</surname></persName>
		</author>
		<idno type="DOI">10.1148/ryai.230024</idno>
		<ptr target="https://doi.org/10.1148/ryai.230024" />
	</analytic>
	<monogr>
		<title level="j">Radiology: Artificial Intell</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page">230024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Task driven generative modeling for unsupervised domain adaptation: application to X-ray image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="599" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networkss</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
