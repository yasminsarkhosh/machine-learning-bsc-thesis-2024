<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UOD: Universal One-Shot Detection of Anatomical Landmarks</title>
				<funder ref="#_t4GMCdU">
					<orgName type="full">Open Fund Project of Guangdong Academy of Medical Sciences, China</orgName>
				</funder>
				<funder ref="#_2qmGGSn">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Heqin</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Division of Life Sciences and Medicine</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei, Anhui</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Suzhou Institute for Advanced Research</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>215123</postCode>
									<settlement>Suzhou, Jiangsu</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quan</forename><surname>Quan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingsong</forename><surname>Yao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zaiyi</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Radiology</orgName>
								<orgName type="department" key="dep2">Guangdong Provincial People&apos;s Hospital</orgName>
								<orgName type="institution">Guangdong Academy of Medical Sciences</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Guangdong Provincial People&apos;s Hospital</orgName>
								<orgName type="department" key="dep2">Guangdong Academy of Medical Sciences</orgName>
								<orgName type="laboratory">Guangdong Provincial Key Laboratory of Artificial Intelligence in Medical Image Analysis and Application</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">Kevin</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">Division of Life Sciences and Medicine</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei, Anhui</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Suzhou Institute for Advanced Research</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>215123</postCode>
									<settlement>Suzhou, Jiangsu</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UOD: Universal One-Shot Detection of Anatomical Landmarks</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="24" to="34"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">BFD828C7F3378BF21325BA81453202CA</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>One-shot learning</term>
					<term>Domain-adaptive model</term>
					<term>Anatomical landmark detection</term>
					<term>Transformer network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One-shot medical landmark detection gains much attention and achieves great success for its label-efficient training process. However, existing one-shot learning methods are highly specialized in a single domain and suffer domain preference heavily in the situation of multidomain unlabeled data. Moreover, one-shot learning is not robust that it faces performance drop when annotating a sub-optimal image. To tackle these issues, we resort to developing a domain-adaptive one-shot landmark detection framework for handling multi-domain medical images, named Universal One-shot Detection (UOD). UOD consists of two stages and two corresponding universal models which are designed as combinations of domain-specific modules and domain-shared modules. In the first stage, a domain-adaptive convolution model is self-supervised learned to generate pseudo landmark labels. In the second stage, we design a domain-adaptive transformer to eliminate domain preference and build the global context for multi-domain data. Even though only one annotated sample from each domain is available for training, the domainshared modules help UOD aggregate all one-shot samples to detect more robust and accurate landmarks. We investigated both qualitatively and quantitatively the proposed UOD on three widely-used public X-ray datasets in different anatomical domains (i.e., head, hand, chest) and obtained state-of-the-art performances in each domain. The code is at https://github.com/heqin-zhu/UOD_universal_oneshot_detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Robust and accurate detecting of anatomical landmarks is an essential task in medical image applications <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25]</ref>, which plays vital parts in varieties of clinical treatments, for instance, vertebrae localization <ref type="bibr" target="#b19">[20]</ref>, orthognathic and orthodontic surgeries <ref type="bibr" target="#b8">[9]</ref>, and craniofacial anomalies assessment <ref type="bibr" target="#b3">[4]</ref>. Moreover, anatomical landmarks exert their effectiveness in other medical image tasks such as segmentation <ref type="bibr" target="#b2">[3]</ref>, registration <ref type="bibr" target="#b4">[5]</ref>, and biometry estimation <ref type="bibr" target="#b0">[1]</ref>.</p><p>In the past years, lots of fully supervised methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27</ref>] have been proposed to detect landmarks accurately and automatically. To relieve the burden of experts and reduce the amount of annotated labels, various oneshot and few-shot methods have been come up with. Zhao et al. <ref type="bibr" target="#b23">[23]</ref> demonstrate a model which learns transformations from the images and uses the labeled example to synthesize additional labeled examples, where each transformation is composed of a spatial deformation field and an intensity change. Yao et al. <ref type="bibr" target="#b21">[22]</ref> develop a cascaded self-supervised learning framework for one-shot medical landmark detection. They first train a matching network to calculate the cosine similarity between features from an image and a template patch, then fine-tune the pseudo landmark labels from coarse to fine. Browatzki et al. <ref type="bibr" target="#b1">[2]</ref> propose a semisupervised method that consists of two stages. They first employ an adversarial auto-encoder to learn implicit face knowledge from unlabeled images and then fine-tune the decoder to detect landmarks with few-shot labels.</p><p>However, one-shot methods are not robust enough because they are dependent on the choice of labeled template and the accuracy of detected landmarks may decrease a lot when choosing a sub-optimal image to annotate. To address this issue, Quan et al. <ref type="bibr" target="#b11">[12]</ref> propose a novel Sample Choosing Policy (SCP) to select the most worthy image to annotate. Despite the improved performance, SCP brings an extra computation burden. Another challenge is the scalability of model building when facing multiple domains (such as different anatomical regions). While conventional wisdom is to independently train a model for each domain, Zhu et al. <ref type="bibr" target="#b26">[26]</ref> propose a universal model YOLO for detecting landmarks across different anatomies and achieving better performances than a collection of single models. YOLO is regularly supervised using the CNN as backbone and it is unknown if the YOLO model works for a one-shot scenario and with a modern transformer architecture.</p><p>Motivated by above challenges, to detect robust multi-domain label-efficient landmarks, we design domain-adaptive models and propose a universal oneshot landmark detection framework called Universal One-shot Detection (UOD), illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. A universal model is comprised of domain-specific modules and domain-shared modules, learning the specified features of each domain and common features of all domains to eliminate domain preference and extract representative features for multi-domain data. Moreover, one-shot learning is not robust enough because of the sample selection while multi-domain oneshot learning reaps benefit from different one-shot samples from various domains, in which cross-domain features are excavated by domain-shared modules. Our proposed UOD framework consists of two stages: 1) Contrastive learning for In summary, our contributions can be categorized into three parts: 1) We design the first universal framework for multi-domain one-shot landmark detection, which improves detecting accuracy and relieves domain preference on multidomain data from various anatomical regions. 2) We design a domain-adaptive transformer block (DATB), which is effective for multi-domain learning and can be used in any other transformer network. 3) We carry out comprehensive experiments to demonstrate the effectiveness of UOD for obtaining SOTA performance on three publicly used X-ray datasets of head, hand, and chest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>As Fig. <ref type="figure" target="#fig_0">1</ref> shows, UOD consists of two stages: 1) Contrastive learning and 2) Supervised learning. In stage I, to learn the local appearance of each domain, a universal model is trained via self-supervised learning, which contains domainspecific VGG <ref type="bibr" target="#b14">[15]</ref> and UNet <ref type="bibr" target="#b12">[13]</ref> decoder with each standard convolution replaced by a domain adaptor <ref type="bibr" target="#b6">[7]</ref>. In stage II, to grasp the global constraint and eliminate domain preference, we designed a domain-adaptive transformer (DATR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Stage I: Contrastive Learning</head><p>As Fig. <ref type="figure" target="#fig_0">1</ref> shows, following Yao et al. <ref type="bibr" target="#b21">[22]</ref>, we employ contrastive learning to train siamese network for matching similar patches of original image and augmented image. Given a multi-domain input image X d ∈ R H d ×W d ×C d belongs to domain d from multi-domain data, we randomly select a target point P and crop a halfsize patch X d p which contains P . After applying data augmentation on X d p , the target point is mapped to P p . Then we feed X d and X d p into the siamese network respectively and obtain the multi-scale feature embeddings. We compute cosine similarity of two feature embeddings from each scale and apply softmax to the cosine similarity map to generate a probability matrix. Finally, we calculate the cross entropy loss of the probability matrix and ground truth map which is produced with the one-hot encoding of P d p to optimize the siamese network for learning the latent similarities of patches. At inferring stage, we replace augmented patch X d p with the augmented one-shot sample patch X d s . We use the annotated one-shot landmarks as target points to formulate the ground truth maps. After obtaining probability matrices, we apply arg max to extract the strongest response points as the pseudo landmarks, which will be used in UOD Stage II.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stage II: Supervised Learning</head><p>In stage II, we design a universal transformer to capture global relationship of multi-domain data and train it with the pseudo landmarks generated in stage I. The universal transformer has a domain-adaptive transformer encoder and domain-adaptive convolution decoder. The decoder is based on a U-Net <ref type="bibr" target="#b12">[13]</ref> decoder with each standard convolution replaced by a domain adaptor <ref type="bibr" target="#b6">[7]</ref>. The encoder is based on Swin Transformer <ref type="bibr" target="#b9">[10]</ref> with shifted window and limited self-attention within non-overlapping local windows for computation efficiency. Different from Swin Transformer <ref type="bibr" target="#b9">[10]</ref>, we design a domain-adaptive transformer block (DATB) and use it to replace the original transformer block. Domain-Adaptive Transformer Encoder. As Fig. <ref type="figure" target="#fig_1">2(a)</ref> shows, the transformer encoder is built up with DATB, making full use of the capability of transformer for modeling global relationship and extracting multi-domain representative features. As in Fig. <ref type="figure" target="#fig_1">2</ref>(b), a basic transformer block <ref type="bibr" target="#b16">[17]</ref> consists of a multi-head self-attention module (MSA), followed by a two-layer MLP with GELU activation. Furthermore, layer normalization (LN) is adopted before each MSA and MLP and a residual connection is adopted after each MSA and MLP. Given a feature map x d ∈ R h×w×c from domain d with height h, width w, and c channels, the output feature maps of MSA and MLP, denoted by ŷd and y d , respectively, are formulated as: where MSA = softmax(QK T )V .</p><formula xml:id="formula_0">ŷd = MSA(LN(x d )) + x d y d = MLP(LN(ŷ d )) + ŷd<label>(1)</label></formula><p>As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>(b)(c), DATB is based on Eq. ( <ref type="formula" target="#formula_0">1</ref>). Similar to U2Net <ref type="bibr" target="#b6">[7]</ref> and GU2Net <ref type="bibr" target="#b26">[26]</ref>, we adopt domain-specific and domain-shared parameters in DATB. Since the attention probability is dependent on query and key matrix which are symmetrical, we duplicate the query matrix for each domain to learn domain-specific query features and keep key and value matrix domain-shared to learn common knowledge and reduce parameters. Inspired by LayerScale <ref type="bibr" target="#b15">[16]</ref>, we further adopt learnable diagonal matrix <ref type="bibr" target="#b15">[16]</ref> after each MSA and MLP module to facilitate the learning of domain-specific features, which costs few parameters (O(N ) for N × N diagonal). Different from LayerScale <ref type="bibr" target="#b15">[16]</ref>, proposed domainadaptive diagonal D d 1 and D d 2 are applied for each domain with D d 2 applied after residual connection for generating more representative and direct domain-specific features. The above process can be formulated as:</p><formula xml:id="formula_1">ŷd = D d 1 × MSA Q d (LN(x d )) + x d y d = D d 2 × (MLP(LN(ŷ d )) + ŷd )<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">MSA Q d = softmax(Q d K T )V .</formula><p>Overall Pipeline. Given that a random input </p><formula xml:id="formula_3">X d ∈ R H d ×W d ×C</formula><formula xml:id="formula_4">Ỹ d n ∈ R H d ×W d ×C d with Gaussian function to be Ỹ d n = 1 √ 2πσ e -(i-i d n ) 2 +(j-j d n ) 2 2σ 2 if (i -i d n ) 2 + (j -j d n )</formula><p>2 ≤ σ and 0 otherwise. We further add an exponential weight to the Gaussian distribution to distinguish close heatmap pixels and obtain the ground truth heatmap Y d n (i, j) = α Ỹ d n (i,j) . As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, firstly, the input image from a random batch is partitioned into non-overlapping patches and linearly embedded. Next, these patches are fed into cascaded transformer blocks at each stage, which are merged except in the last stage. Finally, a domain-adaptive convolution decoder makes dense prediction to generate heatmaps, which is further used to extract landmarks via threshold processing and connected components filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>Datasets. For performance evaluation, we adopt three public X-ray datasets from different domains on various anatomical regions of head, hand, and chest. (i) Head dataset is a widely-used dataset for IEEE ISBI 2015 challenge <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> which contains 400 X-ray cephalometric images with 150 images for training and 250 images for testing. Each image is of size 2400 × 1935 with a resolution of 0.1 mm × 0.1 mm, which contains 19 landmarks manually labeled by two medical experts and we use the average labels same as Payer et al. <ref type="bibr" target="#b10">[11]</ref>. (ii) Hand dataset is collected by <ref type="bibr" target="#b5">[6]</ref> which contains 909 X-ray images and 37 landmarks annotated by <ref type="bibr" target="#b10">[11]</ref>. We follow <ref type="bibr" target="#b26">[26]</ref> to split this dataset into a training set of 609 images and a test set of 300 images. Following <ref type="bibr" target="#b10">[11]</ref> we assume the distance between two endpoints of wrist is 50 mm and calculate the physical distance as distance physical = distance pixel × 50 p-q 2 where p, q are the two endpoints of the wrist respectively. (iii) Chest dataset <ref type="bibr" target="#b26">[26]</ref> is a popular chest radiography database collected by Japanese Society of Radiological Technology (JSRT) <ref type="bibr" target="#b13">[14]</ref> which contains 247 images. Each image is of size 2048 × 2048 with a resolution of 0.175 mm × 0.175 mm. We split it into a training set of 197 images and a test set of 50 images and select 6 landmarks from landmark labels at the boundary of the lung as target landmarks.</p><p>Implementation Details. UOD is implemented in Pytorch and trained on a TITAN RTX GPU with CUDA version being 11. All encoders are initialized with corresponding pre-trained weights. We set batch size to 8, σ to 3, and α to 10. We adopt binary cross-entropy (BCE) as loss function for both stages. In stage I, we resize each image to the same shape of 384 × 384 and train universal convolution model by Adam optimizer for 1000 epochs with a learning rate of 0.00001. In stage II, we resize each image to the same shape of 576 × 576 and optimize the universal transformer by Adam optimizer for 300 epochs with a learning rate of 0.0001. When calculating metrics, all predicted landmarks are resized back to the original size. For evaluation, we choose model with minimum validation loss as the inference model and adopt two metrics: mean radial error (MRE) 2 and successful detection rates (SDR) within different thresholds t:  </p><formula xml:id="formula_5">MRE = 1 N N i (x i -xi ) 2 + (y i -ỹi )</formula><formula xml:id="formula_6">SDR(t) = 1 N N i δ( (x i -xi ) 2 + (y i -ỹi ) 2 ≤ t).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Results</head><p>The Effectiveness of Universal Model: To demonstrate the effectiveness of universal model for multi-domain one-shot learning, we adopt head and hand datasets for evaluation. In stage I, the convolution models are trained in two ways: 1) single: trained on every single dataset respectively, and 2) universal: trained on mixed datasets together. With a fixed one-shot sample for the hand dataset, we change the one-shot sample for the head dataset and report the MRE and SDR of the head dataset. As Fig. <ref type="figure" target="#fig_2">3</ref> shows, universal model performs much better than single model on various one-shot samples and metrics. It is proved that universal model learns domain-shared knowledge and promotes domainspecific learning. Furthermore, the MRE and SDR metrics of universal model have a smaller gap among various one-shot samples, which demonstrates the robustness of universal model learned on multi-domain data.</p><p>Comparisons with State-of-the-Art Methods: As Table <ref type="table" target="#tab_1">1</ref> shows, we compare UOD with two open-source landmark detection methods, i.e., YOLO <ref type="bibr" target="#b26">[26]</ref> and CC2D <ref type="bibr" target="#b21">[22]</ref>. YOLO is a multi-domain supervised method while CC2D is a single-domain one-shot method. UOD achieves SOTA results on all datasets under all metrics, outperforming the other one-shot method by a big margin. On the head dataset, benefiting from multi-domain learning, UOD achieves an MRE of 2.43 mm and an SDR of 86.49% within 4 mm, which is comparative with supervised method YOLO trained with at least 10 annotated labels, and much  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>To improve the robustness and reduce domain preference of multi-domain oneshot learning, we design a universal framework in that we first train a universal model via contrastive learning to generate pseudo landmarks and further use these labels to learn a universal transformer for accurate and robust detection of landmarks. UOD is the first universal framework of one-shot landmark detection on multi-domain data, which outperforms other one-shot methods on three public datasets from different anatomical regions. We believe UOD will significantly reduce the labeling burden and pave the path of developing more universal framework for multi-domain one-shot learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of UOD framework. In stage I, two universal models are learned via contrastive learning for matching similar patches from original image and augmented one-shot sample image and generating pseudo labels. In stage II, DATR is designed to better capture global context information among all domains for detecting more accurate landmarks.</figDesc><graphic coords="3,110,04,56,72,233,11,189,91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) The architecture of DATR in stage II, which is composed of domainadaptive transformer encoder and convolution adaptors [7]. (b) Basic transformer block. (c) Domain-adaptive transformer block. Each domain-adaptive transformer is a basic transformer block with query matrix duplicated and domain-adaptive diagonal for each domain. The batch-normalization, activation, and patch merging are omitted.</figDesc><graphic coords="5,62,91,53,87,286,48,144,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of single model and universal model on head dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Qualitative comparison of UOD and CC2D [22] on head, hand, and chest datasets. The red points • indicate predicted landmarks while the green points • indicate ground truth landmarks. The MRE value is displayed in the top left corner of the image. (Color figure online)</figDesc><graphic coords="8,57,48,178,85,337,48,65,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison of UOD with SOTA methods on head, hand, and chest datasets. * denotes the method is trained on every single dataset respectively while †denotes the method is trained on mixed data.</figDesc><table><row><cell>Method</cell><cell cols="3">Label Head [19]</cell><cell></cell><cell>Hand [6]</cell><cell>Chest [14]</cell></row><row><cell></cell><cell></cell><cell cols="2">MRE↓ SDR↑ (%)</cell><cell></cell><cell>MRE↓ SDR↑ (%)</cell><cell>MRE↓ SDR↑ (%)</cell></row><row><cell></cell><cell></cell><cell cols="5">(mm) 2 mm 2.5 mm 3 mm 4 mm ( mm) 2 mm 4 mm 10 mm (mm) 2 mm 4 mm 10 mm</cell></row><row><cell cols="2">YOLO [26] † all</cell><cell>1.32</cell><cell>81.14 87.85</cell><cell>92.12 96.80</cell><cell>0.85 94.93 99.14 99.67</cell><cell>4.65 31.00 69.00 93.67</cell></row><row><cell cols="2">YOLO [26] † 25</cell><cell>1.96</cell><cell>62.05 77.68</cell><cell>88.21 97.11</cell><cell>2.88 72.71 92.32 97.65</cell><cell>7.03 19.33 51.67 89.33</cell></row><row><cell cols="2">YOLO [26] † 10</cell><cell>2.69</cell><cell>47.58 66.47</cell><cell>78.42 90.89</cell><cell cols="2">9.70 48.66 76.69 90.52 16.07 11.67 33.67 76.33</cell></row><row><cell cols="2">YOLO [26] † 5</cell><cell>5.40</cell><cell>26.16 41.32</cell><cell cols="3">54.42 73.74 24.35 20.59 48.91 72.94 34.81</cell><cell>4.33 19.00 56.67</cell></row><row><cell cols="2">CC2D [22]* 1</cell><cell>2.76</cell><cell>42.36 51.82</cell><cell>64.02 78.96</cell><cell cols="2">2.65 51.19 82.56 95.62 10.25 11.37 35.73 68.14</cell></row><row><cell>Ours †</cell><cell>1</cell><cell>2.43</cell><cell cols="4">51.14 62.37 74.40 86.49 2.52 53.37 84.27 97.59 8.49 14.00 39.33 76.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of different components of our DATR. Base is the basic transformer block; MSA Q d denotes the domain-adaptive self-attention and D d denotes the domain-adaptive diagonal matrix. In each column, the best results are in bold. +MSA Q d +D d 2.43 51.14 62.37 74.40 86.49 2.52 53.37 84.27 97.59 8.49 14.00 39.33 76.33</figDesc><table><row><cell>Transformer</cell><cell cols="2">Head [19]</cell><cell></cell><cell></cell><cell cols="2">Hand [6]</cell><cell cols="2">Chest [14]</cell><cell></cell></row><row><cell></cell><cell cols="3">MRE↓ SDR↑ (%)</cell><cell></cell><cell cols="2">MRE↓ SDR↑ (%)</cell><cell cols="3">MRE↓ SDR↑ (%)</cell></row><row><cell></cell><cell cols="10">(mm) 2 mm 2.5 mm 3 mm 4 mm (mm) 2 mm 4 mm 10 mm (mm) 2 mm 4 mm 10 mm</cell></row><row><cell>(a) Base</cell><cell>24.95</cell><cell>2.02</cell><cell>3.17</cell><cell>4.51</cell><cell>5.85 9.83</cell><cell cols="2">5.33 16.79 58.64 58.11</cell><cell>0.37</cell><cell>1.96</cell><cell>3.85</cell></row><row><cell>(b) +D d</cell><cell>22.75</cell><cell>2.13</cell><cell>3.24</cell><cell>4.61</cell><cell>6.96 7.52</cell><cell cols="2">6.13 20.66 68.43 52.98</cell><cell>0.59</cell><cell>2.17</cell><cell>4.68</cell></row><row><cell>(c) +MSA Q d</cell><cell cols="3">2.51 49.29 60.89</cell><cell cols="2">72.17 84.36 2.72</cell><cell>48.56 80.44 94.38</cell><cell cols="4">9.09 12.00 19.33 74.00</cell></row><row><cell>(d)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. Supported by <rs type="funder">Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">62271465</rs> and <rs type="funder">Open Fund Project of Guangdong Academy of Medical Sciences, China</rs> (No. <rs type="grantNumber">YKY-KF202206</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2qmGGSn">
					<idno type="grant-number">62271465</idno>
				</org>
				<org type="funding" xml:id="_t4GMCdU">
					<idno type="grant-number">YKY-KF202206</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BiometryNet: landmark-based fetal biometry estimation from standard ultrasound planes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Avisdris</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16440-8_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16440-8_27" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13434</biblScope>
			<biblScope unit="page" from="279" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3fabrec: fast few-shot face alignment by reconstruction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Browatzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wallraven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6110" to="6120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automated brain structures segmentation from PET/CT images based on landmark-constrained dual-modality atlas registration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">95003</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph convolutional network with probabilistic spatial regression: Application to craniofacial landmark detection from 3D photogrammetry</title>
		<author>
			<persName><forename type="first">C</forename><surname>Elkhill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lebeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Porras</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_55</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_55" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="574" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using multiple images and contours for deformable 3D-2D registration of a preoperative ct in laparoscopic liver surgery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Espinel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Calvet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Botros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tilmant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_63</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-1_63" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="657" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Gertych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sayre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pospiech-Kurkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bone age assessment of children using a digital hand atlas</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="322" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3D U 2 -Net: a 3D universal U-net for multi-domain medical image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32245-8_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32245-8_33" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11765</biblScope>
			<biblScope unit="page" from="291" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CephalFormer: incorporating global structure constraint into visual features for general cephalometric landmark detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_22</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-8_22" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="227" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DentalPointNet: landmark localization on high-resolution 3d digital dental models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_43</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-7_43" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="444" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Integrating spatial configuration into heatmap regression based CNNs for landmark localization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Payer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Štern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="207" to="219" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Which images to label for few-shot medical landmark detection?</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="20606" to="20616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Development of a digital image database for chest radiographs with and without a lung nodule: receiver operating characteristic analysis of radiologists&apos; detection of pulmonary nodules</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shiraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Roentgenol</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="74" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evaluation and comparison of anatomical landmark detection methods for cephalometric x-ray images: a grand challenge</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1890" to="1900" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A benchmark for comparison of dental radiography analysis algorithms</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="63" to="76" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accurate scoliosis vertebral landmark localization on x-ray images via shape-constrained multi-stage cascaded CNNs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundam. Res</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Miss the point: targeted adversarial attack on multiple landmark detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59719-1_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59719-1_67" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12264</biblScope>
			<biblScope unit="page" from="692" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">One-shot medical landmark detection</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="177" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-3_17" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data augmentation using learned transformations for one-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8543" to="8553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review of deep learning in medical imaging: imaging traits, technology trends, case studies with progress highlights, and future promises</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Handbook of Medical Image Computing and Computer Assisted Intervention</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Academic Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">You only learn once: universal anatomical landmark detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_9</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_9" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="85" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to localize cross-anatomy landmarks in x-ray images with a universal model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BME Front</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
