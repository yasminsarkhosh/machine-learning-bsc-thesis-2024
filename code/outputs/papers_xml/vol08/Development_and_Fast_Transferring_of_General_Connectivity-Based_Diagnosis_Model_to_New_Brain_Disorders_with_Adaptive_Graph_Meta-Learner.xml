<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner</title>
				<funder ref="#_pRKmJSA">
					<orgName type="full">The Key R&amp;D Program of Guangdong Province, China</orgName>
				</funder>
				<funder ref="#_WexrtsH">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_UHhHEWf">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality</orgName>
					<orgName type="abbreviated">STCSM</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mianxin</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai Artificial Intelligence Laboratory</orgName>
								<address>
									<postCode>200232</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuanwang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<email>dgshen@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai United Imaging Intelligence Co., Ltd</orgName>
								<address>
									<postCode>200232</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Shanghai Clinical Research and Trial Center</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Development and Fast Transferring of General Connectivity-Based Diagnosis Model to New Brain Disorders with Adaptive Graph Meta-Learner</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">19BB0484FA4FD53EE423CE13659B86D8</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_10</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Brain disorders</term>
					<term>Graph convolutional network</term>
					<term>Brain functional network</term>
					<term>Few-shot adaptation</term>
					<term>Meta-learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The accurate and automatic diagnosis of new brain disorders (BDs) is crucial in the clinical stage. However, previous deep learning based methods require training new models with large data from new BDs, which is often not practical. Recent neuroscience studies suggested that BDs could share commonness from the perspective of functional connectivity derived from fMRI. This potentially enables developing a connectivity-based general model that can be transferred to new BDs to address the difficulty of training new models under data limitations. In this work, we demonstrate this possibility by employing the meta-learning algorithm to develop a general adaptive graph meta-learner and transfer it to new BDs. Specifically, we use an adaptive multi-view graph classifier to select the appropriate view for specific disease classification and a reinforcement-learning-based meta-controller to alleviate the over-fitting when adapting to new datasets with small sizes. Experiments on 4,114 fMRI data from multiple datasets covering a broad range of BDs demonstrate the effectiveness of modules in our framework and the advantages over other comparison methods. This work may pave the basis for fMRI-based deep learning models being widely used in clinical applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Brain disorders (BDs) pose severe challenges to public mental health in the global world. To understand the pathology, and for accurate diagnosis as well, functional MRI (fMRI), one of the MRI modalities, is widely studied for BDs. The fMRI provides assessments of the disease-induced changes in the brain functional connectivity networks (FCNs) among different brain regions of interest (ROIs). And a huge body of studies has successfully built effective classifiers for different BDs based on FCN and deep learning methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. However, when facing new BDs, it is often needed to train a new classification model, which requires collections of large clinical data. During this process, the high costs in time, money, and labor prevent collecting sufficient data and thus the applications of deep learning models to new BDs with a small number of samples, especially for some rare BDs. Recently, there has been study <ref type="bibr" target="#b23">[24]</ref> illustrating that BDs share significant commonness under the perspective of FCN alternations. Based on this knowledge, developing and transferring a general model to new BDs could be possible, which is promising to address the issues of building new classifiers for new BDs under data limitation.</p><p>Meta-learning based algorithm is one of the advanced methods to develop a general model based on heterogeneous information from data in different domains or for different tasks. It aims to learn optimal initial parameters for the model (meta-learner) which can be quickly generalized to new tasks, directly or with a few new training data for fine-tuning. There have been extensive discussions in the literature on developing more general models utilizing meta-learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> in medical fields.</p><p>Upon this evidence, it would be promising to develop a general BD diagnosis model based on FCN and further use meta-learning to solve the above-mentioned issues. However, there are still at least two challenges to achieve this goal. First, how to optimally extract the generalizable common knowledge (features) from the procedure of diagnosing various BDs? Previous methods focused on conventional FCNs (computed using simple linear correlations) and only treated FCN as a vector <ref type="bibr" target="#b11">[12]</ref>. This manner of analysis neither properly explores topological features in the FCN, nor fully characterizes the complex, high-order functional interactions among brain regions, which are demonstrated to be associated with BDs <ref type="bibr" target="#b1">[2]</ref>. Second, after developing a general model, how to optimally adapt the model to new datasets with different conditions? The best tuning of the model may exist only within a critical range of parameters, but previous studies blindly search for optimal parameters using ad hoc manual configurations, such as the adaption step size, which is easy to cause over-fitting and degrading the performance on small datasets. Theoretically, it would be beneficial to let the model adaptively configure the adaption step size and other parameters, according to the given dataset.</p><p>In this paper, we develop a novel framework (illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>) to explore the aforementioned issues. First, we assemble a large amount of data from both public datasets and in-house datasets (i.e., a total of 6 datasets, 4,114 subjects) to develop a general BD diagnosis model with meta-learning. Second, during the meta-learning procedure, we propose an adaptive multi-view graph classifier to mine topological information of low-and high-order FCNs, as different views of brain dynamics for classification. The attention mechanism is implemented to dynamically weigh and fuse different views of the information under given tasks, which collaborate with meta-learning and helps the model to learn to adapt to diagnoses for different BDs. Third, we apply a meta-controller driven by reinforcement learning <ref type="bibr" target="#b24">[25]</ref> to choose the optimal adaption step size for the general model, for properly adapting to new BDs with small datasets and alleviating the over-fitting issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation and Problem Formulation</head><p>We define the entire set of BD diagnosis tasks using different datasets with T . For a specific task T j ∈ T (j = 1, 2, ..., 6), we have n pairs of FCN and labeled data</p><formula xml:id="formula_0">D = {(F i , Y i )} n i=1</formula><p>, where F is the fMRI data and Y is the label set of all subjects. ROIs are defined in individual fMRI spaces based on the brain atlas <ref type="bibr" target="#b19">[20]</ref> to extract ROI-based fMRI signals.</p><p>We extract three features from individual fMRI data for the graph deep learning analysis. First, we compute the low-order FCN A low , where A low ∈ R NROI×NROI is a graph adjacency matrix containing the pair-wise correlations among different ROI signals. Second, we compute the high-order FCN A high based on the topological information in A low (described in detail in Sect. 2.3). These two features will be used as edge features. Third, we use the corresponding order adjacency matrix, together with the mean and standard deviation value of ROI signals, as the node features for both lowand high-order graphs <ref type="bibr" target="#b13">[14]</ref>.</p><p>We have meta-training and meta-testing stages for our meta-learner. The corresponding tasks are named as meta-training task T train and meta-testing task T test . The meta-training stage mimics cross-task adaptations, aiming to make the model learning to adapt to new task T test with a small number of samples under initial parameters. To simulate the cross-task scenario, we utilized the episodic training mechanism <ref type="bibr" target="#b12">[13]</ref>, which samples small sets across all meta-training datasets. In detail, for each round the in meta-training stage, we randomly sample the non-overlapping support data</p><formula xml:id="formula_1">D train sup = F train i , Y train i s i=1 and query data D train que = F train i , Y train i q</formula><p>i=1 across all meta-training datasets, based on which, the T train is constructed. With constructed T train , we have two loops to update the initial parameters of the meta-learner, which are inner and outer loops. During inner loops, initial parameters update gradients are first estimated using the back-propagation learned from the D train sup . Then, if initial parameters are updated by the first gradient, we further estimate the gradients based on D train que as the outer loop to finally update them. The two loops will be combined to update the initial meta-learner parameters for better fine-tuning on new tasks as detailed in Sect. 2.2. At the meta-testing stage, we adapt the initialized parameters from the meta-training stage to the new T test with a few data and test its performance. We first fine-tune the meta-learner on</p><formula xml:id="formula_2">D test sup = {(F test i , Y test i )} s i=1</formula><p>, and then report classification performance on</p><formula xml:id="formula_3">D test que = {(F test i , Y test i )} q i=1</formula><p>. Our target is to make the accuracy on D test que as high as possible when the size of the support data s is only a small portion of T test . If s = K, we denote the setting as K-shot classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Meta-Learner Training Algorithm</head><p>Our meta-learner consists of two modules, which are 1) multi-view classifier as detailed in Sect. 2.3 and 2) meta-controller as detailed in Sect. 2.4. The pseudo-codes for the meta-learner training algorithm are given in Algorithm 1. During the meta-training stage, our meta-learner algorithm has two iterative parameter update loops, which are inner (lines 3-9) and outer loops (lines 10-14) <ref type="bibr" target="#b4">[5]</ref>. The inner loop aims to fast adapt the multi-view graph classifier parameterized with θ c to the new sampled T train under given initial parameters (θ 0 c , θ m ). During the inner loop, the meta-controller parameterized with θ m decides whether to stop at the adaption step t to avoid over-fitting. While the outer loop aims to improve the generality of the meta-learner by exploring the optimal initial parameters (θ 0 c , θ m ) according to the loss on D train que which can easily adapt to other tasks. The initial parameter updated in the outer loop will be set for the next sampled T train . At the meta-testing stage, the meta-learner will utilize the meta-training stage initialized (θ 0 c , θ m ), fixing θ m and fine-tuning θ 0 c on D test sup , finally predicting the label D test que .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-view Graph Classifier θ c</head><p>As mentioned above, exploring the topological information of the FCN is necessary for BD diagnosis. However, a single low-order FCN view can only illustrate the simple pair-wise relation. So, we additionally construct the high-order FCN, which reflects the correlation among ROIs in terms of their own FC patterns, as calculated below:</p><formula xml:id="formula_4">A high ij = Pearson Correlation(A low i * , A low j * ).<label>(1)</label></formula><p>After constructing high-order adjacency matrix A high as a complementary view, we input them into the multi-view graph classifier parameterized with θ t c at the adaption step t. To extract the disease-related features of different-view graphs, we use convolution in GCN <ref type="bibr" target="#b10">[11]</ref> to aggregate the neighboring node features.</p><p>Then, we use the pooling operation to further extract the global topological features of different graph views. Here, we opt for gPOOL operation <ref type="bibr" target="#b6">[7]</ref> for its parameter-saving and ability to extract global topological features. For each pooling stage, it acquires the importance of each node by calculating the inner product between the node feature vectors and a learnable vector. The top-k important nodes and their corresponding subgraph will be sampled for the following graph convolutions. By iteratively repeating the node feature aggregation and pooling illustrated in Fig. <ref type="figure" target="#fig_0">1</ref> (3), we can finally acquire the representation of the graph, denoted as H. Furthermore, to let the model adaptively choose the proper view for classification, we apply an attention-based mechanism to aggregate the learned embeddings by feeding the concatenation of the learned representations into an attention module as below:</p><formula xml:id="formula_5">α = Softmax ReLU H low H high W 1 W 2 ).<label>(2)</label></formula><p>The attention mechanism allows the model to decide which view should rely on for specific tasks more adaptively. Then, we forward attention-weighted features into an MLP and acquire the final prediction label. Here, we use the cross-entropy loss as a constraint to the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Meta-Controller θ m</head><p>In machine learning, over-fitting is one of the critical problems that restrict the performance of models, especially in small datasets. Previous works utilize the early stopping to alleviate this problem, but the hand-crafted early stopping parameter is hard to choose. Here, we utilize a neural network to learn the stop policy adaptively, which we call meta-controller parameterized with θ m depicted in Fig. <ref type="figure" target="#fig_0">1 (4</ref>). The meta-controller uses a reinforcement learning based algorithm <ref type="bibr" target="#b16">[17]</ref> to decide optimal adaption step sizes for cross-task adaptions. Considering the characteristics of the graph data, we let the model determine when to stop not only according to the classification loss, but also the graph embedding quality on the support data. For the embedding quality, we use the Average Node Information (ANI), denoted as Q, to measure it. It represents how a node can be measured by the neighboring nodes. A high ANI value indicates that the embedding module has learned the most information about the graph. If we keep aggregating the nodal features by graph convolution when the ANI is high, the over-smoothing will happen, making all nodes have similar features and thus degrading the performance. We define the ANI Q sup of the support data within T train with the L1 norm <ref type="bibr" target="#b8">[9]</ref> as follows:</p><formula xml:id="formula_6">Q i = 1 N ROI j=1 I -D i -1 A i X L i j 1 , Q sup = 1/s × s i=1 Q high i + Q low i ,<label>(3)</label></formula><p>where D i X L i represent the degree matrix of A i and the feature matrix in the last layer L, respectively; j denotes j-th node which is also the j-th row in those matrices, and</p><p>• 1 denotes the L1-norm of row vector.</p><p>For classification losses L sup and ANI values Q sup on D train sup across t adaption steps, we use them to compute the stop probability p t at step t with an LSTM <ref type="bibr" target="#b7">[8]</ref> model by considering the temporal information as follows:</p><formula xml:id="formula_7">o t = LSTM Q t sup , L t sup , o t-1 , p t = σ W o t + b , (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where o t is the output of the LSTM model at step t and σ is the SoftMax function.</p><p>Finally, we sample the choices c t by Bernoulli distribution to decide whether we should stop at step t.</p><formula xml:id="formula_9">c t ∼ Bernouli(p t ) = 1, stop the adaption 0, keep adaption . (<label>5</label></formula><formula xml:id="formula_10">)</formula><p>Since the relation between θ m and θ c is undifferentiable, it is impossible to take direct gradient descent on θ m . We use stochastic policy gradient to optimize θ m . Once we sample the stop choice at step T , we train the meta-controller according to loss changes on D train que across T steps. Given the parameter update trajectory of classifier {θ 0 c , θ 1 c , ...θ T c } during T steps, we calculate the corresponding loss change trajectory of D train que . Based on that, we further define the controller immediate rewards r at step t as the loss change on D train que (caused by parameter update of step t):</p><formula xml:id="formula_11">r (t) = L D train que ; θ t-1 c -L D train que ; θ t c (6)</formula><p>Then, the accumulative reward R at step t is</p><formula xml:id="formula_12">R t = T i=t r i = L D train que ; θ t-1 c -L D train que ; θ T c , (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>where T is the total number of steps and R t is the change of classification loss on D train que from step t to the end of adaption. Then we update our meta-controller by policy gradients, which is a typical method in Reinforcement learning <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_14">θ m ← θ m + β 2 R t ∇ θm ln p t , (<label>8</label></formula><formula xml:id="formula_15">)</formula><p>where∇ θm is the gradients over θ m and β 2 is the learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>We use fMRI meta-training data from five datasets including Alzheimer's Disease Neuroimaging Initiative (ADNI) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>, Open Access Series of Imaging Studies (OASIS) <ref type="bibr" target="#b18">[19]</ref>, and in-house dataset from Huashan Hospital (elder BDs datasets); ADHD-200 <ref type="bibr" target="#b2">[3]</ref> and Autism Brain Imaging Data Exchange (ABIDE) <ref type="bibr" target="#b3">[4]</ref> (youth BD datasets). For the meta-testing dataset, we use the in-house dataset from Zhongshan Hospital which is about vascular cognitive impairment (VCI). All datasets are shown in Table <ref type="table" target="#tab_0">1</ref> The details of image acquisition parameters and processing procedures can be found in <ref type="bibr" target="#b15">[16]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Settings</head><p>To ensure a fair comparison, we use three graph convolutional layers, followed by corresponding pooling layers, for all GNN based methods. We use the ADAM optimizer with 1e-4 for learning rate and 5e-4 for weight decay, 100 for epochs, 0.001 for both β 1 and β 2 , respectively. For the inner loop fast adaption, we set the minimum and maximum steps by 4 and 16. In the meta-training stage, we sample D train sup and D train que from all training datasets; and, in the meta-testing stage, we only randomly sample D test sup and D test que from the meta-testing dataset. The size of support data for both meta-training and meta-testing stages is depicted in the first line of Table <ref type="table" target="#tab_2">2</ref>, and we set the size of the query data as 256 for two stages. For different datasets, we only diagnose whether they are BD or not. We randomly select the support and the query data five times to report the mean and standard deviation value of the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results and Discussions</head><p>In Table <ref type="table" target="#tab_2">2</ref>, we compare our method with two SOTA meta-learning based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref> (in lines 3 and 4) under different K-shot configurations. It can be observed that, when the size of the support data increases, the performances of all methods increase and our proposed method outperforms the SOTA methods in terms of all performance metrics. We also validate the effectiveness of the use of multiple-BD datasets for training our model. When compared with the model without the meta-training stage (line 5) or reducing either elder or youth BD datasets (lines 8 and 9), we find that the performances all drop significantly. This can validate that not only the information from MCI/AD, which is more similar to VCI, is useful, but also the general BD commonness suggested by data of youth BDs is beneficial. For the ablation study, we test the performance without a multi-view graph classifier or meta-controller as shown in Table <ref type="table" target="#tab_2">2</ref> (lines 6 and 7) and Fig. <ref type="figure" target="#fig_2">2 (a)</ref>.  Finally, we visualize the predictive importance of different resting-state networks, including visual network (VIS), somatomotor network (SM), dorsal attention network (DAN), salience network (SAL), limbic network (LIM), executive control network (ECN) and default mode network (DMN) when adapting to elder BD and younger BD as shown in Fig. <ref type="figure" target="#fig_2">2</ref> (d) by GradCAM <ref type="bibr" target="#b20">[21]</ref>. In the results, the LIM consistently shows importance when diagnosing elder and younger BDs, which is in line with previous neuroscience studies <ref type="bibr" target="#b23">[24]</ref>. This validates that our proposed method can properly detect meaningful common features among different BDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work, we focus on the issues of developing a classifier on new BD datasets with small samples and propose a novel framework. A broad of datasets covering elder and youth BDs are used to train the model to estimate the common features among BDs. An adaptive multi-view graph classifier is proposed to enable the model efficiently extract features for different BD diagnosis tasks. In addition, to avoid over-fitting during the adaptions to new data, we utilize a novel meta-controller driven by RL. Extensive experiments demonstrate the effectiveness and generalization of our proposed method. It is expected that advanced graph embedding methods can be integrated into our framework to improve performance. Our work is also promising to be extended to neuroscience studies to reveal both common and unique characteristics of different BDs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The framework of our proposed method. During the meta-training stage (1), we first tune the initial parameters θ 0 c for T steps on the support data D train sup as the inner loop when adapting to new meta-training tasks. At the step t, meta-controller (4) decides whether to stop based on graph embedding quality Q t sup and loss L t sup on the support data. If the controller decides to stop at step t, then we tune θm and θ 0 c as the outer loop based on loss reward R ∈ R t×1 and L t que separately on the query data D train que for the next inner loop. Finally, we fix θm, and fine-tune the meta-training stage initialized θ 0 c on D test sup in the meta-testing stage (2), predicting the labels on D test que .</figDesc><graphic coords="3,57,81,59,18,338,41,150,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2 (a) demonstrates that, when the epoch increases, the accuracy of the model assisted with both classifier and controller acquires steady and continuous improvement. In addition, we investigate the associations among ANI, support data accuracy and stop probability. As we can see from Figs. 2(b) and 2(c)), with the increment of ANI (red curve in Fig. 2(b)) and accuracy (green curve in Fig. 2(c)) on the support data, the stop probability also begins to increase (purple curves in Figs. 2(b) and 2(c)) to alleviate over-fitting in the query data, which supports the effectiveness of the meta-controller.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a) Accuracy curves; (b) ANI and stop probability curves; (c) Support data accuracy and stop probability curves; (d) Activation values of different subnetworks. (Color figure online)</figDesc><graphic coords="8,47,28,380,33,334,45,53,29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Meta-training algorithm.</figDesc><table><row><cell></cell><cell>Input: learning rates: β1, β2, random initialization (θ  *  c , θ  *  m ) Output: Trained parameters θ 0 c , θm .</cell></row><row><cell cols="2">1 θ 0 c , θm = (θ  *  c , θ  *  m )</cell></row><row><cell cols="2">2 while not done do</cell></row><row><cell>3</cell><cell>Sample T train with D train sup and D train que from meta-training datasets</cell></row><row><cell>4</cell><cell>while t ∈ [1, TMAX] do</cell></row><row><cell>5 6 7</cell><cell>Calculate ∇ θ t-1 c θ t c ← θ t-1 c -β1∇ θ t-1 L t sup on D train sup L t sup c Calculate Q t sup by Eq. 3 on D train sup</cell></row><row><cell>8</cell><cell>Calculate stop probability by Q t sup and L t sup and determine whether to stop at step</cell></row><row><cell></cell><cell>t via Eq. 4, 5</cell></row><row><cell>9</cell><cell>Calculate reward R t with loss changes on D train que via Eq. 7</cell></row><row><cell>10</cell><cell>end</cell></row><row><cell>11</cell><cell>θ 0 c ← θ 0 c -β1∇ θ 0 c L T que on D train que</cell></row><row><cell>12</cell><cell>for t = 0 : T do</cell></row></table><note><p>13 θm ← θm + β2R t ∇ θm ln p(t) 14 end 15 end</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Dataset for experiments</figDesc><table><row><cell cols="2">Dataset ADNI</cell><cell cols="5">OASIS ABIDE ADHD-200 Huashan Zhongshan</cell></row><row><cell cols="3">Disease MCI, AD AD</cell><cell cols="2">Autism ADHD</cell><cell cols="2">MCI, AD VCI</cell></row><row><cell>BD</cell><cell>785</cell><cell>83</cell><cell>499</cell><cell>280</cell><cell>100</cell><cell>151</cell></row><row><cell>HC</cell><cell>566</cell><cell>634</cell><cell>512</cell><cell>488</cell><cell>167</cell><cell>246</cell></row><row><cell>Total</cell><cell>1351</cell><cell>717</cell><cell>1011</cell><cell>768</cell><cell>267</cell><cell>397</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison of different methods, with bold denoting the highest performance. The proposed method shows statistically significant improvements (in level of p-value &lt; 0.05) over all compared methods in terms of all metrics.</figDesc><table><row><cell>1</cell><cell>10-shot</cell><cell></cell><cell></cell><cell></cell><cell>30-shot</cell><cell></cell><cell></cell><cell></cell><cell>50-shot</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 methods</cell><cell>Acc</cell><cell>Sen</cell><cell>Spe</cell><cell>AUC</cell><cell>Acc</cell><cell>Sen</cell><cell>Spe</cell><cell>AUC</cell><cell>Acc</cell><cell>Sen</cell><cell>Spe</cell><cell>AUC</cell></row><row><cell>3 Jaein</cell><cell cols="12">52.6±5.7 52.7±6.0 55.2±5.3 55.8±3.7 58.8±4.1 59.3±3.9 56.8±4.9 59.7±4.7 63.3±4.9 65.2±5.2 61.3±4.1 63.9±5.0</cell></row><row><cell>4 Ning</cell><cell cols="12">54.3±5.1 53.1±5.5 56.7±6.0 55.4±4.3 60.7±4.3 61.7±5.7 60.3±6.1 61.4±4.3 65.1±5.1 64.7±4.7 66.3±6.1 65.7±5.5</cell></row><row><cell>5 Ours (w/o pre-training)</cell><cell cols="12">50.4±6.3 47.0±5.7 53.9±6.2 51.3±4.9 54.1±4.3 52.8±4.4 55.1±5.3 52.1±4.9 58.4±4.7 59.9±4.9 56.8±5.2 58.1±6.3</cell></row><row><cell cols="13">6 Ours (w/o multi-view classifier) 54.7±5.8 53.1±5.5 55.2±6.1 54.4±3.0 59.9±5.7 59.1±4.7 57.3±5.4 57.6±3.7 65.3±4.9 64.6±6.0 66.1±7.1 64.9±3.8</cell></row><row><cell>7 Ours (w/o meta-controller)</cell><cell cols="12">53.8±6.7 51.0±7.7 54.9±5.2 54.4±3.9 60.1±7.1 52.8±6.2 55.1±8.3 57.1±4.4 66.4±7.7 66.9±4.3 65.8±7.6 64.1±6.6</cell></row><row><cell>8 Ours (w/o elder BD)</cell><cell cols="12">53.3±5.2 52.7±7.7 54.9±5.9 55.7±5.3 57.8±4.5 57.6±3.7 58.1±4.7 58.8±6.1 62.4±5.0 59.8±4.9 66.9±5.8 64.4±5.4</cell></row><row><cell>9 Ours (w/o youth BD)</cell><cell cols="12">55.8±4.9 55.0±5.7 56.4±6.3 57.4±4.8 59.9±5.7 59.1±5.7 57.3±6.0 56.6±5.5 65.3±5.3 64.6±4.9 66.1±6.5 64.9±6.3</cell></row><row><cell>10 Ours</cell><cell cols="12">59.8±4.8 58.0±4.3 60.1±4.1 59.4±4.0 67.9±3.7 66.1±4.2 68.3±3.3 67.6±3.7 71.3±3.3 70.7±3.2 72.1±3.5 70.0±3.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>c The Author(s), under exclusive license to Springer Nature Switzerland AG 2023 H. Greenspan et al. (Eds.): MICCAI 2023, LNCS 14227, pp. 99-108, 2023. https://doi.org/10.1007/978-3-031-43993-3_10</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported in part by <rs type="funder">National Natural Science Foundation of China</rs> (grant number <rs type="grantNumber">62131015</rs>), <rs type="funder">Science and Technology Commission of Shanghai Municipality (STCSM)</rs> (grant number <rs type="grantNumber">21010502600</rs>), and <rs type="funder">The Key R&amp;D Program of Guangdong Province, China</rs> (grant number <rs type="grantNumber">2021B0101420006</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WexrtsH">
					<idno type="grant-number">62131015</idno>
				</org>
				<org type="funding" xml:id="_UHhHEWf">
					<idno type="grant-number">21010502600</idno>
				</org>
				<org type="funding" xml:id="_pRKmJSA">
					<idno type="grant-number">2021B0101420006</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Alzheimer&apos;s disease neuroimaging initiative 2 clinical core: progress and plans</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Aisen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimer&apos;s Dementia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="734" to="739" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High-order resting-state functional connectivity network for MCI classification. Hum</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Mapp</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3282" to="3296" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ADHD-200 Consortium: The ADHD-200 consortium: a model to advance the translational potential of neuroimaging in clinical neuroscience</title>
	</analytic>
	<monogr>
		<title level="j">Front. Syst. Neurosci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">62</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mol. Psychiatry</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="659" to="667" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolution for resting-state fMRI analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gadgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pfefferbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Pohl</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59728-3_52</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59728-3_52" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12267</biblScope>
			<biblScope unit="page" from="528" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph U-nets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Measuring and improving the use of graph information in graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.13170</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Magnetic resonance imaging in Alzheimer&apos;s disease neuroimaging initiative 2. Alzheimer&apos;s</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dementia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="740" to="756" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Meta-modulation network for domain generalization in multi-site fMRI classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_48</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_48" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="500" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BrainGNN: interpretable brain graph neural network for fMRI analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">102233</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiscale functional connectome abnormality predicts cognitive outcomes in subcortical ischemic vascular disease</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb. Cortex</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="4641" to="4656" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep learning reveals the common spectrum underlying multiple brain disorders in youth and elders from brain functional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.11871</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive-step graph meta-learner for few-shot graph classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1055" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Meta-DermDiagnosis: few-shot skin disease identification using meta-learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>CVPR) Workshops</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Open Access Series of Imaging Studies (OASIS): cross-sectional MRI data in young, middle aged, nondemented, and demented older adults</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Csernansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Buckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cogn. Neurosci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1498" to="1507" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Local-global parcellation of the human cerebral cortex from intrinsic functional connectivity MRI</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schaefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cereb. Cortex</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3095" to="3114" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Grad-CAM: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">MetaMed: fewshot medical image classification using gradient-based meta-learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">108111</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Few-shot medical image segmentation using a global correlation network with discriminative embedding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">105067</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A transdiagnostic network for psychiatric illness derived from atrophy and lesions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Hum. Behav</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="420" to="429" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00992696</idno>
		<ptr target="https://doi.org/10.1007/BF00992696" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
