<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">B-Cos Aligned Transformers Learn Human-Interpretable Features</title>
				<funder ref="#_pnTzneG">
					<orgName type="full">Helmholtz Association</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Manuel</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Roche Diagnostics</orgName>
								<address>
									<settlement>Penzberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Helmholtz AI</orgName>
								<orgName type="institution">Helmholtz Munich</orgName>
								<address>
									<settlement>Neuherberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amal</forename><surname>Lahiani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Roche Diagnostics</orgName>
								<address>
									<settlement>Penzberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yashin</forename><surname>Dicente Cid</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Roche Diagnostics</orgName>
								<address>
									<settlement>Penzberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Melanie</forename><surname>Boxberg</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Pathology Munich-North</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Lienemann</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Helmholtz AI</orgName>
								<orgName type="institution">Helmholtz Munich</orgName>
								<address>
									<settlement>Neuherberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Ludwig Maximilian University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Matek</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Helmholtz AI</orgName>
								<orgName type="institution">Helmholtz Munich</orgName>
								<address>
									<settlement>Neuherberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">University Hospital Erlangen</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sophia</forename><forename type="middle">J</forename><surname>Wagner</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Helmholtz AI</orgName>
								<orgName type="institution">Helmholtz Munich</orgName>
								<address>
									<settlement>Neuherberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabian</forename><forename type="middle">J</forename><surname>Theis</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Helmholtz AI</orgName>
								<orgName type="institution">Helmholtz Munich</orgName>
								<address>
									<settlement>Neuherberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eldad</forename><surname>Klaiman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Roche Diagnostics</orgName>
								<address>
									<settlement>Penzberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tingying</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Helmholtz AI</orgName>
								<orgName type="institution">Helmholtz Munich</orgName>
								<address>
									<settlement>Neuherberg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">B-Cos Aligned Transformers Learn Human-Interpretable Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">314C77E89ECA5A7718695BD7C8C00508</idno>
					<idno type="DOI">10.1007/978-3-031-43993-3_50</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>transformer</term>
					<term>self-attention</term>
					<term>explainability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision Transformers (ViTs) and Swin Transformers (Swin) are currently state-of-the-art in computational pathology. However, domain experts are still reluctant to use these models due to their lack of interpretability. This is not surprising, as critical decisions need to be transparent and understandable. The most common approach to understanding transformers is to visualize their attention. However, attention maps of ViTs are often fragmented, leading to unsatisfactory explanations. Here, we introduce a novel architecture called the B-cos Vision Transformer (BvT) that is designed to be more interpretable. It replaces all linear transformations with the B-cos transform to promote weightinput alignment. In a blinded study, medical experts clearly ranked BvTs above ViTs, suggesting that our network is better at capturing biomedically relevant structures. This is also true for the B-cos Swin Transformer (Bwin). Compared to the Swin Transformer, it even improves the F1-score by up to 4.7% on two public datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Making artificial neural networks more interpretable, transparent, and trustworthy remains one of the biggest challenges in deep learning. They are often still considered black boxes, limiting their application in safety-critical domains such as healthcare. Histopathology is a prime example of this. For years, the number of pathologists has been decreasing while their workload has been increasing <ref type="bibr" target="#b22">[23]</ref>. Consequently, the need for explainable computer-aided diagnostic tools has become more urgent.</p><p>As a result, research in explainable artificial intelligence is thriving <ref type="bibr" target="#b19">[20]</ref>. Much of it focuses on convolutional neural networks (CNNs) <ref type="bibr" target="#b12">[13]</ref>. However, with the rise of transformers <ref type="bibr" target="#b30">[31]</ref> in computational pathology, and their increasing application to cancer classification, segmentation, survival prediction, and mutation detection tasks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, the old tools need to be reconsidered. Visualizing filter maps does not work for transformers, and Grad-CAM <ref type="bibr" target="#b29">[30]</ref> has known limitations for both CNNs and transformers.</p><p>The usual way to interpret transformer-based models is to plot their multihead self-attention scores <ref type="bibr" target="#b7">[8]</ref>. But these often lead to fragmented and unsatisfactory explanations <ref type="bibr" target="#b9">[10]</ref>. In addition, there is an ongoing controversy about their trustworthiness <ref type="bibr" target="#b4">[5]</ref>. To address these issues, we propose a novel family of transformer architectures based on the B-cos transform originally developed for CNNs <ref type="bibr" target="#b6">[7]</ref>. By aligning the inputs and weights during training, the models are implicitly forced to learn more biomedically relevant and meaningful features (Fig. <ref type="figure" target="#fig_0">1</ref>). Overall, our contributions are as follows:</p><p>• We propose the B-cos Vision Transformer (BvT) as a more explainable alternative to the Vision Transformer (ViT) <ref type="bibr" target="#b11">[12]</ref>. • We extensively evaluate both models on three public datasets: NCT-CRC-HE-100K <ref type="bibr" target="#b17">[18]</ref>, TCGA-COAD-20X <ref type="bibr" target="#b18">[19]</ref>, Munich-AML-Morphology <ref type="bibr" target="#b24">[25]</ref>. • We apply various post-hoc visualization techniques and conduct a blind study with domain experts to assess model interpretability. • We derive the B-cos Swin Transformer (Bwin) based on the Swin Transformer <ref type="bibr" target="#b20">[21]</ref> (Swin) in a generalization study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Explainability, interpretability, and relevancy are terms used to describe the ability of machine learning models to provide insight into their decision-making process. Although these terms have subtle differences, they are often used interchangeably in the literature <ref type="bibr" target="#b14">[15]</ref>. Recent research on understanding vision models has mostly focused on attribution methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>, which aim to identify important parts of an image and highlight them in a saliency map. Gradient-based approaches like Grad-CAM <ref type="bibr" target="#b29">[30]</ref> or attribution propagation strategies such as Deep Taylor Decomposition <ref type="bibr" target="#b26">[27]</ref> and LRP <ref type="bibr" target="#b5">[6]</ref> are commonly used methods. Perturbation-based techniques, such as SHAP <ref type="bibr" target="#b21">[22]</ref>, are another way to extract salient features from images. Besides saliency maps, one can also visualize the activations of the model using Activation Maximization <ref type="bibr" target="#b13">[14]</ref>.</p><p>However, it is still controversial whether the above methods can correctly reflect the behavior of the model and accurately explain the learned function (model-faithfulness <ref type="bibr" target="#b16">[17]</ref>). For example, it has been shown that some saliency maps are independent of both the data on which the model was trained and the model parameters <ref type="bibr" target="#b1">[2]</ref>. In addition, they are often considered unreliable for medical applications <ref type="bibr" target="#b3">[4]</ref>. As a result, inherently interpretable models have been proposed as a more reliable and transparent solution. The most recent contribution are B-cos CNNs <ref type="bibr" target="#b6">[7]</ref>, which use a novel nonlinear transformation (the B-cos transformation) instead of the traditional linear transformation.</p><p>Compared to CNNs, there is limited research on understanding transformers beyond attention visualization <ref type="bibr" target="#b9">[10]</ref>. Post-hoc methods such as Grad-CAM <ref type="bibr" target="#b29">[30]</ref> Fig. <ref type="figure">3</ref>. Rollout, Attention-Last (Attn-Last), Grad-CAM, LRP, LRP of the second layer (LRP-Second), LRP of the last layer (LRP-Last), and Transformer Attribution (TA) applied on the test set of Munich-AML-Morphology. The image shows an eosinophil, which is characterized by its split, but connected nucleus, large specific granules (pink structures in the cytoplasm), and dense chromatin (dark spots inside the nuclei) <ref type="bibr" target="#b28">[29]</ref>. Across all visualization techniques, BvT focuses on these exact features unlike ViT.</p><p>and Activation Maximization <ref type="bibr" target="#b13">[14]</ref> used for CNNs can also be applied to transformers. But in practice, the focus is on visualizing the raw attention values (see Attention-Last <ref type="bibr" target="#b15">[16]</ref>, Integrated Attention Maps <ref type="bibr" target="#b11">[12]</ref>, Rollout <ref type="bibr" target="#b0">[1]</ref>, or Attention Flow <ref type="bibr" target="#b0">[1]</ref>). More recent approaches such as Generic Attention <ref type="bibr" target="#b8">[9]</ref>, Transformer Attribution <ref type="bibr" target="#b9">[10]</ref>, and Conservative Propagation <ref type="bibr" target="#b2">[3]</ref> go a step further and introduce novel visualization techniques that better integrate the attention modules with contributions from different parts of the network. Note that these methods are all post-hoc methods applied after training to visualize the model's reasoning.</p><p>On the other hand, the ConceptTransformer <ref type="bibr" target="#b27">[28]</ref>, achieves better explainability by cross-attending user-defined concept tokens in the classifier head during training. More recently, HIPT <ref type="bibr" target="#b10">[11]</ref> combines multi-scale images and DINO <ref type="bibr" target="#b7">[8]</ref> pre-training to learn hierarchical visual concepts in a self-supervised fashion. Unlike all of these methods, interpretability is already an integral part of our architecture. Therefore, these methods can be easily applied to our models. In Fig. <ref type="figure">3</ref> and Fig. <ref type="figure" target="#fig_2">6</ref>, we show that the B-Cos Transformer produces superior feature maps over various post-hoc approaches -suggesting that our architecture does indeed learn human-plausible features that are independent of the specific visualization technique used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>We focus on the original Vision Transformer <ref type="bibr" target="#b11">[12]</ref>: The input image is divided into non-overlapping patches, flattened, and projected into a latent space of dimension d. Class tokens [cls] are then prepended to these patch embeddings. In addition, positional encodings [pos] are added to preserve topological information. In the scaled dot-product attention <ref type="bibr" target="#b30">[31]</ref>, the model learns different features (query Q, key K, and value V ) from the input vectors through a linear transformation. Both query and key are then correlated with a scaled dot-product and normalized with a softmax. These self-attention scores are then used to weight the value by importance:</p><formula xml:id="formula_0">Attention(Q, K, V ) = softmax QK T √ d V.<label>(1)</label></formula><p>To extract more information, this process is repeated h times in parallel (multi-headed self-attention). Each self-attention layer is followed by a fullyconnected layer consisting of two linear transformations and a ReLU activation.</p><p>We propose to replace all linear transforms in the original ViT (Fig. <ref type="figure" target="#fig_1">2</ref>)</p><formula xml:id="formula_1">Linear(x, w) = w T x = w x c(x, w),<label>(2)</label></formula><p>c(x, w) = cos(∠(x, w)), ∠...angle between vectors</p><p>with the B-cos* transform <ref type="bibr" target="#b6">[7]</ref> B-cos*(x;</p><formula xml:id="formula_3">w) = ŵ =1 x |c(x, ŵ)| B × sgn(c(x, ŵ)),<label>(4)</label></formula><p>where B ∈ N. Similar to <ref type="bibr" target="#b6">[7]</ref>, an additional nonlinearity is applied after each B-cos* transform. Specifically, each input is processed by two B-cos* transforms, and the subsequent MaxOut activation passes only the larger output. This ensures that only weight vectors with higher cosine similarity to the inputs are selected, which further increases the alignment pressure during optimization. Thus, the final B-cos transform is given by</p><formula xml:id="formula_4">B-cos(x; w) = max i∈{1,2} B-cos*(x; w i ).<label>(5)</label></formula><p>To see the significance of these changes, we look at Eq. 4 and derive</p><formula xml:id="formula_5">ŵ = 1 ⇒ B-cos*(x; w) ≤ x . (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>Fig. <ref type="figure">4</ref>. We compute the central kernel alignment (CKA), which measures the representation similarity between each hidden layer. Since the B-cos transform aligns the weights with the inputs, BvT (ours) achieves a more uniform representation structure compared to ViT (values closer to 1). When trained with the binary cross-entropy loss (BCE) instead of the categorical cross-entropy loss (CCE), the alignment is higher.</p><p>Since |c(x, ŵ)| ≤ 1, equality is only achieved if x and w are collinear, i.e., if they are aligned. Intuitively, this forces the weight vector to be more similar to the input. Query, key, and value thus capture more patterns in an image -which the attention mechanism can then attend to. This can be shown visually by plotting the centered kernel alignment (CKA). It measures the similarity between layers by comparing their internal representation structure. Compared to ViTs, BvTs achieve a highly uniform representation across all layers (Fig. <ref type="figure">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation and Evaluation Details</head><p>Task-Based Evaluation: Cancer classification and segmentation is an important first step for many downstream tasks such as grading or staging. Therefore, we choose this problem as our target. We classify image patches from the public colorectal cancer dataset NCT-CRC-HE-100K <ref type="bibr" target="#b17">[18]</ref>. We then apply our method to TCGA-COAD-20X <ref type="bibr" target="#b18">[19]</ref>, which consists of 38 annotated slides from the TCGA colorectal cancer cohort, to evaluate the effectiveness of transfer learning. This dataset is highly unbalanced and not color normalized compared Fig. <ref type="figure">5</ref>. In a blinded study, domain experts ranked models (lower is better) based on whether the models focus on biomedically relevant features that are known in the literature to be important for diagnosis. We then performed the Conover post-hoc test after Friedman with adjusted p-values according to the two-stage Benjamini-Hochberg procedure. BvT ranks above ViT with p &lt; 0.1 (underlined) and p &lt; 0.05 (bold). to the first dataset. Additionally, we demonstrate that the B-cos Vision Transformer is adaptable to domains beyond histopathology by training the model on the single white blood cell dataset Munich-AML-Morphology <ref type="bibr" target="#b24">[25]</ref>, which is also highly unbalanced and also publicly available.</p><p>Domain-Expert Evaluation: Our primary objective is to develop an extension of the Vision Transformer that is more transparent and trusted by medical professionals. To assess this, we propose a blinded study with four steps: (i) randomly selecting images from the test set of TCGA-COAD-20X (32 samples) and Munich-AML-Morphology (56 samples), (ii) plotting the last-layer attention and transformer attributions for each image, (iii) anonymizing and randomly shuffling the outputs, (iv) submitting them to two domain experts in histology and cytology for evaluation. Most importantly, we show them all the available saliency maps without pre-selecting them to get their unbiased opinion.</p><p>Implementation Details: In our experiments, we compare different variants of the B-cos Vision Transformer and the Vision Transformer. Specifically, we implement two versions of ViT: ViT-T/8 and ViT-S/8. They only differ in parameter size (5M for T models and 22M for S models) and use the same patch size of 8. All BvT models (BvT-T/8 and BvT-S/8) are derivatives of the corresponding ViT models. The B-cos transform used in the BvT models has an exponent of B = 2. We use AdamW with a cosine learning rate scheduler for optimization and a separate validation set for hyperparameter selection. Following the findings of <ref type="bibr" target="#b6">[7]</ref>, we add [1 -r, 1 -g, 1 -b] to the RGB channels [r, g, b] of BvT. This allows us to encode each pixel with the direction of the color channel vector, forcing the model to capture more color information. Furthermore, we train models with two different loss functions: the standard categorical cross-entropy loss (CCE) and the binary cross-entropy loss (BCE) with one-hot encoded entries. It was suggested in <ref type="bibr" target="#b6">[7]</ref> that BCE is a more appropriate loss for B-cos CNNs. We explore whether this is also true for transformers in our experiments. Additional details on training, optimization, and datasets can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>Task-Based Evaluation: When trained from scratch, all BvT models underperform their ViT counterparts by about 2% on NCT-CRC-HE-100K and 3% on Munich AML-Morphology (Table <ref type="table" target="#tab_0">1</ref>). However, when we use the pretrained weights from NCT-CRC-HE-100K and transfer them to TCGA-COAD-20X for fine-tuning, BvT outperforms ViT by up to 5% (Table <ref type="table" target="#tab_0">1</ref>). We believe this is due to the simultaneous optimization of two objectives: classification loss and weight-input alignment. With a pre-trained model, BvT is likely to focus more on the former. In addition, we observe that models trained with BCE tend to perform worse than those trained with CCE. However, their saliency maps seem to be more interpretable (see Fig. <ref type="figure">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain-Expert Evaluation:</head><p>The results show that BvTs are significantly more trustworthy than ViTs (p &lt; 0.05). This indicates that BvT consistently attends to biomedically relevant features such as cancer cells, nuclei, cytoplasm, or membrane <ref type="bibr" target="#b23">[24]</ref> (Fig. <ref type="figure">5</ref>). In many visualization techniques, we see that BvT, unlike ViT, focuses exclusively on these structures (Fig. <ref type="figure">3</ref>). In contrast, ViT attributes high attention to seemingly irrelevant features, such as the edges of the cells. A third expert points out that ViT might overfit certain patterns in this dataset, which could aid the model in improving its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Generalization to Other Architectures</head><p>We aim to explore whether the B-cos transform can enhance the interpretability of other transformer-based architectures. The Swin Transformer (Swin) <ref type="bibr" target="#b20">[21]</ref> is a popular alternative to ViT (e.g., it is currently the SOTA feature extractor for histopathological images <ref type="bibr" target="#b32">[33]</ref>). Swin utilizes window attention and feed-forward layers. In this study, we replace all its linear transforms with the B-cos transform, resulting in the B-cos Swin Transformer (Bwin). However, unlike BvT and ViT, it is not obvious how to visualize the window attention. Therefore, we introduce a modified variant here that has a regular ViT/BvT block in the last layer.</p><p>In our experiments (Table <ref type="table" target="#tab_1">2</ref>), we observe that Bwin outperforms Swin by up to 2.7% and 4.8% in F1-score on NCT-CRC-HE-100K and Munich-AML-Morphology, respectively. This is consistent with the observations made in Sect. 5: When BvT is trained from scratch, the model faces a trade-off between learning the weight and input alignment and finding the appropriate inductive bias to solve the classification task. By reintroducing many of the inductive biases of CNNs through the window attention in the case of Swin or transfer learning in the case of BvT, the model likely overcomes this initial problem.</p><p>Moreover, we would like to emphasize that the modified models have no negative impact on the model's performance. In fact, all metrics remain similar or even improve. The accumulated attention heads (we keep 50% of the mass) demonstrate that Bwin solely focuses on nuclei and other cellular features (Fig. <ref type="figure" target="#fig_2">6</ref>). Conversely, Swin has very sparse attention heads, pointing to a few spots. Consistent with the BvT vs ViT blind study, our pathologists also agree that Bwin is more plausible than Swin (p &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have introduced the B-cos Vision Transformer (BvT) and the B-cos Swin Transformer (Bwin) as two alternatives to the Vision Transformer (ViT) and the Swin Transformer (Swin) that are more interpretable and explainable. These models use the B-cos transform to enforce similarity between weights and inputs. In a blinded study, domain experts clearly preferred both BvT and Bwin over ViT and Swin. We have also shown that BvT is competitive with ViT in terms of quantitative performance. Moreover, using Bwin or transfer learning for BvT, we can even outperform the original models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Attention maps of ViT and BvT (ours) on the test set of (a) NCT-CRC-HE-100K, (b) TCGA-COAD-20X, and (c) Munich-AML-Morphology. BvT attends to various diagnostically relevant features such as cancer tissue, cells, and nuclei.</figDesc><graphic coords="2,74,46,54,08,303,55,212,62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The model architecture of ViT and BvT (ours). We replace all linear transformations in ViT with the B-cos transform and remove all ReLU activation functions.</figDesc><graphic coords="3,58,80,54,08,306,94,140,62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Attention maps of the last layer of the modified Swin and Bwin (ours). Bwin focuses on cells and nuclei, while Swin mostly focuses on a few spots.</figDesc><graphic coords="8,123,48,53,84,205,30,94,87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>F1-score, top-1, and top-3 accuracy from the test set of NCT-CRC-HE-100K, Munich-AML-Morphology, and TCGA-COAD-20X. We compare ViT and BvT (ours) trained with categorical cross-entropy (CCE) and binary cross-entropy (BCE) loss using two model configurations: T/8 and S/8 (see Sect. 4).</figDesc><table><row><cell>Models</cell><cell>NCT</cell><cell cols="2">Munich</cell><cell>TCGA</cell></row><row><cell></cell><cell>F1</cell><cell>Top-1 Top-3 F1</cell><cell cols="2">Top-1 Top-3 F1</cell><cell>Top-1 Top-3</cell></row><row><cell cols="5">ViT-T/8 CCE 90.9 92.7 99.1 57.3 90.1 98.9 57.1 78.8 94.4</cell></row><row><cell cols="5">ViT-S/8 CCE 89.2 91.1 99.5 56.3 93.1 99.0 56.3 78.6 92.9</cell></row><row><cell cols="5">BvT-T/8 CCE 88.8 91.1 99.3 54.0 87.1 98.6 61.0 77.4 93.1</cell></row><row><cell cols="5">BvT-S/8 CCE 88.4 90.1 99.4 52.9 89.8 98.6 60.2 76.3 92.9</cell></row><row><cell cols="5">ViT-T/8 BCE 90.0 91.4 98.4 54.8 90.0 99.0 53.6 79.6 93.9</cell></row><row><cell cols="5">ViT-S/8 BCE 90.2 92.2 99.3 55.4 92.8 99.0 54.1 77.0 88.9</cell></row><row><cell cols="5">BvT-T/8 BCE 86.7 90.1 98.5 51.1 83.5 97.9 57.7 79.8 93.4</cell></row><row><cell cols="5">BvT-S/8 BCE 87.5 90.4 99.4 52.4 85.0 98.3 59.0 74.5 88.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results of Swin and Bwin (ours) experiments on the test set of NCT-CRC-HE-100K and Munich-AML-Morphology. We report F1-score, top-1, and top-3 accuracy.</figDesc><table><row><cell>Models</cell><cell>NCT</cell><cell>AML</cell></row><row><cell></cell><cell>F1</cell><cell>Top-1 Top-3 F1</cell><cell>Top-1 Top-3</cell></row><row><cell>Swin-TCCE</cell><cell cols="3">89.1 92.1 99.0 48.2 94.1 98.8</cell></row><row><cell cols="4">Swin-TCCE (modified) 89.8 92.0 99.6 49.1 94.2 98.9</cell></row><row><cell>Bwin-TCCE</cell><cell cols="3">91.5 93.5 99.5 53.0 93.9 98.6</cell></row><row><cell cols="4">Bwin-TCCE (modified) 92.5 94.3 99.6 53.3 93.8 98.7</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. M.T. and S.J.W. are supported by the <rs type="funder">Helmholtz Association</rs> under the joint research school "<rs type="programName">Munich School for Data Science"</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pnTzneG">
					<orgName type="program" subtype="full">Munich School for Data Science&quot;</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_50.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">58th ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4190" to="4197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sanity checks for saliency map</title>
		<author>
			<persName><forename type="first">J</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">32th NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018">2018</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">XAI for transformers: better explanations through conservative propagation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schnake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">th ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="435" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Assessing the trustworthiness of saliency maps for localizing abnormalities in medical imaging</title>
		<author>
			<persName><forename type="first">N</forename><surname>Arun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiol. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Is attention explanation? An introduction to the debate</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bibal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">60th ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3889" to="3900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Layer-wise relevance propagation for neural networks with local renormalization layers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-44781-0_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-44781-0_8" />
	</analytic>
	<monogr>
		<title level="m">ICANN 2016</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">E P</forename><surname>Villa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Masulli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Pons Rivero</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9887</biblScope>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">B-Cos networks: alignment is all we need for interpretability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Böhle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10329" to="10338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="397" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformer interpretability beyond attention visualization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="782" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scaling vision transformers to gigapixel images via hierarchical self-supervised learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16144" to="16155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An image is worth 16 × 16 words: transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In: 9th ICLR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Techniques for interpretable machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technical reports University of Montreal</title>
		<imprint>
			<biblScope unit="volume">1341</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Explaining explanations: an overview of interpretability of machine learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Gilpin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Specter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kagal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 5th DSAA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relative importance in sentence processing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hollenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beinborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">59th ACL and the 11th IJCNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards faithfully interpretable NLP systems: how should we define and evaluate faithfulness?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jacovi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th ACL</title>
		<meeting>the 58th ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4198" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predicting survival from colorectal cancer histology slides using deep learning: a retrospective multicenter study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Med</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Radiology data from the Cancer Genome Atlas Colon Adenocarcinoma [TCGA-COAD] collection. The cancer imaging archive</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brigham &amp; Women&apos;s Hospital Boston</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Roswell Park Cancer Institute</publisher>
		</imprint>
		<respStmt>
			<orgName>University of North Carolina</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Explainable AI: a review of machine learning interpretability methods</title>
		<author>
			<persName><forename type="first">P</forename><surname>Linardatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Papastefanopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kotsiantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9992" to="10002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th NeurIPS</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Number of pathologists in Germany: comparison with European countries, USA, and Canada</title>
		<author>
			<persName><forename type="first">B</forename><surname>Märkl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Füzesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Huss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Virchows Arch</title>
		<imprint>
			<biblScope unit="volume">478</biblScope>
			<biblScope unit="page" from="335" to="341" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Highly accurate differentiation of bone marrow cell morphologies using deep neural networks on a large image data set</title>
		<author>
			<persName><forename type="first">C</forename><surname>Matek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krappe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Münzenmayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Haferlach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Blood</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="1917" to="1927" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human-level recognition of blast cells in acute myeloid leukemia with convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Matek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Spiekermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="538" to="544" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What makes transfer learning work for medical images: feature reuse &amp; other factors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Haslum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sorkhei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Söderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9225" to="9234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Explaining nonlinear classification decisions with deep Taylor decomposition. Pattern Recogn</title>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="211" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention-based interpretability with concept transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rigotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Miksovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gschwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Scotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">10th ICLR</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Eosinophils: changing perspectives in health and disease</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Immunol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="22" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Grad-CAM: visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">st NIPS</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.09617</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transformer-based unsupervised contrastive learning for histopathological image classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2022">102559. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
