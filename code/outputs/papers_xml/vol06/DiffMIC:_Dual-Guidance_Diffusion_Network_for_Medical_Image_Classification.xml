<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification</title>
				<funder ref="#_c55vdMR">
					<orgName type="full">National Research Foundation, Singapore</orgName>
				</funder>
				<funder ref="#_eghdKZM">
					<orgName type="full">Guangzhou Municipal Science and Technology Project</orgName>
				</funder>
				<funder ref="#_pE7Q2dY">
					<orgName type="full">A*STAR Central Research Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yijun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology (Guangzhou)</orgName>
								<address>
									<settlement>Nansha, Guangzhou, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Agency for Science, Technology and Research</orgName>
								<orgName type="institution">Institute of High Performance Computing</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Angelica</forename><forename type="middle">I</forename><surname>Aviles-Rivero</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carola-Bibiane</forename><surname>Schönlieb</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
							<email>leizhu@ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology (Guangzhou)</orgName>
								<address>
									<settlement>Nansha, Guangzhou, Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="95" to="105"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">494A95D13F6D8A63DA90B7417837BB97</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_10</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>diffusion probabilistic model</term>
					<term>medical image classification</term>
					<term>placental maturity</term>
					<term>skin lesion</term>
					<term>diabetic retinopathy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Diffusion Probabilistic Models have recently shown remarkable performance in generative image modeling, attracting significant attention in the computer vision community. However, while a substantial amount of diffusion-based research has focused on generative tasks, few studies have applied diffusion models to general medical image classification. In this paper, we propose the first diffusion-based model (named DiffMIC) to address general medical image classification by eliminating unexpected noise and perturbations in medical images and robustly capturing semantic representation. To achieve this goal, we devise a dual conditional guidance strategy that conditions each diffusion step with multiple granularities to improve step-wise regional attention. Furthermore, we propose learning the mutual information in each granularity by enforcing Maximum-Mean Discrepancy regularization during the diffusion forward process. We evaluate the effectiveness of our DiffMIC on three medical classification tasks with different image modalities, including placental maturity grading on ultrasound images, skin lesion classification using dermatoscopic images, and diabetic retinopathy grading using fundus images. Our experimental results demonstrate that DiffMIC outperforms state-of-the-art methods by a significant margin, indicating the universality and effectiveness of the proposed model. Our code is publicly available at https://github.com/scott-yjyang/DiffMIC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical image analysis plays an indispensable role in clinical therapy because of the implications of digital medical imaging in modern healthcare <ref type="bibr" target="#b4">[5]</ref>. Medical image classification, a fundamental step in the analysis of medical images, strives to distinguish medical images from different modalities based on certain criteria. An automatic and reliable classification system can help doctors interpret medical images quickly and accurately. Massive solutions for medical image classification have been developed over the past decades in the literature, most of which adopt deep learning ranging from popular CNNs to vision transformers <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. These methods have the potential to reduce the time and effort required for manual classification and improve the accuracy and consistency of results. However, medical images with diverse modalities still challenge existing methods due to the presence of various ambiguous lesions and fine-grained tissues, such as ultrasound (US), dermatoscopic, and fundus images. Moreover, generating medical images under hardware limitations can cause noisy and blurry effects, which can degrade image quality and thus demand a more effective feature representation modeling for robust classifications.</p><p>Recently, Denoising Diffusion Probabilistic Models (DDPM) <ref type="bibr" target="#b13">[14]</ref> have achieved excellent results in image generation and synthesis tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref> by iteratively improving the quality of a given image. Specifically, DDPM is a generative model based on a Markov chain, which models the data distribution by simulating a diffusion process that evolves the input data towards a target distribution. Although a few pioneer works tried to adopt the diffusion model for image segmentation and object detection tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29]</ref>, their potential for high-level vision has yet to be fully explored.</p><p>Motivated by the achievements of diffusion probabilistic models in generative image modeling, 1) we present a novel Denoising Diffusion-based model named DiffMIC for accurate classification of diverse medical image modalities. As far as we know, we are the first to propose a Diffusion-based model for general medical image classification. Our method can appropriately eliminate undesirable noise in medical images as the diffusion process is stochastic in nature for each sampling step. 2) In particular, we introduce a Dual-granularity Conditional Guidance (DCG) strategy to guide the denoising procedure, conditioning each step with both global and local priors in the diffusion process. By conducting the diffusion process on smaller patches, our method can distinguish critical tissues with fine-grained capability. 3) Moreover, we introduce Condition-specific Maximum-Mean Discrepancy (MMD) regularization to learn the mutual information in the latent space for each granularity, enabling the network to model a robust feature representation shared by the whole image and patches. 4) We evaluate the effectiveness of DiffMIC on three 2D medical image classification tasks including placental maturity grading, skin lesion classification, and diabetic retinopathy grading. The experimental results demonstrate that our diffusion-based classification method consistently and significantly surpasses state-of-the-art methods for all three tasks. Figure <ref type="figure" target="#fig_0">1</ref> shows the schematic illustration of our network for medical image classification. Given an input medical image x, we pass it to an image encoder to obtain the image feature embedding ρ(x), and a dual-granularity conditional guidance (DCG) model to produce the global prior ŷg and local prior ŷl . At the training stage, we apply the diffusion process on ground truth y 0 and different priors to generate three noisy variables y g t , y l t , and y t (the global prior for y g t , the local prior for y l t , and dual priors for y t ). Then, we combine the three noisy variables y g t , y l t , and y t and their respective priors and project them into a latent space, respectively. We further integrate three projected embeddings with the image feature embedding ρ(x) in the denoising U-Net, respectively, and predict the noise distribution sampled for y g t , y l t , and y t . We devise condition-specific maximum-mean discrepancy (MMD) regularization loss on the predicted noise of y g t and y l t , and employ the noise estimation loss by mean squared error (MSE) on the predicted noise of y t to collaboratively train our DiffMIC network. Diffusion Model. Following DDPM <ref type="bibr" target="#b13">[14]</ref>, our diffusion model also has two stages: a forward diffusion stage (training) and a reverse diffusion stage (inference). In the forward process, the ground truth response variable y 0 is added Gaussian noise through the diffusion process conditioned by time step t sampled from a uniform distribution of [1, T ], and such noisy variables are denoted as {y 1 , ..., y t , .., y T }. As suggested by the standard implementation of DDPM, we adopt a UNet as the denoising network to parameterize the reverse diffusion process and learn the noise distribution in the forward process. In the reverse diffusion process, the trained UNet θ generates the final prediction ŷ0 by transforming the noisy variable distribution p θ (y T ) to the ground truth distribution p θ (y 0 ):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><formula xml:id="formula_0">p θ (y 0:T -1 |y T , ρ(x)) = T t=1 p θ (y t-1 |y t , ρ(x)), and p θ (y T ) = N ( ŷg + ŷl 2 , I), (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where θ is parameters of the denoising UNet, N (•, •) denotes the Gaussian distribution, and I is the identity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dual-Granularity Conditional Guidance (DCG) Strategy</head><p>DCG Model. In most conditional DDPM, the conditional prior will be a unique given information. However, medical image classification is particularly challenging due to the ambiguity of objects. It is difficult to differentiate lesions and tissues from the background, especially in low-contrast image modalities, such as ultrasound images. Moreover, unexpected noise or blurry effects may exist in regions of interest (ROIs), thereby hindering the understanding of high-level semantics. Taking only a raw image x as the condition in each diffusion step will be insufficient to robustly learn the fine-grained information, resulting in classification performance degradation.</p><p>To alleviate this issue, we design a Dual-granularity Conditional Guidance (DCG) for encoding each diffusion step. Specifically, we introduce a DCG model τ D to compute the global and local conditional priors for the diffusion process. Similar to the diagnostic process of a radiologist, we can obtain a holistic understanding from the global prior and also concentrate on areas corresponding to lesions from the local prior when removing the negative noise effects. As shown in Fig. <ref type="figure" target="#fig_0">1 (c</ref>), for the global stream, the raw image data x is fed into the global encoder τ g and then a 1 × 1 convolutional layer to generate a saliency map of the whole image. The global prior ŷg is then predicted from the whole saliency map by averaging the responses. For the local stream, we further crop the ROIs whose responses are significant in the saliency map of the whole image. Each ROI is fed into the local encoder τ l to obtain a feature vector. We then leverage the gated attention mechanism <ref type="bibr" target="#b14">[15]</ref> to fuse all feature vectors from ROIs to obtain a weighted vector, which is then utilized for computing the local prior ŷl by one linear layer. Denoising Model. The noisy variable y t is sampled in the diffusion process based on the global and local priors computed by the DCG model following:</p><formula xml:id="formula_2">y t = √ ᾱt y 0 + √ 1 -ᾱt + (1 - √ ᾱt )(ŷ g + ŷl ),<label>(2)</label></formula><p>where ∼ N(0, I), ᾱt = t α t , α t = 1β t with a linear noise schedule {β t } t=1:T ∈ (0, 1) T . After that, we feed the concatenated vector of the noisy variable y t and dual priors into our denoising model UNet θ to estimate the noise distribution, which is formulated as:</p><formula xml:id="formula_3">θ (ρ(x), y t , ŷg , ŷl , t) = D(E(f ([y t , ŷg , ŷl ]), ρ(x), t), t),<label>(3)</label></formula><p>where f (•) denotes the projection layer to the latent space. [•] is the concatenation operation. E(•) and D(•) are the encoder and decoder of UNet. Note that the image feature embedding ρ(x) is further integrated with the projected noisy embedding in the UNet to make the model focus on high-level semantics and thus obtain more robust feature representations. In the forward process, we seek to minimize the noise estimation loss L :</p><formula xml:id="formula_4">L = || -θ (ρ(x), y t , ŷg , ŷl , t)|| 2 . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>Our method improves the vanilla diffusion model by conditioning each step estimation function on priors that combine information derived from the raw image and ROIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Condition-Specific MMD Regularization</head><p>Maximum-Mean Discrepancy (MMD) is to measure the similarity between two distributions by comparing all of their moments <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>, which can be efficiently achieved by a kernel function. Inspired by InfoVAE <ref type="bibr" target="#b30">[31]</ref>, we introduce an additional pair of condition-specific MMD regularization loss to learn mutual information between the sampled noise distribution and the Gaussian distribution.</p><p>To be specific, we sample the noisy variable y g t from the diffusion process at time step t conditioned only by the global prior and then compute an MMDregularization loss as:</p><formula xml:id="formula_6">L g MMD (n||m) = K(n, n ) -2K(m, n) + K(m, m ), with n = , m = θ (ρ(x), √ ᾱt y 0 + √ 1 -ᾱt + (1 - √ ᾱt )ŷ g , ŷg , t),<label>(5)</label></formula><p>where K(•, •) is a positive definite kernel to reproduce distributions in the Hilbert space. The condition-specific MMD regularization is also applied on the local prior, as shown in Fig. <ref type="figure" target="#fig_0">1 (a)</ref>. While the general noise estimation loss L captures the complementary information from both priors, the condition-specific MMD regularization maintains the mutual information between each prior and target distribution. This also helps the network better model the robust feature representation shared by dual priors and converge faster in a stable way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training and Inference Scheme</head><p>Total Loss. By adding the noise estimation loss and the MMD-regularization loss, we compute the total loss L dif f of our denoising network as follows:</p><formula xml:id="formula_7">L dif f = L + λ(L g MMD + L l MMD ), (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where λ is a balancing hyper-parameter, and it is empirically set as λ=0. <ref type="bibr" target="#b4">5</ref>.</p><p>Training Details. The diffusion model in this study leverages a standard DDPM training process, where the diffusion time step t is selected from a uniform distribution of [1, T ], and the noise is linearly scheduled with β 1 = 1 × 10 -4 and β T = 0.02. We adopt ResNet18 as the image encoder ρ(•). Following <ref type="bibr" target="#b11">[12]</ref>, we concatenate y t ,ŷ g ,ŷ l , and apply a linear layer with an output dimension of 6144 to obtain the fused vector in the latent space. To condition the response embedding on the timestep, we perform a Hadamard product between the fused vector and a timestep embedding. We then integrate the image feature embedding and response embedding by performing another Hadamard product between them. The output vector is sent through two consecutive fully-connected layers, each followed by a Hadamard product with a timestep embedding. Finally, we use a fully-connected layer to predict the noise with an output dimension of classes. It is worth noting that all fully-connected layers are accompanied by a batch normalization layer and a Softplus non-linearity, with the exception of the output layer. For the DCG model τ D , the backbone of its global and local stream is ResNet. We adopt the standard cross-entropy loss as the objective of the DCG model. We jointly train the denoising diffusion model and DCG model after pretraining the DCG model 10 epochs for warm-up, thereby resulting in an end-to-end DiffMIC for medical image classification. Inference Stage. As displayed in Fig. <ref type="figure" target="#fig_0">1</ref> (b), given an input image x, we first feed it into the DCG model to obtain dual priors ŷg , ŷl . Then, following the pipeline of DDPM, the final prediction ŷ0 is iteratively denoised from the random prediction y T using the trained UNet conditioned by dual priors ŷg , ŷl and the image feature embedding ρ(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets and Evaluation: We evaluate the effectiveness of our network on an in-home dataset and two public datasets, e.g., PMG2000, HAM10000 <ref type="bibr" target="#b26">[27]</ref>, and APTOS2019 <ref type="bibr" target="#b15">[16]</ref>. (a) PMG2000. We collect and annotate a benchmark dataset (denoted as PMG2000) for placental maturity grading (PMG) with four categories<ref type="foot" target="#foot_0">1</ref> . PMG2000 is composed of 2,098 ultrasound images, and we randomly divide the entire dataset into a training part and a testing part at an 8:2 ratio. (b) HAM10000. HAM10000 <ref type="bibr" target="#b26">[27]</ref> is from the Skin Lesion Analysis Toward Melanoma Detection 2018 challenge, and it contains 10,015 skin lesion images with predefined 7 categories. (c) APTOS2019. In APTOS2019 <ref type="bibr" target="#b15">[16]</ref>, A total of 3,662 fundus images have been labeled to classify diabetic retinopathy into five different categories. Following the same protocol in <ref type="bibr" target="#b9">[10]</ref>, we split HAM10000 and APTOS2019 into a train part and a test part at a 7:3 ratio. These three datasets are with different medical image modalities. PMG2000 is gray-scale and class-balanced ultrasound images; HAM10000 is colorful but classimbalanced dermatoscopic images; and APTOS2019 is another class-imbalanced dataset with colorful Fundus images. Moreover, we introduce two widely-used metrics Accuracy and F1-score to quantitatively compare our framework against existing SOTA methods. Implementation Details: Our framework is implemented with the PyTorch on one NVIDIA RTX 3090 GPU. We center-crop the image and then resize the spatial resolution of the cropped image to 224×224. Random flipping and rotation for data augmentation are implemented during the training processing. In all experiments, we extract six 32×32 ROI patches from each image. We trained our network end-to-end using the batch size of 32 and the Adam optimizer. The initial learning rate for the denoising model U-Net is set as 1×10 -3 , while for the DCG model (see Sect. 2.1) it is set to 2×10 -4 when training the entire network. Following <ref type="bibr" target="#b19">[20]</ref>, the number of training epochs is set as 1,000 for all three datasets. In inference, we empirically set the total diffusion time step T as 100 for PMG2000, 250 for HAM10000, and 60 for APTOS2019, which is much smaller than most of the existing works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. The average running time of our DiffMIC is about 0.056 s for classifying an image with a spatial resolution of 224×224. For HAM10000, our method produces a promising improvement over the secondbest method ProCo of 0.019 and 0.053 in terms of Accuracy and F1-score, respectively. For APTOS2019, our method obtains a considerable improvement over ProCo of 0.021 and 0.042 in Accuracy and F1-score respectively.  Ablation Study: Extensive experiments are conducted to evaluate the effectiveness of major modules of our network. To do so, we build three baseline networks from our method. The first baseline (denoted as "basic") is to remove all diffusion operations and the MMD regularization loss from our network. It means that "basic" is equal to the classical ResNet18. Then, we apply the vanilla diffusion process onto "basic" to construct another baseline network (denoted as "C1"), and further add our dual-granularity conditional guidance into the diffusion process to build a baseline network, which is denoted as "C2". Hence, "C2" is equal to removing the MMD regularization loss from our network for image classification. Table <ref type="table" target="#tab_2">2</ref> reports the Accuracy and F1-score results of our method and three baseline networks on our PMG2000 dataset. Apparently, compared to "basic", "C1" has an Accuracy improvement of 0.027 and an F1-score improvement of 0.018, which indicates that the diffusion mechanism can learn more discriminate features for medical image classification, thereby improving the PMG performance. Moreover, the better Accuracy and F1-score results of "C2" over "C1" demonstrates that introducing our dual-granularity conditional guidance into the vanilla diffusion process can benefit the PMG performance. Furthermore, our method outperforms "C2" in terms of Accuracy and F1-score, which indicates that exploring the MMD regularization loss in the diffusion process can further help to enhance the PMG results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Our Diffusion Procedure:</head><p>To illustrate the diffusion reverse process guided by our dual-granularity conditional encoding, we used the t-SNE tool to visualize the denoised feature embeddings at consecutive time steps. Figure <ref type="figure" target="#fig_1">2</ref> presents the results of this process on all three datasets. As the time step encoding progresses, the denoise diffusion model gradually removes noise from the feature representation, resulting in a clearer distribution of classes from the Gaussian distribution. The total number of time steps required for inference depends on the complexity of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This work presents a diffusion-based network (DiffMIC) to boost medical image classification. The main idea of our DiffMIC is to introduce dual-granularity conditional guidance over vanilla DDPM, and enforce condition-specific MMD regularization to improve classification performance. Experimental results on three medical image classification datasets with diverse image modalities show the superior performance of our network over state-of-the-art methods. As the first diffusion-based model for general medical image classification, our DiffMIC has the potential to serve as an essential baseline for future research in this area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our DiffMIC framework. (a) The training phase (forward process) and (b) The inference phase (reverse process) are constructed, respectively. (The noise of feature embedding is greater with the darker color.) (c) The DCG Model τD guides the diffusion process by the dual priors from the raw image and ROIs.</figDesc><graphic coords="3,58,98,103,49,334,63,172,09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. t-SNE obtained from the denoised feature embedding by the diffusion reverse process during inference on three datasets. t is the current diffusion time step for inference. As the time step encoding progresses, the noise is gradually removed, thereby obtaining a clear distribution of classes; see the last column (please zoom in).</figDesc><graphic coords="8,77,52,145,73,279,49,137,41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison to SOTA methods on three classification tasks. The best results are marked in bold font.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a) PMG2000</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Methods</cell><cell cols="7">ResNet [13] ViT [7] Swin [19] PVT [28] GMIC [24] Our DiffMIC</cell></row><row><cell cols="2">PMG2000</cell><cell cols="2">Accuracy F1-score</cell><cell>0.879 0.881</cell><cell>0.886 0.890</cell><cell>0.893 0.892</cell><cell>0.907 0.902</cell><cell>0.900 0.901</cell><cell>0.931 0.926</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) HAM10000 and APTOS2019</cell><cell></cell></row><row><cell cols="2">Methods</cell><cell></cell><cell cols="7">LDAM [3] OHEM [25] MTL [18] DANIL [10] CL [20] ProCo [30] Our DiffMIC</cell></row><row><cell>HAM10000</cell><cell cols="2">Accuracy F1-score</cell><cell>0.857 0.734</cell><cell>0.818 0.660</cell><cell></cell><cell>0.811 0.667</cell><cell>0.825 0.674</cell><cell>0.865 0.739</cell><cell>0.887 0.763</cell><cell>0.906 0.816</cell></row><row><cell>APTOS2019</cell><cell cols="2">Accuracy F1-score</cell><cell>0.813 0.620</cell><cell>0.813 0.631</cell><cell></cell><cell>0.813 0.632</cell><cell>0.825 0.660</cell><cell>0.825 0.652</cell><cell>0.837 0.674</cell><cell>0.858 0.716</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Comparison with State-of-the-Art Methods: InTable 1</head><label>1</label><figDesc></figDesc><table><row><cell>(a), we compare</cell></row><row><cell>our DiffMIC against many state-of-the-art CNNs and transformer-based net-</cell></row><row><cell>works, including ResNet, Vision Transformer (ViT), Swin Transformer (Swin),</cell></row><row><cell>Pyramid Transformer (PVT), and a medical image classification method (i.e.,</cell></row><row><cell>GMIC) on PMG2000. Apparently, PVT has the largest Accuracy of 0.907,</cell></row><row><cell>and the largest F1-score of 0.902 among these methods. More importantly, our</cell></row><row><cell>method further outperforms PVT. It improves the Accuracy from 0.907 to 0.931,</cell></row><row><cell>and the F1-score from 0.902 to 0.926.</cell></row><row><cell>Note that both HAM10000 and APTOS2019 have a class imbalance issue.</cell></row><row><cell>Hence, we compare our DiffMIC against state-of-the-art long-tailed medical</cell></row><row><cell>image classification methods, and report the comparison results in Table 1(b).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Effectiveness of each module in our DiffMIC on the PMG2000 dataset.</figDesc><table><row><cell></cell><cell cols="5">Diffusion DCG MMD-reg Accuracy F1-score</cell></row><row><cell>basic</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.879</cell><cell>0.881</cell></row><row><cell>C1</cell><cell></cell><cell>-</cell><cell>-</cell><cell>0.906</cell><cell>0.899</cell></row><row><cell>C2</cell><cell></cell><cell></cell><cell>-</cell><cell>0.920</cell><cell>0.914</cell></row><row><cell>Our method</cell><cell></cell><cell></cell><cell></cell><cell>0.931</cell><cell>0.926</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our data collection is approved by the Institutional Review Board (IRB).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This research is supported by <rs type="funder">Guangzhou Municipal Science and Technology Project</rs> (Grant No. <rs type="grantNumber">2023A03J0671</rs>), the <rs type="funder">National Research Foundation, Singapore</rs> under its <rs type="programName">AI Singapore Programme (AISG Award</rs> No: <rs type="grantNumber">AISG2-TC-2021-003</rs>), A*<rs type="programName">STAR AME Programmatic Funding Scheme Under Project A20H4b0141</rs>, and <rs type="funder">A*STAR Central Research Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eghdKZM">
					<idno type="grant-number">2023A03J0671</idno>
				</org>
				<org type="funding" xml:id="_c55vdMR">
					<idno type="grant-number">AISG2-TC-2021-003</idno>
					<orgName type="program" subtype="full">AI Singapore Programme (AISG Award</orgName>
				</org>
				<org type="funding" xml:id="_pE7Q2dY">
					<orgName type="program" subtype="full">STAR AME Programmatic Funding Scheme Under Project A20H4b0141</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 10.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">SegDiff: image segmentation with diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaharbany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.00390</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Conditional image generation with score-based diffusion models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Batzolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stanczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Schönlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Etmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13606</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09788</idno>
		<title level="m">DiffusionDet: diffusion model for object detection</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Machine learning approaches in medical image analysis: from detection to diagnosis</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8780" to="8794" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="issue">7639</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A guide to deep learning in healthcare</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="29" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distractor-aware neuron intrinsic learning for generic 2d medical image classifications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59713-9_57</idno>
		<idno>978-3-030-59713-9 57</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12262</biblScope>
			<biblScope unit="page" from="591" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.07275</idno>
		<title level="m">Card: classification and regression diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Aptos 2019 blindness detection</title>
		<author>
			<persName><forename type="first">Maggie</forename><surname>Karthik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename></persName>
		</author>
		<ptr target="https://kaggle.com/competitions/aptos2019-blindness-detection" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1718" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A deep multi-task learning approach to skin lesion classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03527</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fighting class imbalance with contrastive learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87199-4_44</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87199-444" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12903</biblScope>
			<biblScope unit="page" from="466" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2021-07">July 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AI in health and medicine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="38" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Shamshad</surname></persName>
		</author>
		<title level="m">Transformers in medical imaging: a survey. arXiv</title>
		<imprint>
			<date type="published" when="2022-01">January 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An interpretable classifier for high-resolution breast cancer screening images utilizing weakly supervised localization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">101908</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">High-fidelity guided image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.17084</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: a versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Diffusion models for implicit image segmentation ensembles</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wolleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valmaggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1336" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ProCo: prototype-aware contrastive learning for long-tailed medical image classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_17</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-117" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
	<note>MICCAI 2022</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02262</idno>
		<title level="m">Infovae: information maximizing variational autoencoders</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
