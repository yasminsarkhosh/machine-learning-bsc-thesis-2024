<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detection of Basal Cell Carcinoma in Whole Slide Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongyan</forename><surname>Xu</surname></persName>
							<email>hongyan.xu@unsw.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of New South Wales</orgName>
								<address>
									<settlement>Kensington</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Data61</orgName>
								<orgName type="institution">The Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dadong</forename><surname>Wang</surname></persName>
							<email>dadong.wang@csiro.au</email>
							<affiliation key="aff1">
								<orgName type="department">Data61</orgName>
								<orgName type="institution">The Commonwealth Scientific and Industrial Research Organisation</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arcot</forename><surname>Sowmya</surname></persName>
							<email>a.sowmya@unsw.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of New South Wales</orgName>
								<address>
									<settlement>Kensington</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Katz</surname></persName>
							<email>ian.katz@southernsun.com.au</email>
							<affiliation key="aff2">
								<orgName type="institution">Southern Sun Pathology Pty Ltd</orgName>
								<address>
									<settlement>Thornleigh</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detection of Basal Cell Carcinoma in Whole Slide Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="263" to="272"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">4936FD61DFB8A3BD1B17DF09F28C3482</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_26</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Basal cell carcinoma (BCC) is a prevalent and increasingly diagnosed form of skin cancer that can benefit from automated whole slide image (WSI) analysis. However, traditional methods that utilize popular network structures designed for natural images, such as the ImageNet dataset, may result in reduced accuracy due to the significant differences between natural and pathology images. In this paper, we analyze skin cancer images using the optimal network obtained by neural architecture search (NAS) on the skin cancer dataset. Compared with traditional methods, our network is more applicable to the task of skin cancer detection. Furthermore, unlike traditional unilaterally augmented (UA) methods, the proposed supernet Skin-Cancer net (SC-net) considers the fairness of training and alleviates the effects of evaluation bias. We use the SC-net to fairly treat all the architectures in the search space and leveraged evolutionary search to obtain the optimal architecture for a skin cancer dataset. Our experiments involve 277,000 patches split from 194 slides. Under the same FLOPs budget (4.1G), our searched ResNet50 model achieves 96.2% accuracy and 96.5% area under the ROC curve (AUC), which are 4.8% and 4.7% higher than those with the baseline settings, respectively. Keywords: Basal cell carcinoma (BCC) • whole-slide pathological images • deep learning • neural architecture search (NAS)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Skin cancer, the most prevalent cancer globally, has seen increasing incidences over recent decades <ref type="bibr" target="#b0">[1]</ref>. It constitutes a third of all cancer diagnoses, affecting one in five Americans <ref type="bibr" target="#b1">[2]</ref>. Basal cell carcinoma (BCC), comprising 70% of cases, has surged by 20-80% in the last 30 years, exerting a significant healthcare strain. Timely BCC diagnosis is crucial to avoid complex treatments. Although histological evaluation remains the gold standard for detection <ref type="bibr" target="#b2">[3]</ref>, deep learning and computer vision advancements can streamline this process. Scanned traditional histology slides result in whole slide images (WSIs) that can be analyzed by deep learning models, significantly easing the histological evaluation burden. Recent advancements underscore the promise of this approach <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Existing skin cancer detection methods <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> typically employs models like Inception Net and ResNet, designed for natural images like those in the ImageNet dataset. The significant variance in pathology and natural images can compromise these models' accuracy. Neural architecture search (NAS) addresses this issue by auto-designing superior models <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, exploring a vast architecture space. However, current NAS methods often overlook fairness in architecture ranking, impeding the discovery of top-performing models.</p><p>In this study, we utilized the NAS approach to identify the optimal network for skin cancer detection. To improve the efficiency and accuracy of the search, we developed a new framework named SC-net, which focuses on identifying highly valuable architectures. We observed that conventional NAS methods often overlook fairness ranking during the search, hindering the search for optimal solutions. Our SC-Net framework addresses this by ensuring fair training and precise ranking. The efficacy of SC-net was confirmed by our experimental results, with our ResNet50 achieving 96.2% top-1 accuracy and 96.5% AUC, outperforming baseline methods by 4.8% and 4.7% respectively.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the proposed framework, which integrates two modules. Module (a) extracts the region of interest (ROI) from WSI and generates patches, while Module (b) uses optimal model architecture from NAS to analyze features from patches and generate classifications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>The proposed method (Fig. <ref type="figure" target="#fig_1">2</ref>) involves dividing the input WSI into patches for training a supernet and the search for optimal architectures <ref type="bibr" target="#b13">[14]</ref>. Section 2.2 provides further details about the supernet. A balanced evolutionary algorithm is then used to select the optimal structure from the search space, with the candidate structures' performance evaluated using mini-batch patch data. We evaluate the searched architectures on the skin cancer dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">One-Shot Channel Number Search</head><p>To extract an optimal architecture γ ∈ G from a vast search space G, a weightsharing strategy <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> is used to prevent training from scratch. The search leverages a supernet S with weights W, with each path γ inheriting weights from W. This makes one-shot NAS a two-step optimization process: supernet training and architecture search. The original dataset is typically split into training D t and validation datasets Dv. The weights W of the supernet S are trained by uniformly sampling the network width d and optimizing the sub-network with weights wd ⊂ W. The optimization function is defined as follows:</p><formula xml:id="formula_0">W * = arg min w d ∈W E d∈U (D) [[L t (w d ; S, d, D t ]])<label>(1)</label></formula><p>where U (D) is a uniform distribution of network widths, E is the expected value of random variables, and L t is the training loss function. Then, the optimal network width d * corresponds to the network width with the best performance (e.g. classification accuracy) on the validation dataset, i.e.,</p><formula xml:id="formula_1">d * = arg max d∈D Acc(d, w * d ; W * , S, D v ), s.t. F LOP s(d) ≤ F p , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where F p is the resource budget of FLOPs. The search for Eq. 2 can be efficiently performed by various algorithms, such as random or evolutionary search <ref type="bibr" target="#b17">[18]</ref>. Afterward, the performance of the searched optimal width d * is analyzed by training from scratch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SC-Net as a Balanced Supernet</head><p>Current approaches for neural architecture search <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> often employ a unilaterally augmented (UA) principle to evaluate each width, resulting in unfair training of channels in the supernet. As illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>(a), to search for a dimension d at a layer with a maximum of n channels, the UA principle assigns the left d channels in the supernet to indicate the corresponding architecture as</p><formula xml:id="formula_3">γ A (d) = [1 : d] , d ≤ n (3)</formula><p>where γ A (d) means the selected d channels from the left (smaller-index) side. However, the UA principle leads to channel training imbalance in the supernet due to its constraints, as illustrated in Fig. <ref type="figure" target="#fig_2">3(a)</ref>. Channels with smaller indices are used for various sizes, resulting in over-training of the left channel kernels since widths are uniformly sampled. The unfairness can be quantified by T , which represents how often a channel is utilized, reflecting its level of training. Given a layer has a maximum of n channels, the T for the i-th channel under the UA principle is</p><formula xml:id="formula_4">T (i) = n -i + 1 (4)</formula><p>Correspondingly, the probability of i -th channel being trained can be expressed as P i = n-i+1 n . Therefore, channels closer to the left will get more attempts during training, which leads the degree of training to vary widely between channels. This introduces evaluation bias and leads to sub-optimal results.</p><p>To mitigate evaluation bias on width, we propose a new SC-net that promotes the fairness of channels during training. As shown in Fig. <ref type="figure" target="#fig_2">3(b)</ref>, in the proposed supernet, each width is simultaneously evaluated by the sub-networks corresponding to the left and right channels. This can be seen as two identical networks S l and S r that are bilaterally coupled and evaluated using the UA principle, but counting channels in reverse order. Therefore, the number of all channels used for evaluating the width d can be expressed as:</p><formula xml:id="formula_5">H(d) = H l UA (d) H r UA (d) (5) = [1 : d] [(n -d + 1) : (n -d)],<label>(6)</label></formula><p>where represents the union of two lists with repeatable elements. In detail, the left channel in S l follows the same UA principle setup as in Eq. ( <ref type="formula">3</ref>), while for the right channel in S r , we count channels from the right H r (d) = [(nd + 1) :</p><p>(nd)]. Therefore, the training degree of each channel is the sum of the two supernets S l and S r . Since the channels are counted from the right within S r , the training degree of the d-th channel on the left corresponds to the training degree of the (n-d+1)-th channel on the right in Eq. ( <ref type="formula">4</ref>). Therefore, the training degree T (d) of the d-th channel in our proposed method is</p><formula xml:id="formula_6">T (d) = T UA (d) + T UA (n + 1 -d) (7) = (n -d + 1) + (n + 1 -n -1 + d) = n + 1 (8)</formula><p>Therefore, the training degree T for each channel will always be equal to the same constant value of the width, independent of the channel index, ensuring fairness in terms of channel (filter) levels. Thus the network width can be fairly ranked using our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Balanced Evolutionary Search with SC-Net</head><p>Using a trained SC-net, the architecture can be evaluated. However, the search space involved in NAS is large, with more than 10 20 possible architectures, requiring an evolutionary search using the multi-objective NSGA-II algorithm to improve the search performance. During the evolutionary search, the width d of each network is represented by the average precision of its corresponding left and right paths in the supernet S, as shown in Eq. ( <ref type="formula" target="#formula_7">9</ref>). The optimal width (not subnetwork) is determined as the one that achieves the best performance when trained from scratch. Here, S l and S r refer to the two paths of S that correspond to the width d during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acc(W, d, D</head><formula xml:id="formula_7">v ) = 1 2 (Acc(S l , d; D v ) + Acc(S r , d; D v ))<label>(9)</label></formula><p>3 Experiments The dataset, comprised of 194 skin slides acquired from the Southern Sun Pathology laboratory, includes 148 BCC cases and 46 other types (common nevus, SCC), all manually annotated by a dermatopathologist. BCC slides served as positive samples and the rest as negatives. These slides were scanned at ×20 magnification with a 0.44 µm pixel size using a Leica Aperio AT2 Scanner. The patient data were separated between training and testing to prevent overlap. Details are shown in Table <ref type="table" target="#tab_0">1</ref>. The experimental setup involved training models on two NVIDIA RTX A6000 GPUs using PyTorch. These models, initialized from a zero-mean Gaussian with standard deviation σ = 0.001, were trained for 200 epochs with a batch size of 256. Training used the Adam optimizer with a dynamic learning rate reduction strategy, starting with a learning rate of 5e-5 following a cosine schedule. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Evaluation</head><p>We validated our algorithm using the curated skin cancer dataset and SC-net as a supernet, testing both heavy and light models. We performed a search on ResNet50 and MobileNetV2 models, compared against original ResNet50 (ori ResNet50) and MobileNetV2 (ori MobileNetV2) models as baselines. The resulting models are denoted as s ResNet50 and s MobileNetV2.</p><p>Comparison with Related Methods. To ensure a fair comparison on our dataset, we selected several papers in the field of pathological image analysis, such as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, as well as others using the UA principle, such as <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Evaluation Metrics. Our model was evaluated on: As shown in Table <ref type="table" target="#tab_1">2</ref>, the s ResNet50 model outperformed in all metrics, showing 4.8%, 4.5%, 5.4%, 4.7% and 4.7% improvements in accuracy, sensitivity, specificity, F 1 Score, and AUC, respectively, over ori ResNet50, and surpassing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>Effect of SC-net as a Supernet.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this paper, we introduce SC-net, a novel NAS framework for skin cancer detection in pathology images. By formulating SC-net as a balanced supernet, we ensure fair ranking and treatment of all potential architectures. With SCnet and evolutionary search, we obtained optimal architectures, achieving 96.2% Top-1 and 96.5% accuracy on a skin cancer dataset, improvements of 4.8% and 4.7% over baselines. Future work will apply our approach to larger datasets for wider-scale validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Compliance with Ethical Standards</head><p>This study was performed in line with the principles of the Declaration of Helsinki. Ethics approval was granted by CSIRO Health and Medical Human Research Ethics Committee (CHMHREC). The ethics approval number is 2021 030 LR, and the validity period is from 07 Apr 2021 to 31 Dec 2024.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overall framework of the proposed model. (a) Region of interest (ROI) extraction and patch generation, and (b) patch detection and WSI classification.</figDesc><graphic coords="2,60,81,143,96,302,20,192,49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The schematic diagram of the proposed method. Module (a) extracts ROI from WSI, creating 224 × 224 patches. Module (b) uses these patches to train and search optimal structure within a supernet via an evolutionary algorithm, yielding dataset predictions.</figDesc><graphic coords="3,61,47,99,11,329,62,121,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Schematic diagram comparing (a) the UA method and (b) our method. In the UA principle, some channels are trained twice while others are trained only once or not at all, leading to channel training unfairness and evaluation bias. In contrast, our proposed method ensures that all channels are trained evenly (twice) by training both the width d and its complementary width.</figDesc><graphic coords="4,56,31,182,45,311,26,148,18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example of prediction probability heatmaps produced by the s ResNet50 model. Column (a) shows the original WSIs that contain annotations represented by green circles. Column (b) displays the prediction probability heatmaps of our ResNet50 model. (Color figure online)</figDesc><graphic coords="5,119,46,54,47,213,58,172,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 1 )</head><label>1</label><figDesc>Accuracy (Acc): percentage of correct classifications. (2) Sensitivity (Se): proportion of true positives identified. (3) Specificity (Sp): proportion of true negatives identified. (4) F1 Score (F1): precision and recall's harmonic mean, indicating label alignment. (5) AUC: ROC curve area, reflecting the false/true positive rate trade-off.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Generated dataset split</figDesc><table><row><cell cols="3">BCC-positive BCC-negative</cell></row><row><cell cols="3">Training WSI Patch WSI Patch</cell></row><row><cell cols="2">118 132,981 37</cell><cell>90,291</cell></row><row><cell cols="3">Testing WSI Patch WSI Patch</cell></row><row><cell>30</cell><cell>31,651 9</cell><cell>22,838</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison on skin cancer dataset</figDesc><table><row><cell>Type</cell><cell>Model</cell><cell cols="3">FLOPs Parameters Acc Se</cell><cell>Sp</cell><cell>F1</cell><cell>AUC</cell></row><row><cell cols="2">WSI analysis-Related Tian et al. [22]</cell><cell>4.1G</cell><cell>25.5M</cell><cell cols="3">92.5 92.3 92.1 92.2 92.7</cell></row><row><cell></cell><cell>Hekler et al. [9]</cell><cell>4.1G</cell><cell>25.5M</cell><cell cols="3">93.4 92.9 92.2 92.8 93.5</cell></row><row><cell></cell><cell>Jiang et al. [23]</cell><cell>2.9G</cell><cell>27.2M</cell><cell cols="3">92.1 91.2 91.7 91.4 92.4</cell></row><row><cell>NAS-Related</cell><cell cols="2">MetaPruning [18] 4.1G</cell><cell>25.5M</cell><cell cols="3">94.1 93.2 92.7 93.3 94.2</cell></row><row><cell></cell><cell>AutoSlim [24]</cell><cell>4.1G</cell><cell>25.5M</cell><cell cols="3">93.8 92.9 92.2 93.0 93.0</cell></row><row><cell>Ours</cell><cell>ori ResNet50</cell><cell>4.1G</cell><cell>25.5M</cell><cell cols="3">91.4 90.2 90.4 90.5 91.8</cell></row><row><cell></cell><cell>s ResNet50</cell><cell>4.1G</cell><cell>27.2M</cell><cell cols="3">96.2 94.7 95.8 95.2 96.5</cell></row><row><cell></cell><cell cols="2">ori MobileNetV2 300M</cell><cell>3.5M</cell><cell cols="3">86.7 86.2 85.8 86.2 87.3</cell></row><row><cell></cell><cell>s MobileNetV2</cell><cell>300M</cell><cell>4.0M</cell><cell cols="3">91.9 90.4 91.7 90.7 92.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance of searched models with different searching methods. Figure 4 demonstrates an example of the prediction probability heatmaps produced by the s ResNet50 model. A comparison of the labeled areas in Column (a) and the red areas in Column (b) indicates that the predicted areas are generally similar in scope to the corresponding labeled areas. This suggests that the model is accurately identifying regions of interest for BCC diagnosis.</figDesc><table><row><cell cols="2">Evaluator Searching</cell><cell>Models</cell><cell></cell><cell></cell></row><row><cell>SC-net</cell><cell>Greedy search Evolutionary</cell><cell>ResNet50</cell><cell></cell><cell cols="2">MobileNetV2</cell></row><row><cell></cell><cell>search</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Acc Se</cell><cell>Sp</cell><cell>AUC Acc Se</cell><cell>Sp</cell><cell>AUC</cell></row><row><cell></cell><cell></cell><cell cols="4">93.8 92.9 92.2 93.0 89.7 90.3 89.9 90.2</cell></row><row><cell></cell><cell></cell><cell cols="4">94.9 93.7 93.1 95.3 90.7 89.8 90.0 91.2</cell></row><row><cell></cell><cell></cell><cell cols="4">94.0 93.5 92.7 94.2 89.8 90.2 90.5 90.5</cell></row><row><cell></cell><cell></cell><cell cols="4">96.2 94.7 95.8 96.5 91.9 90.4 91.7 92.4</cell></row><row><cell cols="6">the Hekler et al. [9] method by 2.8%, 1.8%, 3.6%, 2.4%, and 3.0%. It also com-</cell></row><row><cell cols="6">pared favorably to the s MobileNetV2, adding only 1.7M and 0.5M parameters,</cell></row><row><cell cols="6">respectively. In terms of UA principle method, s ResNet50 advanced by 2.1%,</cell></row><row><cell cols="6">1.5%, 3.1%, 1.9%, and 2.3% in the same metrics compared to MetaPruning [18],</cell></row><row><cell cols="6">showcasing our method's efficacy while keeping model complexity reasonable.</cell></row><row><cell cols="3">Visualization of Probability Heatmaps.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>We tested our model's generalization on the ChestMNIST and DermaMNIST subsets of MedMNISTv2<ref type="bibr" target="#b24">[25]</ref>, following established protocols. As shown in Table4, our s ResNet50 surpassed the original ResNet50 on all datasets, gaining 2.3% and 1.8% more AUC on ChestMNIST and DermaMNIST respectively, proving the model's robust generalization.</figDesc><table><row><cell>presents our experiments testing the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Performance comparison on ChestMNIST and DermaMNIST datasets</figDesc><table><row><cell>Model</cell><cell cols="2">ChestMNIST DermaMNIST</cell></row><row><cell></cell><cell cols="2">AUC ACC AUC ACC</cell></row><row><cell>ResNet18</cell><cell>76.8 94.7</cell><cell>91.7 73.5</cell></row><row><cell>ResNet50</cell><cell>76.9 94.7</cell><cell>91.3 73.5</cell></row><row><cell>auto-sklearn</cell><cell>64.9 77.9</cell><cell>90.2 71.9</cell></row><row><cell>AutoKeras</cell><cell>74.2 93.7</cell><cell>91.5 74.9</cell></row><row><cell cols="2">Google AutoML 77.8 94.8</cell><cell>91.4 76.8</cell></row><row><cell>s ResNet50</cell><cell>79.2 95.5</cell><cell>93.1 77.8</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incidence estimate of nonmelanoma skin cancer (keratinocyte carcinomas) in the us population</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Weinstock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Coldiron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA Dermatol</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1081" to="1086" />
			<date type="published" when="2012">2012. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Prevalence of a history of skin cancer in 2007: results of an incidencebased model</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arch. Dermatol</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="282" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CAMEL: a weakly supervised learning framework for histopathology image segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10682" to="10691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data-efficient and weakly supervised computational pathology on whole-slide images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="555" to="570" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cluster-to-conquer: a framework for end-to-end multi-instance learning for whole slide image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ehsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Moskaluk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brown</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Imaging with Deep Learning</title>
		<imprint>
			<biblScope unit="page" from="682" to="698" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DSNet: a dual-stream framework for weakly-supervised gigapixel pathology image analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2180" to="2190" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automated analysis and diagnosis of skin melanoma on whole slide histopathological images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2738" to="2750" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmentation and classification of melanoma and nevus in whole slide images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Zon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="263" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pathologist-level classification of histopathological melanoma images with deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hekler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Cancer</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="79" to="83" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BCNet: searching for network width with bilaterally coupled network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2175" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-scale fusion with matching attention model: a novel decoding network cooperated with NAS for real-time semantic segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Intell. Transp. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="12622" to="12632" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13700</idno>
		<title level="m">Vision transformer architecture search</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Yun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03916</idno>
		<title level="m">Supernet in neural architecture search: a taxonomic survey</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">DARTS: differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weight-sharing neural architecture search: a battle to shrink the optimization gap</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv. (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fitting the search space of weight-sharing NAS with graph convolutional networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="7064" to="7072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Metapruning: meta learning for automatic neural network channel pruning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3296" to="3305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Autoformer: searching transformers for visual recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12270" to="12280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fbnetv2: differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12965" to="12974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">FP-NAS: fast probabilistic neural architecture search</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15139" to="15148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Computer-aided detection of squamous carcinoma of the cervix in whole slide images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10959</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing basal cell carcinoma on smartphone-captured digital histopathology images with a deep neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br. J. Dermatol</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="754" to="762" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Autoslim: towards one-shot architecture search for channel numbers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11728</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MedMNIST v2-a large-scale lightweight benchmark for 2D and 3D biomedical image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
