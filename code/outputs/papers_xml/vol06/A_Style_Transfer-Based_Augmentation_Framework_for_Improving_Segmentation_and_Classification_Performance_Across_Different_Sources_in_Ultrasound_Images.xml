<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bin</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ziyue</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nvidia Corporation</orgName>
								<address>
									<postCode>20814</postCode>
									<settlement>Bethesda</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shing-Chow</forename><surname>Chan</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">SAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University Medical School</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huiying</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University Medical School</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University Medical School</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qicai</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University Medical School</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meiqin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University Medical School</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Changfeng</forename><surname>Dong</surname></persName>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Institute of Hepatology</orgName>
								<orgName type="department" key="dep2">Shenzhen Third People&apos;s Hospital</orgName>
								<address>
									<postCode>518000</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Zeng</surname></persName>
							<affiliation key="aff6">
								<orgName type="department" key="dep1">Department of Medical Ultrasonics</orgName>
								<orgName type="department" key="dep2">The Third Affiliated Hospital</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<postCode>510000</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruhai</forename><surname>Zou</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">State Key Laboratory of Oncology in South China</orgName>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="department" key="dep1">Collaborative Innovation Center of Cancer Medicine</orgName>
								<orgName type="department" key="dep2">Department of Ultrasound</orgName>
								<orgName type="institution">Sun Yat-sen University Cancer Center</orgName>
								<address>
									<postCode>510000</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bingsheng</forename><surname>Huang</surname></persName>
							<email>huangb@szu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University Medical School</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="department" key="dep1">Medical AI Lab</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit1">Shenzhen University Medical School</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<postCode>518000</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="department" key="dep1">Medical AI Lab</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering</orgName>
								<orgName type="institution" key="instit1">Shenzhen University Medical School</orgName>
								<orgName type="institution" key="instit2">Shenzhen University</orgName>
								<address>
									<postCode>518000</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
							<email>chenxin@szu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Shenzhen University Medical School</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<postCode>518060</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuo</forename><surname>Li</surname></persName>
							<affiliation key="aff10">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Case Western Reserve University</orgName>
								<address>
									<settlement>Cleveland</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Style Transfer-Based Augmentation Framework for Improving Segmentation and Classification Performance Across Different Sources in Ultrasound Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="44" to="53"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">1D0FC9D5E9449B5BA06BC771CB0DEDE9</idno>
					<idno type="DOI">10.1007/978-3-031-43987-2_5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ultrasound</term>
					<term>Segmentation</term>
					<term>Classification</term>
					<term>Style transfer</term>
					<term>Data augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ultrasound imaging can vary in style/appearance due to differences in scanning equipment and other factors, resulting in degraded segmentation and classification performance of deep learning models for ultrasound image analysis. Previous studies have attempted to solve this problem by using style transfer and augmentation techniques, but these methods usually require a large amount of data from multiple sources and source-specific discriminators, which are not feasible for medical datasets with limited samples. Moreover, finding suitable augmentation methods for ultrasound data can be difficult. To address these challenges, we propose a novel style transfer-based augmentation framework that consists of three components: mixed style augmentation (MixStyleAug), feature augmentation (FeatAug), and mask-based style augmentation (MaskAug). MixStyleAug uses a style transfer network to transform the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>style of a training image into various reference styles, which enriches the information from different sources for the network. FeatAug augments the styles at the feature level to compensate for possible style variations, especially for small-size datasets with limited styles. MaskAug leverages segmentation masks to highlight the key regions in the images, which enhances the model's generalizability. We evaluate our framework on five ultrasound datasets collected from different scanners and centers. Our framework outperforms previous methods on both segmentation and classification tasks, especially on small-size datasets. Our results suggest that our framework can effectively improve the performance of deep learning models across different ultrasound sources with limited data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Classification and segmentation are two common tasks that use deep learning techniques to solve clinical problems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. However, training deep learning models reliably usually requires a large amount of data samples. Models trained with limited data are susceptible to overfitting and possible variations due to small Fig. <ref type="figure">1</ref>. Limitations of previous studies and our improvements for multi-source ultrasound data.</p><p>sample size, which can lead to poor performance across different sources. Different sources refer to the same modality collected from different scanners. In medical imaging, one of the main reasons for poor performance is the variation in the imaging process, such as the type of scanner, the settings, the protocol, etc. <ref type="bibr" target="#b2">[3]</ref>. This can cause changes in the intensity distributions of the images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. While training deep learning models with a large number of high-quality data could potentially address this problem, this approach is often challenging due to limited resources and difficulties in collecting medical images, as well as the manual annotation required by experienced radiologists or experts with professional domain knowledge. Thus, limited labeled data is commonly used to model the classification and segmentation network.</p><p>To prevent overfitting and improve generalization, data augmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> has been proposed to generate more similar but different samples for the training dataset. Very often, this can be done by applying various transformations to the training images to create new images that reflect natural variations within each class. However, the model's performance across different sources heavily depends on the augmentation strategies. Another popular technique is style transfer <ref type="bibr" target="#b10">[11]</ref>, which adapts the style of test images to match the selected reference images (standard distributions) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. However, these methods have a limitation that their performance depends on the quality of the reference images. Moreover, these methods tend to transfer the style of the whole images, which may introduce irrelevant distribution information for medical imaging applications, as shown in Fig. <ref type="figure">1</ref>. This problem is more severe in ultrasound images due to the presence of acoustic shadow.</p><p>To address the above challenges, we propose a novel framework that combines the advantages of data augmentation and style transfer to enhance the model's segmentation and classification performance on ultrasound images from different sources. Our contributions (Fig. <ref type="figure">1</ref>) are: 1) a mixed style augmentation strategy that integrates the information from different sources to improve the model's generalizability. 2) A feature-based augmentation that shifts the style at the feature level rather than the image level to better account for the potential variations. 3) a mask-based style augmentation strategy that avoids the influence of the irrelevant style information on ultrasound images during the style transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Our proposed framework for ultrasonic image style augmentation consists of three stages, as illustrated in Fig.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mixed Style Augmentation (MixStyleAug)</head><p>To improve the performance of the multi-task network, we design MixStyleAug, combining traditional transformations and style transfer to incorporate image information from target sources during training (Fig. <ref type="figure" target="#fig_1">2A</ref>). In this method, the content and the style images are sampled from training and target sources, respectively. Firstly, the traditional augmentation is applied to transform the content image, which can prevent overfitting. The traditional augmentation includes rotation, translation, scaling, and deformation transformations. Next, we translate the style of the augmented content image to that of the style image using the WCT 2 [14] style transfer network, generating a stylized content image. Finally, inspired by AugMix <ref type="bibr" target="#b14">[15]</ref>, we mix the stylized and augmented content images using random weights to create a style-augmented image that includes information from the training source. MixStyleAug allows the augmented training dataset to implicitly contain information from multiple sources, improving the model's performance across different sources. However, this method requires a large number of available images as reference styles for style augmentation, making it impractical for small-sized datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Architecture and Feature Augmentation (FeatAug)</head><p>To address the limitation of MixStyleAug in small-size medical datasets, FeatAug is applied for augmenting image styles at the feature level during the network training (Fig. <ref type="figure" target="#fig_1">2B</ref>). In this work, we design a simple multi-task network for simultaneous segmentation and classification, and FeatAug is applied to the feature maps for feature augmentation.</p><p>The architecture of our designed multi-task network (Fig. <ref type="figure">S1</ref> in the Supplementary Materials) includes four encoders, four decoders, and a classification head. Each encoder includes two 3 × 3 convolutional layers with padding that are used to fuse the features. Each convolutional layer is followed by a rectified linear unit (ReLU) and a batch normalization (BN) <ref type="bibr" target="#b15">[16]</ref>. Max-pooling layer is used to downsample the feature maps for dimension reduction. Through these encoders, the feature maps are generated and fed into the decoders and classification head to generate segmentation and classification results, respectively. Each decoder consists of three 3 × 3 convolutional layers with padding, three BN layers, three ReLUs, and a max-unpooling layer. In the classification head, the feature maps from the encoders are reduced to 128 channels by using a 3 × 3 convolutional layer with padding followed by ReLU and BN layer. Then, a global average pooling is used to downsample the feature maps. Finally, the features are fed into a fully connected layer followed by a sigmoid layer to output the classification result.</p><p>Previous studies reported that changing the mean and standard deviation of the feature maps could lead to different image styles <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Thus, we design a module to randomly alter these values to augment the styles at the feature level. To avoid over-augmentation at the feature level, this module is randomly applied with a 50% probability after the residual connection in each encoder. The module is defined as follows:</p><formula xml:id="formula_0">A = A -μ A σ A • σ A + N (μ, σ) + μ A + N (μ, σ)<label>(1)</label></formula><p>where A indicates the feature map, A indicates the augmented feature map, μ A indicates the mean of feature map A, σ A indicates the standard deviation of feature map A, and N (μ, σ) indicates a value randomly generated from a normal distribution with mean μ and standard deviation σ. In this study, the μ and σ of the normal distribution were empirically set to 0 and 0.1 according to preliminary experimental results, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mask-Based Style Augmentation (MaskAug)</head><p>In general, the style transfer uses the style information of the entire image, but this approach may not be ideal when the regions outside of the ROIs contain conflicting style information as compared to the regions within the ROIs, as illustrated in Fig. <ref type="figure">1</ref>. To mitigate the impact of irrelevant or even adverse style information, we propose a mask-based augmentation technique (MaskAug) that emphasize the ROIs in the ultrasound image during style transfer network training.</p><p>Figure <ref type="figure" target="#fig_1">2C</ref> shows the pipeline of MaskAug and the steps are: 1) Content and style images are randomly chosen from training and target sources, respectively. 2) A trained multi-task network, which has been trained for several epochs and will be updated in the later epochs, is used to automatically generate ROIs of these images. 3) The content image, style image and their ROIs are input to the style transfer network. 4) During the style transfer, the intensity distribution of the ROI in the content image is changed to that of the style image. 5) Finally, mask-based style augmented images are produced and these images are then input to the multi-task network for further training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Loss Function and Implementation Details</head><p>We utilized cross-entropy (CE) as the primary loss function for segmentation and classification during the training stage. Additionally, Dice loss <ref type="bibr" target="#b18">[19]</ref> was computed as an auxiliary loss for segmentation. These loss functions are defined as:</p><formula xml:id="formula_1">L m = L Seg CE + L Seg Dice + L Cls CE (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where L CE denotes CE loss, L Dice denotes Dice loss, L m denotes the loss for the multi-task network optimization, L Seg denotes the loss computed from the segmentation result, and L Cls denotes the loss computed from the classification result. We adopted Pytorch to implement the proposed framework, and the multitask network was trained on Nvidia RTX 3070 with 8 GB memory. During training, the batch size was set to 16, the maximum epoch number was 300, and the initial learning rate was set to 0.0005. We decayed the learning rate with cosine annealing <ref type="bibr" target="#b19">[20]</ref> for each epoch, and the minimum learning rate was set to 0.000001. The restart epoch of cosine annealing was set to 300, ensuring that the learning rate monotonically decreased during the training process. For optimization, we used the AdamW optimizer <ref type="bibr" target="#b20">[21]</ref> in our experiments. The whole training takes about 6 h and the inference time for a sample is about 0.2 s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results and Discussion</head><p>Datasets and Evaluation Metrics. We evaluated our framework on five ultrasound datasets (each representing a source) collected from multiple centers using different ultrasound scanners, including three liver datasets and two thyroid nodules datasets. A detailed description of the collected datasets is provided in Table <ref type="table" target="#tab_0">S1</ref> of the Supplementary Materials. We used the dataset with the largest sample size as the training source to prevent overfitting, while the other datasets were the target sources. For each datasets, we randomly split 20% of the samples for test, and used the remaining 80% for training the network. All the results in this study are based on the test set. In the training set, 20% data was randomly selected as validation set. In the data preprocessing, the input images were resized to 224×224 and were normalized by dividing 255.</p><p>AUROC is used to evaluate the classification performance. DSC is used to assess the performance of the segmentation. The DSC is defined as:</p><formula xml:id="formula_3">DSC = 2T P F P + 2T P + F N<label>(3)</label></formula><p>where T P refers to the pixels where both the predicted results and the gold standard are positive, F P refers to the pixels where the predicted results are positive and the gold standard are negative, and F N refers to the pixels where the predicted results are negative and the gold standard are positive. Ablation Study. We evaluated the effects of MixStyleAug, FeatAug, and MaskAug by training a multi-task network with different combinations of these augmentation strategies. Table <ref type="table" target="#tab_0">1</ref> shows that MixStyleAug improves the segmentation and classification performance on the target sources compared to traditional augmentation. Furthermore, The combination of FeatAug and MixStyleAug improves the classification performance slightly in the liver datasets and significantly in the thyroid nodule datasets. This improvement is due to the style transfer at the feature level, which make the augmented features more similar to the target sources. Using MaskAug improved both segmentation and classification performance on both training and target sources, compared to the combination of FeatAug and MixStyleAug. This resulted in excellent performance. Figure <ref type="figure" target="#fig_2">3</ref> shows that the mask-based stylized content image has a more similar distribution to the style image than the other images, which helps the model perform better on both training and target sources.</p><p>Comparison with Previous Studies. We compared our proposed method with BigAug <ref type="bibr" target="#b2">[3]</ref>, the style augmentation method by Hesse et al. <ref type="bibr" target="#b7">[8]</ref>, AutoAug <ref type="bibr" target="#b9">[10]</ref>, and UDA <ref type="bibr" target="#b21">[22]</ref> on our collected datasets. Table <ref type="table" target="#tab_1">2</ref> shows that our method performs excellently on both training and target sources. Unlike BigAug <ref type="bibr" target="#b2">[3]</ref>, our method uses style augmentation instead of intensity transformations, which avoids a drop in classification performance. Hesse et al. <ref type="bibr" target="#b7">[8]</ref> only uses training sources for style  augmentation, which fail to improve performance on target sources, especially in classification tasks, when using a small-sized, single-source training dataset. Our method outperforms AutoAug <ref type="bibr" target="#b9">[10]</ref>, which relies on large samples to obtain the optimal augmentation strategy. UDA <ref type="bibr" target="#b21">[22]</ref> is hard to train with a small-sized dataset due to overfitting and the complex adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We proposed an augmentation framework based on style transfer method to improve the segmentation and classification performance of the network on ultrasound images from multiple sources. Our framework consists of MixStyleAug, FeatAug, and MaskAug. MixStyleAug integrates the image information from various sources for well generalization, while FeatAug increases the number of styles at the feature level to compensate for potential style variations. MaskAug uses the segmentation results to guide the network to focus on the style information of the ROI in the ultrasound image. We evaluated our framework on five datasets from various sources, and the results showed that our framework improved the segmentation and classification performance across different sources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 .</head><label>2</label><figDesc>Stage A. Mixed style augmentation (MixStyleAug) integrates the style information from different sources simultaneously. Stage B. Feature augmentation transfers the style at the feature level during the training of the multi-task network. Stage C. Mask-based style augmentation uses the style information of the region of interest (ROI) in the ultrasound image based on the segmentation results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our proposed style transfer-based augmentation framework. The whole framework consists of mixed style augmentation, feature augmentation, and mask-based style augmentation.</figDesc><graphic coords="4,58,98,54,35,334,48,222,34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustrations of the conventional style transfer and mask-based style transfer in an ultrasound image. A neural style transfer network is used to translate the content image to the style image, resulting in a stylized image with reference to the style of the entire style image. In contrast, mask-based stylized images are generated with reference to the style of the liver substance in the stylized image. The histogram shows the intensity distribution of the liver region, with μ and σ representing the mean and standard deviation of the liver parenchyma in the ultrasound image, respectively.</figDesc><graphic coords="8,58,98,149,00,334,48,131,62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,58,98,350,30,334,48,215,50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of segmentation and classification performance of different augmentation methods in five ultrasound datasets in terms of DSC (%) and AUROC (×100%). Training/Target: Training/Target source datasets. MixStyleAug: mixed style augmentation. FeatAug: feature augmentation. MaskAug: mask-based style augmentation. LD: liver dataset. TD: thyroid nodule dataset.</figDesc><table><row><cell>Method</cell><cell>Metric</cell><cell>LD1</cell><cell>LD2</cell><cell>LD3</cell><cell>TD1</cell><cell>TD2</cell></row><row><cell></cell><cell></cell><cell cols="5">Training Target Target Training Target</cell></row><row><cell>Traditional Augmentation</cell><cell>DSC</cell><cell>94.5</cell><cell>88.3</cell><cell>89.6</cell><cell>64.0</cell><cell>63.1</cell></row><row><cell></cell><cell cols="2">AUROC 86.6</cell><cell>61.3</cell><cell>65.6</cell><cell>72.6</cell><cell>62.3</cell></row><row><cell>MixStyleAug</cell><cell>DSC</cell><cell>94.0</cell><cell>87.3</cell><cell>91.1</cell><cell>62.8</cell><cell>65.7</cell></row><row><cell></cell><cell cols="2">AUROC 87.9</cell><cell>64.0</cell><cell>68.9</cell><cell>78.1</cell><cell>62.3</cell></row><row><cell>MixStyleAug+FeatAug</cell><cell>DSC</cell><cell>94.0</cell><cell>86.9</cell><cell>90.2</cell><cell>63.9</cell><cell>65.2</cell></row><row><cell></cell><cell cols="2">AUROC 89.7</cell><cell>66.3</cell><cell>68.8</cell><cell>85.5</cell><cell>64.2</cell></row><row><cell cols="2">MixStyleAug+FeatAug+MaskAug DSC</cell><cell>94.8</cell><cell>89.7</cell><cell>91.2</cell><cell>77.9</cell><cell>65.9</cell></row><row><cell></cell><cell cols="2">AUROC 92.3</cell><cell>67.3</cell><cell>69.3</cell><cell>83.0</cell><cell>62.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Segmentation and classification performance of our proposed framework and previous studies in five ultrasound datasets in terms of DSC (%) and AUROC (×100%). Training/Target: Training/Target source datasets. LD: liver dataset. TD: thyroid nodules dataset. UDA: unsupervised domain adaptation.</figDesc><table><row><cell>Method</cell><cell>Metric</cell><cell>LD1</cell><cell>LD2</cell><cell>LD3</cell><cell>TD1</cell><cell>TD2</cell></row><row><cell></cell><cell></cell><cell cols="5">Training Target Target Training Target</cell></row><row><cell>BigAug [3]</cell><cell>DSC</cell><cell>93.8</cell><cell>88.0</cell><cell>90.8</cell><cell>54.5</cell><cell>56.4</cell></row><row><cell></cell><cell cols="2">AUROC 82.1</cell><cell>59.2</cell><cell>51.9</cell><cell>62.7</cell><cell>63.9</cell></row><row><cell>Hesse et al. [8]</cell><cell>DSC</cell><cell>92.6</cell><cell>86.9</cell><cell>91.3</cell><cell>61.4</cell><cell>62.7</cell></row><row><cell></cell><cell cols="2">AUROC 79.6</cell><cell>64.9</cell><cell>68.3</cell><cell>71.7</cell><cell>58.8</cell></row><row><cell>AutoAug [10]</cell><cell>DSC</cell><cell>93.9</cell><cell>87.6</cell><cell>91.4</cell><cell>68.7</cell><cell>53.5</cell></row><row><cell></cell><cell cols="2">AUROC 87.1</cell><cell>65.3</cell><cell>67.8</cell><cell>76.9</cell><cell>39.2</cell></row><row><cell>UDA [22]</cell><cell>DSC</cell><cell>94.2</cell><cell>57.7</cell><cell>64.9</cell><cell>65.1</cell><cell>35.2</cell></row><row><cell></cell><cell cols="2">AUROC 84.6</cell><cell>61.9</cell><cell>67.9</cell><cell>67.2</cell><cell>55.3</cell></row><row><cell cols="2">Proposed method DSC</cell><cell>94.8</cell><cell>89.7</cell><cell>91.2</cell><cell>77.9</cell><cell>65.9</cell></row><row><cell></cell><cell cols="2">AUROC 92.3</cell><cell>67.3</cell><cell>69.3</cell><cell>83.0</cell><cell>62.4</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Suk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="248" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalizing deep learning for medical image segmentation to unseen domains via deep stacked transformation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2531" to="2540" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Remove appearance shift for ultrasound image segmentation via fast and universal style transfer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A universal intensity standardization method based on a manyto-one weak-paired cycle generative adversarial network for magnetic resonance images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2059" to="2069" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Style augmentation: data augmentation via style randomization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Intensity augmentation to improve generalizability of breast segmentation across different MRI scan protocols</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Hesse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="759" to="770" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning domain-agnostic visual representation for computational pathology using medically-irrelevant style transfer augmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yamashita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3945" to="3954" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Autoaugment: learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural style transfer: a review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visual. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3365" to="3385" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image generation by GAN and style transfer for agar plate image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Andreini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Prog. Biomed</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<date type="published" when="2020">105268. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalizing deep models for ultrasound image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00937-3_57</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00937-3_57" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2018</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Schnabel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Alberola-López</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11073</biblScope>
			<biblScope unit="page" from="497" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Photorealistic style transfer via wavelet transforms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02781</idno>
		<title level="m">Augmix: asimple data processing method to improve robustness and uncertainty</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Crossnorm and selfnorm for generalization under distribution shifts</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">V-net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">SGDR: stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
