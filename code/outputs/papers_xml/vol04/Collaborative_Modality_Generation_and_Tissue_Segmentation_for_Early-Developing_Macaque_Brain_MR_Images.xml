<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images</title>
				<funder ref="#_5wZCkZR #_xjdZ4yP">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_M2dWhjc">
					<orgName type="full">Guangzhou Basic and Applied Basic Research Project</orgName>
				</funder>
				<funder ref="#_ntAFjw4">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_esBTpYc">
					<orgName type="full">National Natural Science Foundation of Youth Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xueyang</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<postCode>510515</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Guangdong Provincial Key Laboratory of Medical Image Processing</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<postCode>510515</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Guangdong Province Engineering Laboratory for Medical Imaging and Diagnostic Technology</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<postCode>510515</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Zhong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<postCode>510515</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Guangdong Provincial Key Laboratory of Medical Image Processing</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<postCode>510515</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Guangdong Province Engineering Laboratory for Medical Imaging and Diagnostic Technology</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<postCode>510515</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shujun</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<postCode>510515</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Guangdong Provincial Key Laboratory of Medical Image Processing</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<postCode>510515</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Guangdong Province Engineering Laboratory for Medical Imaging and Diagnostic Technology</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<postCode>510515</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<settlement>Chapel Hill</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gang</forename><surname>Li</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
							<email>yuzhang@smu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<postCode>510515</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Guangdong Provincial Key Laboratory of Medical Image Processing</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<postCode>510515</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Guangdong Province Engineering Laboratory for Medical Imaging and Diagnostic Technology</orgName>
								<orgName type="institution">Southern Medical University</orgName>
								<address>
									<postCode>510515</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<settlement>Chapel Hill</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Collaborative Modality Generation and Tissue Segmentation for Early-Developing Macaque Brain MR Images</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="470" to="480"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">648D367F3DD8A5D063E3446B9F8E3DA8</idno>
					<idno type="DOI">10.1007/978-3-031-43901-8_45</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Tissue segmentation</term>
					<term>Missing-modality generation</term>
					<term>Macaque</term>
					<term>Early brain development</term>
					<term>Multi-task collaboration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In neuroscience research, automatic segmentation of macaque brain tissues in magnetic resonance imaging (MRI) is crucial for understanding brain structure and function during development and evolution. Acquisition of multimodal information is a key enabler of accurate tissue segmentation, especially in early-developing macaques with extremely low contrast and dynamic myelination. However, many MRI scans of early-developing macaques are acquired only in a single modality. While various generative adversarial networks (GAN) have been developed to impute missing modality data, current solutions treat modality generation and image segmentation as two independent tasks, neglecting their inherent relationship and mutual benefits. To address these issues, this study proposes a novel Collaborative Segmentation-Generation Framework (CSGF) that enables joint missing modality generation and tissue segmentation of macaque brain MR images. Specifically, the CSGF consists of a modality generation module (MGM) and a tissue segmentation module (TSM) that are trained jointly by a crossmodule feature sharing (CFS) and transferring generated modality. The training of the MGM under the supervision of the TSM enforces anatomical feature consistency, while the TSM learns multi-modality information related to anatomical structures from both real and synthetic multimodality MR images. Experiments show that the CSGF outperforms the conventional independent-task mode on an early-developing macaque MRI dataset with 155 scans, achieving superior quality in both missing modality generation and tissue segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The precise processing and analysis of brain MR images are critical in advancing neuroscience research. As a widely used animal model with high systematic similarity to humans in various aspects, macaques play an indispensable role in understanding brain mechanisms in development, aging and evolution, exploring the pathogenesis of neurological diseases, and validating the effectiveness of clinical techniques and drugs <ref type="bibr" target="#b0">[1]</ref>. In both humans and macaques, early brain development is a complex, dynamic, and regionally heterogeneous process that plays a crucial role in shaping later brain structural, functional, and cognitive outcomes <ref type="bibr" target="#b18">[19]</ref>. While the human neuroimaging research is advancing rapidly, the macaque neuroimaging research lags behind, partly due to challenges in data acquisition, lack of tailored processing tools, and the inapplicability of human neuroimaging analysis tools to macaques <ref type="bibr" target="#b1">[2]</ref>. Therefore, the macaque neuroimaging research is a rapidly evolving field that demands specialized tools and expertise. Of particular importance is brain tissue segmentation, a crucial prerequisite for quantitative volumetric analysis and surface-based studies, which aim to partition the brain into distinct regions such as white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) <ref type="bibr" target="#b2">[3]</ref>. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the anatomical structures, age-related contrasts, and size of macaque brains undergo significant changes during early development <ref type="bibr" target="#b17">[18]</ref>, posing challenges for accurate tissue segmentation. Especially during the early postnatal months, the WM and GM show extremely low contrast, making their boundaries difficult to detect. The development of automatic and robust tissue segmentation algorithms for macaque brains during early developing stages is thus of great importance.</p><p>Deep learning has emerged as a powerful tool in medical image processing and analysis, with Convolutional Neural Networks (CNNs) showing remarkable performance in various applications, e.g., image segmentation <ref type="bibr" target="#b3">[4]</ref>, cross-modality generation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and multi-task learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> with state-of-the-art results. However, there is currently a lack of dedicated deep learning-based tools for brain tissue segmentation of macaque MRI data during early development, which can be attributed to several reasons. First, collecting macaque brain MRI data is challenging, and there is a dearth of publicly available large-scale labeled datasets for deep learning-based tissue segmentation. Second, brain tissue segmentation in early postnatal stages usually requires multi-modality images to provide more useful information than a single modality <ref type="bibr" target="#b7">[8]</ref>. However, macaque datasets that are publicly available often only contain 3D T1-weighted (T1w) images and lack corresponding tissue labels and 3D T2-weighted (T2w) information. For example, the UW-Madison Rhesus MRI dataset <ref type="bibr" target="#b23">[24]</ref> includes 592 macaque samples with T1w images only. To compensate for the missing modality information, generative adversarial networks (GAN) <ref type="bibr" target="#b12">[13]</ref> have been applied to impute the missing modality in neuroimages, due to their potential for image-to-image translation <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. Among them, pix2pix algorithm first proposes a solution for style transfer from image to image, which can be applied in the modality translation of medical images. Recently PTNet3D transforms the CNN-based frame into a Transformer-based frame, which reduces the computation amount required when processing high-resolution images. In these solutions, modality generation and downstream tasks are treated as two cascading independent tasks, as shown in Fig. <ref type="figure" target="#fig_1">2(a</ref>). However, it is difficult to determine whether the generated missing modality has a natural positive effect on the downstream tasks <ref type="bibr" target="#b10">[11]</ref>. In fact, both modality generation and tissue segmentation tasks require the feature extraction and learning of brain anatomical structures to achieve voxel-wise translation, showing a high task similarity and correlation. Essentially, the tissue map can guide the missing modality generation with anatomical information, while the generated missing modality can also provide useful complementary information for refining tissue segmentation <ref type="bibr" target="#b11">[12]</ref>. To effectively capture and utilize the features and task correlation between two tasks, it is desirable to integrate tissue segmentation and modality generation into a unified framework. This can be achieved by imputing the missing modality image in a tissue-oriented manner, as illustrated in Fig. <ref type="figure" target="#fig_1">2(b)</ref>.</p><p>In this study, we present a 3D Collaborative Segmentation-Generation Framework (CSGF) for early-developing macaque MR images. The CSGF is designed in the form of multi-task collaboration and feature sharing, which enables it to complete the missing modality generation and tissue segmentation simultaneously. As depicted in Fig. <ref type="figure" target="#fig_2">3</ref>(a), the CSGF comprises two modules: a modality generation module (MGM) and a tissue segmentation module (TSM), which are trained collaboratively through two forward information flows. Specifically, MGM and TSM will be linked by transferring generated missing-modality and cross-module feature sharing (CFS), ensuring that the MGM is trained under the supervision of the TSM, thereby imposing a constraint on anatomical feature consistency and providing multi-modality information.</p><p>The proposed CSGF offers several advantages over existing methods: (1) The collaborative learning of both MGM and TSM enables the missing modality to be imputed in a tissue-oriented manner, and hence the generated neuroimages are more consistent with real neuroimages from an anatomical point of view; <ref type="bibr" target="#b1">(2)</ref> The CFS mechanism between the two modules provides more prosperous feature guidance for the TSM, thus also imposing constraints on the anatomical feature consistency of the MGM encoder; (3) The CSGF achieves improvements in both missing modality generation and brain tissue segmentation, especially for infant macaques with exceptionally low contrast between different tissues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset</head><p>The experimental dataset was from the public UNC-Wisconsin Neurodevelopment Rhesus Database <ref type="bibr" target="#b9">[10]</ref>, acquired by a GE MR750 3.0T MRI scanner and covered by animal research protocols approved by relevant Institutional Animal Care and Use committees. A total of 155 developing macaque structural MRI samples between 0 and 36 months of age were used for experiments. Each sample contains T1w image and T2w image with following parameters: T1w, matrix = 256 × 256, resolution = 0.5469 × 0.5469 × 0.8 mm 3 ; T2w, matrix = 256 × 256, resolution= 0.6016×0.6016×0.6 mm 3 . For image preprocessing, FMRIB's Linear Image Registration Tool (FLIRT) in FSL (version 5.0) <ref type="bibr" target="#b19">[20]</ref> was used to rigidly align each T2w image to its corresponding T1w image, followed by resampling all images to 0.5469 mm mm isotropic resolution. Brain skulls were removed using the method in <ref type="bibr" target="#b1">[2]</ref> and intensity inhomogeneity correction was performed using N4 bias correction <ref type="bibr" target="#b24">[25]</ref>. The initial tissue segmentation of each scan was obtained by LINKS <ref type="bibr" target="#b16">[17]</ref>, which was then manually corrected significantly by experienced experts using ITK-SNAP software <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Formulation</head><p>We construct this Collaborative Segmentation-Generation Framework (CSGF) based on the primary modality T1w (denoted as T 1 i ) and the auxiliary modality T2w (denoted as T 2 i ) data. Let {T 1 i , T 2 i , M i } i=1∼N be the dataset consisting of N scans, where T 1 i and T 2 i represent the T1w and T2w of i th scan, respectively; M i represents the tissue label of i th scan. This framework can be formulated as</p><formula xml:id="formula_0">Mi = S(T 1 i , T 2 i ),<label>(1)</label></formula><p>where Mi is the estimated tissue label of the i th scan. However, in practical application, it is often the case that some s only contain the T1w modality. Therefore we construct a modality generation module (MGM) by a mapping function G to generate the missing T2w modality as</p><formula xml:id="formula_1">T2 i = G(T 1 i ),<label>(2)</label></formula><p>where T2 i is the generated T2w based on T1w modality for the i th scan. Then, the tissue segmentation module (TSM) constructs a mapping function S as</p><formula xml:id="formula_2">Mi = S(T 1 i , G(T 1 i )) = S(T 1 i , T2 i ) ≈ S(T 1 i , T 2 i ).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Framework Overview and Collaborative Learning</head><p>The proposed CSGF consists of two key modules: the MGM and the TSM. These two modules are linked through the cross-module feature sharing (CFS), thus being trained collaboratively, as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>(a). The CFS was created by a skip connection between the generator's down-sampling and the segmenter's up-sampling, as depicted in Fig. <ref type="figure" target="#fig_2">3(b</ref>). This connection enables greater interaction and anatomy constraints between the two modules by additional back-propagating the gradient of the loss function.</p><p>The proposed MGM and TSM are trained collaboratively. Given a dataset {T 1  i , T 2 i , M i } i=1∼N , the generator in MGM can be trained as</p><formula xml:id="formula_3">Ĝ = argmin N i=1 { G(T 1 i ) -T 2 i GAN + G(T 1 i ) -T 2 i MSE }, (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where * GAN represents the GAN loss and * MSE represents mean square error (MSE) loss. Meanwhile, the segmenter in TSM can be trained as</p><formula xml:id="formula_5">Ŝ = argmin N i=1 { S(T 1 i , G(T 1 i )) -M i CE + S(T 1 i , G(T 1 i )) -M i DICE }, (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where * CE represents the cross-entropy loss and * DICE represents dice coefficient loss. During testing, for each input scan, if it has both T1w and T2w, we predict its tissue label as Mi = S(T 1 i , T 2 i ). If it only has T1w modality, we first generate its T2w modality by T2 i = G(T 1 i ), and then predict its tissue label as Mi = S(T 1 i , G(T 1 i )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Details</head><p>The total scans of MRI data used in this study was 155, divided into training and test sets in a 4:1 ratio. Before training, the data had the redundant background removed and cropped into a size of 160 × 160 × 160, and then overlapping patches of size 64 were generated. The intensity values of all patches were normalized to [-1, 1]. Both MGM and TSM were optimized simultaneously during training. We employed the Adam optimizer with a learning rate of 0.0002 for the first 50 epochs, after which the learning rate gradually decreased to 0 over the following 50 epochs. In the testing phase, overlapping patches with stride 32 generated from testing volumes were input into the model, and the obtained results were rebuilt to a volume, during which the overlaps between adjacent patches were averaged. All experiments were implemented based on Pytorch 1.13.0 and conducted on NVIDIA RTX 4090 GPUs with 24GB VRAM in Ubuntu 18.04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation of Framework</head><p>Totally 31 scans of early-developing macaques were used for framework evaluation. To provide a comparison baseline, we used PTNet3D (PTNet), one stateof-the-art algorithm for medical image translation, and U-Net <ref type="bibr" target="#b22">[23]</ref>, the most widely used segmentation model, to construct the CSGF. First, we trained two U-Net models as baseline models, one of which used only the T1w modality as input, while the other used both T1w and T2w modalities. Next, we trained a PTNet model, which generates corresponding T2w images based on the input T1w images. Finally, we constructed the CSGF framework using the PTNet and U-Net models described in Sect. 2. It is important to note that we constructed two versions of CSGF, one with the embedding of CFS to reflect the role of feature sharing and one without. According to the developmental stage of macaques <ref type="bibr" target="#b21">[22]</ref>, we divided the evaluation scans into three groups, including 7 in early infancy (age range of 0 to 6 months), 9 in infancy (age range of 7 to 12 months) and 15 in yearlings and juveniles (age range of 13 to 36 months).</p><p>Compared with the other two stages, the brain in the early infancy stage from 0 to 6 months is in a state of vigorous development, especially the extremely low contrast between tissues from 0 to 2 months, which brings great challenges to this study. In the evaluation of tissue segmentation, we assumed the practical scenario that the test scans contain only T1w images. We compared four results, including U-Net with T1w input, U-Net with T1w and generated T2w input from PTNet (i.e. independent-task mode), CSGF without CFS embedding and the full CSGF framework. We quantitatively evaluate the results by using the average surface distance (ASD) and the dice coefficient, as reported in Table <ref type="table" target="#tab_0">1</ref>. The results show that compared to U-Net with a single T1w images input, U-Net with independent PTNet-generated T2w images as an additional modality input has worse results, especially for data during infancy. This may be due to the fact that the T2w images generated in the independent-task mode do not accurately express the anatomical structures of different tissues based on the real T1w images, thus introducing unnecessary noise for subsequent tissue segmentation. In comparison, the results of the two CSGF methods are superior to those of the above two, which may be due to the fact that under task-oriented supervised training, the T2w images generated by PTNet tend to retain anatomical structures that are conducive to subsequent tissue segmentation. Notably, the method in which CFS is embedded achieves the best results, showing that tissue segmentation also benefits from the deconstruction of features by the MGM during the encoding stage. Figure <ref type="figure" target="#fig_3">4</ref> presents representative visual results of tissue segmentation at two age stages. It can be seen that the result of CSGF is the closest to the ground truth, retaining more anatomical details.</p><p>In the evaluation of the modality generation, we compared generated T2w images from T1w images based on three methods, including PTNet, CSGF w/o CFS (i.e. PTNet under the supervision of U-Net) and the final CSGF framework. We quantitatively evaluated the results by using the peak signal to noise ratio (PSNR) and structural similarity index (SSIM), as reported in Table <ref type="table" target="#tab_1">2</ref>. It demonstrates that co-training supervised by subsequent tissue segmentation leads to PTNet in CSGF with improved generative results, as compared to a single PTNet. Especially in the infancy stage, the results of CSGF have been significantly improved. This improvement can be attributed to the fact that the generation module is constrained by task-oriented anatomical features, which encourage the preservation of complete tissue structures. This helps to address the challenge of low tissue contrast to some extent. The stable performance of the generative module suggests that anatomical feature consistency and task relevance can coexist and be co-optimized between MGM loss and TSM loss, leading to the preservation of image quality in the generative model. Furthermore, the incorporation of CFS enhances the generation results, possibly due to the added anatomical feature constraints that encourage the MGM to prioritize the encoding of anatomical information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose the novel Collaborative Segmentation-Generation Framework (CSGF) to deal with the missing-modality brain MR images for tissue segmentation in early-developing macaques. Under this framework, the modality generation module (MGM) and tissue segmentation module (TSM) are jointly trained through cross-module feature sharing (CFS). The MGM is trained under the supervision of the TSM, while the TSM is trained with real and generated neuroimages. Comparative experiments on 155 scans of developing macaque data show that our CSGF outperforms conventional independent-task mode in both modality generation and tissue segmentation, showing its great potential in neuroimage research. Furthermore, as the proposed CSGF is a general framework, it can be easily extended to other types of modality generation, such as CT and PET, combined with other image segmentation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. T1w and T2w brain MR image of macaques at different ages.</figDesc><graphic coords="2,87,96,54,65,276,70,133,06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A brief comparison of two modes: (a) Independent-task mode conducts image generation and tissue segmentation separately and step by step; (b) the proposed collaborative-task mode combines them through feature sharing and supervised learning.</figDesc><graphic coords="3,61,80,53,72,300,25,58,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of the proposed 3D Collaborative Segmentation-Generation Framework (CSGF) (a). We use the T1w as the input of the modality generation module (MGM) to generate the missing modality T2w (T2wˆ). Meanwhile, the generated T2w (T2wˆ) and T1w are used as the input to the tissue segmentation module (TSM), and the results will be rebuilt into complete 3D MR images and represent the MGM loss and the TSM loss, respectively. In the structure diagram of cross-module feature sharing (CFS) (b), green blocks represent the decoder of the generator, and orange blocks represent the encoder of the segmenter. (Color figure online)</figDesc><graphic coords="4,73,47,54,20,305,86,222,49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Representative results of tissue segmentation. Red, green, and yellow colors represent cerebrospinal fluid (CSF), gray matter (GM), and white matter (WM), respectively. Blue dotted circles indicate significantly different results across different methods. Typical results of the reconstructed inner surface during infancy are also displayed on the top row, where red dotted circles indicate apparent differences. (Color figure online)</figDesc><graphic coords="8,70,47,54,29,311,23,154,42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison of different ablation settings. And 'ind.' means independent-task mode. 11.51 0.31 ± 0.19 90.59 ± 5.60 0.30 ± 0.21 79.57 ± 14.37 0.36 ± 0.28 PTNet+U-Net(ind.) 73.30 ± 11.23 0.49 ± 0.23 90.02 ± 5.46 0.35 ± 0.22 77.34 ± 14.43 0.41 ± 0.27</figDesc><table><row><cell>Method</cell><cell cols="2">Cerebrospinal fluid</cell><cell cols="2">Gray matter</cell><cell cols="2">White matter</cell></row><row><cell></cell><cell>Dice (%)</cell><cell>ASD (mm)</cell><cell>Dice (%)</cell><cell>ASD (mm)</cell><cell>Dice (%)</cell><cell>ASD (mm)</cell></row><row><cell></cell><cell></cell><cell cols="3">Early infancy (0-6 months)</cell><cell></cell></row><row><cell cols="7">U-Net 74.82 ± CSGF w\o CFS 76.90 ± 11.80 0.28 ± 0.19 90.97 ± 5.54 0.27 ± 0.19 80.21 ± 14.20 0.35 ± 0.28</cell></row><row><cell>CSGF with CFS</cell><cell cols="6">77.54 ± 11.44 0.26 ± 0.18 90.99 ± 5.73 0.27 ± 0.19 80.57 ± 14.40 0.33 ± 0.27</cell></row><row><cell></cell><cell></cell><cell cols="2">Infancy (7-12 months)</cell><cell></cell><cell></cell></row><row><cell>U-Net</cell><cell>92.07 ± 3.21</cell><cell cols="3">0.06 ± 0.09 96.65 ± 0.73 0.06 ± 0.02</cell><cell>94.46 ± 1.21</cell><cell>0.08 ± 0.03</cell></row><row><cell cols="2">PTNet+U-Net(ind.) 91.11 ± 3.85</cell><cell cols="3">0.13 ± 0.09 96.30 ± 0.83 0.09 ± 0.05</cell><cell>93.82 ± 1.36</cell><cell>0.09 ± 0.03</cell></row><row><cell>CSGF w\o CFS</cell><cell>93.12 ± 3.10</cell><cell cols="3">0.06 ± 0.09 96.89 ± 0.75 0.05 ± 0.02</cell><cell cols="2">94.58 ± 1.19 0.07 ± 0.02</cell></row><row><cell>CSGF with CFS</cell><cell cols="6">93.44 ± 3.01 0.05 ± 0.09 97.01 ± 0.76 0.05 ± 0.02 94.80 ± 1.27 0.07 ± 0.03</cell></row><row><cell></cell><cell cols="4">Yearlings and juveniles (13-36 months)</cell><cell></cell></row><row><cell>U-Net</cell><cell>92.71 ± 0.99</cell><cell cols="3">0.04 ± 0.01 96.69 ± 0.41 0.05 ± 0.01</cell><cell>94.70 ± 0.89</cell><cell>0.07 ± 0.02</cell></row><row><cell cols="2">PTNet+U-Net(ind.) 91.30 ± 2.01</cell><cell cols="3">0.11 ± 0.09 96.36 ± 0.49 0.07 ± 0.02</cell><cell>94.04 ± 1.01</cell><cell>0.08 ± 0.03</cell></row><row><cell>CSGF w ± o CFS</cell><cell>93.65 ± 1.00</cell><cell cols="3">0.03 ± 0.01 96.95 ± 0.43 0.05 ± 0.01</cell><cell>94.81 ± 0.87</cell><cell>0.07 ± 0.02</cell></row><row><cell>CSGF with CFS</cell><cell cols="6">93.79 ± 0.46 0.03 ± 0.01 97.08 ± 0.43 0.04 ± 0.01 95.03 ± 0.88 0.06 ± 0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Modality generation results on early infancy (0-6 months), infancy (7-12 months), yearlings and juveniles months) macaques. PTNet 25.74 ± 2.12 84.51 ± 6.19 28.07 ± 1.54 92.22 ± 1.92 27.44 ± 3.71 88.74 ± 12.53 CSGF w\o CFS 26.64 ± 1.90 84.97 ± 5.62 27.95 ± 1.39 92.02 ± 1.63 26.99 ± 3.46 88.72 ± 11.41 CSGF with CFS 27.74 ± 1.56 85.92 ± 5.38 28.17 ± 1.66 92.03 ± 1.60 27.37 ± 3.53 88.97 ± 11.57</figDesc><table><row><cell>Method</cell><cell>Early infancy</cell><cell></cell><cell>Infancy</cell><cell></cell><cell cols="2">Yearlings and juveniles</cell></row><row><cell></cell><cell>PSNR(dB)</cell><cell>SSIM(%)</cell><cell>PSNR(dB)</cell><cell>SSIM(%)</cell><cell>PSNR(dB)</cell><cell>SSIM(%)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> #<rs type="grantNumber">61971213</rs>, and #<rs type="grantNumber">U22A20350</rs>, the <rs type="funder">National Natural Science Foundation of Youth Science Foundation</rs> project of China #<rs type="grantNumber">62201246</rs>, and #<rs type="grantNumber">62001206</rs>, and the <rs type="funder">Guangzhou Basic and Applied Basic Research Project</rs> #<rs type="grantNumber">2023A04J2262</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ntAFjw4">
					<idno type="grant-number">61971213</idno>
				</org>
				<org type="funding" xml:id="_esBTpYc">
					<idno type="grant-number">U22A20350</idno>
				</org>
				<org type="funding" xml:id="_5wZCkZR">
					<idno type="grant-number">62201246</idno>
				</org>
				<org type="funding" xml:id="_M2dWhjc">
					<idno type="grant-number">62001206</idno>
				</org>
				<org type="funding" xml:id="_xjdZ4yP">
					<idno type="grant-number">2023A04J2262</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Basic neuroscience research with nonhuman primates: a small but indispensable component of biomedical research</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Roelfsema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1200" to="1204" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DIKA-Nets: Domain-invariant knowledge-guided attention networks for brain skull stripping of early developing macaques</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">227</biblScope>
			<biblScope unit="page">117649</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computational neuroanatomy of baby brains: a review</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page" from="906" to="925" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Medical image synthesis via deep learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Learn. Med. Image Anal</title>
		<imprint>
			<biblScope unit="page" from="23" to="44" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Anatomy-constrained contrastive learning for synthetic segmentation without ground-truth</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_5" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Phase Collaborative Network for Two-Phase Medical Imaging Segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11814</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A review: deep learning for medical image segmentation using multi-modality fusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Array</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">100004</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Collaborative image synthesis and disease diagnosis for classification of neurodegenerative disorders with incomplete multimodal neuroimages</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-3_46" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="480" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The UNC-Wisconsin rhesus macaque neurodevelopment database: a structural MRI and DTI database of early postnatal development</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Young</surname></persName>
		</author>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/28210206/" />
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.06997</idno>
		<title level="m">A Survey of Cross-Modality Brain Image Synthesis</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MouseGAN: GAN-based multiple MRI modalities synthesis and segmentation for mouse brain structures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87193-2_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87193-2_42" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12901</biblScope>
			<biblScope unit="page" from="442" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PTNet3D: a 3D high-resolution longitudinal infant brain MRI synthesizer based on transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2925" to="2940" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Links: learning-based multi-source integration framework for segmentation of infant brain images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeruoImage</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="160" to="172" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Longitudinal brain atlases of early developing cynomolgus macaques from birth to 48 months of age</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeruoImage</title>
		<imprint>
			<biblScope unit="volume">247</biblScope>
			<biblScope unit="page">118799</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Developmental topography of cortical thickness during infancy</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci</title>
		<meeting>Natl. Acad. Sci</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="15855" to="15860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Jenkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="782" to="790" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>FSL</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">User-guided 3D active contour segmentation of anatomical structures: significantly improved efficiency and reliability</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1116" to="1128" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="https://macaques.nc3rs.org.uk/about-macaques/life-history" />
		<title level="m">Reduction and Refinement of Animals in Research</title>
		<imprint/>
		<respStmt>
			<orgName>National Centre for the Replacement</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mri</forename><surname>Uw-Madison Rhesus</surname></persName>
		</author>
		<author>
			<persName><surname>Dataset</surname></persName>
		</author>
		<ptr target="https://fcon_1000.projects.nitrc.org/indi/PRIME/uwmadison.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">N4ITK: improved N3 bias correction</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Tustison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1310" to="1320" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
