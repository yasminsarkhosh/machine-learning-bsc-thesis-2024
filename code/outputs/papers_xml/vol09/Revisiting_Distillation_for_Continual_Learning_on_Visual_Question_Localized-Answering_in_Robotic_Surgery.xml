<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery</title>
				<funder ref="#_J6smRMn">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder ref="#_fjge2YU">
					<orgName type="full">Hong Kong RGC CRF</orgName>
				</funder>
				<funder ref="#_Fztwajn #_yK8yDVe #_wqAXXJY #_yceHA2j #_kk52s9s #_WuPJp7m #_Gjqjrg9">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Long</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong (CUHK)</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mobarakol</forename><surname>Islam</surname></persName>
							<email>mobarakol.islam@ucl.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS)</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongliang</forename><surname>Ren</surname></persName>
							<email>hlren@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong (CUHK)</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Shun Hing Institute of Advanced Engineering</orgName>
								<orgName type="institution" key="instit2">CUHK</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Convenɵonal</forename><surname>Cl-Vqa</surname></persName>
						</author>
						<title level="a" type="main">Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9553D01E960D230B8397E11411231E50</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_7</idno>
					<note type="submission">Grasping Retractor CL with Overlapping Classes Previous Surgical Domain SucƟon Clip Applier Current Surgical Domain Prograsp Forceps Vessel Sealer Grasping Retractor What is the state of bipolar forceps? Bipolar Forceps Bipolar Forceps</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The visual-question localized-answering (VQLA) system can serve as a knowledgeable assistant in surgical education. Except for providing text-based answers, the VQLA system can highlight the interested region for better surgical scene understanding. However, deep neural networks (DNNs) suffer from catastrophic forgetting when learning new knowledge. Specifically, when DNNs learn on incremental classes or tasks, their performance on old tasks drops dramatically. Furthermore, due to medical data privacy and licensing issues, it is often difficult to access old data when updating continual learning (CL) models. Therefore, we develop a non-exemplar continual surgical VQLA framework, to explore and balance the rigidity-plasticity trade-off of DNNs in a sequential learning paradigm. We revisit the distillation loss in CL tasks, and propose rigidity-plasticity-aware distillation (RP-Dist) and self-calibrated heterogeneous distillation (SH-Dist) to preserve the old knowledge. The weight aligning (WA) technique is also integrated to adjust the weight bias between old and new tasks. We further establish a CL framework on three public surgical datasets in the context of surgical settings that consist of overlapping classes between old and new surgical VQLA tasks. With extensive experiments, we demonstrate that our proposed method excellently reconciles learning and forgetting on the continual surgical VQLA over conventional CL methods. Our code is publicly accessible at github.com/longbai1006/CS-VQLA.</p><p>L. Bai and M. Islam-Co-first authors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Trustworthy and reliable visual question-answering (VQA) models have proved their potential in the medical domain <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>. A deep learning (DL)-based surgical VQA system <ref type="bibr" target="#b21">[22]</ref> has been developed as a surgical training and popularization tool for junior surgeons, medical students, and patients. However, one pivotal problem with surgical VQA is the lack of localized answers. VQA can provide the answer to the question, but cannot relate the answers to its localization at an instance level. Surgical scenarios with various similar instruments and actions may further confuse the learners. Answers with localization can further assist learners in dealing with confusion. In this case, a surgical visual-question localized-answering (VQLA) system can thereby be established for effective surgical training and scene understanding <ref type="bibr" target="#b2">[3]</ref>.</p><p>Meanwhile, catastrophic forgetting has become a largely discussed topic in deep neural networks. Deep neural networks (DNNs) shall abruptly and drastically forget old knowledge when learning new <ref type="bibr" target="#b15">[16]</ref>. Various continual learning (CL) methods have been proposed to mitigate catastrophic forgetting and study the balance of rigidity and plasticity in deep models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20]</ref>. Rigidity refers to the ability of the model not to diverge and remember old knowledge, while plasticity represents the acquisition of new knowledge by DNNs <ref type="bibr" target="#b3">[4]</ref>. Some pioneering works have attempted to tackle the CL problem in the medical domain <ref type="bibr" target="#b5">[6]</ref>. Catastrophic forgetting may occur in various real-world medical scenarios, e.g., data collected (i) over time, and (ii) across devices/institutions. More seriously, due to issues of data privacy, storage, and licensing, old data may not be accessible anymore <ref type="bibr" target="#b12">[13]</ref>. Therefore, it is necessary to develop a non-exemplar CL method for surgical VQLA tasks to resist catastrophic forgetting in clinical applications.</p><p>Furthermore, most medical decision-making tasks shall include classes overlapping with the old tasks and newly appeared classes, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We should not distillate the entire previous model when we deal with CL with overlapping classes. Firstly, the model will not emphasize new classes and have a high bias toward overlapping classes rather than new classes. Overlapping classes will dominate the model prediction if we naively follow the distillation from existing CL models. Secondly, catastrophic forgetting will be severe in old non-overlapping classes and the overlapping classes will dominate in the model prediction, and forget the old classes. For this purpose, we revisit distillation methods in CL and design a Continual Surgical VQLA (CS-VQLA) framework for learning incremental classes by balancing the performance of the old overlapping and non-overlapping classes. CS-VQLA has the following attributes: (i) it is a multi-task model including answering and localization, (ii) domain shift and class increment problems both exist, (iii) there may be overlapping classes between old and new tasks. These points shall further complicate the CL tasks.</p><p>In this work, (1) We establish a non-exemplar CS-VQLA framework. While being applied to surgical education and scene understanding, the framework can learn data in a streaming manner and effectively resist catastrophic forgetting. (2) We revisit the distillation method for CL, and propose rigidityplasticity-aware distillation (RP-Dist) and self-calibrated heterogeneous distilla- tion (SH-Dist) for the output logits and intermediate feature maps, respectively. The weight aligning (WA) technique is further integrated to adjust model bias between old and new data. (3) Extensive comparison and ablation studies prove the outstanding performance of our method in mitigating catastrophic forgetting, demonstrating its potential in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Problem Definition. We define the continual learning sequence with T P time periods, and t ∈ {1, ..., T P} means the current time period. D t denotes the training dataset at time period t, with x representing a sample of the input question and image pair in D t . C old denotes the classes appearing in previous time period {1, ..., t -1}, and C new represents the classes appearing in current time period t. Furthermore, we define the classes existing in both C old and C new as overlapping classes C op , and define unique classes in C old as old non-overlapping classes C no . F stands for the output feature map from the network backbone.</p><p>Knowledge Distillation (KD) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">26]</ref> on output logits <ref type="bibr" target="#b15">[16]</ref> or intermediate feature map <ref type="bibr" target="#b18">[19]</ref> is a widely used approach to retain knowledge on old tasks. With z o and z cl denote the output logits from the old and CL model, respectively, we can formulate the logits distillation loss <ref type="bibr" target="#b15">[16]</ref> as:</p><formula xml:id="formula_0">L LKD = C old c=0 -p o T (x) log p cl T (x)<label>(1)</label></formula><p>in which p o T (x) = SM (z o /T ) and p cl T (x) = SM (z cl /T ) represent the probabilities. SM means Softmax. T is temperature normalization for all old classes.</p><p>Weight Aligning (WA) <ref type="bibr" target="#b27">[27]</ref> is a simple technique to align the weight bias in the classifier layer. We use W new to represent the weights for newly appeared classes in the classifier, and W old to denote those of old classes, then we have:</p><formula xml:id="formula_1">Ŵnew = Mean[Norm(W old )] Mean[Norm(W new )] • W new (2)</formula><p>where norm means normalizing all the elements in the vector. In classincremental learning, WA can effectively avoid the model bias towards new classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Old Model ConƟnual Learning Model</head><note type="other">Input</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Continual Surgical VQLA (CS-VQLA)</head><p>Visual-Question Localized Answering. We define our VQLA framework following <ref type="bibr" target="#b2">[3]</ref>, by building a parallel detector on top of the VQA-based classification model. Therefore, the VLQA model includes the following components: a ResNet18 <ref type="bibr" target="#b7">[8]</ref> pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref> as a prior image feature extractor, a BERT tokenizer <ref type="bibr" target="#b6">[7]</ref>, the VisualBERT <ref type="bibr" target="#b14">[15]</ref> as the backbone (it can also be called as the deep feature extractor), a fully-connected layer as the classifier, and a 3-layer MLP as the detector. The classification task is optimized via the cross-entropy loss L CE , and the bounding box regression is optimized by the sum of L 1 and GIoU loss <ref type="bibr" target="#b20">[21]</ref>. Thus, the VQLA loss can be formulated as:</p><formula xml:id="formula_2">L V QLA = μ • L CE + (L 1 + L GIoU )</formula><p>, where μ is set as 100 to balance the optimization progress of the two tasks.</p><p>Rigidity-Plasticity-Aware Distillation (RP-Dist). The current rigidityplasticity trade-off is towards the entire model. However, we shall make the rigidity-plasticity aware in overlapping and non-overlapping classes.</p><p>There is no overlap between C old and C new in an ideal class-incremental learning setup, so the temperature T in Equ. 1 is set to 2 by <ref type="bibr" target="#b15">[16]</ref>. However, in a real-world application setup, T should not smooth the logits equally for old nonoverlapping C on and overlapping classes C op . Specifically, through adjusting for T , we shall endow the model greater plasticity on C op , and keep the rigidity on C on . We first establish a regularized distillation loss. Originally, the old model shall serve as the 'teacher' model in CL-based distillation. Instead of directly distilling the old model output logits, we construct a perfect pseudo teacher for distillation. To begin with, a pseudo answering label set a can be built from the old model classification probabilities p o (x) via a = Max[p(x)]. Based on the idea of label smoothing, we can manually setup a pseudo old model to have a high probability of predicting a correct class, and its probability distribution shall be:</p><formula xml:id="formula_3">p o (x) = λ if x = a (1 -λ)/(C old -1) if x = a (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>When λ is set to a very high number (e.g., λ ≥ 0.9), we will have a high probability of getting a correct class, allowing the teacher model to have perfect performance. The probability output of the CL model can be optimized with this pseudo-teacher based on the Kullback-Leibler divergence D KL :</p><formula xml:id="formula_5">L RKD = C old c=0 D KL p o T (x), p cl T (x)<label>(4)</label></formula><p>T is the KD temperature used to generate soft probabilities for the pseudo old model. As discussed above, this naive setting of T is not suitable for general CL scenarios. Therefore, we treat T op and T on differently to strengthen the plasticity on C op and the rigidity on C on respectively. L RKD can thereby be rewritten as:</p><formula xml:id="formula_6">L RKD = Cop c=0 D KL p o Top (x), p cl Top (x) + Con c=0 D KL p o Ton (x), p cl Ton (x)<label>(5)</label></formula><p>We keep T op &gt; T on to balance the rigidity and plasticity trade-off in the CL model, and set T op = 25, T on = 20 empirically in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Calibrated Heterogeneous Distillation (SH-Dist).</head><p>Works have discussed the use of self-calibration to improve model performance <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">32]</ref>. However, assuming we obtain an old model and we would like to conduct CL training on it, we can hardly modify the old model itself directly. Therefore, we perform a self-calibration operation on the heterogeneous output features F t from the Visu-alBERT backbone to get self-calibrated feature F t . The details can be referred to at the bottom of Fig. <ref type="figure" target="#fig_1">2</ref>. Without engaging more learnable parameters, we endow the heterogeneous features with adaptively modeled long-range context information. Therefore, we can construct our feature distillation using the selfcalibrated feature map F t and the old model feature map F t-1 . L 2 loss is used to minimize the distance between F t and F t-1 empirically by following <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_7">L F KD = F t-1 (x) -F t (x) 2 2 |D t | (6)</formula><p>Subsequently, the self-calibrated feature map F t shall be propagated through the parallel classifier and detector for the multi-task prediction.</p><p>Overall Framework. Figure <ref type="figure" target="#fig_1">2</ref> shows the overview of our CS-VQLA framework. The given image and question input are respectively processed as feature embedding by pre-trained ResNet18 and BERT tokenier, and fed to the VisualBERT backbone after embedding fusion. Then the output heterogeneous feature map is used to train the parallel predictors. The loss functions establish the essential components of our CS-VQLA framework. In the initial time period t = 0, the model is only trained on the VQLA loss. When t &gt; 0, we combine the VQLA loss for training on the current dataset D t , with the RP-Dist &amp; SH-Dist loss to retain the old knowledge. We can summarize our final loss function as follows:</p><formula xml:id="formula_8">L = L V QLA t = 0 α • L V QLA + β • L RKD + γ • L F KD t &gt; 0<label>(7)</label></formula><p>We set α = β = 1 and γ = 5 in our implementation. Furthermore, WA is deployed after training on each time period, to balance the weight bias of new classes on the classification layer. Through the combination of multiple distillation paradigms and model weight adjustment, we successfully realize the general continual learning framework in the VQLA scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Setup</head><p>Dataset. We construct our continual procedure as follows: when t = 0, we train on EndoVis18 Dataset, t = 1 on EndoVis17 Dataset, and t = 2 on M2CAI Dataset. Therefore, we can establish our CS-VQLA framework with a large initial step, and several smaller sequential steps. When splitting the dataset, we isolate the training and test sets in different sequences to avoid information leakage.</p><p>EndoVis18 Dataset is a public dataset with 14 videos on robotic surgery <ref type="bibr" target="#b0">[1]</ref>. The question-answer (QA) pairs are accessible in <ref type="bibr" target="#b21">[22]</ref>, and the bounding box annotations are from <ref type="bibr" target="#b9">[10]</ref>. The answers are in single-word form with three categories (organ, interaction, and locations). We further extend the QA pairs and include cases when the answer is a surgical tool. Besides, if the answer is regarding the organ-tool interaction, the bounding box shall contain both the organ and the tool. Statistically, the training set contains 1560 frames with 12741 QA pairs, and the test set contains 447 frames with 3930 QA pairs.</p><p>EndoVis17 Dataset is a public dataset with 10 videos on robotic surgery <ref type="bibr" target="#b1">[2]</ref>. We randomly select frames and manually annotate the QA pairs and bounding boxes. The training set contains 167 frames with 1034 QA pairs, and the test set contains 40 frames with 201 QA pairs.</p><p>M2CAI Dataset is also a public robotic surgery dataset <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25]</ref>, and the location bounding box is publicly accessible in <ref type="bibr" target="#b10">[11]</ref>. Similarly, we randomly select 167 Implementation Details. We compare our solution against the fine-tuning (FT) baseline and state-of-the-art (SOTA) methods, including LwF <ref type="bibr" target="#b15">[16]</ref>, WA <ref type="bibr" target="#b27">[27]</ref>, iCaRL <ref type="bibr" target="#b19">[20]</ref>, IL2A <ref type="bibr" target="#b29">[29]</ref>, PASS <ref type="bibr" target="#b30">[30]</ref>, SSRE <ref type="bibr" target="#b31">[31]</ref>, CLVQA <ref type="bibr" target="#b13">[14]</ref>, and CLiMB <ref type="bibr" target="#b23">[23]</ref>. All the methods are implemented using <ref type="bibr" target="#b28">[28]</ref> <ref type="foot" target="#foot_0">1</ref> , with PyTorch and on NVIDIA RTX 3090 GPU. We removed the exemplars in all methods for a non-exemplar comparison. All methods are firstly trained on EndoVis18 (t = 0) for 60 epochs with a learning rate of 1 × 10 -5 , and then trained on EndoVis17 (t = 2) and M2CAI (t = 2) for 30 epochs with a learning rate of 5 × 10 -5 . We use Adam optimizer <ref type="bibr" target="#b11">[12]</ref> and a batch size of 64. Answering and localization performance are evaluated by Accuracy (Acc) and mean intersection over union (mIoU), respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Except for testing on three datasets separately, we set three specific categories in our continual learning setup: old non-overlapping (old N/O) classes, overlapping classes, and new non-overlapping (new N/O) classes. By measuring the performance in these three categories, we can easily observe the catastrophic forgetting phenomenon and the performance of mitigating catastrophic forgetting.</p><p>As shown in Table <ref type="table" target="#tab_0">1</ref> &amp; 2, firstly, catastrophic forgetting can be apparently observed in the performance of FT. Then, among all baselines, iCaRL achieves the best performance when the model learns from t = 0 to t = 1, and gets to forget when there are more time periods. On the contrary, LwF exhibits a strong retention of old knowledge, but a lack of ability to learn new. Our proposed methods demonstrate superior performance in almost all metrics and classes. In classification tasks, the overall average of our methods outperforms the second best with 2.60% accuracy improvement at t = 1 and 2.68% at t = 2. In localization tasks, our method is 0.44 mIoU higher than the second best at t = 1 and 0.94 mIoU higher at t = 2. The results prove the remarkable ability of our method to balance the rigidity-plasticity trade-off. Furthermore, an ablation study is conducted to demonstrate the effectiveness of each component in our proposed method. We (i) degenerate the RP-Dist to original logits distillation <ref type="bibr" target="#b15">[16]</ref>, (ii) degenerate the SH-Dist to normal feature distillation <ref type="bibr" target="#b18">[19]</ref>, and (iii) remove the WA module, as shown in Table <ref type="table" target="#tab_2">3</ref>. Experimental results show that each component we propose or integrate plays an essential role in the final rigidity-plasticity trade-off. Therefore, we demonstrate that each of our components is indispensable. More evaluation and ablation studies can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper introduces CS-VQLA, a general continual learning framework on surgical VQLA tasks. This is a significant attempt to continue learning under complicated clinical tasks. Specifically, we propose the RP-Dist on output logits, and the SH-Dist on the intermediate feature space, respectively. The WA technique is further integrated for model weight bias adjustment. Superior performance on VQLA tasks demonstrates that our method has an excellent ability to deal with CL-based surgical scenarios. Except for giving localized answers for better surgical scene understanding, our solution can conduct continual learning in any questions in surgical applications to solve the problem of class increment, domain shift, and overlapping/non-overlapping classes. Our framework can also be applied when adapting a vision-language foundation model in the surgical domain. Therefore, our solution holds promise for deploying auxiliary surgical education tools across time/institutions. Potential future works also include combining various surgical training systems (e.g., mixed reality-based training, surgical skill assessment) to develop an effective and comprehensive virtual teaching system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Comparison between conventional CL-VQA and our CS-VQLA. Besides providing localized answers, our CS-VQLA framework also pays attention to the issue of overlapping and non-overlapping classes in sequential surgical domains.</figDesc><graphic coords="3,43,80,63,14,336,31,109,57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our CS-VQLA network. The VQLA model is used to process bimodal input (image and text) and provide predictions for two tasks (answering and localization). The proposed RP-Dist and SH-Dist are designed to help the CL model retain old knowledge from the old model and trade-off model rigidity-plasticity. 'N/O' means non-overlapping classes.</figDesc><graphic coords="4,83,22,204,11,235,66,166,90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison</figDesc><table><row><cell>t = 1</cell><cell>Methods</cell><cell>Old N/O</cell><cell cols="3">Overlapping New N/O</cell><cell cols="2">EndoVis18</cell><cell>EndoVis17</cell><cell>Average</cell></row><row><cell></cell><cell></cell><cell cols="2">Acc mIoU Acc</cell><cell>mIoU Acc</cell><cell cols="2">mIoU Acc</cell><cell>mIoU Acc</cell><cell>mIoU Acc</cell><cell>mIoU</cell></row><row><cell cols="5">W/O CL Base (t = 0) 6.11 62.12 64.60 75.68</cell><cell></cell><cell cols="2">62.65 75.23</cell></row><row><cell></cell><cell>FT</cell><cell cols="6">0.00 62.55 38.07 71.88 86.96 80.41 34.86 71.29 81.59 78.30 58.23 74.79</cell></row><row><cell cols="2">W/I CL LwF [16]</cell><cell cols="6">0.00 63.40 54.36 69.94 73.91 77.47 53.20 69.53 43.79 74.64 48.50 72.08</cell></row><row><cell></cell><cell>WA [27]</cell><cell cols="6">0.76 60.70 55.11 71.61 52.17 78.10 52.85 71.02 63.68 76.71 58.26 73.86</cell></row><row><cell></cell><cell cols="7">iCaRL [20] 0.76 62.23 55.87 72.36 43.48 79.51 53.85 71.76 58.21 78.41 56.03 75.08</cell></row><row><cell></cell><cell>IL2A [29]</cell><cell cols="6">0.00 57.74 53.00 69.88 56.52 78.20 51.48 69.23 48.76 75.75 50.12 72.49</cell></row><row><cell></cell><cell>PASS [30]</cell><cell cols="6">0.00 56.49 54.01 70.08 69.57 77.60 51.70 69.46 65.67 74.56 58.69 72.01</cell></row><row><cell></cell><cell>SSRE [31]</cell><cell cols="6">0.00 60.29 54.04 70.34 65.22 76.07 51.76 69.78 64.68 75.44 58.22 72.61</cell></row><row><cell></cell><cell cols="7">CLVQA [14] 0.00 59.87 51.83 72.98 65.22 78.36 49.14 72.40 72.14 76.40 60.64 74.40</cell></row><row><cell></cell><cell cols="7">CLiMB [23] 0.00 60.16 52.88 72.99 69.57 77.37 50.13 72.44 74.13 75.87 62.13 74.16</cell></row><row><cell></cell><cell>Ours</cell><cell cols="2">1.53 61.08 56</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>experiments from the time period t = 0 to t = 1. Bold and underlined represent best and second best, respectively. 'W/O' denotes 'without', and 'W/I' denotes 'within'. 'N/O' means non-overlapping. 'Old N/O' represents the classes that exist in t = 0 but do not exist in t = 1, and 'New N/O' represents the opposite. 'Overlapping' denotes the classes that exist in both t = 0, 1.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.98 74.57 78.26 78.59 54.33 74.02 75.12 77.02 64.73 75.52Table 2 .</head><label>2</label><figDesc>Comparison experiments from the time period t = 0, 1 to t = 2. 'Old N/O' represents the classes that exist in t = 0, 1 but do not exist in t = 2, and 'New N/O' represents the opposite. 'Overlapping' denotes the classes that exist in t = 0, 1, 2.</figDesc><table><row><cell cols="9">frames and annotate 449 QA pairs for the training set, and 40 frames with 94</cell></row><row><cell cols="8">QA pairs in different videos for the test set.</cell></row><row><cell>t = 2</cell><cell>Methods</cell><cell cols="2">Old N/O</cell><cell cols="3">Overlapping New N/O</cell><cell cols="2">EndoVis18</cell><cell>EndoVis17</cell><cell>M2CAI16</cell><cell>Average</cell></row><row><cell></cell><cell></cell><cell>Acc</cell><cell cols="2">mIoU Acc</cell><cell>mIoU Acc</cell><cell cols="2">mIoU Acc</cell><cell>mIoU Acc</cell><cell>mIoU Acc</cell><cell>mIoU Acc</cell><cell>mIoU</cell></row><row><cell cols="2">W/O CL FT</cell><cell>4.00</cell><cell cols="6">60.90 19.08 58.69 55.56 70.83 15.57 58.75 41.79 60.18 51.06 69.48 36.14 62.80</cell></row><row><cell cols="2">W/I CL LwF [16]</cell><cell>4.20</cell><cell cols="6">62.91 42.75 63.34 27.78 72.68 38.04 63.25 41.29 62.71 31.91 69.65 37.08 65.20</cell></row><row><cell></cell><cell>WA [27]</cell><cell>6.80</cell><cell cols="6">59.32 40.62 61.55 55.56 73.06 36.67 61.20 36.32 60.86 40.43 69.80 37.81 63.96</cell></row><row><cell></cell><cell cols="2">iCaRL [20] 2.00</cell><cell cols="6">58.55 38.06 58.59 41.67 73.42 33.46 58.30 38.81 61.12 38.30 71.01 36.85 63.48</cell></row><row><cell></cell><cell>IL2A [29]</cell><cell cols="7">11.00 57.25 33.80 58.50 27.78 72.71 30.61 58.28 37.31 58.29 36.17 67.10 34.70 61.22</cell></row><row><cell></cell><cell>PASS [30]</cell><cell cols="7">22.60 56.57 24.80 58.39 30.56 72.07 23.52 58.07 37.81 58.91 41.49 66.68 34.27 61.22</cell></row><row><cell></cell><cell>SSRE [31]</cell><cell cols="7">13.80 58.12 19.49 57.02 47.22 73.31 18.12 57.00 27.36 58.09 40.43 67.47 28.64 60.85</cell></row><row><cell></cell><cell cols="8">CLVQA [14] 21.80 58.01 36.54 62.92 25.00 74.09 34.40 62.35 39.80 61.05 36.17 68.89 36.79 64.10</cell></row><row><cell></cell><cell cols="8">CLiMB [23] 23.00 57.03 38.90 64.30 33.33 73.45 36.62 63.50 42.29 61.35 40.43 69.20 39.78 64.68</cell></row><row><cell></cell><cell>Ours</cell><cell cols="7">28.20 68.14 41.04 65.74 44.44 74.41 39.13 66.21 46.77 62.17 41.49 70.04 42.46 66.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Ablation experiments from the time period t = 0 to t = 1, and from t = 1 to t = 2. To observe the contribution of each component, we degenerate the proposed RP-Dist and SH-Dist to the normal distillation paradigm, and remove the WA module.</figDesc><table><row><cell>Methods</cell><cell>t = 0 to t = 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>t = 1 to t = 2</cell></row><row><cell></cell><cell>Old N/O</cell><cell cols="3">Overlapping New N/O</cell><cell cols="2">Average</cell><cell>Old N/O</cell><cell>Overlapping New N/O</cell><cell>Average</cell></row><row><cell cols="3">RP SH WA Acc mIoU Acc</cell><cell>mIoU Acc</cell><cell cols="2">mIoU Acc</cell><cell cols="2">mIoU Acc</cell><cell>mIoU Acc mIoU Acc</cell><cell>mIoU Acc</cell><cell>mIoU</cell></row><row><cell></cell><cell cols="7">0.00 60.58 55.30 72.94 73.91 77.62 62.66 74.37 11.40 58.29 40.96 64.72 30.56 73.31 40.80 64.85</cell></row><row><cell></cell><cell cols="7">0.00 59.94 56.23 73.66 82.61 78.04 64.33 74.90 9.80</cell><cell>57.17 38.14 65.30 38.89 74.15 40.26 64.35</cell></row><row><cell></cell><cell cols="7">0.00 60.33 53.08 72.45 52.17 76.18 61.94 74.30 12.20 59.49 39.75 64.81 41.67 72.12 39.07 64.81</cell></row><row><cell></cell><cell cols="7">0.00 60.97 54.59 74.12 73.91 78.52 61.83 74.89 11.00 65.16 40.69 64.45 41.67 74.18 39.63 65.45</cell></row><row><cell></cell><cell cols="7">0.00 61.04 56.08 74.11 60.87 78.91 61.60 74.88 11.00 59.10 39.25 62.60 22.22 71.66 39.16 64.55</cell></row><row><cell></cell><cell cols="7">0.00 60.07 54.56 74.15 73.91 79.79 61.81 75.00 10.40 63.32 39.14 64.40 27.78 73.89 40.21 65.65</cell></row><row><cell></cell><cell>1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>53 61.08 56.98 74.57 78</head><label></label><figDesc>.26 78.59 64.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>73 75.52 28.20 68.14 41.04 65.74 44.44 74.41 42.46 66.14</head><label></label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>github.com/G-U-N/PyCIL.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>. This work was funded by <rs type="funder">Hong Kong RGC CRF</rs> <rs type="grantNumber">C4063-18G</rs>, <rs type="grantNumber">CRF C4026-21GF</rs>, <rs type="grantNumber">RIF R4020-22</rs>, <rs type="grantNumber">GRF 14203323</rs>, <rs type="grantNumber">GRF 14216022</rs>, <rs type="grantNumber">GRF 14211420</rs>, <rs type="grantNumber">NSFC/RGC JRS N CUHK420/22</rs>; <rs type="affiliation">Shenzhen-Hong Kong-Macau Technology Research Programme (Type C 202108233000303</rs>); Guangdong GBABF #<rs type="grantNumber">2021B1515120035</rs>. M. Islam was funded by <rs type="funder">EPSRC</rs> grant <rs type="grantNumber">[EP/W00805X/1</rs>].</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fjge2YU">
					<idno type="grant-number">C4063-18G</idno>
				</org>
				<org type="funding" xml:id="_Fztwajn">
					<idno type="grant-number">CRF C4026-21GF</idno>
				</org>
				<org type="funding" xml:id="_yK8yDVe">
					<idno type="grant-number">RIF R4020-22</idno>
				</org>
				<org type="funding" xml:id="_wqAXXJY">
					<idno type="grant-number">GRF 14203323</idno>
				</org>
				<org type="funding" xml:id="_yceHA2j">
					<idno type="grant-number">GRF 14216022</idno>
				</org>
				<org type="funding" xml:id="_kk52s9s">
					<idno type="grant-number">GRF 14211420</idno>
				</org>
				<org type="funding" xml:id="_WuPJp7m">
					<idno type="grant-number">NSFC/RGC JRS N CUHK420/22</idno>
				</org>
				<org type="funding" xml:id="_J6smRMn">
					<idno type="grant-number">2021B1515120035</idno>
				</org>
				<org type="funding" xml:id="_Gjqjrg9">
					<idno type="grant-number">[EP/W00805X/1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 7.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.11190</idno>
		<title level="m">robotic scene segmentation challenge</title>
		<imprint>
			<date type="published" when="2018">2018. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Allan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06426</idno>
		<title level="m">robotic instrument segmentation challenge</title>
		<imprint>
			<date type="published" when="2017">2017. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Surgical-VQLA: transformer with gated vision-language embedding for visual question localized-answering in robotic surgery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Seenivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.11692</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A continual learning survey: defying forgetting in classification tasks</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3366" to="3385" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lifelonger: a benchmark for continual disease classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Derakhshani</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16434-7_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16434-7" />
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
	<note>MICCAI 2022</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning and reasoning with the graph structure representation in robotic surgery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Seenivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59716-0_60</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59716-060" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12263</biblScope>
			<biblScope unit="page" from="627" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tool detection and operative skill assessment in surgical videos using region-based convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: a method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Clinical applications of continual learning machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Digit. Health</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="279" to="e281" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Symbolic replay: scene graph as prompt for continual learning on VQA task</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1250" to="1259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: a simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10056</idno>
		<title level="m">Medical visual question answering: a survey</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving convolutional networks with self-calibrated convolutions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10096" to="10105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Incremental learning techniques for semantic segmentation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zanuttigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">iCaRL: incremental classifier and representation learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2001" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: a metric and a loss for bounding box regression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Surgical-VQA: visual question answering in surgical scenes using transformer</title>
		<author>
			<persName><forename type="first">L</forename><surname>Seenivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022. MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-14" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Climb: a continual learning benchmark for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pinto Alva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chochlakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rostami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="29440" to="29453" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Stauder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ostler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kranzfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feußner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09278</idno>
		<title level="m">The tum lapchole dataset for the m2cai 2016 workflow challenge</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">EndoNet: a deep architecture for recognition tasks on laparoscopic videos</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Twinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marescaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="97" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3903" to="3911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Maintaining discrimination and fairness in class incremental learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13208" to="13217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Zhan</surname></persName>
		</author>
		<title level="m">PyCIL: a python toolbox for classincremental learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Class-incremental learning via dual augmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="14306" to="14318" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prototype augmentation and self-supervision for incremental learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5871" to="5880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-sustaining representation expansion for non-exemplar class-incremental learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9296" to="9305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-calibrated efficient transformer for lightweight super-resolution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="930" to="939" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
