<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning</title>
				<funder ref="#_s2f9apu #_vtJEHmU #_zzKFDg3">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">M R</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37235</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yubo</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37235</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benoit</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37235</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jack</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<postCode>37235</postCode>
									<settlement>Nashville</settlement>
									<region>TN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cochlear Implant Fold Detection in Intra-operative CT Using Weakly Supervised Multi-task Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4A412DB93E1C4FEE72A6D02072206F8C</idno>
					<idno type="DOI">10.1007/978-3-031-43996-4_24</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Tip fold-over</term>
					<term>cochlear implant</term>
					<term>synthetic CT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In cochlear implant (CI) procedures, an electrode array is surgically inserted into the cochlea. The electrodes are used to stimulate the auditory nerve and restore hearing sensation for the recipient. If the array folds inside the cochlea during the insertion procedure, it can lead to trauma, damage to the residual hearing, and poor hearing restoration. Intraoperative detection of such a case can allow a surgeon to perform reimplantation. However, this intraoperative detection requires experience and electrophysiological tests sometimes fail to detect an array folding. Due to the low incidence of array folding, we generated a dataset of CT images with folded synthetic electrode arrays with realistic metal artifact. The dataset was used to train a multitask custom 3D-UNet model for array fold detection. We tested the trained model on real post-operative CTs (7 with folded arrays and 200 without). Our model could correctly classify all the fold-over cases while misclassifying only 3 non fold-over cases. Therefore, the model is a promising option for array fold detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cochlear implantations (CIs) are considered to be a standard treatment in case of individuals with severe-to-profound hearing loss <ref type="bibr" target="#b0">[1]</ref>. During CI surgical procedure an electrode array (EA) is implanted in the cochlea for stimulating the auditory nerve. Along the cochlear duct length, the neural pathways are arranged in a tonotopic manner by decreasing frequency <ref type="bibr" target="#b1">[2]</ref>. Naturally, these pathways get activated according to their characteristic frequencies present in the incoming sound. After implantation, the electrode arrays are used to stimulate the nerve pathways and induce hearing sensation <ref type="bibr" target="#b2">[3]</ref>.</p><p>During the insertion process, one complication surgeons aim to avoid is called "tip fold-over," where the tip of the array curls in an irregular manner inside the cochlear volume resulting in array folding <ref type="bibr" target="#b3">[4]</ref>. This can occur when the tip of the electrode array gets stuck within an intracochlear cavity as the surgeon threads the array into the cochlea, largely blind to the intra-cochlear path of the array and with little tactile feedback available to indicate the tip of the array is folding <ref type="bibr" target="#b4">[5]</ref>. Therefore, further pushing the base of the EA into the cochlea results in folding of the array (shown in Fig. <ref type="figure" target="#fig_0">1</ref>) <ref type="bibr" target="#b4">[5]</ref>. Tip fold-over can result in many complications which include trauma, damage to residual hearing and poor positioning of the EA inside cochlea, which ultimately leads to poor hearing restoration for the patient. If intra-operative detection of a tip fold-over is possible, the surgeon can address it through re-implantation of the electrode array <ref type="bibr" target="#b5">[6]</ref>. In addition, post-operative detection of minor fold-over can help audiologists to deactivate the affected electrodes to attempt to reach a more satisfactory hearing outcome <ref type="bibr" target="#b3">[4]</ref>.</p><p>Although most CI centers do not currently attempt to detect tip foldovers, the current standard approach among sites that do is visual inspection of intraoperative fluoroscopy. However, these identification methods require experience to align the view optimally and limit the radiation exposure during the fluoroscopy <ref type="bibr" target="#b6">[7]</ref>.  <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. In some studies, it is reported that the intraoperative electrophysiological measures, such as NRT or EcochG, sometimes fail to identify the tip fold-over cases <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Pile et al. <ref type="bibr" target="#b10">[11]</ref> developed a robotic system for tip fold-over detection where the support vector machine classifier is used on the EA insertion force profile. This approach is associated with robot assisted insertion techniques. This broad body of work emphasizes the need for an accurate approach for intra and/or post operative fold-over detection. Therefore, the goal of this study is to develop an approach to detect tip fold-overs in cone beam or conventional CT images using state-of-the-art deep neural network-based image analysis.</p><p>As tip fold-over cases are reported to be rare, it would be difficult to acquire a substantial number of cases for fully supervised training of any data-driven method. Zuniga et al. <ref type="bibr" target="#b3">[4]</ref> studied CI surgeries in 303 ears and reported 6 tip fold-over cases (less than 2%). Dhanasingh et al. <ref type="bibr" target="#b4">[5]</ref> investigated 3177 CI recipients' cases from 13 studies and reported 50 tip fold-over cases (1.57%). Only 0.87% (15 cases) tip fold-over cases were reported among 1722 CI recipients according to Gabrielpillai et al. <ref type="bibr" target="#b11">[12]</ref>. Dhanasingh et al. <ref type="bibr" target="#b4">[5]</ref> analyzed 38 peer reviewed publications and reported that the rate of tip fold-over with certain types of arrays might be as high as 4.7%. Data scarcity thus makes it difficult to curate a balanced training dataset of tip fold-over cases. Deep learning methods are the current state-of-the-art in medical image analysis, including image classification tasks. Numerous approaches have been proposed for using 3D networks to solve image classification tasks, e.g. <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. Multi-tasking neural networks have also been used for simultaneous medical image segmentation and classification tasks. These networks have shared layers from the input side which branch into multiple paths for multiple outputs, e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Along with choosing the appropriate network for the CT image classification task one of the typical concerns is the class balance of the training dataset <ref type="bibr" target="#b16">[17]</ref>. As tip foldovers are rare, augmentation approaches are needed to reduce the effect of data imbalance.</p><p>Therefore, in this work we design a dataset of CT images with folded synthetic arrays with realistic metal artifact to assist with training. We propose a multi-task neural network based on the U-Net <ref type="bibr" target="#b17">[18]</ref>, train it using the synthetic CT dataset, and then test it on a small dataset of real tip fold-over cases. Our results indicate that the model performs well in detecting fold-over cases in real CT images, and therefore, the trained model could help the intra-operative detection of tip fold-over in CI surgery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset</head><p>In this study, we utilize CT images from 312 CI patients acquired under local IRB protocols. This included 192 post-implantation CTs (185 normal and 7 tip-fold), acquired either intra-operatively (cone beam) or post-operatively (conventional or cone beam), and 120 pre-implantation CTs used to create synthetic post-implantation CT. As image acquisition parameters (dimensionality, resolution and voxel intensity) varies among the images, we preprocessed all the CT images to homogenize these parameters. First, the intracochlear structures (e.g., Scala Tympani (ST) and Scala Vestibuli (SV)) were segmented from the CT image using previously developed automatic segmentation techniques <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>. Using the ST segmentation, a region-of-interest CT image was cropped from the full-sized CT image keeping the ST at the center of the cropped image. Then the cropped CT resolution was resampled to an isotropic voxel size of 0.3 mm with a 32 × 32 × 32 grid. As the final step of the preprocessing, the voxel intensity of the cropped image was normalized to ensure comparable intensity distribution among all the CT images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Synthetic CT Generation</head><p>Our synthetic post-operative CT generation approach is inspired by the process of synthetic preoperative CT generation by Khan et al. <ref type="bibr" target="#b21">[22]</ref>. First, a random but realistic location for the electrodes is estimated in a real pre-implantation CT image. This was done by considering some constraints, such as the relative location of the ST, electrode spacing, active array length, relative smoothness of the electrode curve, whether a fold exists, and if so, the location of the fold. Randomized variability within plausible margins ensured generating a unique and realistic electrode array shape for each case.</p><p>Once we estimated the probable locations of the electrodes, we placed high intensity (around 3-4 times the bone intensity) cubic blocks with a dimensionality of 4×4× 4 voxels in an empty 32 × 32 × 32 grid in locations corresponding to the electrode sites in the preoperative CT. We also added small high intensity cubic blocks (with a dimensionality of 2 × 2 × 2 voxels) between the electrodes to represent the wires that connect to the electrodes. This resulted in an ideal image with a synthetic electrode array (shown in Fig. <ref type="figure" target="#fig_1">2</ref>) but lacking realistic reconstruction artifacts. To get a realistic metal artifact, we applied radon transformation on the high intensity blocks to project the data on the detector space. Then, with the resulting sinogram, we applied inverse radon transformation to backproject the sinogram into the world space. The backprojection process is done two times separately: first time with low frequency scaling (to achieve blurry metal edge) and second time with high frequency scaling (to achieve dominant metal artifact). A Hamming filter was used in the backprojection process. Finally, the preoperative CT and the images with back-projected synthetic EA with realistic metal artifact were merged together additively to generate a synthetic postoperative CT image. Wang et al. <ref type="bibr" target="#b22">[23]</ref> also presented a method to generate synthetic CT images with metal artifact, however, the aim of the study was to remove metal artifact from post-implant CT images.  Using the described synthetic CT generation process, we produced 215 synthetic pseudo CTs (PCTs) with random electrode locations sampled from 100 preoperative real CT images with stratified sampling into the training, validation, and testing datasets. In these PCTs, 155 images have a synthetic EA with tip fold-over and the remaining 60 do not have any fold over. The combined dataset including 379 real CTs and 215 synthetic CTs was divided into training (335), validation <ref type="bibr" target="#b21">(22)</ref> and testing (237) subsets. The overall distribution of the dataset is presented in Table <ref type="table" target="#tab_1">1</ref>. As the number of real fold-over cases is very low (about 1.85% in this study), we allotted all of them in the testing dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-task Deep Learning Network</head><p>The neural network model proposed in this study for EA fold-over detection was inspired by the 3D U-Net architectures proposed by Isensee et al. and Ronneberger et al. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">24]</ref>. The model is a multitasking network where the outputs are the segmentation of the EA and fold-over classification. Our hypothesis is that this multi-task approach helps the network focus attention on the shape of the EA when learning to classify the CT, rather than overfitting to a spurious local minima driven by non-EA features in the training dataset. The architecture is comprised of a context pathway for encoding the increasingly abstract representation of the input as we advance deeper into the neural network. A localization pathway recombines the representation for localizing the EA <ref type="bibr" target="#b19">[20]</ref>. In the model, the context modules compute the context pathways. Each of these modules is a pre-activation residual block <ref type="bibr" target="#b25">[25]</ref> which has a dropout layer p dropout = 0.2 in between two convolutional layers of 3 × 3 × 3 dimensionality.</p><p>The localization pathways collect features at lower spatial resolution where the contextual information is encoded and transfer it to the higher resolution. This is done by using an upsampling step followed by a convolutional layer. The upsampled features are then concatenated with the corresponding context pathway level. Segmentation layers from different levels of the architecture are convolutional layers that are combined by elementwise summation to build the segmentation in a multi-scale fashion and obtain the final segmentation output. This approach was inspired by Kayalibay et al. <ref type="bibr" target="#b26">[26]</ref>.</p><p>A classification branch is added to the network at the point where the contextual information is encoded at the lowest resolution (shown in Fig. <ref type="figure" target="#fig_2">3</ref>). The classification branch consists of 4 residual blocks <ref type="bibr" target="#b27">[27]</ref> followed by an average pooling layer and a sigmoid function layer.</p><p>We used binary cross entropy (BCE) loss between the EA ground truth, created by manually selected thresholding of the CT image, and the predicted segmentation. For fold-over classification, we also used BCE loss between the ground truth and the predicted class. However, to place emphasis on the classification performance of the model, the classification loss was weighted 5 times the loss for the segmentation. The learning rate and the batch size were considered 3e-5 and 20, respectively. While training the model, random horizontal flipping and 90°rotation were used as data augmentation techniques for generalization.</p><p>To evaluate the performance of the proposed model compared to some other neural network models, we implemented 3D versions of ResNet18 <ref type="bibr" target="#b14">[15]</ref>, Variational Autoencoder (VAE) <ref type="bibr" target="#b28">[28]</ref> and Generative Adversarial Network (GAN) <ref type="bibr" target="#b29">[29]</ref>. The overall performance comparison among these network architectures is presented in the result section. Similar to the multitasking 3D U-Net, proposed in this study, the VAE and the GAN architectures were designed with the same multi-task objective. In the VAE network, the information of the input image was encoded in 128 latent variables in the encoder section of the model. With these latent variables the decoder section reconstructs a 3D image with the EA segmentation. A classification branch with the same architectures as proposed above was added to the encoder section for fold-over detection. Similarly, the GAN also had a classification branch in the generator model of the architecture. The ResNet18 performs only classification, and thus represents classification performance achievable without multi-task training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The training and validation loss curves of multitasking 3D U-Net are presented in Fig. <ref type="figure" target="#fig_3">4</ref>. The graph at the top presents the overall training and validation loss. A rapid drop is visible in the segmentation loss curves where the validation loss curve swiftly follows the training loss curve. On the other hand, the classification loss demonstrates a gradual drop.</p><p>Next, we compare the performance of different models for fold-over classification. In addition to the performance analysis of these networks for the whole testing data, we separately reported the performance analysis for the synthetic as well as the real portions of the testing data. The separate analysis provides insight about the applicability of the trained model for the real CT images. As reported in Table <ref type="table" target="#tab_2">2</ref>, the proposed multitask network has a classification accuracy of 98% for all the testing data (237 CT images). Among those 207 (7 tip fold-over and 200 non fold-over cases) are real CT images where the proposed model is 99% accurate regarding the classification by misclassifying 3 non fold-over test cases. In addition, the model misclassified 1 synthetic CT with folded over condition which degraded its accuracy to 97% for synthetic data. Although segmentation is not our primary goal, the model was able to consistently capture the location of the EA (example shown in Fig. <ref type="figure" target="#fig_4">5</ref>). Inference time for our network was 0.60 ± 0.10 s.  On the other hand, ResNet18 demonstrated lower classification accuracy (around 87%) while misclassifying several synthetic CTs and the real CTs with tip folded over.</p><p>In our study, the 3D versions of VAE and the GAN networks rendered promising segmentation results comparable to those of the 3D U-Net. However, in case of tip fold-over detection both the networks classified all the fold-over cases as non fold-overs. This suboptimal performance made our implemented VAE and GAN impractical for intra and/or postoperative fold-over detection.</p><p>Table <ref type="table" target="#tab_3">3</ref> presents results of a hyperparameter settings evaluation study. As the optimizer, we considered the Adam and Stochastic Gradient Descent (SGD). The learning rate was varied between 1e-3 to 1e-5; however, the best classification output was obtained using a learning rate of 3e-4. As stated in the methodology, we assigned a higher weight with the classification loss for emphasizing on the classification branch during the training process. Better classification accuracy was obtained for real CTs when the weight was either 2 or 5. From the classification accuracy, sensitivity, and specificity analysis in Table <ref type="table" target="#tab_3">3</ref> it is evident that the Adam optimizer with a learning rate of 3e-5 outperforms the other hyperparameter combinations for real CT fold-over detection. The classification loss weight and the batch size were considered 5 and 20, respectively. Similar hyperparameter analysis was done to select parameters for the other networks evaluated in this study, but not included here for the sake of brevity.</p><p>Using Adam optimizer with learning rate 3e-4, batch size of 20, and classification loss weight of 5, we repeated the training process 8 times to investigate training stability. In one out of eight cases, the resulting model could again correctly classify all the real CTs (7 with folded EA and 13 without) in the testing dataset. For the remaining 7 of 8 models, the network could correctly classify 19 out of 20 real CTs (misclassifying one fold-over case), which results in a classification accuracy of 95% for real postoperative CT images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and Conclusion</head><p>In a CI surgical procedure, the relative positioning of the EA influences the overall outcome of the surgery <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31]</ref>. A tip fold-over case results in poor positioning of the apical electrodes and, hence, can lead to severe consequences including trauma, damage to the residual hearing region and poor hearing restoration. Upon intraoperative detection of such a case, the surgeon can extract and reimplant the array to avoid any folding. The conventional detection processes require experience yet are prone to failure. In addition, due to the incidences of fold-over cases being low, training a model to detect these cases with real data is difficult. Therefore, in this study, we generated a dataset of CT images with folded synthetic electrode arrays with realistic metal artifact. A multitask custom network was proposed and trained with the dataset for array fold detection. We tested the trained model on real post-implantation CTs (7 with folded arrays and 200 without). We were able to train a model that could correctly classify all the fold-over cases while misclassifying only 3 non fold-over cases. In future work, clinical deployment of the model will be investigated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Coronal view (left) and axial view (middle) of electrode array tip fold-over in a postoperative CT. 3D rendering (right) of a folded over electrode array.</figDesc><graphic coords="2,88,29,471,20,247,87,76,81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Synthetic postoperative CT with realistic metal artifact generation process from preoperative CT image.</figDesc><graphic coords="4,59,31,268,52,305,08,181,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Multitasking 3D U-Net architecture for EA segmentation and tip fold-over classification.</figDesc><graphic coords="5,55,47,184,85,341,56,175,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Training and validation loss curves of 3D U-Net.</figDesc><graphic coords="7,69,48,56,54,314,02,84,85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Real CT with tip fold-over condition (left) and segmentation output of the multitasking 3D U-Net (right).</figDesc><graphic coords="7,155,46,463,19,141,40,71,08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Various studies (McJunkin et al., Sabban et al., Dirr et al., Timm et al., Sipari et al., Gabrielpillai et al., Jia et al., Garaycochea et al.) have reported on other approaches for detecting tip fold-over through CT imaging, NRT (Neural Response Telemetry) and EcochG (Electrocochleography)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Overall distribution of the dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Tip fold-over detection result comparison among different networks.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Tip fold-over detection result comparison among different hyperparameter settings.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This study is conducted under the support of <rs type="funder">NIH</rs> grants <rs type="grantNumber">R01DC014037</rs>, <rs type="grantNumber">R01DC008408</rs> and <rs type="grantNumber">T32EB021937</rs>. This content is solely the responsibility of the authors and does not necessarily represent the official views of this institute.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_s2f9apu">
					<idno type="grant-number">R01DC014037</idno>
				</org>
				<org type="funding" xml:id="_vtJEHmU">
					<idno type="grant-number">R01DC008408</idno>
				</org>
				<org type="funding" xml:id="_zzKFDg3">
					<idno type="grant-number">T32EB021937</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">National Institute on Deafness and Other Communication Disorders</title>
	</analytic>
	<monogr>
		<title level="j">Cochlear implants</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4798</biblScope>
			<date type="published" when="2014">2014</date>
			<publisher>US Department of Health and Human Services</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Effects of insertion depth of cochlear implant electrodes upon speech perception</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Audiol. Neurotol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="163" to="172" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frequency map for the human cochlear spiral ganglion: implications for cochlear implants</title>
		<author>
			<persName><forename type="first">O</forename><surname>Stakhovskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Assoc. Res. Otolaryngol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">220</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tip fold-over in cochlear implantation: case series</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Zuniga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Otol. Neurotol. Official Publ. Am. Otological Soc. Amer. Neurotol. Soc. Eur. Acad. Otol. Neurotol</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">199</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Review on cochlear implant electrode array tip fold-over and scalar deviation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhanasingh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Otol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="94" to="100" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Potential insertion complications with cochlear implant electrodes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ishiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Risi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cochlear Implants Int</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Value of routine plain x-ray position checks after cochlear implantation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dirr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Otol. Neurotol</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1666" to="1669" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Early outcomes with a slim, modiolar cochlear implant electrode array</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcjunkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Durakovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Buchman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Otol. Neurotol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="e33" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Intra-operative radiological diagnosis of a tip roll-over electrode array displacement using fluoroscopy, when electrophysiological testing is normal: the importance of both techniques in cochlear implant surgery</title>
		<author>
			<persName><forename type="first">O</forename><surname>Garaycochea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manrique-Huarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manrique</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Braz. J. Otorhinolaryngol</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="38" to="40" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial spread of neural excitation: comparison of compound action potential and forward-masking data in cochlear implant recipients</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Audiol</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="346" to="355" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robot-assisted perception augmentation for online detection of insertion failure during cochlear implant surgery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Wanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Simaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotica</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1598" to="1615" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incidence for tip foldover during cochlear implantation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gabrielpillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Burck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stöver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Helbig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Otol. Neurotol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1115" to="1121" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The compact 3D convolutional neural network for medical images</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Ahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Standford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning deep spatial lung features by 3d convolutional neural network for early cancer detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Digital Image Computing: Techniques and Applications (DICTA)</title>
		<meeting><address><addrLine>Piscataway</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international Conference on Computer Vision workshops</title>
		<meeting>the IEEE international Conference on Computer Vision workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3154" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HeadLocNet: deep convolutional neural networks for accurate classification and multi-landmark localization of head CTs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">101659</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep 3D convolution neural network for CT brain hemorrhage classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jnawali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Arbabshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer-Aided Diagnosis</title>
		<imprint>
			<biblScope unit="volume">10575</biblScope>
			<biblScope unit="page" from="307" to="313" />
			<date type="published" when="2018">2018. 2018</date>
			<publisher>SPIE</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-4_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic segmentation of intracochlear anatomy in conventional CT</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Labadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Majdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2625" to="2632" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic identification and 3D rendering of temporal bone anatomy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Labadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Otol. Neurotol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="436" to="442" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic segmentation of the facial nerve and chorda tympani using image registration and statistical priors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Warren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Labadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Dawant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing</title>
		<imprint>
			<biblScope unit="volume">6914</biblScope>
			<biblScope unit="page">69140</biblScope>
			<date type="published" when="2008">2008. 2008</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sensitivity of intra-cochlear anatomy segmentation methods to varying image acquisition parameters</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Banalagay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Labadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image-Guided Procedures, Robotic Interventions, and Modeling</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="111" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning based metal artifacts reduction in post-operative cochlear implant CT imaging</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11769</biblScope>
			<biblScope unit="page" from="121" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32226-7_14</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32226-7_14" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation and radiomics survival prediction: Contribution to the brats 2017 challenge</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kickingereder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bendszus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-75238-9_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-75238-9_25" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2017</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Kuijf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Menze</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Reyes</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10670</biblScope>
			<biblScope unit="page" from="287" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46493-0_38</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46493-0_38" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">CNN-based segmentation of medical imaging data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kayalibay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1701.03056</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Variational autoencoders for deforming 3d mesh models</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5841" to="5850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vox2Vox: 3D-GAN for brain tumour segmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Cirillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abramian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eklund</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-72084-1_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-72084-1_25" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2020</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12658</biblScope>
			<biblScope unit="page" from="274" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">How cochlear implants encode speech</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Otolaryngol. Head Neck Surg</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="444" to="448" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cochlear implants: current designs and future possibilities</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Dorman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Rehabil. Res. Dev</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="695" to="730" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
