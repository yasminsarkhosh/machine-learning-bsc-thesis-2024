<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation</title>
				<funder ref="#_S9wnPq2 #_9FDpBMY #_SxU2sgP">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_fjmsqZF">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhihao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Artificial Intelligence Institute of Wuhan University</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Hubei Key Laboratory of Multimedia and Network Communication Engineering</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">National Englineering Research Center for Multimedia Software</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Computer Vision Laboratory</orgName>
								<orgName type="department" key="dep2">Swiss Federal Institute of Technology Lausanne (EPFL)</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Dianei Technology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
							<email>yongchao.xu@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Artificial Intelligence Institute of Wuhan University</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Dianei Technology</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenhui</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Artificial Intelligence Institute of Wuhan University</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Du</surname></persName>
							<email>dubo@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Artificial Intelligence Institute of Wuhan University</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Hubei Key Laboratory of Multimedia and Network Communication Engineering</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">National Englineering Research Center for Multimedia Software</orgName>
								<address>
									<settlement>Hubei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="681" to="691"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">798B327A8B6E965AED5F575AE6F3B459</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_65</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pulmonary lesion segmentation</term>
					<term>Pulmonary mass segmentation</term>
					<term>Test-time adaptation</term>
					<term>Multi-scale</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pulmonary nodules and masses are crucial imaging features in lung cancer screening that require careful management in clinical diagnosis. Despite the success of deep learning-based medical image segmentation, the robust performance on various sizes of lesions of nodule and mass is still challenging. In this paper, we propose a multi-scale neural network with scale-aware test-time adaptation to address this challenge. Specifically, we introduce an adaptive Scale-aware Test-time Click Adaptation method based on effortlessly obtainable lesion clicks as test-time cues to enhance segmentation performance, particularly for large lesions. The proposed method can be seamlessly integrated into existing networks. Extensive experiments on both open-source and in-house datasets consistently demonstrate the effectiveness of the proposed method over some CNN and Transformer-based segmentation methods. Our code is available at https://github.com/SplinterLi/SaTTCA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Lung cancer is the main cause of cancer death worldwide <ref type="bibr" target="#b18">[18]</ref>. Pulmonary nodules and masses are both features present in computed tomography images that aid in the diagnosis of lung cancer. The primary difference is that a nodule is smaller than 30 mm in diameter, while a mass is larger than 30 mm <ref type="bibr" target="#b22">[22]</ref>. Early detection of these features is crucial to aid physicians in making a diagnosis of Z. Li and J. Yang-Equal contributions. Visualization on results of four large-scale mass segmentation given by nnU-Net baseline <ref type="bibr" target="#b6">[7]</ref>. Compared with the ground-truth segmentation, the recall rate for these four samples is 46.29%, 58.34%, 79.51%, and 68.51%, respectively. This is significantly lower than the mean value of 81.68%. (b): Statistics of the number of nodules at different scales in three datasets. The range of nodule diameter corresponding to Micro, Small, Medium, and Mass is (0, 10], <ref type="bibr" target="#b10">(10,</ref><ref type="bibr" target="#b20">20]</ref>, <ref type="bibr" target="#b20">(20,</ref><ref type="bibr">30</ref>], [30, ∞), respectively. (c) <ref type="bibr">:</ref> The distribution of recall rate with respect to the nodule size. Existing methods have low recall rates for the segmentation of large scale nodules and masses.</p><p>benign or malignant tumors <ref type="bibr" target="#b27">[27]</ref> and determining follow-up treatment. Lesion segmentation can be utilized to evaluate two important factors: the volume of the lesion and its growth rate <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b12">12]</ref>. Furthermore, obtaining accurate information regarding the nodule can assist in determining the appropriate resection method and surgical margin required to preserve as much lung function as possible. <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b17">17]</ref>.</p><p>Segmenting nodules is a tedious task that requires significant human labor. Computer aided diagnosis (CAD) systems can significantly reduce such heavy workloads. The accuracy of the existing nodule detection model reaches 96.1% <ref type="bibr" target="#b9">[9]</ref> accuracy. However, the accuracy of the 3D nodule segmentation model is prone to significantly decline in the application, regardless of whether its structure is based on CNN or Transformer <ref type="bibr" target="#b1">[2]</ref>. As shown in Fig. <ref type="figure" target="#fig_0">1(a-c</ref>), the recall rate of the large-scale nodule and mass is usually lower than the average level. The main reason is that the lesion scale in the two public datasets are relatively small, which matches the fact few patients have very large nodule or mass. This makes the pulmonary nodule and mass segmentation task resemble a long-tail problem rather than a mere large scale span problem. This leads to unsatisfactory results when segmenting large lesions that require more accurate delineation <ref type="bibr" target="#b26">[26]</ref>.</p><p>Several studies have proposed solutions to tackle the large scale span challenges at both the input and feature level. For instance, some approaches adopt multi-scale inputs <ref type="bibr" target="#b3">[4]</ref>, where the input images are resized to different resolu-tion ratios. Some other methods leverage multi-scale feature maps to capture information from different scales, such as cross-scale feature fusion <ref type="bibr" target="#b19">[19]</ref> or using multi-scale convolutional filters <ref type="bibr" target="#b2">[3]</ref>. Furthermore, the attention mechanisms <ref type="bibr" target="#b23">[23]</ref> has also been utilized to emphasize the features that are more relevant for segmentation. Though these methods have achieved impressive performance, they still struggle to accurately segment the extremely imbalanced multi-scale lesions.</p><p>Recently, some click-based lesion segmentation methods <ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref> introduce the click at the input or feature level and modify the network accordingly, resulting in higher accuracy results. Yet, the click input does not provide the scale information of lesions for the network.</p><p>In this paper, we propose a scale-aware test-time click adaptation (SaTTCA) method, which simply utilizes easily obtainable lesion click (i.e., the center detected nodule) to adjust the parameters of the network normalization layers <ref type="bibr" target="#b24">[24]</ref> during testing. Note that we do not need to exploit any data from the training set. Specifically, we expand the click into an ellipsoid mask, which supervises the test-time adaptation. This helps to improve the segmentation performance of large-scale nodules and masses. Additionally, we also propose a multi-scale input encoder to further address the problem of imbalanced lesion scales. Experimental results on two public datasets and one in-house dataset demonstrate that the proposed method outperforms existing methods with different backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Restatement of Image Segmentation Based on Click</head><p>For pulmonary nodule and mass segmentation, existing methods mostly rely on regions of interest (ROI) obtained by lesion detection networks. A set of 3D ROI inputs I can be represented as I ∈ R D×H×W with size (D, H, W ), along with its corresponding segmentation ground truth of nodules and masses represented by S ∈ (0, 1)</p><p>D×H×W . Typically, a neural network with weighted parameters θ is trained to predict the lesion area Ŝ = θ(I), with the goal of minimizing the loss function L (S, Ŝ). The stochastic gradient descent (SGD) and the automatic data acquisition module weight decay (AdamW) optimizers are usually used to optimize the weighted parameters.</p><p>For each ROI input, the center point C of the lesion, which is represented as</p><formula xml:id="formula_0">P c = ( D 2 , H 2 , W</formula><p>2 ) in Cartesian coordinate system, can be used as a reference point to assist the network in improving segmentation performance. This can be achieved either through an artificial or automatic approach, for instance, by adding click channels directly to the input or by adding a prior encoder to the network as demonstrated by the methods <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref>. However, incorporating clicks in this way does not focus on addressing the extremely imbalanced lesion scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Architecture</head><p>The network structure of the proposed method, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>, is enhanced with a multi-scale (MS) input encoder to address the issue of multi-scale lesions. We first get the predicted segmentation Ŝi and compute its minimum 3D bounding box B from the trained model. Then we generate an ellipsoid mask Mi (around the center Ci of the detected nodule) whose size is proportional to the size of B to supervise the parameter updating during test-time adaptation. Our SaTTCA method is applicable to backbones on CNN and Transformer. We also adopt a multiscale input encoder to further improve the segmentation performance of nodules and masses with different scales.</p><p>To achieve this, we employ a clipping strategy to adjust the proportion of foreground and background in the input image, producing a group of input images with dimensions of 64 × 96 × 96, 32 × 48 × 48, and 16 × 24 × 24. These images are then passed through three convolution paths. The feature maps are concatenated as they are down-sampled to the same scale. The subsequent modules can be based on either CNN or transformer structures. The multi-scale input encoder allows the network to capture more scale information of the nodules and masses, thus mitigating the problem of large lesion scale span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Scale-Aware Test-Time Click Adaptation</head><p>In clinical scenarios, the neural network for assisted diagnosis is generally a pretraining model. Due to differences in the statistical distribution of pulmonary nodule scale in image data from different medical centers, the segmentation results of some images, especially for large nodules, are worse than expected. For such scenarios, we propose the Scale-aware Test-time Click Adaptation method, which can improve the performance of segmentation results for largescale nodules and masses by adjusting some of the network parameters during testing. The pipeline of the proposed method is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. First, we use the pre-trained network to pre-segment the input CT from the test set, getting Ŝi = θ</p><formula xml:id="formula_1">(I i ) (i = 1, 2, • • • , n)</formula><p>where n is the number of samples in the test set. Then we make a projection on the main connected region of Ŝi along three coordinate axes to obtain the size of the bounding box B i = (d, w, h) of the pre-segmentation result, and generate an ellipsoid M i with three axes length proportional to the corresponding side length of the bounding box B i . More formally, the coordinates of any foreground voxel point V : (x, y, z) in M i meets the following requirement:</p><formula xml:id="formula_2">(x -D 2 ) 2 R (d) 2 + (y -H 2 ) 2 R (h) 2 + (z -W 2 ) 2 R (w) 2 = 1,<label>(1)</label></formula><p>where R represents the mapping function between the axis length of the ellipsoid and the side length of the bounding box B i . Taking the x-axis as an example, R(d) is given by: R (d) = min (0.02</p><formula xml:id="formula_3">× d 2 , 0.8 × d). (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>To account for the introduction of error information at some voxels during adaptive click adjustment, we develop a mapping function to generate M i adaptively based on the size of nodules and masses. If the nodule's length and diameter are less than 7 mm, M i degenerates into a voxel. When the predicted nodule size ranges from 7 mm to 40 mm, the axial length of B i and the side length of the bounding box follow a quadratic nonlinear relationship. If the predicted nodule size is greater than 40 mm, the axial length of M i has a linear relationship with the side length. To determine the super parametric values for the mapping function R, we perform cross-validation on three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training Objective of SaTTCA</head><p>We use the foreground range of adaptively adjusted ellipsoid M i to mask Ŝi to obtain a masked segmentation ŜM i . Then we use M i to adjust the normalization layer parameters in the network during testing <ref type="bibr" target="#b24">[24]</ref>. The test-time loss function L tt is the weighted sum of the binary cross-entropy loss L BCE and the Dice loss with sigmoid L Dice of M i and ŜM i , and the information entropy loss L ent of Ŝi . Formally, L tt is given by:</p><formula xml:id="formula_5">L tt = L BCE + σL Dice + γL ent ,<label>(3)</label></formula><p>where σ and γ are hyper-parameters set to 0.5 and 1 in all experiments, respectively. The sum of the first two equations is referred to as click loss L Click .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Evaluation Protocols</head><p>We experiment on two public datasets and one in-house dataset. All three datasets are divided into training, validation, and test sets using a 7:1:2 ratio. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIDC [1]:</head><p>The LIDC dataset is a publicly available lung CT image database containing 1018 scans, developed by the Lung Image Database Consortium (LIDC). All pulmonary nodules and masses in the dataset have been annotated by multiple raters. To generate the ground truth for each nodule and mass, we combined the segmentation annotations from different raters. Overall, we selected a total of 1625 nodules and masses that were annotated by more than three raters from the LIDC dataset for the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LNDb [16]:</head><p>The LNDb dataset published in 2019, comprises 294 CT scans collected between 2016 and 2018. Each CT scan in the dataset has been segmented by at least one radiologist. The nodules included in this dataset are larger than 3 mm. The mean scale of the lesion in LNDb dataset is the shortest among the three datasets. We adopt 1968 nodules and masses from the LNDb dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In-House Data (ours):</head><p>The in-house data (ours) contains 4055 CT scans and 6864 nodules and masses. Every CT scans are annotated with voxel-level nodule masks by radiologists. We exclude nodules and masses with diameters larger than 64 mm or smaller than 2 mm, as the diameter of the largest mass in the public dataset is no more than 64 mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics:</head><p>The performance of the nodule segmentation is evaluated by three metrics: volume-based Dice Similarity Coefficient (DSC), surface-based Normalized Surface Dice (NSD) <ref type="bibr" target="#b13">[13]</ref>, and recall rate, which calculates the shape similarity between predictions and ground truth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>The ROI of the lesion is a patch cropped around nodules or masses from the original CT scans with shape 64 × 96 × 96. During pre-processing, Hounsfield Units (HU) values in all patches are first clipped to the range of <ref type="bibr">[-1350, 150</ref>]. Min-max normalization is then applied, scaling HU values into the range of [0, 1]. All models are trained using AdamW <ref type="bibr" target="#b11">[11]</ref> optimizer, cosine annealing learning rate schedule <ref type="bibr" target="#b10">[10]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We adopt nnUNet <ref type="bibr" target="#b6">[7]</ref> and TransBTS <ref type="bibr" target="#b25">[25]</ref> as the backbone to evaluate the proposed method on pulmonary nodule and mass segmentation. nnUNet is a robust baseline with a complete CNN structure. Its adaptive framework makes it wellsuited for pulmonary nodule segmentation. TransBTS is a 3D medical image segmentation network with a hybrid architecture of transformer and CNN. It incorporates long-range dependencies into the traditional CNN structure to achieve a larger receptive field. The experimental results presented in Table <ref type="table" target="#tab_0">1</ref>, consistently demonstrate that the CNN-based network can achieve better results in multiscale pulmonary nodule and mass segmentation tasks across all three datasets. This is mainly due to the fact that large receptive fields may involve background features that are not conducive to segmentation inference for micro or small nodules. In datasets such as LIDC and In-House, where the number imbalance of multi-scale lesion phenomena is more notable, the multi-input method consistently outperforms the other two baselines. We also implemented comparative experiments with other click methods <ref type="bibr" target="#b20">[20]</ref> and <ref type="bibr" target="#b19">[19]</ref>. As depicted in Table <ref type="table" target="#tab_0">1</ref>, the experimental results show that using a point and a fixed range of gaussian intensity expansion in the case of large fluctuations in the size of the nodules does not take effect in improving the segmentation performance. The inferior segmentation results of <ref type="bibr" target="#b19">[19]</ref> can be attributed to the fact that when it fuses features of different depths and scales, the number of channels in the feature map remains the same, and some of the up-sampling or down-sampling strides are too large, leading to redundancy in shallow features and a lack of deep features. Moreover, the SaTTCA improves the Dice coefficient and surface-based Normalized Surface Dice of segmentation results in both networks. In particular, as demonstrated in Fig. <ref type="figure" target="#fig_3">3</ref>, the recall rate of large nodule segmentation is significantly improved.</p><p>We further analyze the performance of SaTTCA. Firstly, we present the quantitative comparison in Table <ref type="table" target="#tab_1">2</ref>, where we group the nodules and masses in each dataset at 10 mm intervals and calculate the average segmentation performance differences of the nodules in each scale group. The statistical results show that the proposed SaTTCA significantly improves the recall rate of the segmentation on large nodules and masses. As shown in Fig. <ref type="figure" target="#fig_3">3</ref>(a), for nodules smaller than 20 mm, both TTCA and SaTTCA effectively increase the recall rate of predicted segmentation. For the medium nodule and mass, our SaTTCA proves to be more effective in improving segmentation performance. Fig. <ref type="figure" target="#fig_3">3(b)</ref> shows the mean recall rate for lesions at every scale. The difference between the two scatter diagrams indicates that the proposed SaTTCA effectively alleviates the issue of extremely imbalanced lesion scales, and improves the segmentation performance for large lesions. In addition, for ten epochs of TTA, the inference time of each sample will increase approximately one second comparing with baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper introduces a novel approach called the Scale-aware Test-time Click Adaptation for nodule and mass segmentation, which aims to address the issue of extremely imbalanced lesion scale and poor segmentation performance on largescale nodules and masses. The network parameters are adapted at the instance level according to the scale-aware click during testing without altering the model architecture. This allows the network to achieve high recall for large-scale lesions. Then, a multi-scale input encoder is also proposed to enhance the segmentation performance of multi-scale nodules and masses. Extensive experiments on two public datasets and one in-house dataset demonstrate that though SATTCA increases inference time for each sample by about one second, it outperforms the corresponding baseline and click-based methods with different backbones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a): Visualization on results of four large-scale mass segmentation given by nnU-Net baseline [7]. Compared with the ground-truth segmentation, the recall rate for these four samples is 46.29%, 58.34%, 79.51%, and 68.51%, respectively. This is significantly lower than the mean value of 81.68%. (b): Statistics of the number of nodules at different scales in three datasets. The range of nodule diameter corresponding to Micro, Small, Medium, and Mass is (0, 10], (10, 20], (20, 30], [30, ∞), respectively. (c): The distribution of recall rate with respect to the nodule size. Existing methods have low recall rates for the segmentation of large scale nodules and masses.</figDesc><graphic coords="2,43,29,53,66,337,36,171,04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The pipeline of the proposed Scale-aware Test-time Click Adaptation (SaTTCA). We first get the predicted segmentation Ŝi and compute its minimum 3D bounding box B from the trained model. Then we generate an ellipsoid mask Mi (around the center Ci of the detected nodule) whose size is proportional to the size of B to supervise the parameter updating during test-time adaptation. Our SaTTCA method is applicable to backbones on CNN and Transformer. We also adopt a multiscale input encoder to further improve the segmentation performance of nodules and masses with different scales.</figDesc><graphic coords="4,42,81,54,41,338,20,165,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>from 10 -3 to 10 -6 and batch size of 32. The training epoch is set to 200, and the test-time training epoch is set to 10. All experiments are conducted on 4 NVIDIA RTX 3090 GPUs with PyTorch 1.11.0 [15].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a): Visualization of some segmentation results predicted by the baseline without test-time adaptation (TTA), with test-time click adaptation (TTCA), and scaleaware test-time click adaptation (SaTTCA). The recall rates show that SaTTCA significantly improves the segmentation performance for large nodules and masses. (b):The recall rate with respect to different scale of nodules/masses for the baseline method (top) and the proposed SaTTCA (bottom).</figDesc><graphic coords="8,46,29,54,38,331,60,188,32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of different backbones with or without the proposed SaTTCA and other click-based methods. Experiments are conducted with various pulmonary nodule segmentation datasets using 3D nnUNet<ref type="bibr" target="#b6">[7]</ref>, TransBTS<ref type="bibr" target="#b25">[25]</ref> and nnUNet with multi-scale input encoder (MS) as the backbone. Comparative experiments are carried out with the click-based methods in<ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref> and simple test-time click adaptation on MS-UNet.</figDesc><table><row><cell>Backbones</cell><cell>LIDC</cell><cell>LNDb</cell><cell>In-House</cell></row><row><cell></cell><cell cols="3">DSC↑ NSD↑ Recall↑ DSC↑ NSD↑ Recall↑ DSC↑ NSD↑ Recall↑</cell></row><row><cell>TransBTS [25]</cell><cell>71.42 88.76 81.58</cell><cell>64.91 90.69 78.99</cell><cell>73.48 88.49 81.45</cell></row><row><cell>TransBTS + SaTTCA</cell><cell>72.08 89.55 82.46</cell><cell cols="2">65.72 91.89 80.18 74.52 89.53 82.09</cell></row><row><cell>nnUNet [7]</cell><cell>74.86 92.12 82.32</cell><cell>70.30 94.86 75.28</cell><cell>77.88 92.37 81.68</cell></row><row><cell>nnUnet + SaTTCA</cell><cell cols="2">75.62 93.05 83.61 71.46 96.01 78.68</cell><cell>78.87 93.55 83.53</cell></row><row><cell>nnUNet + MS</cell><cell>76.62 93.75 81.21</cell><cell>69.82 94.69 75.82</cell><cell>78.54 93.07 83.15</cell></row><row><cell>nnUNet + MS + [20]</cell><cell>76.96 92.39 78.53</cell><cell>69.56 93.76 76.11</cell><cell>77.39 92.03 79.14</cell></row><row><cell>nnUNet + MS + [19]</cell><cell>71.97 91.78 73.90</cell><cell>67.62 91.02 71.75</cell><cell>75.84 91.10 77.74</cell></row><row><cell>nnUNet + MS + TTCA</cell><cell>76.74 93.85 82.03</cell><cell>69.81 94.69 75.81</cell><cell>78.66 93.19 83.43</cell></row><row><cell cols="2">nnUNet + MS + SaTTCA 77.40 94.63 82.60</cell><cell>70.62 95.04 76.09</cell><cell>79.62 94.09 85.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The performance variation (%) of using Test-time Click Adaptation (TTCA) and Scale-aware TTCA (SaTTCA) on the nnUNet<ref type="bibr" target="#b6">[7]</ref> with multi-scale input encoder.</figDesc><table><row><cell>Scale</cell><cell>LIDC</cell><cell></cell><cell></cell><cell>LNDb</cell><cell></cell><cell></cell><cell>In-House</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="9">Δ DSC Δ NSD Δ Recall Δ DSC Δ NSD Δ Recall Δ DSC Δ NSD Δ Recall</cell></row><row><cell cols="4">w/Test-time Click Adaptation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Micro</cell><cell cols="3">-0.034 -0.072 0.440</cell><cell cols="3">-0.078 -0.017 -0.089</cell><cell cols="3">-0.063 -0.042 0.334</cell></row><row><cell>Small</cell><cell>0.042</cell><cell cols="2">-0.087 0.824</cell><cell>0.038</cell><cell>0.066</cell><cell>0.218</cell><cell cols="3">-0.016 -0.046 0.399</cell></row><row><cell cols="2">Medium 0.147</cell><cell>0.114</cell><cell>0.795</cell><cell>0.136</cell><cell>0.195</cell><cell>0.217</cell><cell>0.040</cell><cell>0.023</cell><cell>0.431</cell></row><row><cell>Mass</cell><cell>0.530</cell><cell>0.465</cell><cell>0.994</cell><cell cols="3">-0.007 -0.006 -0.002</cell><cell>0.107</cell><cell>0.099</cell><cell>0.378</cell></row><row><cell cols="6">w/Scale-aware Test-time Click Adaptation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Micro</cell><cell>0.593</cell><cell>0.642</cell><cell>1.256</cell><cell>0.783</cell><cell>0.277</cell><cell>0.253</cell><cell>0.900</cell><cell>0.892</cell><cell>0.518</cell></row><row><cell>Small</cell><cell>0.831</cell><cell>0.944</cell><cell>1.547</cell><cell>0.740</cell><cell>0.296</cell><cell>0.278</cell><cell>1.109</cell><cell>1.045</cell><cell>0.881</cell></row><row><cell cols="2">Medium 1.337</cell><cell>1.892</cell><cell>1.693</cell><cell>0.930</cell><cell>1.014</cell><cell>0.964</cell><cell>1.324</cell><cell>1.219</cell><cell>1.646</cell></row><row><cell>Mass</cell><cell>2.963</cell><cell>3.182</cell><cell>2.701</cell><cell>2.590</cell><cell>3.558</cell><cell>3.093</cell><cell>1.710</cell><cell>1.656</cell><cell>2.676</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgement. This work was supported by the <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">2018AAA0100400</rs>), and in part by the <rs type="funder">National Natural Science Foundation of China</rs> (under Grants <rs type="grantNumber">62225113</rs>, <rs type="grantNumber">62222112</rs> and <rs type="grantNumber">62176186</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fjmsqZF">
					<idno type="grant-number">2018AAA0100400</idno>
				</org>
				<org type="funding" xml:id="_S9wnPq2">
					<idno type="grant-number">62225113</idno>
				</org>
				<org type="funding" xml:id="_9FDpBMY">
					<idno type="grant-number">62222112</idno>
				</org>
				<org type="funding" xml:id="_SxU2sgP">
					<idno type="grant-number">62176186</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on ct scans</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Armato</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="915" to="931" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transdeeplab: convolution-free transformer-based deeplab v3+ for medical image segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiresolution aggregation transformer unet based on multiscale input and coordinate attention for medical image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3820</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluation of individuals with pulmonary nodules: when is it lung cancer?: diagnosis and management of lung cancer: American college of chest physicians evidence-based clinical practice guidelines</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chest</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>e93S-e120S</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimisation of volume-doubling time cutoff for fastgrowing lung nodules in ct lung cancer screening reduces false-positive referrals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heuvelmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1836" to="1845" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">nnu-net: a self-configuring method for deep learning-based biomedical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Methods</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning tumor growth via follow-up volume prediction for lung nodules</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="508" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stbi-yolo: a real-time object detection method for lung nodule recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="75385" to="75394" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sgdr: stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Guidelines for management of incidental pulmonary nodules detected on ct images: from the fleischner society</title>
		<author>
			<persName><forename type="first">H</forename><surname>Macmahon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="228" to="243" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep learning to achieve clinically applicable segmentation of head and neck anatomy for radiotherapy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04430</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Anatomic thoracoscopic pulmonary segmentectomy under 3dimensional multidetector computed tomography simulation: a report of 52 consecutive cases</title>
		<author>
			<persName><forename type="first">H</forename><surname>Oizumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Thoracic Cardiovasc. Surg</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="678" to="682" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pytorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pedrosa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08434</idno>
		<title level="m">Lndb: a lung nodule database on computed tomography</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Anatomic segmentectomy in the treatment of stage i nonsmall cell lung cancer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Schuchert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Thoracic Surg</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="926" to="933" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Global cancer statistics 2020: Globocan estimates of incidence and mortality worldwide for 36 cancers in 185 countries</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CA Cancer J. Clin</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="249" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lesion segmentation and recist diameter prediction via click-driven attention and dual-path connection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="341" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">One click lesion recist measurement and segmentation on ct scans</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="573" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accurate and robust lesion recist diameter prediction and segmentation with transformers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="535" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The probability of lung cancer in patients with incidentally detected pulmonary nodules: clinical characteristics and accuracy of prediction models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vachani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Osuji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chest</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="562" to="571" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tent: fully test-time adaptation by entropy minimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transbts: multimodal brain tumor segmentation using transformer</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="109" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Probabilistic radiomics: ambiguous diagnosis with controllable shape analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical classification of pulmonary lesions: a large-scale radiopathomics study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>International Conference on Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="497" to="507" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
