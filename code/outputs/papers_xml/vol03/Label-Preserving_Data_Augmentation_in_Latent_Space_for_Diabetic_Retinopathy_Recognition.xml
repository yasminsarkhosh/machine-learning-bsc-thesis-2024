<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhihao</forename><surname>Zhao</surname></persName>
							<email>zhihao.zhao@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Information and Technology</orgName>
								<orgName type="institution" key="instit1">TUM School of Computation</orgName>
								<orgName type="institution" key="instit2">Technical University of Munich</orgName>
								<address>
									<addrLine>Arcisstrasse 21</addrLine>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Klinik und Poliklinik für Augenheilkunde</orgName>
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<addrLine>IsmaningerStr. 22</addrLine>
									<postCode>81675</postCode>
									<settlement>München</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junjie</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information and Technology</orgName>
								<orgName type="institution" key="instit1">TUM School of Computation</orgName>
								<orgName type="institution" key="instit2">Technical University of Munich</orgName>
								<address>
									<addrLine>Arcisstrasse 21</addrLine>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Klinik und Poliklinik für Augenheilkunde</orgName>
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<addrLine>IsmaningerStr. 22</addrLine>
									<postCode>81675</postCode>
									<settlement>München</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shahrooz</forename><surname>Faghihroohi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information and Technology</orgName>
								<orgName type="institution" key="instit1">TUM School of Computation</orgName>
								<orgName type="institution" key="instit2">Technical University of Munich</orgName>
								<address>
									<addrLine>Arcisstrasse 21</addrLine>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Sun Yat-Sen University</orgName>
								<address>
									<addrLine>Panyu District 132</addrLine>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathias</forename><surname>Maier</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Klinik und Poliklinik für Augenheilkunde</orgName>
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<addrLine>IsmaningerStr. 22</addrLine>
									<postCode>81675</postCode>
									<settlement>München</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nassir</forename><surname>Navab</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information and Technology</orgName>
								<orgName type="institution" key="instit1">TUM School of Computation</orgName>
								<orgName type="institution" key="instit2">Technical University of Munich</orgName>
								<address>
									<addrLine>Arcisstrasse 21</addrLine>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">Ali</forename><surname>Nasseri</surname></persName>
							<email>nasseri@in.tum.de</email>
							<affiliation key="aff1">
								<orgName type="department">Klinik und Poliklinik für Augenheilkunde</orgName>
								<orgName type="institution">Technische Universität München</orgName>
								<address>
									<addrLine>IsmaningerStr. 22</addrLine>
									<postCode>81675</postCode>
									<settlement>München</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08BEBC5BE97E77548D4A1AAAACCAEDCD</idno>
					<idno type="DOI">10.1007/978-3-031-43898-128.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Diabetic Retinopathy</term>
					<term>Latent Space</term>
					<term>Data Augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>AI based methods have achieved considerable performance in screening for common retinal diseases using fundus images, particularly in the detection of Diabetic Retinopathy (DR). However, these methods rely heavily on large amounts of data, which is challenging to obtain due to limited access to medical data that complies with medical data protection legislation. One of the crucial aspects to improve performance of the AI model is using data augmentation strategy on public datasets. However, standard data augmentation methods do not keep the labels. This paper presents a label-preserving data augmentation method for DR detection using latent space manipulation. The proposed approach involves computing the contribution score of each latent code to the lesions in fundus images, and manipulating the lesion of real fundus images based on the latent code with the highest contribution score. This allows for a more targeted and effective label-preserving data augmentation approach for DR detection tasks, which is especially useful given the imbalanced classes and limited available data. The experiments in our study include two tasks, DR classification and DR grading, with 4000 and 2000 labeled images in their training sets, respectively. The results of our experiments demonstrate that our data augmentation method was able to achieve a 6% increase in accuracy for the DR classification task, and a 4% increase in accuracy for the DR grading task without any further optimization of the model architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Retinal fundus images are widely used in diabetic retinopathy (DR) detection through a data-driven approach <ref type="bibr" target="#b3">[4]</ref>. Specifically, lesions in fundus images play a major role in DR recognition. Some of most prominent lesions associated with DR include microaneurysms, hard exudates, soft exudates, hemorrhages, intraretinal microvascular abnormalities, neovascularization <ref type="bibr" target="#b15">[15]</ref>. Hard exudates and soft exudates <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b23">23]</ref> are commonly observed in the early stages of diabetic retinopathy. Therefore, it is important using data augmentation on the lesions of DR images in AI-based models, especially exudates. However, the size and quantity of exudates can potentially serve as discriminatory indicators of the severity of diabetic retinopathy <ref type="bibr" target="#b18">[18]</ref>. As a result, the most crucial issue is that we need augment the lesions without changing the severity level of DR, especially in the region of exudates.</p><p>Classic image processing methods for data augmentation include random rotation, vertical and horizontal flipping, cropping, random erasing and so on. In some cases, standard data augmentation methods can increase the size of dataset and prevent overfitting of the neural network <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">22]</ref>. But in medical field, conventional data augmentation may damage the semantic content of the image. In ophthalmology diagnosis, the eyes sometimes need to be divided into left and right, and each eye has its own features that are important for image recognition. In this case, we cannot augment the image by flipping method. The same is true for random clipping as the clipped area may not contain any pathological information, which may cause labeling errors. Therefore, the standard methods may damage the semantics of the medical image and also cannot add pathological diversity in the lesion region. Synthetic images generated by GANs can handle the problem of pathological diversity <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">11]</ref>, but the generated images are unlabeled. Even if a GAN model is trained separately for different categories of images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref>, there is still a possibility of labeling errors in the generated images. Because the generated images are only a sample of the distribution of a given dataset, and the label of generated images are not controlled. Moreover, it is time consuming to train a new model for each category separately. Therefore, it is important to find a way to use the GAN model to increase the size and diversity of the data while keeping the labels in data augmentation.</p><p>For the aforementioned problems, this paper proposes a data augmentation method for dealing with the class imbalanced problem by manipulating the lesions in DR images to increase the size and diversity of the pathologies while preserving the labels of the data. In this paper, we train a DR image generator based on StyleGAN3 <ref type="bibr" target="#b14">[14]</ref> to perform manipulation on lesions in latent space. To make the fundus images after manipulation more realistic in terms of DR pathologies, we add an LPIPS <ref type="bibr" target="#b26">[26]</ref> loss based on DR detection to the network. In a well-disentangled latent space, one latent code can control the generation of a specific attribute <ref type="bibr" target="#b5">[6]</ref>. In this paper, we mainly focus on exudates augmentation, our purpose is to find the latent code that individually controls the generation of exudates region. Different from StyleSpace <ref type="bibr" target="#b25">[25]</ref>, we do not need to detect the position of latent code in latent space by calculating the jacobian matrix of the generated image with respect to latent space. Instead, we set the mask of exudates as the contribution map of generated image to DR pathologies, and apply backpropagation to obtain the contribution map of DR lesion with respect to latent space. Then we analyze the contribution score of each latent code, the position of latent code that controls the exudates region is determined by highest contribution score.</p><p>The major contributions of this work can be summarized as follows. Firstly, we introduce LPIPS loss based on DR detection in the training phase of Style-GAN3 to make the reconstructed images have more realistic lesions. Secondly we apply channel locating method with gradient and backpropagation algorithms to identify the position of latent code with the highest contribution score to lesions in DR images. Finally, we perform manipulation on lesions for data augmentation. We also show the performance of proposed method for DR recognition, and explore the correlation between label preservation and editing strength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates our overall structure of the data augmentation method. In part A, StyleGAN3 model is trained to generate DR images. The network maps the noise vector Z to W and W + space through two linear connection layers, and then maps it to S space via affine transformation layer A. LPIPS loss based on DR detection was added in training phase. In part B, we use projector P to embed real image to S space, then decoder D trained in part A is used for reconstruction. We also define y as the importance score of DR pathology (exudates), and then the lesion map can be used as the contribution map of y with respect to output image. The meaning of lesion map is the contribution score matrix of each pixel to DR pathology in output image, because 0 in the lesion map means non-lesion, 1 means lesion. If we want to get the contribution score of each latent code to DR pathology in S space, we can compute the gradient of y with respect to S space by applying back-propagation. The gradient list of all latent codes is denoted as R which can be used as a list of contribution score for latent space because of the meaning of lesion map. In part C, we perform manipulation on real image by fusing the latent code of position (l, c) in S space and average space μ with editing strength α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Part A: Training Phase of StyleGAN3</head><p>StyleGAN3 has the ability to finely control the style of generated images. It achieves style variations by manipulating different directions in the latent vector space. In latent space of StyleGAN3, the latent code z ∈ Z is a stochastic normally distributed vector, always called Z space. After the fully connected mapping layers, z is transformed into a new latent space W <ref type="bibr" target="#b0">[1]</ref>. In affine transformation layers of StyleGAN3, the input vectors are denoted as W + space, and the output vectors are denoted as S space. In <ref type="bibr" target="#b25">[25]</ref>, the experiments demonstrate that the style code s ∈ S can better reflect the disentangled nature of the learned distribution than W and W + spaces.</p><p>LPIPS Loss Based on DR Detection. Due to the good performance of LPIPS in generation tasks <ref type="bibr" target="#b26">[26]</ref>, we use additional LPIPS loss in generator of StyleGAN3. But the original LPIPS loss is based on the Vgg and Alex network trained on ImageNet, which don't include fundus images. As we want to make the fundus images after manipulation have more realistic DR pathology, we customized the LPIPS loss based on DR detection tasks. Firstly, we train a DR detection network based on VGG16 <ref type="bibr" target="#b24">[24]</ref> and select the 5 feature maps after max pooling layers as input for LPIPS. In this way, the loss function will make the generator focus more on lesion of fundus images. At last, we add original StyleGAN loss and LPIPS loss together when training.</p><formula xml:id="formula_0">Loss = λ lpips L lpips + λ gan L gan (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Part B: Detecting the Channel of Style Code Having Highest Contribution to Exudates</head><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, we define y = f (x) as the function to compute the importance score of DR pathology for x, so y represents the importance score. The meaning of lesion map is the contribution score matrix of each pixel to DR pathology in output image. In the paper, because the binary segmentation mask only values 1 and 0. the value 1 can give the meaning of contribution, 0 means no contribution. So we can regard binary semantic segmentation mask as a contribution matrix. Due to these prerequisites, the binary semantic mask has same meaning with gradient map. Because of these characteristic that we can consider set the mask as the contribution matrix of y with respect to the output image.</p><formula xml:id="formula_1">⎧ ⎨ ⎩ mask = y = ∂f ∂x img = x (2)</formula><p>When we get output image from the generator of a well-trained StyleGAN3, we apply a pre-trained exudates segmentation network based on Unet <ref type="bibr" target="#b20">[20]</ref> to obtain the mask of the exudates. As Eq. 3 shown, we can compute the importance score y by integral calculus. But images and vectors in middle of StyleGAN3 are not continuous. We also do not know how function f works, the things we know are x is the output image, and lesion mask means the contribution matrix. And if a pixel value in gradient map of output image is 0, then the pixel in output image will also contribute 0 to the lesion because of forward propagation in network. So we can treat lesion map as gradient matrix of y . Since the lesion map has the property that 1 means contribution and 0 means no contribution, and we want to compute the importance score of DR pathology for x. We can rewrite the integral calculus as computing the sum of all pixels in the image based on the mask. Here, denote the pixel-wise dot product. M is the number of pixels in image.</p><formula xml:id="formula_2">y = M -1 0 xdx = M -1 n=0 img mask<label>(3)</label></formula><p>After y, the importance score of DR pathology is computed. We can calculate the contribution of each latent code to DR pathology in S space by computing the gradient of y with respect to each latent code in S space, based on backpropagation algorithm in neural network as shown in the Eq. 4.</p><formula xml:id="formula_3">R k = y L-l = ∂ L-l f ∂S L-l k (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where R is the contribution score list of latent codes in S space, k ∈ K is the dimension of S space, R k is the contribution score of kth-dimension of S space.</p><p>In our paper, S space has K dimensions in total including 6048 dimensions of feature maps and 3040 dimensions of tRGB blocks. So, in StyleGAN3, depending on the structure of the network, the K dimensions can be correspond to different layers l and channels c depending on the number of channels in each layers. L is the number of layers in StyleGAN3. Since we want to do image editing without affecting other features of the image as much as possible to avoid changes in image label. From the experiments in <ref type="bibr" target="#b5">[6]</ref>, a well disentanglement latent space, one latent code can control the generation of a specific attribute. So when editing the image, it is not necessary to manipulate on whole S space. We can only modify the style code with highest contribution score.</p><p>To ensure that the collected style codes are not impacted by individual noise and are consistent across all images, we calculate the average of 1K images. Afterwards, the collected contribution list R are sorted from high to low based on contribution score of each latent code. To make it easier to understand we can convert k to (l, c) based on the layers and channels in StyleGAN3 network structure.</p><formula xml:id="formula_5">(l, c) = k = arg max k∈K 1K i=0 R i k (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Part C: Manipulation on Real Images in Inference Phase</head><p>We project the real images to style spaces using projector provided by Karras et al. <ref type="bibr" target="#b14">[14]</ref>. Based on the position (l, c) of contribution score in R, we perform manipulation on the layer and channel of highest contribution score when the data flow forward in decoder of <ref type="figure" target="#fig_2">StyleGAN3 D(•)</ref>.</p><formula xml:id="formula_6">S new = S (1 -m) + [S + αμ] m I aug = G(S new ) (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where S new denotes the S space after manipulation, I aug denotes the generated image after manipulation. α is the editing strength. μ is the mean vector of style space, which is obtained by projecting lots of real images to S space. m is a special vector with same dimension with S, the value of all elements in the vector is 0 except for the element at the (l, c) position, which is equal to 1. It means which layer and channel is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup Details</head><p>Dataset. We carry out all experiments on publicly available datasets. We train StyleGAN3 on EyePACS <ref type="bibr" target="#b8">[9]</ref> dataset. We use EyeQ <ref type="bibr" target="#b9">[10]</ref> to grade the quality of EyePACS and select 12K images for training and 8K images for testing from the dataset that has been classified as "Good" or "Usable". For evaluation of our data augmentation method. ODIR-5K (Ocular Disease Intelligent Recognition) <ref type="bibr" target="#b17">[17]</ref> dataset was used for DR classification tasks. It include 6392 images with different ocular diseases of which 4000 are training set and 2392 are testing set. APTOS 2019 <ref type="bibr" target="#b13">[13]</ref> dataset was used for grading evaluation of diabetic retinopathy as no DR, mild, moderate, severe and proliferative DR. There are 3662 image with labels in total, we split 2000 as training set and 1662 as testing set.</p><p>Evaluation Metric. We employ Frechet Inception Distance (FID) <ref type="bibr" target="#b12">[12]</ref> and Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b26">[26]</ref> to measure the distance between synthetic and real data distribution. FID and LPIPS can evaluate the quality of images created by a generative model. To measure the performance of human perception score, we apply Precision, Recall, Accuracy and F1 score as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Settings.</head><p>All experiments are conducted on a single NVIDIA RTX A5000 GPU with 24GB memory using PyTorch implementation. For Style-GAN3, we initialize the learning rate with 0.0025 for G(•)and 0.002 for D(•). For optimization, we use the Adam optimizer with β 1 = 0.0, β 2 = 0.99. We empirically set λ lpips = 0.6, λ gan = 0.4. For evaluation of classification and detection tasks, we both initialize the learning rate with 0.001 for the training of VGG16. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation and Results</head><p>Results of Synthetic Images. For visual evaluation of generation of Style-GAN3, we show some synthetic images in Fig. <ref type="figure" target="#fig_1">2</ref>. Generated images in Fig. <ref type="figure" target="#fig_1">2a</ref> show the image without LPIPS loss in training phase. The exudates are sometimes not in the reasonable region, or even not generated. But the synthetic images are visually comparable to real fundus images, in Fig. <ref type="figure" target="#fig_1">2b</ref>. For subjective quality evaluation, we also conduct a user study by randomly choosing 200 images which consists of 100 synthetic images and 100 real images, then professional ophthalmologists are asked to determine which images are synthetic. We evaluate the human perception score by Accuracy, Precision, Recall and F1 score in Table <ref type="table" target="#tab_0">1</ref>. Evidently, the proposed loss function based on LPIPS significantly improves the quality of generated images. And from the human perception score, we can also know that even for professional doctors there is only a 50% possibility to predict which one is real and which one is synthetic. This means that synthetic images might be highly confusing even for experienced clinicians. In ODIR-5k dataset, we perform the manipulation on DR image in training dataset to make DR images have same number with NDR images <ref type="bibr" target="#b16">[16]</ref>. And then we train a classification network based on VGG16 with training dataset. We also perform manipulation on APTOS-2019 dataset for level 2-4. It is important to note that, level 1 in APTOS-2019 means mild severity of NPDR, the main lesion is microaneurysms in this stage. Therefore we will not edit the image at this stage because the exudation has not yet occurred. We compare our data augmentation method with standard method in these two tasks. Quantitative evaluation results are shown in Table <ref type="table" target="#tab_1">2</ref>. The results show that standard data augmentation method will not improve the accuracy too much, even the rotation(-30 • , 30 • ) will reduce the accuracy. Patho-GAN <ref type="bibr" target="#b19">[19]</ref> can not perform augmentation in detection task, because it is not label-preserving method. In detection task, we calculate the accuracy for each level of severity. In level 2 and 3, the accuracy improvement is significant, especially level 3 is most imbalanced in original dataset. In level 4, the accuracy also have been improved, even though the main lesion in this stage is the growing of new abnormal blood vessels, augmenting the dataset can still improve the accuracy of recognition after changing the exudation area. The accuracy of level 0 and 1 have not changed much, the accuracy of level 1 has even decreased, and the reason for this phenomenon is most likely due to the imbalance of data categories caused by not performing augmentation on level 1. This also shows that data imbalance does have an impact on accuracy. Ablation Study. Since our approach is label-preserving data augmentation, it is more concerned with whether our augmentation method affects the label of the image. In order to investigate the correlation between label preservation and editing strength, we conduct comprehensive ablation studies to examine the effect of different editing strength α on the recognition accuracy. As we know, if we change exduates too much, the labels of images will be changed. Such as, level 1 will change to level 2, level 2 will change to level 3. If such a thing happens, our method will not only have no improvement on the accuracy rate, but will also have a negative impact. The Fig. <ref type="figure" target="#fig_2">3a</ref> show the manipulation results with different editing strength. We also conduct experiments of effect on the accuracy with different editing strength as shown in Fig. <ref type="figure" target="#fig_2">3b</ref>. Results show that when editing strength are in the range of (-0.6 0.6), the accuracy will be improved, otherwise the accuracy is significantly reduced. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper we propose a label-preserving data augmentation method to deal with the classes imbalanced problem in DR detection. The proposed approach computes the contribution score of latent code, and perform manipulation on the lesion of real images. In this way, the method can augment the dataset in a label-preserving manner. It is a targeted and effective label-preserving data augmentation approach. Although the paper mainly discusses exudates, the proposed method can be applied to other types of lesions as long as we have the lesion mask and the GAN model can generate realistic lesions.</p><p>Data Declaration. Data underlying the results presented in this paper are available in APTOS <ref type="bibr" target="#b13">[13]</ref>, ODIR-5K <ref type="bibr" target="#b17">[17]</ref>, EyePACS <ref type="bibr" target="#b8">[9]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The flowchart of the proposed data augmentation method. Part A is the training phase of StyleGAN3 which include additional LPIPS loss based on DR detection. In part B, we detect the position (l, c) of latent code with highest contribution score to DR pathologies. Part C is the inference phase of image manipulation on S space of real image.</figDesc><graphic coords="3,45,54,406,16,332,80,102,13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visual results of StyleGAN3.</figDesc><graphic coords="7,45,81,147,32,332,80,81,07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Effects of different editing strengths on DR recognition.</figDesc><graphic coords="9,41,79,193,40,163,72,112,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Image Quality Assessment In our work, we perform the data augmentation method in two tasks. One is the classification of DR images based on ODIR-5K dataset, another is the grading of the DR severity in APTOS-2019. The distribution of the images in ODIR-5K dataset is {34.2,65.8}% for {DR,NDR} in training set and {31.5,68.5}% for {DR,NDR} in testing set.</figDesc><table><row><cell></cell><cell cols="3">Objective Quality Metrics Human Perception Score</cell><cell></cell></row><row><cell></cell><cell cols="2">FID(↓) LPIPS(↓)</cell><cell cols="2">Acc Recall Precision F1</cell></row><row><cell cols="2">w/o LPIPS 9.329</cell><cell>0.457</cell><cell>0.595 0.760 0.571</cell><cell>0.652</cell></row><row><cell>LPIPS</cell><cell>6.459</cell><cell>0.382</cell><cell>0.460 0.462 0.490</cell><cell>0.476</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison with standard data augmentation.</figDesc><table><row><cell></cell><cell>Classification(ACC)</cell><cell></cell><cell></cell><cell cols="3">Detection(level: 0-4 ACC)</cell></row><row><cell></cell><cell>DR</cell><cell cols="6">NoDR Mild Moderate Severe PDR ALL</cell></row><row><cell>No</cell><cell>0.681</cell><cell cols="3">0.988 0.644 0.830</cell><cell cols="3">0.307 0.427 0.798</cell></row><row><cell>Flipping</cell><cell>0.689</cell><cell cols="3">0.991 0.611 0.846</cell><cell cols="3">0.153 0.541 0.805</cell></row><row><cell>rotation</cell><cell>0.676</cell><cell cols="3">0.988 0.688 0.772</cell><cell cols="3">0.307 0.468 0.787</cell></row><row><cell cols="2">Patho-GAN 0.729</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell>0.743</cell><cell cols="3">0.991 0.611 0.891</cell><cell cols="3">0.480 0.552 0.841</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image2stylegan: how to embed images into the stylegan latent space?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4432" to="4441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Implementation of data augmentation to improve performance CNN method for detecting diabetic retinopathy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Agustin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Utami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Al Fatta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 3rd International Conference on Information and Communications Technology (ICOIACT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="83" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automated detection of dark and bright lesions in retinal images for early detection of diabetic retinopathy</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Syst</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3151" to="3162" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diabetic retinopathy detection through deep learning techniques: a review</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Alyoubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Shalash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Abulkhair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Med. Unlocked</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">100377</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data augmentation for improving proliferative diabetic retinopathy detection in eye fundus images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="182462" to="182474" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gan dissection: visualizing and understanding generative adversarial networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.10597</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative Adversarial Networks (GANs) for Retinal Fundus Image Synthesis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bellemo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S W</forename><surname>Ting</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-21074-8_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-21074-824" />
	</analytic>
	<monogr>
		<title level="m">ACCV 2018</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11367</biblScope>
			<biblScope unit="page" from="289" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cataract grading method based on deep convolutional neural networks and stacking ensemble learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Elloumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Imaging Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="798" to="814" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="https://www.kaggle.com/datasets/tanlikesmath/diabetic-retinopathy-resized" />
		<title level="m">EyePACS: diabetic retinopathy detection</title>
		<imprint>
			<date type="published" when="2015-07-20">2015. 20 July 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evaluation of Retinal Image Quality Assessment Networks in Different Color-Spaces</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11764</biblScope>
			<biblScope unit="page" from="48" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32239-7_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32239-76" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Study on the method of fundus image generation based on improved GAN</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Probl. Eng</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hospital</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/competitions/aptos2019-blindness-detection" />
		<title level="m">APTOS 2019 Blindness Detection</title>
		<imprint>
			<date type="published" when="2019-06-27">2019. 27 June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="852" to="863" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The SUSTech-SYSU dataset for automated exudate detection and diabetic retinopathy grading</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">409</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Balancing data through data augmentation improves the generality of transfer learning for diabetic retinopathy classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Mungloo-Dilmohamud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heenaye-Mamode Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jhumka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Beedassy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Mungloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peña-Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">5363</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><surname>Nihds-Pku</surname></persName>
		</author>
		<ptr target="https://odir2019.grand-challenge.org" />
		<title level="m">Ocular Disease Intelligent Recognition ODIR-5K</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodality analysis of hyper-reflective foci and hard exudates in patients with diabetic retinopathy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1568</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Explainable diabetic retinopathy detection and retinal image generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Biomed. Health Inf</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="55" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Segmentation and classification of bright lesions to diagnose diabetic retinopathy in retinal images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Santhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manimegalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parvathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karkuzhali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Eng. (Berl)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discrimination of retinal images containing bright lesions using sparse coded features and SVM</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sidibé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sadek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mériaudeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Bio. Med</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="175" to="184" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stylespace analysis: disentangled controls for stylegan image generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12863" to="12872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
