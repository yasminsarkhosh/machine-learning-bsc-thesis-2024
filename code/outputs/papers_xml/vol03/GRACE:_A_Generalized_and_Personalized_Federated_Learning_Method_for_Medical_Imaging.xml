<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging</title>
				<funder ref="#_WMfHYe3">
					<orgName type="full">State Key Laboratory of UHD Video and Audio Production and Presentation</orgName>
				</funder>
				<funder ref="#_pt3jdUd #_GCfjUpq #_W2JdE76">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_DvCtgnU">
					<orgName type="full">STCSM</orgName>
				</funder>
				<funder>
					<orgName type="full">Wu Wen Jun Honorary Doctoral Scholarship, AI Institute</orgName>
				</funder>
				<funder ref="#_pY4b6PT">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Shanghai Jiao Tong University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruipeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CMIC</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziqing</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CMIC</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qinwei</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CMIC</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CMIC</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CMIC</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
							<email>wangyanfeng@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CMIC</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="14" to="24"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">3E5837AC692FB385B239E811DF317547</idno>
					<idno type="DOI">10.1007/978-3-031-43898-1_2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-24T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Federated learning</term>
					<term>Domain generalization</term>
					<term>Domain shift</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Federated learning has been extensively explored in privacypreserving medical image analysis. However, the domain shift widely existed in real-world scenarios still greatly limits its practice, which requires to consider both generalization and personalization, namely generalized and personalized federated learning (GPFL). Previous studies almost focus on the partial objective of GPFL: personalized federated learning mainly cares about its local performance, which cannot guarantee a generalized global model for unseen clients; federated domain generalization only considers the out-of-domain performance, ignoring the performance of the training clients. To achieve both objectives effectively, we propose a novel GRAdient CorrEction (GRACE) method. GRACE incorporates a feature alignment regularization under a meta-learning framework on the client side to correct the personalized gradients from overfitting. Simultaneously, GRACE employs a consistency-enhanced reweighting aggregation to calibrate the uploaded gradients on the server side for better generalization. Extensive experiments on two medical image benchmarks demonstrate the superiority of our method under various GPFL settings. Code available at https://github.com/MediaBrain-SJTU/GPFL-GRACE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data-driven deep networks maintain the great potential to achieve superior performance under the large-scale medical data <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref>. However, privacy concerns from patients and institutions prevent centralized training from access to the data in multiple centers. Federated learning (FL) thereby becomes a promising compromise that preserves privacy by distributing the model to data sources to train a global model without sharing their data directly <ref type="bibr" target="#b26">[27]</ref>.</p><p>A major challenge to FL in real-world scenarios is domain shift, which refers to the difference of marginal data distributions across centers and induces significant performance degradation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>. Current methods to address the problem of domain shift can be categorized into two directions. One is federated domain generalization (FedDG) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>, which tackles the domain shift between training and testing clients. FedDG aims at obtaining a generalizable global model, but the optimal performance on local training clients cannot be guaranteed. Another direction is personalized federated learning (PFL) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>, which tackles the domain shifts among training clients by personalizing the global model locally. However, both FedDG and PFL only consider the partial objective of GPFL, ignoring either personalization or generalization in real-world scenarios.</p><p>In this paper, we focus on generalized and personalized federated learning (GPFL), which considers both generalization and personalization to holistically combat the domain shift. We notice that one recent work IOP-FL <ref type="bibr" target="#b12">[13]</ref> also studied the problem of GPFL, but it mainly resorts to the model personalization for the unseen clients by test-time training on deployment stage, which did not directly consider enhancing both objectives of GPFL in the training phase.</p><p>We seek a more effective and efficient solution to GPFL in this work. Specifically, our intuition is based on the following conjecture: a more generalizable global model can facilitate the local models to better adapt to the corresponding local distribution, and better adapted local models can then provide positive feedbacks to the global model with improved gradients.</p><p>Based on the above intuition, we propose a novel method named GRAdient CorrEction (GRACE) that can achieve both generalization and personalization during training by enhancing the model consistency on both the client side and the server side. By analyzing the federated training stage in Fig. <ref type="figure" target="#fig_0">1</ref>(a), we discover a significant discrepancy in the feature distributions between the global and local models due to domain shifts, which will influence the training process on both client and server side. To address this problem, we aim to correct the inconsistent gradients on both sides. On the client side, we leverage a meta-learning strategy to align the feature spaces of global and local models while fitting the local data distribution, as depicted in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. Furthermore, on the server side, Fig. <ref type="figure" target="#fig_0">1</ref>. The overall framework of (a) the federated learning system which has two stages for algorithm design and (b) our GRACE method on both the client and server sides.</p><p>we estimate the gradient consistency by computing the cosine similarity among the gradients from clients and re-weight the aggregation weights to mitigate the negative effect of domain shifts on the global model update. Through these two components, GRACE preserves the generalizability in global model as much as possible when personalizing it on local distributions. Comprehensive experiments are conducted on two medical FL benchmarks, which show that GRACE outperforms state-of-the-art methods in terms of both generalization and personalization. We also perform insightful analysis to validate the rationale of our algorithm design. N and we denote the weight vector as p = [p 1 , . . . , p M ] ∈ R M . This method implicitly assumes that all clients share the same data distribution, thus failing to adapt to domain shift scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>To solve the GPFL problem, we propose a GRAdient CorrEction (GRACE) method for both local client training and server aggregation during the federated training stage. Unlike IPO-FL <ref type="bibr" target="#b12">[13]</ref>, our method constrains the model's generalizability and personality only during the federated training stage. Our motivation comes from an observation that domain shift causes a significant mismatch between the feature spaces of the local models and the initial global model after client training at each round. It leads to inconsistent gradients uploaded from the biased local models, which hurts the generalizability of the global model. Therefore, in GRACE, we alleviate the inconsistency between gradients obtained from different clients. The details of GRACE are elaborated in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Local Training Phase: Feature Alignment &amp; Personalization</head><p>We calibrate the gradient during local training via a feature alignment constraint by meta-learning, which preserves the generalizable feature while adapting to the local distributions. We conduct the client-side gradient correction in two steps. Meta-Train: We denote θ t,k m as the personalized model parameter at the local update step k of client m in round t and η as the local learning rate. The first step is a personalization step by the task loss:</p><formula xml:id="formula_0">θ t,k m = θ t,k m -ηΔL task m (θ t,k m ) = θ t,k m -ηΔL m (f (x tr , θ t,k m ), y tr )<label>(1)</label></formula><p>where (x tr , y tr ) ∈ D m is the sampled data and θ t,k m is the updated parameter that will be used in the second step.</p><p>Meta-Update: After optimizing the local task objective, we need a metaupdate to virtually evaluate the updated parameters θ t,k m on the held-out metatest data (x te , y te ) ∈ D m with a meta-objective L meta m . We add the feature alignment regularizer into the loss function of meta-update:</p><formula xml:id="formula_1">L meta m (xte, yte; θ t,k m ) = Lm(f (xte, θ t,k m ), yte) + βL align m (h(xte; φ t g ), h(xte; φ t,k m ))) (2)</formula><p>where h(•; φ) is the feature extractor part of model f (•; θ) and φ is the corresponding parameter, and β is the weight for alignment loss which has a default value of 1.0. Here, we apply three widely-used alignment losses to minimize the discrepancy in the feature space: CORAL <ref type="bibr" target="#b31">[32]</ref>, MMD <ref type="bibr" target="#b15">[16]</ref>, and adversarial training <ref type="bibr" target="#b10">[11]</ref>. We show that varying alignment loss functions can boost the generalization capability during local training and report the results in Table <ref type="table">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Aggregation Phase: Consistency-Enhanced Re-weighting</head><p>We introduce a novel aggregation method on the server side that corrects the global gradient by enhancing the consistency of the gradients received from training clients. We measure the consistency of two gradient vectors by their cosine similarity and use the average cosine similarity among all uploaded gradients as an indicator of gradient quality on generalization. </p><formula xml:id="formula_2">t = {σ t ij } i,j=1,••• ,M , where σ t ij = cos(Δθ t i , Δθ t j ) ∈ R M ×M .</formula><p>The corrected global gradient is:</p><formula xml:id="formula_3">Δθ t g = [Δθ t 1 , • • • , Δθ t M ] • p T , p = Norm[(Σ t • 1 T ) • p].<label>(3)</label></formula><p>"•" means element multiplication of two vectors and "Norm" means to normalize the new weight vector p with M m=1 p m = 1, p m ∈ (0, 1). Then the updated global model for round t + 1 will be θ t+1 g = θ t g -η g Δθ t g , where η g is the global learning rate with default value 1. Theoretical Analysis: We prove that the re-weighting method in Eq.(3) will enhance the consistency of the global gradient based on the FedAvg. First, we define the averaged cosine similarity of Δθ t m as c t m , where c t m = 1 M M n=1 σ t mn . Then, the consistency degree of the global gradient in FedAvg is M m=1 p m c t m . Thus, the consistency degree after applying our GRACE method has</p><formula xml:id="formula_4">M j=1 p j c t j 2 M m=1 p m c t m ≥ M m=1 p m c t m , if ∀m ∈ [1, M], p m ≤ 1 √ M , .<label>(4)</label></formula><p>Note that, we can easily prove the inequality in Eq. ( <ref type="formula" target="#formula_4">4</ref>) by using the generalized arithmetic-geometric mean inequality, which holds when 3 Experiments</p><formula xml:id="formula_5">c t 1 = • • • = c t M .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Experimental Setting</head><p>We evaluate our approach on two open source federated medical benchmarks: Fed-ISIC2019 and Fed-Prostate. The former is a dermoscopy image classification task <ref type="bibr" target="#b8">[9]</ref> among eight different melanoma classes, which is a 6-client federated version of ISIC2019 <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b33">34]</ref>. And the latter is a federated prostate segmentation task <ref type="bibr" target="#b12">[13]</ref> with T2-weighted MRI images from 6 different domains <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>. We follow the settings of <ref type="bibr" target="#b8">[9]</ref> for Fed-ISIC2019 and <ref type="bibr" target="#b21">[22]</ref> for Fed-Prostate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison with SOTA Methods</head><p>We conduct the leave-one-client-out experiment for both benchmarks. In each experiment, one client is selected as the unseen client and the model is trained on the remaining clients. The average performance on all internal clients' test set is the in-domain personalization results, while the unseen client's performance is the out-of-domain generalization results. The final results of each method are the average of all leave-one-domain-out splits, and all results are over three independent runs. (Please see the details of experimental setup in open-source code.) Performance of In-Domain Personalization. For a fair comparison, the baseline method FedAvg <ref type="bibr" target="#b26">[27]</ref> and several current SOTA personalized FL methods are chosen. Ditto <ref type="bibr" target="#b17">[18]</ref> and FedMTL <ref type="bibr" target="#b30">[31]</ref> treat the personalized process as a kind of multi-task learning and generate different model parameters for each client. FedBN <ref type="bibr" target="#b19">[20]</ref> keeps the parameters of BatchNorm layers locally as considering those parameters to contain domain information. FedRep <ref type="bibr" target="#b0">[1]</ref>, Fed-BABU <ref type="bibr" target="#b28">[29]</ref>, FedPer <ref type="bibr" target="#b0">[1]</ref> and FedRoD <ref type="bibr" target="#b3">[4]</ref> all use a personalized head to better fit the local data distribution, where FedRoD also retains a global head for OOD generalization. Besides FedRoD, PerFedAvg <ref type="bibr" target="#b9">[10]</ref> and pFedMe <ref type="bibr" target="#b32">[33]</ref> can also implement personalized and generalized in FL by federated meta-learning. These three methods are also involved in the comparison of out-of-domain generalization. Table <ref type="table" target="#tab_1">1</ref> presents the results of two tasks. GRACE achieves comparable personalization performance on the in-domain clients with SOTA methods. Note that GRACE outperforms most of PFL methods in the table and other methods like FedRoD and Per-FedAvg also achieve good results, which means that improving generalization is also beneficial for model personalization. Performance of Out-of-Domain Generalization. For out-of-domain comparison, we select several FL methods that aim to solve the data heterogeneity problem, such as FedProx <ref type="bibr" target="#b18">[19]</ref>, Scaffold <ref type="bibr" target="#b13">[14]</ref> and MOON <ref type="bibr" target="#b16">[17]</ref>. Some FedDG methods are also chosen for comparison, like ELCFS <ref type="bibr" target="#b21">[22]</ref> and HarmoFL <ref type="bibr" target="#b11">[12]</ref>. (We mark the A-distance corresponding to the round.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Further Analysis</head><p>Ablation Study on Different Parts of Our Method. The detailed ablation studies are shown in Table <ref type="table">4</ref> to further validate the effectiveness of each component in GRACE. Three widely-used approaches from domain adaptation/generalization area are used for feature alignment loss in local training.</p><p>In Table <ref type="table">4</ref>, "Adv." <ref type="bibr" target="#b10">[11]</ref> means using adversarial training between features from the global model and the local model, and "CORAL" <ref type="bibr" target="#b31">[32]</ref> and "MMD" <ref type="bibr" target="#b15">[16]</ref> are classic regularization losses for domain alignment. From the table, our framework can obtain performance improvements on different alignment approaches. Considering both performance and efficiency, we prefer "CORAL" for alignment. Add the server-side correction on top of MOON can also obtain some gains, but the overall effect is limited, since its alignment loss might push the current feature away from features in previous round and thus reduces the discriminativeness.</p><p>Visualization of the Feature Alignment Loss in Eq. (2). The A-distance measurement is used to evaluate the dissimilarity between the local and global models, which is suggested to measure the cross-domain discrepancy in the domain adaptation theory <ref type="bibr" target="#b1">[2]</ref>. We follow the proxy implementation in <ref type="bibr" target="#b23">[24]</ref> and trace the curve of A-distance on FedAvg and our method on each client throughout the training process in Fig. <ref type="figure" target="#fig_1">2(a)</ref>. The curves demonstrate a substantial reduction of feature discrepancy compared with FedAvg. It validates the efficacy of our algorithm design and corroborates our claim that generalization and personalization are compatible objectives. Our personalized local models can close the distance with the global model while preserving a good fit for local data distribution. In addition, we use t-SNE <ref type="bibr" target="#b25">[26]</ref> to visualize the feature distributions in Fig. <ref type="figure" target="#fig_1">2</ref>(b) and GRACE can reduce the discrepancy between global and local features.</p><p>Loss Curves Comparison of FedAvg and Our Method. Fig. <ref type="figure" target="#fig_2">3</ref> shows the loss curves of FedAvg and our GRACE method with only server-side aggregation (Model A in Table <ref type="table">4</ref>). Our method can achieve better global minimization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We introduce GPFL for multi-center distributed medical data with domain shift problems, which aims to achieve both generalization for unseen clients and personalization for internal clients. Existing approaches only focus on either generalization (FedDG) or personalization (PFL). We argue that a more generalizable global model can facilitate the local models to adapt to the clients' distribution, and the better-adapted local models can contribute higher quality gradients to the global model. Thus, we propose a new method GRAdient CorrEction (GRACE), which corrects the model gradient at both the client and server sides during training to enhance both the local personalization and the global generalization. The experimental results on two medical benchmarks show that GRACE can enhance both local adaptation and global generalization and outperform existing SOTA methods in generalization and personalization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2. 1</head><label>1</label><figDesc>Overview of the GPFL Framework Consider a federated learning scenario where M clients possess local private data with one unique domain per client. Let D = {D 1 , D 2 , • • • , D M } be the set of M distributed training source domains, and D o be the distribution of the unseen client. We denote P (X , Y) as the joint input and label space of a task. For client m = 1, 2, . . . , M, the local dataset D m = {(x m i , y m i )} Nm i=1 , with N m = |D m | and N = M m=1 N m being the number of local samples and total samples respectively. Let L m be the task loss function of the local model f (x m ; θ m ), and f g (x; θ g ) be the global model. The goal of GPFL is to learn a generalized global model f g that minimizes the expected risk E (x,y)∈Do [L o (f (x; θ g ), y)] on unseen clients while enabling each participating client to have a personalized local model f that adapts to its local data distribution D m . We characterize the GPFL system in Fig. 1(a), which consists of the federated training stage and the OOD deployment stage. For federated training, there are two iterative phases, namely local training on the client's private data and server aggregation for the global model. The Standard method FedAvg [27] optimizes a global model by an empirical objective min θ M m=1 p m L m ( f (x; θ), y), which is implemented as a weighted aggregation of local models {θ m } M m=1 trained in the clients (θ g = M m=1 p m θ m ). Here p m = Nm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A-distance between global and local models on FedAvg and GRACE. (a) Comparison of change curves over training rounds on each training client (dashed line: FedAvg; solid line: GRACE). (b) t-SNE of global and local features at different rounds. (We mark the A-distance corresponding to the round.)</figDesc><graphic coords="8,62,97,54,11,326,08,85,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Loss curves of FedAvg and GRACE-Server in 3 experiments of Fed-Prostate.</figDesc><graphic coords="9,55,29,54,44,313,60,81,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The main idea is to balance the quantity and quality of data represented by each client's gradients and generate more robust global model parameters by reducing the influence of inconsistent gradients. It is vital for medical scenarios, and existing methods that only measure gradient importance by data size need to be revised.</figDesc><table><row><cell>Let Δθ t m = θ t m -θ t g be the gradient of t round from each client after local training, where θ t m and θ t g are the local and global models on client m at round</cell></row><row><cell>t. The similarity matrix is Σ</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Personalization results for Fed-ISIC2019 and Fed-Prostate benchmark.</figDesc><table><row><cell>Method</cell><cell cols="3">Fed-ISIC2019</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Fed-Prostate</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>avg</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>E</cell><cell>F</cell><cell>avg</cell></row><row><cell>FedAvg</cell><cell cols="14">66.28 67.36 63.27 69.71 71.36 65.29 67.21 90.71 90.29 89.65 91.36 89.50 89.30 90.14</cell></row><row><cell>FedRep</cell><cell cols="14">68.03 67.93 65.41 69.15 68.59 74.96 69.01 91.20 91.00 89.95 91.86 91.60 90.40 91.00</cell></row><row><cell>Ditto</cell><cell cols="14">67.05 70.39 68.79 69.16 69.75 75.52 70.11 91.09 90.83 90.91 91.55 91.23 89.87 90.91</cell></row><row><cell>FedMTL</cell><cell cols="14">69.45 71.07 69.37 71.85 72.29 76.77 71.97 91.09 90.83 90.91 91.55 91.23 89.87 90.91</cell></row><row><cell>FedPer</cell><cell cols="14">71.90 73.00 73.18 75.05 74.45 76.54 74.02 93.20 93.66 93.44 93.66 93.25 92.99 93.37</cell></row><row><cell cols="15">FedBABU 74.40 74.34 73.34 75.96 76.98 77.64 75.44 92.09 93.25 91.33 91.54 92.48 91.12 91.97</cell></row><row><cell>FedBN</cell><cell cols="14">74.74 77.13 75.42 78.19 78.80 78.75 77.17 93.08 92.82 93.20 93.40 93.07 93.17 93.13</cell></row><row><cell>FedRoD</cell><cell cols="14">71.98 71.48 72.05 75.11 75.70 76.64 73.83 92.90 92.92 92.90 93.37 93.74 93.04 93.14</cell></row><row><cell cols="15">Per-FedAvg 75.50 77.64 75.00 76.52 75.72 76.80 76.20 93.54 93.93 93.75 94.22 93.89 93.40 93.79</cell></row><row><cell>pFedMe</cell><cell cols="14">71.75 71.08 68.40 71.60 74.02 77.55 72.40 89.20 88.86 88.97 89.97 89.66 88.73 89.24</cell></row><row><cell cols="15">GRACE 75.71 75.40 74.57 79.08 79.48 78.60 77.14 93.56 93.72 94.08 94.23 93.91 93.44 93.82</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Generalization results for Fed-ISIC2019 and Fed-Prostate benchmark.</figDesc><table><row><cell>Method</cell><cell cols="3">Fed-ISIC2019</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Fed-Prostate</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>avg</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>E</cell><cell>F</cell><cell>avg</cell></row><row><cell>FedAvg</cell><cell cols="14">36.94 62.70 54.25 40.86 39.68 73.37 51.30 89.57 88.63 82.69 85.39 79.21 89.74 85.87</cell></row><row><cell>ELCFS</cell><cell cols="14">37.08 69.37 62.63 38.48 38.44 72.47 53.08 89.91 90.80 84.89 88.19 83.88 87.24 87.47</cell></row><row><cell>FedProx</cell><cell cols="14">34.13 62.77 64.35 36.52 40.25 75.47 52.25 90.55 87.69 83.27 85.42 79.05 90.18 86.03</cell></row><row><cell cols="15">HarmoFL 36.80 74.00 65.74 43.63 46.75 70.21 56.19 92.07 89.17 83.60 85.55 81.86 90.00 87.04</cell></row><row><cell>Scaffold</cell><cell cols="14">36.31 60.83 70.60 40.11 41.37 73.10 53.72 90.47 87.98 84.15 85.27 81.56 89.37 86.47</cell></row><row><cell>MOON</cell><cell cols="14">35.54 61.25 71.53 38.82 44.22 68.26 53.27 89.04 84.12 82.64 85.22 75.38 87.05 83.91</cell></row><row><cell>FedRoD</cell><cell cols="14">45.36 65.12 66.33 47.36 39.36 69.79 55.55 90.09 88.67 81.59 86.88 78.95 87.90 85.68</cell></row><row><cell cols="15">Per-FedAvg 39.44 61.88 69.50 42.63 41.13 70.89 54.24 92.08 89.63 84.94 87.11 78.17 89.35 86.88</cell></row><row><cell>pFedMe</cell><cell cols="14">38.05 63.40 64.08 42.82 42.63 74.33 54.22 83.86 82.68 82.52 78.93 78.54 87.30 82.30</cell></row><row><cell cols="15">GRACE 43.57 76.52 73.58 44.80 41.74 68.99 58.20 91.53 90.15 84.28 87.55 81.39 92.37 87.88</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. The data used in this paper are open by previous works, i.e., Fed-Prostate in [22]; Fed-ISIC2019 in [9] with licence <rs type="grantNumber">CC-BY-NC 4.0</rs>. This work is supported by the <rs type="funder">National Key R&amp;D Program of China</rs> (No. <rs type="grantNumber">2022ZD0160702</rs>), <rs type="funder">STCSM</rs> (No. <rs type="grantNumber">22511106101</rs>, No. <rs type="grantNumber">18DZ2270700</rs>, No. <rs type="grantNumber">21DZ1100-100</rs>), 111 plan (No. <rs type="grantNumber">BP0719010</rs>), and <rs type="funder">State Key Laboratory of UHD Video and Audio Production and Presentation</rs>. <rs type="person">Ruipeng Zhang</rs> is partially supported by <rs type="funder">Wu Wen Jun Honorary Doctoral Scholarship, AI Institute</rs>, <rs type="funder">Shanghai Jiao Tong University</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pY4b6PT">
					<idno type="grant-number">CC-BY-NC 4.0</idno>
				</org>
				<org type="funding" xml:id="_DvCtgnU">
					<idno type="grant-number">2022ZD0160702</idno>
				</org>
				<org type="funding" xml:id="_pt3jdUd">
					<idno type="grant-number">22511106101</idno>
				</org>
				<org type="funding" xml:id="_GCfjUpq">
					<idno type="grant-number">18DZ2270700</idno>
				</org>
				<org type="funding" xml:id="_W2JdE76">
					<idno type="grant-number">21DZ1100-100</idno>
				</org>
				<org type="funding" xml:id="_WMfHYe3">
					<idno type="grant-number">BP0719010</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_2. The results are summarized in Table <ref type="table">2</ref>, and according to the comparison, GRACE shows a significant improvement in the unseen client. Combining Table <ref type="table">1</ref> and<ref type="table">2</ref>, it indicates that our gradient correction on both the local and global sides effectively enhances the generalization of the global model based on personalization. Generalization with TTDA. Considering the promise of recent test-data domain adaptation (TTDA) techniques for domain generalization in medical imaging <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, we also compare GRACE under the TTDA scenario with IOP-FL <ref type="bibr" target="#b12">[13]</ref>, DSBN <ref type="bibr" target="#b2">[3]</ref> and Tent <ref type="bibr" target="#b34">[35]</ref>). As shown in Table <ref type="table">3</ref>, GRACE obtains better results combined with DSBN and Tent, which means our method is orthogonal with TTDA, and TTDA can benefit from our generalized global model.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choudhary</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00818</idno>
		<title level="m">Federated learning with personalization layers</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Domain-specific batch normalization for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7354" to="7362" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On bridging generic and personalized federated learning for image classification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Federated domain generalization for image recognition via cross-client style transfer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>WACV</publisher>
			<biblScope unit="page" from="361" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hosted by the International Skin Imaging Collaboration (ISIC)</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gutman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Skin lesion analysis toward melanoma detection: a challenge at the 2017 International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>ISBI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Exploiting shared representations for personalized federated learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shakkottai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2089" to="2099" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Combalia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02288</idno>
		<title level="m">Bcn20000: dermoscopic lesions in the wild</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FLamby: datasets and benchmarks for cross-silo federated learning in realistic healthcare settings</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Du Terrail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Personalized federated learning: a metalearning approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07948</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2030" to="2096" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Harmofl: harmonizing local and global drifts in federated learning on heterogeneous medical images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1087" to="1095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08467</idno>
		<title level="m">IOP-FL: inside-outside personalization for federated medical image segmentation</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Scaffold: stochastic controlled averaging for federated learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Karimireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5132" to="5143" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computer-aided detection and diagnosis for prostate cancer based on mono and multi-parametric MRI: a review</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lemaître</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freixenet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="8" to="31" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Model-contrastive federated learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10713" to="10722" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Ditto: fair and robust federated learning through personalization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beirami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6357" to="6368" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Federated optimization in heterogeneous networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MLSys</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="429" to="450" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">FedBN: federated learning on non-IID features via local batch normalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluation of prostate segmentation algorithms for MRI: the promise12 challenge</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Toth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="359" to="373" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">FedDG: federated domain generalization on medical image segmentation via episodic learning in continuous frequency space</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1013" to="1023" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MS-net: multi-site network for improving prostate segmentation with heterogeneous MRI data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2713" to="2724" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transferable representation learning with deep adaptation networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3071" to="3085" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Test-time adaptation with calibration of medical image classification nets for label distribution shift</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
		<respStmt>
			<orgName>MICCAI</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
		<respStmt>
			<orgName>AISTATS</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">NCI-proc. IEEE-ISBI conference 2013 challenge: automated segmentation of prostate structures</title>
		<author>
			<persName><forename type="first">B</forename><surname>Nicholas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Can. Imaging Arch</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">FedBABU: toward enhanced representation for federated image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Federated learning for breast density classification: a real-world implementation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<editor>MICCAI Workshop</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="181" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Federated multi-task learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep coral: correlation alignment for deep domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="443" to="450" />
		</imprint>
		<respStmt>
			<orgName>ECCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Personalized federated learning with moreau envelopes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">J</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21394" to="21405" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tent: fully test-time adaptation by entropy minimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
